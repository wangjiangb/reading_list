<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 07 Dec 2023 06:00:08 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Relightable Gaussian Codec Avatars</title><link>http://arxiv.org/abs/2312.03704v1</link><description>The fidelity of relighting is bounded by both geometry and appearancerepresentations. For geometry, both mesh and volumetric approaches havedifficulty modeling intricate structures like 3D hair geometry. For appearance,existing relighting models are limited in fidelity and often too slow to renderin real-time with high-resolution continuous environments. In this work, wepresent Relightable Gaussian Codec Avatars, a method to build high-fidelityrelightable head avatars that can be animated to generate novel expressions.Our geometry model based on 3D Gaussians can capture 3D-consistentsub-millimeter details such as hair strands and pores on dynamic facesequences. To support diverse materials of human heads such as the eyes, skin,and hair in a unified manner, we present a novel relightable appearance modelbased on learnable radiance transfer. Together with global illumination-awarespherical harmonics for the diffuse components, we achieve real-time relightingwith spatially all-frequency reflections using spherical Gaussians. Thisappearance model can be efficiently relit under both point light and continuousillumination. We further improve the fidelity of eye reflections and enableexplicit gaze control by introducing relightable explicit eye models. Ourmethod outperforms existing approaches without compromising real-timeperformance. We also demonstrate real-time relighting of avatars on a tetheredconsumer VR headset, showcasing the efficiency and fidelity of our avatars.</description><author>Shunsuke Saito, Gabriel Schwartz, Tomas Simon, Junxuan Li, Giljoo Nam</author><pubDate>Wed, 06 Dec 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03704v1</guid></item><item><title>Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning</title><link>http://arxiv.org/abs/2312.03703v1</link><description>In-context learning provides a new perspective for multi-task modeling forvision and NLP. Under this setting, the model can perceive tasks from promptsand accomplish them without any extra task-specific head predictions or modelfine-tuning. However, Skeleton sequence modeling via in-context learningremains unexplored. Directly applying existing in-context models from otherareas onto skeleton sequences fails due to the inter-frame and cross-task posesimilarity that makes it outstandingly hard to perceive the task correctly froma subtle context. To address this challenge, we propose Skeleton-in-Context(SiC), an effective framework for in-context skeleton sequence modeling. OurSiC is able to handle multiple skeleton-based tasks simultaneously after asingle training process and accomplish each task from context according to thegiven prompt. It can further generalize to new, unseen tasks according tocustomized prompts. To facilitate context perception, we additionally propose atask-unified prompt, which adaptively learns tasks of different natures, suchas partial joint-level generation, sequence-level prediction, or 2D-to-3Dmotion prediction. We conduct extensive experiments to evaluate theeffectiveness of our SiC on multiple tasks, including motion prediction, poseestimation, joint completion, and future pose estimation. We also evaluate itsgeneralization capability on unseen tasks such as motion-in-between. Theseexperiments show that our model achieves state-of-the-art multi-taskperformance and even outperforms single-task methods on certain tasks.</description><author>Xinshun Wang, Zhongbin Fang, Xia Li, Xiangtai Li, Chen Chen, Mengyuan Liu</author><pubDate>Wed, 06 Dec 2023 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03703v1</guid></item><item><title>Self-conditioned Image Generation via Generating Representations</title><link>http://arxiv.org/abs/2312.03701v1</link><description>This paper presents $\textbf{R}$epresentation-$\textbf{C}$onditioned image$\textbf{G}$eneration (RCG), a simple yet effective image generation frameworkwhich sets a new benchmark in class-unconditional image generation. RCG doesnot condition on any human annotations. Instead, it conditions on aself-supervised representation distribution which is mapped from the imagedistribution using a pre-trained encoder. During generation, RCG samples fromsuch representation distribution using a representation diffusion model (RDM),and employs a pixel generator to craft image pixels conditioned on the sampledrepresentation. Such a design provides substantial guidance during thegenerative process, resulting in high-quality image generation. Tested onImageNet 256$\times$256, RCG achieves a Frechet Inception Distance (FID) of3.31 and an Inception Score (IS) of 253.4. These results not only significantlyimprove the state-of-the-art of class-unconditional image generation but alsorival the current leading methods in class-conditional image generation,bridging the long-standing performance gap between these two tasks. Code isavailable at https://github.com/LTH14/rcg.</description><author>Tianhong Li, Dina Katabi, Kaiming He</author><pubDate>Wed, 06 Dec 2023 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03701v1</guid></item><item><title>OneLLM: One Framework to Align All Modalities with Language</title><link>http://arxiv.org/abs/2312.03700v1</link><description>Multimodal large language models (MLLMs) have gained significant attentiondue to their strong multimodal understanding capability. However, existingworks rely heavily on modality-specific encoders, which usually differ inarchitecture and are limited to common modalities. In this paper, we presentOneLLM, an MLLM that aligns eight modalities to language using a unifiedframework. We achieve this through a unified multimodal encoder and aprogressive multimodal alignment pipeline. In detail, we first train an imageprojection module to connect a vision encoder with LLM. Then, we build auniversal projection module (UPM) by mixing multiple image projection modulesand dynamic routing. Finally, we progressively align more modalities to LLMwith the UPM. To fully leverage the potential of OneLLM in followinginstructions, we also curated a comprehensive multimodal instruction dataset,including 2M items from image, audio, video, point cloud, depth/normal map, IMUand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,encompassing tasks such as multimodal captioning, question answering andreasoning, where it delivers excellent performance. Code, data, model andonline demo are available at https://github.com/csuhan/OneLLM</description><author>Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue</author><pubDate>Wed, 06 Dec 2023 18:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03700v1</guid></item><item><title>A unified framework for information-theoretic generalization bounds</title><link>http://arxiv.org/abs/2305.11042v2</link><description>This paper presents a general methodology for deriving information-theoreticgeneralization bounds for learning algorithms. The main technical tool is aprobabilistic decorrelation lemma based on a change of measure and a relaxationof Young's inequality in $L_{\psi_p}$ Orlicz spaces. Using the decorrelationlemma in combination with other techniques, such as symmetrization, couplings,and chaining in the space of probability measures, we obtain new upper boundson the generalization error, both in expectation and in high probability, andrecover as special cases many of the existing generalization bounds, includingthe ones based on mutual information, conditional mutual information,stochastic chaining, and PAC-Bayes inequalities. In addition, theFernique-Talagrand upper bound on the expected supremum of a subgaussianprocess emerges as a special case.</description><author>Yifeng Chu, Maxim Raginsky</author><pubDate>Wed, 06 Dec 2023 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11042v2</guid></item><item><title>PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration</title><link>http://arxiv.org/abs/2312.03699v1</link><description>The advent of increasingly powerful language models has raised expectationsfor language-based interactions. However, controlling these models is achallenge, emphasizing the need to be able to investigate the feasibility andvalue of their application. We present PROMISE, a framework that facilitatesthe development of complex language-based interactions with informationsystems. Its use of state machine modeling concepts enables model-driven,dynamic prompt orchestration across hierarchically nested states andtransitions. This improves the control of the behavior of language models andthus enables their effective and efficient use. We show the benefits of PROMISEin the context of application scenarios within health information systems anddemonstrate its ability to handle complex interactions.</description><author>Wenyuan Wu, Jasmin Heierli, Max Meisterhans, Adrian Moser, Andri FÃ¤rber, Mateusz Dolata, Elena Gavagnin, Alexandre de Spindler, Gerhard Schwabe</author><pubDate>Wed, 06 Dec 2023 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03699v1</guid></item><item><title>Intrinsic Harmonization for Illumination-Aware Compositing</title><link>http://arxiv.org/abs/2312.03698v1</link><description>Despite significant advancements in network-based image harmonizationtechniques, there still exists a domain disparity between typical trainingpairs and real-world composites encountered during inference. Most existingmethods are trained to reverse global edits made on segmented image regions,which fail to accurately capture the lighting inconsistencies between theforeground and background found in composited images. In this work, weintroduce a self-supervised illumination harmonization approach formulated inthe intrinsic image domain. First, we estimate a simple global lighting modelfrom mid-level vision representations to generate a rough shading for theforeground region. A network then refines this inferred shading to generate aharmonious re-shading that aligns with the background scene. In order to matchthe color appearance of the foreground and background, we utilize ideas fromprior harmonization approaches to perform parameterized image edits in thealbedo domain. To validate the effectiveness of our approach, we presentresults from challenging real-world composites and conduct a user study toobjectively measure the enhanced realism achieved compared to state-of-the-artharmonization methods.</description><author>Chris Careaga, YaÄÄ±z Aksoy, S. Mahdi H. Miangoleh</author><pubDate>Wed, 06 Dec 2023 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03698v1</guid></item><item><title>Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US</title><link>http://arxiv.org/abs/2302.02560v3</link><description>In policy research, one of the most critical analytic tasks is to estimatethe causal effect of a policy-relevant shift to the distribution of acontinuous exposure/treatment on an outcome of interest. We call this problemshift-response function (SRF) estimation. Existing neural network methodsinvolving robust causal-effect estimators lack theoretical guarantees andpractical implementations for SRF estimation. Motivated by a keypolicy-relevant question in public health, we develop a neural network methodand its theoretical underpinnings to estimate SRFs with robustness andefficiency guarantees. We then apply our method to data consisting of 68million individuals and 27 million deaths across the U.S. to estimate thecausal effect from revising the US National Ambient Air Quality Standards(NAAQS) for PM 2.5 from 12 $\mu g/m^3$ to 9 $\mu g/m^3$. This change has beenrecently proposed by the US Environmental Protection Agency (EPA). Our goal isto estimate, for the first time, the reduction in deaths that would result fromthis anticipated revision using causal methods for SRFs. Our proposed method,called {T}argeted {R}egularization for {E}xposure {S}hifts with Neural{Net}works (TRESNET), contributes to the neural network literature for causalinference in two ways: first, it proposes a targeted regularization loss withtheoretical properties that ensure double robustness and achieves asymptoticefficiency specific for SRF estimation; second, it enables loss functions fromthe exponential family of distributions to accommodate non-continuous outcomedistributions (such as hospitalization or mortality counts). We complement ourapplication with benchmark experiments that demonstrate TRESNET's broadapplicability and competitiveness.</description><author>Mauricio Tec, Oladimeji Mudele, Kevin Josey, Francesca Dominici</author><pubDate>Wed, 06 Dec 2023 18:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02560v3</guid></item><item><title>Memory Triggers: Unveiling Memorization in Text-To-Image Generative Models through Word-Level Duplication</title><link>http://arxiv.org/abs/2312.03692v1</link><description>Diffusion-based models, such as the Stable Diffusion model, haverevolutionized text-to-image synthesis with their ability to producehigh-quality, high-resolution images. These advancements have promptedsignificant progress in image generation and editing tasks. However, thesemodels also raise concerns due to their tendency to memorize and potentiallyreplicate exact training samples, posing privacy risks and enabling adversarialattacks. Duplication in training datasets is recognized as a major factorcontributing to memorization, and various forms of memorization have beenstudied so far. This paper focuses on two distinct and underexplored types ofduplication that lead to replication during inference in diffusion-basedmodels, particularly in the Stable Diffusion model. We delve into theselesser-studied duplication phenomena and their implications through two casestudies, aiming to contribute to the safer and more responsible use ofgenerative models in various applications.</description><author>Ali Naseh, Jaechul Roh, Amir Houmansadr</author><pubDate>Wed, 06 Dec 2023 18:54:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03692v1</guid></item><item><title>On the Role of Edge Dependency in Graph Generative Models</title><link>http://arxiv.org/abs/2312.03691v1</link><description>In this work, we introduce a novel evaluation framework for generative modelsof graphs, emphasizing the importance of model-generated graph overlap(Chanpuriya et al., 2021) to ensure both accuracy and edge-diversity. Wedelineate a hierarchy of graph generative models categorized into three levelsof complexity: edge independent, node independent, and fully dependent models.This hierarchy encapsulates a wide range of prevalent methods. We derivetheoretical bounds on the number of triangles and other short-length cyclesproducible by each level of the hierarchy, contingent on the model overlap. Weprovide instances demonstrating the asymptotic optimality of our bounds.Furthermore, we introduce new generative models for each of the threehierarchical levels, leveraging dense subgraph discovery (Gionis &amp; Tsourakakis,2015). Our evaluation, conducted on real-world datasets, focuses on assessingthe output quality and overlap of our proposed models in comparison to otherpopular models. Our results indicate that our simple, interpretable modelsprovide competitive baselines to popular generative models. Through thisinvestigation, we aim to propel the advancement of graph generative models byoffering a structured framework and robust evaluation metrics, therebyfacilitating the development of models capable of generating accurate andedge-diverse graphs.</description><author>Sudhanshu Chanpuriya, Cameron Musco, Konstantinos Sotiropoulos, Charalampos Tsourakakis</author><pubDate>Wed, 06 Dec 2023 18:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03691v1</guid></item><item><title>Inverse Design of Vitrimeric Polymers by Molecular Dynamics and Generative Modeling</title><link>http://arxiv.org/abs/2312.03690v1</link><description>Vitrimer is a new class of sustainable polymers with the ability ofself-healing through rearrangement of dynamic covalent adaptive networks.However, a limited choice of constituent molecules restricts their propertyspace, prohibiting full realization of their potential applications. Through acombination of molecular dynamics (MD) simulations and machine learning (ML),particularly a novel graph variational autoencoder (VAE) model, we establish amethod for generating novel vitrimers and guide their inverse design based ondesired glass transition temperature (Tg). We build the first vitrimer datasetof one million and calculate Tg on 8,424 of them by high-throughput MDsimulations calibrated by a Gaussian process model. The proposed VAE employsdual graph encoders and a latent dimension overlapping scheme which allows forindividual representation of multi-component vitrimers. By constructing acontinuous latent space containing necessary information of vitrimers, wedemonstrate high accuracy and efficiency of our framework in discovering novelvitrimers with desirable Tg beyond the training regime. The proposed vitrimerswith reasonable synthesizability cover a wide range of Tg and broaden thepotential widespread usage of vitrimeric materials.</description><author>Yiwen Zheng, Prakash Thakolkaran, Jake A. Smith, Ziheng Lu, Shuxin Zheng, Bichlien H. Nguyen, Siddhant Kumar, Aniruddh Vashisth</author><pubDate>Wed, 06 Dec 2023 18:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03690v1</guid></item><item><title>Evaluating and Mitigating Discrimination in Language Model Decisions</title><link>http://arxiv.org/abs/2312.03689v1</link><description>As language models (LMs) advance, interest is growing in applying them tohigh-stakes societal decisions, such as determining financing or housingeligibility. However, their potential for discrimination in such contextsraises ethical concerns, motivating the need for better methods to evaluatethese risks. We present a method for proactively evaluating the potentialdiscriminatory impact of LMs in a wide range of use cases, includinghypothetical use cases where they have not yet been deployed. Specifically, weuse an LM to generate a wide array of potential prompts that decision-makersmay input into an LM, spanning 70 diverse decision scenarios across society,and systematically vary the demographic information in each prompt. Applyingthis methodology reveals patterns of both positive and negative discriminationin the Claude 2.0 model in select settings when no interventions are applied.While we do not endorse or permit the use of language models to make automateddecisions for the high-risk use cases we study, we demonstrate techniques tosignificantly decrease both positive and negative discrimination throughcareful prompt engineering, providing pathways toward safer deployment in usecases where they may be appropriate. Our work enables developers andpolicymakers to anticipate, measure, and address discrimination as languagemodel capabilities and applications continue to expand. We release our datasetand prompts at https://huggingface.co/datasets/Anthropic/discrim-eval</description><author>Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph, Shauna Kravec, Karina Nguyen, Jared Kaplan, Deep Ganguli</author><pubDate>Wed, 06 Dec 2023 18:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03689v1</guid></item><item><title>MatterGen: a generative model for inorganic materials design</title><link>http://arxiv.org/abs/2312.03687v1</link><description>The design of functional materials with desired properties is essential indriving technological advances in areas like energy storage, catalysis, andcarbon capture. Generative models provide a new paradigm for materials designby directly generating entirely novel materials given desired propertyconstraints. Despite recent progress, current generative models have lowsuccess rate in proposing stable crystals, or can only satisfy a very limitedset of property constraints. Here, we present MatterGen, a model that generatesstable, diverse inorganic materials across the periodic table and can furtherbe fine-tuned to steer the generation towards a broad range of propertyconstraints. To enable this, we introduce a new diffusion-based generativeprocess that produces crystalline structures by gradually refining atom types,coordinates, and the periodic lattice. We further introduce adapter modules toenable fine-tuning towards any given property constraints with a labeleddataset. Compared to prior generative models, structures produced by MatterGenare more than twice as likely to be novel and stable, and more than 15 timescloser to the local energy minimum. After fine-tuning, MatterGen successfullygenerates stable, novel materials with desired chemistry, symmetry, as well asmechanical, electronic and magnetic properties. Finally, we demonstratemulti-property materials design capabilities by proposing structures that haveboth high magnetic density and a chemical composition with low supply-chainrisk. We believe that the quality of generated materials and the breadth ofMatterGen's capabilities represent a major advancement towards creating auniversal generative model for materials design.</description><author>Claudio Zeni, Robert Pinsler, Daniel ZÃ¼gner, Andrew Fowler, Matthew Horton, Xiang Fu, Sasha Shysheya, Jonathan CrabbÃ©, Lixin Sun, Jake Smith, Ryota Tomioka, Tian Xie</author><pubDate>Wed, 06 Dec 2023 18:52:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03687v1</guid></item><item><title>What Planning Problems Can A Relational Neural Network Solve?</title><link>http://arxiv.org/abs/2312.03682v1</link><description>Goal-conditioned policies are generally understood to be "feed-forward"circuits, in the form of neural networks that map from the current state andthe goal specification to the next action to take. However, under whatcircumstances such a policy can be learned and how efficient the policy will beare not well understood. In this paper, we present a circuit complexityanalysis for relational neural networks (such as graph neural networks andtransformers) representing policies for planning problems, by drawingconnections with serialized goal regression search (S-GRS). We show that thereare three general classes of planning problems, in terms of the growth ofcircuit width and depth as a function of the number of objects and planninghorizon, providing constructive proofs. We also illustrate the utility of thisanalysis for designing neural networks for policy learning.</description><author>Jiayuan Mao, TomÃ¡s Lozano-PÃ©rez, Joshua B. Tenenbaum, Leslie Pack Kaelbling</author><pubDate>Wed, 06 Dec 2023 18:47:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03682v1</guid></item><item><title>WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation</title><link>http://arxiv.org/abs/2312.02934v2</link><description>Generating multi-camera street-view videos is critical for augmentingautonomous driving datasets, addressing the urgent demand for extensive andvaried data. Due to the limitations in diversity and challenges in handlinglighting conditions, traditional rendering-based methods are increasingly beingsupplanted by diffusion-based methods. However, a significant challenge indiffusion-based methods is ensuring that the generated sensor data preserveboth intra-world consistency and inter-sensor coherence. To address thesechallenges, we combine an additional explicit world volume and propose theWorld Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This systemis specifically designed to leverage 4D world volume as a foundational elementfor video generation. Our model operates in two distinct phases: (i)envisioning the future 4D temporal world volume based on vehicle controlsequences, and (ii) generating multi-camera videos, informed by this envisioned4D temporal world volume and sensor interconnectivity. The incorporation of the4D world volume empowers WoVoGen not only to generate high-quality street-viewvideos in response to vehicle control inputs but also to facilitate sceneediting tasks.</description><author>Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, Li Zhang</author><pubDate>Wed, 06 Dec 2023 18:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02934v2</guid></item><item><title>Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching</title><link>http://arxiv.org/abs/2312.03678v1</link><description>Non-isometric shape correspondence remains a fundamental challenge incomputer vision. Traditional methods using Laplace-Beltrami operator (LBO)eigenmodes face limitations in characterizing high-frequency extrinsic shapechanges like bending and creases. We propose a novel approach of combining thenon-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shellhessian with the intrinsic ones of the LBO, creating a hybrid spectral space inwhich we construct functional maps. To this end, we present a theoreticalframework to effectively integrate non-orthogonal basis functions intodescriptor- and learning-based functional map methods. Our approach can beincorporated easily into existing functional map pipelines across varyingapplications and is able to handle complex deformations beyond isometries. Weshow extensive evaluations across various supervised and unsupervised settingsand demonstrate significant improvements. Notably, our approach achieves up to15% better mean geodesic error for non-isometric correspondence settings and upto 45% improvement in scenarios with topological noise.</description><author>Lennart Bastian, Yizheng Xie, Nassir Navab, Zorah LÃ¤hner</author><pubDate>Wed, 06 Dec 2023 18:41:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03678v1</guid></item><item><title>GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models</title><link>http://arxiv.org/abs/2312.03675v1</link><description>This paper introduces GeoShapley, a game theory approach to measuring spatialeffects in machine learning models. GeoShapley extends the Nobel Prize-winningShapley value framework in game theory by conceptualizing location as a playerin a model prediction game, which enables the quantification of the importanceof location and the synergies between location and other features in a model.GeoShapley is a model-agnostic approach and can be applied to statistical orblack-box machine learning models in various structures. The interpretation ofGeoShapley is directly linked with spatially varying coefficient models forexplaining spatial effects and additive models for explaining non-spatialeffects. Using simulated data, GeoShapley values are validated against knowndata-generating processes and are used for cross-comparison of sevenstatistical and machine learning models. An empirical example of house pricemodeling is used to illustrate GeoShapley's utility and interpretation withreal world data. The method is available as an open-source Python package namedgeoshapley.</description><author>Ziqi Li</author><pubDate>Wed, 06 Dec 2023 18:39:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03675v1</guid></item><item><title>On the Role of the Action Space in Robot Manipulation Learning and Sim-to-Real Transfer</title><link>http://arxiv.org/abs/2312.03673v1</link><description>We study the choice of action space in robot manipulation learning andsim-to-real transfer. We define metrics that assess the performance, andexamine the emerging properties in the different action spaces. We train over250 reinforcement learning~(RL) agents in simulated reaching and pushing tasks,using 13 different control spaces. The choice of action spaces spans popularchoices in the literature as well as novel combinations of common designcharacteristics. We evaluate the training performance in simulation and thetransfer to a real-world environment. We identify good and bad characteristicsof robotic action spaces and make recommendations for future designs. Ourfindings have important implications for the design of RL algorithms for robotmanipulation tasks, and highlight the need for careful consideration of actionspaces when training and transferring RL agents for real-world robotics.</description><author>Elie Aljalbout, Felix Frank, Maximilian Karl, Patrick van der Smagt</author><pubDate>Wed, 06 Dec 2023 18:38:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03673v1</guid></item><item><title>Direct Exoplanet Detection Using Deep Convolutional Image Reconstruction (ConStruct): A New Algorithm for Post-Processing High-Contrast Images</title><link>http://arxiv.org/abs/2312.03671v1</link><description>We present a novel machine-learning approach for detecting faint pointsources in high-contrast adaptive optics imaging datasets. The most widely usedalgorithms for primary subtraction aim to decouple bright stellar speckle noisefrom planetary signatures by subtracting an approximation of the temporallyevolving stellar noise from each frame in an imaging sequence. Our approachaims to improve the stellar noise approximation and increase the planetdetection sensitivity by leveraging deep learning in a novel direct imagingpost-processing algorithm. We show that a convolutional autoencoder neuralnetwork, trained on an extensive reference library of real imaging sequences,accurately reconstructs the stellar speckle noise at the location of apotential planet signal. This tool is used in a post-processing algorithm wecall Direct Exoplanet Detection with Convolutional Image Reconstruction, orConStruct. The reliability and sensitivity of ConStruct are assessed using realKeck/NIRC2 angular differential imaging datasets. Of the 30 unique pointsources we examine, ConStruct yields a higher S/N than traditional PCA-basedprocessing for 67$\%$ of the cases and improves the relative contrast by up toa factor of 2.6. This work demonstrates the value and potential of deeplearning to take advantage of a diverse reference library of point spreadfunction realizations to improve direct imaging post-processing. ConStruct andits future improvements may be particularly useful as tools for post-processinghigh-contrast images from the James Webb Space Telescope and extreme adaptiveoptics instruments, both for the current generation and those being designedfor the upcoming 30 meter-class telescopes.</description><author>Trevor N. Wolf, Brandon A. Jones, Brendan P. Bowler</author><pubDate>Wed, 06 Dec 2023 18:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03671v1</guid></item><item><title>An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition</title><link>http://arxiv.org/abs/2312.03668v1</link><description>Advances in machine learning have made it possible to perform various textand speech processing tasks, including automatic speech recognition (ASR), inan end-to-end (E2E) manner. Since typical E2E approaches require large amountsof training data and resources, leveraging pre-trained foundation modelsinstead of training from scratch is gaining attention. Although there have beenattempts to use pre-trained speech and language models in ASR, most of them arelimited to using either. This paper explores the potential of integrating apre-trained speech representation model with a large language model (LLM) forE2E ASR. The proposed model enables E2E ASR by generating text tokens in anautoregressive manner via speech representations as speech prompts, takingadvantage of the vast knowledge provided by the LLM. Furthermore, the proposedmodel can incorporate remarkable developments for LLM utilization, such asinference optimization and parameter-efficient domain adaptation. Experimentalresults show that the proposed model achieves performance comparable to modernE2E ASR models.</description><author>Yukiya Hono, Koh Mitsuda, Tianyu Zhao, Kentaro Mitsui, Toshiaki Wakatsuki, Kei Sawada</author><pubDate>Wed, 06 Dec 2023 18:34:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03668v1</guid></item><item><title>WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual Try-on</title><link>http://arxiv.org/abs/2312.03667v1</link><description>Image-based Virtual Try-On (VITON) aims to transfer an in-shop garment imageonto a target person. While existing methods focus on warping the garment tofit the body pose, they often overlook the synthesis quality around thegarment-skin boundary and realistic effects like wrinkles and shadows on thewarped garments. These limitations greatly reduce the realism of the generatedresults and hinder the practical application of VITON techniques. Leveragingthe notable success of diffusion-based models in cross-modal image synthesis,some recent diffusion-based methods have ventured to tackle this issue.However, they tend to either consume a significant amount of training resourcesor struggle to achieve realistic try-on effects and retain garment details. Forefficient and high-fidelity VITON, we propose WarpDiffusion, which bridges thewarping-based and diffusion-based paradigms via a novel informative and localgarment feature attention mechanism. Specifically, WarpDiffusion incorporateslocal texture attention to reduce resource consumption and uses a novelauto-mask module that effectively retains only the critical areas of the warpedgarment while disregarding unrealistic or erroneous portions. Notably,WarpDiffusion can be integrated as a plug-and-play component into existingVITON methodologies, elevating their synthesis quality. Extensive experimentson high-resolution VITON benchmarks and an in-the-wild test set demonstrate thesuperiority of WarpDiffusion, surpassing state-of-the-art methods bothqualitatively and quantitatively.</description><author>xujie zhang, Xiu Li, Michael Kampffmeyer, Xin Dong, Zhenyu Xie, Feida Zhu, Haoye Dong, Xiaodan Liang</author><pubDate>Wed, 06 Dec 2023 18:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03667v1</guid></item><item><title>Towards small and accurate convolutional neural networks for acoustic biodiversity monitoring</title><link>http://arxiv.org/abs/2312.03666v1</link><description>Automated classification of animal sounds is a prerequisite for large-scalemonitoring of biodiversity. Convolutional Neural Networks (CNNs) are among themost promising algorithms but they are slow, often achieve poor classificationin the field and typically require large training data sets. Our objective wasto design CNNs that are fast at inference time and achieve good classificationperformance while learning from moderate-sized data. Recordings from arainforest ecosystem were used. Start and end-point of sounds from 20 birdspecies were manually annotated. Spectrograms from 10 second segments were usedas CNN input. We designed simple CNNs with a frequency unwrapping layer(SIMP-FU models) such that any output unit was connected to all spectrogramfrequencies but only to a sub-region of time, the Receptive Field (RF). Ourmodels allowed experimentation with different RF durations. Models either usedthe time-indexed labels that encode start and end-point of sounds or simplersegment-level labels. Models learning from time-indexed labels performedconsiderably better than their segment-level counterparts. Best classificationperformances was achieved for models with intermediate RF duration of 1.5seconds. The best SIMP-FU models achieved AUCs over 0.95 in 18 of 20 classes onthe test set. On compact low-cost hardware the best SIMP-FU models evaluated upto seven times faster than real-time data acquisition. RF duration was a majordriver of classification performance. The optimum of 1.5 s was in the samerange as the duration of the sounds. Our models achieved good classificationperformance while learning from moderate-sized training data. This is explainedby the usage of time-indexed labels during training and adequately sized RF.Results confirm the feasibility of deploying small CNNs with goodclassification performance on compact low-cost devices.</description><author>Serge Zaugg, Mike van der Schaar, Florence Erbs, Antonio Sanchez, Joan V. Castell, Emiliano Ramallo, Michel AndrÃ©</author><pubDate>Wed, 06 Dec 2023 18:34:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03666v1</guid></item><item><title>Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia</title><link>http://arxiv.org/abs/2312.03664v1</link><description>Agent-based modeling has been around for decades, and applied widely acrossthe social and natural sciences. The scope of this research method is nowpoised to grow dramatically as it absorbs the new affordances provided by LargeLanguage Models (LLM)s. Generative Agent-Based Models (GABM) are not justclassic Agent-Based Models (ABM)s where the agents talk to one another. Rather,GABMs are constructed using an LLM to apply common sense to situations, act"reasonably", recall common semantic knowledge, produce API calls to controldigital technologies like apps, and communicate both within the simulation andto researchers viewing it from the outside. Here we present Concordia, alibrary to facilitate constructing and working with GABMs. Concordia makes iteasy to construct language-mediated simulations of physically- ordigitally-grounded environments. Concordia agents produce their behavior usinga flexible component system which mediates between two fundamental operations:LLM calls and associative memory retrieval. A special agent called the GameMaster (GM), which was inspired by tabletop role-playing games, is responsiblefor simulating the environment where the agents interact. Agents take actionsby describing what they want to do in natural language. The GM then translatestheir actions into appropriate implementations. In a simulated physical world,the GM checks the physical plausibility of agent actions and describes theireffects. In digital environments simulating technologies such as apps andservices, the GM may handle API calls to integrate with external tools such asgeneral AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar,Email, Search, etc.). Concordia was designed to support a wide array ofapplications both in scientific research and for evaluating performance of realdigital services by simulating users and/or generating synthetic data.</description><author>Alexander Sasha Vezhnevets, John P. Agapiou, Avia Aharon, Ron Ziv, Jayd Matyas, Edgar A. DuÃ©Ã±ez-GuzmÃ¡n, William A. Cunningham, Simon Osindero, Danny Karmon, Joel Z. Leibo</author><pubDate>Wed, 06 Dec 2023 18:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03664v1</guid></item><item><title>Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving</title><link>http://arxiv.org/abs/2312.03661v1</link><description>Large vision-language models (VLMs) have garnered increasing interest inautonomous driving areas, due to their advanced capabilities in complexreasoning tasks essential for highly autonomous vehicle behavior. Despite theirpotential, research in autonomous systems is hindered by the lack of datasetswith annotated reasoning chains that explain the decision-making processes indriving. To bridge this gap, we present Reason2Drive, a benchmark dataset withover 600K video-text pairs, aimed at facilitating the study of interpretablereasoning in complex driving environments. We distinctly characterize theautonomous driving process as a sequential combination of perception,prediction, and reasoning steps, and the question-answer pairs areautomatically collected from a diverse range of open-source outdoor drivingdatasets, including nuScenes, Waymo and ONCE. Moreover, we introduce a novelaggregated evaluation metric to assess chain-based reasoning performance inautonomous systems, addressing the semantic ambiguities of existing metricssuch as BLEU and CIDEr. Based on the proposed benchmark, we conduct experimentsto assess various existing VLMs, revealing insights into their reasoningcapabilities. Additionally, we develop an efficient approach to empower VLMs toleverage object-level perceptual elements in both feature extraction andprediction, further enhancing their reasoning accuracy. The code and datasetwill be released.</description><author>Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, Jianhua Han, Hang Xu, Li Zhang</author><pubDate>Wed, 06 Dec 2023 18:32:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03661v1</guid></item><item><title>Interpretability Illusions in the Generalization of Simplified Models</title><link>http://arxiv.org/abs/2312.03656v1</link><description>A common method to study deep learning systems is to use simplified modelrepresentations -- for example, using singular value decomposition to visualizethe model's hidden states in a lower dimensional space. This approach assumesthat the results of these simplified are faithful to the original model. Here,we illustrate an important caveat to this assumption: even if the simplifiedrepresentations can accurately approximate the full model on the training set,they may fail to accurately capture the model's behavior out of distribution --the understanding developed from simplified representations may be an illusion.We illustrate this by training Transformer models on controlled datasets withsystematic generalization splits. First, we train models on the Dyckbalanced-parenthesis languages. We simplify these models using tools likedimensionality reduction and clustering, and then explicitly test how thesesimplified proxies match the behavior of the original model on variousout-of-distribution test sets. We find that the simplified proxies aregenerally less faithful out of distribution. In cases where the original modelgeneralizes to novel structures or deeper depths, the simplified versions mayfail, or generalize better. This finding holds even if the simplifiedrepresentations do not directly depend on the training distribution. Next, westudy a more naturalistic task: predicting the next character in a dataset ofcomputer code. We find similar generalization gaps between the original modeland simplified proxies, and conduct further analysis to investigate whichaspects of the code completion task are associated with the largest gaps.Together, our results raise questions about the extent to which mechanisticinterpretations derived using tools like SVD can reliably predict what a modelwill do in novel situations.</description><author>Dan Friedman, Andrew Lampinen, Lucas Dixon, Danqi Chen, Asma Ghandeharioun</author><pubDate>Wed, 06 Dec 2023 18:25:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03656v1</guid></item><item><title>Source-Free Domain Adaptation for RGB-D Semantic Segmentation with Vision Transformers</title><link>http://arxiv.org/abs/2305.14269v2</link><description>With the increasing availability of depth sensors, multimodal frameworks thatcombine color information with depth data are gaining interest. However, groundtruth data for semantic segmentation is burdensome to provide, thus makingdomain adaptation a significant research area. Yet most domain adaptationmethods are not able to effectively handle multimodal data. Specifically, weaddress the challenging source-free domain adaptation setting where theadaptation is performed without reusing source data. We propose MISFIT:MultImodal Source-Free Information fusion Transformer, a depth-aware frameworkwhich injects depth data into a segmentation module based on visiontransformers at multiple stages, namely at the input, feature and outputlevels. Color and depth style transfer helps early-stage domain alignment whilere-wiring self-attention between modalities creates mixed features, allowingthe extraction of better semantic content. Furthermore, a depth-based entropyminimization strategy is also proposed to adaptively weight regions atdifferent distances. Our framework, which is also the first approach usingRGB-D vision transformers for source-free semantic segmentation, showsnoticeable performance improvements with respect to standard strategies.</description><author>Giulia Rizzoli, Donald Shenaj, Pietro Zanuttigh</author><pubDate>Wed, 06 Dec 2023 18:21:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14269v2</guid></item><item><title>Efficient Inverse Design Optimization through Multi-fidelity Simulations, Machine Learning, and Search Space Reduction Strategies</title><link>http://arxiv.org/abs/2312.03654v1</link><description>This paper introduces a methodology designed to augment the inverse designoptimization process in scenarios constrained by limited compute, through thestrategic synergy of multi-fidelity evaluations, machine learning models, andoptimization algorithms. The proposed methodology is analyzed on two distinctengineering inverse design problems: airfoil inverse design and the scalarfield reconstruction problem. It leverages a machine learning model trainedwith low-fidelity simulation data, in each optimization cycle, therebyproficiently predicting a target variable and discerning whether ahigh-fidelity simulation is necessitated, which notably conserves computationalresources. Additionally, the machine learning model is strategically deployedprior to optimization to reduce the search space, thereby further acceleratingconvergence toward the optimal solution. The methodology has been employed toenhance two optimization algorithms, namely Differential Evolution and ParticleSwarm Optimization. Comparative analyses illustrate performance improvementsacross both algorithms. Notably, this method is adeptly adaptable across anyinverse design application, facilitating a harmonious synergy between arepresentative low-fidelity machine learning model, and high-fidelitysimulation, and can be seamlessly applied across any variety ofpopulation-based optimization algorithms.</description><author>Luka Grbcic, Juliane MÃ¼ller, Wibe Albert de Jong</author><pubDate>Wed, 06 Dec 2023 18:20:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03654v1</guid></item><item><title>MICRACLE: Inverse Reinforcement and Curriculum Learning Model for Human-inspired Mobile Robot Navigation</title><link>http://arxiv.org/abs/2312.03651v1</link><description>In emergency scenarios, mobile robots must navigate like humans, interpretingstimuli to locate potential victims rapidly without interfering with firstresponders. Existing socially-aware navigation algorithms face computationaland adaptability challenges. To overcome these, we propose a solution, MIRACLE-- an inverse reinforcement and curriculum learning model, that employsgamified learning to gather stimuli-driven human navigational data. This datais then used to train a Deep Inverse Maximum Entropy Reinforcement Learningmodel, reducing reliance on demonstrator abilities. Testing reveals a low lossof 2.7717 within a 400-sized environment, signifying human-like responsereplication. Current databases lack comprehensive stimuli-driven data,necessitating our approach. By doing so, we enable robots to navigate emergencysituations with human-like perception, enhancing their life-savingcapabilities.</description><author>Nihal Gunukula, Kshitij Tiwari, Aniket Bera</author><pubDate>Wed, 06 Dec 2023 18:13:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03651v1</guid></item><item><title>Targeted Separation and Convergence with Kernel Discrepancies</title><link>http://arxiv.org/abs/2209.12835v3</link><description>Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD)have grown central to a wide range of applications, including hypothesistesting, sampler selection, distribution approximation, and variationalinference. In each setting, these kernel-based discrepancy measures arerequired to (i) separate a target P from other probability measures or even(ii) control weak convergence to P. In this article we derive new sufficientand necessary conditions to ensure (i) and (ii). For MMDs on separable metricspaces, we characterize those kernels that separate Bochner embeddable measuresand introduce simple conditions for separating all measures with unboundedkernels and for controlling convergence with bounded kernels. We use theseresults on $\mathbb{R}^d$ to substantially broaden the known conditions for KSDseparation and convergence control and to develop the first KSDs known toexactly metrize weak convergence to P. Along the way, we highlight theimplications of our results for hypothesis testing, measuring and improvingsample quality, and sampling with Stein variational gradient descent.</description><author>Alessandro Barp, Carl-Johann Simon-Gabriel, Mark Girolami, Lester Mackey</author><pubDate>Wed, 06 Dec 2023 18:09:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.12835v3</guid></item><item><title>Incorporating Crowdsourced Annotator Distributions into Ensemble Modeling to Improve Classification Trustworthiness for Ancient Greek Papyri</title><link>http://arxiv.org/abs/2210.16380v3</link><description>Performing classification on noisy, crowdsourced image datasets can provechallenging even for the best neural networks. Two issues which complicate theproblem on such datasets are class imbalance and ground-truth uncertainty inlabeling. The AL-ALL and AL-PUB datasets - consisting of tightly cropped,individual characters from images of ancient Greek papyri - are stronglyaffected by both issues. The application of ensemble modeling to such datasetscan help identify images where the ground-truth is questionable and quantifythe trustworthiness of those samples. As such, we apply stacked generalizationconsisting of nearly identical ResNets with different loss functions: oneutilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence(KLD). Both networks use labels drawn from a crowd-sourced consensus. Thisconsensus is derived from a Normalized Distribution of Annotations (NDA) basedon all annotations for a given character in the dataset. For the secondnetwork, the KLD is calculated with respect to the NDA. For our ensemble model,we apply a k-nearest neighbors model to the outputs of the CXE and KLDnetworks. Individually, the ResNet models have approximately 93% accuracy,while the ensemble model achieves an accuracy of &gt; 95%, increasing theclassification trustworthiness. We also perform an analysis of the Shannonentropy of the various models' output distributions to measure classificationuncertainty. Our results suggest that entropy is useful for predicting modelmisclassifications.</description><author>Graham West, Matthew I. Swindall, Ben Keener, Timothy Player, Alex C. Williams, James H. Brusuelas, John F. Wallin</author><pubDate>Wed, 06 Dec 2023 18:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16380v3</guid></item><item><title>Editable Stain Transformation Of Histological Images Using Unpaired GANs</title><link>http://arxiv.org/abs/2312.03647v1</link><description>Double staining in histopathology, particularly for metaplastic breastcancer, typically employs H&amp;E and P63 dyes. However, P63's tissue damage andhigh cost necessitate alternative methods. This study introduces xAI-CycleGAN,an advanced architecture combining Mask CycleGAN with explainability featuresand structure-preserving capabilities for transforming H&amp;E stained breasttissue images into P63-like images. The architecture allows for output editing,enhancing resemblance to actual images and enabling further model refinement.We showcase xAI-CycleGAN's efficacy in maintaining structural integrity andgenerating high-quality images. Additionally, a histopathologist surveyindicates the generated images' realism is often comparable to actual images,validating our model's high-quality output.</description><author>Tibor Sloboda, LukÃ¡Å¡ Hudec, Wanda BeneÅ¡ovÃ¡</author><pubDate>Wed, 06 Dec 2023 18:05:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03647v1</guid></item><item><title>MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment</title><link>http://arxiv.org/abs/2312.03644v1</link><description>Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarioswhere online interaction is impractical or risky. While independent learning inMARL offers flexibility and scalability, accurately assigning credit toindividual agents in offline settings poses challenges due to partialobservability and emergent behavior. Directly transferring the online creditassignment method to offline settings results in suboptimal outcomes due to theabsence of real-time feedback and intricate agent interactions. Our approach,MACCA, characterizing the generative process as a Dynamic Bayesian Network,captures relationships between environmental variables, states, actions, andrewards. Estimating this model on offline data, MACCA can learn each agent'scontribution by analyzing the causal relationship of their individual rewards,ensuring accurate and interpretable credit assignment. Additionally, themodularity of our approach allows it to seamlessly integrate with variousoffline MARL methods. Theoretically, we proved that under the setting of theoffline dataset, the underlying causal structure and the function forgenerating the individual rewards of agents are identifiable, which laid thefoundation for the correctness of our modeling. Experimentally, we tested MACCAin two environments, including discrete and continuous action settings. Theresults show that MACCA outperforms SOTA methods and improves performance upontheir backbones.</description><author>Ziyan Wang, Yali Du, Yudi Zhang, Meng Fang, Biwei Huang</author><pubDate>Wed, 06 Dec 2023 17:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03644v1</guid></item><item><title>Transformer-Powered Surrogates Close the ICF Simulation-Experiment Gap with Extremely Limited Data</title><link>http://arxiv.org/abs/2312.03642v1</link><description>Recent advances in machine learning, specifically transformer architecture,have led to significant advancements in commercial domains. These powerfulmodels have demonstrated superior capability to learn complex relationships andoften generalize better to new data and problems. This paper presents a noveltransformer-powered approach for enhancing prediction accuracy in multi-modaloutput scenarios, where sparse experimental data is supplemented withsimulation data. The proposed approach integrates transformer-basedarchitecture with a novel graph-based hyper-parameter optimization technique.The resulting system not only effectively reduces simulation bias, but alsoachieves superior prediction accuracy compared to the prior method. Wedemonstrate the efficacy of our approach on inertial confinement fusionexperiments, where only 10 shots of real-world data are available, as well assynthetic versions of these experiments.</description><author>Matthew L. Olson, Shusen Liu, Jayaraman J. Thiagarajan, Bogdan Kustowski, Weng-Keen Wong, Rushil Anirudh</author><pubDate>Wed, 06 Dec 2023 17:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03642v1</guid></item><item><title>Improved Convergence of Score-Based Diffusion Models via Prediction-Correction</title><link>http://arxiv.org/abs/2305.14164v2</link><description>Score-based generative models (SGMs) are powerful tools to sample fromcomplex data distributions. Their underlying idea is to (i) run a forwardprocess for time $T_1$ by adding noise to the data, (ii) estimate its scorefunction, and (iii) use such estimate to run a reverse process. As the reverseprocess is initialized with the stationary distribution of the forward one, theexisting analysis paradigm requires $T_1\to\infty$. This is howeverproblematic: from a theoretical viewpoint, for a given precision of the scoreapproximation, the convergence guarantee fails as $T_1$ diverges; from apractical viewpoint, a large $T_1$ increases computational costs and leads toerror propagation. This paper addresses the issue by considering a version ofthe popular predictor-corrector scheme: after running the forward process, wefirst estimate the final distribution via an inexact Langevin dynamics and thenrevert the process. Our key technical contribution is to provide convergenceguarantees which require to run the forward process only for a fixed finitetime $T_1$. Our bounds exhibit a mild logarithmic dependence on the inputdimension and the subgaussian norm of the target distribution, have minimalassumptions on the data, and require only to control the $L^2$ loss on thescore approximation, which is the quantity minimized in practice.</description><author>Francesco Pedrotti, Jan Maas, Marco Mondelli</author><pubDate>Wed, 06 Dec 2023 17:50:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14164v2</guid></item><item><title>MotionCtrl: A Unified and Flexible Motion Controller for Video Generation</title><link>http://arxiv.org/abs/2312.03641v1</link><description>Motions in a video primarily consist of camera motion, induced by cameramovement, and object motion, resulting from object movement. Accurate controlof both camera and object motion is essential for video generation. However,existing works either mainly focus on one type of motion or do not clearlydistinguish between the two, limiting their control capabilities and diversity.Therefore, this paper presents MotionCtrl, a unified and flexible motioncontroller for video generation designed to effectively and independentlycontrol camera and object motion. The architecture and training strategy ofMotionCtrl are carefully devised, taking into account the inherent propertiesof camera motion, object motion, and imperfect training data. Compared toprevious methods, MotionCtrl offers three main advantages: 1) It effectivelyand independently controls camera motion and object motion, enabling morefine-grained motion control and facilitating flexible and diverse combinationsof both types of motion. 2) Its motion conditions are determined by cameraposes and trajectories, which are appearance-free and minimally impact theappearance or shape of objects in generated videos. 3) It is a relativelygeneralizable model that can adapt to a wide array of camera poses andtrajectories once trained. Extensive qualitative and quantitative experimentshave been conducted to demonstrate the superiority of MotionCtrl over existingmethods.</description><author>Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan</author><pubDate>Wed, 06 Dec 2023 17:49:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03641v1</guid></item><item><title>Training Neural Networks on RAW and HDR Images for Restoration Tasks</title><link>http://arxiv.org/abs/2312.03640v1</link><description>The vast majority of standard image and video content available online isrepresented in display-encoded color spaces, in which pixel values areconveniently scaled to a limited range (0-1) and the color distribution isapproximately perceptually uniform. In contrast, both camera RAW and highdynamic range (HDR) images are often represented in linear color spaces, inwhich color values are linearly related to colorimetric quantities of light.While training on commonly available display-encoded images is awell-established practice, there is no consensus on how neural networks shouldbe trained for tasks on RAW and HDR images in linear color spaces. In thiswork, we test several approaches on three popular image restorationapplications: denoising, deblurring, and single-image super-resolution. Weexamine whether HDR/RAW images need to be display-encoded using populartransfer functions (PQ, PU21, mu-law), or whether it is better to train inlinear color spaces, but use loss functions that correct for perceptualnon-uniformity. Our results indicate that neural networks train significantlybetter on HDR and RAW images represented in display-encoded color spaces, whichoffer better perceptual uniformity than linear spaces. This small change to thetraining strategy can bring a very substantial gain in performance, up to 10-15dB.</description><author>Lei Luo, Alexandre Chapiro, Xiaoyu Xiang, Yuchen Fan, Rakesh Ranjan, Rafal Mantiuk</author><pubDate>Wed, 06 Dec 2023 17:47:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03640v1</guid></item><item><title>Not All Large Language Models (LLMs) Succumb to the "Reversal Curse": A Comparative Study of Deductive Logical Reasoning in BERT and GPT Models</title><link>http://arxiv.org/abs/2312.03633v1</link><description>The "Reversal Curse" refers to the scenario where auto-regressive decoderlarge language models (LLMs), such as ChatGPT, trained on "A is B" fail tolearn "B is A", demonstrating a basic failure of logical deduction. This raisesa red flag in the use of GPT models for certain general tasks such asconstructing knowledge graphs, considering their adherence to this symmetricprinciple. In our study, we examined a bidirectional LLM, BERT, and found thatit is immune to the reversal curse. Driven by ongoing efforts to constructbiomedical knowledge graphs with LLMs, we also embarked on evaluating morecomplex but essential deductive reasoning capabilities. This process includedfirst training encoder and decoder language models to master the intersection($\cap$) and union ($\cup$) operations on two sets and then moving on to assesstheir capability to infer different combinations of union ($\cup$) andintersection ($\cap$) operations on three newly created sets. The findingsshowed that while both encoder and decoder language models, trained for tasksinvolving two sets (union/intersection), were proficient in such scenarios,they encountered difficulties when dealing with operations that included threesets (various combinations of union and intersection). Our research highlightsthe distinct characteristics of encoder and decoder models in simple andcomplex logical reasoning. In practice, the choice between BERT and GPT shouldbe guided by the specific requirements and nature of the task at hand,leveraging their respective strengths in bidirectional context comprehensionand sequence prediction.</description><author>Jingye Yang, Da Wu, Kai Wang</author><pubDate>Wed, 06 Dec 2023 17:29:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03633v1</guid></item><item><title>Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models</title><link>http://arxiv.org/abs/2312.03632v1</link><description>Interactions with virtual assistants typically start with a trigger phrasefollowed by a command. In this work, we explore the possibility of making theseinteractions more natural by eliminating the need for a trigger phrase. Ourgoal is to determine whether a user addressed the virtual assistant based onsignals obtained from the streaming audio recorded by the device microphone. Weaddress this task by combining 1-best hypotheses and decoder signals from anautomatic speech recognition system with acoustic representations from an audioencoder as input features to a large language model (LLM). In particular, weare interested in data and resource efficient systems that require only a smallamount of training data and can operate in scenarios with only a single frozenLLM available on a device. For this reason, our model is trained on 80k or lessexamples of multimodal data using a combination of low-rank adaptation andprefix tuning. We compare the proposed system to unimodal baselines and showthat the multimodal approach achieves lower equal-error-rates (EERs), whileusing only a fraction of the training data. We also show that low-dimensionalspecialized audio representations lead to lower EERs than high-dimensionalgeneral audio representations.</description><author>Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi</author><pubDate>Wed, 06 Dec 2023 17:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03632v1</guid></item><item><title>MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations</title><link>http://arxiv.org/abs/2312.03631v1</link><description>While recent years have seen rapid progress in image-conditioned textgeneration, image captioning still suffers from the fundamental issue ofhallucinations, the generation of spurious details that cannot be inferred fromthe given image. Dedicated methods for reducing hallucinations in imagecaptioning largely focus on closed-vocabulary object tokens, ignoring mosttypes of hallucinations that occur in practice. In this work, we propose MOCHa,an approach that harnesses advancements in reinforcement learning (RL) toaddress the sequence-level nature of hallucinations in an open-world setup. Tooptimize for caption fidelity to the input image, we leverage ground-truthreference captions as proxies to measure the logical consistency of generatedcaptions. However, optimizing for caption fidelity alone fails to preserve thesemantic adequacy of generations; therefore, we propose a multi-objectivereward function that jointly targets these qualities, without requiring anystrong supervision. We demonstrate that these goals can be simultaneouslyoptimized with our framework, enhancing performance for various captioningmodels of different scales. Our qualitative and quantitative resultsdemonstrate MOCHa's superior performance across various established metrics. Wealso demonstrate the benefit of our method in the open-vocabulary setting. Tothis end, we contribute OpenCHAIR, a new benchmark for quantifyingopen-vocabulary hallucinations in image captioning models, constructed usinggenerative foundation models. We will release our code, benchmark, and trainedmodels.</description><author>Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Averbuch-Elor</author><pubDate>Wed, 06 Dec 2023 17:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03631v1</guid></item><item><title>Boosting Segment Anything Model Towards Open-Vocabulary Learning</title><link>http://arxiv.org/abs/2312.03628v1</link><description>The recent Segment Anything Model (SAM) has emerged as a new paradigmaticvision foundation model, showcasing potent zero-shot generalization andflexible prompting. Despite SAM finding applications and adaptations in variousdomains, its primary limitation lies in the inability to grasp objectsemantics. In this paper, we present Sambor to seamlessly integrate SAM withthe open-vocabulary object detector in an end-to-end framework. While retainingall the remarkable capabilities inherent to SAM, we enhance it with thecapacity to detect arbitrary objects based on human inputs like category namesor reference expressions. To accomplish this, we introduce a novel SideFormermodule that extracts SAM features to facilitate zero-shot object localizationand inject comprehensive semantic information for open-vocabulary recognition.In addition, we devise an open-set region proposal network (Open-set RPN),enabling the detector to acquire the open-set proposals generated by SAM.Sambor demonstrates superior zero-shot performance across benchmarks, includingCOCO and LVIS, proving highly competitive against previous SoTA methods. Weaspire for this work to serve as a meaningful endeavor in endowing SAM torecognize diverse object categories and advancing open-vocabulary learning withthe support of vision foundation models.</description><author>Xumeng Han, Longhui Wei, Xuehui Yu, Zhiyang Dou, Xin He, Kuiran Wang, Zhenjun Han, Qi Tian</author><pubDate>Wed, 06 Dec 2023 17:19:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03628v1</guid></item><item><title>KPI Extraction from Maintenance Work Orders -- A Comparison of Expert Labeling, Text Classification and AI-Assisted Tagging for Computing Failure Rates of Wind Turbines</title><link>http://arxiv.org/abs/2311.04064v2</link><description>Maintenance work orders are commonly used to document information about windturbine operation and maintenance. This includes details about proactive andreactive wind turbine downtimes, such as preventative and correctivemaintenance. However, the information contained in maintenance work orders isoften unstructured and difficult to analyze, presenting challenges fordecision-makers wishing to use it for optimizing operation and maintenance. Toaddress this issue, this work compares three different approaches to calculatereliability by performance indicators from maintenance work orders. The firstapproach involves manual labeling of the maintenance work orders by domainexperts, using the schema defined in an industrial guideline to assign thelabel accordingly. The second approach involves the development of a model thatautomatically labels the maintenance work orders using text classificationmethods. Through this method, we are able to achieve macro average and weightedaverage F1-Scores of 0.75 and 0.85 respectively. The third technique uses anAI-assisted tagging tool to tag and structure the raw maintenance information,together with a novel rule-based approach for extracting relevant maintenancework orders for failure rate calculation. In our experiments the AI-assistedtool leads to a 88% drop in tagging time in comparison to the other twoapproaches, while expert labeling and text classification are more accurate inKPI extraction. Overall, our findings make extracting maintenance informationfrom maintenance work orders more efficient, enable the assessment ofreliability key performance indicators and therefore support the optimizationof wind turbine operation and maintenance.</description><author>Marc-Alexander Lutz, Bastian SchÃ¤fermeier, Rachael Sexton, Michael Sharp, Alden Dima, Stefan Faulstich, Jagan Mohini Aluri</author><pubDate>Wed, 06 Dec 2023 17:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04064v2</guid></item><item><title>Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation</title><link>http://arxiv.org/abs/2311.18260v2</link><description>Radiology reports are an instrumental part of modern medicine, informing keyclinical decisions such as diagnosis and treatment. The worldwide shortage ofradiologists, however, restricts access to expert care and imposes heavyworkloads, contributing to avoidable errors and delays in report delivery.While recent progress in automated report generation with vision-languagemodels offer clear potential in ameliorating the situation, the path toreal-world adoption has been stymied by the challenge of evaluating theclinical quality of AI-generated reports. In this study, we build astate-of-the-art report generation system for chest radiographs,\textit{Flamingo-CXR}, by fine-tuning a well-known vision-language foundationmodel on radiology data. To evaluate the quality of the AI-generated reports, agroup of 16 certified radiologists provide detailed evaluations of AI-generatedand human written reports for chest X-rays from an intensive care setting inthe United States and an inpatient setting in India. At least one radiologist(out of two per case) preferred the AI report to the ground truth report inover 60$\%$ of cases for both datasets. Amongst the subset of AI-generatedreports that contain errors, the most frequently cited reasons were related tothe location and finding, whereas for human written reports, most mistakes wererelated to severity and finding. This disparity suggested potentialcomplementarity between our AI system and human experts, prompting us todevelop an assistive scenario in which \textit{Flamingo-CXR} generates afirst-draft report, which is subsequently revised by a clinician. This is thefirst demonstration of clinician-AI collaboration for report writing, and theresultant reports are assessed to be equivalent or preferred by at least oneradiologist to reports written by experts alone in 80$\%$ of in-patient casesand 60$\%$ of intensive care cases.</description><author>Ryutaro Tanno, David G. T. Barrett, Andrew Sellergren, Sumedh Ghaisas, Sumanth Dathathri, Abigail See, Johannes Welbl, Karan Singhal, Shekoofeh Azizi, Tao Tu, Mike Schaekermann, Rhys May, Roy Lee, SiWai Man, Zahra Ahmed, Sara Mahdavi, Danielle Belgrave, Vivek Natarajan, Shravya Shetty, Pushmeet Kohli, Po-Sen Huang, Alan Karthikesalingam, Ira Ktena</author><pubDate>Wed, 06 Dec 2023 17:16:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18260v2</guid></item><item><title>TokenCompose: Grounding Diffusion with Token-level Supervision</title><link>http://arxiv.org/abs/2312.03626v1</link><description>We present TokenCompose, a Latent Diffusion Model for text-to-imagegeneration that achieves enhanced consistency between user-specified textprompts and model-generated images. Despite its tremendous success, thestandard denoising process in the Latent Diffusion Model takes text prompts asconditions only, absent explicit constraint for the consistency between thetext prompts and the image contents, leading to unsatisfactory results forcomposing multiple object categories. TokenCompose aims to improvemulti-category instance composition by introducing the token-wise consistencyterms between the image content and object segmentation maps in the finetuningstage. TokenCompose can be applied directly to the existing training pipelineof text-conditioned diffusion models without extra human labeling information.By finetuning Stable Diffusion, the model exhibits significant improvements inmulti-category instance composition and enhanced photorealism for its generatedimages.</description><author>Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, Zhuowen Tu</author><pubDate>Wed, 06 Dec 2023 17:13:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03626v1</guid></item><item><title>Evaluation of Active Feature Acquisition Methods for Static Feature Settings</title><link>http://arxiv.org/abs/2312.03619v1</link><description>Active feature acquisition (AFA) agents, crucial in domains like healthcarewhere acquiring features is often costly or harmful, determine the optimal setof features for a subsequent classification task. As deploying an AFA agentintroduces a shift in missingness distribution, it's vital to assess itsexpected performance at deployment using retrospective data. In a companionpaper, we introduce a semi-offline reinforcement learning (RL) framework foractive feature acquisition performance evaluation (AFAPE) where features areassumed to be time-dependent. Here, we study and extend the AFAPE problem tocover static feature settings, where features are time-invariant, and henceprovide more flexibility to the AFA agents in deciding the order of theacquisitions. In this static feature setting, we derive and adapt new inverseprobability weighting (IPW), direct method (DM), and double reinforcementlearning (DRL) estimators within the semi-offline RL framework. Theseestimators can be applied when the missingness in the retrospective datasetfollows a missing-at-random (MAR) pattern. They also can be applied tomissing-not-at-random (MNAR) patterns in conjunction with appropriate existingmissing data techniques. We illustrate the improved data efficiency offered bythe semi-offline RL estimators in synthetic and real-world data experimentsunder synthetic MAR and MNAR missingness.</description><author>Henrik von Kleist, Alireza Zamanian, Ilya Shpitser, Narges Ahmidi</author><pubDate>Wed, 06 Dec 2023 17:07:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03619v1</guid></item><item><title>LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models</title><link>http://arxiv.org/abs/2310.05736v2</link><description>Large language models (LLMs) have been applied in various applications due totheir astonishing capabilities. With advancements in technologies such aschain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fedto LLMs are becoming increasingly lengthy, even exceeding tens of thousands oftokens. To accelerate model inference and reduce cost, this paper presentsLLMLingua, a coarse-to-fine prompt compression method that involves a budgetcontroller to maintain semantic integrity under high compression ratios, atoken-level iterative compression algorithm to better model the interdependencebetween compressed contents, and an instruction tuning based method fordistribution alignment between language models. We conduct experiments andanalysis over four datasets from different scenarios, i.e., GSM8K, BBH,ShareGPT, and Arxiv-March23; showing that the proposed approach yieldsstate-of-the-art performance and allows for up to 20x compression with littleperformance loss. Our code is available at https://aka.ms/LLMLingua.</description><author>Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu</author><pubDate>Wed, 06 Dec 2023 17:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05736v2</guid></item><item><title>Physical Symbolic Optimization</title><link>http://arxiv.org/abs/2312.03612v1</link><description>We present a framework for constraining the automatic sequential generationof equations to obey the rules of dimensional analysis by construction.Combining this approach with reinforcement learning, we built $\Phi$-SO, aPhysical Symbolic Optimization method for recovering analytical functions fromphysical data leveraging units constraints. Our symbolic regression algorithmachieves state-of-the-art results in contexts in which variables and constantshave known physical units, outperforming all other methods on SRBench's Feynmanbenchmark in the presence of noise (exceeding 0.1%) and showing resilience evenin the presence of significant (10%) levels of noise.</description><author>Wassim Tenachi, Rodrigo Ibata, Foivos I. Diakogiannis</author><pubDate>Wed, 06 Dec 2023 16:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03612v1</guid></item><item><title>DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</title><link>http://arxiv.org/abs/2312.03611v1</link><description>Utilizing pre-trained 2D large-scale generative models, recent works arecapable of generating high-quality novel views from a single in-the-wild image.However, due to the lack of information from multiple views, these worksencounter difficulties in generating controllable novel views. In this paper,we present DreamComposer, a flexible and scalable framework that can enhanceexisting view-aware diffusion models by injecting multi-view conditions.Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain3D representations of an object from multiple views. Then, it renders thelatent features of the target view from 3D representations with the multi-viewfeature fusion module. Finally the target view features extracted frommulti-view inputs are injected into a pre-trained diffusion model. Experimentsshow that DreamComposer is compatible with state-of-the-art diffusion modelsfor zero-shot novel view synthesis, further enhancing them to generatehigh-fidelity novel view images with multi-view conditions, ready forcontrollable 3D object reconstruction and various other applications.</description><author>Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu</author><pubDate>Wed, 06 Dec 2023 16:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03611v1</guid></item><item><title>Automated Multimodal Data Annotation via Calibration With Indoor Positioning System</title><link>http://arxiv.org/abs/2312.03608v1</link><description>Learned object detection methods based on fusion of LiDAR and camera datarequire labeled training samples, but niche applications, such as warehouserobotics or automated infrastructure, require semantic classes not available inlarge existing datasets. Therefore, to facilitate the rapid creation ofmultimodal object detection datasets and alleviate the burden of humanlabeling, we propose a novel automated annotation pipeline. Our method uses anindoor positioning system (IPS) to produce accurate detection labels for bothpoint clouds and images and eliminates manual annotation entirely. In anexperiment, the system annotates objects of interest 261.8 times faster than ahuman baseline and speeds up end-to-end dataset creation by 61.5%.</description><author>Ryan Rubel, Andrew Dudash, Mohammad Goli, James O'Hara, Karl Wunderlich</author><pubDate>Wed, 06 Dec 2023 16:54:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03608v1</guid></item><item><title>From concrete mixture to structural design -- a holistic optimization procedure in the presence of uncertainties</title><link>http://arxiv.org/abs/2312.03607v1</link><description>Designing civil structures such as bridges, dams or buildings is a complextask requiring many synergies from several experts. Each is responsible fordifferent parts of the process. This is often done in a sequential manner, e.g.the structural engineer makes a design under the assumption of certain materialproperties (e.g. the strength class of the concrete), and then the materialengineer optimizes the material with these restrictions. This paper proposes aholistic optimization procedure, which combines the concrete mixture design andstructural simulations in a joint, forward workflow that we ultimately seek toinvert. In this manner, new mixtures beyond standard ranges can be considered.Any design effort should account for the presence of uncertainties which can bealeatoric or epistemic as when data is used to calibrate physical models oridentify models that fill missing links in the workflow. Inverting the causalrelations established poses several challenges especially when these involvephysics-based models which most often than not do not providederivatives/sensitivities or when design constraints are present. To this end,we advocate Variational Optimization, with proposed extensions andappropriately chosen heuristics to overcome the aforementioned challenges. Theproposed methodology is illustrated using the design of a precast concrete beamwith the objective to minimize the global warming potential while satisfying anumber of constraints associated with its load-bearing capacity after 28daysaccording to the Eurocode, the demoulding time as computed by a complexnonlinear Finite Element model, and the maximum temperature during thehydration.</description><author>Atul Agrawal, Erik Tamsen, Phaedon-Stelios Koutsourelakis, Joerg F. Unger</author><pubDate>Wed, 06 Dec 2023 16:54:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03607v1</guid></item><item><title>DiffusionSat: A Generative Foundation Model for Satellite Imagery</title><link>http://arxiv.org/abs/2312.03606v1</link><description>Diffusion models have achieved state-of-the-art results on many modalitiesincluding images, speech, and video. However, existing models are not tailoredto support remote sensing data, which is widely used in important applicationsincluding environmental monitoring and crop-yield prediction. Satellite imagesare significantly different from natural images -- they can be multi-spectral,irregularly sampled across time -- and existing diffusion models trained onimages from the Web do not support them. Furthermore, remote sensing data isinherently spatio-temporal, requiring conditional generation tasks notsupported by traditional methods based on captions or images. In this paper, wepresent DiffusionSat, to date the largest generative foundation model trainedon a collection of publicly available large, high-resolution remote sensingdatasets. As text-based captions are sparsely available for satellite images,we incorporate the associated metadata such as geolocation as conditioninginformation. Our method produces realistic samples and can be used to solvemultiple generative tasks including temporal generation, superresolution givenmulti-spectral inputs and in-painting. Our method outperforms previousstate-of-the-art methods for satellite image generation and is the firstlarge-scale $\textit{generative}$ foundation model for satellite imagery.</description><author>Samar Khanna, Patrick Liu, Linqi Zhou, Chenlin Meng, Robin Rombach, Marshall Burke, David Lobell, Stefano Ermon</author><pubDate>Wed, 06 Dec 2023 16:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03606v1</guid></item><item><title>A Hyperparameter Study for Quantum Kernel Methods</title><link>http://arxiv.org/abs/2310.11891v2</link><description>Quantum kernel methods are a promising method in quantum machine learningthanks to the guarantees connected to them. Their accessibility for analyticconsiderations also opens up the possibility of prescreening datasets based ontheir potential for a quantum advantage. To do so, earlier works developed thegeometric difference, which can be understood as a closeness measure betweentwo kernel-based machine learning approaches, most importantly between aquantum kernel and classical kernel. This metric links the quantum andclassical model complexities. Therefore, it raises the question of whether thegeometric difference, based on its relation to model complexity, can be auseful tool in evaluations other than for the potential for quantum advantage.In this work, we investigate the effects of hyperparameter choice on the modelperformance and the generalization gap between classical and quantum kernels.The importance of hyperparameter optimization is well known also for classicalmachine learning. Especially for the quantum Hamiltonian evolution feature map,the scaling of the input data has been shown to be crucial. However, there areadditional parameters left to be optimized, like the best number of qubits totrace out before computing a projected quantum kernel. We investigate theinfluence of these hyperparameters and compare the classically reliable methodof cross validation with the method of choosing based on the geometricdifference. Based on the thorough investigation of the hyperparameters across11 datasets we identified commodities that can be exploited when examining anew dataset. In addition, our findings contribute to better understanding ofthe applicability of the geometric difference.</description><author>Sebastian Egginger, Alona Sakhnenko, Jeanette Miriam Lorenz</author><pubDate>Wed, 06 Dec 2023 16:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11891v2</guid></item><item><title>Dimensionless Anomaly Detection on Multivariate Streams with Variance Norm and Path Signature</title><link>http://arxiv.org/abs/2006.03487v2</link><description>In this paper, we propose a dimensionless anomaly detection method formultivariate streams. Our method is independent of the unit of measurement forthe different stream channels, therefore dimensionless. We first propose thevariance norm, a generalisation of Mahalanobis distance to handleinfinite-dimensional feature space and singular empirical covariance matrixrigorously. We then combine the variance norm with the path signature, aninfinite collection of iterated integrals that provide global features ofstreams, to propose SigMahaKNN, a method for anomaly detection on(multivariate) streams. We show that SigMahaKNN is invariant to streamreparametrisation, stream concatenation and has a graded discrimination powerdepending on the truncation level of the path signature. We implementSigMahaKNN as an open-source software, and perform extensive numericalexperiments, showing significantly improved anomaly detection on streamscompared to isolation forest and local outlier factors in applications rangingfrom language analysis, hand-writing analysis, ship movement paths analysis andunivariate time-series analysis.</description><author>Zhen Shao, Ryan Sze-Yin Chan, Thomas Cochrane, Peter Foster, Terry Lyons</author><pubDate>Wed, 06 Dec 2023 16:46:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2006.03487v2</guid></item><item><title>MMM: Generative Masked Motion Model</title><link>http://arxiv.org/abs/2312.03596v1</link><description>Recent advances in text-to-motion generation using diffusion andautoregressive models have shown promising results. However, these models oftensuffer from a trade-off between real-time performance, high fidelity, andmotion editability. To address this gap, we introduce MMM, a novel yet simplemotion generation paradigm based on Masked Motion Model. MMM consists of twokey components: (1) a motion tokenizer that transforms 3D human motion into asequence of discrete tokens in latent space, and (2) a conditional maskedmotion transformer that learns to predict randomly masked motion tokens,conditioned on the pre-computed text tokens. By attending to motion and texttokens in all directions, MMM explicitly captures inherent dependency amongmotion tokens and semantic mapping between motion and text tokens. Duringinference, this allows parallel and iterative decoding of multiple motiontokens that are highly consistent with fine-grained text descriptions,therefore simultaneously achieving high-fidelity and high-speed motiongeneration. In addition, MMM has innate motion editability. By simply placingmask tokens in the place that needs editing, MMM automatically fills the gapswhile guaranteeing smooth transitions between editing and non-editing parts.Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMMsurpasses current leading methods in generating high-quality motion (evidencedby superior FID scores of 0.08 and 0.429), while offering advanced editingfeatures such as body-part modification, motion in-betweening, and thesynthesis of long motion sequences. In addition, MMM is two orders of magnitudefaster on a single mid-range GPU than editable motion diffusion models. Ourproject page is available at \url{https://exitudio.github.io/MMM-page}.</description><author>Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, Chen Chen</author><pubDate>Wed, 06 Dec 2023 16:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03596v1</guid></item><item><title>A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting</title><link>http://arxiv.org/abs/2312.03594v1</link><description>Achieving high-quality versatile image inpainting, where user-specifiedregions are filled with plausible content according to user intent, presents asignificant challenge. Existing methods face difficulties in simultaneouslyaddressing context-aware image inpainting and text-guided object inpainting dueto the distinct optimal training strategies required. To overcome thischallenge, we introduce PowerPaint, the first high-quality and versatileinpainting model that excels in both tasks. First, we introduce learnable taskprompts along with tailored fine-tuning strategies to guide the model's focuson different inpainting targets explicitly. This enables PowerPaint toaccomplish various inpainting tasks by utilizing different task prompts,resulting in state-of-the-art performance. Second, we demonstrate theversatility of the task prompt in PowerPaint by showcasing its effectiveness asa negative prompt for object removal. Additionally, we leverage promptinterpolation techniques to enable controllable shape-guided object inpainting.Finally, we extensively evaluate PowerPaint on various inpainting benchmarks todemonstrate its superior performance for versatile image inpainting. We releaseour codes and models on our project page: https://powerpaint.github.io/.</description><author>Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, Kai Chen</author><pubDate>Wed, 06 Dec 2023 16:34:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03594v1</guid></item><item><title>Conditions for Length Generalization in Learning Reasoning Skills</title><link>http://arxiv.org/abs/2311.16173v2</link><description>Reasoning is a fundamental capability of AI agents. Recently, large languagemodels (LLMs) have shown remarkable abilities to perform reasoning tasks.However, numerous evaluations of the reasoning capabilities of LLMs have alsoshowed some limitations. An outstanding limitation is length generalization,meaning that when trained on reasoning problems of smaller lengths or sizes,the resulting models struggle with problems of larger sizes or lengths. Thispotentially indicates some theoretical limitations of generalization inlearning reasoning skills. These evaluations and their observations motivatedus to perform a theoretical study of the length generalization problem. Thiswork focuses on reasoning tasks that can be formulated as Markov dynamicprocesses (MDPs) and/or directed acyclic graphs (DAGs). It identifies andproves conditions that decide whether the length generalization problem can besolved or not for a reasoning task in a particular representation. Experimentsare also conducted to verify the theoretical results.</description><author>Changnan Xiao, Bing Liu</author><pubDate>Wed, 06 Dec 2023 16:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16173v2</guid></item><item><title>Language-Informed Visual Concept Learning</title><link>http://arxiv.org/abs/2312.03587v1</link><description>Our understanding of the visual world is centered around various conceptaxes, characterizing different aspects of visual entities. While differentconcept axes can be easily specified by language, e.g. color, the exact visualnuances along each axis often exceed the limitations of linguisticarticulations, e.g. a particular style of painting. In this work, our goal isto learn a language-informed visual concept representation, by simplydistilling large pre-trained vision-language models. Specifically, we train aset of concept encoders to encode the information pertinent to a set oflanguage-informed concept axes, with an objective of reproducing the inputimage through a pre-trained Text-to-Image (T2I) model. To encourage betterdisentanglement of different concept encoders, we anchor the concept embeddingsto a set of text embeddings obtained from a pre-trained Visual QuestionAnswering (VQA) model. At inference time, the model extracts concept embeddingsalong various axes from new test images, which can be remixed to generateimages with novel compositions of visual concepts. With a lightweight test-timefinetuning procedure, it can also generalize to novel concepts unseen attraining.</description><author>Sharon Lee, Yunzhi Zhang, Shangzhe Wu, Jiajun Wu</author><pubDate>Wed, 06 Dec 2023 16:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03587v1</guid></item><item><title>Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems</title><link>http://arxiv.org/abs/2305.01090v3</link><description>While many phenomena in physics and engineering are formallyhigh-dimensional, their long-time dynamics often live on a lower-dimensionalmanifold. The present work introduces an autoencoder framework that combinesimplicit regularization with internal linear layers and $L_2$ regularization(weight decay) to automatically estimate the underlying dimensionality of adata set, produce an orthogonal manifold coordinate system, and provide themapping functions between the ambient space and manifold space, allowing forout-of-sample projections. We validate our framework's ability to estimate themanifold dimension for a series of datasets from dynamical systems of varyingcomplexities and compare to other state-of-the-art estimators. We analyze thetraining dynamics of the network to glean insight into the mechanism oflow-rank learning and find that collectively each of the implicit regularizinglayers compound the low-rank representation and even self-correct duringtraining. Analysis of gradient descent dynamics for this architecture in thelinear case reveals the role of the internal linear layers in leading to fasterdecay of a "collective weight variable" incorporating all layers, and the roleof weight decay in breaking degeneracies and thus driving convergence alongdirections in which no decay would occur in its absence. We show that thisframework can be naturally extended for applications of state-space modelingand forecasting by generating a data-driven dynamic model of a spatiotemporallychaotic partial differential equation using only the manifold coordinates.Finally, we demonstrate that our framework is robust to hyperparameter choices.</description><author>Kevin Zeng, Carlos E. PÃ©rez De JesÃºs, Andrew J. Fox, Michael D. Graham</author><pubDate>Wed, 06 Dec 2023 16:23:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01090v3</guid></item><item><title>Foundation Model Assisted Weakly Supervised Semantic Segmentation</title><link>http://arxiv.org/abs/2312.03585v1</link><description>This work aims to leverage pre-trained foundation models, such as contrastivelanguage-image pre-training (CLIP) and segment anything model (SAM), to addressweakly supervised semantic segmentation (WSSS) using image-level labels. Tothis end, we propose a coarse-to-fine framework based on CLIP and SAM forgenerating high-quality segmentation seeds. Specifically, we construct an imageclassification task and a seed segmentation task, which are jointly performedby CLIP with frozen weights and two sets of learnable task-specific prompts. ASAM-based seeding (SAMS) module is designed and applied to each task to produceeither coarse or fine seed maps. Moreover, we design a multi-label contrastiveloss supervised by image-level labels and a CAM activation loss supervised bythe generated coarse seed map. These losses are used to learn the prompts,which are the only parts need to be learned in our framework. Once the promptsare learned, we input each image along with the learned segmentation-specificprompts into CLIP and the SAMS module to produce high-quality segmentationseeds. These seeds serve as pseudo labels to train an off-the-shelfsegmentation network like other two-stage WSSS methods. Experiments show thatour method achieves the state-of-the-art performance on PASCAL VOC 2012 andcompetitive results on MS COCO 2014.</description><author>Xiaobo Yang, Xiaojin Gong</author><pubDate>Wed, 06 Dec 2023 16:21:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03585v1</guid></item><item><title>Biased Random-Key Genetic Algorithms: A Review</title><link>http://arxiv.org/abs/2312.00961v2</link><description>This paper is a comprehensive literature review of Biased Random-Key GeneticAlgorithms (BRKGA). BRKGA is a metaheuristic that employs random-key-basedchromosomes with biased, uniform, and elitist mating strategies in a geneticalgorithm framework. The review encompasses over 150 papers with a wide rangeof applications, including classical combinatorial optimization problems,real-world industrial use cases, and non-orthodox applications such as neuralnetwork hyperparameter tuning in machine learning. Scheduling is by far themost prevalent application area in this review, followed by network design andlocation problems. The most frequent hybridization method employed is localsearch, and new features aim to increase population diversity. Overall, thissurvey provides a comprehensive overview of the BRKGA metaheuristic and itsapplications and highlights important areas for future research.</description><author>Mariana A. Londe, Luciana S. Pessoa, Carlos E. Andrade, Mauricio G. C. Resende</author><pubDate>Wed, 06 Dec 2023 16:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00961v2</guid></item><item><title>Context Diffusion: In-Context Aware Image Generation</title><link>http://arxiv.org/abs/2312.03584v1</link><description>We propose Context Diffusion, a diffusion-based framework that enables imagegeneration models to learn from visual examples presented in context. Recentwork tackles such in-context learning for image generation, where a query imageis provided alongside context examples and text prompts. However, the qualityand fidelity of the generated images deteriorate when the prompt is notpresent, demonstrating that these models are unable to truly learn from thevisual context. To address this, we propose a novel framework that separatesthe encoding of the visual context and preserving the structure of the queryimages. This results in the ability to learn from the visual context and textprompts, but also from either one of them. Furthermore, we enable our model tohandle few-shot settings, to effectively address diverse in-context learningscenarios. Our experiments and user study demonstrate that Context Diffusionexcels in both in-domain and out-of-domain tasks, resulting in an overallenhancement in image quality and fidelity compared to counterpart models.</description><author>Ivona Najdenkoska, Animesh Sinha, Abhimanyu Dubey, Dhruv Mahajan, Vignesh Ramanathan, Filip Radenovic</author><pubDate>Wed, 06 Dec 2023 16:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03584v1</guid></item><item><title>Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models</title><link>http://arxiv.org/abs/2310.03059v4</link><description>The popularity of pre-trained large models has revolutionized downstreamtasks across diverse fields, such as language, vision, and multi-modality. Tominimize the adaption cost for downstream tasks, many Parameter-EfficientFine-Tuning (PEFT) techniques are proposed for language and 2D imagepre-trained models. However, the specialized PEFT method for 3D pre-trainedmodels is still under-explored. To this end, we introduce Point-PEFT, a novelframework for adapting point cloud pre-trained models with minimal learnableparameters. Specifically, for a pre-trained 3D model, we freeze most of itsparameters, and only tune the newly added PEFT modules on downstream tasks,which consist of a Point-prior Prompt and a Geometry-aware Adapter. ThePoint-prior Prompt adopts a set of learnable prompt tokens, for which wepropose to construct a memory bank with domain-specific knowledge, and utilizea parameter-free attention to enhance the prompt tokens. The Geometry-awareAdapter aims to aggregate point cloud features within spatial neighborhoods tocapture fine-grained geometric information through local interactions.Extensive experiments indicate that our Point-PEFT can achieve betterperformance than the full fine-tuning on various downstream tasks, while usingonly 5% of the trainable parameters, demonstrating the efficiency andeffectiveness of our approach. Code will be released athttps://github.com/Even-JK/PEFT-3D.</description><author>Ivan Tang, Ray Zhang, Zoey Guo, Xianzheng Ma, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li</author><pubDate>Wed, 06 Dec 2023 16:16:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03059v4</guid></item><item><title>Invariance &amp; Causal Representation Learning: Prospects and Limitations</title><link>http://arxiv.org/abs/2312.03580v1</link><description>In causal models, a given mechanism is assumed to be invariant to changes ofother mechanisms. While this principle has been utilized for inference insettings where the causal variables are observed, theoretical insights when thevariables of interest are latent are largely missing. We assay the connectionbetween invariance and causal representation learning by establishingimpossibility results which show that invariance alone is insufficient toidentify latent causal variables. Together with practical considerations, weuse these theoretical findings to highlight the need for additional constraintsin order to identify representations by exploiting invariance.</description><author>Simon Bing, Jonas Wahl, Urmi Ninad, Jakob Runge</author><pubDate>Wed, 06 Dec 2023 16:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03580v1</guid></item><item><title>Improving Bias Mitigation through Bias Experts in Natural Language Understanding</title><link>http://arxiv.org/abs/2312.03577v1</link><description>Biases in the dataset often enable the model to achieve high performance onin-distribution data, while poorly performing on out-of-distribution data. Tomitigate the detrimental effect of the bias on the networks, previous workshave proposed debiasing methods that down-weight the biased examples identifiedby an auxiliary model, which is trained with explicit bias labels. However,finding a type of bias in datasets is a costly process. Therefore, recentstudies have attempted to make the auxiliary model biased without the guidance(or annotation) of bias labels, by constraining the model's trainingenvironment or the capability of the model itself. Despite the promisingdebiasing results of recent works, the multi-class learning objective, whichhas been naively used to train the auxiliary model, may harm the biasmitigation effect due to its regularization effect and competitive natureacross classes. As an alternative, we propose a new debiasing framework thatintroduces binary classifiers between the auxiliary model and the main model,coined bias experts. Specifically, each bias expert is trained on a binaryclassification task derived from the multi-class classification task via theOne-vs-Rest approach. Experimental results demonstrate that our proposedstrategy improves the bias identification ability of the auxiliary model.Consequently, our debiased model consistently outperforms the state-of-the-arton various challenge datasets.</description><author>Eojin Jeon, Mingyu Lee, Juhyeong Park, Yeachan Kim, Wing-Lam Mok, SangKeun Lee</author><pubDate>Wed, 06 Dec 2023 16:15:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03577v1</guid></item><item><title>Harnessing machine learning for accurate treatment of overlapping opacity species in general circulation models</title><link>http://arxiv.org/abs/2311.00775v3</link><description>To understand high precision observations of exoplanets and brown dwarfs, weneed detailed and complex general circulation models (GCMs) that incorporatehydrodynamics, chemistry, and radiation. For this study, we specificallyexamined the coupling between chemistry and radiation in GCMs and compareddifferent methods for the mixing of opacities of different chemical species inthe correlated-k assumption, when equilibrium chemistry cannot be assumed. Wepropose a fast machine learning method based on DeepSets (DS), whicheffectively combines individual correlated-k opacities (k-tables). We evaluatedthe DS method alongside other published methods such as adaptive equivalentextinction (AEE) and random overlap with rebinning and resorting (RORR). Weintegrated these mixing methods into our GCM (expeRT/MITgcm) and assessed theiraccuracy and performance for the example of the hot Jupiter HD~209458 b. Ourfindings indicate that the DS method is both accurate and efficient for GCMusage, whereas RORR is too slow. Additionally, we observed that the accuracy ofAEE depends on its specific implementation and may introduce numerical issuesin achieving radiative transfer solution convergence. We then applied the DSmixing method in a simplified chemical disequilibrium situation, where wemodeled the rainout of TiO and VO, and confirmed that the rainout of TiO and VOwould hinder the formation of a stratosphere. To further expedite thedevelopment of consistent disequilibrium chemistry calculations in GCMs, weprovide documentation and code for coupling the DS mixing method withcorrelated-k radiative transfer solvers. The DS method has been extensivelytested to be accurate enough for GCMs; however, other methods might be neededfor accelerating atmospheric retrievals.</description><author>Aaron David Schneider, Paul MolliÃ¨re, Gilles Louppe, Ludmila Carone, Uffe GrÃ¥e JÃ¸rgensen, Leen Decin, Christiane Helling</author><pubDate>Wed, 06 Dec 2023 16:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00775v3</guid></item><item><title>TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks</title><link>http://arxiv.org/abs/2310.00752v2</link><description>We present TIGERScore, a \textbf{T}rained metric that follows\textbf{I}nstruction \textbf{G}uidance to perform \textbf{E}xplainable, and\textbf{R}eference-free evaluation over a wide spectrum of text generationtasks. Different from other automatic evaluation methods that only providearcane scores, TIGERScore is guided by natural language instruction to provideerror analysis to pinpoint the mistakes in the generated text. Our metric isbased on LLaMA-2, trained on our meticulously curated instruction-tuningdataset MetricInstruct which covers 6 text generation tasks and 23 textgeneration datasets. The dataset consists of 42K quadruple in the form of(instruction, input, system output $\rightarrow$ error analysis). We collectedthe `system outputs' through from a large variety of models to cover differenttypes of errors. To quantitatively assess our metric, we evaluate itscorrelation with human ratings on 5 held-in datasets, 2 held-out datasets andshow that TIGERScore can achieve the open-source SoTA correlation with humanratings across these datasets and almost approaches GPT-4 evaluator. As areference-free metric, its correlation can even surpass the best existingreference-based metrics. To further qualitatively assess the rationalegenerated by our metric, we conduct human evaluation on the generatedexplanations and found that the explanations are 70.8\% accurate. Through theseexperimental results, we believe TIGERScore demonstrates the possibility ofbuilding universal explainable metrics to evaluate any text generation task.</description><author>Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, Wenhu Chen</author><pubDate>Wed, 06 Dec 2023 16:06:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00752v2</guid></item><item><title>DocBinFormer: A Two-Level Transformer Network for Effective Document Image Binarization</title><link>http://arxiv.org/abs/2312.03568v1</link><description>In real life, various degradation scenarios exist that might damage documentimages, making it harder to recognize and analyze them, thus binarization is afundamental and crucial step for achieving the most optimal performance in anydocument analysis task. We propose DocBinFormer (Document BinarizationTransformer), a novel two-level vision transformer (TL-ViT) architecture basedon vision transformers for effective document image binarization. The presentedarchitecture employs a two-level transformer encoder to effectively captureboth global and local feature representation from the input images. Thesecomplimentary bi-level features are exploited for efficient document imagebinarization, resulting in improved results for system-generated as well ashandwritten document images in a comprehensive approach. With the absence ofconvolutional layers, the transformer encoder uses the pixel patches andsub-patches along with their positional information to operate directly onthem, while the decoder generates a clean (binarized) output image from thelatent representation of the patches. Instead of using a simple visiontransformer block to extract information from the image patches, the proposedarchitecture uses two transformer blocks for greater coverage of the extractedfeature space on a global and local scale. The encoded feature representationis used by the decoder block to generate the corresponding binarized output.Extensive experiments on a variety of DIBCO and H-DIBCO benchmarks show thatthe proposed model outperforms state-of-the-art techniques on four metrics. Thesource code will be made available athttps://github.com/RisabBiswas/DocBinFormer.</description><author>Risab Biswas, Swalpa Kumar Roy, Ning Wang, Umapada Pal, Guang-Bin Huang</author><pubDate>Wed, 06 Dec 2023 16:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03568v1</guid></item><item><title>XAIQA: Explainer-Based Data Augmentation for Extractive Question Answering</title><link>http://arxiv.org/abs/2312.03567v1</link><description>Extractive question answering (QA) systems can enable physicians andresearchers to query medical records, a foundational capability for designingclinical studies and understanding patient medical history. However, buildingthese systems typically requires expert-annotated QA pairs. Large languagemodels (LLMs), which can perform extractive QA, depend on high quality data intheir prompts, specialized for the application domain. We introduce a novelapproach, XAIQA, for generating synthetic QA pairs at scale from data naturallyavailable in electronic health records. Our method uses the idea of aclassification model explainer to generate questions and answers about medicalconcepts corresponding to medical codes. In an expert evaluation with twophysicians, our method identifies $2.2\times$ more semantic matches and$3.8\times$ more clinical abbreviations than two popular approaches that usesentence transformers to create QA pairs. In an ML evaluation, adding our QApairs improves performance of GPT-4 as an extractive QA model, including ondifficult questions. In both the expert and ML evaluations, we examinetrade-offs between our method and sentence transformers for QA pair generationdepending on question difficulty.</description><author>Joel Stremmel, Ardavan Saeedi, Hamid Hassanzadeh, Sanjit Batra, Jeffrey Hertzberg, Jaime Murillo, Eran Halperin</author><pubDate>Wed, 06 Dec 2023 15:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03567v1</guid></item><item><title>Enhancing Kinship Verification through Multiscale Retinex and Combined Deep-Shallow features</title><link>http://arxiv.org/abs/2312.03562v1</link><description>The challenge of kinship verification from facial images represents acutting-edge and formidable frontier in the realms of pattern recognition andcomputer vision. This area of study holds a myriad of potential applications,spanning from image annotation and forensic analysis to social media research.Our research stands out by integrating a preprocessing method named MultiscaleRetinex (MSR), which elevates image quality and amplifies contrast, ultimatelybolstering the end results. Strategically, our methodology capitalizes on theharmonious blend of deep and shallow texture descriptors, merging themproficiently at the score level through the Logistic Regression (LR) method. Toelucidate, we employ the Local Phase Quantization (LPQ) descriptor to extractshallow texture characteristics. For deep feature extraction, we turn to theprowess of the VGG16 model, which is pre-trained on a convolutional neuralnetwork (CNN). The robustness and efficacy of our method have been put to thetest through meticulous experiments on three rigorous kinship datasets, namely:Cornell Kin Face, UB Kin Face, and TS Kin Face.</description><author>El Ouanas Belabbaci, Mohammed Khammari, Ammar Chouchane, Mohcene Bessaoudi, Abdelmalik Ouamane, Yassine Himeur, Shadi Atalla, Wathiq Mansoor</author><pubDate>Wed, 06 Dec 2023 15:52:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03562v1</guid></item><item><title>Towards Causal Representations of Climate Model Data</title><link>http://arxiv.org/abs/2312.02858v2</link><description>Climate models, such as Earth system models (ESMs), are crucial forsimulating future climate change based on projected Shared SocioeconomicPathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticatedand invaluable, machine learning-based emulators trained on existing simulationdata can project additional climate scenarios much faster and arecomputationally efficient. However, they often lack generalizability andinterpretability. This work delves into the potential of causal representationlearning, specifically the \emph{Causal Discovery with Single-parent Decoding}(CDSD) method, which could render climate model emulation efficient\textit{and} interpretable. We evaluate CDSD on multiple climate datasets,focusing on emissions, temperature, and precipitation. Our findings shed lighton the challenges, limitations, and promise of using CDSD as a stepping stonetowards more interpretable and robust climate model emulation.</description><author>Julien Boussard, Chandni Nagda, Julia Kaltenborn, Charlotte Emilie Elektra Lange, Philippe Brouillard, Yaniv Gurwicz, Peer Nowack, David Rolnick</author><pubDate>Wed, 06 Dec 2023 15:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02858v2</guid></item><item><title>Deeply Coupled Cross-Modal Prompt Learning</title><link>http://arxiv.org/abs/2305.17903v3</link><description>Recent advancements in multimodal foundation models (e.g., CLIP) haveexcelled in zero-shot generalization. Prompt tuning involved in the knowledgetransfer from foundation models to downstream tasks has gained significantattention recently. Existing prompt-tuning methods in cross-modal learning,however, either solely focus on language branch, or learn vision-languageinteraction in a shallow mechanism. In this context, we propose a Deeplycoupled Cross-modal Prompt learning (DCP) method based on CLIP. DCP flexiblyaccommodates the interplay between vision and language with a Cross-ModalPrompt Attention (CMPA) mechanism, which enables the mutual exchange ofrespective representation through a well-connected multi-head attention moduleprogressively and strongly. We then conduct comprehensive few-shot learningexperiments on 11 image classification datasets and analyze the robustness todomain shift as well. Thorough experimental analysis evidently demonstrates thesuperb few-shot generalization and compelling domain adaption capacity of awell-executed DCP. The code can be found at https://github.com/GingL/CMPA.</description><author>Xuejing Liu, Wei Tang, Jinghui Lu, Rui Zhao, Zhaojun Guo, Fei Tan</author><pubDate>Wed, 06 Dec 2023 15:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17903v3</guid></item><item><title>Blueprinting the Future: Automatic Item Categorization using Hierarchical Zero-Shot and Few-Shot Classifiers</title><link>http://arxiv.org/abs/2312.03561v1</link><description>In testing industry, precise item categorization is pivotal to align examquestions with the designated content domains outlined in the assessmentblueprint. Traditional methods either entail manual classification, which islaborious and error-prone, or utilize machine learning requiring extensivetraining data, often leading to model underfit or overfit issues. This studyunveils a novel approach employing the zero-shot and few-shot GenerativePretrained Transformer (GPT) classifier for hierarchical item categorization,minimizing the necessity for training data, and instead, leveraging human-likelanguage descriptions to define categories. Through a structured pythondictionary, the hierarchical nature of examination blueprints is navigatedseamlessly, allowing for a tiered classification of items across multiplelevels. An initial simulation with artificial data demonstrates the efficacy ofthis method, achieving an average accuracy of 92.91% measured by the F1 score.This method was further applied to real exam items from the 2022 In-TrainingExamination (ITE) conducted by the American Board of Family Medicine (ABFM),reclassifying 200 items according to a newly formulated blueprint swiftly in 15minutes, a task that traditionally could span several days among editors andphysicians. This innovative approach not only drastically cuts downclassification time but also ensures a consistent, principle-drivencategorization, minimizing human biases and discrepancies. The ability torefine classifications by adjusting definitions adds to its robustness andsustainability.</description><author>Ting Wang, Keith Stelter, Jenn Floyd, Thomas O'Neill, Nathaniel Hendrix, Andrew Bazemore, Kevin Rode, Warren Newton</author><pubDate>Wed, 06 Dec 2023 15:51:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03561v1</guid></item><item><title>When an Image is Worth 1,024 x 1,024 Words: A Case Study in Computational Pathology</title><link>http://arxiv.org/abs/2312.03558v1</link><description>This technical report presents LongViT, a vision Transformer that can processgigapixel images in an end-to-end manner. Specifically, we split the gigapixelimage into a sequence of millions of patches and project them linearly intoembeddings. LongNet is then employed to model the extremely long sequence,generating representations that capture both short-range and long-rangedependencies. The linear computation complexity of LongNet, along with itsdistributed algorithm, enables us to overcome the constraints of bothcomputation and memory. We apply LongViT in the field of computationalpathology, aiming for cancer diagnosis and prognosis within gigapixelwhole-slide images. Experimental results demonstrate that LongViT effectivelyencodes gigapixel images and outperforms previous state-of-the-art methods oncancer subtyping and survival prediction. Code and models will be available athttps://aka.ms/LongViT.</description><author>Wenhui Wang, Shuming Ma, Hanwen Xu, Naoto Usuyama, Jiayu Ding, Hoifung Poon, Furu Wei</author><pubDate>Wed, 06 Dec 2023 15:40:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03558v1</guid></item><item><title>Personalized Face Inpainting with Diffusion Models by Parallel Visual Attention</title><link>http://arxiv.org/abs/2312.03556v1</link><description>Face inpainting is important in various applications, such as photorestoration, image editing, and virtual reality. Despite the significantadvances in face generative models, ensuring that a person's unique facialidentity is maintained during the inpainting process is still an elusive goal.Current state-of-the-art techniques, exemplified by MyStyle, necessitateresource-intensive fine-tuning and a substantial number of images for each newidentity. Furthermore, existing methods often fall short in accommodatinguser-specified semantic attributes, such as beard or expression. To improveinpainting results, and reduce the computational complexity during inference,this paper proposes the use of Parallel Visual Attention (PVA) in conjunctionwith diffusion models. Specifically, we insert parallel attention matrices toeach cross-attention module in the denoising network, which attends to featuresextracted from reference images by an identity encoder. We train the addedattention modules and identity encoder on CelebAHQ-IDI, a dataset proposed foridentity-preserving face inpainting. Experiments demonstrate that PVA attainsunparalleled identity resemblance in both face inpainting and face inpaintingwith language guidance tasks, in comparison to various benchmarks, includingMyStyle, Paint by Example, and Custom Diffusion. Our findings reveal that PVAensures good identity preservation while offering effectivelanguage-controllability. Additionally, in contrast to Custom Diffusion, PVArequires just 40 fine-tuning steps for each new identity, which translates to asignificant speed increase of over 20 times.</description><author>Jianjin Xu, Saman Motamed, Praneetha Vaddamanu, Chen Henry Wu, Christian Haene, Jean-Charles Bazin, Fernando de la Torre</author><pubDate>Wed, 06 Dec 2023 15:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03556v1</guid></item><item><title>Entailment Semantics Can Be Extracted from an Ideal Language Model</title><link>http://arxiv.org/abs/2209.12407v2</link><description>Language models are often trained on text alone, without additionalgrounding. There is debate as to how much of natural language semantics can beinferred from such a procedure. We prove that entailment judgments betweensentences can be extracted from an ideal language model that has perfectlylearned its target distribution, assuming the training sentences are generatedby Gricean agents, i.e., agents who follow fundamental principles ofcommunication from the linguistic theory of pragmatics. We also show entailmentjudgments can be decoded from the predictions of a language model trained onsuch Gricean data. Our results reveal a pathway for understanding the semanticinformation encoded in unlabeled linguistic data and a potential framework forextracting semantics from language models.</description><author>William Merrill, Alex Warstadt, Tal Linzen</author><pubDate>Wed, 06 Dec 2023 15:36:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.12407v2</guid></item><item><title>Fluctuation without dissipation: Microcanonical Langevin Monte Carlo</title><link>http://arxiv.org/abs/2303.18221v2</link><description>Stochastic sampling algorithms such as Langevin Monte Carlo are inspired byphysical systems in a heat bath. Their equilibrium distribution is thecanonical ensemble given by a prescribed target distribution, so they mustbalance fluctuation and dissipation as dictated by the fluctuation-dissipationtheorem. In contrast to the common belief, we show that thefluctuation-dissipation theorem is not required because only the configurationspace distribution, and not the full phase space distribution, needs to becanonical. We propose a continuous-time Microcanonical Langevin Monte Carlo(MCLMC) as a dissipation-free system of stochastic differential equations(SDE). We derive the corresponding Fokker-Planck equation and show that thestationary distribution is the microcanonical ensemble with the desiredcanonical distribution on configuration space. We prove that MCLMC is ergodicfor any nonzero amount of stochasticity, and for smooth, convex potentials, theexpectation values converge exponentially fast. Furthermore, the deterministicdrift and the stochastic diffusion separately preserve the stationarydistribution. This uncommon property is attractive for practicalimplementations as it implies that the drift-diffusion discretization schemesare bias-free, so the only source of bias is the discretization of thedeterministic dynamics. We applied MCLMC on a lattice $\phi^4$ model, whereHamiltonian Monte Carlo (HMC) is currently the state-of-the-art integrator. Forthe same accuracy, MCLMC converges 12 times faster than HMC on an $8\times8$lattice. On a $64\times64$ lattice, it is already 32 times faster. The trend isexpected to persist to larger lattices, which are of particular interest, forexample, in lattice quantum chromodynamics.</description><author>Jakob Robnik, UroÅ¡ Seljak</author><pubDate>Wed, 06 Dec 2023 15:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.18221v2</guid></item><item><title>A Comprehensive Review of Visual-Textual Sentiment Analysis from Social Media Networks</title><link>http://arxiv.org/abs/2207.02160v2</link><description>Social media networks have become a significant aspect of people's lives,serving as a platform for their ideas, opinions and emotions. Consequently,automated sentiment analysis (SA) is critical for recognising people's feelingsin ways that other information sources cannot. The analysis of these feelingsrevealed various applications, including brand evaluations, YouTube filmreviews and healthcare applications. As social media continues to develop,people post a massive amount of information in different forms, including text,photos, audio and video. Thus, traditional SA algorithms have become limited,as they do not consider the expressiveness of other modalities. By includingsuch characteristics from various material sources, these multimodal datastreams provide new opportunities for optimising the expected results beyondtext-based SA. Our study focuses on the forefront field of multimodal SA, whichexamines visual and textual data posted on social media networks. Many peopleare more likely to utilise this information to express themselves on theseplatforms. To serve as a resource for academics in this rapidly growing field,we introduce a comprehensive overview of textual and visual SA, including datapre-processing, feature extraction techniques, sentiment benchmark datasets,and the efficacy of multiple classification methodologies suited to each field.We also provide a brief introduction of the most frequently utilised datafusion strategies and a summary of existing research on visual-textual SA.Finally, we highlight the most significant challenges and investigate severalimportant sentiment applications.</description><author>Israa Khalaf Salman Al-Tameemi, Mohammad-Reza Feizi-Derakhshi, Saeed Pashazadeh, Mohammad Asadpour</author><pubDate>Wed, 06 Dec 2023 15:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.02160v2</guid></item><item><title>Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment</title><link>http://arxiv.org/abs/2312.03549v1</link><description>Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstratedremarkable accuracy in a wide range of tasks. However, training these modelscan incur significant expenses, often requiring tens of thousands of GPUs formonths of continuous operation. Typically, this training is carried out inspecialized GPU clusters equipped with homogeneous high-speed Remote DirectMemory Access (RDMA) network interface cards (NICs). The acquisition andmaintenance of such dedicated clusters is challenging. Current LLM trainingframeworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily onoptimizing training within homogeneous cluster settings. In this paper, weintroduce Holmes, a training framework for LLMs that employs thoughtfullycrafted data and model parallelism strategies over the heterogeneous NICenvironment. Our primary technical contribution lies in a novel schedulingmethod that intelligently allocates distinct computational tasklets in LLMtraining to specific groups of GPU devices based on the characteristics oftheir connected NICs. Furthermore, our proposed framework, utilizing pipelineparallel techniques, demonstrates scalability to multiple GPU clusters, even inscenarios without high-speed interconnects between nodes in distinct clusters.We conducted comprehensive experiments that involved various scenarios in theheterogeneous NIC environment. In most cases, our framework achievesperformance levels close to those achievable with homogeneous RDMA-capablenetworks (InfiniBand or RoCE), significantly exceeding training efficiencywithin the pure Ethernet environment. Additionally, we verified that ourframework outperforms other mainstream LLM frameworks under heterogeneous NICenvironment in terms of training efficiency and can be seamlessly integratedwith them.</description><author>Fei Yang, Shuang Peng, Ning Sun, Fangyu Wang, Ke Tan, Fu Wu, Jiezhong Qiu, Aimin Pan</author><pubDate>Wed, 06 Dec 2023 15:27:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03549v1</guid></item><item><title>Texture-Semantic Collaboration Network for ORSI Salient Object Detection</title><link>http://arxiv.org/abs/2312.03548v1</link><description>Salient object detection (SOD) in optical remote sensing images (ORSIs) hasbecome increasingly popular recently. Due to the characteristics of ORSIs,ORSI-SOD is full of challenges, such as multiple objects, small objects, lowilluminations, and irregular shapes. To address these challenges, we propose aconcise yet effective Texture-Semantic Collaboration Network (TSCNet) toexplore the collaboration of texture cues and semantic cues for ORSI-SOD.Specifically, TSCNet is based on the generic encoder-decoder structure. Inaddition to the encoder and decoder, TSCNet includes a vital Texture-SemanticCollaboration Module (TSCM), which performs valuable feature modulation andinteraction on basic features extracted from the encoder. The main idea of ourTSCM is to make full use of the texture features at the lowest level and thesemantic features at the highest level to achieve the expression enhancement ofsalient regions on features. In the TSCM, we first enhance the position ofpotential salient regions using semantic features. Then, we render and restorethe object details using the texture features. Meanwhile, we also perceiveregions of various scales, and construct interactions between differentregions. Thanks to the perfect combination of TSCM and generic structure, ourTSCNet can take care of both the position and details of salient objects,effectively handling various scenes. Extensive experiments on three datasetsdemonstrate that our TSCNet achieves competitive performance compared to 14state-of-the-art methods. The code and results of our method are available athttps://github.com/MathLee/TSCNet.</description><author>Gongyang Li, Zhen Bai, Zhi Liu</author><pubDate>Wed, 06 Dec 2023 15:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03548v1</guid></item><item><title>Multiple Instance Learning for Digital Pathology: A Review on the State-of-the-Art, Limitations &amp; Future Potential</title><link>http://arxiv.org/abs/2206.04425v2</link><description>Digital whole slides images contain an enormous amount of informationproviding a strong motivation for the development of automated image analysistools. Particularly deep neural networks show high potential with respect tovarious tasks in the field of digital pathology. However, a limitation is givenby the fact that typical deep learning algorithms require (manual) annotationsin addition to the large amounts of image data, to enable effective training.Multiple instance learning exhibits a powerful tool for learning deep neuralnetworks in a scenario without fully annotated data. These methods areparticularly effective in this domain, due to the fact that labels for acomplete whole slide image are often captured routinely, whereas labels forpatches, regions or pixels are not. This potential already resulted in aconsiderable number of publications, with the majority published in the lastthree years. Besides the availability of data and a high motivation from themedical perspective, the availability of powerful graphics processing unitsexhibits an accelerator in this field. In this paper, we provide an overview ofwidely and effectively used concepts of used deep multiple instance learningapproaches, recent advances and also critically discuss remaining challengesand future potential.</description><author>Michael Gadermayr, Maximilian Tschuchnig</author><pubDate>Wed, 06 Dec 2023 15:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.04425v2</guid></item><item><title>GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models</title><link>http://arxiv.org/abs/2312.03543v1</link><description>In the field of autonomous vehicles (AVs), accurately discerning commanderintent and executing linguistic commands within a visual context presents asignificant challenge. This paper introduces a sophisticated encoder-decoderframework, developed to address visual grounding in AVs.Our Context-AwareVisual Grounding (CAVG) model is an advanced system that integrates five coreencoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder. Thisintegration enables the CAVG model to adeptly capture contextual semantics andto learn human emotional features, augmented by state-of-the-art Large LanguageModels (LLMs) including GPT-4. The architecture of CAVG is reinforced by theimplementation of multi-head cross-modal attention mechanisms and aRegion-Specific Dynamic (RSD) layer for attention modulation. Thisarchitectural design enables the model to efficiently process and interpret arange of cross-modal inputs, yielding a comprehensive understanding of thecorrelation between verbal commands and corresponding visual scenes. Empiricalevaluations on the Talk2Car dataset, a real-world benchmark, demonstrate thatCAVG establishes new standards in prediction accuracy and operationalefficiency. Notably, the model exhibits exceptional performance even withlimited training data, ranging from 50% to 75% of the full dataset. Thisfeature highlights its effectiveness and potential for deployment in practicalAV applications. Moreover, CAVG has shown remarkable robustness andadaptability in challenging scenarios, including long-text commandinterpretation, low-light conditions, ambiguous command contexts, inclementweather conditions, and densely populated urban environments. The code for theproposed model is available at our Github.</description><author>Haicheng Liao, Huanming Shen, Zhenning Li, Chengyue Wang, Guofa Li, Yiming Bie, Chengzhong Xu</author><pubDate>Wed, 06 Dec 2023 15:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03543v1</guid></item><item><title>Towards Ordinal Data Science</title><link>http://arxiv.org/abs/2307.09477v2</link><description>Order is one of the main instruments to measure the relationship betweenobjects in (empirical) data. However, compared to methods that use numericalproperties of objects, the amount of ordinal methods developed is rather small.One reason for this is the limited availability of computational resources inthe last century that would have been required for ordinal computations.Another reason -- particularly important for this line of research -- is thatorder-based methods are often seen as too mathematically rigorous for applyingthem to real-world data. In this paper, we will therefore discuss differentmeans for measuring and 'calculating' with ordinal structures -- a specificclass of directed graphs -- and show how to infer knowledge from them. Our aimis to establish Ordinal Data Science as a fundamentally new research agenda.Besides cross-fertilization with other cornerstone machine learning andknowledge representation methods, a broad range of disciplines will benefitfrom this endeavor, including, psychology, sociology, economics, web science,knowledge engineering, scientometrics.</description><author>Gerd Stumme, Dominik DÃ¼rrschnabel, Tom Hanika</author><pubDate>Wed, 06 Dec 2023 15:09:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09477v2</guid></item><item><title>FoodFusion: A Latent Diffusion Model for Realistic Food Image Generation</title><link>http://arxiv.org/abs/2312.03540v1</link><description>Current state-of-the-art image generation models such as Latent DiffusionModels (LDMs) have demonstrated the capacity to produce visually strikingfood-related images. However, these generated images often exhibit an artisticor surreal quality that diverges from the authenticity of real-world foodrepresentations. This inadequacy renders them impractical for applicationsrequiring realistic food imagery, such as training models for image-baseddietary assessment. To address these limitations, we introduce FoodFusion, aLatent Diffusion model engineered specifically for the faithful synthesis ofrealistic food images from textual descriptions. The development of theFoodFusion model involves harnessing an extensive array of open-source fooddatasets, resulting in over 300,000 curated image-caption pairs. Additionally,we propose and employ two distinct data cleaning methodologies to ensure thatthe resulting image-text pairs maintain both realism and accuracy. TheFoodFusion model, thus trained, demonstrates a remarkable ability to generatefood images that exhibit a significant improvement in terms of both realism anddiversity over the publicly available image generation models. We openly sharethe dataset and fine-tuned models to support advancements in this criticalfield of food image synthesis at https://bit.ly/genai4good.</description><author>Olivia Markham, Yuhao Chen, Chi-en Amy Tai, Alexander Wong</author><pubDate>Wed, 06 Dec 2023 15:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03540v1</guid></item><item><title>PaintNet: Unstructured Multi-Path Learning from 3D Point Clouds for Robotic Spray Painting</title><link>http://arxiv.org/abs/2211.06930v3</link><description>Popular industrial robotic problems such as spray painting and weldingrequire (i) conditioning on free-shape 3D objects and (ii) planning of multipletrajectories to solve the task. Yet, existing solutions make strong assumptionson the form of input surfaces and the nature of output paths, resulting inlimited approaches unable to cope with real-data variability. By leveraging onrecent advances in 3D deep learning, we introduce a novel framework capable ofdealing with arbitrary 3D surfaces, and handling a variable number of unorderedoutput paths (i.e. unstructured). Our approach predicts local path segments,which can be later concatenated to reconstruct long-horizon paths. Weextensively validate the proposed method in the context of robotic spraypainting by releasing PaintNet, the first public dataset of expertdemonstrations on free-shape 3D objects collected in a real industrialscenario. A thorough experimental analysis demonstrates the capabilities of ourmodel to promptly predict smooth output paths that cover up to 95% ofpreviously unseen object surfaces, even without explicitly optimizing for paintcoverage.</description><author>Gabriele Tiboni, Raffaello Camoriano, Tatiana Tommasi</author><pubDate>Wed, 06 Dec 2023 14:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06930v3</guid></item><item><title>Low-shot Object Learning with Mutual Exclusivity Bias</title><link>http://arxiv.org/abs/2312.03533v1</link><description>This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias(LSME), the first computational framing of mutual exclusivity bias, aphenomenon commonly observed in infants during word learning. We provide anovel dataset, comprehensive baselines, and a state-of-the-art method to enablethe ML community to tackle this challenging learning task. The goal of LSME isto analyze an RGB image of a scene containing multiple objects and correctlyassociate a previously-unknown object instance with a provided category label.This association is then used to perform low-shot learning to test categorygeneralization. We provide a data generation pipeline for the LSME problem andconduct a thorough analysis of the factors that contribute to its difficulty.Additionally, we evaluate the performance of multiple baselines, includingstate-of-the-art foundation models. Finally, we present a baseline approachthat outperforms state-of-the-art models in terms of low-shot accuracy.</description><author>Anh Thai, Ahmad Humayun, Stefan Stojanov, Zixuan Huang, Bikram Boote, James M. Rehg</author><pubDate>Wed, 06 Dec 2023 14:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03533v1</guid></item><item><title>Personalized Pose Forecasting</title><link>http://arxiv.org/abs/2312.03528v1</link><description>Human pose forecasting is the task of predicting articulated human motiongiven past human motion. There exists a number of popular benchmarks thatevaluate an array of different models performing human pose forecasting. Thesebenchmarks do not reflect that a human interacting system, such as a deliveryrobot, observes and plans for the motion of the same individual over anextended period of time. Every individual has unique and distinct movementpatterns. This is however not reflected in existing benchmarks that evaluate amodel's ability to predict an average human's motion rather than a particularindividual's. We reformulate the human motion forecasting problem and present amodel-agnostic personalization method. Motion forecasting personalization canbe performed efficiently online by utilizing a low-parametric time-seriesanalysis model that personalizes neural network pose predictions.</description><author>Maria Priisalu, Ted Kronvall, Cristian Sminchisescu</author><pubDate>Wed, 06 Dec 2023 14:43:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03528v1</guid></item><item><title>On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm</title><link>http://arxiv.org/abs/2312.03526v1</link><description>Contemporary machine learning requires training large neural networks onmassive datasets and thus faces the challenges of high computational demands.Dataset distillation, as a recent emerging strategy, aims to compressreal-world datasets for efficient training. However, this line of researchcurrently struggle with large-scale and high-resolution datasets, hindering itspracticality and feasibility. To this end, we re-examine the existing datasetdistillation methods and identify three properties required for large-scalereal-world applications, namely, realism, diversity, and efficiency. As aremedy, we propose RDED, a novel computationally-efficient yet effective datadistillation paradigm, to enable both diversity and realism of the distilleddata. Extensive empirical results over various neural architectures anddatasets demonstrate the advancement of RDED: we can distill the fullImageNet-1K to a small dataset comprising 10 images per class within 7 minutes,achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU(while the SOTA only achieves 21% but requires 6 hours).</description><author>Peng Sun, Bei Shi, Daiwei Yu, Tao Lin</author><pubDate>Wed, 06 Dec 2023 14:40:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03526v1</guid></item><item><title>Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling</title><link>http://arxiv.org/abs/2312.03523v1</link><description>We present an open-source, pip installable toolkit, Sig-Networks, the firstof its kind for longitudinal language modelling. A central focus is theincorporation of Signature-based Neural Network models, which have recentlyshown success in temporal tasks. We apply and extend published researchproviding a full suite of signature-based models. Their components can be usedas PyTorch building blocks in future architectures. Sig-Networks enablestask-agnostic dataset plug-in, seamless pre-processing for sequential data,parameter flexibility, automated tuning across a range of models. We examinesignature networks under three different NLP tasks of varying temporalgranularity: counselling conversations, rumour stance switch and mood changesin social media threads, showing SOTA performance in all three, and provideguidance for future tasks. We release the Toolkit as a PyTorch package with anintroductory video, Git repositories for preprocessing and modelling includingsample notebooks on the modeled NLP tasks.</description><author>Talia Tseriotou, Ryan Sze-Yin Chan, Adam Tsakalidis, Iman Munire Bilal, Elena Kochkina, Terry Lyons, Maria Liakata</author><pubDate>Wed, 06 Dec 2023 14:34:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03523v1</guid></item><item><title>Optimal Wildfire Escape Route Planning for Drones under Dynamic Fire and Smoke</title><link>http://arxiv.org/abs/2312.03521v1</link><description>In recent years, the increasing prevalence and intensity of wildfires haveposed significant challenges to emergency response teams. The utilization ofunmanned aerial vehicles (UAVs), commonly known as drones, has shown promise inaiding wildfire management efforts. This work focuses on the development of anoptimal wildfire escape route planning system specifically designed for drones,considering dynamic fire and smoke models. First, the location of the source ofthe wildfire can be well located by information fusion between UAV andsatellite, and the road conditions in the vicinity of the fire can be assessedand analyzed using multi-channel remote sensing data. Second, the road networkcan be extracted and segmented in real time using UAV vision technology, andeach road in the road network map can be given priority based on the results ofroad condition classification. Third, the spread model of dynamic firescalculates the new location of the fire source based on the fire intensity,wind speed and direction, and the radius increases as the wildfire spreads.Smoke is generated around the fire source to create a visual representation ofa burning fire. Finally, based on the improved A* algorithm, which considersall the above factors, the UAV can quickly plan an escape route based on thestarting and destination locations that avoid the location of the fire sourceand the area where it is spreading. By considering dynamic fire and smokemodels, the proposed system enhances the safety and efficiency of droneoperations in wildfire environments.</description><author>Chang Liu, Tamas Sziranyi</author><pubDate>Wed, 06 Dec 2023 14:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03521v1</guid></item><item><title>Defense Against Adversarial Attacks using Convolutional Auto-Encoders</title><link>http://arxiv.org/abs/2312.03520v1</link><description>Deep learning models, while achieving state-of-the-art performance on manytasks, are susceptible to adversarial attacks that exploit inherentvulnerabilities in their architectures. Adversarial attacks manipulate theinput data with imperceptible perturbations, causing the model to misclassifythe data or produce erroneous outputs. This work is based on enhancing therobustness of targeted classifier models against adversarial attacks. Toachieve this, an convolutional autoencoder-based approach is employed thateffectively counters adversarial perturbations introduced to the input images.By generating images closely resembling the input images, the proposedmethodology aims to restore the model's accuracy.</description><author>Shreyasi Mandal</author><pubDate>Wed, 06 Dec 2023 14:29:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03520v1</guid></item><item><title>Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching</title><link>http://arxiv.org/abs/2311.17030v2</link><description>Mechanistic interpretability aims to understand model behaviors in terms ofspecific, interpretable features, often hypothesized to manifest aslow-dimensional subspaces of activations. Specifically, recent studies haveexplored subspace interventions (such as activation patching) as a way tosimultaneously manipulate model behavior and attribute the features behind itto given subspaces. In this work, we demonstrate that these two aims diverge, potentially leadingto an illusory sense of interpretability. Counterintuitively, even if asubspace intervention makes the model's output behave as if the value of afeature was changed, this effect may be achieved by activating a dormantparallel pathway leveraging another subspace that is causally disconnected frommodel outputs. We demonstrate this phenomenon in a distilled mathematicalexample, in two real-world domains (the indirect object identification task andfactual recall), and present evidence for its prevalence in practice. In thecontext of factual recall, we further show a link to rank-1 fact editing,providing a mechanistic explanation for previous work observing aninconsistency between fact editing performance and fact localization. However, this does not imply that activation patching of subspaces isintrinsically unfit for interpretability. To contextualize our findings, wealso show what a success case looks like in a task (indirect objectidentification) where prior manual circuit analysis informs an understanding ofthe location of a feature. We explore the additional evidence needed to arguethat a patched subspace is faithful.</description><author>Aleksandar Makelov, Georg Lange, Neel Nanda</author><pubDate>Wed, 06 Dec 2023 14:28:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17030v2</guid></item><item><title>Active Wildfires Detection and Dynamic Escape Routes Planning for Humans through Information Fusion between Drones and Satellites</title><link>http://arxiv.org/abs/2312.03519v1</link><description>UAVs are playing an increasingly important role in the field of wildernessrescue by virtue of their flexibility. This paper proposes a fusion of UAVvision technology and satellite image analysis technology for active wildfiresdetection and road networks extraction of wildfire areas and real-time dynamicescape route planning for people in distress. Firstly, the fire source locationand the segmentation of smoke and flames are targeted based on Sentinel 2satellite imagery. Secondly, the road segmentation and the road conditionassessment are performed by D-linkNet and NDVI values in the central area ofthe fire source by UAV. Finally, the dynamic optimal route planning for humansin real time is performed by the weighted A* algorithm in the road network withthe dynamic fire spread model. Taking the Chongqing wildfire on August 24,2022, as a case study, the results demonstrate that the dynamic escape routeplanning algorithm can provide an optimal real-time navigation path for humansin the presence of fire through the information fusion of UAVs and satellites.</description><author>Chang Liu, Tamas Sziranyi</author><pubDate>Wed, 06 Dec 2023 14:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03519v1</guid></item><item><title>FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion Models</title><link>http://arxiv.org/abs/2312.03517v1</link><description>The substantial computational costs of diffusion models, particularly due tothe repeated denoising steps crucial for high-quality image generation, presenta major obstacle to their widespread adoption. While several studies haveattempted to address this issue by reducing the number of score functionevaluations using advanced ODE solvers without fine-tuning, the decreasednumber of denoising iterations misses the opportunity to update fine details,resulting in noticeable quality degradation. In our work, we introduce anadvanced acceleration technique that leverages the temporal redundancy inherentin diffusion models. Reusing feature maps with high temporal similarity opensup a new opportunity to save computation without sacrificing output quality. Torealize the practical benefits of this intuition, we conduct an extensiveanalysis and propose a novel method, FRDiff. FRDiff is designed to harness theadvantages of both reduced NFE and feature reuse, achieving a Pareto frontierthat balances fidelity and latency trade-offs in various generative tasks.</description><author>Junhyuk So, Jungwon Lee, Eunhyeok Park</author><pubDate>Wed, 06 Dec 2023 14:24:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03517v1</guid></item><item><title>Clustering by Contour coreset and variational quantum eigensolver</title><link>http://arxiv.org/abs/2312.03516v1</link><description>Recent work has proposed solving the k-means clustering problem on quantumcomputers via the Quantum Approximate Optimization Algorithm (QAOA) and coresettechniques. Although the current method demonstrates the possibility of quantumk-means clustering, it does not ensure high accuracy and consistency across awide range of datasets. The existing coreset techniques are designed forclassical algorithms and there has been no quantum-tailored coreset techniquewhich is designed to boost the accuracy of quantum algorithms. In this work, wepropose solving the k-means clustering problem with the variational quantumeigensolver (VQE) and a customised coreset method, the Contour coreset, whichhas been formulated with specific focus on quantum algorithms. Extensivesimulations with synthetic and real-life data demonstrated that our VQE+ContourCoreset approach outperforms existing QAOA+Coreset k-means clusteringapproaches with higher accuracy and lower standard deviation. Our work hasshown that quantum tailored coreset techniques has the potential tosignificantly boost the performance of quantum algorithms when compared tousing generic off-the-shelf coreset techniques.</description><author>Canaan Yung, Muhammad Usman</author><pubDate>Wed, 06 Dec 2023 14:21:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03516v1</guid></item><item><title>Constrained Parameter Regularization</title><link>http://arxiv.org/abs/2311.09058v2</link><description>Regularization is a critical component in deep learning training, with weightdecay being a commonly used approach. It applies a constant penalty coefficientuniformly across all parameters. This may be unnecessarily restrictive for someparameters, while insufficiently restricting others. To dynamically adjustpenalty coefficients for different parameter groups, we present constrainedparameter regularization (CPR) as an alternative to traditional weight decay.Instead of applying a single constant penalty to all parameters, we enforce anupper bound on a statistical measure (e.g., the L$_2$-norm) of parametergroups. Consequently, learning becomes a constraint optimization problem, whichwe address by an adaptation of the augmented Lagrangian method. CPR onlyrequires two hyperparameters and incurs no measurable runtime overhead.Additionally, we propose a simple but efficient mechanism to adapt the upperbounds during the optimization. We provide empirical evidence of CPR's efficacyin experiments on the "grokking" phenomenon, computer vision, and languagemodeling tasks. Our results demonstrate that CPR counteracts the effects ofgrokking and consistently matches or outperforms traditional weight decay.</description><author>JÃ¶rg K. H. Franke, Michael Hefenbrock, Gregor Koehler, Frank Hutter</author><pubDate>Wed, 06 Dec 2023 14:20:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09058v2</guid></item><item><title>Kandinsky 3.0 Technical Report</title><link>http://arxiv.org/abs/2312.03511v1</link><description>We present Kandinsky 3.0, a large-scale text-to-image generation model basedon latent diffusion, continuing the series of text-to-image Kandinsky modelsand reflecting our progress to achieve higher quality and realism of imagegeneration. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0leverages a two times larger U-Net backbone, a ten times larger text encoderand removes diffusion mapping. We describe the architecture of the model, thedata collection procedure, the training technique, and the production system ofuser interaction. We focus on the key components that, as we have identified asa result of a large number of experiments, had the most significant impact onimproving the quality of our model compared to the others. By our side-by-sidecomparisons, Kandinsky becomes better in text understanding and works better onspecific domains. Project page: https://ai-forever.github.io/Kandinsky-3</description><author>Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, Denis Dimitrov</author><pubDate>Wed, 06 Dec 2023 14:13:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03511v1</guid></item><item><title>Towards Sobolev Training</title><link>http://arxiv.org/abs/2312.03510v1</link><description>The increasing use of stochastic models for describing complex phenomenawarrants surrogate models that capture the reference model characteristics at afraction of the computational cost, foregoing potentially expensive Monte Carlosimulation. The predominant approach of fitting a large neural network and thenpruning it to a reduced size has commonly neglected shortcomings. The producedsurrogate models often will not capture the sensitivities and uncertaintiesinherent in the original model. In particular, (higher-order) derivativeinformation of such surrogates could differ drastically. Given a large enoughnetwork, we expect this derivative information to match. However, the prunedmodel will almost certainly not share this behavior. In this paper, we propose to find surrogate models by using sensitivityinformation throughout the learning and pruning process. We build on work usingInterval Adjoint Significance Analysis for pruning and combine it with therecent advancements in Sobolev Training to accurately model the originalsensitivity information in the pruned neural network based surrogate model. Weexperimentally underpin the method on an example of pricing a multidimensionalBasket option modelled through a stochastic differential equation with Brownianmotion. The proposed method is, however, not limited to the domain ofquantitative finance, which was chosen as a case study for intuitiveinterpretations of the sensitivities. It serves as a foundation for buildingfurther surrogate modelling techniques considering sensitivity information.</description><author>Neil Kichler, Sher Afghan, Uwe Naumann</author><pubDate>Wed, 06 Dec 2023 14:13:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03510v1</guid></item><item><title>Error Detection for Text-to-SQL Semantic Parsing</title><link>http://arxiv.org/abs/2305.13683v2</link><description>Despite remarkable progress in text-to-SQL semantic parsing in recent years,the performance of existing parsers is still far from perfect. Specifically,modern text-to-SQL parsers based on deep learning are often over-confident,thus casting doubt on their trustworthiness when deployed for real use. In thispaper, we propose a parser-independent error detection model for text-to-SQLsemantic parsing. Using a language model of code as its bedrock, we enhance ourerror detection model with graph neural networks that learn structural featuresof both natural language questions and SQL queries. We train our model onrealistic parsing errors collected from a cross-domain setting, which leads tostronger generalization ability. Experiments with three strong text-to-SQLparsers featuring different decoding mechanisms show that our approachoutperforms parser-dependent uncertainty metrics. Our model could alsoeffectively improve the performance and usability of text-to-SQL semanticparsers regardless of their architectures. (Our implementation is available athttps://github.com/OSU-NLP-Group/Text2SQL-Error-Detection)</description><author>Shijie Chen, Ziru Chen, Huan Sun, Yu Su</author><pubDate>Wed, 06 Dec 2023 14:09:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13683v2</guid></item><item><title>Gravitational cell detection and tracking in fluorescence microscopy data</title><link>http://arxiv.org/abs/2312.03509v1</link><description>Automatic detection and tracking of cells in microscopy images are majorapplications of computer vision technologies in both biomedical research andclinical practice. Though machine learning methods are increasingly common inthese fields, classical algorithms still offer significant advantages for bothtasks, including better explainability, faster computation, lower hardwarerequirements and more consistent performance. In this paper, we present a novelapproach based on gravitational force fields that can compete with, andpotentially outperform modern machine learning models when applied tofluorescence microscopy images. This method includes detection, segmentation,and tracking elements, with the results demonstrated on a Cell TrackingChallenge dataset.</description><author>Nikomidisz Eftimiu, Michal Kozubek</author><pubDate>Wed, 06 Dec 2023 14:08:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03509v1</guid></item><item><title>Nash Learning from Human Feedback</title><link>http://arxiv.org/abs/2312.00886v3</link><description>Reinforcement learning from human feedback (RLHF) has emerged as the mainparadigm for aligning large language models (LLMs) with human preferences.Typically, RLHF involves the initial step of learning a reward model from humanfeedback, often expressed as preferences between pairs of text generationsproduced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned byoptimizing it to maximize the reward model through a reinforcement learningalgorithm. However, an inherent limitation of current reward models is theirinability to fully represent the richness of human preferences and theirdependency on the sampling distribution. In this study, we introduce an alternative pipeline for the fine-tuning ofLLMs using pairwise human feedback. Our approach entails the initial learningof a preference model, which is conditioned on two inputs given a prompt,followed by the pursuit of a policy that consistently generates responsespreferred over those generated by any competing policy, thus defining the Nashequilibrium of this preference model. We term this approach Nash learning fromhuman feedback (NLHF). In the context of a tabular policy representation, we present a novelalgorithmic solution, Nash-MD, founded on the principles of mirror descent.This algorithm produces a sequence of policies, with the last iterationconverging to the regularized Nash equilibrium. Additionally, we exploreparametric representations of policies and introduce gradient descentalgorithms for deep-learning architectures. To demonstrate the effectiveness ofour approach, we present experimental results involving the fine-tuning of aLLM for a text summarization task. We believe NLHF offers a compelling avenuefor preference learning and policy optimization with the potential of advancingthe field of aligning LLMs with human preferences.</description><author>RÃ©mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, Bilal Piot</author><pubDate>Wed, 06 Dec 2023 14:07:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00886v3</guid></item><item><title>Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation</title><link>http://arxiv.org/abs/2312.03502v1</link><description>The success of large language models has inspired the computer visioncommunity to explore image segmentation foundation model that is able tozero/few-shot generalize through prompt engineering. Segment-Anything(SAM),among others, is the state-of-the-art image segmentation foundation modeldemonstrating strong zero/few-shot generalization. Despite the success, recentstudies reveal the weakness of SAM under strong distribution shift. Inparticular, SAM performs awkwardly on corrupted natural images, camouflagedimages, medical images, etc. Motivated by the observations, we aim to develop aself-training based strategy to adapt SAM to target distribution. Given theunique challenges of large source dataset, high computation cost and incorrectpseudo label, we propose a weakly supervised self-training architecture withanchor regularization and low-rank finetuning to improve the robustness andcomputation efficiency of adaptation. We validate the effectiveness on 5 typesof downstream segmentation tasks including natural clean/corrupted images,medical images, camouflaged images and robotic images. Our proposed method istask-agnostic in nature and outperforms pre-trained SAM and state-of-the-artdomain adaptation methods on almost all downstream tasks with the same testingprompt inputs.</description><author>Haojie Zhang, Yongyi Su, Xun Xu, Kui Jia</author><pubDate>Wed, 06 Dec 2023 13:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03502v1</guid></item></channel></rss>