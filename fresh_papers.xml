<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 06 Jan 2025 13:00:10 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</title><link>http://arxiv.org/abs/2501.01957v1</link><description>Recent Multimodal Large Language Models (MLLMs) have typically focused onintegrating visual and textual modalities, with less emphasis placed on therole of speech in enhancing interaction. However, speech plays a crucial rolein multimodal dialogue systems, and implementing high-performance in bothvision and speech tasks remains a significant challenge due to the fundamentalmodality differences. In this paper, we propose a carefully designedmulti-stage training methodology that progressively trains LLM to understandboth visual and speech information, ultimately enabling fluent vision andspeech interaction. Our approach not only preserves strong vision-languagecapacity, but also enables efficient speech-to-speech dialogue capabilitieswithout separate ASR and TTS modules, significantly accelerating multimodalend-to-end response speed. By comparing our method against state-of-the-artcounterparts across benchmarks for image, video, and speech tasks, wedemonstrate that our model is equipped with both strong visual and speechcapabilities, making near real-time vision and speech interaction.</description><author>Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</author><pubDate>Fri, 03 Jan 2025 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01957v1</guid></item><item><title>InvSeg: Test-Time Prompt Inversion for Semantic Segmentation</title><link>http://arxiv.org/abs/2410.11473v2</link><description>Visual-textual correlations in the attention maps derived from text-to-imagediffusion models are proven beneficial to dense visual prediction tasks, e.g.,semantic segmentation. However, a significant challenge arises due to the inputdistributional discrepancy between the context-rich sentences used for imagegeneration and the isolated class names typically used in semanticsegmentation. This discrepancy hinders diffusion models from capturing accuratevisual-textual correlations. To solve this, we propose InvSeg, a test-timeprompt inversion method that tackles open-vocabulary semantic segmentation byinverting image-specific visual context into text prompt embedding space,leveraging structure information derived from the diffusion model'sreconstruction process to enrich text prompts so as to associate each classwith a structure-consistent mask. Specifically, we introduce Contrastive SoftClustering (CSC) to align derived masks with the image's structure information,softly selecting anchors for each class and calculating weighted distances topush inner-class pixels closer while separating inter-class pixels, therebyensuring mask distinction and internal consistency. By incorporatingsample-specific context, InvSeg learns context-rich text prompts in embeddingspace and achieves accurate semantic alignment across modalities. Experimentsshow that InvSeg achieves state-of-the-art performance on the PASCAL VOC,PASCAL Context and COCO Object datasets.</description><author>Jiayi Lin, Jiabo Huang, Jian Hu, Shaogang Gong</author><pubDate>Fri, 03 Jan 2025 18:59:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11473v2</guid></item><item><title>Metadata Conditioning Accelerates Language Model Pre-training</title><link>http://arxiv.org/abs/2501.01956v1</link><description>The vast diversity of styles, domains, and quality levels present in languagemodel pre-training corpora is essential in developing general modelcapabilities, but efficiently learning and deploying the correct behaviorsexemplified in each of these heterogeneous data sources is challenging. Toaddress this, we propose a new method, termed Metadata Conditioning thenCooldown (MeCo), to incorporate additional learning cues during pre-training.MeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside thetext during training and later uses a cooldown phase with only the standardtext, thereby enabling the model to function normally even without metadata.MeCo significantly accelerates pre-training across different model scales (600Mto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). Forinstance, a 1.6B language model trained with MeCo matches the downstream taskperformance of standard pre-training while using 33% less data. Additionally,MeCo enables us to steer language models by conditioning the inference prompton either real or fabricated metadata that encodes the desired properties ofthe output: for example, prepending wikipedia.org to reduce harmful generationsor factquizmaster.com (fabricated) to improve common knowledge taskperformance. We also demonstrate that MeCo is compatible with different typesof metadata, such as model-generated topics. MeCo is remarkably simple, adds nocomputational overhead, and demonstrates promise in producing more capable andsteerable language models.</description><author>Tianyu Gao, Alexander Wettig, Luxi He, Yihe Dong, Sadhika Malladi, Danqi Chen</author><pubDate>Fri, 03 Jan 2025 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01956v1</guid></item><item><title>MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of Accelerators</title><link>http://arxiv.org/abs/2501.01951v1</link><description>Graph convolutional networks (GCNs) have demonstrated superiority ingraph-based learning tasks. However, training GCNs on full graphs isparticularly challenging, due to the following two challenges: (1) theassociated feature tensors can easily explode the memory and block thecommunication bandwidth of modern accelerators, and (2) the computationworkflow in training GCNs alternates between sparse and dense matrixoperations, complicating the efficient utilization of computational resources.Existing solutions for scalable distributed full-graph GCN training mostlyadopt partition parallelism, which is unsatisfactory as they only partiallyaddress the first challenge while incurring scaled-out communication volume. Tothis end, we propose MixGCN aiming to simultaneously address both theaforementioned challenges towards GCN training. To tackle the first challenge,MixGCN integrates mixture of parallelism. Both theoretical and empiricalanalysis verify its constant communication volumes and enhanced balancedworkload; For handling the second challenge, we consider mixture ofaccelerators (i.e., sparse and dense accelerators) with a dedicated acceleratorfor GCN training and a fine-grain pipeline. Extensive experiments show thatMixGCN achieves boosted training efficiency and scalability.</description><author>Cheng Wan, Runkao Tao, Zheng Du, Yang Katie Zhao, Yingyan Celine Lin</author><pubDate>Fri, 03 Jan 2025 18:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01951v1</guid></item><item><title>MADGEN -- Mass-Spec attends to De Novo Molecular generation</title><link>http://arxiv.org/abs/2501.01950v1</link><description>The annotation (assigning structural chemical identities) of MS/MS spectraremains a significant challenge due to the enormous molecular diversity inbiological samples and the limited scope of reference databases. Currently, thevast majority of spectral measurements remain in the "dark chemical space"without structural annotations. To improve annotation, we propose MADGEN(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based methodfor de novo molecular structure generation guided by mass spectrometry data.MADGEN operates in two stages: scaffold retrieval and spectra-conditionedmolecular generation starting with the scaffold. In the first stage, given anMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employcontrastive learning to align mass spectra with candidate molecular scaffolds.In the second stage, starting from the retrieved scaffold, we employ the MS/MSspectrum to guide an attention-based generative model to generate the finalmolecule. Our approach constrains the molecular generation search space,reducing its complexity and improving generation accuracy. We evaluate MADGENon three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN'sperformance with a predictive scaffold retriever and with an oracle retriever.We demonstrate the effectiveness of using attention to integrate spectralinformation throughout the generation process to achieve strong results withthe oracle retriever.</description><author>Yinkai Wang, Xiaohui Chen, Liping Liu, Soha Hassoun</author><pubDate>Fri, 03 Jan 2025 18:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01950v1</guid></item><item><title>VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment</title><link>http://arxiv.org/abs/2501.01949v1</link><description>Efficiently reconstructing accurate 3D models from monocular video is a keychallenge in computer vision, critical for advancing applications in virtualreality, robotics, and scene understanding. Existing approaches typicallyrequire pre-computed camera parameters and frame-by-frame reconstructionpipelines, which are prone to error accumulation and entail significantcomputational overhead. To address these limitations, we introduce VideoLifter,a novel framework that leverages geometric priors from a learnable model toincrementally optimize a globally sparse to dense 3D representation directlyfrom video sequences. VideoLifter segments the video sequence into localwindows, where it matches and registers frames, constructs consistentfragments, and aligns them hierarchically to produce a unified 3D model. Bytracking and propagating sparse point correspondences across frames andfragments, VideoLifter incrementally refines camera poses and 3D structure,minimizing reprojection error for improved accuracy and robustness. Thisapproach significantly accelerates the reconstruction process, reducingtraining time by over 82% while surpassing current state-of-the-art methods invisual fidelity and computational efficiency.</description><author>Wenyan Cong, Kevin Wang, Jiahui Lei, Colton Stearns, Yuanhao Cai, Dilin Wang, Rakesh Ranjan, Matt Feiszli, Leonidas Guibas, Zhangyang Wang, Weiyao Wang, Zhiwen Fan</author><pubDate>Fri, 03 Jan 2025 18:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01949v1</guid></item><item><title>Cold-Start Recommendation towards the Era of Large Language Models (LLMs): A Comprehensive Survey and Roadmap</title><link>http://arxiv.org/abs/2501.01945v1</link><description>Cold-start problem is one of the long-standing challenges in recommendersystems, focusing on accurately modeling new or interaction-limited users oritems to provide better recommendations. Due to the diversification of internetplatforms and the exponential growth of users and items, the importance ofcold-start recommendation (CSR) is becoming increasingly evident. At the sametime, large language models (LLMs) have achieved tremendous success and possessstrong capabilities in modeling user and item information, providing newpotential for cold-start recommendations. However, the research community onCSR still lacks a comprehensive review and reflection in this field. Based onthis, in this paper, we stand in the context of the era of large languagemodels and provide a comprehensive review and discussion on the roadmap,related literature, and future directions of CSR. Specifically, we haveconducted an exploration of the development path of how existing CSR utilizesinformation, from content features, graph relations, and domain information, tothe world knowledge possessed by large language models, aiming to provide newinsights for both the research and industrial communities on CSR. Relatedresources of cold-start recommendations are collected and continuously updatedfor the community inhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.</description><author>Weizhi Zhang, Yuanchen Bei, Liangwei Yang, Henry Peng Zou, Peilin Zhou, Aiwei Liu, Yinghui Li, Hao Chen, Jianling Wang, Yu Wang, Feiran Huang, Sheng Zhou, Jiajun Bu, Allen Lin, James Caverlee, Fakhri Karray, Irwin King, Philip S. Yu</author><pubDate>Fri, 03 Jan 2025 18:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01945v1</guid></item><item><title>A Hassle-free Algorithm for Private Learning in Practice: Don't Use Tree Aggregation, Use BLTs</title><link>http://arxiv.org/abs/2408.08868v2</link><description>The state-of-the-art for training on-device language models for mobilekeyboard applications combines federated learning (FL) with differentialprivacy (DP) via the DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm. Twovariants of DP-FTRL are used in practice, tree aggregation and matrixfactorization. However, tree aggregation suffers from significantly suboptimalprivacy/utility tradeoffs, while matrix mechanisms require expensiveoptimization parameterized by hard-to-estimate-in-advance constants, and highruntime memory costs.This paper extends the recently introduced Buffered LinearToeplitz (BLT) mechanism to multi-participation scenarios. Our BLT-DP-FTRLmaintains the ease-of-use advantages of tree aggregation, while essentiallymatching matrix factorization in terms of utility and privacy. We evaluateBLT-DP-FTRL on the StackOverflow dataset, serving as a re-producible simulationbenchmark, and across four on-device language model tasks in a production FLsystem. Our empirical results highlight the advantages of the BLT mechanism andelevate the practicality and effectiveness of DP in real-world scenarios.</description><author>H. Brendan McMahan, Zheng Xu, Yanxiang Zhang</author><pubDate>Fri, 03 Jan 2025 18:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08868v2</guid></item><item><title>Transfer Learning with Partially Observable Offline Data via Causal Bounds</title><link>http://arxiv.org/abs/2308.03572v4</link><description>Transfer learning has emerged as an effective approach to accelerate learningby integrating knowledge from related source agents. However, challenges arisedue to data heterogeneity-such as differences in feature sets or incompletedatasets-which often results in the nonidentifiability of causal effects. Inthis paper, we investigate transfer learning in partially observable contextualbandits, where agents operate with incomplete information and limited access tohidden confounders. To address the challenges posed by unobserved confounders,we formulate optimization problems to derive tight bounds on thenonidentifiable causal effects. We then propose an efficient method thatdiscretizes the functional constraints of unknown distributions into linearconstraints, allowing us to sample compatible causal models through asequential process of solving linear programs. This method takes into accountestimation errors and exhibits strong convergence properties, ensuring robustand reliable causal bounds. Leveraging these causal bounds, we improveclassical bandit algorithms, achieving tighter regret upper and lower boundsrelative to the sizes of action sets and function spaces. In tasks involvingfunction approximation, which are crucial for handling complex context spaces,our method significantly improves the dependence on function space sizecompared to previous work. We formally prove that our causally enhancedalgorithms outperform classical bandit algorithms, achieving notably fasterconvergence rates. The applicability of our approach is further illustratedthrough an example of offline pricing policy learning with censored demand.Simulations confirm the superiority of our approach over state-of-the-artmethods, demonstrating its potential to enhance contextual bandit agents inreal-world applications, especially when data is scarce, costly, or restricteddue to privacy concerns.</description><author>Xueping Gong, Wei You, Jiheng Zhang</author><pubDate>Fri, 03 Jan 2025 18:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03572v4</guid></item><item><title>Improving Transducer-Based Spoken Language Understanding with Self-Conditioned CTC and Knowledge Transfer</title><link>http://arxiv.org/abs/2501.01936v1</link><description>In this paper, we propose to improve end-to-end (E2E) spoken languageunderstand (SLU) in an RNN transducer model (RNN-T) by incorporating a jointself-conditioned CTC automatic speech recognition (ASR) objective. Our proposedmodel is akin to an E2E differentiable cascaded model which performs ASR andSLU sequentially and we ensure that the SLU task is conditioned on the ASR taskby having CTC self conditioning. This novel joint modeling of ASR and SLUimproves SLU performance significantly over just using SLU optimization. Wefurther improve the performance by aligning the acoustic embeddings of thismodel with the semantically richer BERT model. Our proposed knowledge transferstrategy makes use of a bag-of-entity prediction layer on the alignedembeddings and the output of this is used to condition the RNN-T based SLUdecoding. These techniques show significant improvement over several strongbaselines and can perform at par with large models like Whisper withsignificantly fewer parameters.</description><author>Vishal Sunder, Eric Fosler-Lussier</author><pubDate>Fri, 03 Jan 2025 18:19:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01936v1</guid></item><item><title>On robust recovery of signals from indirect observations</title><link>http://arxiv.org/abs/2501.01935v1</link><description>We consider an uncertain linear inverse problem as follows. Given observation$\omega=Ax_*+\zeta$ where $A\in {\bf R}^{m\times p}$ and $\zeta\in {\bf R}^{m}$is observation noise, we want to recover unknown signal $x_*$, known to belongto a convex set ${\cal X}\subset{\bf R}^{n}$. As opposed to the "standard"setting of such problem, we suppose that the model noise $\zeta$ is "corrupted"-- contains an uncertain (deterministic dense or singular) component.Specifically, we assume that $\zeta$ decomposes into $\zeta=N\nu_*+\xi$ where$\xi$ is the random noise and $N\nu_*$ is the "adversarial contamination" withknown $\cal N\subset {\bf R}^n$ such that $\nu_*\in \cal N$ and $N\in {\bfR}^{m\times n}$. We consider two "uncertainty setups" in which $\cal N$ iseither a convex bounded set or is the set of sparse vectors (with at most $s$nonvanishing entries). We analyse the performance of "uncertainty-immunized"polyhedral estimates -- a particular class of nonlinear estimates as introducedin [15, 16] -- and show how "presumably good" estimates of the sort may beconstructed in the situation where the signal set is an ellitope (essentially,a symmetric convex set delimited by quadratic surfaces) by means of efficientconvex optimization routines.</description><author>Yannis Bekri, Anatoli Juditsky, Arkadi Nemirovski</author><pubDate>Fri, 03 Jan 2025 18:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01935v1</guid></item><item><title>Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent Hypersonic Flows on Arbitrary Grids</title><link>http://arxiv.org/abs/2501.01934v1</link><description>Designing re-entry vehicles requires accurate predictions of hypersonic flowaround their geometry. Rapid prediction of such flows can revolutionize vehicledesign, particularly for morphing geometries. We evaluate advanced neuraloperator models such as Deep Operator Networks (DeepONet),parameter-conditioned U-Net, Fourier Neural Operator (FNO), and MeshGraphNet,with the objective of addressing the challenge of learning geometry-dependenthypersonic flow fields with limited data. Specifically, we compare theperformance of these models for two grid types: uniform Cartesian and irregulargrids. To train these models, we use 36 unique elliptic geometries forgenerating high-fidelity simulations with a high-order entropy-stable DGSEMsolver, emphasizing the challenge of working with a scarce dataset. We evaluateand compare the four operator-based models for their efficacy in predictinghypersonic flow field around the elliptic body. Moreover, we develop a novelframework, called Fusion DeepONet, which leverages neural field concepts andgeneralizes effectively across varying geometries. Despite the scarcity oftraining data, Fusion DeepONet achieves performance comparable toparameter-conditioned U-Net on uniform grids while it outperforms MeshGraphNetand vanilla DeepONet on irregular, arbitrary grids. Fusion DeepONet requiressignificantly fewer trainable parameters as compared to U-Net, MeshGraphNet,and FNO, making it computationally efficient. We also analyze the basisfunctions of the Fusion DeepONet model using Singular Value Decomposition. Thisanalysis reveals that Fusion DeepONet generalizes effectively to unseensolutions and adapts to varying geometries and grid points, demonstrating itsrobustness in scenarios with limited training data.</description><author>Ahmad Peyvan, Varun Kumar</author><pubDate>Fri, 03 Jan 2025 18:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01934v1</guid></item><item><title>Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues and Challenges</title><link>http://arxiv.org/abs/2501.01933v1</link><description>This thesis presents Abstractive Text Summarization models for contemporarySanskrit prose. The first chapter, titled Introduction, presents the motivationbehind this work, the research questions, and the conceptual framework.Sanskrit is a low-resource inflectional language. The key research questionthat this thesis investigates is what the challenges in developing anabstractive TS for Sanskrit. To answer the key research questions,sub-questions based on four different themes have been posed in this work. Thesecond chapter, Literature Review, surveys the previous works done. The thirdchapter, data preparation, answers the remaining three questions from the thirdtheme. It reports the data collection and preprocessing challenges for bothlanguage model and summarization model trainings. The fourth chapter reportsthe training and inference of models and the results obtained therein. Thisresearch has initiated a pipeline for Sanskrit abstractive text summarizationand has reported the challenges faced at every stage of the development. Theresearch questions based on every theme have been answered to answer the keyresearch question.</description><author>Shagun Sinha</author><pubDate>Fri, 03 Jan 2025 18:12:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01933v1</guid></item><item><title>Physics-constrained coupled neural differential equations for one dimensional blood flow modeling</title><link>http://arxiv.org/abs/2411.05631v2</link><description>Computational cardiovascular flow modeling plays a crucial role inunderstanding blood flow dynamics. While 3D models provide acute details, theyare computationally expensive, especially with fluid-structure interaction(FSI) simulations. 1D models offer a computationally efficient alternative, bysimplifying the 3D Navier-Stokes equations through axisymmetric flow assumptionand cross-sectional averaging. However, traditional 1D models based on finiteelement methods (FEM) often lack accuracy compared to 3D averaged solutions.This study introduces a novel physics-constrained machine learning techniquethat enhances the accuracy of 1D blood flow models while maintainingcomputational efficiency. Our approach, utilizing a physics-constrained coupledneural differential equation (PCNDE) framework, demonstrates superiorperformance compared to conventional FEM-based 1D models across a wide range ofinlet boundary condition waveforms and stenosis blockage ratios. A keyinnovation lies in the spatial formulation of the momentum conservationequation, departing from the traditional temporal approach and capitalizing onthe inherent temporal periodicity of blood flow. This spatial neuraldifferential equation formulation switches space and time and overcomes issuesrelated to coupling stability and smoothness, while simplifying boundarycondition implementation. The model accurately captures flow rate, area, andpressure variations for unseen waveforms and geometries. We evaluate themodel's robustness to input noise and explore the loss landscapes associatedwith the inclusion of different physics terms. This advanced 1D modelingtechnique offers promising potential for rapid cardiovascular simulations,achieving computational efficiency and accuracy. By combining the strengths ofphysics-based and data-driven modeling, this approach enables fast and accuratecardiovascular simulations.</description><author>Hunor Csala, Arvind Mohan, Daniel Livescu, Amirhossein Arzani</author><pubDate>Fri, 03 Jan 2025 18:11:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05631v2</guid></item><item><title>Bridging Classification and Segmentation in Osteosarcoma Assessment via Foundation and Discrete Diffusion Models</title><link>http://arxiv.org/abs/2501.01932v1</link><description>Osteosarcoma, the most common primary bone cancer, often requires accuratenecrosis assessment from whole slide images (WSIs) for effective treatmentplanning and prognosis. However, manual assessments are subjective and prone tovariability. In response, we introduce FDDM, a novel framework bridging the gapbetween patch classification and region-based segmentation. FDDM operates intwo stages: patch-based classification, followed by region-based refinement,enabling cross-patch information intergation. Leveraging a newly curateddataset of osteosarcoma images, FDDM demonstrates superior segmentationperformance, achieving up to a 10% improvement mIOU and a 32.12% enhancement innecrosis rate estimation over state-of-the-art methods. This framework sets anew benchmark in osteosarcoma assessment, highlighting the potential offoundation models and diffusion-based refinements in complex medical imagingtasks.</description><author>Manh Duong Nguyen, Dac Thai Nguyen, Trung Viet Nguyen, Homi Yamada, Huy Hieu Pham, Phi Le Nguyen</author><pubDate>Fri, 03 Jan 2025 18:06:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01932v1</guid></item><item><title>GoBERT: Gene Ontology Graph Informed BERT for Universal Gene Function Prediction</title><link>http://arxiv.org/abs/2501.01930v1</link><description>Exploring the functions of genes and gene products is crucial to a wide rangeof fields, including medical research, evolutionary biology, and environmentalscience. However, discovering new functions largely relies on expensive andexhaustive wet lab experiments. Existing methods of automatic functionannotation or prediction mainly focus on protein function prediction withsequence, 3D-structures or protein family information. In this study, wepropose to tackle the gene function prediction problem by exploring GeneOntology graph and annotation with BERT (GoBERT) to decipher the underlyingrelationships among gene functions. Our proposed novel function prediction taskutilizes existing functions as inputs and generalizes the function predictionto gene and gene products. Specifically, two pre-train tasks are designed tojointly train GoBERT to capture both explicit and implicit relations offunctions. Neighborhood prediction is a self-supervised multi-labelclassification task that captures the explicit function relations. Specifiedmasking and recovering task helps GoBERT in finding implicit patterns amongfunctions. The pre-trained GoBERT possess the ability to predict novelfunctions for various gene and gene products based on known functionalannotations. Extensive experiments, biological case studies, and ablationstudies are conducted to demonstrate the superiority of our proposed GoBERT.</description><author>Yuwei Miao, Yuzhi Guo, Hehuan Ma, Jingquan Yan, Feng Jiang, Rui Liao, Junzhou Huang</author><pubDate>Fri, 03 Jan 2025 18:02:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01930v1</guid></item><item><title>Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding</title><link>http://arxiv.org/abs/2501.01926v1</link><description>Large vision-language models (LVLMs) have shown remarkable capabilities invisual-language understanding for downstream multi-modal tasks. Despite theirsuccess, LVLMs still suffer from generating hallucinations in complexgeneration tasks, leading to inconsistencies between visual inputs andgenerated content. To address this issue, some approaches have introducedinference-time interventions, such as contrastive decoding and attentionrectification, to reduce overreliance on language priors. However, theseapproaches overlook hallucinations stemming from spurious inter-modalitycorrelations. In this paper, we propose an Inter-Modality CorrelationCalibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in atraining-free manner. In this method, we design a Cross-Modal Value-EnhancedDecoding(CMVED) module to alleviate hallucination by a novel contrastivedecoding mechanism. During the estimation of distorted distribution, CMVEDmasks the value vectors associated with significant cross-modal attentionweights, which address both uni-modality overreliance and misleadinginter-modality correlations. Additionally, a Content-Driven AttentionRefinement(CDAR) module refines cross-modal attention weights, guiding LVLMs tofocus on important visual content. Experimental results on diversehallucination benchmarks validate the superiority of our method over existingstate-of-the-art techniques in reducing hallucinations in LVLM text generation.Our code will be available at https://github.com/lijm48/IMCCD.</description><author>Jiaming Li, Jiacheng Zhang, Zequn Jie, Lin Ma, Guanbin Li</author><pubDate>Fri, 03 Jan 2025 17:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01926v1</guid></item><item><title>Transformer-Driven Inverse Problem Transform for Fast Blind Hyperspectral Image Dehazing</title><link>http://arxiv.org/abs/2501.01924v1</link><description>Hyperspectral dehazing (HyDHZ) has become a crucial signal processingtechnology to facilitate the subsequent identification and classificationtasks, as the airborne visible/infrared imaging spectrometer (AVIRIS) dataportal reports a massive portion of haze-corrupted areas in typicalhyperspectral remote sensing images. The idea of inverse problem transform(IPT) has been proposed in recent remote sensing literature in order toreformulate a hardly tractable inverse problem (e.g., HyDHZ) into a relativelysimple one. Considering the emerging spectral super-resolution (SSR) technique,which spectrally upsamples multispectral data to hyperspectral data, we aim tosolve the challenging HyDHZ problem by reformulating it as an SSR problem.Roughly speaking, the proposed algorithm first automatically selects someuncorrupted/informative spectral bands, from which SSR is applied to spectrallyupsample the selected bands in the feature space, thereby obtaining a cleanhyperspectral image (HSI). The clean HSI is then further refined by a deeptransformer network to obtain the final dehazed HSI, where a global attentionmechanism is designed to capture nonlocal information. There are very few HyDHZworks in existing literature, and this article introduces the powerfulspatial-spectral transformer into HyDHZ for the first time. Remarkably, theproposed transformer-driven IPT-based HyDHZ (T2HyDHZ) is a blind algorithmwithout requiring the user to manually select the corrupted region. Extensiveexperiments demonstrate the superiority of T2HyDHZ with less color distortion.</description><author>Po-Wei Tang, Chia-Hsiang Lin, Yangrui Liu</author><pubDate>Fri, 03 Jan 2025 17:52:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01924v1</guid></item><item><title>Quantum Channel Learning</title><link>http://arxiv.org/abs/2407.04406v2</link><description>The problem of an optimal mapping between Hilbert spaces $IN$ and $OUT$,based on a series of density matrix mapping measurements $\rho^{(l)} \to\varrho^{(l)}$, $l=1\dots M$, is formulated as an optimization problemmaximizing the total fidelity $\mathcal{F}=\sum_{l=1}^{M} \omega^{(l)}F\left(\varrho^{(l)},\sum_s B_s \rho^{(l)} B^{\dagger}_s\right)$ subject toprobability preservation constraints on Kraus operators $B_s$. For$F(\varrho,\sigma)$ in the form that total fidelity can be represented as aquadratic form with superoperator $\mathcal{F}=\sum_s\left\langleB_s\middle|S\middle| B_s \right\rangle$ (either exactly or as an approximation)an iterative algorithm is developed. The work introduces two importantgeneralizations of unitary learning: 1. $IN$/$OUT$ states are represented asdensity matrices. 2. The mapping itself is formulated as a mixed unitaryquantum channel $A^{OUT}=\sum_s |w_s|^2 \mathcal{U}_s A^{IN}\mathcal{U}_s^{\dagger}$ (no general quantum channel yet). This marks a crucialadvancement from the commonly studied unitary mapping of pure states$\phi_l=\mathcal{U} \psi_l$ to a quantum channel, what allows us to distinguishprobabilistic mixture of states and their superposition. An application of theapproach is demonstrated on unitary learning of density matrix mapping$\varrho^{(l)}=\mathcal{U} \rho^{(l)} \mathcal{U}^{\dagger}$, in this case aquadratic on $\mathcal{U}$ fidelity can be constructed by considering$\sqrt{\rho^{(l)}} \to \sqrt{\varrho^{(l)}}$ mapping, and on a quantum channel,where quadratic on $B_s$ fidelity is an approximation -- a quantum channel isthen obtained as a hierarchy of unitary mappings, a mixed unitary channel. Theapproach can be applied to studying quantum inverse problems, variationalquantum algorithms, quantum tomography, and more.</description><author>Mikhail Gennadievich Belov, Victor Victorovich Dubov, Alexey Vladimirovich Filimonov, Vladislav Gennadievich Malyshkin</author><pubDate>Fri, 03 Jan 2025 17:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04406v2</guid></item><item><title>DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring</title><link>http://arxiv.org/abs/2411.07976v6</link><description>Coronary artery disease (CAD), one of the leading causes of mortalityworldwide, necessitates effective risk assessment strategies, with coronaryartery calcium (CAC) scoring via computed tomography (CT) being a key methodfor prevention. Traditional methods, primarily based on UNET architecturesimplemented on pre-built models, face challenges like the scarcity of annotatedCT scans containing CAC and imbalanced datasets, leading to reduced performancein segmentation and scoring tasks. In this study, we address these limitationsby incorporating the self-supervised learning (SSL) technique of DINO(self-distillation with no labels), which trains without requiring CAC-specificannotations, enhancing its robustness in generating distinct features. TheDINO-LG model, which leverages label guidance to focus on calcified areas,achieves significant improvements, with a sensitivity of 89% and specificity of90% for detecting CAC-containing CT slices, compared to the standard DINOmodel's sensitivity of 79% and specificity of 77%. Additionally, false-negativeand false-positive rates are reduced by 49% and 59%, respectively, instillinggreater confidence in clinicians when ruling out calcification in low-riskpatients and minimizing unnecessary imaging reviews by radiologists. Further,CAC scoring and segmentation tasks are conducted using a basic UNETarchitecture, applied specifically to CT slices identified by the DINO-LG modelas containing calcified areas. This targeted approach enhances CAC scoringaccuracy by feeding the UNET model with relevant slices, significantlyimproving diagnostic precision, reducing both false positives and falsenegatives, and ultimately lowering overall healthcare costs by minimizingunnecessary tests and treatments, presenting a valuable advancement in CAD riskassessment.</description><author>Mahmut S. Gokmen, Caner Ozcan, Moneera N. Haque, Steve W. Leung, C. Seth Parker, W. Brent Seales, Cody Bumgardner</author><pubDate>Fri, 03 Jan 2025 17:40:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07976v6</guid></item><item><title>Social Processes: Probabilistic Meta-learning for Adaptive Multiparty Interaction Forecasting</title><link>http://arxiv.org/abs/2501.01915v1</link><description>Adaptively forecasting human behavior in social settings is an important steptoward achieving Artificial General Intelligence. Most existing research insocial forecasting has focused either on unfocused interactions, such aspedestrian trajectory prediction, or on monadic and dyadic behaviorforecasting. In contrast, social psychology emphasizes the importance of groupinteractions for understanding complex social dynamics. This creates a gap thatwe address in this paper: forecasting social interactions at the group(conversation) level. Additionally, it is important for a forecasting model tobe able to adapt to groups unseen at train time, as even the same individualbehaves differently across different groups. This highlights the need for aforecasting model to explicitly account for each group's unique dynamics. Toachieve this, we adopt a meta-learning approach to human behavior forecasting,treating every group as a separate meta-learning task. As a result, our methodconditions its predictions on the specific behaviors within the group, leadingto generalization to unseen groups. Specifically, we introduce Social Process(SP) models, which predict a distribution over future multimodal cues jointlyfor all group members based on their preceding low-level multimodal cues, whileincorporating other past sequences of the same group's interactions. In thiswork we also analyze the generalization capabilities of SP models in both theiroutputs and latent spaces through the use of realistic synthetic datasets.</description><author>Augustinas Jučas, Chirag Raman</author><pubDate>Fri, 03 Jan 2025 17:34:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01915v1</guid></item><item><title>Mingling with the Good to Backdoor Federated Learning</title><link>http://arxiv.org/abs/2501.01913v1</link><description>Federated learning (FL) is a decentralized machine learning technique thatallows multiple entities to jointly train a model while preserving datasetprivacy. However, its distributed nature has raised various security concerns,which have been addressed by increasingly sophisticated defenses. Theseprotections utilize a range of data sources and metrics to, for example, filterout malicious model updates, ensuring that the impact of attacks is minimizedor eliminated. This paper explores the feasibility of designing a generic attack methodcapable of installing backdoors in FL while evading a diverse array ofdefenses. Specifically, we focus on an attacker strategy called MIGO, whichaims to produce model updates that subtly blend with legitimate ones. Theresulting effect is a gradual integration of a backdoor into the global model,often ensuring its persistence long after the attack concludes, whilegenerating enough ambiguity to hinder the effectiveness of defenses. MIGO was employed to implant three types of backdoors across five datasetsand different model architectures. The results demonstrate the significantthreat posed by these backdoors, as MIGO consistently achieved exceptionallyhigh backdoor accuracy (exceeding 90%) while maintaining the utility of themain task. Moreover, MIGO exhibited strong evasion capabilities against tendefenses, including several state-of-the-art methods. When compared to fourother attack strategies, MIGO consistently outperformed them across mostconfigurations. Notably, even in extreme scenarios where the attacker controlsjust 0.1% of the clients, the results indicate that successful backdoorinsertion is possible if the attacker can persist for a sufficient number ofrounds.</description><author>Nuno Neves</author><pubDate>Fri, 03 Jan 2025 17:30:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01913v1</guid></item><item><title>Exoplanet Detection via Differentiable Rendering</title><link>http://arxiv.org/abs/2501.01912v1</link><description>Direct imaging of exoplanets is crucial for advancing our understanding ofplanetary systems beyond our solar system, but it faces significant challengesdue to the high contrast between host stars and their planets. Wavefrontaberrations introduce speckles in the telescope science images, which arepatterns of diffracted starlight that can mimic the appearance of planets,complicating the detection of faint exoplanet signals. Traditionalpost-processing methods, operating primarily in the image intensity domain, donot integrate wavefront sensing data. These data, measured mainly for adaptiveoptics corrections, have been overlooked as a potential resource forpost-processing, partly due to the challenge of the evolving nature ofwavefront aberrations. In this paper, we present a differentiable renderingapproach that leverages these wavefront sensing data to improve exoplanetdetection. Our differentiable renderer models wave-based light propagationthrough a coronagraphic telescope system, allowing gradient-based optimizationto significantly improve starlight subtraction and increase sensitivity tofaint exoplanets. Simulation experiments based on the James Webb SpaceTelescope configuration demonstrate the effectiveness of our approach,achieving substantial improvements in contrast and planet detection limits. Ourresults showcase how the computational advancements enabled by differentiablerendering can revitalize previously underexploited wavefront data, opening newavenues for enhancing exoplanet imaging and characterization.</description><author>Brandon Y. Feng, Rodrigo Ferrer-Chávez, Aviad Levis, Jason J. Wang, Katherine L. Bouman, William T. Freeman</author><pubDate>Fri, 03 Jan 2025 17:30:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01912v1</guid></item><item><title>Conditional Consistency Guided Image Translation and Enhancement</title><link>http://arxiv.org/abs/2501.01223v2</link><description>Consistency models have emerged as a promising alternative to diffusionmodels, offering high-quality generative capabilities through single-stepsample generation. However, their application to multi-domain image translationtasks, such as cross-modal translation and low-light image enhancement remainslargely unexplored. In this paper, we introduce Conditional Consistency Models(CCMs) for multi-domain image translation by incorporating additionalconditional inputs. We implement these modifications by introducingtask-specific conditional inputs that guide the denoising process, ensuringthat the generated outputs retain structural and contextual information fromthe corresponding input domain. We evaluate CCMs on 10 different datasetsdemonstrating their effectiveness in producing high-quality translated imagesacross multiple domains. Code is available athttps://github.com/amilbhagat/Conditional-Consistency-Models.</description><author>Amil Bhagat, Milind Jain, A. V. Subramanyam</author><pubDate>Fri, 03 Jan 2025 17:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01223v2</guid></item><item><title>Detecting and Mitigating Adversarial Attacks on Deep Learning-Based MRI Reconstruction Without Any Retraining</title><link>http://arxiv.org/abs/2501.01908v1</link><description>Deep learning (DL) methods, especially those based on physics-driven DL, havebecome the state-of-the-art for reconstructing sub-sampled magnetic resonanceimaging (MRI) data. However, studies have shown that these methods aresusceptible to small adversarial input perturbations, or attacks, resulting inmajor distortions in the output images. Various strategies have been proposedto reduce the effects of these attacks, but they require retraining and maylower reconstruction quality for non-perturbed/clean inputs. In this work, wepropose a novel approach for detecting and mitigating adversarial attacks onMRI reconstruction models without any retraining. Our detection strategy isbased on the idea of cyclic measurement consistency. The output of the model ismapped to another set of MRI measurements for a different sub-sampling pattern,and this synthesized data is reconstructed with the same model. Intuitively,without an attack, the second reconstruction is expected to be consistent withthe first, while with an attack, disruptions are present. Subsequently, thisidea is extended to devise a novel objective function, which is minimizedwithin a small ball around the attack input for mitigation. Experimentalresults show that our method substantially reduces the impact of adversarialperturbations across different datasets, attack types/strengths and PD-DLnetworks, and qualitatively and quantitatively outperforms conventionalmitigation methods that involve retraining.</description><author>Mahdi Saberi, Chi Zhang, Mehmet Akcakaya</author><pubDate>Fri, 03 Jan 2025 17:23:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01908v1</guid></item><item><title>Alleviating Overfitting in Transformation-Interaction-Rational Symbolic Regression with Multi-Objective Optimization</title><link>http://arxiv.org/abs/2501.01905v1</link><description>The Transformation-Interaction-Rational is a representation for symbolicregression that limits the search space of functions to the ratio of twononlinear functions each one defined as the linear regression of transformedvariables. This representation has the main objective to bias the searchtowards simpler expressions while keeping the approximation power of standardapproaches. The performance of using Genetic Programming with this representation wassubstantially better than with its predecessor (Interaction-Transformation) andranked close to the state-of-the-art on a contemporary Symbolic Regressionbenchmark. On a closer look at these results, we observed that the performancecould be further improved with an additional selective pressure for smallerexpressions when the dataset contains just a few data points. The introductionof a penalization term applied to the fitness measure improved the results onthese smaller datasets. One problem with this approach is that it introducestwo additional hyperparameters: i) a criteria to when the penalization shouldbe activated and, ii) the amount of penalization to the fitness function. In this paper, we extend Transformation-Interaction-Rational to supportmulti-objective optimization, specifically the NSGA-II algorithm, and applythat to the same benchmark. A detailed analysis of the results show that theuse of multi-objective optimization benefits the overall performance on asubset of the benchmarks while keeping the results similar to thesingle-objective approach on the remainder of the datasets. Specifically to thesmall datasets, we observe a small (and statistically insignificant)improvement of the results suggesting that further strategies must be explored.</description><author>Fabricio Olivetti de Franca</author><pubDate>Fri, 03 Jan 2025 17:21:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01905v1</guid></item><item><title>Virgo: A Preliminary Exploration on Reproducing o1-like MLLM</title><link>http://arxiv.org/abs/2501.01904v1</link><description>Recently, slow-thinking reasoning systems, built upon large language models(LLMs), have garnered widespread attention by scaling the thinking time duringinference. There is also growing interest in adapting this capability tomultimodal large language models (MLLMs). Given that MLLMs handle more complexdata semantics across different modalities, it is intuitively more challengingto implement multimodal slow-thinking systems. To address this issue, in this paper, we explore a straightforward approachby fine-tuning a capable MLLM with a small amount of textual long-form thoughtdata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoningwith long thought). We find that these long-form reasoning processes, expressedin natural language, can be effectively transferred to MLLMs. Moreover, itseems that such textual reasoning data can be even more effective than visualreasoning data in eliciting the slow-thinking capacities of MLLMs. While thiswork is preliminary, it demonstrates that slow-thinking capacities arefundamentally associated with the language model component, which can betransferred across modalities or domains. This finding can be leveraged toguide the development of more powerful slow-thinking reasoning systems. Werelease our resources at https://github.com/RUCAIBox/Virgo.</description><author>Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen</author><pubDate>Fri, 03 Jan 2025 17:14:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01904v1</guid></item><item><title>Can AI Help with Your Personal Finances?</title><link>http://arxiv.org/abs/2412.19784v2</link><description>In recent years, Large Language Models (LLMs) have emerged as atransformative development in artificial intelligence (AI), drawing significantattention from industry and academia. Trained on vast datasets, thesesophisticated AI systems exhibit impressive natural language processing andcontent generation capabilities. This paper explores the potential of LLMs toaddress key challenges in personal finance, focusing on the United States. Weevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,Anthropic's Claude, and Meta's Llama, to assess their effectiveness inproviding accurate financial advice on topics such as mortgages, taxes, loans,and investments. Our findings show that while these models achieve an averageaccuracy rate of approximately 70%, they also display notable limitations incertain areas. Specifically, LLMs struggle to provide accurate responses forcomplex financial queries, with performance varying significantly acrossdifferent topics. Despite these limitations, the analysis reveals notableimprovements in newer versions of these models, highlighting their growingutility for individuals and financial advisors. As these AI systems continue toevolve, their potential for advancing AI-driven applications in personalfinance becomes increasingly promising.</description><author>Oudom Hean, Utsha Saha, Binita Saha</author><pubDate>Fri, 03 Jan 2025 17:03:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19784v2</guid></item><item><title>EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation</title><link>http://arxiv.org/abs/2501.01895v1</link><description>We introduce EnerVerse, a comprehensive framework for embodied future spacegeneration specifically designed for robotic manipulation tasks. EnerVerseseamlessly integrates convolutional and bidirectional attention mechanisms forinner-chunk space modeling, ensuring low-level consistency and continuity.Recognizing the inherent redundancy in video data, we propose a sparse memorycontext combined with a chunkwise unidirectional generative paradigm to enablethe generation of infinitely long sequences. To further augment roboticcapabilities, we introduce the Free Anchor View (FAV) space, which providesflexible perspectives to enhance observation and analysis. The FAV spacemitigates motion modeling ambiguity, removes physical constraints in confinedenvironments, and significantly improves the robot's generalization andadaptability across various tasks and settings. To address the prohibitivecosts and labor intensity of acquiring multi-camera observations, we present adata engine pipeline that integrates a generative model with 4D GaussianSplatting (4DGS). This pipeline leverages the generative model's robustgeneralization capabilities and the spatial constraints provided by 4DGS,enabling an iterative enhancement of data quality and diversity, thus creatinga data flywheel effect that effectively narrows the sim-to-real gap. Finally,our experiments demonstrate that the embodied future space generation priorsubstantially enhances policy predictive capabilities, resulting in improvedoverall performance, particularly in long-range robotic manipulation tasks.</description><author>Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren</author><pubDate>Fri, 03 Jan 2025 17:00:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01895v1</guid></item><item><title>QuArch: A Question-Answering Dataset for AI Agents in Computer Architecture</title><link>http://arxiv.org/abs/2501.01892v1</link><description>We introduce QuArch, a dataset of 1500 human-validated question-answer pairsdesigned to evaluate and enhance language models' understanding of computerarchitecture. The dataset covers areas including processor design, memorysystems, and performance optimization. Our analysis highlights a significantperformance gap: the best closed-source model achieves 84% accuracy, while thetop small open-source model reaches 72%. We observe notable struggles in memorysystems, interconnection networks, and benchmarking. Fine-tuning with QuArchimproves small model accuracy by up to 8%, establishing a foundation foradvancing AI-driven computer architecture research. The dataset and leaderboardare at https://harvard-edge.github.io/QuArch/.</description><author>Shvetank Prakash, Andrew Cheng, Jason Yik, Arya Tschand, Radhika Ghosal, Ikechukwu Uchendu, Jessica Quaye, Jeffrey Ma, Shreyas Grampurohit, Sofia Giannuzzi, Arnav Balyan, Fin Amin, Aadya Pipersenia, Yash Choudhary, Ankita Nayak, Amir Yazdanbakhsh, Vijay Janapa Reddi</author><pubDate>Fri, 03 Jan 2025 16:55:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01892v1</guid></item><item><title>Learning Chemical Reaction Representation with Reactant-Product Alignment</title><link>http://arxiv.org/abs/2411.17629v2</link><description>Organic synthesis stands as a cornerstone of the chemical industry. Thedevelopment of robust machine learning models to support tasks associated withorganic reactions is of significant interest. However, current methods rely onhand-crafted features or direct adaptations of model architectures from otherdomains, which lack feasibility as data scales increase or ignore the richchemical information inherent in reactions. To address these issues, this paperintroduces RAlign, a novel chemical reaction representation learning model forvarious organic reaction-related tasks. By integrating atomic correspondencebetween reactants and products, our model discerns the moleculartransformations that occur during the reaction, thereby enhancing comprehensionof the reaction mechanism. We have designed an adapter structure to incorporatereaction conditions into the chemical reaction representation, allowing themodel to handle various reaction conditions and to adapt to various datasetsand downstream tasks. Additionally, we introduce a reaction-center-awareattention mechanism that enables the model to concentrate on key functionalgroups, thereby generating potent representations for chemical reactions. Ourmodel has been evaluated on a range of downstream tasks. Experimental resultsindicate that our model markedly outperforms existing chemical reactionrepresentation learning architectures on most of the datasets. We plan toopen-source the code contingent upon the acceptance of the paper.</description><author>Kaipeng Zeng, Xianbin Liu, Yu Zhang, Xiaokang Yang, Yaohui Jin, Yanyan Xu</author><pubDate>Fri, 03 Jan 2025 16:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17629v2</guid></item><item><title>Exploring Equality: An Investigation into Custom Loss Functions for Fairness Definitions</title><link>http://arxiv.org/abs/2501.01889v1</link><description>This paper explores the complex tradeoffs between various fairness metricssuch as equalized odds, disparate impact, and equal opportunity and predictiveaccuracy within COMPAS by building neural networks trained with custom lossfunctions optimized to specific fairness criteria. This paper creates the firstfairness-driven implementation of the novel Group Accuracy Parity (GAP)framework, as theoretically proposed by Gupta et al. (2024), and applies it toCOMPAS. To operationalize and accurately compare the fairness of COMPAS modelsoptimized to differing fairness ideals, this paper develops and proposes acombinatory analytical procedure that incorporates Pareto front andmultivariate analysis, leveraging data visualizations such as violin graphs.This paper concludes that GAP achieves an enhanced equilibrium between fairnessand accuracy compared to COMPAS's current nationwide implementation andalternative implementations of COMPAS optimized to more traditional fairnessdefinitions. While this paper's algorithmic improvements of COMPASsignificantly augment its fairness, external biases undermine the fairness ofits implementation. Practices such as predictive policing and issues such asthe lack of transparency regarding COMPAS's internal workings have contributedto the algorithm's historical injustice. In conjunction with developmentsregarding COMPAS's predictive methodology, legal and institutional changes musthappen for COMPAS's just deployment.</description><author>Gordon Lee, Simeon Sayer</author><pubDate>Fri, 03 Jan 2025 16:49:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01889v1</guid></item><item><title>Agent Planning with World Knowledge Model</title><link>http://arxiv.org/abs/2405.14205v4</link><description>Recent endeavors towards directly using large language models (LLMs) as agentmodels to execute interactive planning tasks have shown commendable results.Despite their achievements, however, they still struggle with brainlesstrial-and-error in global planning and generating hallucinatory actions inlocal planning due to their poor understanding of the ``real'' physical world.Imitating humans' mental world knowledge model which provides global priorknowledge before the task and maintains local dynamic knowledge during thetask, in this paper, we introduce parametric World Knowledge Model (WKM) tofacilitate agent planning. Concretely, we steer the agent model toself-synthesize knowledge from both expert and sampled trajectories. Then wedevelop WKM, providing prior task knowledge to guide the global planning anddynamic state knowledge to assist the local planning. Experimental results onthree complex real-world simulated datasets with three state-of-the-artopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that ourmethod can achieve superior performance compared to various strong baselines.Besides, we analyze to illustrate that our WKM can effectively alleviate theblind trial-and-error and hallucinatory action issues, providing strong supportfor the agent's understanding of the world. Other interesting findings include:1) our instance-level task knowledge can generalize better to unseen tasks, 2)weak WKM can guide strong agent model planning, and 3) unified WKM training haspromising potential for further development. The code is available athttps://github.com/zjunlp/WKM.</description><author>Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</author><pubDate>Fri, 03 Jan 2025 16:44:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14205v4</guid></item><item><title>Knowledge Circuits in Pretrained Transformers</title><link>http://arxiv.org/abs/2405.17969v4</link><description>The remarkable capabilities of modern large language models are rooted intheir vast repositories of knowledge encoded within their parameters, enablingthem to perceive the world and engage in reasoning. The inner workings of howthese models store knowledge have long been a subject of intense interest andinvestigation among researchers. To date, most studies have concentrated onisolated components within these models, such as the Multilayer Perceptrons andattention head. In this paper, we delve into the computation graph of thelanguage model to uncover the knowledge circuits that are instrumental inarticulating specific knowledge. The experiments, conducted with GPT2 andTinyLLAMA, have allowed us to observe how certain information heads, relationheads, and Multilayer Perceptrons collaboratively encode knowledge within themodel. Moreover, we evaluate the impact of current knowledge editing techniqueson these knowledge circuits, providing deeper insights into the functioning andconstraints of these editing methodologies. Finally, we utilize knowledgecircuits to analyze and interpret language model behaviors such ashallucinations and in-context learning. We believe the knowledge circuits holdpotential for advancing our understanding of Transformers and guiding theimproved design of knowledge editing. Code and data are available inhttps://github.com/zjunlp/KnowledgeCircuits.</description><author>Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen</author><pubDate>Fri, 03 Jan 2025 16:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17969v4</guid></item><item><title>Evaluating Scenario-based Decision-making for Interactive Autonomous Driving Using Rational Criteria: A Survey</title><link>http://arxiv.org/abs/2501.01886v1</link><description>Autonomous vehicles (AVs) can significantly promote the advances in roadtransport mobility in terms of safety, reliability, and decarbonization.However, ensuring safety and efficiency in interactive during within dynamicand diverse environments is still a primary barrier to large-scale AV adoption.In recent years, deep reinforcement learning (DRL) has emerged as an advancedAI-based approach, enabling AVs to learn decision-making strategies adaptivelyfrom data and interactions. DRL strategies are better suited than traditionalrule-based methods for handling complex, dynamic, and unpredictable drivingenvironments due to their adaptivity. However, varying driving scenariospresent distinct challenges, such as avoiding obstacles on highways andreaching specific exits at intersections, requiring different scenario-specificdecision-making algorithms. Many DRL algorithms have been proposed ininteractive decision-making. However, a rationale review of these DRLalgorithms across various scenarios is lacking. Therefore, a comprehensiveevaluation is essential to assess these algorithms from multiple perspectives,including those of vehicle users and vehicle manufacturers. This survey reviewsthe application of DRL algorithms in autonomous driving across typicalscenarios, summarizing road features and recent advancements. The scenariosinclude highways, on-ramp merging, roundabouts, and unsignalized intersections.Furthermore, DRL-based algorithms are evaluated based on five rationalecriteria: driving safety, driving efficiency, training efficiency,unselfishness, and interpretability (DDTUI). Each criterion of DDTUI isspecifically analyzed in relation to the reviewed algorithms. Finally, thechallenges for future DRL-based decision-making algorithms are summarized.</description><author>Zhen Tian, Zhihao Lin, Dezong Zhao, Wenjing Zhao, David Flynn, Shuja Ansari, Chongfeng Wei</author><pubDate>Fri, 03 Jan 2025 16:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01886v1</guid></item><item><title>CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings</title><link>http://arxiv.org/abs/2501.01257v2</link><description>With the increasing code reasoning capabilities of existing large languagemodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,there is a growing need to develop more challenging and comprehensivebenchmarks that effectively test their sophisticated competition-level codingabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due tothe unavailability of private test cases, lack of support for special judges,and misaligned execution environments. To bridge this gap, we introduceCodeElo, a standardized competition-level code generation benchmark thateffectively addresses all these challenges for the first time. CodeElobenchmark is mainly based on the official CodeForces platform and tries toalign with the platform as much as possible. We compile the recent six monthsof contest problems on CodeForces with detailed information such as contestdivisions, problem difficulty ratings, and problem algorithm tags. We introducea unique judging method in which problems are submitted directly to theplatform and develop a reliable Elo rating calculation system that aligns withthe platform and is comparable with human participants but has lower variance.By testing on our CodeElo, we provide the Elo ratings of 30 existing popularopen-source and 3 proprietary LLMs for the first time. The results show thato1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of1578 and 1261, respectively, while other models struggle even with the easiestproblems, placing in the lowest 25 percent among all human participants.Detailed analysis experiments are also conducted to provide insights intoperformance across algorithms and comparisons between using C++ and Python,which can suggest directions for future studies.</description><author>Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, Junyang Lin</author><pubDate>Fri, 03 Jan 2025 16:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01257v2</guid></item><item><title>FS-Net: Full Scale Network and Adaptive Threshold for Improving Extraction of Micro-Retinal Vessel Structures</title><link>http://arxiv.org/abs/2311.08059v4</link><description>Retinal vascular segmentation, a widely researched topic in biomedical imageprocessing, aims to reduce the workload of ophthalmologists in treating anddetecting retinal disorders. Segmenting retinal vessels presents uniquechallenges; previous techniques often failed to effectively segment branchesand microvascular structures. Recent neural network approaches struggle tobalance local and global properties and frequently miss tiny end vessels,hindering the achievement of desired results. To address these issues inretinal vessel segmentation, we propose a comprehensive micro-vessel extractionmechanism based on an encoder-decoder neural network architecture. This networkincludes residual, encoder booster, bottleneck enhancement, squeeze, andexcitation building blocks. These components synergistically enhance featureextraction and improve the prediction accuracy of the segmentation map. Oursolution has been evaluated using the DRIVE, CHASE-DB1, and STARE datasets,yielding competitive results compared to previous studies. The AUC and accuracyon the DRIVE dataset are 0.9884 and 0.9702, respectively. For the CHASE-DB1dataset, these scores are 0.9903 and 0.9755, respectively, and for the STAREdataset, they are 0.9916 and 0.9750. Given its accurate and robust performance,the proposed approach is a solid candidate for being implemented in real-lifediagnostic centers and aiding ophthalmologists.</description><author>Melaku N. Getahun, Oleg Y. Rogov, Dmitry V. Dylov, Andrey Somov, Ahmed Bouridane, Rifat Hamoudi</author><pubDate>Fri, 03 Jan 2025 16:30:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08059v4</guid></item><item><title>LMS-AutoTSF: Learnable Multi-Scale Decomposition and Integrated Autocorrelation for Time Series Forecasting</title><link>http://arxiv.org/abs/2412.06866v2</link><description>Time series forecasting is an important challenge with significantapplications in areas such as weather prediction, stock market analysis,scientific simulations and industrial process analysis. In this work, weintroduce LMS-AutoTSF, a novel time series forecasting architecture thatincorporates autocorrelation while leveraging dual encoders operating atmultiple scales. Unlike models that rely on predefined trend and seasonalcomponents, LMS-AutoTSF employs two separate encoders per scale: one focusingon low-pass filtering to capture trends and the other utilizing high-passfiltering to model seasonal variations. These filters are learnable, allowingthe model to dynamically adapt and isolate trend and seasonal componentsdirectly in the frequency domain. A key innovation in our approach is theintegration of autocorrelation, achieved by computing lagged differences intime steps, which enables the model to capture dependencies across time moreeffectively. Each encoder processes the input through fully connected layers tohandle temporal and channel interactions. By combining frequency-domainfiltering, autocorrelation-based temporal modeling, and channel-wisetransformations, LMS-AutoTSF not only accurately captures long-termdependencies and fine-grained patterns but also operates more efficientlycompared to other state-of-the-art methods. Its lightweight design ensuresfaster processing while maintaining high precision in forecasting acrossdiverse time horizons. The source code is publicly available at\url{http://github.com/mribrahim/LMS-TSF}</description><author>Ibrahim Delibasoglu, Sanjay Chakraborty, Fredrik Heintz</author><pubDate>Fri, 03 Jan 2025 16:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06866v2</guid></item><item><title>Simultaneous Latent State Estimation and Latent Linear Dynamics Discovery from Image Observations</title><link>http://arxiv.org/abs/2501.01339v2</link><description>The problem of state estimation has a long history with many successfulalgorithms that allow analytical derivation or approximation of posteriorfiltering distribution given the noisy observations. This report tries toconclude previous works to resolve the problem of latent state estimation givenimage-based observations and also suggests a new solution to this problem.</description><author>Nikita Kostin</author><pubDate>Fri, 03 Jan 2025 16:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01339v2</guid></item><item><title>NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals</title><link>http://arxiv.org/abs/2407.14561v3</link><description>We introduce NNsight and NDIF, technologies that work in tandem to enablescientific study of very large neural networks. NNsight is an open-sourcesystem that extends PyTorch to introduce deferred remote execution. NDIF is ascalable inference service that executes NNsight requests, allowing users toshare GPU resources and pretrained models. These technologies are enabled bythe intervention graph, an architecture developed to decouple experiment designfrom model runtime. Together, this framework provides transparent and efficientaccess to the internals of deep neural networks such as very large languagemodels (LLMs) without imposing the cost or complexity of hosting customizedmodels individually. We conduct a quantitative survey of the machine learningliterature that reveals a growing gap in the study of the internals oflarge-scale AI. We demonstrate the design and use of our framework to addressthis gap by enabling a range of research methods on huge models. Finally, weconduct benchmarks to compare performance with previous approaches. Codedocumentation, and materials are available at https://nnsight.net/.</description><author>Jaden Fiotto-Kaufman, Alexander R. Loftus, Eric Todd, Jannik Brinkmann, Koyena Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Nikhil Prakash, Carla Brodley, Arjun Guha, Jonathan Bell, Byron C. Wallace, David Bau</author><pubDate>Fri, 03 Jan 2025 16:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14561v3</guid></item><item><title>TabTreeFormer: Tabular Data Generation Using Hybrid Tree-Transformer</title><link>http://arxiv.org/abs/2501.01216v2</link><description>Transformers have achieved remarkable success in tabular data generation.However, they lack domain-specific inductive biases which are critical topreserving the intrinsic characteristics of tabular data. Meanwhile, theysuffer from poor scalability and efficiency due to quadratic computationalcomplexity. In this paper, we propose TabTreeFormer, a hybrid transformerarchitecture that incorporates a tree-based model that retains tabular-specificinductive biases of non-smooth and potentially low-correlated patterns due toits discreteness and non-rotational invariance, and hence enhances the fidelityand utility of synthetic data. In addition, we devise a dual-quantizationtokenizer to capture the multimodal continuous distribution and furtherfacilitate the learning of numerical value distribution. Moreover, our proposedtokenizer reduces the vocabulary size and sequence length due to the limiteddimension-wise semantic meaning and training set size of tabular data,rendering a significant model size shrink without sacrificing the capability ofthe transformer model. We evaluate TabTreeFormer on 10 datasets againstmultiple generative models on various metrics; our experimental results showthat TabTreeFormer achieves superior fidelity, utility, privacy, andefficiency. Our best model yields a 40% utility improvement with 1/16 of thebaseline model size.</description><author>Jiayu Li, Bingyin Zhao, Zilong Zhao, Kevin Yee, Uzair Javaid, Yingjie Lao, Biplab Sikdar</author><pubDate>Fri, 03 Jan 2025 15:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01216v2</guid></item><item><title>ANTHROPOS-V: benchmarking the novel task of Crowd Volume Estimation</title><link>http://arxiv.org/abs/2501.01877v1</link><description>We introduce the novel task of Crowd Volume Estimation (CVE), defined as theprocess of estimating the collective body volume of crowds using only RGBimages. Besides event management and public safety, CVE can be instrumental inapproximating body weight, unlocking weight sensitive applications such asinfrastructure stress assessment, and assuring even weight balance. We proposethe first benchmark for CVE, comprising ANTHROPOS-V, a synthetic photorealisticvideo dataset featuring crowds in diverse urban environments. Its annotationsinclude each person's volume, SMPL shape parameters, and keypoints. Also, weexplore metrics pertinent to CVE, define baseline models adapted from HumanMesh Recovery and Crowd Counting domains, and propose a CVE specificmethodology that surpasses baselines. Although synthetic, the weights andheights of individuals are aligned with the real-world population distributionacross genders, and they transfer to the downstream task of CVE from realimages. Benchmark and code are available atgithub.com/colloroneluca/Crowd-Volume-Estimation.</description><author>Luca Collorone, Stefano D'Arrigo, Massimiliano Pappa, Guido Maria D'Amely di Melendugno, Giovanni Ficarra, Fabio Galasso</author><pubDate>Fri, 03 Jan 2025 15:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01877v1</guid></item><item><title>DFF: Decision-Focused Fine-tuning for Smarter Predict-then-Optimize with Limited Data</title><link>http://arxiv.org/abs/2501.01874v1</link><description>Decision-focused learning (DFL) offers an end-to-end approach to thepredict-then-optimize (PO) framework by training predictive models directly ondecision loss (DL), enhancing decision-making performance within PO contexts.However, the implementation of DFL poses distinct challenges. Primarily, DL canresult in deviation from the physical significance of the predictions underlimited data. Additionally, some predictive models are non-differentiable orblack-box, which cannot be adjusted using gradient-based methods. To tackle theabove challenges, we propose a novel framework, Decision-Focused Fine-tuning(DFF), which embeds the DFL module into the PO pipeline via a novel biascorrection module. DFF is formulated as a constrained optimization problem thatmaintains the proximity of the DL-enhanced model to the original predictivemodel within a defined trust region. We theoretically prove that DFF strictlyconfines prediction bias within a predetermined upper bound, even with limiteddatasets, thereby substantially reducing prediction shifts caused by DL underlimited data. Furthermore, the bias correction module can be integrated intodiverse predictive models, enhancing adaptability to a broad range of PO tasks.Extensive evaluations on synthetic and real-world datasets, including networkflow, portfolio optimization, and resource allocation problems with differentpredictive models, demonstrate that DFF not only improves decision performancebut also adheres to fine-tuning constraints, showcasing robust adaptabilityacross various scenarios.</description><author>Jiaqi Yang, Enming Liang, Zicheng Su, Zhichao Zou, Peng Zhen, Jiecheng Guo, Wanjing Ma, Kun An</author><pubDate>Fri, 03 Jan 2025 15:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01874v1</guid></item><item><title>Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions</title><link>http://arxiv.org/abs/2501.01872v1</link><description>Despite significant efforts to align large language models with human valuesand ethical guidelines, these models remain susceptible to sophisticatedjailbreak attacks that exploit their reasoning capabilities. Traditional safetymechanisms often focus on detecting explicit malicious intent, leaving deepervulnerabilities unaddressed. In this work, we introduce a jailbreak technique,POATE (Polar Opposite query generation, Adversarial Template construction, andElaboration), which leverages contrastive reasoning to elicit unethicalresponses. POATE generates prompts with semantically opposite intents andcombines them with adversarial templates to subtly direct models towardproducing harmful responses. We conduct extensive evaluations across sixdiverse language model families of varying parameter sizes, including LLaMA3,Gemma2, Phi3, and GPT-4, to demonstrate the robustness of the attack, achievingsignificantly higher attack success rates (~44%) compared to existing methods.We evaluate our proposed attack against seven safety defenses, revealing theirlimitations in addressing reasoning-based vulnerabilities. To counteract this,we propose a defense strategy that improves reasoning robustness throughchain-of-thought prompting and reverse thinking, mitigating reasoning-drivenadversarial exploits.</description><author>Rachneet Sachdeva, Rima Hazra, Iryna Gurevych</author><pubDate>Fri, 03 Jan 2025 15:40:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01872v1</guid></item><item><title>PB-UAP: Hybrid Universal Adversarial Attack For Image Segmentation</title><link>http://arxiv.org/abs/2412.16651v2</link><description>With the rapid advancement of deep learning, the model robustness has becomea significant research hotspot, \ie, adversarial attacks on deep neuralnetworks. Existing works primarily focus on image classification tasks, aimingto alter the model's predicted labels. Due to the output complexity and deepernetwork architectures, research on adversarial examples for segmentation modelsis still limited, particularly for universal adversarial perturbations. In thispaper, we propose a novel universal adversarial attack method designed forsegmentation models, which includes dual feature separation and low-frequencyscattering modules. The two modules guide the training of adversarial examplesin the pixel and frequency space, respectively. Experiments demonstrate thatour method achieves high attack success rates surpassing the state-of-the-artmethods, and exhibits strong transferability across different models.</description><author>Yufei Song, Ziqi Zhou, Minghui Li, Xianlong Wang, Hangtao Zhang, Menghao Deng, Wei Wan, Shengshan Hu, Leo Yu Zhang</author><pubDate>Fri, 03 Jan 2025 15:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.16651v2</guid></item><item><title>Metric3Dv2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation</title><link>http://arxiv.org/abs/2404.15506v4</link><description>We introduce Metric3D v2, a geometric foundation model for zero-shot metricdepth and surface normal estimation from a single image, which is crucial formetric 3D recovery. While depth and normal are geometrically related and highlycomplimentary, they present distinct challenges. SoTA monocular depth methodsachieve zero-shot generalization by learning affine-invariant depths, whichcannot recover real-world metrics. Meanwhile, SoTA normal estimation methodshave limited zero-shot performance due to the lack of large-scale labeled data.To tackle these issues, we propose solutions for both metric depth estimationand surface normal estimation. For metric depth estimation, we show that thekey to a zero-shot single-view model lies in resolving the metric ambiguityfrom various camera models and large-scale data training. We propose acanonical camera space transformation module, which explicitly addresses theambiguity problem and can be effortlessly plugged into existing monocularmodels. For surface normal estimation, we propose a joint depth-normaloptimization module to distill diverse data knowledge from metric depth,enabling normal estimators to learn beyond normal labels. Equipped with thesemodules, our depth-normal models can be stably trained with over 16 million ofimages from thousands of camera models with different-type annotations,resulting in zero-shot generalization to in-the-wild images with unseen camerasettings. Our method enables the accurate recovery of metric 3D structures onrandomly collected internet images, paving the way for plausible single-imagemetrology. Our project page is at https://JUGGHM.github.io/Metric3Dv2.</description><author>Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Kaixuan Wang, Hao Chen, Gang Yu, Chunhua Shen, Shaojie Shen</author><pubDate>Fri, 03 Jan 2025 15:39:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15506v4</guid></item><item><title>Towards Hard and Soft Shadow Removal via Dual-Branch Separation Network and Vision Transformer</title><link>http://arxiv.org/abs/2501.01864v1</link><description>Image shadow removal is a crucial task in computer vision. In real-worldscenes, shadows alter image color and brightness, posing challenges forperception and texture recognition. Traditional and deep learning methods oftenoverlook the distinct needs for handling hard and soft shadows, thereby lackingdetailed processing to specifically address each type of shadow in images.Wepropose a dual-path model that processes these shadows separately usingspecially designed loss functions to accomplish the hard and soft shadowremoval. The model classifies shadow types and processes them throughappropriate paths to produce shadow-free outputs, integrating a VisionTransformer with UNet++ for enhanced edge detail and feature fusion. Our modeloutperforms state-of-the-art methods and achieves 2.905 RMSE value on the ISTDdataset, which demonstrates greater effectiveness than typical single-pathapproaches.</description><author>Jiajia Liang</author><pubDate>Fri, 03 Jan 2025 15:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01864v1</guid></item><item><title>ESG Rating Disagreement and Corporate Total Factor Productivity:Inference and Prediction</title><link>http://arxiv.org/abs/2408.13895v4</link><description>This paper examines how ESG rating disagreement (Dis) affects corporate totalfactor productivity (TFP) in China based on data of A-share listed companiesfrom 2015 to 2022. We find that Dis reduces TFP, especially in state-owned,non-capital-intensive, low-pollution and high-tech firms, green innovationstrengthens the dampening effect of Dis on TFP, and that Dis lowers corporateTFP by increasing financing constraints and weakening human capital.Furthermore, XGBoost regression demonstrates that Dis plays a significant rolein predicting TFP, with SHAP showing that the dampening effect of ESG ratingdisagreement on TFP is still pronounced in firms with large Dis values.</description><author>Zhanli Li, Zichao Yang</author><pubDate>Fri, 03 Jan 2025 15:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13895v4</guid></item><item><title>Coverage and Bias of Street View Imagery in Mapping the Urban Environment</title><link>http://arxiv.org/abs/2409.15386v2</link><description>Street View Imagery (SVI) has emerged as a valuable data form in urbanstudies, enabling new ways to map and sense urban environments. However,fundamental concerns regarding the representativeness, quality, and reliabilityof SVI remain underexplored, e.g. to what extent can cities be captured by suchdata and do data gaps result in bias. This research, positioned at theintersection of spatial data quality and urban analytics, addresses theseconcerns by proposing a novel and effective method to estimate SVI'selement-level coverage in the urban environment. The method integrates thepositional relationships between SVI and target elements, as well as the impactof physical obstructions. Expanding the domain of data quality to SVI, weintroduce an indicator system that evaluates the extent of coverage, focusingon the completeness and frequency dimensions. Taking London as a case study,three experiments are conducted to identify potential biases in SVI's abilityto cover and represent urban environmental elements, using building facades asan example. It is found that despite their high availability along urban roadnetworks, Google Street View covers only 62.4% of buildings in the case studyarea. The average facade coverage per building is 12.4%. SVI tends toover-represent non-residential buildings, thus possibly resulting in biasedanalyses, and its coverage of environmental elements is position-dependent. Theresearch also highlights the variability of SVI coverage under different dataacquisition practices and proposes an optimal sampling interval range of 50-60m for SVI collection. The findings suggest that while SVI offers valuableinsights, it is no panacea - its application in urban research requires carefulconsideration of data coverage and element-level representativeness to ensurereliable results.</description><author>Zicheng Fan, Chen-Chieh Feng, Filip Biljecki</author><pubDate>Fri, 03 Jan 2025 15:18:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.15386v2</guid></item><item><title>ViiNeuS: Volumetric Initialization for Implicit Neural Surface reconstruction of urban scenes with limited image overlap</title><link>http://arxiv.org/abs/2403.10344v4</link><description>Neural implicit surface representation methods have recently shown impressive3D reconstruction results. However, existing solutions struggle to reconstructdriving scenes due to their large size, highly complex nature and their limitedvisual observation overlap. Hence, to achieve accurate reconstructions,additional supervision data such as LiDAR, strong geometric priors, and longtraining times are required. To tackle such limitations, we present ViiNeuS, anew hybrid implicit surface learning method that efficiently initializes thesigned distance field to reconstruct large driving scenes from 2D street viewimages. ViiNeuS's hybrid architecture models two separate implicit fields: onerepresenting the volumetric density of the scene, and another one representingthe signed distance to the surface. To accurately reconstruct urban outdoordriving scenarios, we introduce a novel volume-rendering strategy that relieson self-supervised probabilistic density estimation to sample points near thesurface and transition progressively from volumetric to surface representation.Our solution permits a proper and fast initialization of the signed distancefield without relying on any geometric prior on the scene, compared toconcurrent methods. By conducting extensive experiments on four outdoor drivingdatasets, we show that ViiNeuS can learn an accurate and detailed 3D surfacerepresentation of various urban scene while being two times faster to traincompared to previous state-of-the-art solutions.</description><author>Hala Djeghim, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou, Désiré Sidibé</author><pubDate>Fri, 03 Jan 2025 15:18:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10344v4</guid></item><item><title>CUQDS: Conformal Uncertainty Quantification under Distribution Shift for Trajectory Prediction</title><link>http://arxiv.org/abs/2406.12100v3</link><description>Trajectory prediction models that can infer both finite future trajectoriesand their associated uncertainties of the target vehicles in an online setting(e.g., real-world application scenarios) is crucial for ensuring the safe androbust navigation and path planning of autonomous vehicle motion. However, themajority of existing trajectory prediction models have neither consideredreducing the uncertainty as one objective during the training stage norprovided reliable uncertainty quantification during inference stage underpotential distribution shift. Therefore, in this paper, we propose theConformal Uncertainty Quantification under Distribution Shift framework, CUQDS,to quantify the uncertainty of the predicted trajectories of existingtrajectory prediction models under potential data distribution shift, whileconsidering improving the prediction accuracy of the models and reducing theestimated uncertainty during the training stage. Specifically, CUQDS includes1) a learning-based Gaussian process regression module that models the outputdistribution of the base model (any existing trajectory prediction or timeseries forecasting neural networks) and reduces the estimated uncertainty byadditional loss term, and 2) a statistical-based Conformal P control module tocalibrate the estimated uncertainty from the Gaussian process regression modulein an online setting under potential distribution shift between training andtesting data.</description><author>Huiqun Huang, Sihong He, Fei Miao</author><pubDate>Fri, 03 Jan 2025 15:17:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12100v3</guid></item><item><title>UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial Vehicle Imagery</title><link>http://arxiv.org/abs/2501.01855v1</link><description>Unmanned aerial vehicle object detection (UAV-OD) has been widely used invarious scenarios. However, most existing UAV-OD algorithms rely on manuallydesigned components, which require extensive tuning. End-to-end models that donot depend on such manually designed components are mainly designed for naturalimages, which are less effective for UAV imagery. To address such challenges,this paper proposes an efficient detection transformer (DETR) frameworktailored for UAV imagery, i.e., UAV-DETR. The framework includes a multi-scalefeature fusion with frequency enhancement module, which captures both spatialand frequency information at different scales. In addition, a frequency-focuseddown-sampling module is presented to retain critical spatial details duringdown-sampling. A semantic alignment and calibration module is developed toalign and fuse features from different fusion paths. Experimental resultsdemonstrate the effectiveness and generalization of our approach across variousUAV imagery datasets. On the VisDrone dataset, our method improves AP by 3.1\%and $\text{AP}_{50}$ by 4.2\% over the baseline. Similar enhancements areobserved on the UAVVaste dataset. The project page:https://github.com/ValiantDiligent/UAV-DETR</description><author>Huaxiang Zhang, Kai Liu, Zhongxue Gan, Guo-Niu Zhu</author><pubDate>Fri, 03 Jan 2025 15:11:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01855v1</guid></item><item><title>Generic Objects as Pose Probes for Few-shot View Synthesis</title><link>http://arxiv.org/abs/2408.16690v3</link><description>Radiance fields including NeRFs and 3D Gaussians demonstrate great potentialin high-fidelity rendering and scene reconstruction, while they require asubstantial number of posed images as inputs. COLMAP is frequently employed forpreprocessing to estimate poses, while it necessitates a large number offeature matches to operate effectively, and it struggles with scenescharacterized by sparse features, large baselines between images, or a limitednumber of input images. We aim to tackle few-view NeRF reconstruction usingonly 3 to 6 unposed scene images. Traditional methods often use calibrationboards but they are not common in images. We propose a novel idea of utilizingeveryday objects, commonly found in both images and real life, as "poseprobes". The probe object is automatically segmented by SAM, whose shape isinitialized from a cube. We apply a dual-branch volume rendering optimization(object NeRF and scene NeRF) to constrain the pose optimization and jointlyrefine the geometry. Specifically, object poses of two views are firstestimated by PnP matching in an SDF representation, which serves as initialposes. PnP matching, requiring only a few features, is suitable forfeature-sparse scenes. Additional views are incrementally incorporated torefine poses from preceding views. In experiments, PoseProbe achievesstate-of-the-art performance in both pose estimation and novel view synthesisacross multiple datasets. We demonstrate its effectiveness, particularly infew-view and large-baseline scenes where COLMAP struggles. In ablations, usingdifferent objects in a scene yields comparable performance. Our project page isavailable at: \href{https://zhirui-gao.github.io/PoseProbe.github.io/}{thishttps URL}</description><author>Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu</author><pubDate>Fri, 03 Jan 2025 15:05:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16690v3</guid></item><item><title>LCFed: An Efficient Clustered Federated Learning Framework for Heterogeneous Data</title><link>http://arxiv.org/abs/2501.01850v1</link><description>Clustered federated learning (CFL) addresses the performance challenges posedby data heterogeneity in federated learning (FL) by organizing edge deviceswith similar data distributions into clusters, enabling collaborative modeltraining tailored to each group. However, existing CFL approaches strictlylimit knowledge sharing to within clusters, lacking the integration of globalknowledge with intra-cluster training, which leads to suboptimal performance.Moreover, traditional clustering methods incur significant computationaloverhead, especially as the number of edge devices increases. In this paper, wepropose LCFed, an efficient CFL framework to combat these challenges. Byleveraging model partitioning and adopting distinct aggregation strategies foreach sub-model, LCFed effectively incorporates global knowledge intointra-cluster co-training, achieving optimal training performance.Additionally, LCFed customizes a computationally efficient model similaritymeasurement method based on low-rank models, enabling real-time cluster updateswith minimal computational overhead. Extensive experiments show that LCFedoutperforms state-of-the-art benchmarks in both test accuracy and clusteringcomputational efficiency.</description><author>Yuxin Zhang, Haoyu Chen, Zheng Lin, Zhe Chen, Jin Zhao</author><pubDate>Fri, 03 Jan 2025 14:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01850v1</guid></item><item><title>Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification</title><link>http://arxiv.org/abs/2501.01849v1</link><description>The remarkable generative capability of large language models (LLMs) hassparked a growing interest in automatically generating responses for differentapplications. Given the dynamic nature of user preferences and the uncertaintyof LLM response performance, it is crucial to design efficient online learningalgorithms to identify optimal LLM responses (i.e., high-quality responses thatalso meet user preferences). Most existing online algorithms adopt acentralized approach and fail to leverage explicit user preferences for moreefficient and personalized LLM response identification. In contrast, this paperintroduces \textit{MACO} (\underline{M}ulti-\underline{A}gent\underline{C}onversational \underline{O}nline Learning for Adaptive LLMResponse Identification): 1) The online LLM response identification process isaccelerated by multiple local agents (such as smartphones), while enhancingdata privacy; 2) A novel conversational mechanism is proposed to adaptivelyconduct conversations for soliciting user preferences (e.g., a preference for ahumorous tone over a serious one in generated responses), so to minimizeuncertainty in preference estimation. Our theoretical analysis demonstratesthat \cadi\ is near-optimal regarding cumulative regret. Additionally, \cadi\offers reduced communication costs and computational complexity by eliminatingthe traditional, computing-intensive ``G-optimal design" found in previousworks. Extensive experiments with the open LLM \textit{Llama}, coupled with twodifferent embedding models from Google and OpenAI for text vectorrepresentation, demonstrate that \cadi\ significantly outperforms the currentstate-of-the-art in online LLM response identification.</description><author>Xiangxiang Dai, Yuejin Xie, Maoli Liu, Xuchuang Wang, Zhuohua Li, Huanyu Wang, John C. S. Lui</author><pubDate>Fri, 03 Jan 2025 14:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01849v1</guid></item><item><title>TAB: Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models</title><link>http://arxiv.org/abs/2412.18675v2</link><description>Multi-head self-attention (MHSA) is a key component of Transformers, a widelypopular architecture in both language and vision. Multiple heads intuitivelyenable different parallel processes over the same input. Yet, they also obscurethe attribution of each input patch to the output of a model. We propose anovel 1-head Transformer Attention Bottleneck (TAB) layer, inserted after thetraditional MHSA architecture, to serve as an attention bottleneck forinterpretability and intervention. Unlike standard self-attention, TABconstrains the total attention over all patches to $\in [0, 1]$. That is, whenthe total attention is 0, no visual information is propagated further into thenetwork and the vision-language model (VLM) would default to a generic,image-independent response. To demonstrate the advantages of TAB, we train VLMswith TAB to perform image difference captioning. Over three datasets, ourmodels perform similarly to baseline VLMs in captioning but the bottleneck issuperior in localizing changes and in identifying when no changes occur. TAB isthe first architecture to enable users to intervene by editing attention, whichoften produces expected outputs by VLMs.</description><author>Pooyan Rahmanzadehgervi, Hung Huy Nguyen, Rosanne Liu, Long Mai, Anh Totti Nguyen</author><pubDate>Fri, 03 Jan 2025 14:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18675v2</guid></item><item><title>Semantic Segmentation for Sequential Historical Maps by Learning from Only One Map</title><link>http://arxiv.org/abs/2501.01845v1</link><description>Historical maps are valuable resources that capture detailed geographicalinformation from the past. However, these maps are typically available inprinted formats, which are not conducive to modern computer-based analyses.Digitizing these maps into a machine-readable format enables efficientcomputational analysis. In this paper, we propose an automated approach todigitization using deep-learning-based semantic segmentation, which assigns asemantic label to each pixel in scanned historical maps. A key challenge inthis process is the lack of ground-truth annotations required for training deepneural networks, as manual labeling is time-consuming and labor-intensive. Toaddress this issue, we introduce a weakly-supervised age-tracing strategy formodel fine-tuning. This approach exploits the similarity in appearance andland-use patterns between historical maps from neighboring time periods toguide the training process. Specifically, model predictions for one map areutilized as pseudo-labels for training on maps from adjacent time periods.Experiments conducted on our newly curated \textit{Hameln} dataset demonstratethat the proposed age-tracing strategy significantly enhances segmentationperformance compared to baseline models. In the best-case scenario, the meanIntersection over Union (mIoU) achieved 77.3\%, reflecting an improvement ofapproximately 20\% over baseline methods. Additionally, the fine-tuned modelachieved an average overall accuracy of 97\%, highlighting the effectiveness ofour approach for digitizing historical maps.</description><author>Yunshuang Yuan, Frank Thiemann, Monika Sester</author><pubDate>Fri, 03 Jan 2025 14:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01845v1</guid></item><item><title>Learning from Ambiguous Data with Hard Labels</title><link>http://arxiv.org/abs/2501.01844v1</link><description>Real-world data often contains intrinsic ambiguity that the commonsingle-hard-label annotation paradigm ignores. Standard training usingambiguous data with these hard labels may produce overly confident models andthus leading to poor generalization. In this paper, we propose a novelframework called Quantized Label Learning (QLL) to alleviate this issue. First,we formulate QLL as learning from (very) ambiguous data with hard labels:ideally, each ambiguous instance should be associated with a ground-truthsoft-label distribution describing its corresponding probabilistic weight ineach class, however, this is usually not accessible; in practice, we can onlyobserve a quantized label, i.e., a hard label sampled (quantized) from thecorresponding ground-truth soft-label distribution, of each instance, which canbe seen as a biased approximation of the ground-truth soft-label. Second, wepropose a Class-wise Positive-Unlabeled (CPU) risk estimator that allows us totrain accurate classifiers from only ambiguous data with quantized labels.Third, to simulate ambiguous datasets with quantized labels in the real world,we design a mixing-based ambiguous data generation procedure for empiricalevaluation. Experiments demonstrate that our CPU method can significantlyimprove model generalization performance and outperform the baselines.</description><author>Zeke Xie, Zheng He, Nan Lu, Lichen Bai, Bao Li, Shuo Yang, Mingming Sun, Ping Li</author><pubDate>Fri, 03 Jan 2025 14:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01844v1</guid></item><item><title>Dedicated Inference Engine and Binary-Weight Neural Networks for Lightweight Instance Segmentation</title><link>http://arxiv.org/abs/2501.01841v1</link><description>Reducing computational costs is an important issue for development ofembedded systems. Binary-weight Neural Networks (BNNs), in which weights arebinarized and activations are quantized, are employed to reduce computationalcosts of various kinds of applications. In this paper, a design methodology ofhardware architecture for inference engines is proposed to handle modern BNNswith two operation modes. Multiply-Accumulate (MAC) operations can besimplified by replacing multiply operations with bitwise operations. Theproposed method can effectively reduce the gate count of inference engines byremoving a part of computational costs from the hardware system. Thearchitecture of MAC operations can calculate the inference results of BNNsefficiently with only 52% of hardware costs compared with the related works. Toshow that the inference engine can handle practical applications, twolightweight networks which combine the backbones of SegNeXt and the decoder ofSparseInst for instance segmentation are also proposed. The output results ofthe lightweight networks are computed using only bitwise operations and addoperations. The proposed inference engine has lower hardware costs than relatedworks. The experimental results show that the proposed inference engine canhandle the proposed instance-segmentation networks and achieves higher accuracythan YOLACT on the "Person" category although the model size is 77.7$\times$smaller compared with YOLACT.</description><author>Tse-Wei Chen, Wei Tao, Dongyue Zhao, Kazuhiro Mima, Tadayuki Ito, Kinya Osa, Masami Kato</author><pubDate>Fri, 03 Jan 2025 14:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01841v1</guid></item><item><title>Signal Recovery Using a Spiked Mixture Model</title><link>http://arxiv.org/abs/2501.01840v1</link><description>We introduce the spiked mixture model (SMM) to address the problem ofestimating a set of signals from many randomly scaled and noisy observations.Subsequently, we design a novel expectation-maximization (EM) algorithm torecover all parameters of the SMM. Numerical experiments show that in lowsignal-to-noise ratio regimes, and for data types where the SMM is relevant,SMM surpasses the more traditional Gaussian mixture model (GMM) in terms ofsignal recovery performance. The broad relevance of the SMM and itscorresponding EM recovery algorithm is demonstrated by applying the techniqueto different data types. The first case study is a biomedical researchapplication, utilizing an imaging mass spectrometry dataset to explore themolecular content of a rat brain tissue section at micrometer scale. The secondcase study demonstrates SMM performance in a computer vision application,segmenting a hyperspectral imaging dataset into underlying patterns. While themeasurement modalities differ substantially, in both case studies SMM is shownto recover signals that were missed by traditional methods such as k-meansclustering and GMM.</description><author>Paul-Louis Delacour, Sander Wahls, Jeffrey M. Spraggins, Lukasz Migas, Raf Van de Plas</author><pubDate>Fri, 03 Jan 2025 14:43:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01840v1</guid></item><item><title>Practical machine learning is learning on small samples</title><link>http://arxiv.org/abs/2501.01836v1</link><description>Based on limited observations, machine learning discerns a dependence whichis expected to hold in the future. What makes it possible? Statistical learningtheory imagines indefinitely increasing training sample to justify itsapproach. In reality, there is no infinite time or even infinite generalpopulation for learning. Here I argue that practical machine learning is basedon an implicit assumption that underlying dependence is relatively ``smooth" :likely, there are no abrupt differences in feedback between cases with closedata points. From this point of view learning shall involve selection of thehypothesis ``smoothly" approximating the training set. I formalize this asPractical learning paradigm. The paradigm includes terminology and rules fordescription of learners. Popular learners (local smoothing, k-NN, decisiontrees, Naive Bayes, SVM for classification and for regression) are shown hereto be implementations of this paradigm.</description><author>Marina Sapir</author><pubDate>Fri, 03 Jan 2025 14:38:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01836v1</guid></item><item><title>ASKCOS: an open source software suite for synthesis planning</title><link>http://arxiv.org/abs/2501.01835v1</link><description>The advancement of machine learning and the availability of large-scalereaction datasets have accelerated the development of data-driven models forcomputer-aided synthesis planning (CASP) in the past decade. Here, we detailthe newest version of ASKCOS, an open source software suite for synthesisplanning that makes available several research advances in a freely available,practical tool. Four one-step retrosynthesis models form the basis of bothinteractive planning and automatic planning modes. Retrosynthetic planning iscomplemented by other modules for feasibility assessment and pathwayevaluation, including reaction condition recommendation, reaction outcomeprediction, and auxiliary capabilities such as solubility prediction andquantum mechanical descriptor prediction. ASKCOS has assisted hundreds ofmedicinal, synthetic, and process chemists in their day-to-day tasks,complementing expert decision making. It is our belief that CASP tools likeASKCOS are an important part of modern chemistry research, and that they offerever-increasing utility and accessibility.</description><author>Zhengkai Tu, Sourabh J. Choure, Mun Hong Fong, Jihye Roh, Itai Levin, Kevin Yu, Joonyoung F. Joung, Nathan Morgan, Shih-Cheng Li, Xiaoqi Sun, Huiqian Lin, Mark Murnin, Jordan P. Liles, Thomas J. Struble, Michael E. Fortunato, Mengjie Liu, William H. Green, Klavs F. Jensen, Connor W. Coley</author><pubDate>Fri, 03 Jan 2025 14:38:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01835v1</guid></item><item><title>MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning</title><link>http://arxiv.org/abs/2501.01834v1</link><description>Image captioning is a critical task at the intersection of computer visionand natural language processing, with wide-ranging applications across variousdomains. For complex tasks such as diagnostic report generation, deep learningmodels require not only domain-specific image-caption datasets but also theincorporation of relevant general knowledge to provide contextual accuracy.Existing approaches exhibit inherent limitations: specialized models excel incapturing domain-specific details but lack generalization, whilevision-language models (VLMs) built on large language models (LLMs) leveragegeneral knowledge but struggle with domain-specific adaptation. To addressthese limitations, this paper proposes a novel agent-enhanced modelcollaboration framework, which we called \textbf{MoColl}, designed toeffectively integrate domain-specific and general knowledge. Specifically, ourapproach is to decompose complex image captioning tasks into a series ofinterconnected question-answer subtasks. A trainable visual question answering(VQA) model is employed as a specialized tool to focus on domain-specificvisual analysis, answering task-specific questions based on image content.Concurrently, an LLM-based agent with general knowledge formulates thesequestions and synthesizes the resulting question-answer pairs into coherentcaptions. Beyond its role in leveraging the VQA model, the agent further guidesits training to enhance its domain-specific capabilities. Experimental resultson radiology report generation validate the effectiveness of the proposedframework, demonstrating significant improvements in the quality of generatedreports.</description><author>Pu Yang, Bin Dong</author><pubDate>Fri, 03 Jan 2025 14:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01834v1</guid></item><item><title>Time Series Language Model for Descriptive Caption Generation</title><link>http://arxiv.org/abs/2501.01832v1</link><description>The automatic generation of representative natural language descriptions forobservable patterns in time series data enhances interpretability, simplifiesanalysis and increases cross-domain utility of temporal data. While pre-trainedfoundation models have made considerable progress in natural languageprocessing (NLP) and computer vision (CV), their application to time seriesanalysis has been hindered by data scarcity. Although several large languagemodel (LLM)-based methods have been proposed for time series forecasting, timeseries captioning is under-explored in the context of LLMs. In this paper, weintroduce TSLM, a novel time series language model designed specifically fortime series captioning. TSLM operates as an encoder-decoder model, leveragingboth text prompts and time series data representations to capture subtletemporal patterns across multiple phases and generate precise textualdescriptions of time series inputs. TSLM addresses the data scarcity problem intime series captioning by first leveraging an in-context prompting syntheticdata generation, and second denoising the generated data via a novelcross-modal dense retrieval scoring applied to time series-caption pairs.Experimental findings on various time series captioning datasets demonstratethat TSLM outperforms existing state-of-the-art approaches from multiple datamodalities by a significant margin.</description><author>Mohamed Trabelsi, Aidan Boyd, Jin Cao, Huseyin Uzunalioglu</author><pubDate>Fri, 03 Jan 2025 14:34:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01832v1</guid></item><item><title>Ano-SuPs: Multi-size anomaly detection for manufactured products by identifying suspected patches</title><link>http://arxiv.org/abs/2309.11120v2</link><description>Image-based systems have gained popularity owing to their capacity to providerich manufacturing status information, low implementation costs and highacquisition rates. However, the complexity of the image background and variousanomaly patterns pose new challenges to existing matrix decomposition methods,which are inadequate for modeling requirements. Moreover, the uncertainty ofthe anomaly can cause anomaly contamination problems, making the designed modeland method highly susceptible to external disturbances. To address thesechallenges, we propose a two-stage strategy anomaly detection method thatdetects anomalies by identifying suspected patches (Ano-SuPs). Specifically, wepropose to detect the patches with anomalies by reconstructing the input imagetwice: the first step is to obtain a set of normal patches by removing thosesuspected patches, and the second step is to use those normal patches to refinethe identification of the patches with anomalies. To demonstrate itseffectiveness, we evaluate the proposed method systematically throughsimulation experiments and case studies. We further identified the keyparameters and designed steps that impact the model's performance andefficiency.</description><author>Hao Xu, Juan Du, Andi Wang, YingCong Chen</author><pubDate>Fri, 03 Jan 2025 14:31:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11120v2</guid></item><item><title>Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models</title><link>http://arxiv.org/abs/2501.01830v1</link><description>Automated red-teaming has become a crucial approach for uncoveringvulnerabilities in large language models (LLMs). However, most existing methodsfocus on isolated safety flaws, limiting their ability to adapt to dynamicdefenses and uncover complex vulnerabilities efficiently. To address thischallenge, we propose Auto-RT, a reinforcement learning framework thatautomatically explores and optimizes complex attack strategies to effectivelyuncover security vulnerabilities through malicious queries. Specifically, weintroduce two key mechanisms to reduce exploration complexity and improvestrategy optimization: 1) Early-terminated Exploration, which accelerateexploration by focusing on high-potential attack strategies; and 2) ProgressiveReward Tracking algorithm with intermediate downgrade models, which dynamicallyrefine the search trajectory toward successful vulnerability exploitation.Extensive experiments across diverse LLMs demonstrate that, by significantlyimproving exploration efficiency and automatically optimizing attackstrategies, Auto-RT detects a boarder range of vulnerabilities, achieving afaster detection speed and 16.63\% higher success rates compared to existingmethods.</description><author>Yanjiang Liu, Shuhen Zhou, Yaojie Lu, Huijia Zhu, Weiqiang Wang, Hongyu Lin, Ben He, Xianpei Han, Le Sun</author><pubDate>Fri, 03 Jan 2025 14:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01830v1</guid></item><item><title>Age-Based Device Selection and Transmit Power Optimization in Over-the-Air Federated Learning</title><link>http://arxiv.org/abs/2501.01828v1</link><description>Recently, over-the-air federated learning (FL) has attracted significantattention for its ability to enhance communication efficiency. However, theperformance of over-the-air FL is often constrained by device selectionstrategies and signal aggregation errors. In particular, neglecting stragglerdevices in FL can lead to a decline in the fairness of model updates andamplify the global model's bias toward certain devices' data, ultimatelyimpacting the overall system performance. To address this issue, we propose ajoint device selection and transmit power optimization framework that ensuresthe appropriate participation of straggler devices, maintains efficienttraining performance, and guarantees timely updates. First, we conduct atheoretical analysis to quantify the convergence upper bound of over-the-air FLunder age-of-information (AoI)-based device selection. Our analysis furtherreveals that both the number of selected devices and the signal aggregationerrors significantly influence the convergence upper bound. To minimize theexpected weighted sum peak age of information, we calculate device prioritiesfor each communication round using Lyapunov optimization and select thehighest-priority devices via a greedy algorithm. Then, we formulate and solve atransmit power and normalizing factor optimization problem for selected devicesto minimize the time-average mean squared error (MSE). Experimental resultsdemonstrate that our proposed method offers two significant advantages: (1) itreduces MSE and improves model performance compared to baseline methods, and(2) it strikes a balance between fairness and training efficiency whilemaintaining satisfactory timeliness, ensuring stable model performance.</description><author>Jingyuan Liu, Zheng Chang, Ying-Chang Liang</author><pubDate>Fri, 03 Jan 2025 14:27:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01828v1</guid></item><item><title>The Proof is in the Almond Cookies</title><link>http://arxiv.org/abs/2501.01827v1</link><description>This paper presents a case study on how to process cooking recipes (and moregenerally, how-to instructions) in a way that makes it possible for a robot orartificial cooking assistant to support human chefs in the kitchen. Such AIassistants would be of great benefit to society, as they can help to sustainthe autonomy of aging adults or people with a physical impairment, or they mayreduce the stress in a professional kitchen. We propose a novel approach tocomputational recipe understanding that mimics the human sense-making process,which is narrative-based. Using an English recipe for almond crescent cookiesas illustration, we show how recipes can be modelled as rich narrativestructures by integrating various knowledge sources such as languageprocessing, ontologies, and mental simulation. We show how such narrativestructures can be used for (a) dealing with the challenges of recipe language,such as zero anaphora, (b) optimizing a robot's planning process, (c) measuringhow well an AI system understands its current tasks, and (d) allowing recipeannotations to become language-independent.</description><author>Remi van Trijp, Katrien Beuls, Paul Van Eecke</author><pubDate>Fri, 03 Jan 2025 14:25:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01827v1</guid></item><item><title>Can Transformers Do Enumerative Geometry?</title><link>http://arxiv.org/abs/2408.14915v2</link><description>How can Transformers model and learn enumerative geometry? What is a robustprocedure for using Transformers in abductive knowledge discovery within amathematician-machine collaboration? In this work, we introduce aTransformer-based approach to computational enumerative geometry, specificallytargeting the computation of $\psi$-class intersection numbers on the modulispace of curves. By reformulating the problem as a continuous optimizationtask, we compute intersection numbers across a wide value range from $10^{-45}$to $10^{45}$. To capture the recursive nature inherent in these intersectionnumbers, we propose the Dynamic Range Activator (DRA), a new activationfunction that enhances the Transformer's ability to model recursive patternsand handle severe heteroscedasticity. Given precision requirements forcomputing the intersections, we quantify the uncertainty of the predictionsusing Conformal Prediction with a dynamic sliding window adaptive to thepartitions of equivalent number of marked points. To the best of our knowledge,there has been no prior work on modeling recursive functions with such ahigh-variance and factorial growth. Beyond simply computing intersectionnumbers, we explore the enumerative "world-model" of Transformers. Ourinterpretability analysis reveals that the network is implicitly modeling theVirasoro constraints in a purely data-driven manner. Moreover, throughabductive hypothesis testing, probing, and causal inference, we uncoverevidence of an emergent internal representation of the the large-genusasymptotic of $\psi$-class intersection numbers. These findings suggest thatthe network internalizes the parameters of the asymptotic closed-form and thepolynomiality phenomenon of $\psi$-class intersection numbers in a non-linearmanner.</description><author>Baran Hashemi, Roderic G. Corominas, Alessandro Giacchetto</author><pubDate>Fri, 03 Jan 2025 14:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14915v2</guid></item><item><title>Large Language Models: An Applied Econometric Framework</title><link>http://arxiv.org/abs/2412.07031v2</link><description>How can we use the novel capacities of large language models (LLMs) inempirical research? And how can we do so while accounting for theirlimitations, which are themselves only poorly understood? We develop aneconometric framework to answer this question that distinguishes between twotypes of empirical tasks. Using LLMs for prediction problems (includinghypothesis generation) is valid under one condition: no ``leakage'' between theLLM's training dataset and the researcher's sample. No leakage can be ensuredby using open-source LLMs with documented training data and published weights.Using LLM outputs for estimation problems to automate the measurement of someeconomic concept (expressed either by some text or from human subjects)requires the researcher to collect at least some validation data: without suchdata, the errors of the LLM's automation cannot be assessed and accounted for.As long as these steps are taken, LLM outputs can be used in empirical researchwith the familiar econometric guarantees we desire. Using two illustrativeapplications to finance and political economy, we find that these requirementsare stringent; when they are violated, the limitations of LLMs now result inunreliable empirical estimates. Our results suggest the excitement around theempirical uses of LLMs is warranted -- they allow researchers to effectivelyuse even small amounts of language data for both prediction and estimation --but only with these safeguards in place.</description><author>Jens Ludwig, Sendhil Mullainathan, Ashesh Rambachan</author><pubDate>Fri, 03 Jan 2025 14:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07031v2</guid></item><item><title>Unified Native Spaces in Kernel Methods</title><link>http://arxiv.org/abs/2501.01825v1</link><description>There exists a plethora of parametric models for positive definite kernels,and their use is ubiquitous in disciplines as diverse as statistics, machinelearning, numerical analysis, and approximation theory. Usually, the kernelparameters index certain features of an associated process. Amongst thosefeatures, smoothness (in the sense of Sobolev spaces, mean squaredifferentiability, and fractal dimensions), compact or global supports, andnegative dependencies (hole effects) are of interest to several theoretical andapplied disciplines. This paper unifies a wealth of well-known kernels into asingle parametric class that encompasses them as special cases, attained eitherby exact parameterization or through parametric asymptotics. We furthermorecharacterize the Sobolev space that is norm equivalent to the RKHS associatedwith the new kernel. As a by-product, we infer the Sobolev spaces that areassociated with existing classes of kernels. We illustrate the main propertiesof the new class, show how this class can switch from compact to globalsupports, and provide special cases for which the kernel attains negativevalues over nontrivial intervals. Hence, the proposed class of kernel is thereproducing kernel of a very rich Hilbert space that contains many specialcases, including the celebrated Mat\'ern and Wendland kernels, as well as theiraliases with hole effects.</description><author>Xavier Emery, Emilio Porcu, Moreno Bevilacqua</author><pubDate>Fri, 03 Jan 2025 14:17:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01825v1</guid></item><item><title>SDPO: Segment-Level Direct Preference Optimization for Social Agents</title><link>http://arxiv.org/abs/2501.01821v1</link><description>Social agents powered by large language models (LLMs) can simulate humansocial behaviors but fall short in handling complex goal-oriented socialdialogues. Direct Preference Optimization (DPO) has proven effective inaligning LLM behavior with human preferences across a variety of agent tasks.Existing DPO-based approaches for multi-turn interactions are divided intoturn-level and session-level methods. The turn-level method is overlyfine-grained, focusing exclusively on individual turns, while session-levelmethods are too coarse-grained, often introducing training noise. To addressthese limitations, we propose Segment-Level Direct Preference Optimization(SDPO), which focuses on specific key segments within interactions to optimizemulti-turn agent behavior while minimizing training noise. Evaluations on theSOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperformboth existing DPO-based methods and proprietary LLMs like GPT-4o, underscoringSDPO's potential to advance the social intelligence of LLM-based agents. Werelease our code and data athttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.</description><author>Aobo Kong, Wentao Ma, Shiwan Zhao, Yongbin Li, Yuchuan Wu, Ke Wang, Xiaoqian Liu, Qicheng Li, Yong Qin, Fei Huang</author><pubDate>Fri, 03 Jan 2025 14:09:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01821v1</guid></item><item><title>Bridging Simplicity and Sophistication using GLinear: A Novel Architecture for Enhanced Time Series Prediction</title><link>http://arxiv.org/abs/2501.01087v2</link><description>Time Series Forecasting (TSF) is an important application across many fields.There is a debate about whether Transformers, despite being good atunderstanding long sequences, struggle with preserving temporal relationshipsin time series data. Recent research suggests that simpler linear models mightoutperform or at least provide competitive performance compared to complexTransformer-based models for TSF tasks. In this paper, we propose a noveldata-efficient architecture, GLinear, for multivariate TSF that exploitsperiodic patterns to provide better accuracy. It also provides betterprediction accuracy by using a smaller amount of historical data compared toother state-of-the-art linear predictors. Four different datasets (ETTh1,Electricity, Traffic, and Weather) are used to evaluate the performance of theproposed predictor. A performance comparison with state-of-the-art lineararchitectures (such as NLinear, DLinear, and RLinear) and transformer-basedtime series predictor (Autoformer) shows that the GLinear, despite beingparametrically efficient, significantly outperforms the existing architecturesin most cases of multivariate TSF. We hope that the proposed GLinear opens newfronts of research and development of simpler and more sophisticatedarchitectures for data and computationally efficient time-series analysis.</description><author>Syed Tahir Hussain Rizvi, Neel Kanwal, Muddasar Naeem, Alfredo Cuzzocrea, Antonio Coronato</author><pubDate>Fri, 03 Jan 2025 14:05:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01087v2</guid></item><item><title>Rerouting LLM Routers</title><link>http://arxiv.org/abs/2501.01818v1</link><description>LLM routers aim to balance quality and cost of generation by classifyingqueries and routing them to a cheaper or more expensive LLM depending on theircomplexity. Routers represent one type of what we call LLM control planes:systems that orchestrate use of one or more LLMs. In this paper, we investigaterouters' adversarial robustness. We first define LLM control plane integrity, i.e., robustness of LLMorchestration to adversarial inputs, as a distinct problem in AI safety. Next,we demonstrate that an adversary can generate query-independent token sequenceswe call ``confounder gadgets'' that, when added to any query, cause LLM routersto send the query to a strong LLM. Our quantitative evaluation shows that this attack is successful both inwhite-box and black-box settings against a variety of open-source andcommercial routers, and that confounding queries do not affect the quality ofLLM responses. Finally, we demonstrate that gadgets can be effective whilemaintaining low perplexity, thus perplexity-based filtering is not an effectivedefense. We finish by investigating alternative defenses.</description><author>Avital Shafran, Roei Schuster, Thomas Ristenpart, Vitaly Shmatikov</author><pubDate>Fri, 03 Jan 2025 14:03:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01818v1</guid></item><item><title>Uncertainty-Aware Label Refinement on Hypergraphs for Personalized Federated Facial Expression Recognition</title><link>http://arxiv.org/abs/2501.01816v1</link><description>Most facial expression recognition (FER) models are trained on large-scaleexpression data with centralized learning. Unfortunately, collecting a largeamount of centralized expression data is difficult in practice due to privacyconcerns of facial images. In this paper, we investigate FER under theframework of personalized federated learning, which is a valuable and practicaldecentralized setting for real-world applications. To this end, we develop anovel uncertainty-Aware label refineMent on hYpergraphs (AMY) method. For localtraining, each local model consists of a backbone, an uncertainty estimation(UE) block, and an expression classification (EC) block. In the UE block, weleverage a hypergraph to model complex high-order relationships betweenexpression samples and incorporate these relationships into uncertaintyfeatures. A personalized uncertainty estimator is then introduced to estimatereliable uncertainty weights of samples in the local client. In the EC block,we perform label propagation on the hypergraph, obtaining high-quality refinedlabels for retraining an expression classifier. Based on the above, weeffectively alleviate heterogeneous sample uncertainty across clients and learna robust personalized FER model in each client. Experimental results on twochallenging real-world facial expression databases show that our proposedmethod consistently outperforms several state-of-the-art methods. Thisindicates the superiority of hypergraph modeling for uncertainty estimation andlabel refinement on the personalized federated FER task. The source code willbe released at https://github.com/mobei1006/AMY.</description><author>Hu Ding, Yan Yan, Yang Lu, Jing-Hao Xue, Hanzi Wang</author><pubDate>Fri, 03 Jan 2025 13:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01816v1</guid></item><item><title>MobileNetV2: A lightweight classification model for home-based sleep apnea screening</title><link>http://arxiv.org/abs/2412.19967v2</link><description>This study proposes a novel lightweight neural network model leveragingfeatures extracted from electrocardiogram (ECG) and respiratory signals forearly OSA screening. ECG signals are used to generate feature spectrograms topredict sleep stages, while respiratory signals are employed to detectsleep-related breathing abnormalities. By integrating these predictions, themethod calculates the apnea-hypopnea index (AHI) with enhanced accuracy,facilitating precise OSA diagnosis. The method was validated on three publicly available sleep apnea databases:the Apnea-ECG database, the UCDDB dataset, and the MIT-BIH Polysomnographicdatabase. Results showed an overall OSA detection accuracy of 0.978,highlighting the model's robustness. Respiratory event classification achievedan accuracy of 0.969 and an area under the receiver operating characteristiccurve (ROC-AUC) of 0.98. For sleep stage classification, in UCDDB dataset, theROC-AUC exceeded 0.85 across all stages, with recall for Sleep reaching 0.906and specificity for REM and Wake states at 0.956 and 0.937, respectively. This study underscores the potential of integrating lightweight neuralnetworks with multi-signal analysis for accurate, portable, and cost-effectiveOSA screening, paving the way for broader adoption in home-based and wearablehealth monitoring systems.</description><author>Hui Pan, Yanxuan Yu, Jilun Ye, Xu Zhang</author><pubDate>Fri, 03 Jan 2025 13:55:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19967v2</guid></item><item><title>QuantumBind-RBFE: Accurate Relative Binding Free Energy Calculations Using Neural Network Potentials</title><link>http://arxiv.org/abs/2501.01811v1</link><description>Accurate prediction of protein-ligand binding affinities is crucial in drugdiscovery, particularly during hit-to-lead and lead optimization phases,however, limitations in ligand force fields continue to impact predictionaccuracy. In this work, we validate relative binding free energy (RBFE)accuracy using neural network potentials (NNPs) for the ligands. We utilize anovel NNP model, AceForce 1.0, based on the TensorNet architecture for smallmolecules that broadens the applicability to diverse drug-like compounds,including all important chemical elements and supporting charged molecules.Using established benchmarks, we show overall improved accuracy and correlationin binding affinity predictions compared with GAFF2 for molecular mechanics andANI2-x for NNPs. Slightly less accuracy but comparable correlations with OPLS4.We also show that we can run the NNP simulations at 2 fs timestep, at least twotimes larger than previous NNP models, providing significant speed gains. Theresults show promise for further evolutions of free energy calculations usingNNPs while demonstrating its practical use already with the current generation.The code and NNP model are publicly available for research use.</description><author>Francesc Sabanés Zariquiey, Stephen E. Farr, Stefan Doerr, Gianni De Fabritiis</author><pubDate>Fri, 03 Jan 2025 13:51:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01811v1</guid></item><item><title>MoEE: Mixture of Emotion Experts for Audio-Driven Portrait Animation</title><link>http://arxiv.org/abs/2501.01808v1</link><description>The generation of talking avatars has achieved significant advancements inprecise audio synchronization. However, crafting lifelike talking head videosrequires capturing a broad spectrum of emotions and subtle facial expressions.Current methods face fundamental challenges: a)the absence of frameworks formodeling single basic emotional expressions, which restricts the generation ofcomplex emotions such as compound emotions; b)the lack of comprehensivedatasets rich in human emotional expressions, which limits the potential ofmodels. To address these challenges, we propose the following innovations:1)the Mixture of Emotion Experts (MoEE) model, which decouples six fundamentalemotions to enable the precise synthesis of both singular and compoundemotional states; 2)the DH-FaceEmoVid-150 dataset, specifically curated toinclude six prevalent human emotional expressions as well as four types ofcompound emotions, thereby expanding the training potential of emotion-drivenmodels. Furthermore, to enhance the flexibility of emotion control, we proposean emotion-to-latents module that leverages multimodal inputs, aligning diversecontrol signals-such as audio, text, and labels-to ensure more varied controlinputs as well as the ability to control emotions using audio alone. Throughextensive quantitative and qualitative evaluations, we demonstrate that theMoEE framework, in conjunction with the DH-FaceEmoVid-150 dataset, excels ingenerating complex emotional expressions and nuanced facial details, setting anew benchmark in the field. These datasets will be publicly released.</description><author>Huaize Liu, Wenzhang Sun, Donglin Di, Shibo Sun, Jiahui Yang, Changqing Zou, Hujun Bao</author><pubDate>Fri, 03 Jan 2025 13:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01808v1</guid></item><item><title>End-to-End Long Document Summarization using Gradient Caching</title><link>http://arxiv.org/abs/2501.01805v1</link><description>Training transformer-based encoder-decoder models for long documentsummarization poses a significant challenge due to the quadratic memoryconsumption during training. Several approaches have been proposed to extendthe input length at test time, but training with these approaches is stilldifficult, requiring truncation of input documents and causing a mismatchbetween training and test conditions. In this work, we propose CachED (Gradient$\textbf{Cach}$ing for $\textbf{E}$ncoder-$\textbf{D}$ecoder models), anapproach that enables end-to-end training of existing transformer-basedencoder-decoder models, using the entire document without truncation.Specifically, we apply non-overlapping sliding windows to input documents,followed by fusion in decoder. During backpropagation, the gradients are cachedat the decoder and are passed through the encoder in chunks by re-computing thehidden vectors, similar to gradient checkpointing. In the experiments on longdocument summarization, we extend BART to CachED BART, processing more than500K tokens during training and achieving superior performance without usingany additional parameters.</description><author>Rohit Saxena, Hao Tang, Frank Keller</author><pubDate>Fri, 03 Jan 2025 13:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01805v1</guid></item><item><title>2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining</title><link>http://arxiv.org/abs/2501.00958v2</link><description>Compared to image-text pair data, interleaved corpora enable Vision-LanguageModels (VLMs) to understand the world more naturally like humans. However, suchexisting datasets are crawled from webpage, facing challenges like lowknowledge density, loose image-text relations, and poor logical coherencebetween images. On the other hand, the internet hosts vast instructional videos(e.g., online geometry courses) that are widely used by humans to learnfoundational subjects, yet these valuable resources remain underexplored in VLMtraining. In this paper, we introduce a high-quality \textbf{multimodaltextbook} corpus with richer foundational knowledge for VLM pretraining. Itcollects over 2.5 years of instructional videos, totaling 22,000 class hours.We first use an LLM-proposed taxonomy to systematically gather instructionalvideos. Then we progressively extract and refine visual (keyframes), audio(ASR), and textual knowledge (OCR) from the videos, and organize as animage-text interleaved corpus based on temporal order. Compared to itscounterparts, our video-centric textbook offers more coherent context, richerknowledge, and better image-text alignment. Experiments demonstrate its superbpretraining performance, particularly in knowledge- and reasoning-intensivetasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbookexhibit outstanding interleaved context awareness, leveraging visual andtextual cues in their few-shot context for task solving~\footnote{Our code areavailable at \url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}.</description><author>Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, Lidong Bing</author><pubDate>Fri, 03 Jan 2025 13:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.00958v2</guid></item><item><title>BERT4MIMO: A Foundation Model using BERT Architecture for Massive MIMO Channel State Information Prediction</title><link>http://arxiv.org/abs/2501.01802v1</link><description>Massive MIMO (Multiple-Input Multiple-Output) is an advanced wirelesscommunication technology, using a large number of antennas to improve theoverall performance of the communication system in terms of capacity, spectral,and energy efficiency. The performance of MIMO systems is highly dependent onthe quality of channel state information (CSI). Predicting CSI is, therefore,essential for improving communication system performance, particularly in MIMOsystems, since it represents key characteristics of a wireless channel,including propagation, fading, scattering, and path loss. This study proposes afoundation model inspired by BERT, called BERT4MIMO, which is specificallydesigned to process high-dimensional CSI data from massive MIMO systems.BERT4MIMO offers superior performance in reconstructing CSI under varyingmobility scenarios and channel conditions through deep learning and attentionmechanisms. The experimental results demonstrate the effectiveness of BERT4MIMOin a variety of wireless environments.</description><author>Ferhat Ozgur Catak, Murat Kuzlu, Umit Cali</author><pubDate>Fri, 03 Jan 2025 13:22:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01802v1</guid></item><item><title>Balanced Residual Distillation Learning for 3D Point Cloud Class-Incremental Semantic Segmentation</title><link>http://arxiv.org/abs/2408.01356v2</link><description>Class-incremental learning (CIL) enables continuous learning of new classeswhile mitigating catastrophic forgetting of old ones. For the performancebreakthrough of CIL, it is essential yet challenging to effectively refine pastknowledge from the base model and balance it with new learning. However, such achallenge has not been considered in current research. This work proposes abalanced residual distillation learning framework (BRDL) to address this gapand advance CIL performance. BRDL introduces a residual distillation strategyto dynamically refine past knowledge by expanding the network structure and abalanced pseudo-label learning strategy to mitigate class bias and balancelearning between old and new classes. We apply the proposed BRDL to achallenging 3D point cloud semantic segmentation task where the data isunordered and unstructured. Extensive experimental results demonstrate thatBRDL sets a new benchmark with an outstanding balance capability inclass-biased scenarios.</description><author>Yuanzhi Su, Siyuan Chen, Yuan-Gen Wang</author><pubDate>Fri, 03 Jan 2025 13:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01356v2</guid></item><item><title>John Ellipsoids via Lazy Updates</title><link>http://arxiv.org/abs/2501.01801v1</link><description>We give a faster algorithm for computing an approximate John ellipsoid around$n$ points in $d$ dimensions. The best known prior algorithms are based onrepeatedly computing the leverage scores of the points and reweighting them bythese scores [CCLY19]. We show that this algorithm can be substantially sped upby delaying the computation of high accuracy leverage scores by using sampling,and then later computing multiple batches of high accuracy leverage scores viafast rectangular matrix multiplication. We also give low-space streamingalgorithms for John ellipsoids using similar ideas.</description><author>David P. Woodruff, Taisuke Yasuda</author><pubDate>Fri, 03 Jan 2025 13:17:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01801v1</guid></item><item><title>JoyGen: Audio-Driven 3D Depth-Aware Talking-Face Video Editing</title><link>http://arxiv.org/abs/2501.01798v1</link><description>Significant progress has been made in talking-face video generation research;however, precise lip-audio synchronization and high visual quality remainchallenging in editing lip shapes based on input audio. This paper introducesJoyGen, a novel two-stage framework for talking-face generation, comprisingaudio-driven lip motion generation and visual appearance synthesis. In thefirst stage, a 3D reconstruction model and an audio2motion model predictidentity and expression coefficients respectively. Next, by integrating audiofeatures with a facial depth map, we provide comprehensive supervision forprecise lip-audio synchronization in facial generation. Additionally, weconstructed a Chinese talking-face dataset containing 130 hours of high-qualityvideo. JoyGen is trained on the open-source HDTF dataset and our curateddataset. Experimental results demonstrate superior lip-audio synchronizationand visual quality achieved by our method.</description><author>Qili Wang, Dajiang Wu, Zihang Xu, Junshi Huang, Jun Lv</author><pubDate>Fri, 03 Jan 2025 13:14:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01798v1</guid></item><item><title>Multi-Label Contrastive Learning : A Comprehensive Study</title><link>http://arxiv.org/abs/2412.00101v2</link><description>Multi-label classification, which involves assigning multiple labels to asingle input, has emerged as a key area in both research and industry due toits wide-ranging applications. Designing effective loss functions is crucialfor optimizing deep neural networks for this task, as they significantlyinfluence model performance and efficiency. Traditional loss functions, whichoften maximize likelihood under the assumption of label independence, maystruggle to capture complex label relationships. Recent research has turned tosupervised contrastive learning, a method that aims to create a structuredrepresentation space by bringing similar instances closer together and pushingdissimilar ones apart. Although contrastive learning offers a promisingapproach, applying it to multi-label classification presents unique challenges,particularly in managing label interactions and data structure. In this paper, we conduct an in-depth study of contrastive learning loss formulti-label classification across diverse settings. These include datasets withboth small and large numbers of labels, datasets with varying amounts oftraining data, and applications in both computer vision and natural languageprocessing. Our empirical results indicate that the promising outcomes of contrastivelearning are attributable not only to the consideration of label interactionsbut also to the robust optimization scheme of the contrastive loss.Furthermore, while the supervised contrastive loss function faces challengeswith datasets containing a small number of labels and ranking-based metrics, itdemonstrates excellent performance, particularly in terms of Macro-F1, ondatasets with a large number of labels.</description><author>Alexandre Audibert, Aurélien Gauffre, Massih-Reza Amini</author><pubDate>Fri, 03 Jan 2025 13:10:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.00101v2</guid></item><item><title>Reading Between the Lines: A dataset and a study on why some texts are tougher than others</title><link>http://arxiv.org/abs/2501.01796v1</link><description>Our research aims at better understanding what makes a text difficult to readfor specific audiences with intellectual disabilities, more specifically,people who have limitations in cognitive functioning, such as reading andunderstanding skills, an IQ below 70, and challenges in conceptual domains. Weintroduce a scheme for the annotation of difficulties which is based onempirical research in psychology as well as on research in translation studies.The paper describes the annotated dataset, primarily derived from the paralleltexts (standard English and Easy to Read English translations) made availableonline. we fine-tuned four different pre-trained transformer models to performthe task of multiclass classification to predict the strategies required forsimplification. We also investigate the possibility to interpret the decisionsof this language model when it is aimed at predicting the difficulty ofsentences. The resources are available fromhttps://github.com/Nouran-Khallaf/why-tough</description><author>Nouran Khallaf, Carlo Eugeni, Serge Sharoff</author><pubDate>Fri, 03 Jan 2025 13:09:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01796v1</guid></item><item><title>CoPRA: Bridging Cross-domain Pretrained Sequence Models with Complex Structures for Protein-RNA Binding Affinity Prediction</title><link>http://arxiv.org/abs/2409.03773v2</link><description>Accurately measuring protein-RNA binding affinity is crucial in manybiological processes and drug design. Previous computational methods forprotein-RNA binding affinity prediction rely on either sequence or structurefeatures, unable to capture the binding mechanisms comprehensively. The recentemerging pre-trained language models trained on massive unsupervised sequencesof protein and RNA have shown strong representation ability for variousin-domain downstream tasks, including binding site prediction. However,applying different-domain language models collaboratively for complex-leveltasks remains unexplored. In this paper, we propose CoPRA to bridge pre-trainedlanguage models from different biological domains via Complex structure forProtein-RNA binding Affinity prediction. We demonstrate for the first time thatcross-biological modal language models can collaborate to improve bindingaffinity prediction. We propose a Co-Former to combine the cross-modal sequenceand structure information and a bi-scope pre-training strategy for improvingCo-Former's interaction understanding. Meanwhile, we build the largestprotein-RNA binding affinity dataset PRA310 for performance evaluation. We alsotest our model on a public dataset for mutation effect prediction. CoPRAreaches state-of-the-art performance on all the datasets. We provide extensiveanalyses and verify that CoPRA can (1) accurately predict the protein-RNAbinding affinity; (2) understand the binding affinity change caused bymutations; and (3) benefit from scaling data and model size.</description><author>Rong Han, Xiaohong Liu, Tong Pan, Jing Xu, Xiaoyu Wang, Wuyang Lan, Zhenyu Li, Zixuan Wang, Jiangning Song, Guangyu Wang, Ting Chen</author><pubDate>Fri, 03 Jan 2025 13:03:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03773v2</guid></item><item><title>Nearly Linear Sparsification of $\ell_p$ Subspace Approximation</title><link>http://arxiv.org/abs/2407.03262v2</link><description>The $\ell_p$ subspace approximation problem is an NP-hard low rankapproximation problem that generalizes the median hyperplane problem ($p = 1$),principal component analysis ($p = 2$), and the center hyperplane problem ($p =\infty$). A popular approach to cope with the NP-hardness of this problem is tocompute a strong coreset, which is a small weighted subset of the input pointswhich simultaneously approximates the cost of every $k$-dimensional subspace,typically to $(1+\varepsilon)$ relative error for a small constant$\varepsilon$. We obtain the first algorithm for constructing a strong coreset for $\ell_p$subspace approximation with a nearly optimal dependence on the rank parameter$k$, obtaining a nearly linear bound of $\tildeO(k)\mathrm{poly}(\varepsilon^{-1})$ for $p&lt;2$ and $\tildeO(k^{p/2})\mathrm{poly}(\varepsilon^{-1})$ for $p&gt;2$. Prior constructionseither achieved a similar size bound but produced a coreset with a modificationof the original points [SW18, FKW21], or produced a coreset of the originalpoints but lost $\mathrm{poly}(k)$ factors in the coreset size [HV20, WY23]. Our techniques also lead to the first nearly optimal online strong coresetsfor $\ell_p$ subspace approximation with similar bounds as the offline setting,resolving a problem of [WY23]. All prior approaches lose $\mathrm{poly}(k)$factors in this setting, even when allowed to modify the original points.</description><author>David P. Woodruff, Taisuke Yasuda</author><pubDate>Fri, 03 Jan 2025 12:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03262v2</guid></item><item><title>FGAseg: Fine-Grained Pixel-Text Alignment for Open-Vocabulary Semantic Segmentation</title><link>http://arxiv.org/abs/2501.00877v2</link><description>Open-vocabulary segmentation aims to identify and segment specific regionsand objects based on text-based descriptions. A common solution is to leveragepowerful vision-language models (VLMs), such as CLIP, to bridge the gap betweenvision and text information. However, VLMs are typically pretrained forimage-level vision-text alignment, focusing on global semantic features. Incontrast, segmentation tasks require fine-grained pixel-level alignment anddetailed category boundary information, which VLMs alone cannot provide. As aresult, information extracted directly from VLMs can't meet the requirements ofsegmentation tasks. To address this limitation, we propose FGAseg, a modeldesigned for fine-grained pixel-text alignment and category boundarysupplementation. The core of FGAseg is a Pixel-Level Alignment module thatemploys a cross-modal attention mechanism and a text-pixel alignment loss torefine the coarse-grained alignment from CLIP, achieving finer-grainedpixel-text semantic alignment. Additionally, to enrich category boundaryinformation, we introduce the alignment matrices as optimizable pseudo-masksduring forward propagation and propose Category Information Supplementationmodule. These pseudo-masks, derived from cosine and convolutional similarity,provide essential global and local boundary information between differentcategories. By combining these two strategies, FGAseg effectively enhancespixel-level alignment and category boundary information, addressing keychallenges in open-vocabulary segmentation. Extensive experiments demonstratethat FGAseg outperforms existing methods on open-vocabulary semanticsegmentation benchmarks.</description><author>Bingyu Li, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li</author><pubDate>Fri, 03 Jan 2025 12:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.00877v2</guid></item><item><title>Creating Artificial Students that Never Existed: Leveraging Large Language Models and CTGANs for Synthetic Data Generation</title><link>http://arxiv.org/abs/2501.01793v1</link><description>In this study, we explore the growing potential of AI and deep learningtechnologies, particularly Generative Adversarial Networks (GANs) and LargeLanguage Models (LLMs), for generating synthetic tabular data. Access toquality students data is critical for advancing learning analytics, but privacyconcerns and stricter data protection regulations worldwide limit theiravailability and usage. Synthetic data offers a promising alternative. Weinvestigate whether synthetic data can be leveraged to create artificialstudents for serving learning analytics models. Using the popular GAN modelCTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetictabular student data. Our results demonstrate the strong potential of thesemethods to produce high-quality synthetic datasets that resemble real studentsdata. To validate our findings, we apply a comprehensive set of utilityevaluation metrics to assess the statistical and predictive performance of thesynthetic data and compare the different generator models used, specially theperformance of LLMs. Our study aims to provide the learning analytics communitywith valuable insights into the use of synthetic data, laying the groundworkfor expanding the field methodological toolbox with new innovative approachesfor learning analytics data generation.</description><author>Mohammad Khalil, Farhad Vadiee, Ronas Shakya, Qinyi Liu</author><pubDate>Fri, 03 Jan 2025 12:52:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01793v1</guid></item><item><title>A Minimal Subset Approach for Efficient and Scalable Loop Closure</title><link>http://arxiv.org/abs/2501.01791v1</link><description>Loop closure detection in large-scale and long-term missions can becomputationally demanding due to the need to identify, verify, and processnumerous candidate pairs to establish edge connections for the pose graphoptimization. Keyframe sampling mitigates this by reducing the number of framesstored and processed in the back-end system. In this article, we address thegap in optimized keyframe sampling for the combined problem of pose graphoptimization and loop closure detection. Our Minimal Subset Approach (MSA)employs an optimization strategy with two key factors, redundancy minimizationand information preservation, within a sliding window framework to efficientlyreduce redundant keyframes, while preserving essential information. This methoddelivers comparable performance to baseline approaches, while enhancingscalability and reducing computational overhead. Finally, we evaluate MSA onrelevant publicly available datasets, showcasing that it consistently performsacross a wide range of environments, without requiring any manual parametertuning.</description><author>Nikolaos Stathoulopoulos, Christoforos Kanellakis, George Nikolakopoulos</author><pubDate>Fri, 03 Jan 2025 12:48:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01791v1</guid></item><item><title>Ingredients: Blending Custom Photos with Video Diffusion Transformers</title><link>http://arxiv.org/abs/2501.01790v1</link><description>This paper presents a powerful framework to customize video creations byincorporating multiple specific identity (ID) photos, with video diffusionTransformers, referred to as \texttt{Ingredients}. Generally, our methodconsists of three primary modules: (\textbf{i}) a facial extractor thatcaptures versatile and precise facial features for each human ID from bothglobal and local perspectives; (\textbf{ii}) a multi-scale projector that mapsface embeddings into the contextual space of image query in video diffusiontransformers; (\textbf{iii}) an ID router that dynamically combines andallocates multiple ID embedding to the corresponding space-time regions.Leveraging a meticulously curated text-video dataset and a multi-stage trainingprotocol, \texttt{Ingredients} demonstrates superior performance in turningcustom photos into dynamic and personalized video content. Qualitativeevaluations highlight the advantages of proposed method, positioning it as asignificant advancement toward more effective generative video control tools inTransformer-based architecture, compared to existing methods. The data, code,and model weights are publicly available at:\url{https://github.com/feizc/Ingredients}.</description><author>Zhengcong Fei, Debang Li, Di Qiu, Changqian Yu, Mingyuan Fan</author><pubDate>Fri, 03 Jan 2025 12:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01790v1</guid></item><item><title>Universal Online Temporal Calibration for Optimization-based Visual-Inertial Navigation Systems</title><link>http://arxiv.org/abs/2501.01788v1</link><description>6-Degree of Freedom (6DoF) motion estimation with a combination of visual andinertial sensors is a growing area with numerous real-world applications.However, precise calibration of the time offset between these two sensor typesis a prerequisite for accurate and robust tracking. To address this, we proposea universal online temporal calibration strategy for optimization-basedvisual-inertial navigation systems. Technically, we incorporate the time offsettd as a state parameter in the optimization residual model to align the IMUstate to the corresponding image timestamp using td, angular velocity andtranslational velocity. This allows the temporal misalignment td to beoptimized alongside other tracking states during the process. As our methodonly modifies the structure of the residual model, it can be applied to variousoptimization-based frameworks with different tracking frontends. We evaluateour calibration method with both EuRoC and simulation data and extensiveexperiments demonstrate that our approach provides more accurate time offsetestimation and faster convergence, particularly in the presence of noisy sensordata.</description><author>Yunfei Fan, Tianyu Zhao, Linan Guo, Chen Chen, Xin Wang, Fengyi Zhou</author><pubDate>Fri, 03 Jan 2025 12:41:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01788v1</guid></item><item><title>Disentangling Speakers in Multi-Talker Speech Recognition with Speaker-Aware CTC</title><link>http://arxiv.org/abs/2409.12388v2</link><description>Multi-talker speech recognition (MTASR) faces unique challenges indisentangling and transcribing overlapping speech. To address these challenges,this paper investigates the role of Connectionist Temporal Classification (CTC)in speaker disentanglement when incorporated with Serialized Output Training(SOT) for MTASR. Our visualization reveals that CTC guides the encoder torepresent different speakers in distinct temporal regions of acousticembeddings. Leveraging this insight, we propose a novel Speaker-Aware CTC(SACTC) training objective, based on the Bayes risk CTC framework. SACTC is atailored CTC variant for multi-talker scenarios, it explicitly models speakerdisentanglement by constraining the encoder to represent different speakers'tokens at specific time frames. When integrated with SOT, the SOT-SACTC modelconsistently outperforms standard SOT-CTC across various degrees of speechoverlap. Specifically, we observe relative word error rate reductions of 10%overall and 15% on low-overlap speech. This work represents an initialexploration of CTC-based enhancements for MTASR tasks, offering a newperspective on speaker disentanglement in multi-talker speech recognition. Thecode is available at https://github.com/kjw11/Speaker-Aware-CTC.</description><author>Jiawen Kang, Lingwei Meng, Mingyu Cui, Yuejiao Wang, Xixin Wu, Xunying Liu, Helen Meng</author><pubDate>Fri, 03 Jan 2025 12:36:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12388v2</guid></item><item><title>Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms</title><link>http://arxiv.org/abs/2501.01785v1</link><description>The increasing use of machine learning in learning analytics (LA) has raisedsignificant concerns around algorithmic fairness and privacy. Synthetic datahas emerged as a dual-purpose tool, enhancing privacy and improving fairness inLA models. However, prior research suggests an inverse relationship betweenfairness and privacy, making it challenging to optimize both. This studyinvestigates which synthetic data generators can best balance privacy andfairness, and whether pre-processing fairness algorithms, typically applied toreal datasets, are effective on synthetic data. Our results highlight that theDEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance betweenprivacy and fairness. However, DECAF suffers in utility, as reflected in itspredictive accuracy. Notably, we found that applying pre-processing fairnessalgorithms to synthetic data improves fairness even more than when applied toreal data. These findings suggest that combining synthetic data generation withfairness pre-processing offers a promising approach to creating fairer LAmodels.</description><author>Qinyi Liu, Oscar Deho, Farhad Vadiee, Mohammad Khalil, Srecko Joksimovic, George Siemens</author><pubDate>Fri, 03 Jan 2025 12:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01785v1</guid></item><item><title>Leveraging Consistent Spatio-Temporal Correspondence for Robust Visual Odometry</title><link>http://arxiv.org/abs/2412.16923v2</link><description>Recent approaches to VO have significantly improved performance by using deepnetworks to predict optical flow between video frames. However, existingmethods still suffer from noisy and inconsistent flow matching, making itdifficult to handle challenging scenarios and long-sequence estimation. Toovercome these challenges, we introduce Spatio-Temporal Visual Odometry (STVO),a novel deep network architecture that effectively leverages inherentspatio-temporal cues to enhance the accuracy and consistency of multi-frameflow matching. With more accurate and consistent flow matching, STVO canachieve better pose estimation through the bundle adjustment (BA).Specifically, STVO introduces two innovative components: 1) the TemporalPropagation Module that utilizes multi-frame information to extract andpropagate temporal cues across adjacent frames, maintaining temporalconsistency; 2) the Spatial Activation Module that utilizes geometric priorsfrom the depth maps to enhance spatial consistency while filtering outexcessive noise and incorrect matches. Our STVO achieves state-of-the-artperformance on TUM-RGBD, EuRoc MAV, ETH3D and KITTI Odometry benchmarks.Notably, it improves accuracy by 77.8% on ETH3D benchmark and 38.9% on KITTIOdometry benchmark over the previous best methods.</description><author>Zhaoxing Zhang, Junda Cheng, Gangwei Xu, Xiaoxiang Wang, Can Zhang, Xin Yang</author><pubDate>Fri, 03 Jan 2025 12:34:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.16923v2</guid></item><item><title>Evaluating the Capabilities of Large Language Models for Multi-label Emotion Understanding</title><link>http://arxiv.org/abs/2412.17837v2</link><description>Large Language Models (LLMs) show promising learning and reasoning abilities.Compared to other NLP tasks, multilingual and multi-label emotion evaluationtasks are under-explored in LLMs. In this paper, we present EthioEmo, amulti-label emotion classification dataset for four Ethiopian languages,namely, Amharic (amh), Afan Oromo (orm), Somali (som), and Tigrinya (tir). Weperform extensive experiments with an additional English multi-label emotiondataset from SemEval 2018 Task 1. Our evaluation includes encoder-only,encoder-decoder, and decoder-only language models. We compare zero and few-shotapproaches of LLMs to fine-tuning smaller language models. The results showthat accurate multi-label emotion classification is still insufficient even forhigh-resource languages such as English, and there is a large gap between theperformance of high-resource and low-resource languages. The results also showvarying performance levels depending on the language and model type. EthioEmois available publicly to further improve the understanding of emotions inlanguage models and how people convey emotions through various languages.</description><author>Tadesse Destaw Belay, Israel Abebe Azime, Abinew Ali Ayele, Grigori Sidorov, Dietrich Klakow, Philipp Slusallek, Olga Kolesnikova, Seid Muhie Yimam</author><pubDate>Fri, 03 Jan 2025 12:32:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17837v2</guid></item><item><title>Nonparametric estimation of a factorizable density using diffusion models</title><link>http://arxiv.org/abs/2501.01783v1</link><description>In recent years, diffusion models, and more generally score-based deepgenerative models, have achieved remarkable success in various applications,including image and audio generation. In this paper, we view diffusion modelsas an implicit approach to nonparametric density estimation and study themwithin a statistical framework to analyze their surprising performance. A keychallenge in high-dimensional statistical inference is leveraginglow-dimensional structures inherent in the data to mitigate the curse ofdimensionality. We assume that the underlying density exhibits alow-dimensional structure by factorizing into low-dimensional components, aproperty common in examples such as Bayesian networks and Markov random fields.Under suitable assumptions, we demonstrate that an implicit density estimatorconstructed from diffusion models adapts to the factorization structure andachieves the minimax optimal rate with respect to the total variation distance.In constructing the estimator, we design a sparse weight-sharing neural networkarchitecture, where sparsity and weight-sharing are key features of practicalarchitectures such as convolutional neural networks and recurrent neuralnetworks.</description><author>Hyeok Kyu Kwon, Dongha Kim, Ilsang Ohn, Minwoo Chae</author><pubDate>Fri, 03 Jan 2025 12:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01783v1</guid></item><item><title>GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models</title><link>http://arxiv.org/abs/2501.01428v2</link><description>In recent years, 2D Vision-Language Models (VLMs) have made significantstrides in image-text understanding tasks. However, their performance in 3Dspatial comprehension, which is critical for embodied intelligence, remainslimited. Recent advances have leveraged 3D point clouds and multi-view imagesas inputs, yielding promising results. However, we propose exploring a purelyvision-based solution inspired by human perception, which merely relies onvisual cues for 3D spatial understanding. This paper empirically investigatesthe limitations of VLMs in 3D spatial knowledge, revealing that their primaryshortcoming lies in the lack of global-local correspondence between the sceneand individual frames. To address this, we introduce GPT4Scene, a novel visualprompting paradigm in VLM training and inference that helps build theglobal-local relationship, significantly improving the 3D spatial understandingof indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV)image from the video and marks consistent object IDs across both frames and theBEV image. The model then inputs the concatenated BEV image and video frameswith markers. In zero-shot evaluations, GPT4Scene improves performance overclosed-source VLMs like GPT-4o. Additionally, we prepare a processed videodataset consisting of 165K text annotation to fine-tune open-source VLMs,achieving state-of-the-art performance on all 3D understanding tasks.Surprisingly, after training with the GPT4Scene paradigm, VLMs consistentlyimprove during inference, even without visual prompting and BEV image asexplicit correspondence. It demonstrates that the proposed paradigm helps VLMsdevelop an intrinsic ability to understand 3D scenes, which paves the way for anoninvasive approach to extending pre-trained VLMs for 3D scene understanding.</description><author>Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, Hengshuang Zhao</author><pubDate>Fri, 03 Jan 2025 12:30:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01428v2</guid></item><item><title>SVFR: A Unified Framework for Generalized Video Face Restoration</title><link>http://arxiv.org/abs/2501.01235v2</link><description>Face Restoration (FR) is a crucial area within image and video processing,focusing on reconstructing high-quality portraits from degraded inputs. Despiteadvancements in image FR, video FR remains relatively under-explored, primarilydue to challenges related to temporal consistency, motion artifacts, and thelimited availability of high-quality video data. Moreover, traditional facerestoration typically prioritizes enhancing resolution and may not give as muchconsideration to related tasks such as facial colorization and inpainting. Inthis paper, we propose a novel approach for the Generalized Video FaceRestoration (GVFR) task, which integrates video BFR, inpainting, andcolorization tasks that we empirically show to benefit each other. We present aunified framework, termed as stable video face restoration (SVFR), whichleverages the generative and motion priors of Stable Video Diffusion (SVD) andincorporates task-specific information through a unified face restorationframework. A learnable task embedding is introduced to enhance taskidentification. Meanwhile, a novel Unified Latent Regularization (ULR) isemployed to encourage the shared feature representation learning amongdifferent subtasks. To further enhance the restoration quality and temporalstability, we introduce the facial prior learning and the self-referredrefinement as auxiliary strategies used for both training and inference. Theproposed framework effectively combines the complementary strengths of thesetasks, enhancing temporal coherence and achieving superior restoration quality.This work advances the state-of-the-art in video FR and establishes a newparadigm for generalized video face restoration. Code and video demo areavailable at https://github.com/wangzhiyaoo/SVFR.git.</description><author>Zhiyao Wang, Xu Chen, Chengming Xu, Junwei Zhu, Xiaobin Hu, Jiangning Zhang, Chengjie Wang, Yuqi Liu, Yiyi Zhou, Rongrong Ji</author><pubDate>Fri, 03 Jan 2025 12:26:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01235v2</guid></item></channel></rss>