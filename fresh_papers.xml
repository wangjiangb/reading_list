<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 26 Jul 2024 01:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>A Unified Framework for Model Editing</title><link>http://arxiv.org/abs/2403.14236v4</link><description>ROME and MEMIT are largely believed to be two different model editingalgorithms, with the major difference between them being the ability to performbatched edits. In this paper, we unify these two algorithms under a singleconceptual umbrella, optimizing for the same goal, which we call thepreservation-memorization objective. ROME uses an equality constraint tooptimize this objective to perform one edit at a time, whereas MEMIT employs amore flexible least-square constraint that allows for batched edits. Wegeneralize ROME and enable batched editing with equality constraint in the formof EMMET - an Equality-constrained Mass Model Editing algorithm forTransformers, a new batched memory-editing algorithm. EMMET can performbatched-edits up to a batch-size of 10,000, with very similar performance toMEMIT across multiple dimensions. With the introduction of EMMET, we trulyunify ROME and MEMIT and show that both algorithms are equivalent in terms oftheir optimization objective, their abilities (singular and batched editing),their model editing performance and their limitations.</description><author>Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli</author><pubDate>Thu, 25 Jul 2024 16:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14236v4</guid></item><item><title>Looking at Model Debiasing through the Lens of Anomaly Detection</title><link>http://arxiv.org/abs/2407.17449v2</link><description>It is widely recognized that deep neural networks are sensitive to bias inthe data. This means that during training these models are likely to learnspurious correlations between data and labels, resulting in limitedgeneralization abilities and low performance. In this context, model debiasingapproaches can be devised aiming at reducing the model's dependency on suchunwanted correlations, either leveraging the knowledge of bias information ornot. In this work, we focus on the latter and more realistic scenario, showingthe importance of accurately predicting the bias-conflicting and bias-alignedsamples to obtain compelling performance in bias mitigation. On this ground, wepropose to conceive the problem of model bias from an out-of-distributionperspective, introducing a new bias identification method based on anomalydetection. We claim that when data is mostly biased, bias-conflicting samplescan be regarded as outliers with respect to the bias-aligned distribution inthe feature space of a biased model, thus allowing for precisely detecting themwith an anomaly detection method. Coupling the proposed bias identificationapproach with bias-conflicting data upsampling and augmentation in a two-stepstrategy, we reach state-of-the-art performance on synthetic and real benchmarkdatasets. Ultimately, our proposed approach shows that the data bias issue doesnot necessarily require complex debiasing methods, given that an accurate biasidentification procedure is defined.</description><author>Vito Paolo Pastore, Massimiliano Ciranni, Davide Marinelli, Francesca Odone, Vittorio Murino</author><pubDate>Thu, 25 Jul 2024 15:33:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17449v2</guid></item><item><title>Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population</title><link>http://arxiv.org/abs/2407.17324v2</link><description>Dementia, a debilitating neurological condition affecting millions worldwide,presents significant diagnostic challenges. In this work, we introduce a novelmethodology for the classification of demented and non-demented elderlypatients using 3D brain Magnetic Resonance Imaging (MRI) scans. Our approachfeatures a unique technique for selectively processing MRI slices, focusing onthe most relevant brain regions and excluding less informative sections. Thismethodology is complemented by a confidence-based classification committeecomposed of three custom deep learning models: Dem3D ResNet, Dem3D CNN, andDem3D EfficientNet. These models work synergistically to enhancedecision-making accuracy, leveraging their collective strengths. Tested on theOpen Access Series of Imaging Studies(OASIS) dataset, our method achieved animpressive accuracy of 94.12%, surpassing existing methodologies. Furthermore,validation on the Alzheimer's Disease Neuroimaging Initiative (ADNI) datasetconfirmed the robustness and generalizability of our approach. The use ofexplainable AI (XAI) techniques and comprehensive ablation studies furthersubstantiate the effectiveness of our techniques, providing insights into thedecision-making process and the importance of our methodology. This researchoffers a significant advancement in dementia diagnosis, providing a highlyaccurate and efficient tool for clinical applications.</description><author>Nikolaos Ntampakis, Konstantinos Diamantaras, Ioanna Chouvarda, Vasileios Argyriou, Panagiotis Sarigianndis</author><pubDate>Thu, 25 Jul 2024 09:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17324v2</guid></item><item><title>LPGen: Enhancing High-Fidelity Landscape Painting Generation through Diffusion Model</title><link>http://arxiv.org/abs/2407.17229v2</link><description>Generating landscape paintings expands the possibilities of artisticcreativity and imagination. Traditional landscape painting methods involveusing ink or colored ink on rice paper, which requires substantial time andeffort. These methods are susceptible to errors and inconsistencies and lackprecise control over lines and colors. This paper presents LPGen, ahigh-fidelity, controllable model for landscape painting generation,introducing a novel multi-modal framework that integrates image prompts intothe diffusion model. We extract its edges and contours by computing canny edgesfrom the target landscape image. These, along with natural language textprompts and drawing style references, are fed into the latent diffusion modelas conditions. We implement a decoupled cross-attention strategy to ensurecompatibility between image and text prompts, facilitating multi-modal imagegeneration. A decoder generates the final image. Quantitative and qualitativeanalyses demonstrate that our method outperforms existing approaches inlandscape painting generation and exceeds the current state-of-the-art. TheLPGen network effectively controls the composition and color of landscapepaintings, generates more accurate images, and supports further research indeep learning-based landscape painting generation.</description><author>Wanggong Yang, Xiaona Wang, Yingrui Qiu, Yifei Zhao</author><pubDate>Thu, 25 Jul 2024 09:29:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17229v2</guid></item><item><title>LangOcc: Self-Supervised Open Vocabulary Occupancy Estimation via Volume Rendering</title><link>http://arxiv.org/abs/2407.17310v2</link><description>The 3D occupancy estimation task has become an important challenge in thearea of vision-based autonomous driving recently. However, most existingcamera-based methods rely on costly 3D voxel labels or LiDAR scans fortraining, limiting their practicality and scalability. Moreover, most methodsare tied to a predefined set of classes which they can detect. In this work wepresent a novel approach for open vocabulary occupancy estimation calledLangOcc, that is trained only via camera images, and can detect arbitrarysemantics via vision-language alignment. In particular, we distill theknowledge of the strong vision-language aligned encoder CLIP into a 3Doccupancy model via differentiable volume rendering. Our model estimatesvision-language aligned features in a 3D voxel grid using only images. It istrained in a self-supervised manner by rendering our estimations back to 2Dspace, where ground-truth features can be computed. This training mechanismautomatically supervises the scene geometry, allowing for a straight-forwardand powerful training method without any explicit geometry supervision. LangOccoutperforms LiDAR-supervised competitors in open vocabulary occupancy by alarge margin, solely relying on vision-based training. We also achievestate-of-the-art results in self-supervised semantic occupancy estimation onthe Occ3D-nuScenes dataset, despite not being limited to a specific set ofcategories, thus demonstrating the effectiveness of our proposedvision-language training.</description><author>Simon Boeder, Fabian Gigengack, Benjamin Risse</author><pubDate>Thu, 25 Jul 2024 07:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17310v2</guid></item><item><title>Behavioral Testing: Can Large Language Models Implicitly Resolve Ambiguous Entities?</title><link>http://arxiv.org/abs/2407.17125v2</link><description>One of the major aspects contributing to the striking performance of largelanguage models (LLMs) is the vast amount of factual knowledge accumulatedduring pre-training. Yet, many LLMs suffer from self-inconsistency, whichraises doubts about their trustworthiness and reliability. In this paper, wefocus on entity type ambiguity and analyze current state-of-the-art LLMs fortheir proficiency and consistency in applying their factual knowledge whenprompted for entities under ambiguity. To do so, we propose an evaluationprotocol that disentangles knowing from applying knowledge, and teststate-of-the-art LLMs on 49 entities. Our experiments reveal that LLMs performpoorly with ambiguous prompts, achieving only 80% accuracy. Our results furtherdemonstrate systematic discrepancies in LLM behavior and their failure toconsistently apply information, indicating that the models can exhibitknowledge without being able to utilize it, significant biases for preferredreadings, as well as self inconsistencies. Our study highlights the importanceof handling entity ambiguity in future for more trustworthy LLMs</description><author>Anastasiia Sedova, Robert Litschko, Diego Frassinelli, Benjamin Roth, Barbara Plank</author><pubDate>Thu, 25 Jul 2024 07:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17125v2</guid></item><item><title>Domain Generalized Recaptured Screen Image Identification Using SWIN Transformer</title><link>http://arxiv.org/abs/2407.17170v2</link><description>An increasing number of classification approaches have been developed toaddress the issue of image rebroadcast and recapturing, a standard attackstrategy in insurance frauds, face spoofing, and video piracy. However, most ofthem neglected scale variations and domain generalization scenarios, performingpoorly in instances involving domain shifts, typically made worse byinter-domain and cross-domain scale variances. To overcome these issues, wepropose a cascaded data augmentation and SWIN transformer domain generalizationframework (DAST-DG) in the current research work Initially, we examine thedisparity in dataset representation. A feature generator is trained to makeauthentic images from various domains indistinguishable. This process is thenapplied to recaptured images, creating a dual adversarial learning setup.Extensive experiments demonstrate that our approach is practical and surpassesstate-of-the-art methods across different databases. Our model achieves anaccuracy of approximately 82\% with a precision of 95\% on high-variancedatasets.</description><author>Preeti Mehta, Aman Sagar, Suchi Kumari</author><pubDate>Thu, 25 Jul 2024 06:40:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17170v2</guid></item><item><title>Statistical Batch-Based Bearing Fault Detection</title><link>http://arxiv.org/abs/2407.17236v2</link><description>In the domain of rotating machinery, bearings are vulnerable to differentmechanical faults, including ball, inner, and outer race faults. Varioustechniques can be used in condition-based monitoring, from classical signalanalysis to deep learning methods. Based on the complex working conditions ofrotary machines, multivariate statistical process control charts such asHotelling's $T^2$ and Squared Prediction Error are useful for providing earlywarnings. However, these methods are rarely applied to condition monitoring ofrotating machinery due to the univariate nature of the datasets. In the presentpaper, we propose a multivariate statistical process control-based faultdetection method that utilizes multivariate data composed of Fourier transformfeatures extracted for fixed-time batches. Our approach makes use of themultidimensional nature of Fourier transform characteristics, which record moredetailed information about the machine's status, in an effort to enhance earlydefect detection and diagnosis. Experiments with varying vibration measurementlocations (Fan End, Drive End), fault types (ball, inner, and outer racefaults), and motor loads (0-3 horsepower) are used to validate the suggestedapproach. The outcomes illustrate our method's effectiveness in fault detectionand point to possible broader uses in industrial maintenance.</description><author>Victoria Jorry, Zina-Sabrina Duma, Tuomas Sihvonen, Satu-Pia Reinikainen, Lassi Roininen</author><pubDate>Thu, 25 Jul 2024 06:21:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17236v2</guid></item><item><title>$A^*$ for Graphs of Convex Sets</title><link>http://arxiv.org/abs/2407.17413v2</link><description>We present a novel algorithm that fuses the existing convex-programming basedapproach with heuristic information to find optimality guarantees andnear-optimal paths for the Shortest Path Problem in the Graph of Convex Sets(SPP-GCS). Our method, inspired by $A^*$, initiates a best-first-like procedurefrom a designated subset of vertices and iteratively expands it until furthergrowth is neither possible nor beneficial. Traditionally, obtaining solutionswith bounds for an optimization problem involves solving a relaxation,modifying the relaxed solution to a feasible one, and then comparing the twosolutions to establish bounds. However, for SPP-GCS, we demonstrate thatreversing this process can be more advantageous, especially with Euclideantravel costs. In other words, we initially employ $A^*$ to find a feasiblesolution for SPP-GCS, then solve a convex relaxation restricted to the verticesexplored by $A^*$ to obtain a relaxed solution, and finally, compare thesolutions to derive bounds. We present numerical results to highlight theadvantages of our algorithm over the existing approach in terms of the sizes ofthe convex programs solved and computation time.</description><author>Kaarthik Sundar, Sivakumar Rathinam</author><pubDate>Thu, 25 Jul 2024 02:10:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17413v2</guid></item><item><title>SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency</title><link>http://arxiv.org/abs/2407.17470v1</link><description>We present Stable Video 4D (SV4D), a latent video diffusion model formulti-frame and multi-view consistent dynamic 3D content generation. Unlikeprevious methods that rely on separately trained generative models for videogeneration and novel view synthesis, we design a unified diffusion model togenerate novel view videos of dynamic 3D objects. Specifically, given amonocular reference video, SV4D generates novel views for each video frame thatare temporally consistent. We then use the generated novel view videos tooptimize an implicit 4D representation (dynamic NeRF) efficiently, without theneed for cumbersome SDS-based optimization used in most prior works. To trainour unified novel view video generation model, we curated a dynamic 3D objectdataset from the existing Objaverse dataset. Extensive experimental results onmultiple datasets and user studies demonstrate SV4D's state-of-the-artperformance on novel-view video synthesis as well as 4D generation compared toprior works.</description><author>Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, Varun Jampani</author><pubDate>Wed, 24 Jul 2024 17:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17470v1</guid></item><item><title>I Could've Asked That: Reformulating Unanswerable Questions</title><link>http://arxiv.org/abs/2407.17469v1</link><description>When seeking information from unfamiliar documents, users frequently posequestions that cannot be answered by the documents. While existing largelanguage models (LLMs) identify these unanswerable questions, they do notassist users in reformulating their questions, thereby reducing their overallutility. We curate CouldAsk, an evaluation benchmark composed of existing andnew datasets for document-grounded question answering, specifically designed tostudy reformulating unanswerable questions. We evaluate state-of-the-artopen-source and proprietary LLMs on CouldAsk. The results demonstrate thelimited capabilities of these models in reformulating questions. Specifically,GPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of thetime, respectively. Error analysis shows that 62% of the unsuccessfulreformulations stem from the models merely rephrasing the questions or evengenerating identical questions. We publicly release the benchmark and the codeto reproduce the experiments.</description><author>Wenting Zhao, Ge Gao, Claire Cardie, Alexander M. Rush</author><pubDate>Wed, 24 Jul 2024 17:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17469v1</guid></item><item><title>WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries</title><link>http://arxiv.org/abs/2407.17468v1</link><description>While hallucinations of large language models (LLMs) prevail as a majorchallenge, existing evaluation benchmarks on factuality do not cover thediverse domains of knowledge that the real-world users of LLMs seek informationabout. To bridge this gap, we introduce WildHallucinations, a benchmark thatevaluates factuality. It does so by prompting LLMs to generate informationabout entities mined from user-chatbot conversations in the wild. Thesegenerations are then automatically fact-checked against a systematicallycurated knowledge source collected from web search. Notably, half of thesereal-world entities do not have associated Wikipedia pages. We evaluate 118,785generations from 15 LLMs on 7,919 entities. We find that LLMs consistentlyhallucinate more on entities without Wikipedia pages and exhibit varyinghallucination rates across different domains. Finally, given the same basemodels, adding a retrieval component only slightly reduces hallucinations butdoes not eliminate hallucinations.</description><author>Wenting Zhao, Tanya Goyal, Yu Ying Chiu, Liwei Jiang, Benjamin Newman, Abhilasha Ravichander, Khyathi Chandu, Ronan Le Bras, Claire Cardie, Yuntian Deng, Yejin Choi</author><pubDate>Wed, 24 Jul 2024 17:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17468v1</guid></item><item><title>CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models</title><link>http://arxiv.org/abs/2407.17467v1</link><description>Large Language Models (LLMs) excel in diverse tasks but often underperform inspecialized fields due to limited domain-specific or proprietary corpus.Continual pre-training (CPT) enhances LLM capabilities by imbuing newdomain-specific or proprietary knowledge while replaying general corpus toprevent catastrophic forgetting. The data mixture ratio of general corpus anddomain-specific corpus, however, has been chosen heuristically, leading tosub-optimal training efficiency in practice. In this context, we attempt tore-visit the scaling behavior of LLMs under the hood of CPT, and discover apower-law relationship between loss, mixture ratio, and training tokens scale.We formalize the trade-off between general and domain-specific capabilities,leading to a well-defined Critical Mixture Ratio (CMR) of general and domaindata. By striking the balance, CMR maintains the model's general ability andachieves the desired domain transfer, ensuring the highest utilization ofavailable resources. Therefore, if we value the balance between efficiency andeffectiveness, CMR can be consider as the optimal mixture ratio.Throughextensive experiments, we ascertain the predictability of CMR, and propose CMRscaling law and have substantiated its generalization. These findings offerpractical guidelines for optimizing LLM training in specialized domains,ensuring both general and domain-specific performance while efficientlymanaging training resources.</description><author>Jiawei Gu, Zacc Yang, Chuanghao Ding, Rui Zhao, Fei Tan</author><pubDate>Wed, 24 Jul 2024 17:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17467v1</guid></item><item><title>Traversing Pareto Optimal Policies: Provably Efficient Multi-Objective Reinforcement Learning</title><link>http://arxiv.org/abs/2407.17466v1</link><description>This paper investigates multi-objective reinforcement learning (MORL), whichfocuses on learning Pareto optimal policies in the presence of multiple rewardfunctions. Despite MORL's significant empirical success, there is still a lackof satisfactory understanding of various MORL optimization targets andefficient learning algorithms. Our work offers a systematic analysis of severaloptimization targets to assess their abilities to find all Pareto optimalpolicies and controllability over learned policies by the preferences fordifferent objectives. We then identify Tchebycheff scalarization as a favorablescalarization method for MORL. Considering the non-smoothness of Tchebycheffscalarization, we reformulate its minimization problem into a new min-max-maxoptimization problem. Then, for the stochastic policy class, we proposeefficient algorithms using this reformulation to learn Pareto optimal policies.We first propose an online UCB-based algorithm to achieve an $\varepsilon$learning error with an $\tilde{\mathcal{O}}(\varepsilon^{-2})$ samplecomplexity for a single given preference. To further reduce the cost ofenvironment exploration under different preferences, we propose apreference-free framework that first explores the environment withoutpre-defined preferences and then generates solutions for any number ofpreferences. We prove that it only requires an$\tilde{\mathcal{O}}(\varepsilon^{-2})$ exploration complexity in theexploration phase and demands no additional exploration afterward. Lastly, weanalyze the smooth Tchebycheff scalarization, an extension of Tchebycheffscalarization, which is proved to be more advantageous in distinguishing thePareto optimal policies from other weakly Pareto optimal policies based onentry values of preference vectors. Furthermore, we extend our algorithms andtheoretical analysis to accommodate this optimization target.</description><author>Shuang Qiu, Dake Zhang, Rui Yang, Boxiang Lyu, Tong Zhang</author><pubDate>Wed, 24 Jul 2024 17:58:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17466v1</guid></item><item><title>u-$μ$P: The Unit-Scaled Maximal Update Parametrization</title><link>http://arxiv.org/abs/2407.17465v1</link><description>The Maximal Update Parametrization ($\mu$P) aims to make the optimalhyperparameters (HPs) of a model independent of its size, allowing them to beswept using a cheap proxy model rather than the full-size target model. Wepresent a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it withUnit Scaling, a method for designing models that makes them easy to train inlow-precision. The two techniques have a natural affinity: $\mu$P ensures thatthe scale of activations is independent of model size, and Unit Scaling ensuresthat activations, weights and gradients begin training with a scale of one.This synthesis opens the door to a simpler scheme, whose default values arenear-optimal. This in turn facilitates a more efficient sweeping strategy, withu-$\mu$P models reaching a lower loss than comparable $\mu$P models and workingout-of-the-box in FP8.</description><author>Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y. Prince, Björn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, Douglas Orr</author><pubDate>Wed, 24 Jul 2024 17:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17465v1</guid></item><item><title>SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning</title><link>http://arxiv.org/abs/2407.17460v1</link><description>Reinforcement Learning (RL) has enabled social robots to generatetrajectories without human-designed rules or interventions, which makes it moreeffective than hard-coded systems for generalizing to complex real-worldscenarios. However, social navigation is a safety-critical task that requiresrobots to avoid collisions with pedestrians while previous RL-based solutionsfall short in safety performance in complex environments. To enhance the safetyof RL policies, to the best of our knowledge, we propose the first algorithm,SoNIC, that integrates adaptive conformal inference (ACI) with constrainedreinforcement learning (CRL) to learn safe policies for social navigation. Morespecifically, our method augments RL observations with ACI-generatednonconformity scores and provides explicit guidance for agents to leverage theuncertainty metrics to avoid safety-critical areas by incorporating safetyconstraints with spatial relaxation. Our method outperforms state-of-the-artbaselines in terms of both safety and adherence to social norms by a largemargin and demonstrates much stronger robustness to out-of-distributionscenarios. Our code and video demos are available on our project website:https://sonic-social-nav.github.io/.</description><author>Jianpeng Yao, Xiaopan Zhang, Yu Xia, Zejin Wang, Amit K. Roy-Chowdhury, Jiachen Li</author><pubDate>Wed, 24 Jul 2024 17:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17460v1</guid></item><item><title>A Unified Framework for Model Editing</title><link>http://arxiv.org/abs/2403.14236v3</link><description>ROME and MEMIT are largely believed to be two different model editingalgorithms, with the major difference between them being the ability to performbatched edits. In this paper, we unify these two algorithms under a singleconceptual umbrella, optimizing for the same goal, which we call thepreservation-memorization objective. ROME uses an equality constraint tooptimize this objective to perform one edit at a time, whereas MEMIT employs amore flexible least-square constraint that allows for batched edits. Wegeneralize ROME and enable batched editing with equality constraint in the formof EMMET - an Equality-constrained Mass Model Editing algorithm forTransformers, a new batched memory-editing algorithm. EMMET can performbatched-edits up to a batch-size of 10,000, with very similar performance toMEMIT across multiple dimensions. With the introduction of EMMET, we trulyunify ROME and MEMIT and show that both algorithms are equivalent in terms oftheir optimization objective, their abilities (singular and batched editing),their model editing performance and their limitations.</description><author>Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli</author><pubDate>Wed, 24 Jul 2024 17:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14236v3</guid></item><item><title>Hidden or Inferred: Fair Learning-To-Rank with Unknown Demographics</title><link>http://arxiv.org/abs/2407.17459v1</link><description>As learning-to-rank models are increasingly deployed for decision-making inareas with profound life implications, the FairML community has been developingfair learning-to-rank (LTR) models. These models rely on the availability ofsensitive demographic features such as race or sex. However, in practice,regulatory obstacles and privacy concerns protect this data from collection anduse. As a result, practitioners may either need to promote fairness despite theabsence of these features or turn to demographic inference tools to attempt toinfer them. Given that these tools are fallible, this paper aims to furtherunderstand how errors in demographic inference impact the fairness performanceof popular fair LTR strategies. In which cases would it be better to keep suchdemographic attributes hidden from models versus infer them? We examine aspectrum of fair LTR strategies ranging from fair LTR with and withoutdemographic features hidden versus inferred to fairness-unaware LTR followed byfair re-ranking. We conduct a controlled empirical investigation modelingdifferent levels of inference errors by systematically perturbing the inferredsensitive attribute. We also perform three case studies with real-worlddatasets and popular open-source inference methods. Our findings reveal that asinference noise grows, LTR-based methods that incorporate fairnessconsiderations into the learning process may increase bias. In contrast, fairre-ranking strategies are more robust to inference errors. All source code,data, and experimental artifacts of our experimental study are available here:https://github.com/sewen007/hoiltr.git</description><author>Oluseun Olulana, Kathleen Cachel, Fabricio Murai, Elke Rundensteiner</author><pubDate>Wed, 24 Jul 2024 17:54:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17459v1</guid></item><item><title>EuroCropsML: A Time Series Benchmark Dataset For Few-Shot Crop Type Classification</title><link>http://arxiv.org/abs/2407.17458v1</link><description>We introduce EuroCropsML, an analysis-ready remote sensing machine learningdataset for time series crop type classification of agricultural parcels inEurope. It is the first dataset designed to benchmark transnational few-shotcrop type classification algorithms that supports advancements in algorithmicdevelopment and research comparability. It comprises 706 683 multi-classlabeled data points across 176 classes, featuring annual time series ofper-parcel median pixel values from Sentinel-2 L1C data for 2021, along withcrop type labels and spatial coordinates. Based on the open-source EuroCropscollection, EuroCropsML is publicly available on Zenodo.</description><author>Joana Reuss, Jan Macdonald, Simon Becker, Lorenz Richter, Marco Körner</author><pubDate>Wed, 24 Jul 2024 17:50:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17458v1</guid></item><item><title>Why Machines Can't Be Moral: Turing's Halting Problem and the Moral Limits of Artificial Intelligence</title><link>http://arxiv.org/abs/2407.16890v1</link><description>In this essay, I argue that explicit ethical machines, whose moral principlesare inferred through a bottom-up approach, are unable to replicate human-likemoral reasoning and cannot be considered moral agents. By utilizing AlanTuring's theory of computation, I demonstrate that moral reasoning iscomputationally intractable by these machines due to the halting problem. Iaddress the frontiers of machine ethics by formalizing moral problems into'algorithmic moral questions' and by exploring moral psychology's dual-processmodel. While the nature of Turing Machines theoretically allows artificialagents to engage in recursive moral reasoning, critical limitations areintroduced by the halting problem, which states that it is impossible topredict with certainty whether a computational process will halt. A thoughtexperiment involving a military drone illustrates this issue, showing that anartificial agent might fail to decide between actions due to the haltingproblem, which limits the agent's ability to make decisions in all instances,undermining its moral agency.</description><author>Massimo Passamonti</author><pubDate>Wed, 24 Jul 2024 17:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16890v1</guid></item><item><title>CSCPR: Cross-Source-Context Indoor RGB-D Place Recognition</title><link>http://arxiv.org/abs/2407.17457v1</link><description>We present a new algorithm, Cross-Source-Context Place Recognition (CSCPR),for RGB-D indoor place recognition that integrates global retrieval andreranking into a single end-to-end model. Unlike prior approaches thatprimarily focus on the RGB domain, CSCPR is designed to handle the RGB-D data.We extend the Context-of-Clusters (CoCs) for handling noisy colorized pointclouds and introduce two novel modules for reranking: the Self-Context Cluster(SCC) and Cross Source Context Cluster (CSCC), which enhance featurerepresentation and match query-database pairs based on local features,respectively. We also present two new datasets, ScanNetIPR and ARKitIPR. Ourexperiments demonstrate that CSCPR significantly outperforms state-of-the-artmodels on these datasets by at least 36.5% in Recall@1 at ScanNet-PR datasetand 44% in new datasets. Code and datasets will be released.</description><author>Jing Liang, Zhuo Deng, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Cheng-Hao Kuo, Arnie Sen, Dinesh Manocha</author><pubDate>Wed, 24 Jul 2024 17:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17457v1</guid></item><item><title>Automated Explanation Selection for Scientific Discovery</title><link>http://arxiv.org/abs/2407.17454v1</link><description>Automated reasoning is a key technology in the young but rapidly growingfield of Explainable Artificial Intelligence (XAI). Explanability helps buildtrust in artificial intelligence systems beyond their mere predictive accuracyand robustness. In this paper, we propose a cycle of scientific discovery thatcombines machine learning with automated reasoning for the generation and theselection of explanations. We present a taxonomy of explanation selectionproblems that draws on insights from sociology and cognitive science. Theseselection criteria subsume existing notions and extend them with newproperties.</description><author>Markus Iser</author><pubDate>Wed, 24 Jul 2024 17:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17454v1</guid></item><item><title>$VILA^2$: VILA Augmented VILA</title><link>http://arxiv.org/abs/2407.17453v1</link><description>Visual language models (VLMs) have rapidly progressed, driven by the successof large language models (LLMs). While model architectures and traininginfrastructures advance rapidly, data curation remains under-explored. Whendata quantity and quality become a bottleneck, existing work either directlycrawls more raw data from the Internet that does not have a guarantee of dataquality or distills from black-box commercial models (e.g., GPT-4V / Gemini)causing the performance upper bounded by that model. In this work, we introducea novel approach that includes a self-augment step and a specialist-augmentstep to iteratively improve data quality and model performance. In theself-augment step, a VLM recaptions its own pretraining data to enhance dataquality, and then retrains from scratch using this refined dataset to improvemodel performance. This process can iterate for several rounds. Onceself-augmentation saturates, we employ several specialist VLMs finetuned fromthe self-augmented VLM with domain-specific expertise, to further infusespecialist knowledge into the generalist VLM through task-oriented recaptioningand retraining. With the combined self-augmented and specialist-augmentedtraining, we introduce $VILA^2$ (VILA-augmented-VILA), a VLM family thatconsistently improves the accuracy on a wide range of tasks over prior art, andachieves new state-of-the-art results on MMMU leaderboard among open-sourcedmodels.</description><author>Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, Hongxu Yin</author><pubDate>Wed, 24 Jul 2024 17:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17453v1</guid></item><item><title>Looking at Model Debiasing through the Lens of Anomaly Detection</title><link>http://arxiv.org/abs/2407.17449v1</link><description>It is widely recognized that deep neural networks are sensitive to bias inthe data. This means that during training these models are likely to learnspurious correlations between data and labels, resulting in limitedgeneralization abilities and low performance. In this context, model debiasingapproaches can be devised aiming at reducing the model's dependency on suchunwanted correlations, either leveraging the knowledge of bias information ornot. In this work, we focus on the latter and more realistic scenario, showingthe importance of accurately predicting the bias-conflicting and bias-alignedsamples to obtain compelling performance in bias mitigation. On this ground, wepropose to conceive the problem of model bias from an out-of-distributionperspective, introducing a new bias identification method based on anomalydetection. We claim that when data is mostly biased, bias-conflicting samplescan be regarded as outliers with respect to the bias-aligned distribution inthe feature space of a biased model, thus allowing for precisely detecting themwith an anomaly detection method. Coupling the proposed bias identificationapproach with bias-conflicting data upsampling and augmentation in a two-stepstrategy, we reach state-of-the-art performance on synthetic and real benchmarkdatasets. Ultimately, our proposed approach shows that the data bias issue doesnot necessarily require complex debiasing methods, given that an accurate biasidentification procedure is defined.</description><author>Vito Paolo Pastore, Massimiliano Ciranni, Davide Marinelli, Francesca Odone, Vittorio Murino</author><pubDate>Wed, 24 Jul 2024 17:30:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17449v1</guid></item><item><title>Investigating Resource-efficient Neutron/Gamma Classification ML Models Targeting eFPGAs</title><link>http://arxiv.org/abs/2404.14436v2</link><description>There has been considerable interest and resulting progress in implementingmachine learning (ML) models in hardware over the last several years from theparticle and nuclear physics communities. A big driver has been the release ofthe Python package, hls4ml, which has enabled porting models specified andtrained using Python ML libraries to register transfer level (RTL) code. Sofar, the primary end targets have been commercial FPGAs or synthesized customblocks on ASICs. However, recent developments in open-source embedded FPGA(eFPGA) frameworks now provide an alternate, more flexible pathway forimplementing ML models in hardware. These customized eFPGA fabrics can beintegrated as part of an overall chip design. In general, the decision betweena fully custom, eFPGA, or commercial FPGA ML implementation will depend on thedetails of the end-use application. In this work, we explored the parameterspace for eFPGA implementations of fully-connected neural network (fcNN) andboosted decision tree (BDT) models using the task of neutron/gammaclassification with a specific focus on resource efficiency. We used datacollected using an AmBe sealed source incident on Stilbene, which was opticallycoupled to an OnSemi J-series SiPM to generate training and test data for thisstudy. We investigated relevant input features and the effects ofbit-resolution and sampling rate as well as trade-offs in hyperparameters forboth ML architectures while tracking total resource usage. The performancemetric used to track model performance was the calculated neutron efficiency ata gamma leakage of 10$^{-3}$. The results of the study will be used to aid thespecification of an eFPGA fabric, which will be integrated as part of a testchip.</description><author>Jyothisraj Johnson, Billy Boxer, Tarun Prakash, Carl Grace, Peter Sorensen, Mani Tripathi</author><pubDate>Wed, 24 Jul 2024 17:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14436v2</guid></item><item><title>Do Generative AI Models Output Harm while Representing Non-Western Cultures: Evidence from A Community-Centered Approach</title><link>http://arxiv.org/abs/2407.14779v2</link><description>Our research investigates the impact of Generative Artificial Intelligence(GAI) models, specifically text-to-image generators (T2Is), on therepresentation of non-Western cultures, with a focus on Indian contexts.Despite the transformative potential of T2Is in content creation, concerns havearisen regarding biases that may lead to misrepresentations andmarginalizations. Through a community-centered approach and grounded theoryanalysis of 5 focus groups from diverse Indian subcultures, we explore how T2Ioutputs to English prompts depict Indian culture and its subcultures,uncovering novel representational harms such as exoticism and culturalmisappropriation. These findings highlight the urgent need for inclusive andculturally sensitive T2I systems. We propose design guidelines informed by asociotechnical perspective, aiming to address these issues and contribute tothe development of more equitable and representative GAI technologies globally.Our work also underscores the necessity of adopting a community-centeredapproach to comprehend the sociotechnical dynamics of these models,complementing existing work in this space while identifying and addressing thepotential negative repercussions and harms that may arise when these models aredeployed on a global scale.</description><author>Sourojit Ghosh, Pranav Narayanan Venkit, Sanjana Gautam, Shomir Wilson, Aylin Caliskan</author><pubDate>Wed, 24 Jul 2024 17:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14779v2</guid></item><item><title>Fluent Student-Teacher Redteaming</title><link>http://arxiv.org/abs/2407.17447v1</link><description>Many publicly available language models have been safety tuned to reduce thelikelihood of toxic or liability-inducing text. Users or security analystsattempt to jailbreak or redteam these models with adversarial prompts whichcause compliance with requests. One attack method is to apply discreteoptimization techniques to the prompt. However, the resulting attack stringsare often gibberish text, easily filtered by defenders due to high measuredperplexity, and may fail for unseen tasks and/or well-tuned models. In thiswork, we improve existing algorithms (primarily GCG and BEAST) to developpowerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Ourtechnique centers around a new distillation-based approach that encourages thevictim model to emulate a toxified finetune, either in terms of outputprobabilities or internal activations. To encourage human-fluent attacks, weadd a multi-model perplexity penalty and a repetition penalty to the objective.We also enhance optimizer strength by allowing token insertions, token swaps,and token deletions and by using longer attack sequences. The resulting processis able to reliably jailbreak the most difficult target models with promptsthat appear similar to human-written prompts. On Advbench we achieve attacksuccess rates $&gt;93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, whilemaintaining model-measured perplexity $&lt;33$; we achieve $95$% attack successfor Phi-3, though with higher perplexity. We also find a universally-optimizedsingle fluent prompt that induces $&gt;88$% compliance on previously unseen tasksacross Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-boxmodels.</description><author>T. Ben Thompson, Michael Sklar</author><pubDate>Wed, 24 Jul 2024 17:23:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17447v1</guid></item><item><title>Fractional signature: a generalisation of the signature inspired by fractional calculus</title><link>http://arxiv.org/abs/2407.17446v1</link><description>In this paper, we propose a novel generalisation of the signature of a path,motivated by fractional calculus, which is able to describe the solutions oflinear Caputo controlled FDEs. We also propose another generalisation of thesignature, inspired by the previous one, but more convenient to use in machinelearning. Finally, we test this last signature in a toy application to theproblem of handwritten digit recognition, where significant improvements inaccuracy rates are observed compared to those of the original signature.</description><author>José Manuel Corcuera, Rubén Jiménez</author><pubDate>Wed, 24 Jul 2024 17:23:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17446v1</guid></item><item><title>AHMF: Adaptive Hybrid-Memory-Fusion Model for Driver Attention Prediction</title><link>http://arxiv.org/abs/2407.17442v1</link><description>Accurate driver attention prediction can serve as a critical reference forintelligent vehicles in understanding traffic scenes and making informeddriving decisions. Though existing studies on driver attention predictionimproved performance by incorporating advanced saliency detection techniques,they overlooked the opportunity to achieve human-inspired prediction byanalyzing driving tasks from a cognitive science perspective. During driving,drivers' working memory and long-term memory play crucial roles in scenecomprehension and experience retrieval, respectively. Together, they formsituational awareness, facilitating drivers to quickly understand the currenttraffic situation and make optimal decisions based on past driving experiences.To explicitly integrate these two types of memory, this paper proposes anAdaptive Hybrid-Memory-Fusion (AHMF) driver attention prediction model toachieve more human-like predictions. Specifically, the model first encodesinformation about specific hazardous stimuli in the current scene to formworking memories. Then, it adaptively retrieves similar situational experiencesfrom the long-term memory for final prediction. Utilizing domain adaptationtechniques, the model performs parallel training across multiple datasets,thereby enriching the accumulated driving experience within the long-termmemory module. Compared to existing models, our model demonstrates significantimprovements across various metrics on multiple public datasets, proving theeffectiveness of integrating hybrid memories in driver attention prediction.</description><author>Dongyang Xu, Qingfan Wang, Ji Ma, Xiangyun Zeng, Lei Chen</author><pubDate>Wed, 24 Jul 2024 17:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17442v1</guid></item><item><title>HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</title><link>http://arxiv.org/abs/2407.17438v1</link><description>Human image animation involves generating videos from a character photo,allowing user control and unlocking potential for video and movie production.While recent approaches yield impressive results using high-quality trainingdata, the inaccessibility of these datasets hampers fair and transparentbenchmarking. Moreover, these approaches prioritize 2D human motion andoverlook the significance of camera motions in videos, leading to limitedcontrol and unstable video generation.To demystify the training data, wepresent HumanVid, the first large-scale high-quality dataset tailored for humanimage animation, which combines crafted real-world and synthetic data. For thereal-world data, we compile a vast collection of copyright-free real-worldvideos from the internet. Through a carefully designed rule-based filteringstrategy, we ensure the inclusion of high-quality videos, resulting in acollection of 20K human-centric videos in 1080P resolution. Human and cameramotion annotation is accomplished using a 2D pose estimator and a SLAM-basedmethod. For the synthetic data, we gather 2,300 copyright-free 3D avatar assetsto augment existing available 3D assets. Notably, we introduce a rule-basedcamera trajectory generation method, enabling the synthetic pipeline toincorporate diverse and precise camera motion annotation, which can rarely befound in real-world data. To verify the effectiveness of HumanVid, we establisha baseline model named CamAnimate, short for Camera-controllable HumanAnimation, that considers both human and camera motions as conditions. Throughextensive experimentation, we demonstrate that such simple baseline training onour HumanVid achieves state-of-the-art performance in controlling both humanpose and camera motions, setting a new benchmark. Code and data will bepublicly available at \url{https://github.com/zhenzhiwang/HumanVid/}.</description><author>Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, Dahua Lin</author><pubDate>Wed, 24 Jul 2024 17:15:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17438v1</guid></item><item><title>Solving Deep Reinforcement Learning Tasks with Evolution Strategies and Linear Policy Networks</title><link>http://arxiv.org/abs/2402.06912v2</link><description>Although deep reinforcement learning methods can learn effective policies forchallenging problems such as Atari games and robotics tasks, algorithms arecomplex, and training times are often long. This study investigates howEvolution Strategies perform compared to gradient-based deep reinforcementlearning methods. We use Evolution Strategies to optimize the weights of aneural network via neuroevolution, performing direct policy search. Webenchmark both deep policy networks and networks consisting of a single linearlayer from observations to actions for three gradient-based methods, such asProximal Policy Optimization. These methods are evaluated against threeclassical Evolution Strategies and Augmented Random Search, which all uselinear policy networks. Our results reveal that Evolution Strategies can findeffective linear policies for many reinforcement learning benchmark tasks,unlike deep reinforcement learning methods that can only find successfulpolicies using much larger networks, suggesting that current benchmarks areeasier to solve than previously assumed. Interestingly, Evolution Strategiesalso achieve results comparable to gradient-based deep reinforcement learningalgorithms for higher-complexity tasks. Furthermore, we find that by directlyaccessing the memory state of the game, Evolution Strategies can findsuccessful policies in Atari that outperform the policies found by DeepQ-Learning. Evolution Strategies also outperform Augmented Random Search inmost benchmarks, demonstrating superior sample efficiency and robustness intraining linear policy networks.</description><author>Annie Wong, Jacob de Nobel, Thomas Bäck, Aske Plaat, Anna V. Kononova</author><pubDate>Wed, 24 Jul 2024 17:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06912v2</guid></item><item><title>Dissecting Language Models: Machine Unlearning via Selective Pruning</title><link>http://arxiv.org/abs/2403.01267v2</link><description>Understanding and shaping the behaviour of Large Language Models (LLMs) isincreasingly important as applications become more powerful and more frequentlyadopted. This paper introduces a machine unlearning method specificallydesigned for LLMs. We introduce a selective pruning method for LLMs thatremoves neurons based on their relative importance on a targeted capabilitycompared to overall network performance. This approach is a compute- anddata-efficient method for identifying and removing neurons that enable specificbehaviours. Our findings reveal that both feed-forward and attention neurons inLLMs are specialized; that is, for specific tasks, certain neurons are morecrucial than others. Code from all experiments is available athttps://github.com/nickypro/selective-pruning</description><author>Nicholas Pochinkov, Nandi Schoots</author><pubDate>Wed, 24 Jul 2024 17:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01267v2</guid></item><item><title>Nerva: a Truly Sparse Implementation of Neural Networks</title><link>http://arxiv.org/abs/2407.17437v1</link><description>We introduce Nerva, a fast neural network library under development in C++.It supports sparsity by using the sparse matrix operations of Intel's MathKernel Library (MKL), which eliminates the need for binary masks. We show thatNerva significantly decreases training time and memory usage while reachingequivalent accuracy to PyTorch. We run static sparse experiments with an MLP onCIFAR-10. On high sparsity levels like $99\%$, the runtime is reduced by afactor of $4\times$ compared to a PyTorch model using masks. Similar to otherpopular frameworks such as PyTorch and Keras, Nerva offers a Python interfacefor users to work with.</description><author>Wieger Wesselink, Bram Grooten, Qiao Xiao, Cassio de Campos, Mykola Pechenizkiy</author><pubDate>Wed, 24 Jul 2024 17:13:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17437v1</guid></item><item><title>Proof-of-Collaborative-Learning: A Multi-winner Federated Learning Consensus Algorithm</title><link>http://arxiv.org/abs/2407.13018v2</link><description>Regardless of their variations, blockchains require a consensus mechanism tovalidate transactions, supervise added blocks, maintain network security,synchronize the network state, and distribute incentives. Proof-of-Work (PoW),one of the most influential implementations of consensus mechanisms, consumesan extraordinary amount of energy for a task that lacks direct productiveoutput. In this paper, we propose Proof-of-Collaborative-Learning (PoCL), amulti-winner federated learning validated consensus mechanism that redirectsthe computation power of blockchains to train federated learning models. Inaddition, we present a novel evaluation mechanism to ensure the efficiency ofthe locally trained models of miners. We evaluated the security of ourevaluation mechanism by introducing and conducting probable attacks. Moreover,we present a novel reward distribution mechanism to incentivize winning minersfairly, and demonstrate that our reward system is fair both within and acrossall rounds.</description><author>Amirreza Sokhankhosh, Sara Rouhani</author><pubDate>Wed, 24 Jul 2024 17:04:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13018v2</guid></item><item><title>The Elements of Differentiable Programming</title><link>http://arxiv.org/abs/2403.14606v2</link><description>Artificial intelligence has recently experienced remarkable advances, fueledby large models, vast datasets, accelerated hardware, and, last but not least,the transformative power of differentiable programming. This new programmingparadigm enables end-to-end differentiation of complex computer programs(including those with control flows and data structures), making gradient-basedoptimization of program parameters possible. As an emerging paradigm,differentiable programming builds upon several areas of computer science andapplied mathematics, including automatic differentiation, graphical models,optimization and statistics. This book presents a comprehensive review of thefundamental concepts useful for differentiable programming. We adopt two mainperspectives, that of optimization and that of probability, with clearanalogies between the two. Differentiable programming is not merely thedifferentiation of programs, but also the thoughtful design of programsintended for differentiation. By making programs differentiable, we inherentlyintroduce probability distributions over their execution, providing a means toquantify the uncertainty associated with program outputs.</description><author>Mathieu Blondel, Vincent Roulet</author><pubDate>Wed, 24 Jul 2024 16:56:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14606v2</guid></item><item><title>SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams</title><link>http://arxiv.org/abs/2407.15708v2</link><description>The spike camera, with its high temporal resolution, low latency, and highdynamic range, addresses high-speed imaging challenges like motion blur. Itcaptures photons at each pixel independently, creating binary spike streamsrich in temporal information but challenging for image reconstruction. Currentalgorithms, both traditional and deep learning-based, still need to be improvedin the utilization of the rich temporal detail and the restoration of thedetails of the reconstructed image. To overcome this, we introduce SwinSpikeformer (SwinSF), a novel model for dynamic scene reconstruction from spikestreams. SwinSF is composed of Spike Feature Extraction, Spatial-TemporalFeature Extraction, and Final Reconstruction Module. It combines shifted windowself-attention and proposed temporal spike attention, ensuring a comprehensivefeature extraction that encapsulates both spatial and temporal dynamics,leading to a more robust and accurate reconstruction of spike streams.Furthermore, we build a new synthesized dataset for spike image reconstructionwhich matches the resolution of the latest spike camera, ensuring its relevanceand applicability to the latest developments in spike camera imaging.Experimental results demonstrate that the proposed network SwinSF sets a newbenchmark, achieving state-of-the-art performance across a series of datasets,including both real-world and synthesized data across various resolutions. Ourcodes and proposed dataset will be available soon.</description><author>Liangyan Jiang, Chuang Zhu, Yanxu Chen</author><pubDate>Wed, 24 Jul 2024 16:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15708v2</guid></item><item><title>Efficient Unbiased Sparsification</title><link>http://arxiv.org/abs/2402.14925v2</link><description>An unbiased $m$-sparsification of a vector $p\in \mathbb{R}^n$ is a randomvector $Q\in \mathbb{R}^n$ with mean $p$ that has at most $m&lt;n$ nonzerocoordinates. Unbiased sparsification compresses the original vector withoutintroducing bias; it arises in various contexts, such as in federated learningand sampling sparse probability distributions. Ideally, unbiased sparsificationshould also minimize the expected value of a divergence function$\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If$Q$ is optimal in this sense, then we call it efficient. Our main resultsdescribe efficient unbiased sparsifications for divergences that are eitherpermutation-invariant or additively separable. Surprisingly, thecharacterization for permutation-invariant divergences is robust to the choiceof divergence function, in the sense that our class of optimal $Q$ for squaredEuclidean distance coincides with our class of optimal $Q$ for Kullback-Leiblerdivergence, or indeed any of a wide variety of divergences.</description><author>Leighton Barnes, Stephen Cameron, Timothy Chow, Emma Cohen, Keith Frankston, Benjamin Howard, Fred Kochman, Daniel Scheinerman, Jeffrey VanderKam</author><pubDate>Wed, 24 Jul 2024 16:54:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14925v2</guid></item><item><title>3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities</title><link>http://arxiv.org/abs/2407.17418v1</link><description>3D Gaussian Splatting (3DGS) has emerged as a prominent technique with thepotential to become a mainstream method for 3D representations. It caneffectively transform multi-view images into explicit 3D Gaussianrepresentations through efficient training, and achieve real-time rendering ofnovel views. This survey aims to analyze existing 3DGS-related works frommultiple intersecting perspectives, including related tasks, technologies,challenges, and opportunities. The primary objective is to provide newcomerswith a rapid understanding of the field and to assist researchers inmethodically organizing existing technologies and challenges. Specifically, wedelve into the optimization, application, and extension of 3DGS, categorizingthem based on their focuses or motivations. Additionally, we summarize andclassify nine types of technical modules and corresponding improvementsidentified in existing works. Based on these analyses, we further examine thecommon challenges and technologies across various tasks, proposing potentialresearch opportunities.</description><author>Yanqi Bao, Tianyu Ding, Jing Huo, Yaoli Liu, Yuxin Li, Wenbin Li, Yang Gao, Jiebo Luo</author><pubDate>Wed, 24 Jul 2024 16:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17418v1</guid></item><item><title>Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?</title><link>http://arxiv.org/abs/2407.17417v1</link><description>Large Language Models (LLMs) have demonstrated impressive capabilities ingenerating diverse and contextually rich text. However, concerns regardingcopyright infringement arise as LLMs may inadvertently produce copyrightedmaterial. In this paper, we first investigate the effectiveness of watermarkingLLMs as a deterrent against the generation of copyrighted texts. Throughtheoretical analysis and empirical evaluation, we demonstrate thatincorporating watermarks into LLMs significantly reduces the likelihood ofgenerating copyrighted content, thereby addressing a critical concern in thedeployment of LLMs. Additionally, we explore the impact of watermarking onMembership Inference Attacks (MIAs), which aim to discern whether a sample waspart of the pretraining dataset and may be used to detect copyright violations.Surprisingly, we find that watermarking adversely affects the success rate ofMIAs, complicating the task of detecting copyrighted text in the pretrainingdataset. Finally, we propose an adaptive technique to improve the success rateof a recent MIA under watermarking. Our findings underscore the importance ofdeveloping adaptive methods to study critical problems in LLMs with potentiallegal implications.</description><author>Michael-Andrei Panaitescu-Liess, Zora Che, Bang An, Yuancheng Xu, Pankayaraj Pathmanathan, Souradip Chakraborty, Sicheng Zhu, Tom Goldstein, Furong Huang</author><pubDate>Wed, 24 Jul 2024 16:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17417v1</guid></item><item><title>Consent in Crisis: The Rapid Decline of the AI Data Commons</title><link>http://arxiv.org/abs/2407.14933v2</link><description>General-purpose artificial intelligence (AI) systems are built on massiveswathes of public web data, assembled into corpora such as C4, RefinedWeb, andDolma. To our knowledge, we conduct the first, large-scale, longitudinal auditof the consent protocols for the web domains underlying AI training corpora.Our audit of 14,000 web domains provides an expansive view of crawlable webdata and how codified data use preferences are changing over time. We observe aproliferation of AI-specific clauses to limit use, acute differences inrestrictions on AI developers, as well as general inconsistencies betweenwebsites' expressed intentions in their Terms of Service and their robots.txt.We diagnose these as symptoms of ineffective web protocols, not designed tocope with the widespread re-purposing of the internet for AI. Our longitudinalanalyses show that in a single year (2023-2024) there has been a rapidcrescendo of data restrictions from web sources, rendering ~5%+ of all tokensin C4, or 28%+ of the most actively maintained, critical sources in C4, fullyrestricted from use. For Terms of Service crawling restrictions, a full 45% ofC4 is now restricted. If respected or enforced, these restrictions are rapidlybiasing the diversity, freshness, and scaling laws for general-purpose AIsystems. We hope to illustrate the emerging crises in data consent, for bothdevelopers and creators. The foreclosure of much of the open web will impactnot only commercial AI, but also non-commercial AI and academic research.</description><author>Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, William Brannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, Kevin Klyman, Christopher Klamm, Hailey Schoelkopf, Nikhil Singh, Manuel Cherep, Ahmad Anis, An Dinh, Caroline Chitongo, Da Yin, Damien Sileo, Deividas Mataciunas, Diganta Misra, Emad Alghamdi, Enrico Shippole, Jianguo Zhang, Joanna Materzynska, Kun Qian, Kush Tiwary, Lester Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Vu Minh Chien, Xuhui Zhou, Yizhi Li, Caiming Xiong, Luis Villa, Stella Biderman, Hanlin Li, Daphne Ippolito, Sara Hooker, Jad Kabbara, Sandy Pentland</author><pubDate>Wed, 24 Jul 2024 16:52:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14933v2</guid></item><item><title>$A^*$ for Graphs of Convex Sets</title><link>http://arxiv.org/abs/2407.17413v1</link><description>We present a novel algorithm that fuses the existing convex-programming basedapproach with heuristic information to find optimality guarantees andnear-optimal paths for the Shortest Path Problem in the Graph of Convex Sets(SPP-GCS). Our method, inspired by $A^*$, initiates a best-first-like procedurefrom a designated subset of vertices and iteratively expands it until furthergrowth is neither possible nor beneficial. Traditionally, obtaining solutionswith bounds for an optimization problem involves solving a relaxation,modifying the relaxed solution to a feasible one, and then comparing the twosolutions to establish bounds. However, for SPP-GCS, we demonstrate thatreversing this process can be more advantageous, especially with Euclideantravel costs. In other words, we initially employ $A^*$ to find a feasiblesolution for SPP-GCS, then solve a convex relaxation restricted to the verticesexplored by $A^*$ to obtain a relaxed solution, and finally, compare thesolutions to derive bounds. We present numerical results to highlight theadvantages of our algorithm over the existing approach in terms of the sizes ofthe convex programs solved and computation time.</description><author>Kaarthik Sundar, Sivakumar Rathinam</author><pubDate>Wed, 24 Jul 2024 16:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17413v1</guid></item><item><title>(PASS) Visual Prompt Locates Good Structure Sparsity through a Recurrent HyperNetwork</title><link>http://arxiv.org/abs/2407.17412v1</link><description>Large-scale neural networks have demonstrated remarkable performance indifferent domains like vision and language processing, although at the cost ofmassive computation resources. As illustrated by compression literature,structural model pruning is a prominent algorithm to encourage modelefficiency, thanks to its acceleration-friendly sparsity patterns. One of thekey questions of structural pruning is how to estimate the channelsignificance. In parallel, work on data-centric AI has shown thatprompting-based techniques enable impressive generalization of large languagemodels across diverse downstream tasks. In this paper, we investigate acharming possibility - \textit{leveraging visual prompts to capture the channelimportance and derive high-quality structural sparsity}. To this end, wepropose a novel algorithmic framework, namely \texttt{PASS}. It is a tailoredhyper-network to take both visual prompts and network weight statistics asinput, and output layer-wise channel sparsity in a recurrent manner. Suchdesigns consider the intrinsic channel dependency between layers. Comprehensiveexperiments across multiple network architectures and six datasets demonstratethe superiority of \texttt{PASS} in locating good structural sparsity. Forexample, at the same FLOPs level, \texttt{PASS} subnetworks achieve $1\%\sim3\%$ better accuracy on Food101 dataset; or with a similar performance of$80\%$ accuracy, \texttt{PASS} subnetworks obtain $0.35\times$ more speedupthan the baselines.</description><author>Tianjin Huang, Fang Meng, Li Shen, Fan Liu, Yulong Pei, Mykola Pechenizkiy, Shiwei Liu, Tianlong Chen</author><pubDate>Wed, 24 Jul 2024 16:47:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17412v1</guid></item><item><title>Self-driving lab discovers principles for steering spontaneous emission</title><link>http://arxiv.org/abs/2407.16083v2</link><description>We developed an autonomous experimentation platform to accelerateinterpretable scientific discovery in ultrafast nanophotonics, targeting anovel method to steer spontaneous emission from reconfigurable semiconductormetasurfaces. Controlling spontaneous emission is crucial for clean-energysolutions in illumination, thermal radiation engineering, and remote sensing.Despite the potential of reconfigurable semiconductor metasurfaces withembedded sources for spatiotemporal control, achieving arbitrary far-fieldcontrol remains challenging. Here, we present a self-driving lab (SDL) platformthat addresses this challenge by discovering the governing equations forpredicting the far-field emission profile from light-emitting metasurfaces. Wediscover that both the spatial gradient (grating-like) and the curvature(lens-like) of the local refractive index are key factors in steeringspontaneous emission. The SDL employs a machine-learning framework comprising:(1) a variational autoencoder for generating complex spatial refractive indexprofiles, (2) an active learning agent for guiding experiments with real-timeclosed-loop feedback, and (3) a neural network-based equation learner touncover structure-property relationships. The SDL demonstrated a four-foldenhancement in peak emission directivity (up to 77%) over a 72{\deg} field ofview within ~300 experiments. Our findings reveal that combinations of positivegratings and lenses are as effective as negative lenses and gratings for allemission angles, offering a novel strategy for controlling spontaneous emissionbeyond conventional Fourier optics.</description><author>Saaketh Desai, Sadhvikas Addamane, Jeffery Y. Tsao, Igal Brener, Remi Dingreville, Prasad P. Iyer</author><pubDate>Wed, 24 Jul 2024 16:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16083v2</guid></item><item><title>Generation of Training Data from HD Maps in the Lanelet2 Framework</title><link>http://arxiv.org/abs/2407.17409v1</link><description>Using HD maps directly as training data for machine learning tasks has seen amassive surge in popularity and shown promising results, e.g. in the field ofmap perception. Despite that, a standardized HD map framework supporting allparts of map-based automated driving and training label generation from mapdata does not exist. Furthermore, feeding map perception models with map dataas part of the input during real-time inference is not addressed by theresearch community. In order to fill this gap, we presentlanelet2_ml_converter,an integrated extension to the HD map framework Lanelet2, widely used inautomated driving systems by academia and industry. With this addition Lanelet2unifies map based automated driving, machine learning inference and training,all from a single source of map data and format. Requirements for a unifiedframework are analyzed and the implementation of these requirements isdescribed. The usability of labels in state of the art machine learning isdemonstrated with application examples from the field of map perception. Thesource code is available embedded in the Lanelet2 framework underhttps://github.com/fzi-forschungszentrum-informatik/Lanelet2/tree/feature_ml_converter</description><author>Fabian Immel, Richard Fehler, Frank Bieder, Christoph Stiller</author><pubDate>Wed, 24 Jul 2024 16:43:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17409v1</guid></item><item><title>Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models</title><link>http://arxiv.org/abs/2407.17406v1</link><description>Syntactic Transformer language models aim to achieve better generalizationthrough simultaneously modeling syntax trees and sentences. While prior workhas been focusing on adding constituency-based structures to Transformers, weintroduce Dependency Transformer Grammars (DTGs), a new class of Transformerlanguage model with explicit dependency-based inductive bias. DTGs simulatedependency transition systems with constrained attention patterns by modifyingattention masks, incorporate the stack information through relative positionalencoding, and augment dependency arc representation with a combination of tokenembeddings and operation embeddings. When trained on a dataset of sentencesannotated with dependency trees, DTGs achieve better generalization whilemaintaining comparable perplexity with Transformer language model baselines.DTGs also outperform recent constituency-based models, showing that dependencycan better guide Transformer language models. Our code is released athttps://github.com/zhaoyd1/Dep_Transformer_Grammars.</description><author>Yida Zhao, Chao Lou, Kewei Tu</author><pubDate>Wed, 24 Jul 2024 16:38:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17406v1</guid></item><item><title>Grammar-based Game Description Generation using Large Language Models</title><link>http://arxiv.org/abs/2407.17404v1</link><description>To lower the barriers to game design development, automated game design,which generates game designs through computational processes, has beenexplored. In automated game design, machine learning-based techniques such asevolutionary algorithms have achieved success. Benefiting from the remarkableadvancements in deep learning, applications in computer vision and naturallanguage processing have progressed in level generation. However, due to thelimited amount of data in game design, the application of deep learning hasbeen insufficient for tasks such as game description generation. To pioneer anew approach for handling limited data in automated game design, we focus onthe in-context learning of large language models (LLMs). LLMs can capture thefeatures of a task from a few demonstration examples and apply the capabilitiesacquired during pre-training. We introduce the grammar of game descriptions,which effectively structures the game design space, into the LLMs' reasoningprocess. Grammar helps LLMs capture the characteristics of the complex task ofgame description generation. Furthermore, we propose a decoding method thatiteratively improves the generated output by leveraging the grammar. Ourexperiments demonstrate that this approach performs well in generating gamedescriptions.</description><author>Tsunehiko Tanaka, Edgar Simo-Serra</author><pubDate>Wed, 24 Jul 2024 16:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17404v1</guid></item><item><title>FDS: Feedback-guided Domain Synthesis with Multi-Source Conditional Diffusion Models for Domain Generalization</title><link>http://arxiv.org/abs/2407.03588v2</link><description>Domain Generalization techniques aim to enhance model robustness bysimulating novel data distributions during training, typically through variousaugmentation or stylization strategies. However, these methods frequentlysuffer from limited control over the diversity of generated images and lackassurance that these images span distinct distributions. To address thesechallenges, we propose FDS, Feedback-guided Domain Synthesis, a novel strategythat employs diffusion models to synthesize novel, pseudo-domains by training asingle model on all source domains and performing domain mixing based onlearned features. By incorporating images that pose classification challengesto models trained on original samples, alongside the original dataset, weensure the generation of a training set that spans a broad distributionspectrum. Our comprehensive evaluations demonstrate that this methodology setsnew benchmarks in domain generalization performance across a range ofchallenging datasets, effectively managing diverse types of domain shifts. Theimplementation is available at: \url{https://github.com/Mehrdad-Noori/FDS.git}.</description><author>Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Gustavo Adolfo Vargas Hakim, David Osowiechi, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers</author><pubDate>Wed, 24 Jul 2024 16:26:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03588v2</guid></item><item><title>Self-Calibrated Variance-Stabilizing Transformations for Real-World Image Denoising</title><link>http://arxiv.org/abs/2407.17399v1</link><description>Supervised deep learning has become the method of choice for image denoising.It involves the training of neural networks on large datasets composed of pairsof noisy and clean images. However, the necessity of training data that arespecific to the targeted application constrains the widespread use of denoisingnetworks. Recently, several approaches have been developed to overcome thisdifficulty by whether artificially generating realistic clean/noisy imagepairs, or training exclusively on noisy images. In this paper, we show that,contrary to popular belief, denoising networks specialized in the removal ofGaussian noise can be efficiently leveraged in favor of real-world imagedenoising, even without additional training. For this to happen, an appropriatevariance-stabilizing transform (VST) has to be applied beforehand. We proposean algorithm termed Noise2VST for the learning of such a model-free VST. Ourapproach requires only the input noisy image and an off-the-shelf Gaussiandenoiser. We demonstrate through extensive experiments the efficiency andsuperiority of Noise2VST in comparison to existing methods trained in theabsence of specific clean/noisy pairs.</description><author>Sébastien Herbreteau, Michael Unser</author><pubDate>Wed, 24 Jul 2024 16:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17399v1</guid></item><item><title>3D Question Answering for City Scene Understanding</title><link>http://arxiv.org/abs/2407.17398v1</link><description>3D multimodal question answering (MQA) plays a crucial role in sceneunderstanding by enabling intelligent agents to comprehend their surroundingsin 3D environments. While existing research has primarily focused on indoorhousehold tasks and outdoor roadside autonomous driving tasks, there has beenlimited exploration of city-level scene understanding tasks. Furthermore,existing research faces challenges in understanding city scenes, due to theabsence of spatial semantic information and human-environment interactioninformation at the city level.To address these challenges, we investigate 3DMQA from both dataset and method perspectives. From the dataset perspective, weintroduce a novel 3D MQA dataset named City-3DQA for city-level sceneunderstanding, which is the first dataset to incorporate scene semantic andhuman-environment interactive tasks within the city. From the methodperspective, we propose a Scene graph enhanced City-level Understanding method(Sg-CityU), which utilizes the scene graph to introduce the spatial semantic. Anew benchmark is reported and our proposed Sg-CityU achieves accuracy of 63.94% and 63.76 % in different settings of City-3DQA. Compared to indoor 3D MQAmethods and zero-shot using advanced large language models (LLMs), Sg-CityUdemonstrates state-of-the-art (SOTA) performance in robustness andgeneralization.</description><author>Penglei Sun, Yaoxian Song, Xiang Liu, Xiaofei Yang, Qiang Wang, Tiefeng Li, Yang Yang, Xiaowen Chu</author><pubDate>Wed, 24 Jul 2024 16:22:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17398v1</guid></item><item><title>MELTing point: Mobile Evaluation of Language Transformers</title><link>http://arxiv.org/abs/2403.12844v3</link><description>Transformers have revolutionized the machine learning landscape, graduallymaking their way into everyday tasks and equipping our computers with "sparksof intelligence". However, their runtime requirements have prevented them frombeing broadly deployed on mobile. As personal devices become increasinglypowerful and prompt privacy becomes an ever more pressing issue, we explore thecurrent state of mobile execution of Large Language Models (LLMs). To achievethis, we have created our own automation infrastructure, MELT, which supportsthe headless execution and benchmarking of LLMs on device, supporting differentmodels, devices and frameworks, including Android, iOS and Nvidia Jetsondevices. We evaluate popular instruction fine-tuned LLMs and leverage differentframeworks to measure their end-to-end and granular performance, tracing theirmemory and energy requirements along the way. Our analysis is the firstsystematic study of on-device LLM execution, quantifying performance, energyefficiency and accuracy across various state-of-the-art models and showcasesthe state of on-device intelligence in the era of hyperscale models. Resultshighlight the performance heterogeneity across targets and corroborates thatLLM inference is largely memory-bound. Quantization drastically reduces memoryrequirements and renders execution viable, but at a non-negligible accuracycost. Drawing from its energy footprint and thermal behavior, the continuousexecution of LLMs remains elusive, as both factors negatively affect userexperience. Last, our experience shows that the ecosystem is still in itsinfancy, and algorithmic as well as hardware breakthroughs can significantlyshift the execution cost. We expect NPU acceleration, and framework-hardwareco-design to be the biggest bet towards efficient standalone execution, withthe alternative of offloading tailored towards edge deployments.</description><author>Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi</author><pubDate>Wed, 24 Jul 2024 16:17:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12844v3</guid></item><item><title>Systematic Reasoning About Relational Domains With Graph Neural Networks</title><link>http://arxiv.org/abs/2407.17396v1</link><description>Developing models that can learn to reason is a notoriously challengingproblem. We focus on reasoning in relational domains, where the use of GraphNeural Networks (GNNs) seems like a natural choice. However, previous work onreasoning with GNNs has shown that such models tend to fail when presented withtest examples that require longer inference chains than those seen duringtraining. This suggests that GNNs lack the ability to generalize from trainingexamples in a systematic way, which would fundamentally limit their reasoningabilities. A common solution is to instead rely on neuro-symbolic methods,which are capable of reasoning in a systematic way by design. Unfortunately,the scalability of such methods is often limited and they tend to rely onoverly strong assumptions, e.g.\ that queries can be answered by inspecting asingle relational path. In this paper, we revisit the idea of reasoning withGNNs, showing that systematic generalization is possible as long as the rightinductive bias is provided. In particular, we argue that node embeddings shouldbe treated as epistemic states and that GNN should be parameterisedaccordingly. We propose a simple GNN architecture which is based on this viewand show that it is capable of achieving state-of-the-art results. Wefurthermore introduce a benchmark which requires models to aggregate evidencefrom multiple relational paths. We show that existing neuro-symbolic approachesfail on this benchmark, whereas our considered GNN model learns to reasonaccurately.</description><author>Irtaza Khalid, Steven Schockaert</author><pubDate>Wed, 24 Jul 2024 16:17:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17396v1</guid></item><item><title>Five reasons against assuming a data-generating distribution in Machine Learning</title><link>http://arxiv.org/abs/2407.17395v1</link><description>Machine Learning research, as most of Statistics, heavily relies on theconcept of a data-generating probability distribution. As data points arethought to be sampled from such a distribution, we can learn from observed dataabout this distribution and, thus, predict future data points drawn from it(with some probability of success). Drawing on scholarship across disciplines,we here argue that this framework is not always a good model. Not only do suchtrue probability distributions not exist; the framework can also be misleadingand obscure both the choices made and the goals pursued in machine learningpractice. We suggest an alternative framework that focuses on finitepopulations rather than abstract distributions; while classical learning theorycan be left almost unchanged, it opens new opportunities, especially to modelsampling. We compile these considerations into five reasons for modellingmachine learning -- in some settings -- with finite distributions rather thangenerative distributions, both to be more faithful to practice and to providenovel theoretical insights.</description><author>Benedikt Höltgen, Robert C. Williamson</author><pubDate>Wed, 24 Jul 2024 16:17:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17395v1</guid></item><item><title>Detecting Throat Cancer from Speech Signals using Machine Learning: A Scoping Literature Review</title><link>http://arxiv.org/abs/2307.09230v2</link><description>Introduction: Cases of throat cancer are rising worldwide. With survivaldecreasing significantly at later stages, early detection is vital. Artificialintelligence (AI) and machine learning (ML) have the potential to detect throatcancer from patient speech, facilitating earlier diagnosis and reducing theburden on overstretched healthcare systems. However, no comprehensive reviewhas explored the use of AI and ML for detecting throat cancer from speech. Thisreview aims to fill this gap by evaluating how these technologies perform andidentifying issues that need to be addressed in future research. Materials andMethods: We conducted a scoping literature review across three databases:Scopus,Web of Science, and PubMed. We included articles that classified speechusing machine learning and specified the inclusion of throat cancer patients intheir data. Articles were categorized based on whether they performed binary ormulti-class classification. Results: We found 27 articles fitting our inclusioncriteria, 12 performing binary classification, 13 performing multi-classclassification, and two that do both binary and multiclass classification. Themost common classification method used was neural networks, and the mostfrequently extracted feature was mel-spectrograms. We also documentedpre-processing methods and classifier performance. We compared each articleagainst the TRIPOD-AI checklist, which showed a significant lack of openscience, with only one article sharing code and only three using open-accessdata. Conclusion: Open-source code is essential for external validation andfurther development in this field. Our review indicates that no single methodor specific feature consistently outperforms others in detecting throat cancerfrom speech. Future research should focus on standardizing methodologies andimproving the reproducibility of results.</description><author>Mary Paterson, James Moor, Luisa Cutillo</author><pubDate>Wed, 24 Jul 2024 16:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09230v2</guid></item><item><title>CovScore: Evaluation of Multi-Document Abstractive Title Set Generation</title><link>http://arxiv.org/abs/2407.17390v1</link><description>This paper introduces CovScore, an automatic reference-less methodology forevaluating thematic title sets, extracted from a corpus of documents. Whilesuch extraction methods are widely used, evaluating their effectiveness remainsan open question. Moreover, some existing practices heavily rely on slow andlaborious human annotation procedures. Inspired by recently introducedLLM-based judge methods, we propose a novel methodology that decomposes qualityinto five main metrics along different aspects of evaluation. This framingsimplifies and expedites the manual evaluation process and enables automaticand independent LLM-based evaluation. As a test case, we apply our approach toa corpus of Holocaust survivor testimonies, motivated both by its relevance totitle set extraction and by the moral significance of this pursuit. We validatethe methodology by experimenting with naturalistic and synthetic title setgeneration systems and compare their performance with the methodology.</description><author>Itamar Trainin, Omri Abend</author><pubDate>Wed, 24 Jul 2024 16:14:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17390v1</guid></item><item><title>Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning</title><link>http://arxiv.org/abs/2406.06348v2</link><description>The aim in many sciences is to understand the mechanisms that underlie theobserved distribution of variables, starting from a set of initial hypotheses.Causal discovery allows us to infer mechanisms as sets of cause and effectrelationships in a generalized way -- without necessarily tailoring to aspecific domain. Causal discovery algorithms search over a structuredhypothesis space, defined by the set of directed acyclic graphs, to find thegraph that best explains the data. For high-dimensional problems, however, thissearch becomes intractable and scalable algorithms for causal discovery areneeded to bridge the gap. In this paper, we define a novel causal graphpartition that allows for divide-and-conquer causal discovery with theoreticalguarantees. We leverage the idea of a superstructure -- a set of learned orexisting candidate hypotheses -- to partition the search space. We prove undercertain assumptions that learning with a causal graph partition always yieldsthe Markov Equivalence Class of the true causal graph. We show our algorithmachieves comparable accuracy and a faster time to solution forbiologically-tuned synthetic networks and networks up to ${10^4}$ variables.This makes our method applicable to gene regulatory network inference and otherdomains with high-dimensional structured hypothesis spaces.</description><author>Ashka Shah, Adela DePavia, Nathaniel Hudson, Ian Foster, Rick Stevens</author><pubDate>Wed, 24 Jul 2024 16:13:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06348v2</guid></item><item><title>PERSONA: A Reproducible Testbed for Pluralistic Alignment</title><link>http://arxiv.org/abs/2407.17387v1</link><description>The rapid advancement of language models (LMs) necessitates robust alignmentwith diverse user values. However, current preference optimization approachesoften fail to capture the plurality of user opinions, instead reinforcingmajority viewpoints and marginalizing minority perspectives. We introducePERSONA, a reproducible test bed designed to evaluate and improve pluralisticalignment of LMs. We procedurally generate diverse user profiles from US censusdata, resulting in 1,586 synthetic personas with varied demographic andidiosyncratic attributes. We then generate a large-scale evaluation datasetcontaining 3,868 prompts and 317,200 feedback pairs obtained from our syntheticpersonas. Leveraging this dataset, we systematically evaluate LM capabilitiesin role-playing diverse users, verified through human judges, and theestablishment of both a benchmark, PERSONA Bench, for pluralistic alignmentapproaches as well as an extensive dataset to create new and future benchmarks.The full dataset and benchmarks are available here:https://www.synthlabs.ai/research/persona.</description><author>Louis Castricato, Nathan Lile, Rafael Rafailov, Jan-Philipp Fränken, Chelsea Finn</author><pubDate>Wed, 24 Jul 2024 16:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17387v1</guid></item><item><title>Causal modelling without counterfactuals and individualised effects</title><link>http://arxiv.org/abs/2407.17385v1</link><description>The most common approach to causal modelling is the potential outcomesframework due to Neyman and Rubin. In this framework, outcomes ofcounterfactual treatments are assumed to be well-defined. This metaphysicalassumption is often thought to be problematic yet indispensable. Theconventional approach relies not only on counterfactuals, but also on abstractnotions of distributions and assumptions of independence that are not directlytestable. In this paper, we construe causal inference as treatment-wisepredictions for finite populations where all assumptions are testable; thismeans that one can not only test predictions themselves (without anyfundamental problem), but also investigate sources of error when they fail. Thenew framework highlights the model-dependence of causal claims as well as thedifference between statistical and scientific inference.</description><author>Benedikt Höltgen, Robert C. Williamson</author><pubDate>Wed, 24 Jul 2024 16:07:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17385v1</guid></item><item><title>A Comprehensive Approach to Misspelling Correction with BERT and Levenshtein Distance</title><link>http://arxiv.org/abs/2407.17383v1</link><description>Writing, as an omnipresent form of human communication, permeates nearlyevery aspect of contemporary life. Consequently, inaccuracies or errors inwritten communication can lead to profound consequences, ranging from financiallosses to potentially life-threatening situations. Spelling mistakes, among themost prevalent writing errors, are frequently encountered due to variousfactors. This research aims to identify and rectify diverse spelling errors intext using neural networks, specifically leveraging the Bidirectional EncoderRepresentations from Transformers (BERT) masked language model. To achieve thisgoal, we compiled a comprehensive dataset encompassing both non-real-word andreal-word errors after categorizing different types of spelling mistakes.Subsequently, multiple pre-trained BERT models were employed. To ensure optimalperformance in correcting misspelling errors, we propose a combined approachutilizing the BERT masked language model and Levenshtein distance. The resultsfrom our evaluation data demonstrate that the system presented herein exhibitsremarkable capabilities in identifying and rectifying spelling mistakes, oftensurpassing existing systems tailored for the Persian language.</description><author>Amirreza Naziri, Hossein Zeinali</author><pubDate>Wed, 24 Jul 2024 16:07:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17383v1</guid></item><item><title>LLMmap: Fingerprinting For Large Language Models</title><link>http://arxiv.org/abs/2407.15847v2</link><description>We introduce LLMmap, a first-generation fingerprinting attack targeted atLLM-integrated applications. LLMmap employs an active fingerprinting approach,sending carefully crafted queries to the application and analyzing theresponses to identify the specific LLM model in use. With as few as 8interactions, LLMmap can accurately identify LLMs with over 95% accuracy. Moreimportantly, LLMmap is designed to be robust across different applicationlayers, allowing it to identify LLMs operating under various system prompts,stochastic sampling hyperparameters, and even complex generation frameworkssuch as RAG or Chain-of-Thought.</description><author>Dario Pasquini, Evgenios M. Kornaropoulos, Giuseppe Ateniese</author><pubDate>Wed, 24 Jul 2024 16:07:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15847v2</guid></item><item><title>2D and 3D Deep Learning Models for MRI-based Parkinson's Disease Classification: A Comparative Analysis of Convolutional Kolmogorov-Arnold Networks, Convolutional Neural Networks, and Graph Convolutional Networks</title><link>http://arxiv.org/abs/2407.17380v1</link><description>Early and accurate diagnosis of Parkinson's Disease (PD) remains challenging.This study compares deep learning architectures for MRI-based PDclassification, introducing the first three-dimensional (3D) implementation ofConvolutional Kolmogorov-Arnold Networks (ConvKANs), a new approach thatcombines convolution layers with adaptive, spline-based activations. Weevaluated Convolutional Neural Networks (CNNs), ConvKANs, and GraphConvolutional Networks (GCNs) using three open-source datasets; a total of 142participants (75 with PD and 67 age-matched healthy controls). For 2D analysis,we extracted 100 axial slices centred on the midbrain from each T1-weightedscan. For 3D analysis, we used the entire volumetric scans. ConvKANs integratelearnable B-spline functions with convolutional layers. GCNs represent MRI dataas graphs, theoretically capturing structural relationships that may beoverlooked by traditional approaches. Interpretability visualizations,including the first ConvKAN spline activation maps, and projections of graphnode embeddings, were depicted. ConvKANs demonstrated high performance acrossdatasets and dimensionalities, achieving the highest 2D AUROC (0.98) in onedataset and matching CNN peak 3D performance (1.00). CNN models performed well,while GCN models improved in 3D analyses, reaching up to 0.97 AUROC. 3Dimplementations yielded higher AUROC values compared to 2D counterparts acrossall models. ConvKAN implementation shows promise for MRI analysis in PDclassification, particularly in the context of early diagnosis. The improvementin 3D analyses highlights the value of volumetric data in capturing subtlePD-related changes. While MRI is not currently used for PD diagnosis, thesefindings suggest its potential as a component of a multimodal diagnosticapproach, especially for early detection.</description><author>Salil B Patel, Vicky Goh, James F FitzGerald, Chrystalina A Antoniades</author><pubDate>Wed, 24 Jul 2024 16:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17380v1</guid></item><item><title>MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view Human Performance Capture and Rendering</title><link>http://arxiv.org/abs/2403.18820v2</link><description>Faithful human performance capture and free-view rendering from sparse RGBobservations is a long-standing problem in Vision and Graphics. The mainchallenges are the lack of observations and the inherent ambiguities of thesetting, e.g. occlusions and depth ambiguity. As a result, radiance fields,which have shown great promise in capturing high-frequency appearance andgeometry details in dense setups, perform poorly when naively supervising themon sparse camera views, as the field simply overfits to the sparse-view inputs.To address this, we propose MetaCap, a method for efficient and high-qualitygeometry recovery and novel view synthesis given very sparse or even a singleview of the human. Our key idea is to meta-learn the radiance field weightssolely from potentially sparse multi-view videos, which can serve as a priorwhen fine-tuning them on sparse imagery depicting the human. This priorprovides a good network weight initialization, thereby effectively addressingambiguities in sparse-view capture. Due to the articulated structure of thehuman body and motion-induced surface deformations, learning such a prior isnon-trivial. Therefore, we propose to meta-learn the field weights in apose-canonicalized space, which reduces the spatial feature range and makesfeature learning more effective. Consequently, one can fine-tune our fieldparameters to quickly generalize to unseen poses, novel illumination conditionsas well as novel and sparse (even monocular) camera views. For evaluating ourmethod under different scenarios, we collect a new dataset, WildDynaCap, whichcontains subjects captured in, both, a dense camera dome and in-the-wild sparsecamera rigs, and demonstrate superior results compared to recentstate-of-the-art methods on, both, public and WildDynaCap dataset.</description><author>Guoxing Sun, Rishabh Dabral, Pascal Fua, Christian Theobalt, Marc Habermann</author><pubDate>Wed, 24 Jul 2024 16:04:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18820v2</guid></item><item><title>Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution</title><link>http://arxiv.org/abs/2405.00424v2</link><description>Ridge regression is an indispensable tool in big data analysis. Yet itsinherent bias poses a significant and longstanding challenge, compromising bothstatistical efficiency and scalability across various applications. To tacklethis critical issue, we introduce an iterative strategy to correct biaseffectively when the dimension $p$ is less than the sample size $n$. For $p&gt;n$,our method optimally mitigates the bias such that any remaining bias in theproposed de-biased estimator is unattainable through linear transformations ofthe response data. To address the remaining bias when $p&gt;n$, we employ aRidge-Screening (RS) method, producing a reduced model suitable for biascorrection. Crucially, under certain conditions, the true model is nestedwithin our selected one, highlighting RS as a novel variable selectionapproach. Through rigorous analysis, we establish the asymptotic properties andvalid inferences of our de-biased ridge estimators for both $p&lt;n$ and $p&gt;n$,where, both $p$ and $n$ may increase towards infinity, along with the number ofiterations. We further validate these results using simulated and real-worlddata examples. Our method offers a transformative solution to the biaschallenge in ridge regression inferences across various disciplines.</description><author>Zhaoxing Gao, Ruey S. Tsay</author><pubDate>Wed, 24 Jul 2024 15:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00424v2</guid></item><item><title>MMRA: A Benchmark for Multi-granularity Multi-image Relational Association</title><link>http://arxiv.org/abs/2407.17379v1</link><description>Given the remarkable success that large visual language models (LVLMs) haveachieved in image perception tasks, the endeavor to make LVMLs perceive theworld like humans is drawing increasing attention. Current multi-modalbenchmarks mainly focus on the objective fact or certain topic relatedpotential knowledge within a image, but overlook the associative relationsbetween multiple images. Therefore, we define a multi-image relationassociation task, and meticulously curate \textbf{MMRA} benchmark, a\textbf{M}ulti-granularity \textbf{M}ulti-image \textbf{R}elational\textbf{A}ssociation benchmark, consisted of \textbf{1026} samples. In order tosystematically and comprehensively evaluate mainstream LVLMs, we establish anassociational relation system among images that contain \textbf{11 subtasks}(e.g, UsageSimilarity, SubEvent, etc.) at two granularity levels (i.e.,"\textbf{image}" and "\textbf{entity}") according to the relations inConceptNet. Our experiments demonstrate that, on our MMRA benchmark, currentmainstream LVLMs all have their own advantages and disadvantages acrossdifferent subtasks. It is worth noting that, at the entity level, theperformance of all models is worse than that of them at the image level,indicating that the fine-grained multi-image perception task is stillchallenging for LVLMs. The tasks related to spatial perception are relativelydifficult for LVLMs to handle. Furthermore, we find that LVMLs exhibit a goodability to perceive image details, and the key to enhancing their multi-imageassociation capability is to strengthen the reasoning ability of their languagemodel component. All our codes and data are released athtt\url{https://github.com/Wusiwei0410/MMRA}.</description><author>Siwei Wu, Kang Zhu, Yu Bai, Yiming Liang, Yizhi Li, Haoning Wu, Jiaheng Liu, Ruibo Liu, Xingwei Qu, Xuxin Cheng, Ge Zhang, Wenhao Huang, Chenghua Lin</author><pubDate>Wed, 24 Jul 2024 15:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17379v1</guid></item><item><title>PrevPredMap: Exploring Temporal Modeling with Previous Predictions for Online Vectorized HD Map Construction</title><link>http://arxiv.org/abs/2407.17378v1</link><description>Temporal information is crucial for detecting occluded instances. Existingtemporal representations have progressed from BEV or PV features to morecompact query features. Compared to these aforementioned features, predictionsoffer the highest level of abstraction, providing explicit information. In thecontext of online vectorized HD map construction, this unique characteristic ofpredictions is potentially advantageous for long-term temporal modeling and theintegration of map priors. This paper introduces PrevPredMap, a pioneeringtemporal modeling framework that leverages previous predictions forconstructing online vectorized HD maps. We have meticulously crafted twoessential modules for PrevPredMap: the previous-predictions-based querygenerator and the dynamic-position-query decoder. Specifically, theprevious-predictions-based query generator is designed to separately encodedifferent types of information from previous predictions, which are theneffectively utilized by the dynamic-position-query decoder to generate currentpredictions. Furthermore, we have developed a dual-mode strategy to ensurePrevPredMap's robust performance across both single-frame and temporal modes.Extensive experiments demonstrate that PrevPredMap achieves state-of-the-artperformance on the nuScenes and Argoverse2 datasets. Code will be available athttps://github.com/pnnnnnnn/PrevPredMap.</description><author>Nan Peng, Xun Zhou, Mingming Wang, Xiaojun Yang, Songming Chen, Guisong Chen</author><pubDate>Wed, 24 Jul 2024 15:58:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17378v1</guid></item><item><title>Entropy Reweighted Conformal Classification</title><link>http://arxiv.org/abs/2407.17377v1</link><description>Conformal Prediction (CP) is a powerful framework for constructing predictionsets with guaranteed coverage. However, recent studies have shown thatintegrating confidence calibration with CP can lead to a degradation inefficiency. In this paper, We propose an adaptive approach that considers theclassifier's uncertainty and employs entropy-based reweighting to enhance theefficiency of prediction sets for conformal classification. Our experimentalresults demonstrate that this method significantly improves efficiency.</description><author>Rui Luo, Nicolo Colombo</author><pubDate>Wed, 24 Jul 2024 15:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17377v1</guid></item><item><title>Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts</title><link>http://arxiv.org/abs/2407.17374v1</link><description>In the evolving landscape of AI regulation, it is crucial for companies toconduct impact assessments and document their compliance through comprehensivereports. However, current reports lack grounding in regulations and often focuson specific aspects like privacy in relation to AI systems, without addressingthe real-world uses of these systems. Moreover, there is no systematic effortto design and evaluate these reports with both AI practitioners and AIcompliance experts. To address this gap, we conducted an iterative co-designprocess with 14 AI practitioners and 6 AI compliance experts and proposed atemplate for impact assessment reports grounded in the EU AI Act, NIST's AIRisk Management Framework, and ISO 42001 AI Management System. We evaluated thetemplate by producing an impact assessment report for an AI-based meetingcompanion at a major tech company. A user study with 8 AI practitioners fromthe same company and 5 AI compliance experts from industry and academiarevealed that our template effectively provides necessary information forimpact assessments and documents the broad impacts of AI systems. Participantsenvisioned using the template not only at the pre-deployment stage forcompliance but also as a tool to guide the design stage of AI uses.</description><author>Edyta Bogucka, Marios Constantinides, Sanja Šćepanović, Daniele Quercia</author><pubDate>Wed, 24 Jul 2024 15:53:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17374v1</guid></item><item><title>How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?</title><link>http://arxiv.org/abs/2404.03302v3</link><description>By leveraging the retrieval of information from external knowledge databases,Large Language Models (LLMs) exhibit enhanced capabilities for accomplishingmany knowledge-intensive tasks. However, due to the inherent flaws of currentretrieval systems, there might exist irrelevant information within thoseretrieving top-ranked passages. In this work, we present a comprehensiveinvestigation into the robustness of LLMs to different types of irrelevantinformation under various conditions. We initially introduce a framework toconstruct high-quality irrelevant information that ranges from semanticallyunrelated, partially related, and related to questions. Furthermore, ouranalysis demonstrates that the constructed irrelevant information not onlyscores highly on similarity metrics, being highly retrieved by existingsystems, but also bears semantic connections to the context. Our investigationreveals that current LLMs still face challenges in discriminating highlysemantically related information and can be easily distracted by theseirrelevant yet misleading content. Besides, we also find that current solutionsfor handling irrelevant information have limitations in improving therobustness of LLMs to such distractions. All the resources are available onGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.</description><author>Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao</author><pubDate>Wed, 24 Jul 2024 15:51:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03302v3</guid></item><item><title>Variation Spaces for Multi-Output Neural Networks: Insights on Multi-Task Learning and Network Compression</title><link>http://arxiv.org/abs/2305.16534v3</link><description>This paper introduces a novel theoretical framework for the analysis ofvector-valued neural networks through the development of vector-valuedvariation spaces, a new class of reproducing kernel Banach spaces. These spacesemerge from studying the regularization effect of weight decay in trainingnetworks with activations like the rectified linear unit (ReLU). This frameworkoffers a deeper understanding of multi-output networks and their function-spacecharacteristics. A key contribution of this work is the development of arepresenter theorem for the vector-valued variation spaces. This representertheorem establishes that shallow vector-valued neural networks are thesolutions to data-fitting problems over these infinite-dimensional spaces,where the network widths are bounded by the square of the number of trainingdata. This observation reveals that the norm associated with thesevector-valued variation spaces encourages the learning of features that areuseful for multiple tasks, shedding new light on multi-task learning withneural networks. Finally, this paper develops a connection between weight-decayregularization and the multi-task lasso problem. This connection leads to novelbounds for layer widths in deep networks that depend on the intrinsicdimensions of the training data representations. This insight not only deepensthe understanding of the deep network architectural requirements, but alsoyields a simple convex optimization method for deep neural network compression.The performance of this compression procedure is evaluated on variousarchitectures.</description><author>Joseph Shenouda, Rahul Parhi, Kangwook Lee, Robert D. Nowak</author><pubDate>Wed, 24 Jul 2024 15:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16534v3</guid></item><item><title>An Experimental Study on the Rashomon Effect of Balancing Methods in Imbalanced Classification</title><link>http://arxiv.org/abs/2405.01557v4</link><description>Predictive models may generate biased predictions when classifying imbalanceddatasets. This happens when the model favors the majority class, leading to lowperformance in accurately predicting the minority class. To address this issue,balancing or resampling methods are critical data-centric AI approaches in themodeling process to improve prediction performance. However, there have beendebates and questions about the functionality of these methods in recent years.In particular, many candidate models may exhibit very similar predictiveperformance, called the Rashomon effect, in model selection, and they may evenproduce different predictions for the same observations. Selecting one of thesemodels without considering the predictive multiplicity -- which is the case ofyielding conflicting models' predictions for any sample -- can result in blindselection. In this paper, the impact of balancing methods on predictivemultiplicity is examined using the Rashomon effect. It is crucial because theblind model selection in data-centric AI is risky from a set of approximatelyequally accurate models. This may lead to severe problems in model selection,validation, and explanation. To tackle this matter, we conducted real datasetexperiments to observe the impact of balancing methods on predictivemultiplicity through the Rashomon effect by using a newly proposed metricobscurity in addition to the existing ones: ambiguity and discrepancy. Ourfindings showed that balancing methods inflate the predictive multiplicity andyield varying results. To monitor the trade-off between the predictionperformance and predictive multiplicity for conducting the modeling processresponsibly, we proposed using the extended version of the performance-gainplot when balancing the training data.</description><author>Mustafa Cavus, Przemysław Biecek</author><pubDate>Wed, 24 Jul 2024 15:43:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01557v4</guid></item><item><title>ViPer: Visual Personalization of Generative Models via Individual Preference Learning</title><link>http://arxiv.org/abs/2407.17365v1</link><description>Different users find different images generated for the same promptdesirable. This gives rise to personalized image generation which involvescreating images aligned with an individual's visual preference. Currentgenerative models are, however, unpersonalized, as they are tuned to produceoutputs that appeal to a broad audience. Using them to generate images alignedwith individual users relies on iterative manual prompt engineering by the userwhich is inefficient and undesirable. We propose to personalize the imagegeneration process by first capturing the generic preferences of the user in aone-time process by inviting them to comment on a small selection of images,explaining why they like or dislike each. Based on these comments, we infer auser's structured liked and disliked visual attributes, i.e., their visualpreference, using a large language model. These attributes are used to guide atext-to-image model toward producing images that are tuned towards theindividual user's visual preference. Through a series of user studies and largelanguage model guided evaluations, we demonstrate that the proposed methodresults in generations that are well aligned with individual users' visualpreferences.</description><author>Sogand Salehi, Mahdi Shafiei, Teresa Yeo, Roman Bachmann, Amir Zamir</author><pubDate>Wed, 24 Jul 2024 15:42:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17365v1</guid></item><item><title>MuST: Multi-Scale Transformers for Surgical Phase Recognition</title><link>http://arxiv.org/abs/2407.17361v1</link><description>Phase recognition in surgical videos is crucial for enhancing computer-aidedsurgical systems as it enables automated understanding of sequential proceduralstages. Existing methods often rely on fixed temporal windows for videoanalysis to identify dynamic surgical phases. Thus, they struggle tosimultaneously capture short-, mid-, and long-term information necessary tofully understand complex surgical procedures. To address these issues, wepropose Multi-Scale Transformers for Surgical Phase Recognition (MuST), a novelTransformer-based approach that combines a Multi-Term Frame encoder with aTemporal Consistency Module to capture information across multiple temporalscales of a surgical video. Our Multi-Term Frame Encoder computesinterdependencies across a hierarchy of temporal scales by sampling sequencesat increasing strides around the frame of interest. Furthermore, we employ along-term Transformer encoder over the frame embeddings to further enhancelong-term reasoning. MuST achieves higher performance than previousstate-of-the-art methods on three different public benchmarks.</description><author>Alejandra Pérez, Santiago Rodríguez, Nicolás Ayobi, Nicolás Aparicio, Eugénie Dessevres, Pablo Arbeláez</author><pubDate>Wed, 24 Jul 2024 15:38:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17361v1</guid></item><item><title>Quantile Learn-Then-Test: Quantile-Based Risk Control for Hyperparameter Optimization</title><link>http://arxiv.org/abs/2407.17358v1</link><description>The increasing adoption of Artificial Intelligence (AI) in engineeringproblems calls for the development of calibration methods capable of offeringrobust statistical reliability guarantees. The calibration of black box AImodels is carried out via the optimization of hyperparameters dictatingarchitecture, optimization, and/or inference configuration. Prior work hasintroduced learn-then-test (LTT), a calibration procedure for hyperparameteroptimization (HPO) that provides statistical guarantees on average performancemeasures. Recognizing the importance of controlling risk-aware objectives inengineering contexts, this work introduces a variant of LTT that is designed toprovide statistical guarantees on quantiles of a risk measure. We illustratethe practical advantages of this approach by applying the proposed algorithm toa radio access scheduling problem.</description><author>Amirmohammad Farzaneh, Sangwoo Park, Osvaldo Simeone</author><pubDate>Wed, 24 Jul 2024 15:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17358v1</guid></item><item><title>Euler Characteristic Tools For Topological Data Analysis</title><link>http://arxiv.org/abs/2303.14040v3</link><description>In this article, we study Euler characteristic techniques in topological dataanalysis. Pointwise computing the Euler characteristic of a family ofsimplicial complexes built from data gives rise to the so-called Eulercharacteristic profile. We show that this simple descriptor achievestate-of-the-art performance in supervised tasks at a very low computationalcost. Inspired by signal analysis, we compute hybrid transforms of Eulercharacteristic profiles. These integral transforms mix Euler characteristictechniques with Lebesgue integration to provide highly efficient compressors oftopological signals. As a consequence, they show remarkable performances inunsupervised settings. On the qualitative side, we provide numerous heuristicson the topological and geometric information captured by Euler profiles andtheir hybrid transforms. Finally, we prove stability results for thesedescriptors as well as asymptotic guarantees in random settings.</description><author>Olympio Hacquard, Vadim Lebovici</author><pubDate>Wed, 24 Jul 2024 15:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14040v3</guid></item><item><title>Gradient-based inference of abstract task representations for generalization in neural networks</title><link>http://arxiv.org/abs/2407.17356v1</link><description>Humans and many animals show remarkably adaptive behavior and can responddifferently to the same input depending on their internal goals. The brain notonly represents the intermediate abstractions needed to perform a computationbut also actively maintains a representation of the computation itself (taskabstraction). Such separation of the computation and its abstraction isassociated with faster learning, flexible decision-making, and broadgeneralization capacity. We investigate if such benefits might extend to neuralnetworks trained with task abstractions. For such benefits to emerge, one needsa task inference mechanism that possesses two crucial abilities: First, theability to infer abstract task representations when no longer explicitlyprovided (task inference), and second, manipulate task representations to adaptto novel problems (task recomposition). To tackle this, we cast task inferenceas an optimization problem from a variational inference perspective and groundour approach in an expectation-maximization framework. We show that gradientsbackpropagated through a neural network to a task representation layer are anefficient heuristic to infer current task demands, a process we refer to asgradient-based inference (GBI). Further iterative optimization of the taskrepresentation layer allows for recomposing abstractions to adapt to novelsituations. Using a toy example, a novel image classifier, and a languagemodel, we demonstrate that GBI provides higher learning efficiency andgeneralization to novel tasks and limits forgetting. Moreover, we show that GBIhas unique advantages such as preserving information for uncertainty estimationand detecting out-of-distribution samples.</description><author>Ali Hummos, Felipe del Río, Brabeeba Mien Wang, Julio Hurtado, Cristian B. Calderon, Guangyu Robert Yang</author><pubDate>Wed, 24 Jul 2024 15:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17356v1</guid></item><item><title>Deep Spherical Superpixels</title><link>http://arxiv.org/abs/2407.17354v1</link><description>Over the years, the use of superpixel segmentation has become very popular invarious applications, serving as a preprocessing step to reduce data size byadapting to the content of the image, regardless of its semantic content. Whilethe superpixel segmentation of standard planar images, captured with a 90{\deg}field of view, has been extensively studied, there has been limited focus ondedicated methods to omnidirectional or spherical images, captured with a360{\deg} field of view. In this study, we introduce the first deeplearning-based superpixel segmentation approach tailored for omnidirectionalimages called DSS (for Deep Spherical Superpixels). Our methodology leverageson spherical CNN architectures and the differentiable K-means clusteringparadigm for superpixels, to generate superpixels that follow the sphericalgeometry. Additionally, we propose to use data augmentation techniquesspecifically designed for 360{\deg} images, enabling our model to efficientlylearn from a limited set of annotated omnidirectional data. Our extensivevalidation across two datasets demonstrates that taking into account theinherent circular geometry of such images into our framework improves thesegmentation performance over traditional and deep learning-based superpixelmethods. Our code is available online.</description><author>Rémi Giraud, Michaël Clément</author><pubDate>Wed, 24 Jul 2024 15:27:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17354v1</guid></item><item><title>Scalify: scale propagation for efficient low-precision LLM training</title><link>http://arxiv.org/abs/2407.17353v1</link><description>Low-precision formats such as float8 have been introduced in machine learningaccelerated hardware to improve computational efficiency for large languagemodels training and inference. Nevertheless, adoption by the ML community hasbeen slowed down by the complex, and sometimes brittle, techniques required tomatch higher precision training accuracy. In this work, we present Scalify, aend-to-end scale propagation paradigm for computational graphs, generalizingand formalizing existing tensor scaling methods. Experiment results show thatScalify supports out-of-the-box float8 matrix multiplication and gradientsrepresentation, as well as float16 optimizer state storage. Our JAXimplementation of Scalify is open-sourced athttps://github.com/graphcore-research/jax-scalify</description><author>Paul Balança, Sam Hosegood, Carlo Luschi, Andrew Fitzgibbon</author><pubDate>Wed, 24 Jul 2024 15:26:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17353v1</guid></item><item><title>MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms</title><link>http://arxiv.org/abs/2402.14154v2</link><description>Social media platforms are hubs for multimodal information exchange,encompassing text, images, and videos, making it challenging for machines tocomprehend the information or emotions associated with interactions in onlinespaces. Multimodal Large Language Models (MLLMs) have emerged as a promisingsolution to these challenges, yet they struggle to accurately interpret humanemotions and complex content such as misinformation. This paper introducesMM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding ofmultimodal social media content. MM-Soc compiles prominent multimodal datasetsand incorporates a novel large-scale YouTube tagging dataset, targeting a rangeof tasks from misinformation detection, hate speech detection, and socialcontext generation. Through our exhaustive evaluation on ten size-variants offour open-source MLLMs, we have identified significant performance disparities,highlighting the need for advancements in models' social understandingcapabilities. Our analysis reveals that, in a zero-shot setting, various typesof MLLMs generally exhibit difficulties in handling social media tasks.However, MLLMs demonstrate performance improvements post fine-tuning,suggesting potential pathways for improvement. Our code and data are availableat https://github.com/claws-lab/MMSoc.git.</description><author>Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, Srijan Kumar</author><pubDate>Wed, 24 Jul 2024 15:19:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14154v2</guid></item><item><title>Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching</title><link>http://arxiv.org/abs/2407.17349v1</link><description>With the introduction of large language models (LLMs), automatic mathreasoning has seen tremendous success. However, current methods primarily focuson providing solutions or using techniques like Chain-of-Thought to enhanceproblem-solving accuracy. In this paper, we focus on improving the capabilityof mathematics teaching via a Socratic teaching-based LLM(\texttt{SocraticLLM}), which guides learners toward profound thinking withclarity and self-discovery via conversation. We collect and release ahigh-quality mathematical teaching dataset, named \texttt{SocraticMATH}, whichprovides Socratic-style conversations of problems with extra knowledge. Also,we propose a knowledge-enhanced LLM as a strong baseline to generate reliableresponses with review, guidance/heuristic, rectification, and summarization.Experimental results show the great advantages of \texttt{SocraticLLM} bycomparing it with several strong generative models. The codes and datasets areavailable on \url{https://github.com/ECNU-ICALK/SocraticMath}.</description><author>Yuyang Ding, Hanglei Hu, Jie Zhou, Qin Chen, Bo Jiang, Liang He</author><pubDate>Wed, 24 Jul 2024 15:18:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17349v1</guid></item><item><title>Label Alignment and Reassignment with Generalist Large Language Model for Enhanced Cross-Domain Named Entity Recognition</title><link>http://arxiv.org/abs/2407.17344v1</link><description>Named entity recognition on the in-domain supervised and few-shot settingshave been extensively discussed in the NLP community and made significantprogress. However, cross-domain NER, a more common task in practical scenarios,still poses a challenge for most NER methods. Previous research efforts in thatarea primarily focus on knowledge transfer such as correlate label informationfrom source to target domains but few works pay attention to the problem oflabel conflict. In this study, we introduce a label alignment and reassignmentapproach, namely LAR, to address this issue for enhanced cross-domain namedentity recognition, which includes two core procedures: label alignment betweensource and target domains and label reassignment for type inference. Theprocess of label reassignment can significantly be enhanced by integrating withan advanced large-scale language model such as ChatGPT. We conduct an extensiverange of experiments on NER datasets involving both supervised and zero-shotscenarios. Empirical experimental results demonstrate the validation of ourmethod with remarkable performance under the supervised and zero-shotout-of-domain settings compared to SOTA methods.</description><author>Ke Bao, Chonghuan Yang</author><pubDate>Wed, 24 Jul 2024 15:13:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17344v1</guid></item><item><title>AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game</title><link>http://arxiv.org/abs/2407.16521v2</link><description>Strategic social deduction games serve as valuable testbeds for evaluatingthe understanding and inference skills of language models, offering crucialinsights into social science, artificial intelligence, and strategic gaming.This paper focuses on creating proxies of human behavior in simulatedenvironments, with Among Us utilized as a tool for studying simulated humanbehavior. The study introduces a text-based game environment, namedAmongAgents, that mirrors the dynamics of Among Us. Players act as crew membersaboard a spaceship, tasked with identifying impostors who are sabotaging theship and eliminating the crew. Within this environment, the behavior ofsimulated language agents is analyzed. The experiments involve diverse gamesequences featuring different configurations of Crewmates and Impostorpersonality archetypes. Our work demonstrates that state-of-the-art largelanguage models (LLMs) can effectively grasp the game rules and make decisionsbased on the current context. This work aims to promote further exploration ofLLMs in goal-oriented games with incomplete information and complex actionspaces, as these settings offer valuable opportunities to assess language modelperformance in socially driven scenarios.</description><author>Yizhou Chi, Lingjun Mao, Zineng Tang</author><pubDate>Wed, 24 Jul 2024 15:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16521v2</guid></item><item><title>Description-Based Text Similarity</title><link>http://arxiv.org/abs/2305.12517v5</link><description>Identifying texts with a given semantics is central for many informationseeking scenarios. Similarity search over vector embeddings appear to becentral to this ability, yet the similarity reflected in current textembeddings is corpus-driven, and is inconsistent and sub-optimal for many usecases. What, then, is a good notion of similarity for effective retrieval oftext? We identify the need to search for texts based on abstract descriptions oftheir content, and the corresponding notion of \emph{description basedsimilarity}. We demonstrate the inadequacy of current text embeddings andpropose an alternative model that significantly improves when used in standardnearest neighbor search. The model is trained using positive and negative pairssourced through prompting a LLM, demonstrating how data from LLMs can be usedfor creating new capabilities not immediately possible using the originalmodel.</description><author>Shauli Ravfogel, Valentina Pyatkin, Amir DN Cohen, Avshalom Manevich, Yoav Goldberg</author><pubDate>Wed, 24 Jul 2024 15:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12517v5</guid></item><item><title>Overview of AI-Debater 2023: The Challenges of Argument Generation Tasks</title><link>http://arxiv.org/abs/2407.14829v2</link><description>In this paper we present the results of the AI-Debater 2023 Challenge held bythe Chinese Conference on Affect Computing (CCAC 2023), and introduce therelated datasets. We organize two tracks to handle the argumentative generationtasks in different scenarios, namely, Counter-Argument Generation (Track 1) andClaim-based Argument Generation (Track 2). Each track is equipped with itsdistinct dataset and baseline model respectively. In total, 32 competing teamsregister for the challenge, from which we received 11 successful submissions.In this paper, we will present the results of the challenge and a summary ofthe systems, highlighting commonalities and innovations among participatingsystems. Datasets and baseline models of the AI-Debater 2023 Challenge havebeen already released and can be accessed through the official website of thechallenge.</description><author>Jiayu Lin, Guanrong Chen, Bojun Jin, Chenyang Li, Shutong Jia, Wancong Lin, Yang Sun, Yuhang He, Caihua Yang, Jianzhu Bao, Jipeng Wu, Wen Su, Jinglu Chen, Xinyi Li, Tianyu Chen, Mingjie Han, Shuaiwen Du, Zijian Wang, Jiyin Li, Fuzhong Suo, Hao Wang, Nuanchen Lin, Xuanjing Huang, Changjian Jiang, RuiFeng Xu, Long Zhang, Jiuxin Cao, Ting Jin, Zhongyu Wei</author><pubDate>Wed, 24 Jul 2024 15:09:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14829v2</guid></item><item><title>Mathematical programming algorithms for convex hull approximation with a hyperplane budget</title><link>http://arxiv.org/abs/2407.17341v1</link><description>We consider the following problem in computational geometry: given, in thed-dimensional real space, a set of points marked as positive and a set ofpoints marked as negative, such that the convex hull of the positive set doesnot intersect the negative set, find K hyperplanes that separate, if possible,all the positive points from the negative ones. That is, we search for a convexpolyhedron with at most K faces, containing all the positive points and nonegative point. The problem is known in the literature for pure convexpolyhedral approximation; our interest stems from its possible applications inconstraint learning, where points are feasible or infeasible solutions of aMixed Integer Program, and the K hyperplanes are linear constraints to befound. We cast the problem as an optimization one, minimizing the number ofnegative points inside the convex polyhedron, whenever exact separation cannotbe achieved. We introduce models inspired by support vector machines and wedesign two mathematical programming formulations with binary variables. Weexploit Dantzig-Wolfe decomposition to obtain extended formulations, and wedevise column generation algorithms with ad-hoc pricing routines. We comparecomputing time and separation error values obtained by all our approaches onsynthetic datasets, with number of points from hundreds up to a few thousands,showing our approaches to perform better than existing ones from theliterature. Furthermore, we observe that key computational differences arise,depending on whether the budget K is sufficient to completely separate thepositive points from the negative ones or not. On 8-dimensional instances (andover), existing convex hull algorithms become computational inapplicable, whileour algorithms allow to identify good convex hull approximations in minutes ofcomputation.</description><author>Michele Barbato, Alberto Ceselli, Rosario Messana</author><pubDate>Wed, 24 Jul 2024 15:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17341v1</guid></item><item><title>Preliminary study on artificial intelligence methods for cybersecurity threat detection in computer networks based on raw data packets</title><link>http://arxiv.org/abs/2407.17339v1</link><description>Most of the intrusion detection methods in computer networks are based ontraffic flow characteristics. However, this approach may not fully exploit thepotential of deep learning algorithms to directly extract features and patternsfrom raw packets. Moreover, it impedes real-time monitoring due to thenecessity of waiting for the processing pipeline to complete and introducesdependencies on additional software components. In this paper, we investigate deep learning methodologies capable ofdetecting attacks in real-time directly from raw packet data within networktraffic. We propose a novel approach where packets are stacked into windows andseparately recognised, with a 2D image representation suitable for processingwith computer vision models. Our investigation utilizes the CIC IDS-2017dataset, which includes both benign traffic and prevalent real-world attacks,providing a comprehensive foundation for our research.</description><author>Aleksander Ogonowski, Michał Żebrowski, Arkadiusz Ćwiek, Tobiasz Jarosiewicz, Konrad Klimaszewski, Adam Padee, Piotr Wasiuk, Michał Wójcik</author><pubDate>Wed, 24 Jul 2024 15:04:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17339v1</guid></item><item><title>Cascaded Light Propagation Volumes using Spherical Radial Basis Functions</title><link>http://arxiv.org/abs/2407.17336v1</link><description>This paper introduces a contribution made to one of the newest methods forsimulating indirect lighting in dynamic scenes , the cascaded light propagationvolumes . Our contribution consists on using Spherical Radial Basis Functionsinstead of Spherical Harmonic, since the first achieves much better resultswhen many coefficients are used. We explain how to integrate the SphericalRadial Basis Functions with the cascaded light propagation volumes, andevaluate our technique against the same implementation, but with Sphericalharmonics.</description><author>Ludovic Silvestre, João Pereira</author><pubDate>Wed, 24 Jul 2024 15:02:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17336v1</guid></item><item><title>QUACK: Quantum Aligned Centroid Kernel</title><link>http://arxiv.org/abs/2405.00304v2</link><description>Quantum computing (QC) seems to show potential for application in machinelearning (ML). In particular quantum kernel methods (QKM) exhibit promisingproperties for use in supervised ML tasks. However, a major disadvantage ofkernel methods is their unfavorable quadratic scaling with the number oftraining samples. Together with the limits imposed by currently availablequantum hardware (NISQ devices) with their low qubit coherence times, smallnumber of qubits, and high error rates, the use of QC in ML at an industriallyrelevant scale is currently impossible. As a small step in improving thepotential applications of QKMs, we introduce QUACK, a quantum kernel algorithmwhose time complexity scales linear with the number of samples during training,and independent of the number of training samples in the inference stage. Inthe training process, only the kernel entries for the samples and the centersof the classes are calculated, i.e. the maximum shape of the kernel for nsamples and c classes is (n, c). During training, the parameters of the quantumkernel and the positions of the centroids are optimized iteratively. In theinference stage, for every new sample the circuit is only evaluated for everycentroid, i.e. c times. We show that the QUACK algorithm nevertheless providessatisfactory results and can perform at a similar level as classical kernelmethods with quadratic scaling during training. In addition, our (simulated)algorithm is able to handle high-dimensional datasets such as MNIST with 784features without any dimensionality reduction.</description><author>Kilian Tscharke, Sebastian Issel, Pascal Debus</author><pubDate>Wed, 24 Jul 2024 15:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00304v2</guid></item><item><title>Natively neuromorphic LMU architecture for encoding-free SNN-based HAR on commercial edge devices</title><link>http://arxiv.org/abs/2407.04076v2</link><description>Neuromorphic models take inspiration from the human brain by adoptingbio-plausible neuron models to build alternatives to traditional MachineLearning (ML) and Deep Learning (DL) solutions. The scarce availability ofdedicated hardware able to actualize the emulation of brain-inspiredcomputation, which is otherwise only simulated, yet still hinders the wideadoption of neuromorphic computing for edge devices and embedded systems. Withthis premise, we adopt the perspective of neuromorphic computing forconventional hardware and we present the L2MU, a natively neuromorphic LegendreMemory Unit (LMU) which entirely relies on Leaky Integrate-and-Fire (LIF)neurons. Specifically, the original recurrent architecture of LMU has beenredesigned by modelling every constituent element with neural populations madeof LIF or Current-Based (CuBa) LIF neurons. To couple neuromorphic computingand off-the-shelf edge devices, we equipped the L2MU with an input module forthe conversion of real values into spikes, which makes it an encoding-freeimplementation of a Recurrent Spiking Neural Network (RSNN) able to directlywork with raw sensor signals on non-dedicated hardware. As a use case tovalidate our network, we selected the task of Human Activity Recognition (HAR).We benchmarked our L2MU on smartwatch signals from hand-oriented activities,deploying it on three different commercial edge devices in compressed versionstoo. The reported results remark the possibility of considering neuromorphicmodels not only in an exclusive relationship with dedicated hardware but alsoas a suitable choice to work with common sensors and devices.</description><author>Vittorio Fra, Benedetto Leto, Andrea Pignata, Enrico Macii, Gianvito Urgese</author><pubDate>Wed, 24 Jul 2024 14:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04076v2</guid></item><item><title>Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</title><link>http://arxiv.org/abs/2407.10969v3</link><description>We introduce, Q-Sparse, a simple yet effective approach to trainingsparsely-activated large language models (LLMs). Q-Sparse enables full sparsityof activations in LLMs which can bring significant efficiency gains ininference. This is achieved by applying top-K sparsification to the activationsand the straight-through-estimator to the training. We also introduce BlockQ-Sparse for batch training and inference. The key results from this work are,(1) Q-Sparse can achieve results comparable to those of baseline LLMs whilebeing much more efficient at inference time; (2) We present aninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse iseffective in different settings, including training-from-scratch,continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works forboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, thesynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides thecornerstone and a clear path to revolutionize the efficiency, including costand energy consumption, of future LLMs.</description><author>Hongyu Wang, Shuming Ma, Ruiping Wang, Furu Wei</author><pubDate>Wed, 24 Jul 2024 14:57:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10969v3</guid></item><item><title>Global and Local Confidence Based Fraud Detection Graph Neural Network</title><link>http://arxiv.org/abs/2407.17333v1</link><description>This paper presents the Global and Local Confidence Graph Neural Network(GLC-GNN), an innovative approach to graph-based anomaly detection thataddresses the challenges of heterophily and camouflage in fraudulentactivities. By introducing a prototype to encapsulate the global features of agraph and calculating a Global Confidence (GC) value for each node, GLC-GNNeffectively distinguishes between benign and fraudulent nodes. The modelleverages GC to generate attention values for message aggregation, enhancingits ability to capture both homophily and heterophily. Through extensiveexperiments on four open datasets, GLC-GNN demonstrates superior performanceover state-of-the-art models in accuracy and convergence speed, whilemaintaining a compact model size and expedited training process. Theintegration of global and local confidence measures in GLC-GNN offers a robustsolution for detecting anomalies in graphs, with significant implications forfraud detection across diverse domains.</description><author>Jiaxun Liu, Yue Tian, Guanjun Liu</author><pubDate>Wed, 24 Jul 2024 14:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17333v1</guid></item><item><title>Multi-label Cluster Discrimination for Visual Representation Learning</title><link>http://arxiv.org/abs/2407.17331v1</link><description>Contrastive Language Image Pre-training (CLIP) has recently demonstratedsuccess across various tasks due to superior feature representation empoweredby image-text contrastive learning. However, the instance discrimination methodused by CLIP can hardly encode the semantic structure of training data. Tohandle this limitation, cluster discrimination has been proposed throughiterative cluster assignment and classification. Nevertheless, most clusterdiscrimination approaches only define a single pseudo-label for each image,neglecting multi-label signals in the image. In this paper, we propose a novelMulti-Label Cluster Discrimination method named MLCD to enhance representationlearning. In the clustering step, we first cluster the large-scale LAION-400Mdataset into one million centers based on off-the-shelf embedding features.Considering that natural images frequently contain multiple visual objects orattributes, we select the multiple closest centers as auxiliary class labels.In the discrimination step, we design a novel multi-label classification loss,which elegantly separates losses from positive classes and negative classes,and alleviates ambiguity on decision boundary. We validate the proposedmulti-label cluster discrimination method with experiments on different scalesof models and pre-training datasets. Experimental results show that our methodachieves state-of-the-art performance on multiple downstream tasks includinglinear probe, zero-shot classification, and image-text retrieval.</description><author>Xiang An, Kaicheng Yang, Xiangzi Dai, Ziyong Feng, Jiankang Deng</author><pubDate>Wed, 24 Jul 2024 14:54:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17331v1</guid></item><item><title>Low dimensional representation of multi-patient flow cytometry datasets using optimal transport for minimal residual disease detection in leukemia</title><link>http://arxiv.org/abs/2407.17329v1</link><description>Representing and quantifying Minimal Residual Disease (MRD) in Acute MyeloidLeukemia (AML), a type of cancer that affects the blood and bone marrow, isessential in the prognosis and follow-up of AML patients. As traditionalcytological analysis cannot detect leukemia cells below 5\%, the analysis offlow cytometry dataset is expected to provide more reliable results. In thispaper, we explore statistical learning methods based on optimal transport (OT)to achieve a relevant low-dimensional representation of multi-patient flowcytometry measurements (FCM) datasets considered as high-dimensionalprobability distributions. Using the framework of OT, we justify the use of theK-means algorithm for dimensionality reduction of multiple large-scale pointclouds through mean measure quantization by merging all the data into a singlepoint cloud. After this quantization step, the visualization of the intra andinter-patients FCM variability is carried out by embedding low-dimensionalquantized probability measures into a linear space using either WassersteinPrincipal Component Analysis (PCA) through linearized OT or log-ratio PCA ofcompositional data. Using a publicly available FCM dataset and a FCM datasetfrom Bordeaux University Hospital, we demonstrate the benefits of our approachover the popular kernel mean embedding technique for statistical learning frommultiple high-dimensional probability distributions. We also highlight theusefulness of our methodology for low-dimensional projection and clusteringpatient measurements according to their level of MRD in AML from FCM. Inparticular, our OT-based approach allows a relevant and informativetwo-dimensional representation of the results of the FlowSom algorithm, astate-of-the-art method for the detection of MRD in AML using multi-patientFCM.</description><author>Erell Gachon, Jérémie Bigot, Elsa Cazelles, Aguirre Mimoun, Jean-Philippe Vial</author><pubDate>Wed, 24 Jul 2024 14:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17329v1</guid></item><item><title>DarSwin-Unet: Distortion Aware Encoder-Decoder Architecture</title><link>http://arxiv.org/abs/2407.17328v1</link><description>Wide-angle fisheye images are becoming increasingly common for perceptiontasks in applications such as robotics, security, and mobility (e.g. drones,avionics). However, current models often either ignore the distortions inwide-angle images or are not suitable to perform pixel-level tasks. In thispaper, we present an encoder-decoder model based on a radial transformerarchitecture that adapts to distortions in wide-angle lenses by leveraging thephysical characteristics defined by the radial distortion profile. In contrastto the original model, which only performs classification tasks, we introduce aU-Net architecture, DarSwin-Unet, designed for pixel level tasks. Furthermore,we propose a novel strategy that minimizes sparsity when sampling the image forcreating its input tokens. Our approach enhances the model capability to handlepixel-level tasks in wide-angle fisheye images, making it more effective forreal-world applications. Compared to other baselines, DarSwin-Unet achieves thebest results across different datasets, with significant gains when trained onbounded levels of distortions (very low, low, medium, and high) and tested onall, including out-of-distribution distortions. We demonstrate its performanceon depth estimation and show through extensive experiments that DarSwin-Unetcan perform zero-shot adaptation to unseen distortions of different wide-anglelenses.</description><author>Akshaya Athwale, Ichrak Shili, Émile Bergeron, Ola Ahmad, Jean-François Lalonde</author><pubDate>Wed, 24 Jul 2024 14:52:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17328v1</guid></item><item><title>Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population</title><link>http://arxiv.org/abs/2407.17324v1</link><description>Dementia, a debilitating neurological condition affecting millions worldwide,presents significant diagnostic challenges. In this work, we introduce a novelmethodology for the classification of demented and non-demented elderlypatients using 3D brain Magnetic Resonance Imaging (MRI) scans. Our approachfeatures a unique technique for selectively processing MRI slices, focusing onthe most relevant brain regions and excluding less informative sections. Thismethodology is complemented by a confidence-based classification committeecomposed of three custom deep learning models: Dem3D ResNet, Dem3D CNN, andDem3D EfficientNet. These models work synergistically to enhancedecision-making accuracy, leveraging their collective strengths. Tested on theOpen Access Series of Imaging Studies(OASIS) dataset, our method achieved animpressive accuracy of 94.12%, surpassing existing methodologies. Furthermore,validation on the Alzheimer's Disease Neuroimaging Initiative (ADNI) datasetconfirmed the robustness and generalizability of our approach. The use ofexplainable AI (XAI) techniques and comprehensive ablation studies furthersubstantiate the effectiveness of our techniques, providing insights into thedecision-making process and the importance of our methodology. This researchoffers a significant advancement in dementia diagnosis, providing a highlyaccurate and efficient tool for clinical applications.</description><author>Nikolaos Ntampakis, Konstantinos Diamantaras, Ioanna Chouvarda, Vasileios Argyriou, Panagiotis Sarigianndis</author><pubDate>Wed, 24 Jul 2024 14:48:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17324v1</guid></item><item><title>Physical Adversarial Attack on Monocular Depth Estimation via Shape-Varying Patches</title><link>http://arxiv.org/abs/2407.17312v1</link><description>Adversarial attacks against monocular depth estimation (MDE) systems posesignificant challenges, particularly in safety-critical applications such asautonomous driving. Existing patch-based adversarial attacks for MDE areconfined to the vicinity of the patch, making it difficult to affect the entiretarget. To address this limitation, we propose a physics-based adversarialattack on monocular depth estimation, employing a framework called Attack withShape-Varying Patches (ASP), aiming to optimize patch content, shape, andposition to maximize effectiveness. We introduce various mask shapes, includingquadrilateral, rectangular, and circular masks, to enhance the flexibility andefficiency of the attack. Furthermore, we propose a new loss function to extendthe influence of the patch beyond the overlapping regions. Experimental resultsdemonstrate that our attack method generates an average depth error of 18meters on the target car with a patch area of 1/9, affecting over 98\% of thetarget area.</description><author>Chenxing Zhao, Yang Li, Shihao Wu, Wenyi Tan, Shuangju Zhou, Quan Pan</author><pubDate>Wed, 24 Jul 2024 14:29:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17312v1</guid></item><item><title>LangOcc: Self-Supervised Open Vocabulary Occupancy Estimation via Volume Rendering</title><link>http://arxiv.org/abs/2407.17310v1</link><description>Semantic occupancy has recently gained significant traction as a prominentmethod for 3D scene representation. However, most existing camera-based methodsrely on costly datasets with fine-grained 3D voxel labels or LiDAR scans fortraining, which limits their practicality and scalability, raising the need forself-supervised approaches in this domain. Moreover, most methods are tied to apredefined set of classes which they can detect. In this work we present anovel approach for open vocabulary occupancy estimation called\textit{LangOcc}, that is trained only via camera images, and can detectarbitrary semantics via vision-language alignment. In particular, we distillthe knowledge of the strong vision-language aligned encoder CLIP into a 3Doccupancy model via differentiable volume rendering. Our model estimatesvision-language aligned features in a 3D voxel grid using only images. It istrained in a self-supervised manner by rendering our estimations back to 2Dspace, where ground-truth features can be computed. This training mechanismautomatically supervises the scene geometry, allowing for a straight-forwardand powerful training method without any explicit geometry supervision. LangOccoutperforms LiDAR-supervised competitors in open vocabulary occupancy by alarge margin, solely relying on vision-based training. We also achievestate-of-the-art results in self-supervised semantic occupancy estimation onthe Occ3D-nuScenes dataset, despite not being limited to a specific set ofcategories, thus demonstrating the effectiveness of our proposedvision-language training.</description><author>Simon Boeder, Fabian Gigengack, Benjamin Risse</author><pubDate>Wed, 24 Jul 2024 14:22:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17310v1</guid></item><item><title>Continual Learning in Bio-plausible Spiking Neural Networks with Hebbian and Spike Timing Dependent Plasticity: A Survey and Perspective</title><link>http://arxiv.org/abs/2407.17305v1</link><description>Recently, the use bio-plausible learning techniques such as Hebbian andSpike-Timing-Dependent Plasticity (STDP) have drawn significant attention forthe design of compute-efficient AI systems that can continuously learn on-lineat the edge. A key differentiating factor regarding this emerging class ofneuromorphic continual learning system lies in the fact that learning must becarried using a data stream received in its natural order, as opposed toconventional gradient-based offline training where a static training dataset isassumed available a priori and randomly shuffled to make the training setindependent and identically distributed (i.i.d). In contrast, the emergingclass of neuromorphic continual learning systems covered in this survey mustlearn to integrate new information on the fly in a non-i.i.d manner, whichmakes these systems subject to catastrophic forgetting. In order to build thenext generation of neuromorphic AI systems that can continuously learn at theedge, a growing number of research groups are studying the use of bio-plausibleHebbian neural network architectures and Spiking Neural Networks (SNNs)equipped with STDP learning. However, since this research field is stillemerging, there is a need for providing a holistic view of the differentapproaches proposed in literature so far. To this end, this survey covers anumber of recent works in the field of neuromorphic continual learning;provides background theory to help interested researchers to quickly learn thekey concepts; and discusses important future research questions in light of thedifferent works covered in this paper. It is hoped that this survey willcontribute towards future research in the field of neuromorphic continuallearning.</description><author>Ali Safa</author><pubDate>Wed, 24 Jul 2024 14:20:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17305v1</guid></item><item><title>MoveLight: Enhancing Traffic Signal Control through Movement-Centric Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2407.17303v1</link><description>This paper introduces MoveLight, a novel traffic signal control system thatenhances urban traffic management through movement-centric deep reinforcementlearning. By leveraging detailed real-time data and advanced machine learningtechniques, MoveLight overcomes the limitations of traditional traffic signalcontrol methods. It employs a lane-level control approach using the FRAPalgorithm to achieve dynamic and adaptive traffic signal control, optimizingtraffic flow, reducing congestion, and improving overall efficiency. Ourresearch demonstrates the scalability and effectiveness of MoveLight acrosssingle intersections, arterial roads, and network levels. Experimental resultsusing real-world datasets from Cologne and Hangzhou show significantimprovements in metrics such as queue length, delay, and throughput compared toexisting methods. This study highlights the transformative potential of deepreinforcement learning in intelligent traffic signal control, setting a newstandard for sustainable and efficient urban transportation systems.</description><author>Junqi Shao, Chenhao Zheng, Yuxuan Chen, Yucheng Huang, Rui Zhang</author><pubDate>Wed, 24 Jul 2024 14:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17303v1</guid></item><item><title>MutDet: Mutually Optimizing Pre-training for Remote Sensing Object Detection</title><link>http://arxiv.org/abs/2407.09920v2</link><description>Detection pre-training methods for the DETR series detector have beenextensively studied in natural scenes, e.g., DETReg. However, the detectionpre-training remains unexplored in remote sensing scenes. In existingpre-training methods, alignment between object embeddings extracted from apre-trained backbone and detector features is significant. However, due todifferences in feature extraction methods, a pronounced feature discrepancystill exists and hinders the pre-training performance. The remote sensingimages with complex environments and more densely distributed objectsexacerbate the discrepancy. In this work, we propose a novel Mutuallyoptimizing pre-training framework for remote sensing object Detection, dubbedas MutDet. In MutDet, we propose a systemic solution against this challenge.Firstly, we propose a mutual enhancement module, which fuses the objectembeddings and detector features bidirectionally in the last encoder layer,enhancing their information interaction.Secondly, contrastive alignment loss isemployed to guide this alignment process softly and simultaneously enhancesdetector features' discriminativity. Finally, we design an auxiliary siamesehead to mitigate the task gap arising from the introduction of enhancementmodule. Comprehensive experiments on various settings show new state-of-the-arttransfer performance. The improvement is particularly pronounced when dataquantity is limited. When using 10% of the DIOR-R data, MutDet improves DetRegby 6.1% in AP50. Codes and models are available at:https://github.com/floatingstarZ/MutDet.</description><author>Ziyue Huang, Yongchao Feng, Qingjie Liu, Yunhong Wang</author><pubDate>Wed, 24 Jul 2024 14:11:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09920v2</guid></item><item><title>High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise</title><link>http://arxiv.org/abs/2310.01860v2</link><description>High-probability analysis of stochastic first-order optimization methodsunder mild assumptions on the noise has been gaining a lot of attention inrecent years. Typically, gradient clipping is one of the key algorithmicingredients to derive good high-probability guarantees when the noise isheavy-tailed. However, if implemented na\"ively, clipping can spoil theconvergence of the popular methods for composite and distributed optimization(Prox-SGD/Parallel SGD) even in the absence of any noise. Due to this reason,many works on high-probability analysis consider only unconstrainednon-distributed problems, and the existing results for composite/distributedproblems do not include some important special cases (like strongly convexproblems) and are not optimal. To address this issue, we propose new stochasticmethods for composite and distributed optimization based on the clipping ofstochastic gradient differences and prove tight high-probability convergenceresults (including nearly optimal ones) for the new methods. Using similarideas, we also develop new methods for composite and distributed variationalinequalities and analyze the high-probability convergence of these methods.</description><author>Eduard Gorbunov, Abdurakhmon Sadiev, Marina Danilova, Samuel Horváth, Gauthier Gidel, Pavel Dvurechensky, Alexander Gasnikov, Peter Richtárik</author><pubDate>Wed, 24 Jul 2024 14:10:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01860v2</guid></item><item><title>Enhanced SMC$^2$: Leveraging Gradient Information from Differentiable Particle Filters Within Langevin Proposals</title><link>http://arxiv.org/abs/2407.17296v1</link><description>Sequential Monte Carlo Squared (SMC$^2$) is a Bayesian method which can inferthe states and parameters of non-linear, non-Gaussian state-space models. Thestandard random-walk proposal in SMC$^2$ faces challenges, particularly withhigh-dimensional parameter spaces. This study outlines a novel approach byharnessing first-order gradients derived from a Common Random Numbers -Particle Filter (CRN-PF) using PyTorch. The resulting gradients can beleveraged within a Langevin proposal without accept/reject. Including Langevindynamics within the proposal can result in a higher effective sample size andmore accurate parameter estimates when compared with the random-walk. Theresulting algorithm is parallelized on distributed memory using Message PassingInterface (MPI) and runs in $\mathcal{O}(\log_2N)$ time complexity. Utilizing64 computational cores we obtain a 51x speed-up when compared to a single core.A GitHub link is given which provides access to the code.</description><author>Conor Rosato, Joshua Murphy, Alessandro Varsi, Paul Horridge, Simon Maskell</author><pubDate>Wed, 24 Jul 2024 14:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17296v1</guid></item></channel></rss>