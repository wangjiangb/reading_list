<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 18 Sep 2024 13:00:25 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion</title><link>http://arxiv.org/abs/2409.11406v1</link><description>In 3D modeling, designers often use an existing 3D model as a reference tocreate new ones. This practice has inspired the development of Phidias, a novelgenerative model that uses diffusion for reference-augmented 3D generation.Given an image, our method leverages a retrieved or user-provided 3D referencemodel to guide the generation process, thereby enhancing the generationquality, generalization ability, and controllability. Our model integratesthree key components: 1) meta-ControlNet that dynamically modulates theconditioning strength, 2) dynamic reference routing that mitigates misalignmentbetween the input image and 3D reference, and 3) self-reference augmentationsthat enable self-supervised training with a progressive curriculum.Collectively, these designs result in a clear improvement over existingmethods. Phidias establishes a unified framework for 3D generation using text,image, and 3D conditions with versatile applications.</description><author>Zhenwei Wang, Tengfei Wang, Zexin He, Gerhard Hancke, Ziwei Liu, Rynson W. H. Lau</author><pubDate>Tue, 17 Sep 2024 17:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11406v1</guid></item><item><title>AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs</title><link>http://arxiv.org/abs/2409.11404v1</link><description>Arabic, with its rich diversity of dialects, remains significantlyunderrepresented in Large Language Models, particularly in dialectalvariations. We address this gap by introducing seven synthetic datasets indialects alongside Modern Standard Arabic (MSA), created using MachineTranslation (MT) combined with human post-editing. We present AraDiCE, abenchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs ondialect comprehension and generation, focusing specifically on low-resourceArabic dialects. Additionally, we introduce the first-ever fine-grainedbenchmark designed to evaluate cultural awareness across the Gulf, Egypt, andLevant regions, providing a novel dimension to LLM evaluation. Our findingsdemonstrate that while Arabic-specific models like Jais and AceGPT outperformmultilingual models on dialectal tasks, significant challenges persist indialect identification, generation, and translation. This work contributes ~45Kpost-edited samples, a cultural benchmark, and highlights the importance oftailored training to improve LLM performance in capturing the nuances ofdiverse Arabic dialects and cultural contexts. We will release the dialectaltranslation models and benchmarks curated in this study.</description><author>Basel Mousi, Nadir Durrani, Fatema Ahmad, Md. Arid Hasan, Maram Hasanain, Tameem Kabbani, Fahim Dalvi, Shammur Absar Chowdhury, Firoj Alam</author><pubDate>Tue, 17 Sep 2024 17:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11404v1</guid></item><item><title>NVLM: Open Frontier-Class Multimodal LLMs</title><link>http://arxiv.org/abs/2409.11402v1</link><description>We introduce NVLM 1.0, a family of frontier-class multimodal large languagemodels (LLMs) that achieve state-of-the-art results on vision-language tasks,rivaling the leading proprietary models (e.g., GPT-4o) and open-access models(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improvedtext-only performance over its LLM backbone after multimodal training. In termsof model design, we perform a comprehensive comparison between decoder-onlymultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,Flamingo). Based on the strengths and weaknesses of both approaches, we proposea novel architecture that enhances both training efficiency and multimodalreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design fortile-based dynamic high-resolution images, which significantly boostsperformance on multimodal reasoning and OCR-related tasks. Regarding trainingdata, we meticulously curate and provide detailed information on our multimodalpretraining and supervised fine-tuning datasets. Our findings indicate thatdataset quality and task diversity are more important than scale, even duringthe pretraining phase, across all architectures. Notably, we developproduction-grade multimodality for the NVLM-1.0 models, enabling them to excelin vision-language tasks while maintaining and even improving text-onlyperformance compared to their LLM backbones. To achieve this, we craft andintegrate a high-quality text-only dataset into multimodal training, alongsidea substantial amount of multimodal math and reasoning data, leading to enhancedmath and coding capabilities across modalities. To advance research in thefield, we are releasing the model weights and will open-source the code for thecommunity: https://nvlm-project.github.io/.</description><author>Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</author><pubDate>Tue, 17 Sep 2024 17:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11402v1</guid></item><item><title>LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents</title><link>http://arxiv.org/abs/2409.11393v1</link><description>The integration of tools in LLM-based agents overcame the difficulties ofstandalone LLMs and traditional agents' limited capabilities. However, theconjunction of these technologies and the proposed enhancements in severalstate-of-the-art works followed a non-unified software architecture resultingin a lack of modularity. Indeed, they focused mainly on functionalities andoverlooked the definition of the component's boundaries within the agent. Thiscaused terminological and architectural ambiguities between researchers whichwe addressed in this paper by proposing a unified framework that establishes aclear foundation for LLM-based agents' development from both functional andsoftware architectural perspectives. Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework),clearly distinguishes between the different components of an agent, settingLLMs, and tools apart from a newly introduced element: the core-agent, playingthe role of the central coordinator of the agent which comprises five modules:planning, memory, profile, action, and security, the latter often neglected inprevious works. Differences in the internal structure of core-agents led us toclassify them into a taxonomy of passive and active types. Based on this, weproposed different multi-core agent architectures combining uniquecharacteristics of various individual agents. For evaluation purposes, we applied this framework to a selection ofstate-of-the-art agents, thereby demonstrating its alignment with theirfunctionalities and clarifying the overlooked architectural aspects. Moreover,we thoroughly assessed four of our proposed architectures by integratingdistinctive agents into hybrid active/passive core-agents' systems. Thisanalysis provided clear insights into potential improvements and highlightedthe challenges involved in the combination of specific agents.</description><author>Amine B. Hassouna, Hana Chaari, Ines Belhaj</author><pubDate>Tue, 17 Sep 2024 17:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11393v1</guid></item><item><title>Says Who? Effective Zero-Shot Annotation of Focalization</title><link>http://arxiv.org/abs/2409.11390v1</link><description>Focalization, the perspective through which narrative is presented, isencoded via a wide range of lexico-grammatical features and is subject toreader interpretation. Moreover, trained readers regularly disagree oninterpretations, suggesting that this problem may be computationallyintractable. In this paper, we provide experiments to test how wellcontemporary Large Language Models (LLMs) perform when annotating literarytexts for focalization mode. Despite the challenging nature of the task, LLMsshow comparable performance to trained human annotators in our experiments. Weprovide a case study working with the novels of Stephen King to demonstrate theusefulness of this approach for computational literary studies, illustratinghow focalization can be studied at scale.</description><author>Rebecca M. M. Hicke, Yuri Bizzoni, Pascale Feldkamp, Ross Deans Kristensen-McLachlan</author><pubDate>Tue, 17 Sep 2024 17:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11390v1</guid></item><item><title>Normalization in Proportional Feature Spaces</title><link>http://arxiv.org/abs/2409.11389v1</link><description>The subject of features normalization plays an important central role in datarepresentation, characterization, visualization, analysis, comparison,classification, and modeling, as it can substantially influence and beinfluenced by all of these activities and respective aspects. The selection ofan appropriate normalization method needs to take into account the type andcharacteristics of the involved features, the methods to be used subsequentlyfor the just mentioned data processing, as well as the specific questions beingconsidered. After briefly considering how normalization constitutes one of themany interrelated parts typically involved in data analysis and modeling, thepresent work addressed the important issue of feature normalization from theperspective of uniform and proportional (right skewed) features and comparisonoperations. More general right skewed features are also considered in anapproximated manner. Several concepts, properties, and results are describedand discussed, including the description of a duality relationship betweenuniform and proportional feature spaces and respective comparisons, specifyingconditions for consistency between comparisons in each of the two domains. Twonormalization possibilities based on non-centralized dispersion of features arealso presented, and also described is a modified version of the Jaccardsimilarity index which incorporates intrinsically normalization. Preliminaryexperiments are presented in order to illustrate the developed concepts andmethods.</description><author>Alexandre Benatti, Luciano da F. Costa</author><pubDate>Tue, 17 Sep 2024 17:46:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11389v1</guid></item><item><title>Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks</title><link>http://arxiv.org/abs/2409.06173v2</link><description>In-Context Learning (ICL) in Large Language Models (LLM) has emerged as thedominant technique for performing natural language tasks, as it does notrequire updating the model parameters with gradient-based methods. ICL promisesto "adapt" the LLM to perform the present task at a competitive orstate-of-the-art level at a fraction of the computational cost. ICL can beaugmented by incorporating the reasoning process to arrive at the final labelexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.However, recent work has found that ICL relies mostly on the retrieval of taskpriors and less so on "learning" to perform tasks, especially for complexsubjective domains like emotion and morality, where priors ossify posteriorpredictions. In this work, we examine whether "enabling" reasoning also createsthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priorsthat remain relatively unchanged despite the evidence in the prompt. We findthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICLfor larger language models. Code is avalaible athttps://github.com/gchochla/cot-priors.</description><author>Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan</author><pubDate>Tue, 17 Sep 2024 17:42:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06173v2</guid></item><item><title>Training Datasets Generation for Machine Learning: Application to Vision Based Navigation</title><link>http://arxiv.org/abs/2409.11383v1</link><description>Vision Based Navigation consists in utilizing cameras as precision sensorsfor GNC after extracting information from images. To enable the adoption ofmachine learning for space applications, one of obstacles is the demonstrationthat available training datasets are adequate to validate the algorithms. Theobjective of the study is to generate datasets of images and metadata suitablefor training machine learning algorithms. Two use cases were selected and arobust methodology was developed to validate the datasets including the groundtruth. The first use case is in-orbit rendezvous with a man-made object: amockup of satellite ENVISAT. The second use case is a Lunar landing scenario.Datasets were produced from archival datasets (Chang'e 3), from the laboratoryat DLR TRON facility and at Airbus Robotic laboratory, from SurRender softwarehigh fidelity image simulator using Model Capture and from GenerativeAdversarial Networks. The use case definition included the selection ofalgorithms as benchmark: an AI-based pose estimation algorithm and a denseoptical flow algorithm were selected. Eventually it is demonstrated thatdatasets produced with SurRender and selected laboratory facilities areadequate to train machine learning algorithms.</description><author>Jérémy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco</author><pubDate>Tue, 17 Sep 2024 17:34:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11383v1</guid></item><item><title>Wave-U-Mamba: An End-To-End Framework For High-Quality And Efficient Speech Super Resolution</title><link>http://arxiv.org/abs/2409.09337v2</link><description>Speech Super-Resolution (SSR) is a task of enhancing low-resolution speechsignals by restoring missing high-frequency components. Conventional approachestypically reconstruct log-mel features, followed by a vocoder that generateshigh-resolution speech in the waveform domain. However, as log-mel featureslack phase information, this can result in performance degradation during thereconstruction phase. Motivated by recent advances with Selective State SpacesModels (SSMs), we propose a method, referred to as Wave-U-Mamba that directlyperforms SSR in time domain. In our comparative study, including models such asWSRGlow, NU-Wave 2, and AudioSR, Wave-U-Mamba demonstrates superiorperformance, achieving the lowest Log-Spectral Distance (LSD) across variouslow-resolution sampling rates, ranging from 8 kHz to 24 kHz. Additionally,subjective human evaluations, scored using Mean Opinion Score (MOS) reveal thatour method produces SSR with natural and human-like quality. Furthermore,Wave-U-Mamba achieves these results while generating high-resolution speechover nine times faster than baseline models on a single A100 GPU, withparameter sizes less than 2% of those in the baseline models.</description><author>Yongjoon Lee, Chanwoo Kim</author><pubDate>Tue, 17 Sep 2024 17:33:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09337v2</guid></item><item><title>S$^3$Attention: Improving Long Sequence Attention with Smoothed Skeleton Sketching</title><link>http://arxiv.org/abs/2408.08567v3</link><description>Attention based models have achieved many remarkable breakthroughs innumerous applications. However, the quadratic complexity of Attention makes thevanilla Attention based models hard to apply to long sequence tasks. Variousimproved Attention structures are proposed to reduce the computation cost byinducing low rankness and approximating the whole sequence by sub-sequences.The most challenging part of those approaches is maintaining the proper balancebetween information preservation and computation reduction: the longersub-sequences used, the better information is preserved, but at the price ofintroducing more noise and computational costs. In this paper, we propose asmoothed skeleton sketching based Attention structure, coined S$^3$Attention,which significantly improves upon the previous attempts to negotiate thistrade-off. S$^3$Attention has two mechanisms to effectively minimize the impactof noise while keeping the linear complexity to the sequence length: asmoothing block to mix information over long sequences and a matrix sketchingmethod that simultaneously selects columns and rows from the input matrix. Weverify the effectiveness of S$^3$Attention both theoretically and empirically.Extensive studies over Long Range Arena (LRA) datasets and six time-seriesforecasting show that S$^3$Attention significantly outperforms both vanillaAttention and other state-of-the-art variants of Attention structures.</description><author>Xue Wang, Tian Zhou, Jianqing Zhu, Jialin Liu, Kun Yuan, Tao Yao, Wotao Yin, Rong Jin, HanQin Cai</author><pubDate>Tue, 17 Sep 2024 17:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08567v3</guid></item><item><title>Ultrasound Image Enhancement with the Variance of Diffusion Models</title><link>http://arxiv.org/abs/2409.11380v1</link><description>Ultrasound imaging, despite its widespread use in medicine, often suffersfrom various sources of noise and artifacts that impact the signal-to-noiseratio and overall image quality. Enhancing ultrasound images requires adelicate balance between contrast, resolution, and speckle preservation. Thispaper introduces a novel approach that integrates adaptive beamforming withdenoising diffusion-based variance imaging to address this challenge. Byapplying Eigenspace-Based Minimum Variance (EBMV) beamforming and employing adenoising diffusion model fine-tuned on ultrasound data, our method computesthe variance across multiple diffusion-denoised samples to produce high-qualitydespeckled images. This approach leverages both the inherent multiplicativenoise of ultrasound and the stochastic nature of diffusion models. Experimentalresults on a publicly available dataset demonstrate the effectiveness of ourmethod in achieving superior image reconstructions from single plane-waveacquisitions. The code is available at:https://github.com/Yuxin-Zhang-Jasmine/IUS2024_Diffusion.</description><author>Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus</author><pubDate>Tue, 17 Sep 2024 17:29:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11380v1</guid></item><item><title>Dated Data: Tracing Knowledge Cutoffs in Large Language Models</title><link>http://arxiv.org/abs/2403.12958v2</link><description>Released Large Language Models (LLMs) are often paired with a claimedknowledge cutoff date, or the dates at which training data was gathered. Suchinformation is crucial for applications where the LLM must provide up to dateinformation. However, this statement only scratches the surface: do allresources in the training data share the same knowledge cutoff date? Does themodel's demonstrated knowledge for these subsets closely align to their cutoffdates? In this work, we define the notion of an effective cutoff. This isdistinct from the LLM designer reported cutoff and applies separately tosub-resources and topics. We propose a simple approach to estimate effectivecutoffs on the resource-level temporal alignment of an LLM by probing acrossversions of the data. Using this analysis, we find that effective cutoffs oftendiffer from reported cutoffs. To understand the root cause of this observation,we conduct a direct large-scale analysis on open pre-training datasets. Ouranalysis reveals two reasons for these inconsistencies: (1) temporal biases ofCommonCrawl data due to non-trivial amounts of old data in new dumps and (2)complications in LLM deduplication schemes involving semantic duplicates andlexical near-duplicates. Overall, our results show that knowledge cutoffs arenot as simple as they have seemed and that care must be taken both by LLMdataset curators as well as practitioners who seek to use information fromthese models.</description><author>Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, Benjamin Van Durme</author><pubDate>Tue, 17 Sep 2024 17:25:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12958v2</guid></item><item><title>Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement</title><link>http://arxiv.org/abs/2409.11378v1</link><description>Finetuning large language models on instruction data is crucial for enhancingpre-trained knowledge and improving instruction-following capabilities. Asinstruction datasets proliferate, selecting optimal data for effective trainingbecomes increasingly important. This work addresses the question: How can wedetermine the optimal subset of data for effective training? While existingresearch often emphasizes local criteria like instance quality for subsetselection, we argue that a global approach focused on data diversity is morecritical. Our method employs k-means clustering to ensure the selected subseteffectively represents the full dataset. We propose an iterative refinementmethod inspired by active learning techniques to resample instances fromclusters, reassessing each cluster's importance and sampling weight in everytraining iteration. This approach reduces the effect of outliers andautomatically filters out clusters containing low-quality data. Throughextensive evaluation across natural language reasoning, general worldknowledge, code and math reasoning tasks, and by fine-tuning models fromvarious families, we observe consistent improvements, achieving a 7% increaseover random selection and a 3.8% improvement over state-of-the-art samplingmethods. Our work highlights the significance of diversity-first sampling whenfinetuning LLMs to enhance performance across a broad array of evaluationtasks. Our code is available athttps://github.com/for-ai/iterative-data-selection.</description><author>Simon Yu, Liangyu Chen, Sara Ahmadian, Marzieh Fadaee</author><pubDate>Tue, 17 Sep 2024 17:25:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11378v1</guid></item><item><title>Machine Learning on Dynamic Functional Connectivity: Promise, Pitfalls, and Interpretations</title><link>http://arxiv.org/abs/2409.11377v1</link><description>An unprecedented amount of existing functional Magnetic Resonance Imaging(fMRI) data provides a new opportunity to understand the relationship betweenfunctional fluctuation and human cognition/behavior using a data-drivenapproach. To that end, tremendous efforts have been made in machine learning topredict cognitive states from evolving volumetric images ofblood-oxygen-level-dependent (BOLD) signals. Due to the complex nature of brainfunction, however, the evaluation on learning performance and discoveries arenot often consistent across current state-of-the-arts (SOTA). By capitalizingon large-scale existing neuroimaging data (34,887 data samples from six publicdatabases), we seek to establish a well-founded empirical guideline fordesigning deep models for functional neuroimages by linking the methodologyunderpinning with knowledge from the neuroscience domain. Specifically, we putthe spotlight on (1) What is the current SOTA performance in cognitive taskrecognition and disease diagnosis using fMRI? (2) What are the limitations ofcurrent deep models? and (3) What is the general guideline for selecting thesuitable machine learning backbone for new neuroimaging applications? We haveconducted a comprehensive evaluation and statistical analysis, in varioussettings, to answer the above outstanding questions.</description><author>Jiaqi Ding, Tingting Dan, Ziquan Wei, Hyuna Cho, Paul J. Laurienti, Won Hwa Kim, Guorong Wu</author><pubDate>Tue, 17 Sep 2024 17:24:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11377v1</guid></item><item><title>Towards Time Series Reasoning with LLMs</title><link>http://arxiv.org/abs/2409.11376v1</link><description>Multi-modal large language models (MLLMs) have enabled numerous advances inunderstanding and reasoning in domains like vision, but we have not yet seenthis broad success for time-series. Although prior works on time-series MLLMshave shown promising performance in time-series forecasting, very few worksshow how an LLM could be used for time-series reasoning in natural language. Wepropose a novel multi-modal time-series LLM approach that learns generalizableinformation across various domains with powerful zero-shot performance. First,we train a lightweight time-series encoder on top of an LLM to directly extracttime-series information. Then, we fine-tune our model with chain-of-thoughtaugmented time-series tasks to encourage the model to generate reasoning paths.We show that our model learns a latent representation that reflects specifictime-series features (e.g. slope, frequency), as well as outperforming GPT-4oon a set of zero-shot reasoning tasks on a variety of domains.</description><author>Winnie Chow, Lauren Gardiner, Haraldur T. Hallgrímsson, Maxwell A. Xu, Shirley You Ren</author><pubDate>Tue, 17 Sep 2024 17:23:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11376v1</guid></item><item><title>Multi-OCT-SelfNet: Integrating Self-Supervised Learning with Multi-Source Data Fusion for Enhanced Multi-Class Retinal Disease Classification</title><link>http://arxiv.org/abs/2409.11375v1</link><description>In the medical domain, acquiring large datasets poses significant challengesdue to privacy concerns. Nonetheless, the development of a robust deep-learningmodel for retinal disease diagnosis necessitates a substantial dataset fortraining. The capacity to generalize effectively on smaller datasets remains apersistent challenge. The scarcity of data presents a significant barrier tothe practical implementation of scalable medical AI solutions. To address thisissue, we've combined a wide range of data sources to improve performance andgeneralization to new data by giving it a deeper understanding of the datarepresentation from multi-modal datasets and developed a self-supervisedframework based on large language models (LLMs), SwinV2 to gain a deeperunderstanding of multi-modal dataset representations, enhancing the model'sability to extrapolate to new data for the detection of eye diseases usingoptical coherence tomography (OCT) images. We adopt a two-phase trainingmethodology, self-supervised pre-training, and fine-tuning on a downstreamsupervised classifier. An ablation study conducted across three datasetsemploying various encoder backbones, without data fusion, with low dataavailability setting, and without self-supervised pre-training scenarios,highlights the robustness of our method. Our findings demonstrate consistentperformance across these diverse conditions, showcasing superior generalizationcapabilities compared to the baseline model, ResNet-50.</description><author>Fatema-E- Jannat, Sina Gholami, Jennifer I. Lim, Theodore Leng, Minhaj Nur Alam, Hamed Tabkhi</author><pubDate>Tue, 17 Sep 2024 17:22:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11375v1</guid></item><item><title>Uncertainty and Prediction Quality Estimation for Semantic Segmentation via Graph Neural Networks</title><link>http://arxiv.org/abs/2409.11373v1</link><description>When employing deep neural networks (DNNs) for semantic segmentation insafety-critical applications like automotive perception or medical imaging, itis important to estimate their performance at runtime, e.g. via uncertaintyestimates or prediction quality estimates. Previous works mostly performeduncertainty estimation on pixel-level. In a line of research, aconnected-component-wise (segment-wise) perspective was taken, approachinguncertainty estimation on an object-level by performing so-called metaclassification and regression to estimate uncertainty and prediction quality,respectively. In those works, each predicted segment is considered individuallyto estimate its uncertainty or prediction quality. However, the neighboringsegments may provide additional hints on whether a given predicted segment isof high quality, which we study in the present work. On the basis ofuncertainty indicating metrics on segment-level, we use graph neural networks(GNNs) to model the relationship of a given segment's quality as a function ofthe given segment's metrics as well as those of its neighboring segments. Wecompare different GNN architectures and achieve a notable performanceimprovement.</description><author>Edgar Heinert, Stephan Tilgner, Timo Palm, Matthias Rottmann</author><pubDate>Tue, 17 Sep 2024 17:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11373v1</guid></item><item><title>Compact Implicit Neural Representations for Plane Wave Images</title><link>http://arxiv.org/abs/2409.11370v1</link><description>Ultrafast Plane-Wave (PW) imaging often produces artifacts and shadows thatvary with insonification angles. We propose a novel approach using ImplicitNeural Representations (INRs) to compactly encode multi-planar sequences whilepreserving crucial orientation-dependent information. To our knowledge, this isthe first application of INRs for PW angular interpolation. Our method employsa Multi-Layer Perceptron (MLP)-based model with a concise physics-enhancedrendering technique. Quantitative evaluations using SSIM, PSNR, and standardultrasound metrics, along with qualitative visual assessments, confirm theeffectiveness of our approach. Additionally, our method demonstratessignificant storage efficiency, with model weights requiring 530 KB compared to8 MB for directly storing the 75 PW images, achieving a notable compressionratio of approximately 15:1.</description><author>Mathilde Monvoisin, Yuxin Zhang, Diana Mateus</author><pubDate>Tue, 17 Sep 2024 17:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11370v1</guid></item><item><title>Learning Spatially-Aware Language and Audio Embedding</title><link>http://arxiv.org/abs/2409.11369v1</link><description>Humans can picture a sound scene given an imprecise natural languagedescription. For example, it is easy to imagine an acoustic environment given aphrase like "the lion roar came from right behind me!". For a machine to havethe same degree of comprehension, the machine must know what a lion is(semantic attribute), what the concept of "behind" is (spatial attribute) andhow these pieces of linguistic information align with the semantic and spatialattributes of the sound (what a roar sounds like when its coming from behind).State-of-the-art audio foundation models which learn to map between audioscenes and natural textual descriptions, are trained on non-spatial audio andtext pairs, and hence lack spatial awareness. In contrast, sound eventlocalization and detection models are limited to recognizing sounds from afixed number of classes, and they localize the source to absolute position(e.g., 0.2m) rather than a position described using natural language (e.g.,"next to me"). To address these gaps, we present ELSA a spatially aware-audioand text embedding model trained using multimodal contrastive learning. ELSAsupports non-spatial audio, spatial audio, and open vocabulary text captionsdescribing both the spatial and semantic components of sound. To train ELSA:(a) we spatially augment the audio and captions of three open-source audiodatasets totaling 4,738 hours of audio, and (b) we design an encoder to capturethe semantics of non-spatial audio, and the semantics and spatial attributes ofspatial audio using contrastive learning. ELSA is competitive withstate-of-the-art for both semantic retrieval and 3D source localization. Inparticular, ELSA achieves +2.8% mean audio-to-text and text-to-audio R@1 abovethe baseline, and outperforms by -11.6{\deg} mean-absolute-error in 3D sourcelocalization over the baseline.</description><author>Bhavika Devnani, Skyler Seto, Zakaria Aldeneh, Alessandro Toso, Elena Menyaylenko, Barry-John Theobald, Jonathan Sheaffer, Miguel Sarabia</author><pubDate>Tue, 17 Sep 2024 17:17:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11369v1</guid></item><item><title>OSV: One Step is Enough for High-Quality Image to Video Generation</title><link>http://arxiv.org/abs/2409.11367v1</link><description>Video diffusion models have shown great potential in generating high-qualityvideos, making them an increasingly popular focus. However, their inherentiterative nature leads to substantial computational and time costs. Whileefforts have been made to accelerate video diffusion by reducing inferencesteps (through techniques like consistency distillation) and GAN training(these approaches often fall short in either performance or trainingstability). In this work, we introduce a two-stage training framework thateffectively combines consistency distillation with GAN training to addressthese challenges. Additionally, we propose a novel video discriminator design,which eliminates the need for decoding the video latents and improves the finalperformance. Our model is capable of producing high-quality videos in merelyone-step, with the flexibility to perform multi-step refinement for furtherperformance enhancement. Our quantitative evaluation on the OpenWebVid-1Mbenchmark shows that our model significantly outperforms existing methods.Notably, our 1-step performance(FVD 171.15) exceeds the 8-step performance ofthe consistency distillation based method, AnimateLCM (FVD 184.79), andapproaches the 25-step performance of advanced Stable Video Diffusion (FVD156.94).</description><author>Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Wenbing Zhu, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang</author><pubDate>Tue, 17 Sep 2024 17:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11367v1</guid></item><item><title>Towards Optimal Branching of Linear and Semidefinite Relaxations for Neural Network Robustness Certification</title><link>http://arxiv.org/abs/2101.09306v3</link><description>In this paper, we study certifying the robustness of ReLU neural networksagainst adversarial input perturbations. To diminish the relaxation errorsuffered by the popular linear programming (LP) and semidefinite programming(SDP) certification methods, we take a branch-and-bound approach to proposepartitioning the input uncertainty set and solving the relaxations on each partseparately. We show that this approach reduces relaxation error, and that theerror is eliminated entirely upon performing an LP relaxation with a partitionintelligently designed to exploit the nature of the ReLU activations. To scalethis approach to large networks, we consider using a coarser partition wherebythe number of parts in the partition is reduced. We prove that computing such acoarse partition that directly minimizes the LP relaxation error is NP-hard. Byinstead minimizing the worst-case LP relaxation error, we develop a closed-formbranching scheme in the single-hidden layer case. We extend the analysis to theSDP, where the feasible set geometry is exploited to design a branching schemethat minimizes the worst-case SDP relaxation error. Experiments on MNIST,CIFAR-10, and Wisconsin breast cancer diagnosis classifiers demonstratesignificant increases in the percentages of test samples certified. Byindependently increasing the input size and the number of layers, weempirically illustrate under which regimes the branched LP and branched SDP arebest applied. Finally, we extend our LP branching method into a multi-layerbranching heuristic, which attains comparable performance to priorstate-of-the-art heuristics on large-scale, deep neural network certificationbenchmarks.</description><author>Brendon G. Anderson, Ziye Ma, Jingqi Li, Somayeh Sojoudi</author><pubDate>Tue, 17 Sep 2024 17:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2101.09306v3</guid></item><item><title>CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration</title><link>http://arxiv.org/abs/2409.11365v1</link><description>The deployment of multimodal large language models (MLLMs) has demonstratedremarkable success in engaging in conversations involving visual inputs, thanksto the superior power of large language models (LLMs). Those MLLMs aretypically built based on the LLMs, with an image encoder to process images intothe token embedding space of the LLMs. However, the integration of visualmodality has introduced a unique vulnerability: the MLLM becomes susceptible tomalicious visual inputs and prone to generating sensitive or harmful responses,even though the LLM has been trained on textual dataset to align with humanvalue. In this paper, we first raise the question: ``Do the MLLMs possesssafety-awareness against malicious image inputs?". We find that after adding aprinciple that specifies the safety requirement into the input of the MLLM, themodel's safety awareness becomes boosted. This phenomenon verifies theexistence of MLLM's safety-awareness against image inputs, it is only weakenedby the modality gap. We then introduce a simple yet effective technique termedCoCA, which amplifies the safety-awareness of the MLLM by calibrating itsoutput distribution. Our proposed strategy helps the model reclaim its originalsafety awareness without losing its original capabilities. We verify theeffectiveness of our approach on both multimodal safety and understandingbenchmarks.</description><author>Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, Zhenguo Li</author><pubDate>Tue, 17 Sep 2024 17:14:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11365v1</guid></item><item><title>SpatialBot: Precise Spatial Understanding with Vision Language Models</title><link>http://arxiv.org/abs/2406.13642v6</link><description>Vision Language Models (VLMs) have achieved impressive performance in 2Dimage understanding, however they are still struggling with spatialunderstanding which is the foundation of Embodied AI. In this paper, we proposeSpatialBot for better spatial understanding by feeding both RGB and depthimages. Additionally, we have constructed the SpatialQA dataset, which involvesmulti-level depth-related questions to train VLMs for depth understanding.Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilitiesin spatial understanding at different levels. Extensive experiments on ourspatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. Themodel, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.</description><author>Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao</author><pubDate>Tue, 17 Sep 2024 17:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13642v6</guid></item><item><title>CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark</title><link>http://arxiv.org/abs/2409.11363v1</link><description>AI agents have the potential to aid users on a variety of consequentialtasks, including conducting scientific research. To spur the development ofuseful agents, we need benchmarks that are challenging, but more crucially,directly correspond to real-world tasks of interest. This paper introduces sucha benchmark, designed to measure the accuracy of AI agents in tackling acrucial yet surprisingly challenging aspect of scientific research:computational reproducibility. This task, fundamental to the scientificprocess, involves reproducing the results of a study using the provided codeand data. We introduce CORE-Bench (Computational Reproducibility AgentBenchmark), a benchmark consisting of 270 tasks based on 90 scientific papersacross three disciplines (computer science, social science, and medicine).Tasks in CORE-Bench consist of three difficulty levels and include bothlanguage-only and vision-language tasks. We provide an evaluation system tomeasure the accuracy of agents in a fast and parallelizable way, saving days ofevaluation time for each run compared to a sequential implementation. Weevaluated two baseline agents: the general-purpose AutoGPT and a task-specificagent called CORE-Agent. We tested both variants using two underlying languagemodels: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% onthe hardest task, showing the vast scope for improvement in automating routinescientific tasks. Having agents that can reproduce existing work is a necessarystep towards building agents that can conduct novel research and could verifyand improve the performance of other research agents. We hope that CORE-Benchcan improve the state of reproducibility and spur the development of futureresearch agents.</description><author>Zachary S. Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan</author><pubDate>Tue, 17 Sep 2024 17:13:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11363v1</guid></item><item><title>AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances</title><link>http://arxiv.org/abs/2409.11360v1</link><description>Large language models (LLMs) are being increasingly integrated into everydayproducts and services, such as coding tools and writing assistants. As theseembedded AI applications are deployed globally, there is a growing concern thatthe AI models underlying these applications prioritize Western values. Thispaper investigates what happens when a Western-centric AI model provideswriting suggestions to users from a different cultural background. We conducteda cross-cultural controlled experiment with 118 participants from India and theUnited States who completed culturally grounded writing tasks with and withoutAI suggestions. Our analysis reveals that AI provided greater efficiency gainsfor Americans compared to Indians. Moreover, AI suggestions led Indianparticipants to adopt Western writing styles, altering not just what is writtenbut also how it is written. These findings show that Western-centric AI modelshomogenize writing toward Western norms, diminishing nuances that differentiatecultural expression.</description><author>Dhruv Agarwal, Mor Naaman, Aditya Vashistha</author><pubDate>Tue, 17 Sep 2024 17:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11360v1</guid></item><item><title>Evaluating language models as risk scores</title><link>http://arxiv.org/abs/2407.14614v2</link><description>Current question-answering benchmarks predominantly focus on accuracy inrealizable prediction tasks. Conditioned on a question and answer-key, does themost likely token match the ground truth? Such benchmarks necessarily fail toevaluate language models' ability to quantify outcome uncertainty. In thiswork, we focus on the use of language models as risk scores for unrealizableprediction tasks. We introduce folktexts, a software package to systematicallygenerate risk scores using language models, and evaluate them against US Censusdata products. A flexible API enables the use of different prompting schemes,local or web-hosted models, and diverse census columns that can be used tocompose custom prediction tasks. We demonstrate the utility of folktextsthrough a sweep of empirical insights into the statistical properties of 17recent large language models across five natural text benchmark tasks. We findthat zero-shot risk scores produced by multiple-choice question-answering havehigh predictive signal but are widely miscalibrated. Base models consistentlyoverestimate outcome uncertainty, while instruction-tuned models underestimateuncertainty and produce over-confident risk scores. In fact, instruction-tuningpolarizes answer distribution regardless of true underlying data uncertainty.Conversely, verbally querying models for probability estimates results insubstantially improved calibration across all instruction-tuned models. Thesedifferences in ability to quantify data uncertainty cannot be revealed inrealizable settings, and highlight a blind-spot in the current evaluationecosystem that \folktexts covers.</description><author>André F. Cruz, Moritz Hardt, Celestine Mendler-Dünner</author><pubDate>Tue, 17 Sep 2024 17:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14614v2</guid></item><item><title>RenderWorld: World Model with Self-Supervised 3D Label</title><link>http://arxiv.org/abs/2409.11356v1</link><description>End-to-end autonomous driving with vision-only is not only morecost-effective compared to LiDAR-vision fusion but also more reliable thantraditional methods. To achieve a economical and robust purely visualautonomous driving system, we propose RenderWorld, a vision-only end-to-endautonomous driving framework, which generates 3D occupancy labels using aself-supervised gaussian-based Img2Occ Module, then encodes the labels byAM-VAE, and uses world model for forecasting and planning. RenderWorld employsGaussian Splatting to represent 3D scenes and render 2D images greatly improvessegmentation accuracy and reduces GPU memory consumption compared withNeRF-based methods. By applying AM-VAE to encode air and non-air separately,RenderWorld achieves more fine-grained scene element representation, leading tostate-of-the-art performance in both 4D occupancy forecasting and motionplanning from autoregressive world model.</description><author>Ziyang Yan, Wenzhen Dong, Yihua Shao, Yuhang Lu, Liu Haiyang, Jingwen Liu, Haozhe Wang, Zhe Wang, Yan Wang, Fabio Remondino, Yuexin Ma</author><pubDate>Tue, 17 Sep 2024 17:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11356v1</guid></item><item><title>Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think</title><link>http://arxiv.org/abs/2409.11355v1</link><description>Recent work showed that large diffusion models can be reused as highlyprecise monocular depth estimators by casting depth estimation as animage-conditional image generation task. While the proposed model achievedstate-of-the-art results, high computational demands due to multi-stepinference limited its use in many scenarios. In this paper, we show that theperceived inefficiency was caused by a flaw in the inference pipeline that hasso far gone unnoticed. The fixed model performs comparably to the bestpreviously reported configuration while being more than 200$\times$ faster. Tooptimize for downstream task performance, we perform end-to-end fine-tuning ontop of the single-step model with task-specific losses and get a deterministicmodel that outperforms all other diffusion-based depth and normal estimationmodels on common zero-shot benchmarks. We surprisingly find that thisfine-tuning protocol also works directly on Stable Diffusion and achievescomparable performance to current state-of-the-art diffusion-based depth andnormal estimation models, calling into question some of the conclusions drawnfrom prior works.</description><author>Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, Bastian Leibe</author><pubDate>Tue, 17 Sep 2024 16:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11355v1</guid></item><item><title>THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models</title><link>http://arxiv.org/abs/2409.11353v1</link><description>Hallucination, the generation of factually incorrect content, is a growingchallenge in Large Language Models (LLMs). Existing detection and mitigationmethods are often isolated and insufficient for domain-specific needs, lackinga standardized pipeline. This paper introduces THaMES (Tool for HallucinationMitigations and EvaluationS), an integrated framework and library addressingthis gap. THaMES offers an end-to-end solution for evaluating and mitigatinghallucinations in LLMs, featuring automated test set generation, multifacetedbenchmarking, and adaptable mitigation strategies. It automates test setcreation from any corpus, ensuring high data quality, diversity, andcost-efficiency through techniques like batch processing, weighted sampling,and counterfactual validation. THaMES assesses a model's ability to detect andreduce hallucinations across various tasks, including text generation andbinary classification, applying optimal mitigation strategies like In-ContextLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-EfficientFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge baseof academic papers, political news, and Wikipedia reveal that commercial modelslike GPT-4o benefit more from RAG than ICL, while open-weight models likeLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFTsignificantly enhances the performance of Llama-3.1-8B-Instruct in bothevaluation tasks.</description><author>Mengfei Liang, Archish Arun, Zekun Wu, Cristian Munoz, Jonathan Lutch, Emre Kazim, Adriano Koshiyama, Philip Treleaven</author><pubDate>Tue, 17 Sep 2024 16:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11353v1</guid></item><item><title>Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry</title><link>http://arxiv.org/abs/2409.11350v1</link><description>Machine-learning (ML) models in flow cytometry have the potential to reduceerror rates, increase reproducibility, and boost the efficiency of clinicallabs. While numerous ML models for flow cytometry data have been proposed, fewstudies have described the clinical deployment of such models. Realizing thepotential gains of ML models in clinical labs requires not only an accuratemodel, but infrastructure for automated inference, error detection, analyticsand monitoring, and structured data extraction. Here, we describe an ML modelfor detection of Acute Myeloid Leukemia (AML), along with the infrastructuresupporting clinical implementation. Our infrastructure leverages the resilienceand scalability of the cloud for model inference, a Kubernetes-based workflowsystem that provides model reproducibility and resource management, and asystem for extracting structured diagnoses from full-text reports. We alsodescribe our model monitoring and visualization platform, an essential elementfor ensuring continued model accuracy. Finally, we present a post-deploymentanalysis of impacts on turn-around time and compare production accuracy to theoriginal validation statistics.</description><author>Lauren M. Zuromski, Jacob Durtschi, Aimal Aziz, Jeffrey Chumley, Mark Dewey, Paul English, Muir Morrison, Keith Simmon, Blaine Whipple, Brendan O'Fallon, David P. Ng</author><pubDate>Tue, 17 Sep 2024 16:53:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11350v1</guid></item><item><title>OmniGen: Unified Image Generation</title><link>http://arxiv.org/abs/2409.11340v1</link><description>In this work, we introduce OmniGen, a new diffusion model for unified imagegeneration. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGenno longer requires additional modules such as ControlNet or IP-Adapter toprocess diverse control conditions. OmniGenis characterized by the followingfeatures: 1) Unification: OmniGen not only demonstrates text-to-imagegeneration capabilities but also inherently supports other downstream tasks,such as image editing, subject-driven generation, and visual-conditionalgeneration. Additionally, OmniGen can handle classical computer vision tasks bytransforming them into image generation tasks, such as edge detection and humanpose recognition. 2) Simplicity: The architecture of OmniGen is highlysimplified, eliminating the need for additional text encoders. Moreover, it ismore user-friendly compared to existing diffusion models, enabling complextasks to be accomplished through instructions without the need for extrapreprocessing steps (e.g., human pose estimation), thereby significantlysimplifying the workflow of image generation. 3) Knowledge Transfer: Throughlearning in a unified format, OmniGen effectively transfers knowledge acrossdifferent tasks, manages unseen tasks and domains, and exhibits novelcapabilities. We also explore the model's reasoning capabilities and potentialapplications of chain-of-thought mechanism. This work represents the firstattempt at a general-purpose image generation model, and there remain severalunresolved issues. We will open-source the related resources athttps://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.</description><author>Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, Zheng Liu</author><pubDate>Tue, 17 Sep 2024 16:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11340v1</guid></item><item><title>CLIP Adaptation by Intra-modal Overlap Reduction</title><link>http://arxiv.org/abs/2409.11338v1</link><description>Numerous methods have been proposed to adapt a pre-trained foundational CLIPmodel for few-shot classification. As CLIP is trained on a large corpus, itgeneralises well through adaptation to few-shot classification. In this work,we analyse the intra-modal overlap in image space in terms of embeddingrepresentation. Our analysis shows that, due to contrastive learning,embeddings from CLIP model exhibit high cosine similarity distribution overlapin the image space between paired and unpaired examples affecting theperformance of few-shot training-free classification methods which rely onsimilarity in the image space for their predictions. To tackle intra-modaloverlap we propose to train a lightweight adapter on a generic set of samplesfrom the Google Open Images dataset demonstrating that this improves accuracyfor few-shot training-free classification. We validate our contribution throughextensive empirical analysis and demonstrate that reducing the intra-modaloverlap leads to a) improved performance on a number of standard datasets, b)increased robustness to distribution shift and c) higher feature variancerendering the features more discriminative for downstream tasks.</description><author>Alexey Kravets, Vinay Namboodiri</author><pubDate>Tue, 17 Sep 2024 16:40:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11338v1</guid></item><item><title>MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks</title><link>http://arxiv.org/abs/2312.13322v2</link><description>With easier access to powerful compute resources, there is a growing trend inAI for software development to develop large language models (LLMs) to addressa variety of programming tasks. Even LLMs applied to tasks from thehigh-performance computing (HPC) domain are huge in size and demand expensivecompute resources for training. This is partly because LLMs for HPC tasks areobtained by finetuning existing LLMs that support several natural and/orprogramming languages. We found this design choice confusing - why do we needLLMs trained on natural languages and programming languages unrelated to HPCfor HPC-specific tasks? In this line of work, we aim to question choices madeby existing LLMs by developing smaller language models (LMs) for specificdomains - we call them domain-specific LMs. Specifically, we start with HPC asa domain and build an HPC-specific LM, named MonoCoder, which is orders ofmagnitude smaller than existing LMs but delivers better performance on non-HPCand HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specificdataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluatedthe performance of MonoCoder against state-of-the-art multi-lingual LLMs.Results demonstrate that MonoCoder, although much smaller than existing LMs,outperforms other LLMs on normalized-perplexity tests (in relation to modelsize) while also delivering competing CodeBLEU scores for high-performance andparallel code generations. In other words, results suggest that MonoCoderunderstands HPC code better than state-of-the-art LLMs.</description><author>Tal Kadosh, Niranjan Hasabnis, Vy A. Vo, Nadav Schneider, Neva Krien, Mihai Capota, Abdul Wasay, Nesreen Ahmed, Ted Willke, Guy Tamir, Yuval Pinter, Timothy Mattson, Gal Oren</author><pubDate>Tue, 17 Sep 2024 16:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13322v2</guid></item><item><title>BoostDream: Efficient Refining for High-Quality Text-to-3D Generation from Multi-View Diffusion</title><link>http://arxiv.org/abs/2401.16764v3</link><description>Witnessing the evolution of text-to-image diffusion models, significantstrides have been made in text-to-3D generation. Currently, two primaryparadigms dominate the field of text-to-3D: the feed-forward generationsolutions, capable of swiftly producing 3D assets but often yielding coarseresults, and the Score Distillation Sampling (SDS) based solutions, known forgenerating high-fidelity 3D assets albeit at a slower pace. The synergisticintegration of these methods holds substantial promise for advancing 3Dgeneration techniques. In this paper, we present BoostDream, a highly efficientplug-and-play 3D refining method designed to transform coarse 3D assets intohigh-quality. The BoostDream framework comprises three distinct processes: (1)We introduce 3D model distillation that fits differentiable representationsfrom the 3D assets obtained through feed-forward generation. (2) A novelmulti-view SDS loss is designed, which utilizes a multi-view aware 2D diffusionmodel to refine the 3D assets. (3) We propose to use prompt and multi-viewconsistent normal maps as guidance in refinement.Our extensive experiment isconducted on different differentiable 3D representations, revealing thatBoostDream excels in generating high-quality 3D assets rapidly, overcoming theJanus problem compared to conventional SDS-based methods. This breakthroughsignifies a substantial advancement in both the efficiency and quality of 3Dgeneration processes.</description><author>Yonghao Yu, Shunan Zhu, Huai Qin, Haorui Li</author><pubDate>Tue, 17 Sep 2024 16:28:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16764v3</guid></item><item><title>Reducing Catastrophic Forgetting in Online Class Incremental Learning Using Self-Distillation</title><link>http://arxiv.org/abs/2409.11329v1</link><description>In continual learning, there is a serious problem of catastrophic forgetting,in which previous knowledge is forgotten when a model learns new tasks. Variousmethods have been proposed to solve this problem. Replay methods which replaydata from previous tasks in later training, have shown good accuracy. However,replay methods have a generalizability problem from a limited memory buffer. Inthis paper, we tried to solve this problem by acquiring transferable knowledgethrough self-distillation using highly generalizable output in shallow layer asa teacher. Furthermore, when we deal with a large number of classes orchallenging data, there is a risk of learning not converging and notexperiencing overfitting. Therefore, we attempted to achieve more efficient andthorough learning by prioritizing the storage of easily misclassified samplesthrough a new method of memory update. We confirmed that our proposed methodoutperformed conventional methods by experiments on CIFAR10, CIFAR100, andMiniimageNet datasets.</description><author>Kotaro Nagata, Hiromu Ono, Kazuhiro Hotta</author><pubDate>Tue, 17 Sep 2024 16:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11329v1</guid></item><item><title>Learning Unstable Continuous-Time Stochastic Linear Control Systems</title><link>http://arxiv.org/abs/2409.11327v1</link><description>We study the problem of system identification for stochastic continuous-timedynamics, based on a single finite-length state trajectory. We present a methodfor estimating the possibly unstable open-loop matrix by employing properlyrandomized control inputs. Then, we establish theoretical performanceguarantees showing that the estimation error decays with trajectory length, ameasure of excitability, and the signal-to-noise ratio, while it grows withdimension. Numerical illustrations that showcase the rates of learning thedynamics, will be provided as well. To perform the theoretical analysis, wedevelop new technical tools that are of independent interest. That includesnon-asymptotic stochastic bounds for highly non-stationary martingales andgeneralized laws of iterated logarithms, among others.</description><author>Reza Sadeghi Hafshejani, Mohamad Kazem Shirani Fradonbeh</author><pubDate>Tue, 17 Sep 2024 16:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11327v1</guid></item><item><title>Learning by Self-Explaining</title><link>http://arxiv.org/abs/2309.08395v3</link><description>Much of explainable AI research treats explanations as a means for modelinspection. Yet, this neglects findings from human psychology that describe thebenefit of self-explanations in an agent's learning process. Motivated by this,we introduce a novel workflow in the context of image classification, termedLearning by Self-Explaining (LSX). LSX utilizes aspects of self-refining AI andhuman-guided explanatory machine learning. The underlying idea is that alearner model, in addition to optimizing for the original predictive task, isfurther optimized based on explanatory feedback from an internal critic model.Intuitively, a learner's explanations are considered "useful" if the internalcritic can perform the same task given these explanations. We provide anoverview of important components of LSX and, based on this, perform extensiveexperimental evaluations via three different example instantiations. Ourresults indicate improvements via Learning by Self-Explaining on severallevels: in terms of model generalization, reducing the influence of confoundingfactors, and providing more task-relevant and faithful model explanations.Overall, our work provides evidence for the potential of self-explaining withinthe learning phase of an AI model.</description><author>Wolfgang Stammer, Felix Friedrich, David Steinmann, Manuel Brack, Hikaru Shindo, Kristian Kersting</author><pubDate>Tue, 17 Sep 2024 16:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08395v3</guid></item><item><title>LYT-NET: Lightweight YUV Transformer-based Network for Low-light Image Enhancement</title><link>http://arxiv.org/abs/2401.15204v6</link><description>This letter introduces LYT-Net, a novel lightweight transformer-based modelfor low-light image enhancement (LLIE). LYT-Net consists of several layers anddetachable blocks, including our novel blocks--Channel-Wise Denoiser (CWD) andMulti-Stage Squeeze &amp; Excite Fusion (MSEF)--along with the traditionalTransformer block, Multi-Headed Self-Attention (MHSA). In our method we adopt adual-path approach, treating chrominance channels U and V and luminance channelY as separate entities to help the model better handle illumination adjustmentand corruption restoration. Our comprehensive evaluation on established LLIEdatasets demonstrates that, despite its low complexity, our model outperformsrecent LLIE methods. The source code and pre-trained models are available athttps://github.com/albrateanu/LYT-Net</description><author>A. Brateanu, R. Balmez, A. Avram, C. Orhei, C. Ancuti</author><pubDate>Tue, 17 Sep 2024 16:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15204v6</guid></item><item><title>TopoMaskV2: Enhanced Instance-Mask-Based Formulation for the Road Topology Problem</title><link>http://arxiv.org/abs/2409.11325v1</link><description>Recently, the centerline has become a popular representation of lanes due toits advantages in solving the road topology problem. To enhance centerlineprediction, we have developed a new approach called TopoMask. Unlike previousmethods that rely on keypoints or parametric methods, TopoMask utilizes aninstance-mask-based formulation coupled with a masked-attention-basedtransformer architecture. We introduce a quad-direction label representation toenrich the mask instances with flow information and design a correspondingpost-processing technique for mask-to-centerline conversion. Additionally, wedemonstrate that the instance-mask formulation provides complementaryinformation to parametric Bezier regressions, and fusing both outputs leads toimproved detection and topology performance. Moreover, we analyze theshortcomings of the pillar assumption in the Lift Splat technique and adapt amulti-height bin configuration. Experimental results show that TopoMaskachieves state-of-the-art performance in the OpenLane-V2 dataset, increasingfrom 44.1 to 49.4 for Subset-A and 44.7 to 51.8 for Subset-B in the V1.1 OLSbaseline.</description><author>M. Esat Kalfaoglu, Halil Ibrahim Ozturk, Ozsel Kilinc, Alptekin Temizel</author><pubDate>Tue, 17 Sep 2024 16:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11325v1</guid></item><item><title>Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on Supervised Regression (Preprint)</title><link>http://arxiv.org/abs/2408.12308v2</link><description>In this tutorial, we present a compact and holistic discussion of DeepLearning with a focus on Convolutional Neural Networks (CNNs) and supervisedregression. While there are numerous books and articles on the individualtopics we cover, comprehensive and detailed tutorials that address DeepLearning from a foundational yet rigorous and accessible perspective are rare.Most resources on CNNs are either too advanced, focusing on cutting-edgearchitectures, or too narrow, addressing only specific applications like imageclassification.This tutorial not only summarizes the most relevant concepts butalso provides an in-depth exploration of each, offering a complete yet agileset of ideas. Moreover, we highlight the powerful synergy between learningtheory, statistic, and machine learning, which together underpin the DeepLearning and CNN frameworks. We aim for this tutorial to serve as an optimalresource for students, professors, and anyone interested in understanding thefoundations of Deep Learning. Upon acceptance we will provide an accompanyingrepository under\href{https://github.com/neoglez/deep-learning-tutorial}{https://github.com/neoglez/deep-learning-tutorial} Keywords: Tutorial, Deep Learning, Convolutional Neural Networks, MachineLearning.</description><author>Yansel Gonzalez Tejeda, Helmut A. Mayer</author><pubDate>Tue, 17 Sep 2024 16:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12308v2</guid></item><item><title>Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains</title><link>http://arxiv.org/abs/2406.11423v2</link><description>Proactive content moderation requires platforms to rapidly and continuouslyevaluate the credibility of websites. Leveraging the direct and indirect pathsusers follow to unreliable websites, we develop a website credibilityclassification and discovery system that integrates both webgraph andlarge-scale social media contexts. We additionally introduce the concept ofdredge words, terms or phrases for which unreliable domains rank highly onsearch engines, and provide the first exploration of their usage on socialmedia. Our graph neural networks that combine webgraph and social mediacontexts generate to state-of-the-art results in website credibilityclassification and significantly improves the top-k identification ofunreliable domains. Additionally, we release a novel dataset of dredge words,highlighting their strong connections to both social media and online commerceplatforms.</description><author>Evan M. Williams, Peter Carragher, Kathleen M. Carley</author><pubDate>Tue, 17 Sep 2024 16:20:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11423v2</guid></item><item><title>LPT++: Efficient Training on Mixture of Long-tailed Experts</title><link>http://arxiv.org/abs/2409.11323v1</link><description>We introduce LPT++, a comprehensive framework for long-tailed classificationthat combines parameter-efficient fine-tuning (PEFT) with a learnable modelensemble. LPT++ enhances frozen Vision Transformers (ViTs) through theintegration of three core components. The first is a universal long-tailedadaptation module, which aggregates long-tailed prompts and visual adapters toadapt the pretrained model to the target domain, meanwhile improving itsdiscriminative ability. The second is the mixture of long-tailed expertsframework with a mixture-of-experts (MoE) scorer, which adaptively calculatesreweighting coefficients for confidence scores from both visual-only andvisual-language (VL) model experts to generate more accurate predictions.Finally, LPT++ employs a three-phase training framework, wherein each criticalmodule is learned separately, resulting in a stable and effective long-tailedclassification training paradigm. Besides, we also propose the simple versionof LPT++ namely LPT, which only integrates visual-only pretrained ViT andlong-tailed prompts to formulate a single model method. LPT can clearlyillustrate how long-tailed prompts works meanwhile achieving comparableperformance without VL pretrained models. Experiments show that, with only ~1%extra trainable parameters, LPT++ achieves comparable accuracy against all thecounterparts.</description><author>Bowen Dong, Pan Zhou, Wangmeng Zuo</author><pubDate>Tue, 17 Sep 2024 16:19:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11323v1</guid></item><item><title>When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather</title><link>http://arxiv.org/abs/2403.13762v2</link><description>In Federated Learning (FL), multiple clients collaboratively train a globalmodel without sharing private data. In semantic segmentation, the Federatedsource Free Domain Adaptation (FFreeDA) setting is of particular interest,where clients undergo unsupervised training after supervised pretraining at theserver side. While few recent works address FL for autonomous vehicles,intrinsic real-world challenges such as the presence of adverse weatherconditions and the existence of different autonomous agents are stillunexplored. To bridge this gap, we address both problems and introduce a newfederated semantic segmentation setting where both car and drone clientsco-exist and collaborate. Specifically, we propose a novel approach for thissetting which exploits a batch-norm weather-aware strategy to dynamically adaptthe model to the different weather conditions, while hyperbolic spaceprototypes are used to align the heterogeneous client representations. Finally,we introduce FLYAWARE, the first semantic segmentation dataset with adverseweather data for aerial vehicles.</description><author>Giulia Rizzoli, Matteo Caligiuri, Donald Shenaj, Francesco Barbato, Pietro Zanuttigh</author><pubDate>Tue, 17 Sep 2024 16:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13762v2</guid></item><item><title>SOAP: Improving and Stabilizing Shampoo using Adam</title><link>http://arxiv.org/abs/2409.11321v1</link><description>There is growing evidence of the effectiveness of Shampoo, a higher-orderpreconditioning method, over Adam in deep learning optimization tasks. However,Shampoo's drawbacks include additional hyperparameters and computationaloverhead when compared to Adam, which only updates running averages of first-and second-moment quantities. This work establishes a formal connection betweenShampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficientapproximation of Adam -- showing that Shampoo is equivalent to runningAdafactor in the eigenbasis of Shampoo's preconditioner. This insight leads tothe design of a simpler and computationally efficient algorithm:$\textbf{S}$hampo$\textbf{O}$ with $\textbf{A}$dam in the$\textbf{P}$reconditioner's eigenbasis (SOAP). With regards to improving Shampoo's computational efficiency, the moststraightforward approach would be to simply compute Shampoo'seigendecomposition less frequently. Unfortunately, as our empirical resultsshow, this leads to performance degradation that worsens with this frequency.SOAP mitigates this degradation by continually updating the running average ofthe second moment, just as Adam does, but in the current (slowly changing)coordinate basis. Furthermore, since SOAP is equivalent to running Adam in arotated space, it introduces only one additional hyperparameter (thepreconditioning frequency) compared to Adam. We empirically evaluate SOAP onlanguage model pre-training with 360m and 660m sized models. In the large batchregime, SOAP reduces the number of iterations by over 40% and wall clock timeby over 35% compared to AdamW, with approximately 20% improvements in bothmetrics compared to Shampoo. An implementation of SOAP is available athttps://github.com/nikhilvyas/SOAP.</description><author>Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, Sham Kakade</author><pubDate>Tue, 17 Sep 2024 16:18:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11321v1</guid></item><item><title>MSDNet: Multi-Scale Decoder for Few-Shot Semantic Segmentation via Transformer-Guided Prototyping</title><link>http://arxiv.org/abs/2409.11316v1</link><description>Few-shot Semantic Segmentation addresses the challenge of segmenting objectsin query images with only a handful of annotated examples. However, manyprevious state-of-the-art methods either have to discard intricate localsemantic features or suffer from high computational complexity. To addressthese challenges, we propose a new Few-shot Semantic Segmentation frameworkbased on the transformer architecture. Our approach introduces the spatialtransformer decoder and the contextual mask generation module to improve therelational understanding between support and query images. Moreover, weintroduce a multi-scale decoder to refine the segmentation mask byincorporating features from different resolutions in a hierarchical manner.Additionally, our approach integrates global features from intermediate encoderstages to improve contextual understanding, while maintaining a lightweightstructure to reduce complexity. This balance between performance and efficiencyenables our method to achieve state-of-the-art results on benchmark datasetssuch as $PASCAL-5^i$ and $COCO-20^i$ in both 1-shot and 5-shot settings.Notably, our model with only 1.5 million parameters demonstrates competitiveperformance while overcoming limitations of existing methodologies.https://github.com/amirrezafateh/MSDNet</description><author>Amirreza Fateh, Mohammad Reza Mohammadi, Mohammad Reza Jahed Motlagh</author><pubDate>Tue, 17 Sep 2024 16:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11316v1</guid></item><item><title>fMRI-3D: A Comprehensive Dataset for Enhancing fMRI-based 3D Reconstruction</title><link>http://arxiv.org/abs/2409.11315v1</link><description>Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI)data, introduced as Recon3DMind in our conference work, is of significantinterest to both cognitive neuroscience and computer vision. To advance thistask, we present the fMRI-3D dataset, which includes data from 15 participantsand showcases a total of 4768 3D objects. The dataset comprises two components:fMRI-Shape, previously introduced and accessible athttps://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape, and fMRI-Objaverse,proposed in this paper and available athttps://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverseincludes data from 5 subjects, 4 of whom are also part of the Core set infMRI-Shape, with each subject viewing 3142 3D objects across 117 categories,all accompanied by text captions. This significantly enhances the diversity andpotential applications of the dataset. Additionally, we propose MinD-3D, anovel framework designed to decode 3D visual information from fMRI signals. Theframework first extracts and aggregates features from fMRI data using aneuro-fusion encoder, then employs a feature-bridge diffusion model to generatevisual features, and finally reconstructs the 3D object using a generativetransformer decoder. We establish new benchmarks by designing metrics at bothsemantic and structural levels to evaluate model performance. Furthermore, weassess our model's effectiveness in an Out-of-Distribution setting and analyzethe attribution of the extracted features and the visual ROIs in fMRI signals.Our experiments demonstrate that MinD-3D not only reconstructs 3D objects withhigh semantic and spatial accuracy but also deepens our understanding of howhuman brain processes 3D visual information. Project page at:https://jianxgao.github.io/MinD-3D.</description><author>Jianxiong Gao, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng Feng, Yanwei Fu</author><pubDate>Tue, 17 Sep 2024 16:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11315v1</guid></item><item><title>SpMis: An Investigation of Synthetic Spoken Misinformation Detection</title><link>http://arxiv.org/abs/2409.11308v1</link><description>In recent years, speech generation technology has advanced rapidly, fueled bygenerative models and large-scale training techniques. While these developmentshave enabled the production of high-quality synthetic speech, they have alsoraised concerns about the misuse of this technology, particularly forgenerating synthetic misinformation. Current research primarily focuses ondistinguishing machine-generated speech from human-produced speech, but themore urgent challenge is detecting misinformation within spoken content. Thistask requires a thorough analysis of factors such as speaker identity, topic,and synthesis. To address this need, we conduct an initial investigation intosynthetic spoken misinformation detection by introducing an open-sourcedataset, SpMis. SpMis includes speech synthesized from over 1,000 speakersacross five common topics, utilizing state-of-the-art text-to-speech systems.Although our results show promising detection capabilities, they also revealsubstantial challenges for practical implementation, underscoring theimportance of ongoing research in this critical area.</description><author>Peizhuo Liu, Li Wang, Renqiang He, Haorui He, Lei Wang, Huadi Zheng, Jie Shi, Tong Xiao, Zhizheng Wu</author><pubDate>Tue, 17 Sep 2024 16:05:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11308v1</guid></item><item><title>GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module</title><link>http://arxiv.org/abs/2409.11307v1</link><description>3D Gaussian Splatting (3DGS) integrates the strengths of primitive-basedrepresentations and volumetric rendering techniques, enabling real-time,high-quality rendering. However, 3DGS models typically overfit to single-scenetraining and are highly sensitive to the initialization of Gaussian ellipsoids,heuristically derived from Structure from Motion (SfM) point clouds, whichlimits both generalization and practicality. To address these limitations, wepropose GS-Net, a generalizable, plug-and-play 3DGS module that densifiesGaussian ellipsoids from sparse SfM point clouds, enhancing geometric structurerepresentation. To the best of our knowledge, GS-Net is the first plug-and-play3DGS module with cross-scene generalization capabilities. Additionally, weintroduce the CARLA-NVS dataset, which incorporates additional cameraviewpoints to thoroughly evaluate reconstruction and rendering quality.Extensive experiments demonstrate that applying GS-Net to 3DGS yields a PSNRimprovement of 2.08 dB for conventional viewpoints and 1.86 dB for novelviewpoints, confirming the method's effectiveness and robustness.</description><author>Yichen Zhang, Zihan Wang, Jiali Han, Peilin Li, Jiaxun Zhang, Jianqiang Wang, Lei He, Keqiang Li</author><pubDate>Tue, 17 Sep 2024 16:03:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11307v1</guid></item><item><title>UniMODE: Unified Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2402.18573v4</link><description>Realizing unified monocular 3D object detection, including both indoor andoutdoor scenes, holds great importance in applications like robot navigation.However, involving various scenarios of data to train models poses challengesdue to their significantly different characteristics, e.g., diverse geometryproperties and heterogeneous domain distributions. To address these challenges,we build a detector based on the bird's-eye-view (BEV) detection paradigm,where the explicit feature projection is beneficial to addressing the geometrylearning ambiguity when employing multiple scenarios of data to traindetectors. Then, we split the classical BEV detection architecture into twostages and propose an uneven BEV grid design to handle the convergenceinstability caused by the aforementioned challenges. Moreover, we develop asparse BEV feature projection strategy to reduce computational cost and aunified domain alignment method to handle heterogeneous domains. Combiningthese techniques, a unified detector UniMODE is derived, which surpasses theprevious state-of-the-art on the challenging Omni3D dataset (a large-scaledataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing thefirst successful generalization of a BEV detector to unified 3D objectdetection.</description><author>Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao</author><pubDate>Tue, 17 Sep 2024 16:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18573v4</guid></item><item><title>Beyond LoRA: Exploring Efficient Fine-Tuning Techniques for Time Series Foundational Models</title><link>http://arxiv.org/abs/2409.11302v1</link><description>Time Series Foundation Models (TSFMs) have recently garnered attention fortheir ability to model complex, large-scale time series data across domainssuch as retail, finance, and transportation. However, their application tosensitive, domain-specific fields like healthcare remains challenging,primarily due to the difficulty of fine-tuning these models for specialized,out-of-domain tasks with scarce publicly available datasets. In this work, weexplore the use of Parameter-Efficient Fine-Tuning (PEFT) techniques to addressthese limitations, focusing on healthcare applications, particularly ICU vitalsforecasting for sepsis patients. We introduce and evaluate two selective(BitFit and LayerNorm Tuning) and two additive (VeRA and FourierFT) PEFTtechniques on multiple configurations of the Chronos TSFM for forecasting vitalsigns of sepsis patients. Our comparative analysis demonstrates that some ofthese PEFT methods outperform LoRA in terms of parameter efficiency and domainadaptation, establishing state-of-the-art (SOTA) results in ICU vitalforecasting tasks. Interestingly, FourierFT applied to the Chronos (Tiny)variant surpasses the SOTA model while fine-tuning only 2,400 parameterscompared to the 700K parameters of the benchmark.</description><author>Divij Gupta, Anubhav Bhatti, Surajsinh Parmar</author><pubDate>Tue, 17 Sep 2024 15:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11302v1</guid></item><item><title>TTT-Unet: Enhancing U-Net with Test-Time Training Layers for biomedical image segmentation</title><link>http://arxiv.org/abs/2409.11299v1</link><description>Biomedical image segmentation is crucial for accurately diagnosing andanalyzing various diseases. However, Convolutional Neural Networks (CNNs) andTransformers, the most commonly used architectures for this task, struggle toeffectively capture long-range dependencies due to the inherent locality ofCNNs and the computational complexity of Transformers. To address thislimitation, we introduce TTT-Unet, a novel framework that integrates Test-TimeTraining (TTT) layers into the traditional U-Net architecture for biomedicalimage segmentation. TTT-Unet dynamically adjusts model parameters during thetesting time, enhancing the model's ability to capture both local andlong-range features. We evaluate TTT-Unet on multiple medical imaging datasets,including 3D abdominal organ segmentation in CT and MR images, instrumentsegmentation in endoscopy images, and cell segmentation in microscopy images.The results demonstrate that TTT-Unet consistently outperforms state-of-the-artCNN-based and Transformer-based segmentation models across all tasks. The codeis available at https://github.com/rongzhou7/TTT-Unet.</description><author>Rong Zhou, Zhengqing Yuan, Zhiling Yan, Weixiang Sun, Kai Zhang, Yiwei Li, Yanfang Ye, Xiang Li, Lifang He, Lichao Sun</author><pubDate>Tue, 17 Sep 2024 15:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11299v1</guid></item><item><title>SMILe: Leveraging Submodular Mutual Information For Robust Few-Shot Object Detection</title><link>http://arxiv.org/abs/2407.02665v2</link><description>Confusion and forgetting of object classes have been challenges of primeinterest in Few-Shot Object Detection (FSOD). To overcome these pitfalls inmetric learning based FSOD techniques, we introduce a novel Submodular MutualInformation Learning (SMILe) framework which adopts combinatorial mutualinformation functions to enforce the creation of tighter and discriminativefeature clusters in FSOD. Our proposed approach generalizes to several existingapproaches in FSOD, agnostic of the backbone architecture demonstratingelevated performance gains. A paradigm shift from instance based objectivefunctions to combinatorial objectives in SMILe naturally preserves thediversity within an object class resulting in reduced forgetting when subjectedto few training examples. Furthermore, the application of mutual informationbetween the already learnt (base) and newly added (novel) objects ensuressufficient separation between base and novel classes, minimizing the effect ofclass confusion. Experiments on popular FSOD benchmarks, PASCAL-VOC and MS-COCOshow that our approach generalizes to State-of-the-Art (SoTA) approachesimproving their novel class performance by up to 5.7% (3.3 mAP points) and 5.4%(2.6 mAP points) on the 10-shot setting of VOC (split 3) and 30-shot setting ofCOCO datasets respectively. Our experiments also demonstrate better retentionof base class performance and up to 2x faster convergence over existingapproaches agnostic of the underlying architecture.</description><author>Anay Majee, Ryan Sharp, Rishabh Iyer</author><pubDate>Tue, 17 Sep 2024 15:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02665v2</guid></item><item><title>EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage</title><link>http://arxiv.org/abs/2409.11295v1</link><description>Generalist web agents have evolved rapidly and demonstrated remarkablepotential. However, there are unprecedented safety risks associated with thesethem, which are nearly unexplored so far. In this work, we aim to narrow thisgap by conducting the first study on the privacy risks of generalist web agentsin adversarial environments. First, we present a threat model that discussesthe adversarial targets, constraints, and attack scenarios. Particularly, weconsider two types of adversarial targets: stealing users' specific personallyidentifiable information (PII) or stealing the entire user request. To achievethese objectives, we propose a novel attack method, termed EnvironmentalInjection Attack (EIA). This attack injects malicious content designed to adaptwell to different environments where the agents operate, causing them toperform unintended actions. This work instantiates EIA specifically for theprivacy scenario. It inserts malicious web elements alongside persuasiveinstructions that mislead web agents into leaking private information, and canfurther leverage CSS and JavaScript features to remain stealthy. We collect 177actions steps that involve diverse PII categories on realistic websites fromthe Mind2Web dataset, and conduct extensive experiments using one of the mostcapable generalist web agent frameworks to date, SeeAct. The resultsdemonstrate that EIA achieves up to 70% ASR in stealing users' specific PII.Stealing full user requests is more challenging, but a relaxed version of EIAcan still achieve 16% ASR. Despite these concerning results, it is important tonote that the attack can still be detectable through careful human inspection,highlighting a trade-off between high autonomy and security. This leads to ourdetailed discussion on the efficacy of EIA under different levels of humansupervision as well as implications on defenses for generalist web agents.</description><author>Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun</author><pubDate>Tue, 17 Sep 2024 15:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11295v1</guid></item><item><title>Navigating Process Mining: A Case study using pm4py</title><link>http://arxiv.org/abs/2409.11294v1</link><description>Process-mining techniques have emerged as powerful tools for analyzing eventdata to gain insights into business processes. In this paper, we present acomprehensive analysis of road traffic fine management processes using thepm4py library in Python. We start by importing an event log dataset and exploreits characteristics, including the distribution of activities and processvariants. Through filtering and statistical analysis, we uncover key patternsand variations in the process executions. Subsequently, we apply variousprocess-mining algorithms, including the Alpha Miner, Inductive Miner, andHeuristic Miner, to discover process models from the event log data. Wevisualize the discovered models to understand the workflow structures anddependencies within the process. Additionally, we discuss the strengths andlimitations of each mining approach in capturing the underlying processdynamics. Our findings shed light on the efficiency and effectiveness of roadtraffic fine management processes, providing valuable insights for processoptimization and decision-making. This study demonstrates the utility of pm4pyin facilitating process mining tasks and its potential for analyzing real-worldbusiness processes.</description><author>Ali Jlidi, László Kovács</author><pubDate>Tue, 17 Sep 2024 15:48:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11294v1</guid></item><item><title>SeFlow: A Self-Supervised Scene Flow Method in Autonomous Driving</title><link>http://arxiv.org/abs/2407.01702v2</link><description>Scene flow estimation predicts the 3D motion at each point in successiveLiDAR scans. This detailed, point-level, information can help autonomousvehicles to accurately predict and understand dynamic changes in theirsurroundings. Current state-of-the-art methods require annotated data to trainscene flow networks and the expense of labeling inherently limits theirscalability. Self-supervised approaches can overcome the above limitations, yetface two principal challenges that hinder optimal performance: pointdistribution imbalance and disregard for object-level motion constraints. Inthis paper, we propose SeFlow, a self-supervised method that integratesefficient dynamic classification into a learning-based scene flow pipeline. Wedemonstrate that classifying static and dynamic points helps design targetedobjective functions for different motion patterns. We also emphasize theimportance of internal cluster consistency and correct object point associationto refine the scene flow estimation, in particular on object details. Ourreal-time capable method achieves state-of-the-art performance on theself-supervised scene flow task on Argoverse 2 and Waymo datasets. The code isopen-sourced at https://github.com/KTH-RPL/SeFlow along with trained modelweights.</description><author>Qingwen Zhang, Yi Yang, Peizheng Li, Olov Andersson, Patric Jensfelt</author><pubDate>Tue, 17 Sep 2024 15:47:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01702v2</guid></item><item><title>Neural Networks for Vehicle Routing Problem</title><link>http://arxiv.org/abs/2409.11290v1</link><description>The Vehicle Routing Problem is about optimizing the routes of vehicles tomeet the needs of customers at specific locations. The route graph consists ofdepots on several levels and customer positions. Several optimization methodshave been developed over the years, most of which are based on some type ofclassic heuristic: genetic algorithm, simulated annealing, tabu search, antcolony optimization, firefly algorithm. Recent developments in machine learningprovide a new toolset, the rich family of neural networks, for tackling complexproblems. The main area of application of neural networks is the area ofclassification and regression. Route optimization can be viewed as a newchallenge for neural networks. The article first presents an analysis of theapplicability of neural network tools, then a novel graphical neural networkmodel is presented in detail. The efficiency analysis based on test experimentsshows the applicability of the proposed NN architecture.</description><author>László Kovács, Ali Jlidi</author><pubDate>Tue, 17 Sep 2024 15:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11290v1</guid></item><item><title>Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling</title><link>http://arxiv.org/abs/2409.11283v1</link><description>LLMs obtain remarkable performance but suffer from hallucinations. Mostresearch on detecting hallucination focuses on the questions with short andconcrete correct answers that are easy to check the faithfulness. Hallucinationdetections for text generation with open-ended answers are more challenging.Some researchers use external knowledge to detect hallucinations in generatedtexts, but external resources for specific scenarios are hard to access. Recentstudies on detecting hallucinations in long text without external resourcesconduct consistency comparison among multiple sampled outputs. To handle longtexts, researchers split long texts into multiple facts and individuallycompare the consistency of each pairs of facts. However, these methods (1)hardly achieve alignment among multiple facts; (2) overlook dependenciesbetween multiple contextual facts. In this paper, we propose a graph-basedcontext-aware (GCA) hallucination detection for text generations, which alignsknowledge facts and considers the dependencies between contextual knowledgetriples in consistency comparison. Particularly, to align multiple facts, weconduct a triple-oriented response segmentation to extract multiple knowledgetriples. To model dependencies among contextual knowledge triple (facts), weconstruct contextual triple into a graph and enhance triples' interactions viamessage passing and aggregating via RGCN. To avoid the omission of knowledgetriples in long text, we conduct a LLM-based reverse verification viareconstructing the knowledge triples. Experiments show that our model enhanceshallucination detection and excels all baselines.</description><author>Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li</author><pubDate>Tue, 17 Sep 2024 15:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11283v1</guid></item><item><title>Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5</title><link>http://arxiv.org/abs/2409.11282v1</link><description>The surge of digital documents in various formats, including lessstandardized documents such as business reports and environmental assessments,underscores the growing importance of Document Understanding. While LargeLanguage Models (LLMs) have showcased prowess across diverse natural languageprocessing tasks, their direct application to Document Understanding remains achallenge. Previous research has demonstrated the utility of LLMs in thisdomain, yet their significant computational demands make them challenging todeploy effectively. Additionally, proprietary Blackbox LLMs often outperformtheir open-source counterparts, posing a barrier to widespread accessibility.In this paper, we delve into the realm of document understanding, leveragingdistillation methods to harness the power of large LLMs while accommodatingcomputational limitations. Specifically, we present a novel approach wherein wedistill document understanding knowledge from the proprietary LLM ChatGPT intoFLAN-T5. Our methodology integrates labeling and curriculum-learning mechanismsto facilitate efficient knowledge transfer. This work contributes to theadvancement of document understanding methodologies by offering a scalablesolution that bridges the gap between resource-intensive LLMs and practicalapplications. Our findings underscore the potential of distillation techniquesin facilitating the deployment of sophisticated language models in real-worldscenarios, thereby fostering advancements in natural language processing anddocument comprehension domains.</description><author>Marcel Lamott, Muhammad Armaghan Shakir</author><pubDate>Tue, 17 Sep 2024 15:37:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11282v1</guid></item><item><title>P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task</title><link>http://arxiv.org/abs/2409.11279v1</link><description>Embodied Everyday Task is a popular task in the embodied AI community,requiring agents to make a sequence of actions based on natural languageinstructions and visual observations. Traditional learning-based approachesface two challenges. Firstly, natural language instructions often lack explicittask planning. Secondly, extensive training is required to equip models withknowledge of the task environment. Previous works based on Large Language Model(LLM) either suffer from poor performance due to the lack of task-specificknowledge or rely on ground truth as few-shot samples. To address the abovelimitations, we propose a novel approach called Progressive Retrieval AugmentedGeneration (P-RAG), which not only effectively leverages the powerful languageprocessing capabilities of LLMs but also progressively accumulatestask-specific knowledge without ground-truth. Compared to the conventional RAGmethods, which retrieve relevant information from the database in a one-shotmanner to assist generation, P-RAG introduces an iterative approach toprogressively update the database. In each iteration, P-RAG retrieves thelatest database and obtains historical information from the previousinteraction as experiential references for the current interaction. Moreover,we also introduce a more granular retrieval scheme that not only retrievessimilar tasks but also incorporates retrieval of similar situations to providemore valuable reference experiences. Extensive experiments reveal that P-RAGachieves competitive results without utilizing ground truth and can evenfurther improve performance through self-iterations.</description><author>Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li</author><pubDate>Tue, 17 Sep 2024 15:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11279v1</guid></item><item><title>Machine Learning and Theory Ladenness -- A Phenomenological Account</title><link>http://arxiv.org/abs/2409.11277v1</link><description>In recent years, the dissemination of machine learning (ML) methodologies inscientific research has prompted discussions on theory ladenness. Morespecifically, the issue of theory ladenness has remerged as questions aboutwhether and how ML models (MLMs) and ML modelling strategies are impacted bythe domain theory of the scientific field in which ML is used and implemented(e.g., physics, chemistry, biology, etc). On the one hand, some have arguedthat there is no difference between traditional (pre ML) and ML assistedscience. In both cases, theory plays an essential and unavoidable role in theanalysis of phenomena and the construction and use of models. Others haveargued instead that ML methodologies and models are theory independent and, insome cases, even theory free. In this article, we argue that both positions areoverly simplistic and do not advance our understanding of the interplay betweenML methods and domain theories. Specifically, we provide an analysis of theoryladenness in ML assisted science. Our analysis reveals that, while theconstruction of MLMs can be relatively independent of domain theory, thepractical implementation and interpretation of these models within a givenspecific domain still relies on fundamental theoretical assumptions andbackground knowledge.</description><author>Alberto Termine, Emanuele Ratti, Alessandro Facchini</author><pubDate>Tue, 17 Sep 2024 15:29:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11277v1</guid></item><item><title>A Systematic Review of Generalization Research in Medical Image Classification</title><link>http://arxiv.org/abs/2403.12167v3</link><description>Numerous Deep Learning (DL) classification models have been developed for alarge spectrum of medical image analysis applications, which promises toreshape various facets of medical practice. Despite early advances in DL modelvalidation and implementation, which encourage healthcare institutions to adoptthem, a fundamental questions remain: how can these models effectively handledomain shift? This question is crucial to limit DL models performancedegradation. Medical data are dynamic and prone to domain shift, due tomultiple factors. Two main shift types can occur over time: 1) covariate shiftmainly arising due to updates to medical equipment and 2) concept shift causedby inter-grader variability. To mitigate the problem of domain shift, existingsurveys mainly focus on domain adaptation techniques, with an emphasis oncovariate shift. More generally, no work has reviewed the state-of-the-artsolutions while focusing on the shift types. This paper aims to exploreexisting domain generalization methods for DL-based classification modelsthrough a systematic review of literature. It proposes a taxonomy based on theshift type they aim to solve. Papers were searched and gathered on Scopus till10 April 2023, and after the eligibility screening and quality evaluation, 77articles were identified. Exclusion criteria included: lack of methodologicalnovelty (e.g., reviews, benchmarks), experiments conducted on a singlemono-center dataset, or articles not written in English. The results of thispaper show that learning based methods are emerging, for both shift types.Finally, we discuss future challenges, including the need for improvedevaluation protocols and benchmarks, and envisioned future developments toachieve robust, generalized models for medical image classification.</description><author>Sarah Matta, Mathieu Lamard, Philippe Zhang, Alexandre Le Guilcher, Laurent Borderie, Béatrice Cochener, Gwenolé Quellec</author><pubDate>Tue, 17 Sep 2024 15:27:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12167v3</guid></item><item><title>MURRE: Multi-Hop Table Retrieval with Removal for Open-Domain Text-to-SQL</title><link>http://arxiv.org/abs/2402.10666v4</link><description>The open-domain text-to-SQL task aims to retrieve question-relevant tablesfrom massive databases and generate SQL. However, the performance of currentmethods is constrained by single-hop retrieval, and existing multi-hopretrieval of open-domain question answering is not directly applicable due tothe tendency to retrieve tables similar to the retrieved ones but irrelevant tothe question. Since the questions in text-to-SQL usually contain all requiredinformation, while previous multi-hop retrieval supplements the questions withretrieved documents. Therefore, we propose the multi-hop table retrieval withremoval (MURRE), which removes previously retrieved information from thequestion to guide the retriever towards unretrieved relevant tables. Ourexperiments on two open-domain text-to-SQL datasets demonstrate an averageimprovement of 5.7% over the previous state-of-the-art results.</description><author>Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Qingfu Zhu, Wanxiang Che</author><pubDate>Tue, 17 Sep 2024 15:25:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10666v4</guid></item><item><title>Task Arithmetic for Language Expansion in Speech Translation</title><link>http://arxiv.org/abs/2409.11274v1</link><description>Recent advances in large language models (LLMs) have gained interest inspeech-text multimodal foundation models, achieving strong performance oninstruction-based speech translation (ST). However, expanding language pairsfrom an existing instruction-tuned ST system is costly due to the necessity ofre-training on a combination of new and previous datasets. We propose to expandnew language pairs by merging the model trained on new language pairs and theexisting model, using task arithmetic. We find that the direct application oftask arithmetic for ST causes the merged model to fail to follow instructions;thus, generating translation in incorrect languages. To eliminate languageconfusion, we propose an augmented task arithmetic method that merges anadditional language control model. It is trained to generate the correct targetlanguage token following the instructions. Our experiments demonstrate that ourproposed language control model can achieve language expansion by eliminatinglanguage confusion. In our MuST-C and CoVoST-2 experiments, it shows up to 4.66and 4.92 BLEU scores improvement, respectively. In addition, we demonstrate theuse of our task arithmetic framework can expand to a language pair whereneither paired ST training data nor a pre-trained ST model is available. Wefirst synthesize the ST system from machine translation (MT) systems via taskanalogy, then merge the synthesized ST system to the existing ST model.</description><author>Yao-Fei Cheng, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Wen Shen Teo, Siddhant Arora, Shinji Watanabe</author><pubDate>Tue, 17 Sep 2024 15:25:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11274v1</guid></item><item><title>LOLA -- An Open-Source Massively Multilingual Large Language Model</title><link>http://arxiv.org/abs/2409.11272v1</link><description>This paper presents LOLA, a massively multilingual large language modeltrained on more than 160 languages using a sparse Mixture-of-ExpertsTransformer architecture. Our architectural and implementation choices addressthe challenge of harnessing linguistic diversity while maintaining efficiencyand avoiding the common pitfalls of multilinguality. Our analysis of theevaluation results shows competitive performance in natural language generationand understanding tasks. Additionally, we demonstrate how the learnedexpert-routing mechanism exploits implicit phylogenetic linguistic patterns topotentially alleviate the curse of multilinguality. We provide an in-depth lookat the training process, an analysis of the datasets, and a balancedexploration of the model's strengths and limitations. As an open-source model,LOLA promotes reproducibility and serves as a robust foundation for futureresearch. Our findings enable the development of compute-efficient multilingualmodels with strong, scalable performance across languages.</description><author>Nikit Srivastava, Denis Kuchelev, Tatiana Moteu, Kshitij Shetty, Michael Roeder, Diego Moussallem, Hamada Zahera, Axel-Cyrille Ngonga Ngomo</author><pubDate>Tue, 17 Sep 2024 15:23:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11272v1</guid></item><item><title>Geometry Aware Meta-Learning Neural Network for Joint Phase and Precoder Optimization in RIS</title><link>http://arxiv.org/abs/2409.11270v1</link><description>In reconfigurable intelligent surface (RIS) aided systems, the jointoptimization of the precoder matrix at the base station and the phase shifts ofthe RIS elements involves significant complexity. In this paper, we propose acomplex-valued, geometry aware meta-learning neural network that maximizes theweighted sum rate in a multi-user multiple input single output system. Byleveraging the complex circle geometry for phase shifts and spherical geometryfor the precoder, the optimization occurs on Riemannian manifolds, leading tofaster convergence. We use a complex-valued neural network for phase shifts andan Euler inspired update for the precoder network. Our approach outperformsexisting neural network-based algorithms, offering higher weighted sum rates,lower power consumption, and significantly faster convergence. Specifically, itconverges faster by nearly 100 epochs, with a 0.7 bps improvement in weightedsum rate and a 1.8 dBm power gain when compared with existing work.</description><author>Dahlia Devapriya, Sheetal Kalyani</author><pubDate>Tue, 17 Sep 2024 15:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11270v1</guid></item><item><title>Do Language Models Exhibit Human-like Structural Priming Effects?</title><link>http://arxiv.org/abs/2406.04847v2</link><description>We explore which linguistic factors -- at the sentence and token level --play an important role in influencing language model predictions, andinvestigate whether these are reflective of results found in humans and humancorpora (Gries and Kootstra, 2017). We make use of the structural primingparadigm, where recent exposure to a structure facilitates processing of thesame structure. We don't only investigate whether, but also where primingeffects occur, and what factors predict them. We show that these effects can beexplained via the inverse frequency effect, known in human priming, where rarerelements within a prime increase priming effects, as well as lexical dependencebetween prime and target. Our results provide an important piece in the puzzleof understanding how properties within their context affect structuralprediction in language models.</description><author>Jaap Jumelet, Willem Zuidema, Arabella Sinclair</author><pubDate>Tue, 17 Sep 2024 15:17:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04847v2</guid></item><item><title>Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids</title><link>http://arxiv.org/abs/2409.11267v1</link><description>This work proposes an approach that integrates reinforcement learning andmodel predictive control (MPC) to efficiently solve finite-horizon optimalcontrol problems in mixed-logical dynamical systems. Optimization-based controlof such systems with discrete and continuous decision variables entails theonline solution of mixed-integer quadratic or linear programs, which sufferfrom the curse of dimensionality. Our approach aims at mitigating this issue byeffectively decoupling the decision on the discrete variables and the decisionon the continuous variables. Moreover, to mitigate the combinatorial growth inthe number of possible actions due to the prediction horizon, we conceive thedefinition of decoupled Q-functions to make the learning problem moretractable. The use of reinforcement learning reduces the online optimizationproblem of the MPC controller from a mixed-integer linear (quadratic) programto a linear (quadratic) program, greatly reducing the computational time.Simulation experiments for a microgrid, based on real-world data, demonstratethat the proposed method significantly reduces the online computation time ofthe MPC approach and that it generates policies with small optimality gaps andhigh feasibility rates.</description><author>Caio Fabio Oliveira da Silva, Azita Dabiri, Bart De Schutter</author><pubDate>Tue, 17 Sep 2024 15:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11267v1</guid></item><item><title>Performance of Cross-Validated Targeted Maximum Likelihood Estimation</title><link>http://arxiv.org/abs/2409.11265v1</link><description>Background: Advanced methods for causal inference, such as targeted maximumlikelihood estimation (TMLE), require certain conditions for statisticalinference. However, in situations where there is not differentiability due todata sparsity or near-positivity violations, the Donsker class condition isviolated. In such situations, TMLE variance can suffer from inflation of thetype I error and poor coverage, leading to conservative confidence intervals.Cross-validation of the TMLE algorithm (CVTMLE) has been suggested to improveon performance compared to TMLE in settings of positivity or Donsker classviolations. We aim to investigate the performance of CVTMLE compared to TMLE invarious settings. Methods: We utilised the data-generating mechanism as described in Leger etal. (2022) to run a Monte Carlo experiment under different Donsker classviolations. Then, we evaluated the respective statistical performances of TMLEand CVTMLE with different super learner libraries, with and without regressiontree methods. Results: We found that CVTMLE vastly improves confidence interval coveragewithout adversely affecting bias, particularly in settings with small samplesizes and near-positivity violations. Furthermore, incorporating regressiontrees using standard TMLE with ensemble super learner-based initial estimatesincreases bias and variance leading to invalid statistical inference. Conclusions: It has been shown that when using CVTMLE the Donsker classcondition is no longer necessary to obtain valid statistical inference whenusing regression trees and under either data sparsity or near-positivityviolations. We show through simulations that CVTMLE is much less sensitive tothe choice of the super learner library and thereby provides better estimationand inference in cases where the super learner library uses more flexiblecandidates and is prone to overfitting.</description><author>Matthew J. Smith, Rachael V. Phillips, Camille Maringe, Miguel Angel Luque Fernandez</author><pubDate>Tue, 17 Sep 2024 15:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11265v1</guid></item><item><title>LC-Protonets: Multi-label Few-shot learning for world music audio tagging</title><link>http://arxiv.org/abs/2409.11264v1</link><description>We introduce Label-Combination Prototypical Networks (LC-Protonets) toaddress the problem of multi-label few-shot classification, where a model mustgeneralize to new classes based on only a few available examples. ExtendingPrototypical Networks, LC-Protonets generate one prototype per labelcombination, derived from the power set of labels present in the limitedtraining items, rather than one prototype per label. Our method is applied toautomatic audio tagging across diverse music datasets, covering variouscultures and including both modern and traditional music, and is evaluatedagainst existing approaches in the literature. The results demonstrate asignificant performance improvement in almost all domains and training setupswhen using LC-Protonets for multi-label classification. In addition to traininga few-shot learning model from scratch, we explore the use of a pre-trainedmodel, obtained via supervised learning, to embed items in the feature space.Fine-tuning improves the generalization ability of all methods, yetLC-Protonets achieve high-level performance even without fine-tuning, incontrast to the comparative approaches. We finally analyze the scalability ofthe proposed method, providing detailed quantitative metrics from ourexperiments. The implementation and experimental setup are made publiclyavailable, offering a benchmark for future research.</description><author>Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</author><pubDate>Tue, 17 Sep 2024 15:13:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11264v1</guid></item><item><title>Bio-Inspired Mamba: Temporal Locality and Bioplausible Learning in Selective State Space Models</title><link>http://arxiv.org/abs/2409.11263v1</link><description>This paper introduces Bio-Inspired Mamba (BIM), a novel online learningframework for selective state space models that integrates biological learningprinciples with the Mamba architecture. BIM combines Real-Time RecurrentLearning (RTRL) with Spike-Timing-Dependent Plasticity (STDP)-like locallearning rules, addressing the challenges of temporal locality and biologicalplausibility in training spiking neural networks. Our approach leverages theinherent connection between backpropagation through time and STDP, offering acomputationally efficient alternative that maintains the ability to capturelong-range dependencies. We evaluate BIM on language modeling, speechrecognition, and biomedical signal analysis tasks, demonstrating competitiveperformance against traditional methods while adhering to biological learningprinciples. Results show improved energy efficiency and potential forneuromorphic hardware implementation. BIM not only advances the field ofbiologically plausible machine learning but also provides insights into themechanisms of temporal information processing in biological neural networks.</description><author>Jiahao Qin</author><pubDate>Tue, 17 Sep 2024 15:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11263v1</guid></item><item><title>The Sounds of Home: A Speech-Removed Residential Audio Dataset for Sound Event Detection</title><link>http://arxiv.org/abs/2409.11262v1</link><description>This paper presents a residential audio dataset to support sound eventdetection research for smart home applications aimed at promoting wellbeing forolder adults. The dataset is constructed by deploying audio recording systemsin the homes of 8 participants aged 55-80 years for a 7-day period. Acousticcharacteristics are documented through detailed floor plans and constructionmaterial information to enable replication of the recording environments for AImodel deployment. A novel automated speech removal pipeline is developed, usingpre-trained audio neural networks to detect and remove segments containingspoken voice, while preserving segments containing other sound events. Theresulting dataset consists of privacy-compliant audio recordings thataccurately capture the soundscapes and activities of daily living withinresidential spaces. The paper details the dataset creation methodology, thespeech removal pipeline utilizing cascaded model architectures, and an analysisof the vocal label distribution to validate the speech removal process. Thisdataset enables the development and benchmarking of sound event detectionmodels tailored specifically for in-home applications.</description><author>Gabriel Bibbó, Thomas Deacon, Arshdeep Singh, Mark D. Plumbley</author><pubDate>Tue, 17 Sep 2024 15:10:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11262v1</guid></item><item><title>The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives</title><link>http://arxiv.org/abs/2409.11261v1</link><description>This paper introduces the concept of an education tool that utilizesGenerative Artificial Intelligence (GenAI) to enhance storytelling forchildren. The system combines GenAI-driven narrative co-creation,text-to-speech conversion, and text-to-video generation to produce an engagingexperience for learners. We describe the co-creation process, the adaptation ofnarratives into spoken words using text-to-speech models, and thetransformation of these narratives into contextually relevant visuals throughtext-to-video technology. Our evaluation covers the linguistics of thegenerated stories, the text-to-speech conversion quality, and the accuracy ofthe generated visuals.</description><author>Samee Arif, Taimoor Arif, Aamina Jamal Khan, Muhammad Saad Haroon, Agha Ali Raza, Awais Athar</author><pubDate>Tue, 17 Sep 2024 15:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11261v1</guid></item><item><title>Attacking Slicing Network via Side-channel Reinforcement Learning Attack</title><link>http://arxiv.org/abs/2409.11258v1</link><description>Network slicing in 5G and the future 6G networks will enable the creation ofmultiple virtualized networks on a shared physical infrastructure. Thisinnovative approach enables the provision of tailored networks to accommodatespecific business types or industry users, thus delivering more customized andefficient services. However, the shared memory and cache in network slicingintroduce security vulnerabilities that have yet to be fully addressed. In thispaper, we introduce a reinforcement learning-based side-channel cache attackframework specifically designed for network slicing environments. Unliketraditional cache attack methods, our framework leverages reinforcementlearning to dynamically identify and exploit cache locations storing sensitiveinformation, such as authentication keys and user registration data. We assumethat one slice network is compromised and demonstrate how the attacker caninduce another shared slice to send registration requests, thereby estimatingthe cache locations of critical data. By formulating the cache timing channelattack as a reinforcement learning-driven guessing game between the attackslice and the victim slice, our model efficiently explores possible actions topinpoint memory blocks containing sensitive information. Experimental resultsshowcase the superiority of our approach, achieving a success rate ofapproximately 95\% to 98\% in accurately identifying the storage locations ofsensitive data. This high level of accuracy underscores the potential risks inshared network slicing environments and highlights the need for robust securitymeasures to safeguard against such advanced side-channel attacks.</description><author>Wei Shao, Chandra Thapa, Rayne Holland, Sarah Ali Siddiqui, Seyit Camtepe</author><pubDate>Tue, 17 Sep 2024 15:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11258v1</guid></item><item><title>Temporal As a Plugin: Unsupervised Video Denoising with Pre-Trained Image Denoisers</title><link>http://arxiv.org/abs/2409.11256v1</link><description>Recent advancements in deep learning have shown impressive results in imageand video denoising, leveraging extensive pairs of noisy and noise-free datafor supervision. However, the challenge of acquiring paired videos for dynamicscenes hampers the practical deployment of deep video denoising techniques. Incontrast, this obstacle is less pronounced in image denoising, where paireddata is more readily available. Thus, a well-trained image denoiser could serveas a reliable spatial prior for video denoising. In this paper, we propose anovel unsupervised video denoising framework, named ``Temporal As a Plugin''(TAP), which integrates tunable temporal modules into a pre-trained imagedenoiser. By incorporating temporal modules, our method can harness temporalinformation across noisy frames, complementing its power of spatial denoising.Furthermore, we introduce a progressive fine-tuning strategy that refines eachtemporal module using the generated pseudo clean video frames, progressivelyenhancing the network's denoising performance. Compared to other unsupervisedvideo denoising methods, our framework demonstrates superior performance onboth sRGB and raw video denoising datasets.</description><author>Zixuan Fu, Lanqing Guo, Chong Wang, Yufei Wang, Zhihao Li, Bihan Wen</author><pubDate>Tue, 17 Sep 2024 15:05:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11256v1</guid></item><item><title>A Dynamical System View of Langevin-Based Non-Convex Sampling</title><link>http://arxiv.org/abs/2210.13867v3</link><description>Non-convex sampling is a key challenge in machine learning, central tonon-convex optimization in deep learning as well as to approximateprobabilistic inference. Despite its significance, theoretically there remainmany important challenges: Existing guarantees (1) typically only hold for theaveraged iterates rather than the more desirable last iterates, (2) lackconvergence metrics that capture the scales of the variables such asWasserstein distances, and (3) mainly apply to elementary schemes such asstochastic gradient Langevin dynamics. In this paper, we develop a newframework that lifts the above issues by harnessing several tools from thetheory of dynamical systems. Our key result is that, for a large class ofstate-of-the-art sampling schemes, their last-iterate convergence inWasserstein distances can be reduced to the study of their continuous-timecounterparts, which is much better understood. Coupled with standardassumptions of MCMC sampling, our theory immediately yields the last-iterateWasserstein convergence of many advanced sampling schemes such as proximal,randomized mid-point, and Runge-Kutta integrators. Beyond existing methods, ourframework also motivates more efficient schemes that enjoy the same rigorousguarantees.</description><author>Mohammad Reza Karimi, Ya-Ping Hsieh, Andreas Krause</author><pubDate>Tue, 17 Sep 2024 15:03:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.13867v3</guid></item><item><title>Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach</title><link>http://arxiv.org/abs/2409.11254v1</link><description>As the complexity and connectivity of networks increase, the need for novelmalware detection approaches becomes imperative. Traditional security defensesare becoming less effective against the advanced tactics of today'scyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology instrengthening network security, offering detailed analysis of network trafficthat goes beyond simple metadata analysis. DPI examines not only the packetheaders but also the payload content within, offering a thorough insight intothe data traversing the network. This study proposes a novel approach thatleverages a large language model (LLM) and few-shot learning to accuratelyrecognizes novel, unseen malware types with few labels samples. Our proposedapproach uses a pretrained LLM on known malware types to extract the embeddingsfrom packets. The embeddings are then used alongside few labeled samples of anunseen malware type. This technique is designed to acclimate the model todifferent malware representations, further enabling it to generate robustembeddings for each trained and unseen classes. Following the extraction ofembeddings from the LLM, few-shot learning is utilized to enhance performancewith minimal labeled data. Our evaluation, which utilized two renowneddatasets, focused on identifying malware types within network traffic andInternet of Things (IoT) environments. Our approach shows promising resultswith an average accuracy of 86.35% and F1-Score of 86.40% on different malwaretypes across the two datasets.</description><author>Kyle Stein, Andrew A. Mahyari, Guillermo Francia III, Eman El-Sheikh</author><pubDate>Tue, 17 Sep 2024 15:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11254v1</guid></item><item><title>Norm of Mean Contextualized Embeddings Determines their Variance</title><link>http://arxiv.org/abs/2409.11253v1</link><description>Contextualized embeddings vary by context, even for the same token, and forma distribution in the embedding space. To analyze this distribution, we focuson the norm of the mean embedding and the variance of the embeddings. In thisstudy, we first demonstrate that these values follow the well-known formula forvariance in statistics and provide an efficient sequential computation method.Then, by observing embeddings from intermediate layers of several Transformermodels, we found a strong trade-off relationship between the norm and thevariance: as the mean embedding becomes closer to the origin, the varianceincreases. This trade-off is likely influenced by the layer normalizationmechanism used in Transformer models. Furthermore, when the sets of tokenembeddings are treated as clusters, we show that the variance of the entireembedding set can theoretically be decomposed into the within-cluster varianceand the between-cluster variance. We found experimentally that as the layers ofTransformer models deepen, the embeddings move farther from the origin, thebetween-cluster variance relatively decreases, and the within-cluster variancerelatively increases. These results are consistent with existing studies on theanisotropy of the embedding spaces across layers.</description><author>Hiroaki Yamagiwa, Hidetoshi Shimodaira</author><pubDate>Tue, 17 Sep 2024 15:02:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11253v1</guid></item><item><title>Design Optimization of NOMA Aided Multi-STAR-RIS for Indoor Environments: A Convex Approximation Imitated Reinforcement Learning Approach</title><link>http://arxiv.org/abs/2406.13280v2</link><description>Non-orthogonal multiple access (NOMA) enables multiple users to share thesame frequency band, and simultaneously transmitting and reflectingreconfigurable intelligent surface (STAR-RIS) provides 360-degree full-spacecoverage, optimizing both transmission and reflection for improved networkperformance and dynamic control of the indoor environment. However, deployingSTAR-RIS indoors presents challenges in interference mitigation, powerconsumption, and real-time configuration. In this work, a novel networkarchitecture utilizing multiple access points (APs), STAR-RISs, and NOMA isproposed for indoor communication. To address these, we formulate anoptimization problem involving user assignment, access point (AP) beamforming,and STAR-RIS phase control. A decomposition approach is used to solve thecomplex problem efficiently, employing a many-to-one matching algorithm foruser-AP assignment and K-means clustering for resource management.Additionally, multi-agent deep reinforcement learning (MADRL) is leveraged tooptimize the control of the STAR-RIS. Within the proposed MADRL framework, anovel approach is introduced in which each decision variable acts as anindependent agent, enabling collaborative learning and decision making. TheMADRL framework is enhanced by incorporating convex approximation (CA), whichaccelerates policy learning through suboptimal solutions from successive convexapproximation (SCA), leading to faster adaptation and convergence. Simulationsdemonstrate significant improvements in network utility compared to baselineapproaches.</description><author>Yu Min Park, Sheikh Salman Hassan, Yan Kyaw Tun, Eui-Nam Huh, Walid Saad, Choong Seon Hong</author><pubDate>Tue, 17 Sep 2024 15:02:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13280v2</guid></item><item><title>WER We Stand: Benchmarking Urdu ASR Models</title><link>http://arxiv.org/abs/2409.11252v1</link><description>This paper presents a comprehensive evaluation of Urdu Automatic SpeechRecognition (ASR) models. We analyze the performance of three ASR modelfamilies: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), alongwith a detailed examination of the most frequent wrong words and error typesincluding insertions, deletions, and substitutions. Our analysis is conductedusing two types of datasets, read speech and conversational speech. Notably, wepresent the first conversational speech dataset designed for benchmarking UrduASR models. We find that seamless-large outperforms other ASR models on theread speech dataset, while whisper-large performs best on the conversationalspeech dataset. Furthermore, this evaluation highlights the complexities ofassessing ASR models for low-resource languages like Urdu using quantitativemetrics alone and emphasizes the need for a robust Urdu text normalizationsystem. Our findings contribute valuable insights for developing robust ASRsystems for low-resource languages like Urdu.</description><author>Samee Arif, Aamina Jamal Khan, Mustafa Abbas, Agha Ali Raza, Awais Athar</author><pubDate>Tue, 17 Sep 2024 15:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11252v1</guid></item><item><title>Linear Recency Bias During Training Improves Transformers' Fit to Reading Times</title><link>http://arxiv.org/abs/2409.11250v1</link><description>Recent psycholinguistic research has compared human reading times tosurprisal estimates from language models to study the factors shaping humansentence processing difficulty. Previous studies have shown a strong fitbetween surprisal values from Transformers and reading times. However, standardTransformers work with a lossless representation of the entire previouslinguistic context, unlike models of human language processing that includememory decay. To bridge this gap, this paper evaluates a modification of theTransformer model that uses ALiBi (Press et al., 2022), a recency bias added toattention scores. Surprisal estimates with ALiBi show an improved fit to humanreading times compared to a standard Transformer baseline. A subsequentanalysis of attention heads suggests that ALiBi's mixture of slopes -- whichdetermine the rate of memory decay in each attention head -- may play a role inthe improvement by helping models with ALiBi to track different kinds oflinguistic dependencies.</description><author>Christian Clark, Byung-Doh Oh, William Schuler</author><pubDate>Tue, 17 Sep 2024 14:57:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11250v1</guid></item><item><title>Fault Detection for agents on power grid topology optimization: A Comprehensive analysis</title><link>http://arxiv.org/abs/2406.16426v3</link><description>Optimizing the topology of transmission networks using Deep ReinforcementLearning (DRL) has increasingly come into focus. Various DRL agents have beenproposed, which are mostly benchmarked on the Grid2Op environment from theLearning to Run a Power Network (L2RPN) challenges. The environments have manyadvantages with their realistic grid scenarios and underlying power flowbackends. However, the interpretation of agent survival or failure is notalways clear, as there are a variety of potential causes. In this work, wefocus on the failures of the power grid simulation to identify patterns anddetect them in advance. We collect the failed scenarios of three differentagents on the WCCI 2022 L2RPN environment, totaling about 40k data points. Byclustering, we are able to detect five distinct clusters, identifying commonfailure types. Further, we propose a multi-class prediction approach to detectfailures beforehand and evaluate five different prediction models. Here, theLight Gradient-Boosting Machine (LightGBM) shows the best failure predictionperformance, with an accuracy of 82%. It also accurately classifies whether athe grid survives or fails in 87% of cases. Finally, we provide a detailedfeature importance analysis that identifies critical features and regions inthe grid.</description><author>Malte Lehna, Mohamed Hassouna, Dmitry Degtyar, Sven Tomforde, Christoph Scholz</author><pubDate>Tue, 17 Sep 2024 14:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16426v3</guid></item><item><title>GenQ: Quantization in Low Data Regimes with Generative Synthetic Data</title><link>http://arxiv.org/abs/2312.05272v3</link><description>In the realm of deep neural network deployment, low-bit quantization presentsa promising avenue for enhancing computational efficiency. However, it oftenhinges on the availability of training data to mitigate quantization errors, asignificant challenge when data availability is scarce or restricted due toprivacy or copyright concerns. Addressing this, we introduce GenQ, a novelapproach employing an advanced Generative AI model to generate photorealistic,high-resolution synthetic data, overcoming the limitations of traditionalmethods that struggle to accurately mimic complex objects in extensive datasetslike ImageNet. Our methodology is underscored by two robust filteringmechanisms designed to ensure the synthetic data closely aligns with theintrinsic characteristics of the actual training data. In case of limited dataavailability, the actual data is used to guide the synthetic data generationprocess, enhancing fidelity through the inversion of learnable tokenembeddings. Through rigorous experimentation, GenQ establishes new benchmarksin data-free and data-scarce quantization, significantly outperforming existingmethods in accuracy and efficiency, thereby setting a new standard forquantization in low data regimes. Code is released at\url{https://github.com/Intelligent-Computing-Lab-Yale/GenQ}.</description><author>Yuhang Li, Youngeun Kim, Donghyun Lee, Souvik Kundu, Priyadarshini Panda</author><pubDate>Tue, 17 Sep 2024 14:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05272v3</guid></item><item><title>Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse</title><link>http://arxiv.org/abs/2409.11242v1</link><description>LLMs are an integral part of retrieval-augmented generation (RAG) systems.While many studies focus on evaluating the quality of end-to-end RAG systems,there is a lack of research on understanding the appropriateness of an LLM forthe RAG task. Thus, we introduce a new metric, Trust-Score, that provides aholistic evaluation of the trustworthiness of LLMs in an RAG framework. We showthat various prompting methods, such as in-context learning, fail to adapt LLMseffectively to the RAG task. Thus, we propose Trust-Align, a framework to alignLLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantlyoutperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up29.2) and ELI5 (up 14.9). We release our code at:https://github.com/declare-lab/trust-align.</description><author>Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria</author><pubDate>Tue, 17 Sep 2024 14:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11242v1</guid></item><item><title>Backdoor Attacks in Peer-to-Peer Federated Learning</title><link>http://arxiv.org/abs/2301.09732v4</link><description>Most machine learning applications rely on centralized learning processes,opening up the risk of exposure of their training datasets. While federatedlearning (FL) mitigates to some extent these privacy risks, it relies on atrusted aggregation server for training a shared global model. Recently, newdistributed learning architectures based on Peer-to-Peer Federated Learning(P2PFL) offer advantages in terms of both privacy and reliability. Still, theirresilience to poisoning attacks during training has not been investigated. Inthis paper, we propose new backdoor attacks for P2PFL that leverage structuralgraph properties to select the malicious nodes, and achieve high attacksuccess, while remaining stealthy. We evaluate our attacks under variousrealistic conditions, including multiple graph topologies, limited adversarialvisibility of the network, and clients with non-IID data. Finally, we show thelimitations of existing defenses adapted from FL and design a new defense thatsuccessfully mitigates the backdoor attacks, without an impact on modelaccuracy.</description><author>Georgios Syros, Gokberk Yar, Simona Boboila, Cristina Nita-Rotaru, Alina Oprea</author><pubDate>Tue, 17 Sep 2024 14:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09732v4</guid></item><item><title>Optimal Potential Shaping on SE(3) via Neural ODEs on Lie Groups</title><link>http://arxiv.org/abs/2401.15107v2</link><description>This work presents a novel approach for the optimization of dynamic systemson finite-dimensional Lie groups. We rephrase dynamic systems as so-calledneural ordinary differential equations (neural ODEs), and formulate theoptimization problem on Lie groups. A gradient descent optimization algorithmis presented to tackle the optimization numerically. Our algorithm is scalable,and applicable to any finite dimensional Lie group, including matrix Liegroups. By representing the system at the Lie algebra level, we reduce thecomputational cost of the gradient computation. In an extensive example,optimal potential energy shaping for control of a rigid body is treated. Theoptimal control problem is phrased as an optimization of a neural ODE on theLie group SE(3), and the controller is iteratively optimized. The finalcontroller is validated on a state-regulation task.</description><author>Yannik P. Wotte, Federico Califano, Stefano Stramigioli</author><pubDate>Tue, 17 Sep 2024 14:46:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15107v2</guid></item><item><title>ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models</title><link>http://arxiv.org/abs/2409.09662v2</link><description>Expressing stressful experiences in words is proven to improve mental andphysical health, but individuals often disengage with writing interventions asthey struggle to organize their thoughts and emotions. Reflective prompts havebeen used to provide direction, and large language models (LLMs) havedemonstrated the potential to provide tailored guidance. Current systems oftenlimit users' flexibility to direct their reflections. We thus presentExploreSelf, an LLM-driven application designed to empower users to controltheir reflective journey. ExploreSelf allows users to receive adaptive supportthrough dynamically generated questions. Through an exploratory study with 19participants, we examine how participants explore and reflect on personalchallenges using ExploreSelf. Our findings demonstrate that participants valuedthe balance between guided support and freedom to control their reflectivejourney, leading to deeper engagement and insight. Building on our findings, wediscuss implications for designing LLM-driven tools that promote userempowerment through effective reflective practices.</description><author>Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim</author><pubDate>Tue, 17 Sep 2024 14:44:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09662v2</guid></item><item><title>Spontaneous Informal Speech Dataset for Punctuation Restoration</title><link>http://arxiv.org/abs/2409.11241v1</link><description>Presently, punctuation restoration models are evaluated almost solely onwell-structured, scripted corpora. On the other hand, real-world ASR systemsand post-processing pipelines typically apply towards spontaneous speech withsignificant irregularities, stutters, and deviations from perfect grammar. Toaddress this discrepancy, we introduce SponSpeech, a punctuation restorationdataset derived from informal speech sources, which includes punctuation andcasing information. In addition to publicly releasing the dataset, wecontribute a filtering pipeline that can be used to generate more data. Ourfiltering pipeline examines the quality of both speech audio and transcriptiontext. We also carefully construct a ``challenging" test set, aimed atevaluating models' ability to leverage audio information to predict otherwisegrammatically ambiguous punctuation. SponSpeech is available athttps://github.com/GitHubAccountAnonymous/PR, along with all code for datasetbuilding and model runs.</description><author>Xing Yi Liu, Homayoon Beigi</author><pubDate>Tue, 17 Sep 2024 14:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11241v1</guid></item><item><title>Federated Learning with Integrated Sensing, Communication, and Computation: Frameworks and Performance Analysis</title><link>http://arxiv.org/abs/2409.11240v1</link><description>With the emergence of integrated sensing, communication, and computation(ISCC) in the upcoming 6G era, federated learning with ISCC (FL-ISCC),integrating sample collection, local training, and parameter exchange andaggregation, has garnered increasing interest for enhancing trainingefficiency. Currently, FL-ISCC primarily includes two algorithms: FedAVG-ISCCand FedSGD-ISCC. However, the theoretical understanding of the performance andadvantages of these algorithms remains limited. To address this gap, weinvestigate a general FL-ISCC framework, implementing both FedAVG-ISCC andFedSGD-ISCC. We experimentally demonstrate the substantial potential of theISCC framework in reducing latency and energy consumption in FL. Furthermore,we provide a theoretical analysis and comparison. The results reveal that:1)Both sample collection and communication errors negatively impact algorithmperformance, highlighting the need for careful design to optimize FL-ISCCapplications. 2) FedAVG-ISCC performs better than FedSGD-ISCC under IID datadue to its advantage with multiple local updates. 3) FedSGD-ISCC is more robustthan FedAVG-ISCC under non-IID data, where the multiple local updates inFedAVG-ISCC worsen performance as non-IID data increases. FedSGD-ISCC maintainsperformance levels similar to IID conditions. 4) FedSGD-ISCC is more resilientto communication errors than FedAVG-ISCC, which suffers from significantperformance degradation as communication errors increase.Extensive simulationsconfirm the effectiveness of the FL-ISCC framework and validate our theoreticalanalysis.</description><author>Yipeng Liang, Qimei Chen, Hao Jiang</author><pubDate>Tue, 17 Sep 2024 14:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11240v1</guid></item><item><title>LLM-as-a-Judge &amp; Reward Model: What They Can and Cannot Do</title><link>http://arxiv.org/abs/2409.11239v1</link><description>LLM-as-a-Judge and reward models are widely used alternatives ofmultiple-choice questions or human annotators for large language model (LLM)evaluation. Their efficacy shines in evaluating long-form responses, serving acritical role as evaluators of leaderboards and as proxies to align LLMs viareinforcement learning. However, despite their popularity, their effectivenessoutside of English remains largely unexplored. In this paper, we conduct acomprehensive analysis on automated evaluators, reporting key findings on theirbehavior in a non-English environment. First, we discover that Englishevaluation capabilities significantly influence language-specific capabilities,often more than the language proficiency itself, enabling evaluators trained inEnglish to easily transfer their skills to other languages. Second, we identifycritical shortcomings, where LLMs fail to detect and penalize errors, such asfactual inaccuracies, cultural misrepresentations, and the presence of unwantedlanguage. Finally, we release Kudge, the first non-English meta-evaluationdataset containing 5,012 human annotations in Korean.</description><author>Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong</author><pubDate>Tue, 17 Sep 2024 14:40:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11239v1</guid></item><item><title>HLTCOE JHU Submission to the Voice Privacy Challenge 2024</title><link>http://arxiv.org/abs/2409.08913v2</link><description>We present a number of systems for the Voice Privacy Challenge, includingvoice conversion based systems such as the kNN-VC method and the WavLM voiceConversion method, and text-to-speech (TTS) based systems includingWhisper-VITS. We found that while voice conversion systems better preserveemotional content, they struggle to conceal speaker identity in semi-white-boxattack scenarios; conversely, TTS methods perform better at anonymization andworse at emotion preservation. Finally, we propose a random admixture systemwhich seeks to balance out the strengths and weaknesses of the two category ofsystems, achieving a strong EER of over 40% while maintaining UAR at arespectable 47%.</description><author>Henry Li Xinyuan, Zexin Cai, Ashi Garg, Kevin Duh, Leibny Paola García-Perera, Sanjeev Khudanpur, Nicholas Andrews, Matthew Wiesner</author><pubDate>Tue, 17 Sep 2024 14:39:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08913v2</guid></item><item><title>Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems</title><link>http://arxiv.org/abs/2409.11238v1</link><description>Tracking controllers enable robotic systems to accurately follow plannedreference trajectories. In particular, reinforcement learning (RL) has shownpromise in the synthesis of controllers for systems with complex dynamics andmodest online compute budgets. However, the poor sample efficiency of RL andthe challenges of reward design make training slow and sometimes unstable,especially for high-dimensional systems. In this work, we leverage the inherentLie group symmetries of robotic systems with a floating base to mitigate thesechallenges when learning tracking controllers. We model a general trackingproblem as a Markov decision process (MDP) that captures the evolution of boththe physical and reference states. Next, we prove that symmetry in theunderlying dynamics and running costs leads to an MDP homomorphism, a mappingthat allows a policy trained on a lower-dimensional "quotient" MDP to be liftedto an optimal tracking controller for the original system. We compare thissymmetry-informed approach to an unstructured baseline, using Proximal PolicyOptimization (PPO) to learn tracking controllers for three systems: theParticle (a forced point mass), the Astrobee (a fullyactuated space robot), andthe Quadrotor (an underactuated system). Results show that a symmetry-awareapproach both accelerates training and reduces tracking error after the samenumber of training steps.</description><author>Jake Welde, Nishanth Rao, Pratik Kunapuli, Dinesh Jayaraman, Vijay Kumar</author><pubDate>Tue, 17 Sep 2024 14:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11238v1</guid></item><item><title>Cost-informed dimensionality reduction for structural digital twin technologies</title><link>http://arxiv.org/abs/2409.11236v1</link><description>Classification models are a key component of structural digital twintechnologies used for supporting asset management decision-making. An importantconsideration when developing classification models is the dimensionality ofthe input, or feature space, used. If the dimensionality is too high, then the`curse of dimensionality' may rear its ugly head; manifesting as reducedpredictive performance. To mitigate such effects, practitioners can employdimensionality reduction techniques. The current paper formulates adecision-theoretic approach to dimensionality reduction for structural assetmanagement. In this approach, the aim is to keep incurred misclassificationcosts to a minimum, as the dimensionality is reduced and discriminatoryinformation may be lost. This formulation is constructed as an eigenvalueproblem, with separabilities between classes weighted according to the cost ofmisclassifying them when considered in the context of a decision process. Theapproach is demonstrated using a synthetic case study.</description><author>Aidan J. Hughes, Keith Worden, Nikolaos Dervilis, Timothy J. Rogers</author><pubDate>Tue, 17 Sep 2024 14:37:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11236v1</guid></item><item><title>SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking</title><link>http://arxiv.org/abs/2409.11235v1</link><description>Open-vocabulary Multiple Object Tracking (MOT) aims to generalize trackers tonovel categories not in the training set. Currently, the best-performingmethods are mainly based on pure appearance matching. Due to the complexity ofmotion patterns in the large-vocabulary scenarios and unstable classificationof the novel objects, the motion and semantics cues are either ignored orapplied based on heuristics in the final matching steps by existing methods. Inthis paper, we present a unified framework SLAck that jointly considerssemantics, location, and appearance priors in the early steps of associationand learns how to integrate all valuable information through a lightweightspatial and temporal object graph. Our method eliminates complexpost-processing heuristics for fusing different cues and boosts the associationperformance significantly for large-scale open-vocabulary tracking. Withoutbells and whistles, we outperform previous state-of-the-art methods for novelclasses tracking on the open-vocabulary MOT and TAO TETA benchmarks. Our codeis available at\href{https://github.com/siyuanliii/SLAck}{github.com/siyuanliii/SLAck}.</description><author>Siyuan Li, Lei Ke, Yung-Hsu Yang, Luigi Piccinelli, Mattia Segù, Martin Danelljan, Luc Van Gool</author><pubDate>Tue, 17 Sep 2024 14:36:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11235v1</guid></item><item><title>STCMOT: Spatio-Temporal Cohesion Learning for UAV-Based Multiple Object Tracking</title><link>http://arxiv.org/abs/2409.11234v1</link><description>Multiple object tracking (MOT) in Unmanned Aerial Vehicle (UAV) videos isimportant for diverse applications in computer vision. Current MOT trackersrely on accurate object detection results and precise matching of targetreidentification (ReID). These methods focus on optimizing target spatialattributes while overlooking temporal cues in modelling object relationships,especially for challenging tracking conditions such as object deformation andblurring, etc. To address the above-mentioned issues, we propose a novelSpatio-Temporal Cohesion Multiple Object Tracking framework (STCMOT), whichutilizes historical embedding features to model the representation of ReID anddetection features in a sequential order. Concretely, a temporal embeddingboosting module is introduced to enhance the discriminability of individualembedding based on adjacent frame cooperation. While the trajectory embeddingis then propagated by a temporal detection refinement module to mine salienttarget locations in the temporal field. Extensive experiments on theVisDrone2019 and UAVDT datasets demonstrate our STCMOT sets a newstate-of-the-art performance in MOTA and IDF1 metrics. The source codes arereleased at https://github.com/ydhcg-BoBo/STCMOT.</description><author>Jianbo Ma, Chuanming Tang, Fei Wu, Can Zhao, Jianlin Zhang, Zhiyong Xu</author><pubDate>Tue, 17 Sep 2024 14:34:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11234v1</guid></item><item><title>Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models</title><link>http://arxiv.org/abs/2409.11233v1</link><description>Large language models (LLMs) offer powerful capabilities but incursubstantial computational costs, driving the need for efficient compressiontechniques. This study evaluates the impact of popular compression methods -Magnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing onthe trade-offs between model size reduction, downstream task performance, andthe role of calibration data. Our findings reveal that while SparseGPT andWanda preserve perplexity even at 50% sparsity, they suffer significantdegradation on downstream tasks, highlighting the inadequacy of perplexity asthe sole evaluation metric. To address this, we introduce Jensen-Shannon (JS)Divergence as a more comprehensive metric that captures nuanced changes inmodel behavior post-compression. We further demonstrate that task-specificcalibration data significantly enhances the downstream performance ofcompressed models compared to general calibration data. This researchunderscores the necessity for diverse evaluation metrics and carefulcalibration data selection to fully understand the complexities of LLMcompression and its implications for practical applications.</description><author>Bishwash Khanal, Jeffery M. Capone</author><pubDate>Tue, 17 Sep 2024 14:34:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11233v1</guid></item><item><title>Survey of Data-driven Newsvendor: Unified Analysis and Spectrum of Achievable Regrets</title><link>http://arxiv.org/abs/2409.03505v2</link><description>In the Newsvendor problem, the goal is to guess the number that will be drawnfrom some distribution, with asymmetric consequences for guessing too high vs.too low. In the data-driven version, the distribution is unknown, and one mustwork with samples from the distribution. Data-driven Newsvendor has beenstudied under many variants: additive vs. multiplicative regret, highprobability vs. expectation bounds, and different distribution classes. Thispaper studies all combinations of these variants, filling in many gaps in theliterature and simplifying many proofs. In particular, we provide a unifiedanalysis based on the notion of clustered distributions, which in conjunctionwith our new lower bounds, shows that the entire spectrum of regrets between$1/\sqrt{n}$ and $1/n$ can be possible.</description><author>Zhuoxin Chen, Will Ma</author><pubDate>Tue, 17 Sep 2024 14:31:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03505v2</guid></item><item><title>Watt For What: Rethinking Deep Learning's Energy-Performance Relationship</title><link>http://arxiv.org/abs/2310.06522v2</link><description>Deep learning models have revolutionized various fields, from imagerecognition to natural language processing, by achieving unprecedented levelsof accuracy. However, their increasing energy consumption has raised concernsabout their environmental impact, disadvantaging smaller entities in researchand exacerbating global energy consumption. In this paper, we explore thetrade-off between model accuracy and electricity consumption, proposing ametric that penalizes large consumption of electricity. We conduct acomprehensive study on the electricity consumption of various deep learningmodels across different GPUs, presenting a detailed analysis of theiraccuracy-efficiency trade-offs. By evaluating accuracy per unit of electricityconsumed, we demonstrate how smaller, more energy-efficient models cansignificantly expedite research while mitigating environmental concerns. Ourresults highlight the potential for a more sustainable approach to deeplearning, emphasizing the importance of optimizing models for efficiency. Thisresearch also contributes to a more equitable research landscape, where smallerentities can compete effectively with larger counterparts. This advocates forthe adoption of efficient deep learning practices to reduce electricityconsumption, safeguarding the environment for future generations whilst alsohelping ensure a fairer competitive landscape.</description><author>Shreyank N Gowda, Xinyue Hao, Gen Li, Shashank Narayana Gowda, Xiaobo Jin, Laura Sevilla-Lara</author><pubDate>Tue, 17 Sep 2024 14:30:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06522v2</guid></item><item><title>Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?</title><link>http://arxiv.org/abs/2409.11232v1</link><description>In this manuscript I present an analysis on the performance of OpenAIO1-preview model in solving random K-SAT instances for K$\in {2,3,4}$ as afunction of $\alpha=M/N$ where $M$ is the number of clauses and $N$ is thenumber of variables of the satisfiable problem. I show that the model can callan external SAT solver to solve the instances, rather than solving themdirectly. Despite using external solvers, the model reports incorrectassignments as output. Moreover, I propose and present an analysis to quantifywhether the OpenAI O1-preview model demonstrates a spark of intelligence ormerely makes random guesses when outputting an assignment for a Booleansatisfiability problem.</description><author>Raffaele Marino</author><pubDate>Tue, 17 Sep 2024 14:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11232v1</guid></item><item><title>EmPO: Emotion Grounding for Empathetic Response Generation through Preference Optimization</title><link>http://arxiv.org/abs/2406.19071v2</link><description>Empathetic response generation is a desirable aspect of conversationalagents, crucial for facilitating engaging and emotionally intelligentmulti-turn conversations between humans and machines. Leveraging large languagemodels for this task has shown promising results, yet challenges persist inensuring both the empathetic quality of the responses and retention of thegeneralization performance of the models. We propose a novel approach where weconstruct theory-driven preference datasets based on emotion grounding and usethem to align LLMs with preference optimization algorithms to address thesechallenges. To evaluate empathetic response generation, we employ theEmpatheticDialogues dataset, assessing empathy with the diff-Epitome andBERTscore metrics and with multi-dimensional human evaluation. Additionally, wemeasure diversity and emotional valence using feature-based methods. We alsoevaluate the impact of training on the generalization performance using theMMLU benchmark and tasks from the Open LLM Leaderboard. The results show thatLLMs can be aligned for empathetic response generation by preferenceoptimization while retaining their general performance and that emotiongrounding can guide preference dataset creation. We make all datasets, sourcecode, and models publicly available. https://github.com/justtherightsize/empo</description><author>Ondrej Sotolar, Vojtech Formanek, Alok Debnath, Allison Lahnala, Charles Welch, Lucie FLek</author><pubDate>Tue, 17 Sep 2024 14:24:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19071v2</guid></item><item><title>Learning Source Disentanglement in Neural Audio Codec</title><link>http://arxiv.org/abs/2409.11228v1</link><description>Neural audio codecs have significantly advanced audio compression byefficiently converting continuous audio signals into discrete tokens. Thesecodecs preserve high-quality sound and enable sophisticated sound generationthrough generative models trained on these tokens. However, existing neuralcodec models are typically trained on large, undifferentiated audio datasets,neglecting the essential discrepancies between sound domains like speech,music, and environmental sound effects. This oversight complicates datamodeling and poses additional challenges to the controllability of soundgeneration. To tackle these issues, we introduce the Source-Disentangled NeuralAudio Codec (SD-Codec), a novel approach that combines audio coding and sourceseparation. By jointly learning audio resynthesis and separation, SD-Codecexplicitly assigns audio signals from different domains to distinct codebooks,sets of discrete representations. Experimental results indicate that SD-Codecnot only maintains competitive resynthesis quality but also, supported by theseparation results, demonstrates successful disentanglement of differentsources in the latent space, thereby enhancing interpretability in audio codecand providing potential finer control over the audio generation process.</description><author>Xiaoyu Bie, Xubo Liu, Gaël Richard</author><pubDate>Tue, 17 Sep 2024 14:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11228v1</guid></item></channel></rss>