<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 12 Jul 2023 06:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Semantic-SAM: Segment and Recognize Anything at Any Granularity</title><link>http://arxiv.org/abs/2307.04767v1</link><description>In this paper, we introduce Semantic-SAM, a universal image segmentationmodel to enable segment and recognize anything at any desired granularity. Ourmodel offers two key advantages: semantic-awareness and granularity-abundance.To achieve semantic-awareness, we consolidate multiple datasets across threegranularities and introduce decoupled classification for objects and parts.This allows our model to capture rich semantic information. For themulti-granularity capability, we propose a multi-choice learning scheme duringtraining, enabling each click to generate masks at multiple levels thatcorrespond to multiple ground-truth masks. Notably, this work represents thefirst attempt to jointly train a model on SA-1B, generic, and part segmentationdatasets. Experimental results and visualizations demonstrate that our modelsuccessfully achieves semantic-awareness and granularity-abundance.Furthermore, combining SA-1B training with other segmentation tasks, such aspanoptic and part segmentation, leads to performance improvements. We willprovide code and a demo for further exploration and evaluation.</description><author>Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, Jianfeng Gao</author><pubDate>Mon, 10 Jul 2023 18:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04767v1</guid></item><item><title>Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos</title><link>http://arxiv.org/abs/2307.04760v1</link><description>We propose a self-supervised method for learning representations based onspatial audio-visual correspondences in egocentric videos. In particular, ourmethod leverages a masked auto-encoding framework to synthesize masked binauralaudio through the synergy of audio and vision, thereby learning useful spatialrelationships between the two modalities. We use our pretrained features totackle two downstream video tasks requiring spatial understanding in socialscenarios: active speaker detection and spatial audio denoising. We showthrough extensive experiments that our features are generic enough to improveover multiple state-of-the-art baselines on two public challenging egocentricvideo datasets, EgoCom and EasyCom. Project:http://vision.cs.utexas.edu/projects/ego_av_corr.</description><author>Sagnik Majumder, Ziad Al-Halah, Kristen Grauman</author><pubDate>Mon, 10 Jul 2023 18:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04760v1</guid></item><item><title>Information decomposition to identify relevant variation in complex systems with machine learning</title><link>http://arxiv.org/abs/2307.04755v1</link><description>One of the fundamental steps toward understanding a complex system isidentifying variation at the scale of the system's components that is mostrelevant to behavior on a macroscopic scale. Mutual information is a naturalmeans of linking variation across scales of a system due to its independence ofthe particular functional relationship between variables. However, estimatingmutual information given high-dimensional, continuous-valued data isnotoriously difficult, and the desideratum -- to reveal important variation ina comprehensible manner -- is only readily achieved through exhaustive search.Here we propose a practical, efficient, and broadly applicable methodology todecompose the information contained in a set of measurements by lossilycompressing each measurement with machine learning. Guided by the distributedinformation bottleneck as a learning objective, the information decompositionsorts variation in the measurements of the system state by relevance tospecified macroscale behavior, revealing the most important subsets ofmeasurements for different amounts of predictive information. Additionalgranularity is achieved by inspection of the learned compression schemes: thevariation transmitted during compression is composed of distinctions amongmeasurement values that are most relevant to the macroscale behavior. We focusour analysis on two paradigmatic complex systems: a Boolean circuit and anamorphous material undergoing plastic deformation. In both examples, specificbits of entropy are identified out of the high entropy of the system state asmost related to macroscale behavior for insight about the connection betweenmicro- and macro- in the complex system. The identification of meaningfulvariation in data, with the full generality brought by information theory, ismade practical for the study of complex systems.</description><author>Kieran A. Murphy, Dani S. Bassett</author><pubDate>Mon, 10 Jul 2023 18:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04755v1</guid></item><item><title>Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement</title><link>http://arxiv.org/abs/2307.04751v1</link><description>We propose a system for rearranging objects in a scene to achieve a desiredobject-scene placing relationship, such as a book inserted in an open slot of abookshelf. The pipeline generalizes to novel geometries, poses, and layouts ofboth scenes and objects, and is trained from demonstrations to operate directlyon 3D point clouds. Our system overcomes challenges associated with theexistence of many geometrically-similar rearrangement solutions for a givenscene. By leveraging an iterative pose de-noising training procedure, we canfit multi-modal demonstration data and produce multi-modal outputs whileremaining precise and accurate. We also show the advantages of conditioning onrelevant local geometric features while ignoring irrelevant global structurethat harms both generalization and precision. We demonstrate our approach onthree distinct rearrangement tasks that require handling multi-modality andgeneralization over object shape and pose in both simulation and the realworld. Project website, code, and videos:https://anthonysimeonov.github.io/rpdiff-multi-modal/</description><author>Anthony Simeonov, Ankit Goyal, Lucas Manuelli, Lin Yen-Chen, Alina Sarmiento, Alberto Rodriguez, Pulkit Agrawal, Dieter Fox</author><pubDate>Mon, 10 Jul 2023 18:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04751v1</guid></item><item><title>Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback</title><link>http://arxiv.org/abs/2307.04749v1</link><description>The field of text-conditioned image generation has made unparalleled progresswith the recent advent of latent diffusion models. While remarkable, as thecomplexity of given text input increases, the state-of-the-art diffusion modelsmay still fail in generating images which accurately convey the semantics ofthe given prompt. Furthermore, it has been observed that such misalignments areoften left undetected by pretrained multi-modal models such as CLIP. To addressthese problems, in this paper we explore a simple yet effective decompositionalapproach towards both evaluation and improvement of text-to-image alignment. Inparticular, we first introduce a Decompositional-Alignment-Score which given acomplex prompt decomposes it into a set of disjoint assertions. The alignmentof each assertion with generated images is then measured using a VQA model.Finally, alignment scores for different assertions are combined aposteriori togive the final text-to-image alignment score. Experimental analysis revealsthat the proposed alignment metric shows significantly higher correlation withhuman ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we alsofind that the assertion level alignment scores provide a useful feedback whichcan then be used in a simple iterative procedure to gradually increase theexpression of different assertions in the final image outputs. Human userstudies indicate that the proposed approach surpasses previous state-of-the-artby 8.7% in overall text-to-image alignment accuracy. Project page for our paperis available at https://1jsingh.github.io/divide-evaluate-and-refine</description><author>Jaskirat Singh, Liang Zheng</author><pubDate>Mon, 10 Jul 2023 18:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04749v1</guid></item><item><title>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models</title><link>http://arxiv.org/abs/2307.04738v1</link><description>We propose a novel approach to multi-robot collaboration that harnesses thepower of pre-trained large language models (LLMs) for both high-levelcommunication and low-level path planning. Robots are equipped with LLMs todiscuss and collectively reason task strategies. They then generate sub-taskplans and task space waypoint paths, which are used by a multi-arm motionplanner to accelerate trajectory planning. We also provide feedback from theenvironment, such as collision checking, and prompt the LLM agents to improvetheir plan and waypoints in-context. For evaluation, we introduce RoCoBench, a6-task benchmark covering a wide range of multi-robot collaboration scenarios,accompanied by a text-only dataset for agent representation and reasoning. Weexperimentally demonstrate the effectiveness of our approach -- it achieveshigh success rates across all tasks in RoCoBench and adapts to variations intask semantics. Our dialog setup offers high interpretability and flexibility-- in real world experiments, we show RoCo easily incorporateshuman-in-the-loop, where a user can communicate and collaborate with a robotagent to complete tasks together. See project websitehttps://project-roco.github.io for videos and code.</description><author>Zhao Mandi, Shreeya Jain, Shuran Song</author><pubDate>Mon, 10 Jul 2023 18:52:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04738v1</guid></item><item><title>A unifying framework for differentially private quantum algorithms</title><link>http://arxiv.org/abs/2307.04733v1</link><description>Differential privacy is a widely used notion of security that enables theprocessing of sensitive information. In short, differentially privatealgorithms map "neighbouring" inputs to close output distributions. Prior workproposed several quantum extensions of differential privacy, each of them builton substantially different notions of neighbouring quantum states. In thispaper, we propose a novel and general definition of neighbouring quantumstates. We demonstrate that this definition captures the underlying structureof quantum encodings and can be used to provide exponentially tighter privacyguarantees for quantum measurements. Our approach combines the addition ofclassical and quantum noise and is motivated by the noisy nature of near-termquantum devices. Moreover, we also investigate an alternative setting where weare provided with multiple copies of the input state. In this case,differential privacy can be ensured with little loss in accuracy combiningconcentration of measure and noise-adding mechanisms. En route, we prove theadvanced joint convexity of the quantum hockey-stick divergence and wedemonstrate how this result can be applied to quantum differential privacy.Finally, we complement our theoretical findings with an empirical estimation ofthe certified adversarial robustness ensured by differentially privatemeasurements.</description><author>Armando Angrisani, Mina Doosti, Elham Kashefi</author><pubDate>Mon, 10 Jul 2023 18:44:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04733v1</guid></item><item><title>Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2307.04726v1</link><description>Offline Reinforcement Learning (RL) methods leverage previous experiences tolearn better policies than the behavior policy used for experience collection.In contrast to behavior cloning, which assumes the data is collected fromexpert demonstrations, offline RL can work with non-expert data and multimodalbehavior policies. However, offline RL algorithms face challenges in handlingdistribution shifts and effectively representing policies due to the lack ofonline interaction during training. Prior work on offline RL uses conditionaldiffusion models to obtain expressive policies to represent multimodal behaviorin the dataset. Nevertheless, they are not tailored toward alleviating theout-of-distribution state generalization. We introduce a novel methodincorporating state reconstruction feature learning in the recent class ofdiffusion policies to address the out-of-distribution generalization problem.State reconstruction loss promotes more descriptive representation learning ofstates to alleviate the distribution shift incurred by the out-of-distributionstates. We design a 2D Multimodal Contextual Bandit environment to demonstrateand evaluate our proposed model. We assess the performance of our model notonly in this new environment but also on several D4RL benchmark tasks,achieving state-of-the-art results.</description><author>Suzan Ece Ada, Erhan Oztop, Emre Ugur</author><pubDate>Mon, 10 Jul 2023 18:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04726v1</guid></item><item><title>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</title><link>http://arxiv.org/abs/2307.04725v1</link><description>With the advance of text-to-image models (e.g., Stable Diffusion) andcorresponding personalization techniques such as DreamBooth and LoRA, everyonecan manifest their imagination into high-quality images at an affordable cost.Subsequently, there is a great demand for image animation techniques to furthercombine generated static images with motion dynamics. In this report, wepropose a practical framework to animate most of the existing personalizedtext-to-image models once and for all, saving efforts in model-specific tuning.At the core of the proposed framework is to insert a newly initialized motionmodeling module into the frozen text-to-image model and train it on video clipsto distill reasonable motion priors. Once trained, by simply injecting thismotion modeling module, all personalized versions derived from the same baseT2I readily become text-driven models that produce diverse and personalizedanimated images. We conduct our evaluation on several public representativepersonalized text-to-image models across anime pictures and realisticphotographs, and demonstrate that our proposed framework helps these modelsgenerate temporally smooth animation clips while preserving the domain anddiversity of their outputs. Code and pre-trained weights will be publiclyavailable at https://animatediff.github.io/ .</description><author>Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai</author><pubDate>Mon, 10 Jul 2023 18:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04725v1</guid></item><item><title>Advances and Challenges in Meta-Learning: A Technical Review</title><link>http://arxiv.org/abs/2307.04722v1</link><description>Meta-learning empowers learning systems with the ability to acquire knowledgefrom multiple tasks, enabling faster adaptation and generalization to newtasks. This review provides a comprehensive technical overview ofmeta-learning, emphasizing its importance in real-world applications where datamay be scarce or expensive to obtain. The paper covers the state-of-the-artmeta-learning approaches and explores the relationship between meta-learningand multi-task learning, transfer learning, domain adaptation andgeneralization, self-supervised learning, personalized federated learning, andcontinual learning. By highlighting the synergies between these topics and thefield of meta-learning, the paper demonstrates how advancements in one area canbenefit the field as a whole, while avoiding unnecessary duplication ofefforts. Additionally, the paper delves into advanced meta-learning topics suchas learning from complex multi-modal task distributions, unsupervisedmeta-learning, learning to efficiently adapt to data distribution shifts, andcontinual meta-learning. Lastly, the paper highlights open problems andchallenges for future research in the field. By synthesizing the latestresearch developments, this paper provides a thorough understanding ofmeta-learning and its potential impact on various machine learningapplications. We believe that this technical overview will contribute to theadvancement of meta-learning and its practical implications in addressingreal-world problems.</description><author>Anna Vettoruzzo, Mohamed-Rafik Bouguelia, Joaquin Vanschoren, Thorsteinn Rögnvaldsson, KC Santosh</author><pubDate>Mon, 10 Jul 2023 18:32:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04722v1</guid></item><item><title>Large Language Models as General Pattern Machines</title><link>http://arxiv.org/abs/2307.04721v1</link><description>We observe that pre-trained large language models (LLMs) are capable ofautoregressively completing complex token sequences -- from arbitrary onesprocedurally generated by probabilistic context-free grammars (PCFG), to morerich spatial patterns found in the Abstract Reasoning Corpus (ARC), a generalAI benchmark, prompted in the style of ASCII art. Surprisingly, patterncompletion proficiency can be partially retained even when the sequences areexpressed using tokens randomly sampled from the vocabulary. These resultssuggest that without any additional training, LLMs can serve as generalsequence modelers, driven by in-context learning. In this work, we investigatehow these zero-shot capabilities may be applied to problems in robotics -- fromextrapolating sequences of numbers that represent states over time to completesimple motions, to least-to-most prompting of reward-conditioned trajectoriesthat can discover and represent closed-loop policies (e.g., a stabilizingcontroller for CartPole). While difficult to deploy today for real systems dueto latency, context size limitations, and compute costs, the approach of usingLLMs to drive low-level control may provide an exciting glimpse into how thepatterns among words could be transferred to actions.</description><author>Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng</author><pubDate>Mon, 10 Jul 2023 18:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04721v1</guid></item><item><title>On the curvature of the loss landscape</title><link>http://arxiv.org/abs/2307.04719v1</link><description>One of the main challenges in modern deep learning is to understand why suchover-parameterized models perform so well when trained on finite data. A way toanalyze this generalization concept is through the properties of the associatedloss landscape. In this work, we consider the loss landscape as an embeddedRiemannian manifold and show that the differential geometric properties of themanifold can be used when analyzing the generalization abilities of a deep net.In particular, we focus on the scalar curvature, which can be computedanalytically for our manifold, and show connections to several settings thatpotentially imply generalization.</description><author>Alison Pouplin, Hrittik Roy, Sidak Pal Singh, Georgios Arvanitidis</author><pubDate>Mon, 10 Jul 2023 18:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04719v1</guid></item><item><title>CVPR MultiEarth 2023 Deforestation Estimation Challenge:SpaceVision4Amazon</title><link>http://arxiv.org/abs/2307.04715v1</link><description>In this paper, we present a deforestation estimation method based onattention guided UNet architecture using Electro-Optical (EO) and SyntheticAperture Radar (SAR) satellite imagery. For optical images, Landsat-8 and forSAR imagery, Sentinel-1 data have been used to train and validate the proposedmodel. Due to the unavailability of temporally and spatially collocated data,individual model has been trained for each sensor. During training timeLandsat-8 model achieved training and validation pixel accuracy of 93.45% andSentinel-2 model achieved 83.87% pixel accuracy. During the test setevaluation, the model achieved pixel accuracy of 84.70% with F1-Score of 0.79and IoU of 0.69.</description><author>Sunita Arya, S Manthira Moorthi, Debajyoti Dhar</author><pubDate>Mon, 10 Jul 2023 18:25:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04715v1</guid></item><item><title>Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions</title><link>http://arxiv.org/abs/2211.12316v2</link><description>Despite the widespread success of Transformers on NLP tasks, recent workshave found that they struggle to model several formal languages when comparedto recurrent models. This raises the question of why Transformers perform wellin practice and whether they have any properties that enable them to generalizebetter than recurrent models. In this work, we conduct an extensive empiricalstudy on Boolean functions to demonstrate the following: (i) RandomTransformers are relatively more biased towards functions of low sensitivity.(ii) When trained on Boolean functions, both Transformers and LSTMs prioritizelearning functions of low sensitivity, with Transformers ultimately convergingto functions of lower sensitivity. (iii) On sparse Boolean functions which havelow sensitivity, we find that Transformers generalize near perfectly even inthe presence of noisy labels whereas LSTMs overfit and achieve poorgeneralization accuracy. Overall, our results provide strong quantifiableevidence that suggests differences in the inductive biases of Transformers andrecurrent models which may help explain Transformer's effective generalizationperformance despite relatively limited expressiveness.</description><author>Satwik Bhattamishra, Arkil Patel, Varun Kanade, Phil Blunsom</author><pubDate>Mon, 10 Jul 2023 18:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.12316v2</guid></item><item><title>A PDE-Based Analysis of the Symmetric Two-Armed Bernoulli Bandit</title><link>http://arxiv.org/abs/2202.05767v4</link><description>This work addresses a version of the two-armed Bernoulli bandit problem wherethe sum of the means of the arms is one (the symmetric two-armed Bernoullibandit). In a regime where the gap between these means goes to zero and thenumber of prediction periods approaches infinity, we obtain the leading orderterms of the minmax optimal regret and pseudoregret for this problem byassociating each of them with a solution of a linear heat equation. Our resultsimprove upon the previously known results; specifically, we explicitly computethese leading order terms in three different scaling regimes for the gap.Additionally, we obtain new non-asymptotic bounds for any given time horizon.</description><author>Vladimir A. Kobzar, Robert V. Kohn</author><pubDate>Mon, 10 Jul 2023 18:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.05767v4</guid></item><item><title>Understanding Real-World AI Planning Domains: A Conceptual Framework</title><link>http://arxiv.org/abs/2307.04701v1</link><description>Planning is a pivotal ability of any intelligent system being developed forreal-world applications. AI planning is concerned with researching anddeveloping planning systems that automatically compute plans that satisfy someuser objective. Identifying and understanding the relevant and realisticaspects that characterise real-world application domains are crucial to thedevelopment of AI planning systems. This provides guidance to knowledgeengineers and software engineers in the process of designing, identifying, andcategorising resources required for the development process. To the best of ourknowledge, such support does not exist. We address this research gap bydeveloping a conceptual framework that identifies and categorises the aspectsof real-world planning domains in varying levels of granularity. Our frameworkprovides not only a common terminology but also a comprehensive overview of abroad range of planning aspects exemplified using the domain of sustainablebuildings as a prominent application domain of AI planning. The framework hasthe potential to impact the design, development, and applicability of AIplanning systems in real-world application domains.</description><author>Ebaa Alnazer, Ilche Georgievski</author><pubDate>Mon, 10 Jul 2023 17:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04701v1</guid></item><item><title>Multimedia Generative Script Learning for Task Planning</title><link>http://arxiv.org/abs/2208.12306v3</link><description>Goal-oriented generative script learning aims to generate subsequent steps toreach a particular goal, which is an essential task to assist robots or humansin performing stereotypical activities. An important aspect of this process isthe ability to capture historical states visually, which provides detailedinformation that is not covered by text and will guide subsequent steps.Therefore, we propose a new task, Multimedia Generative Script Learning, togenerate subsequent steps by tracking historical states in both text and visionmodalities, as well as presenting the first benchmark containing 5,652 tasksand 79,089 multimedia steps. This task is challenging in three aspects: themultimedia challenge of capturing the visual states in images, the inductionchallenge of performing unseen tasks, and the diversity challenge of coveringdifferent information in individual steps. We propose to encode visual statechanges through a selective multimedia encoder to address the multimediachallenge, transfer knowledge from previously observed tasks using aretrieval-augmented decoder to overcome the induction challenge, and furtherpresent distinct information at each step by optimizing a diversity-orientedcontrastive learning objective. We define metrics to evaluate both generationand inductive quality. Experiment results demonstrate that our approachsignificantly outperforms strong baselines.</description><author>Qingyun Wang, Manling Li, Hou Pong Chan, Lifu Huang, Julia Hockenmaier, Girish Chowdhary, Heng Ji</author><pubDate>Mon, 10 Jul 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12306v3</guid></item><item><title>Cobalt: Optimizing Mining Rewards in Proof-of-Work Network Games</title><link>http://arxiv.org/abs/2307.04695v1</link><description>Mining in proof-of-work blockchains has become an expensive affair requiringspecialized hardware capable of executing several megahashes per second at hugeelectricity costs. Miners earn a reward each time they mine a block within thelongest chain, which helps offset their mining costs. It is therefore ofinterest to miners to maximize the number of mined blocks in the blockchain andincrease revenue. A key factor affecting mining rewards earned is theconnectivity between miners in the peer-to-peer network. To maximize rewards aminer must choose its network connections carefully, ensuring existence ofpaths to other miners that are on average of a lower latency compared to pathsbetween other miners. We formulate the problem of deciding whom to connect tofor miners as a combinatorial bandit problem. Each node picks its neighborsstrategically to minimize the latency to reach 90\% of the hash power of thenetwork relative to the 90-th percentile latency from other nodes. A keycontribution of our work is the use of a network coordinates based model forlearning the network structure within the bandit algorithm. Experimentally weshow our proposed algorithm outperforming or matching baselines on diversenetwork settings.</description><author>Arti Vedula, Abhishek Gupta, Shaileshh Bojja Venkatakrishnan</author><pubDate>Mon, 10 Jul 2023 17:50:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04695v1</guid></item><item><title>COMEX: A Tool for Generating Customized Source Code Representations</title><link>http://arxiv.org/abs/2307.04693v1</link><description>Learning effective representations of source code is critical for any MachineLearning for Software Engineering (ML4SE) system. Inspired by natural languageprocessing, large language models (LLMs) like Codex and CodeGen treat code asgeneric sequences of text and are trained on huge corpora of code data,achieving state of the art performance on several software engineering (SE)tasks. However, valid source code, unlike natural language, follows a strictstructure and pattern governed by the underlying grammar of the programminglanguage. Current LLMs do not exploit this property of the source code as theytreat code like a sequence of tokens and overlook key structural and semanticproperties of code that can be extracted from code-views like the Control FlowGraph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc.Unfortunately, the process of generating and integrating code-views for everyprogramming language is cumbersome and time consuming. To overcome thisbarrier, we propose our tool COMEX - a framework that allows researchers anddevelopers to create and combine multiple code-views which can be used bymachine learning (ML) models for various SE tasks. Some salient features of ourtool are: (i) it works directly on source code (which need not be compilable),(ii) it currently supports Java and C#, (iii) it can analyze both method-levelsnippets and program-level snippets by using both intra-procedural andinter-procedural analysis, and (iv) it is easily extendable to other languagesas it is built on tree-sitter - a widely used incremental parser that supportsover 40 languages. We believe this easy-to-use code-view generation andcustomization tool will give impetus to research in source code representationlearning methods and ML4SE. Tool: https://pypi.org/project/comex - GitHub:https://github.com/IBM/tree-sitter-codeviews - Demo:https://youtu.be/GER6U87FVbU</description><author>Debeshee Das, Noble Saji Mathews, Alex Mathai, Srikanth Tamilselvam, Kranthi Sedamaki, Sridhar Chimalakonda, Atul Kumar</author><pubDate>Mon, 10 Jul 2023 17:46:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04693v1</guid></item><item><title>VampNet: Music Generation via Masked Acoustic Token Modeling</title><link>http://arxiv.org/abs/2307.04686v1</link><description>We introduce VampNet, a masked acoustic token modeling approach to musicsynthesis, compression, inpainting, and variation. We use a variable maskingschedule during training which allows us to sample coherent music from themodel by applying a variety of masking approaches (called prompts) duringinference. VampNet is non-autoregressive, leveraging a bidirectionaltransformer architecture that attends to all tokens in a forward pass. Withjust 36 sampling passes, VampNet can generate coherent high-fidelity musicalwaveforms. We show that by prompting VampNet in various ways, we can apply itto tasks like music compression, inpainting, outpainting, continuation, andlooping with variation (vamping). Appropriately prompted, VampNet is capable ofmaintaining style, genre, instrumentation, and other high-level aspects of themusic. This flexible prompting capability makes VampNet a powerful musicco-creation tool. Code and audio samples are available online.</description><author>Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, Bryan Pardo</author><pubDate>Mon, 10 Jul 2023 17:42:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04686v1</guid></item><item><title>FreeDrag: Point Tracking is Not You Need for Interactive Point-based Image Editing</title><link>http://arxiv.org/abs/2307.04684v1</link><description>To serve the intricate and varied demands of image editing, precise andflexible manipulation of image content is indispensable. Recently, DragGAN hasachieved impressive editing results through point-based manipulation. However,we have observed that DragGAN struggles with miss tracking, where DragGANencounters difficulty in effectively tracking the desired handle points, andambiguous tracking, where the tracked points are situated within other regionsthat bear resemblance to the handle points. To deal with the above issues, wepropose FreeDrag, which adopts a feature-oriented approach to free the burdenon point tracking within the point-oriented methodology of DragGAN. TheFreeDrag incorporates adaptive template features, line search, and fuzzylocalization techniques to perform stable and efficient point-based imageediting. Extensive experiments demonstrate that our method is superior to theDragGAN and enables stable point-based editing in challenging scenarios withsimilar structures, fine details, or under multi-point targets.</description><author>Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, Yi Jin</author><pubDate>Mon, 10 Jul 2023 17:37:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04684v1</guid></item><item><title>Point Cloud Diffusion Models for Automatic Implant Generation</title><link>http://arxiv.org/abs/2303.08061v2</link><description>Advances in 3D printing of biocompatible materials make patient-specificimplants increasingly popular. The design of these implants is, however, stilla tedious and largely manual process. Existing approaches to automate implantgeneration are mainly based on 3D U-Net architectures on downsampled orpatch-wise data, which can result in a loss of detail or contextualinformation. Following the recent success of Diffusion Probabilistic Models, wepropose a novel approach for implant generation based on a combination of 3Dpoint cloud diffusion models and voxelization networks. Due to the stochasticsampling process in our diffusion model, we can propose an ensemble ofdifferent implants per defect, from which the physicians can choose the mostsuitable one. We evaluate our method on the SkullBreak and SkullFix datasets,generating high-quality implants and achieving competitive evaluation scores.</description><author>Paul Friedrich, Julia Wolleb, Florentin Bieder, Florian M. Thieringer, Philippe C. Cattin</author><pubDate>Mon, 10 Jul 2023 17:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08061v2</guid></item><item><title>Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles</title><link>http://arxiv.org/abs/2307.04679v1</link><description>In this paper, we provide a novel framework for the analysis ofgeneralization error of first-order optimization algorithms for statisticallearning when the gradient can only be accessed through partial observationsgiven by an oracle. Our analysis relies on the regularity of the gradientw.r.t. the data samples, and allows to derive near matching upper and lowerbounds for the generalization error of multiple learning problems, includingsupervised learning, transfer learning, robust learning, distributed learningand communication efficient learning using gradient quantization. These resultshold for smooth and strongly-convex optimization problems, as well as smoothnon-convex optimization problems verifying a Polyak-Lojasiewicz assumption. Inparticular, our upper and lower bounds depend on a novel quantity that extendsthe notion of conditional standard deviation, and is a measure of the extent towhich the gradient can be approximated by having access to the oracle. As aconsequence, our analysis provides a precise meaning to the intuition thatoptimization of the statistical learning objective is as hard as the estimationof its gradient. Finally, we show that, in the case of standard supervisedlearning, mini-batch gradient descent with increasing batch sizes and a warmstart can reach a generalization error that is optimal up to a multiplicativefactor, thus motivating the use of this optimization scheme in practicalapplications.</description><author>Kevin Scaman, Mathieu Even, Laurent Massoulié</author><pubDate>Mon, 10 Jul 2023 17:29:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04679v1</guid></item><item><title>Biomedical Language Models are Robust to Sub-optimal Tokenization</title><link>http://arxiv.org/abs/2306.17649v3</link><description>As opposed to general English, many concepts in biomedical terminology havebeen designed in recent history by biomedical professionals with the goal ofbeing precise and concise. This is often achieved by concatenating meaningfulbiomedical morphemes to create new semantic units. Nevertheless, most modernbiomedical language models (LMs) are pre-trained using standard domain-specifictokenizers derived from large scale biomedical corpus statistics withoutexplicitly leveraging the agglutinating nature of biomedical language. In thiswork, we first find that standard open-domain and biomedical tokenizers arelargely unable to segment biomedical terms into meaningful components.Therefore, we hypothesize that using a tokenizer which segments biomedicalterminology more accurately would enable biomedical LMs to improve theirperformance on downstream biomedical NLP tasks, especially ones which involvebiomedical terms directly such as named entity recognition (NER) and entitylinking. Surprisingly, we find that pre-training a biomedical LM using a moreaccurate biomedical tokenizer does not improve the entity representationquality of a language model as measured by several intrinsic and extrinsicmeasures such as masked language modeling prediction (MLM) accuracy as well asNER and entity linking performance. These quantitative findings, along with acase study which explores entity representation quality more directly, suggestthat the biomedical pre-training process is quite robust to instances ofsub-optimal tokenization.</description><author>Bernal Jiménez Gutiérrez, Huan Sun, Yu Su</author><pubDate>Mon, 10 Jul 2023 17:22:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17649v3</guid></item><item><title>LINFA: a Python library for variational inference with normalizing flow and annealing</title><link>http://arxiv.org/abs/2307.04675v1</link><description>Variational inference is an increasingly popular method in statistics andmachine learning for approximating probability distributions. We developedLINFA (Library for Inference with Normalizing Flow and Annealing), a Pythonlibrary for variational inference to accommodate computationally expensivemodels and difficult-to-sample distributions with dependent parameters. Wediscuss the theoretical background, capabilities, and performance of LINFA invarious benchmarks. LINFA is publicly available on GitHub athttps://github.com/desResLab/LINFA.</description><author>Yu Wang, Emma R. Cobian, Jubilee Lee, Fang Liu, Jonathan D. Hauenstein, Daniele E. Schiavazzi</author><pubDate>Mon, 10 Jul 2023 17:21:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04675v1</guid></item><item><title>Testing of Detection Tools for AI-Generated Text</title><link>http://arxiv.org/abs/2306.15666v2</link><description>Recent advances in generative pre-trained transformer large language modelshave emphasised the potential risks of unfair use of artificial intelligence(AI) generated content in an academic environment and intensified efforts insearching for solutions to detect such content. The paper examines the generalfunctionality of detection tools for artificial intelligence generated text andevaluates them based on accuracy and error type analysis. Specifically, thestudy seeks to answer research questions about whether existing detection toolscan reliably differentiate between human-written text and ChatGPT-generatedtext, and whether machine translation and content obfuscation techniques affectthe detection of AI-generated text. The research covers 12 publicly availabletools and two commercial systems (Turnitin and PlagiarismCheck) that are widelyused in the academic setting. The researchers conclude that the availabledetection tools are neither accurate nor reliable and have a main bias towardsclassifying the output as human-written rather than detecting AI-generatedtext. Furthermore, content obfuscation techniques significantly worsen theperformance of tools. The study makes several significant contributions. First,it summarises up-to-date similar scientific and non-scientific efforts in thefield. Second, it presents the result of one of the most comprehensive testsconducted so far, based on a rigorous research methodology, an originaldocument set, and a broad coverage of tools. Third, it discusses theimplications and drawbacks of using detection tools for AI-generated text inacademic settings.</description><author>Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba, Tomáš Foltýnek, Jean Guerrero-Dib, Olumide Popoola, Petr Šigut, Lorna Waddington</author><pubDate>Mon, 10 Jul 2023 17:14:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15666v2</guid></item><item><title>Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach</title><link>http://arxiv.org/abs/2307.04668v1</link><description>The rise of social media platforms has facilitated the formation of echochambers, which are online spaces where users predominantly encounterviewpoints that reinforce their existing beliefs while excluding dissentingperspectives. This phenomenon significantly hinders information disseminationacross communities and fuels societal polarization. Therefore, it is crucial todevelop methods for quantifying echo chambers. In this paper, we present theEcho Chamber Score (ECS), a novel metric that assesses the cohesion andseparation of user communities by measuring distances between users in theembedding space. In contrast to existing approaches, ECS is able to functionwithout labels for user ideologies and makes no assumptions about the structureof the interaction graph. To facilitate measuring distances between users, wepropose EchoGAE, a self-supervised graph autoencoder-based user embedding modelthat leverages users' posts and the interaction graph to embed them in a mannerthat reflects their ideological similarity. To assess the effectiveness of ECS,we use a Twitter dataset consisting of four topics - two polarizing and twonon-polarizing. Our results showcase ECS's effectiveness as a tool forquantifying echo chambers and shedding light on the dynamics of onlinediscourse.</description><author>Faisal Alatawi, Paras Sheth, Huan Liu</author><pubDate>Mon, 10 Jul 2023 17:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04668v1</guid></item><item><title>The Benefits of Model-Based Generalization in Reinforcement Learning</title><link>http://arxiv.org/abs/2211.02222v3</link><description>Model-Based Reinforcement Learning (RL) is widely believed to have thepotential to improve sample efficiency by allowing an agent to synthesize largeamounts of imagined experience. Experience Replay (ER) can be considered asimple kind of model, which has proved effective at improving the stability andefficiency of deep RL. In principle, a learned parametric model could improveon ER by generalizing from real experience to augment the dataset withadditional plausible experience. However, given that learned value functionscan also generalize, it is not immediately obvious why model generalizationshould be better. Here, we provide theoretical and empirical insight into when,and how, we can expect data generated by a learned model to be useful. First,we provide a simple theorem motivating how learning a model as an intermediatestep can narrow down the set of possible value functions more than learning avalue function directly from data using the Bellman equation. Second, weprovide an illustrative example showing empirically how a similar effect occursin a more concrete setting with neural network function approximation. Finally,we provide extensive experiments showing the benefit of model-based learningfor online RL in environments with combinatorial complexity, but factoredstructure that allows a learned model to generalize. In these experiments, wetake care to control for other factors in order to isolate, insofar aspossible, the benefit of using experience generated by a learned model relativeto ER alone.</description><author>Kenny Young, Aditya Ramesh, Louis Kirsch, Jürgen Schmidhuber</author><pubDate>Mon, 10 Jul 2023 17:07:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.02222v3</guid></item><item><title>Liquidity takers behavior representation through a contrastive learning approach</title><link>http://arxiv.org/abs/2306.05987v2</link><description>Thanks to the access to the labeled orders on the CAC40 data from Euronext,we are able to analyze agents' behaviors in the market based on their placedorders. In this study, we construct a self-supervised learning model usingtriplet loss to effectively learn the representation of agent market orders. Byacquiring this learned representation, various downstream tasks becomefeasible. In this work, we utilize the K-means clustering algorithm on thelearned representation vectors of agent orders to identify distinct behaviortypes within each cluster.</description><author>Ruihua Ruan, Emmanuel Bacry, Jean-François Muzy</author><pubDate>Mon, 10 Jul 2023 17:03:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05987v2</guid></item><item><title>BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training</title><link>http://arxiv.org/abs/2307.03131v2</link><description>Automatic metrics play a crucial role in machine translation. Despite thewidespread use of n-gram-based metrics, there has been a recent surge in thedevelopment of pre-trained model-based metrics that focus on measuring sentencesemantics. However, these neural metrics, while achieving higher correlationswith human evaluations, are often considered to be black boxes with potentialbiases that are difficult to detect. In this study, we systematically analyzeand compare various mainstream and cutting-edge automatic metrics from theperspective of their guidance for training machine translation systems. ThroughMinimum Risk Training (MRT), we find that certain metrics exhibit robustnessdefects, such as the presence of universal adversarial translations in BLEURTand BARTScore. In-depth analysis suggests two main causes of these robustnessdeficits: distribution biases in the training datasets, and the tendency of themetric paradigm. By incorporating token-level constraints, we enhance therobustness of evaluation metrics, which in turn leads to an improvement in theperformance of machine translation systems. Codes are available at\url{https://github.com/powerpuffpomelo/fairseq_mrt}.</description><author>Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang, Jiajun Chen, Mingxuan Wang</author><pubDate>Mon, 10 Jul 2023 16:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03131v2</guid></item><item><title>On the power of graph neural networks and the role of the activation function</title><link>http://arxiv.org/abs/2307.04661v1</link><description>In this article we present new results about the expressivity of Graph NeuralNetworks (GNNs). We prove that for any GNN with piecewise polynomialactivations, whose architecture size does not grow with the graph input sizes,there exists a pair of non-isomorphic rooted trees of depth two such that theGNN cannot distinguish their root vertex up to an arbitrary number ofiterations. The proof relies on tools from the algebra of symmetricpolynomials. In contrast, it was already known that unbounded GNNs (those whosesize is allowed to change with the graph sizes) with piecewise polynomialactivations can distinguish these vertices in only two iterations. Our resultsimply a strict separation between bounded and unbounded size GNNs, answering anopen question formulated by [Grohe, 2021]. We next prove that if one allowsactivations that are not piecewise polynomial, then in two iterations a singleneuron perceptron can distinguish the root vertices of any pair ofnonisomorphic trees of depth two (our results hold for activations like thesigmoid, hyperbolic tan and others). This shows how the power of graph neuralnetworks can change drastically if one changes the activation function of theneural networks. The proof of this result utilizes the Lindemann-Weierstrausstheorem from transcendental number theory.</description><author>Sammy Khalife, Amitabh Basu</author><pubDate>Mon, 10 Jul 2023 16:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04661v1</guid></item><item><title>BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset</title><link>http://arxiv.org/abs/2307.04657v1</link><description>In this paper, we introduce the BeaverTails dataset, aimed at fosteringresearch on safety alignment in large language models (LLMs). This datasetuniquely separates annotations of helpfulness and harmlessness forquestion-answering pairs, thus offering distinct perspectives on these crucialattributes. In total, we have compiled safety meta-labels for 30,207question-answer (QA) pairs and gathered 30,144 pairs of expert comparison datafor both the helpfulness and harmlessness metrics. We further showcaseapplications of BeaverTails in content moderation and reinforcement learningwith human feedback (RLHF), emphasizing its potential for practical safetymeasures in LLMs. We believe this dataset provides vital resources for thecommunity, contributing towards the safe development and deployment of LLMs.Our project page is available at the following URL:https://sites.google.com/view/pku-beavertails.</description><author>Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang</author><pubDate>Mon, 10 Jul 2023 16:56:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04657v1</guid></item><item><title>Automated Detection of Double Nuclei Galaxies using GOTHIC and the Discovery of a Large Sample of Dual AGN</title><link>http://arxiv.org/abs/2011.12177v3</link><description>We present a novel algorithm to detect double nuclei galaxies (DNG) calledGOTHIC (Graph BOosted iterated HIll Climbing) - that detects whether a givenimage of a galaxy has two or more closely separated nuclei. Our aim is todetect samples of dual or multiple active galactic nuclei (AGN) in galaxies.Although galaxy mergers are common, the detection of dual AGN is rare. Theirdetection is very important as they help us understand the formation ofsupermassive black hole (SMBH) binaries, SMBH growth and AGN feedback effectsin multiple nuclei systems. There is thus a need for an algorithm to do asystematic survey of existing imaging data for the discovery of DNGs and dualAGN. We have tested GOTHIC on a known sample of DNGs and subsequently appliedit to a sample of a million SDSS DR16 galaxies lying in the redshift range of 0to 0.75 approximately, and have available spectroscopic data. We have detected159 dual AGN in this sample, of which 2 are triple AGN systems. Our resultsshow that dual AGN are not common, and triple AGN even rarer. The color (u-r)magnitude plots of the DNGs indicate that star formation is quenched as thenuclei come closer and as the AGN fraction increases. The quenching isespecially prominent for dual/triple AGN galaxies that lie in the extreme endof the red sequence.</description><author>Anwesh Bhattacharya, Nehal C. P., Mousumi Das, Abhishek Paswan, Snehanshu Saha, Francoise Combes</author><pubDate>Mon, 10 Jul 2023 16:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2011.12177v3</guid></item><item><title>Joint Salient Object Detection and Camouflaged Object Detection via Uncertainty-aware Learning</title><link>http://arxiv.org/abs/2307.04651v1</link><description>Salient objects attract human attention and usually stand out clearly fromtheir surroundings. In contrast, camouflaged objects share similar colors ortextures with the environment. In this case, salient objects are typicallynon-camouflaged, and camouflaged objects are usually not salient. Due to thisinherent contradictory attribute, we introduce an uncertainty-aware learningpipeline to extensively explore the contradictory information of salient objectdetection (SOD) and camouflaged object detection (COD) via data-level andtask-wise contradiction modeling. We first exploit the dataset correlation ofthese two tasks and claim that the easy samples in the COD dataset can serve ashard samples for SOD to improve the robustness of the SOD model. Based on theassumption that these two models should lead to activation maps highlightingdifferent regions of the same input image, we further introduce a contrastivemodule with a joint-task contrastive learning framework to explicitly model thecontradictory attributes of these two tasks. Different from conventionalintra-task contrastive learning for unsupervised representation learning, ourcontrastive module is designed to model the task-wise correlation, leading tocross-task representation learning. To better understand the two tasks from theperspective of uncertainty, we extensively investigate the uncertaintyestimation techniques for modeling the main uncertainties of the two tasks,namely task uncertainty (for SOD) and data uncertainty (for COD), and aiming toeffectively estimate the challenging regions for each task to achievedifficulty-aware learning. Experimental results on benchmark datasetsdemonstrate that our solution leads to both state-of-the-art performance andinformative uncertainty estimation.</description><author>Aixuan Li, Jing Zhang, Yunqiu Lv, Tong Zhang, Yiran Zhong, Mingyi He, Yuchao Dai</author><pubDate>Mon, 10 Jul 2023 16:49:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04651v1</guid></item><item><title>Exploring Antitrust and Platform Power in Generative AI</title><link>http://arxiv.org/abs/2306.11342v3</link><description>The concentration of power in a few digital technology companies has become asubject of increasing interest in both academic and non-academic discussions.One of the most noteworthy contributions to the debate is Lina Khan's Amazon'sAntitrust Paradox. In this work, Khan contends that Amazon has systematicallyexerted its dominance in online retail to eliminate competitors andsubsequently charge above-market prices. This work contributed to Khan'sappointment as the chair of the US Federal Trade Commission (FTC), one of themost influential antitrust organisations. Today, several ongoing antitrustlawsuits in the US and Europe involve major technology companies like Apple,Google/Alphabet, and Facebook/Meta. In the realm of generative AI, we are onceagain witnessing the same companies taking the lead in technologicaladvancements, leaving little room for others to compete. This article examinesthe market dominance of these corporations in the technology stack behindgenerative AI from an antitrust law perspective.</description><author>Konrad Kollnig, Qian Li</author><pubDate>Mon, 10 Jul 2023 16:48:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11342v3</guid></item><item><title>GFlowNet Foundations</title><link>http://arxiv.org/abs/2111.09266v4</link><description>Generative Flow Networks (GFlowNets) have been introduced as a method tosample a diverse set of candidates in an active learning context, with atraining objective that makes them approximately sample in proportion to agiven reward function. In this paper, we show a number of additionaltheoretical properties of GFlowNets. They can be used to estimate jointprobability distributions and the corresponding marginal distributions wheresome variables are unspecified and, of particular interest, can representdistributions over composite objects like sets and graphs. GFlowNets amortizethe work typically done by computationally expensive MCMC methods in a singlebut trained generative pass. They could also be used to estimate partitionfunctions and free energies, conditional probabilities of supersets(supergraphs) given a subset (subgraph), as well as marginal distributions overall supersets (supergraphs) of a given set (graph). We introduce variationsenabling the estimation of entropy and mutual information, sampling from aPareto frontier, connections to reward-maximizing policies, and extensions tostochastic environments, continuous actions and modular energy functions.</description><author>Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, Emmanuel Bengio</author><pubDate>Mon, 10 Jul 2023 16:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.09266v4</guid></item><item><title>On the Predictive Accuracy of Neural Temporal Point Process Models for Continuous-time Event Data</title><link>http://arxiv.org/abs/2306.17066v2</link><description>Temporal Point Processes (TPPs) serve as the standard mathematical frameworkfor modeling asynchronous event sequences in continuous time. However,classical TPP models are often constrained by strong assumptions, limitingtheir ability to capture complex real-world event dynamics. To overcome thislimitation, researchers have proposed Neural TPPs, which leverage neuralnetwork parametrizations to offer more flexible and efficient modeling. Whilerecent studies demonstrate the effectiveness of Neural TPPs, they often lack aunified setup, relying on different baselines, datasets, and experimentalconfigurations. This makes it challenging to identify the key factors drivingimprovements in predictive accuracy, hindering research progress. To bridgethis gap, we present a comprehensive large-scale experimental study thatsystematically evaluates the predictive accuracy of state-of-the-art neural TPPmodels. Our study encompasses multiple real-world and synthetic event sequencedatasets, following a carefully designed unified setup. We thoroughlyinvestigate the influence of major architectural components such as eventencoding, history encoder, and decoder parametrization on both time and markprediction tasks. Additionally, we delve into the less explored area ofprobabilistic calibration for neural TPP models. By analyzing our results, wedraw insightful conclusions regarding the significance of history size and theimpact of architectural components on predictive accuracy. Furthermore, we shedlight on the miscalibration of mark distributions in neural TPP models. Ourstudy aims to provide valuable insights into the performance andcharacteristics of neural TPP models, contributing to a better understanding oftheir strengths and limitations.</description><author>Tanguy Bosser, Souhaib Ben Taieb</author><pubDate>Mon, 10 Jul 2023 16:44:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17066v2</guid></item><item><title>DORA: Exploring Outlier Representations in Deep Neural Networks</title><link>http://arxiv.org/abs/2206.04530v4</link><description>Deep Neural Networks (DNNs) excel at learning complex abstractions withintheir internal representations. However, the concepts they learn remain opaque,a problem that becomes particularly acute when models unintentionally learnspurious correlations. In this work, we present DORA (Data-agnOsticRepresentation Analysis), the first data-agnostic framework for analyzing therepresentational space of DNNs. Central to our framework is the proposedExtreme-Activation (EA) distance measure, which assesses similarities betweenrepresentations by analyzing their activation patterns on data points thatcause the highest level of activation. As spurious correlations often manifestin features of data that are anomalous to the desired task, such as watermarksor artifacts, we demonstrate that internal representations capable of detectingsuch artifactual concepts can be found by analyzing relationships within neuralrepresentations. We validate the EA metric quantitatively, demonstrating itseffectiveness both in controlled scenarios and real-world applications.Finally, we provide practical examples from popular Computer Vision models toillustrate that representations identified as outliers using the EA metricoften correspond to undesired and spurious concepts.</description><author>Kirill Bykov, Mayukh Deb, Dennis Grinwald, Klaus-Robert Müller, Marina M. -C. Höhne</author><pubDate>Mon, 10 Jul 2023 16:42:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.04530v4</guid></item><item><title>Multimodal brain age estimation using interpretable adaptive population-graph learning</title><link>http://arxiv.org/abs/2307.04639v1</link><description>Brain age estimation is clinically important as it can provide valuableinformation in the context of neurodegenerative diseases such as Alzheimer's.Population graphs, which include multimodal imaging information of the subjectsalong with the relationships among the population, have been used in literaturealong with Graph Convolutional Networks (GCNs) and have proved beneficial for avariety of medical imaging tasks. A population graph is usually static andconstructed manually using non-imaging information. However, graph constructionis not a trivial task and might significantly affect the performance of theGCN, which is inherently very sensitive to the graph structure. In this work,we propose a framework that learns a population graph structure optimized forthe downstream task. An attention mechanism assigns weights to a set of imagingand non-imaging features (phenotypes), which are then used for edge extraction.The resulting graph is used to train the GCN. The entire pipeline can betrained end-to-end. Additionally, by visualizing the attention weights thatwere the most important for the graph construction, we increase theinterpretability of the graph. We use the UK Biobank, which provides a largevariety of neuroimaging and non-imaging phenotypes, to evaluate our method onbrain age regression and classification. The proposed method outperformscompeting static graph approaches and other state-of-the-art adaptive methods.We further show that the assigned attention scores indicate that there are bothimaging and non-imaging phenotypes that are informative for brain ageestimation and are in agreement with the relevant literature.</description><author>Kyriaki-Margarita Bintsi, Vasileios Baltatzis, Rolandos Alexandros Potamias, Alexander Hammers, Daniel Rueckert</author><pubDate>Mon, 10 Jul 2023 16:35:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04639v1</guid></item><item><title>Local-Global Methods for Generalised Solar Irradiance Forecasting</title><link>http://arxiv.org/abs/2303.06010v2</link><description>As the use of solar power increases, having accurate and timely forecastswill be essential for smooth grid operators. There are many proposed methodsfor forecasting solar irradiance / solar power production. However, many ofthese methods formulate the problem as a time-series, relying on near real-timeaccess to observations at the location of interest to generate forecasts. Thisrequires both access to a real-time stream of data and enough historicalobservations for these methods to be deployed. In this paper, we propose theuse of Global methods to train our models in a generalised way, enabling themto generate forecasts for unseen locations. We apply this approach to bothclassical ML and state of the art methods. Using data from 20 locationsdistributed throughout the UK and widely available weather data, we show thatit is possible to build systems that do not require access to this data. Weutilise and compare both satellite and ground observations (e.g. temperature,pressure) of weather data. Leveraging weather observations and measurementsfrom other locations we show it is possible to create models capable ofaccurately forecasting solar irradiance at new locations. This could facilitateuse planning and optimisation for both newly deployed solar farms and domesticinstallations from the moment they come online. Additionally, we show thattraining a single global model for multiple locations can produce a more robustmodel with more consistent and accurate results across locations.</description><author>Timothy Cargan, Dario Landa-Silva, Isaac Triguero</author><pubDate>Mon, 10 Jul 2023 16:33:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06010v2</guid></item><item><title>Approach to the cellular automaton interpretation with deep learning</title><link>http://arxiv.org/abs/2012.06441v6</link><description>In this paper, we will consider the deep learning systems that can learnfundamental physics theory based on cellular automaton interpretation (CAI).First, assuming that we can map quantum states to cellular automaton (CA) andcalculate the time-evolved CA for any initial CA by knowing the time-evolutionlaw of the given system, we will show that there exists a convolutional neuralnetwork (CNN) architecture that can learn the time-evolution law of this systemwith only the calculated data set for a time-reversible CA. Mathematically,finding a CNN architecture that can learn CA rule is equivalent to showing thata time-evolution operator can be approximated as a finite composition oftime-independent linear functions and ReLU type non-linear functions, as thepossible associated generator of approximation may absorbs the informationabout the dynamics. Going one step further, we will discuss the correspondencebetween the quantum system and deep learning architecture and relate theconcept of moduli space of Riemann surfaces to deep learning parameters whenconsidering interactions. Finally, for the CA model in which the dimensionalreduction in quantum gravity was first presented, we will discuss the CNNarchitecture that can find the non-trivial evolution law for holographicdirection in a deductive way without the label. It is suggested that the limitsto this effort can be improved through AdS/CFT correspondance.</description><author>Hyunju Go</author><pubDate>Mon, 10 Jul 2023 16:31:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.06441v6</guid></item><item><title>A Comparative Study of Self-Supervised Speech Representations in Read and Spontaneous TTS</title><link>http://arxiv.org/abs/2303.02719v2</link><description>Recent work has explored using self-supervised learning (SSL) speechrepresentations such as wav2vec2.0 as the representation medium in standardtwo-stage TTS, in place of conventionally used mel-spectrograms. It is howeverunclear which speech SSL is the better fit for TTS, and whether or not theperformance differs between read and spontaneous TTS, the later of which isarguably more challenging. This study aims at addressing these questions bytesting several speech SSLs, including different layers of the same SSL, intwo-stage TTS on both read and spontaneous corpora, while maintaining constantTTS model architecture and training settings. Results from listening tests showthat the 9th layer of 12-layer wav2vec2.0 (ASR finetuned) outperforms othertested SSLs and mel-spectrogram, in both read and spontaneous TTS. Our worksheds light on both how speech SSL can readily improve current TTS systems, andhow SSLs compare in the challenging generative task of TTS. Audio examples canbe found at https://www.speech.kth.se/tts-demos/ssr_tts</description><author>Siyang Wang, Gustav Eje Henter, Joakim Gustafson, Éva Székely</author><pubDate>Mon, 10 Jul 2023 16:15:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02719v2</guid></item><item><title>Decentralized SGD and Average-direction SAM are Asymptotically Equivalent</title><link>http://arxiv.org/abs/2306.02913v3</link><description>Decentralized stochastic gradient descent (D-SGD) allows collaborativelearning on massive devices simultaneously without the control of a centralserver. However, existing theories claim that decentralization invariablyundermines generalization. In this paper, we challenge the conventional beliefand present a completely new perspective for understanding decentralizedlearning. We prove that D-SGD implicitly minimizes the loss function of anaverage-direction Sharpness-aware minimization (SAM) algorithm under generalnon-convex non-$\beta$-smooth settings. This surprising asymptotic equivalencereveals an intrinsic regularization-optimization trade-off and three advantagesof decentralization: (1) there exists a free uncertainty evaluation mechanismin D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradientsmoothing effect; and (3) the sharpness regularization effect of D-SGD does notdecrease as total batch size increases, which justifies the potentialgeneralization benefit of D-SGD over centralized SGD (C-SGD) in large-batchscenarios.</description><author>Tongtian Zhu, Fengxiang He, Kaixuan Chen, Mingli Song, Dacheng Tao</author><pubDate>Mon, 10 Jul 2023 16:15:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02913v3</guid></item><item><title>Variational Bayes Made Easy</title><link>http://arxiv.org/abs/2304.14251v2</link><description>Variational Bayes is a popular method for approximate inference but itsderivation can be cumbersome. To simplify the process, we give a 3-step recipeto identify the posterior form by explicitly looking for linearity with respectto expectations of well-known distributions. We can then directly write theupdate by simply ``reading-off'' the terms in front of those expectations. Therecipe makes the derivation easier, faster, shorter, and more general.</description><author>Mohammad Emtiyaz Khan</author><pubDate>Mon, 10 Jul 2023 16:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14251v2</guid></item><item><title>Contextual Combinatorial Multi-output GP Bandits with Group Constraints</title><link>http://arxiv.org/abs/2111.14778v2</link><description>In federated multi-armed bandit problems, maximizing global reward whilesatisfying minimum privacy requirements to protect clients is the main goal. Toformulate such problems, we consider a combinatorial contextual bandit settingwith groups and changing action sets, where similar base arms arrive in groupsand a set of base arms, called a super arm, must be chosen in each round tomaximize super arm reward while satisfying the constraints of the rewards ofgroups from which base arms were chosen. To allow for greater flexibility, welet each base arm have two outcomes, modeled as the output of a two-outputGaussian process (GP), where one outcome is used to compute super arm rewardand the other for group reward. We then propose a novel double-UCB GP-banditalgorithm, called Thresholded Combinatorial Gaussian Process Upper ConfidenceBounds (TCGP-UCB), which balances between maximizing cumulative super armreward and satisfying group reward constraints and can be tuned to prefer oneover the other. We also define a new notion of regret that combines super armregret with group reward constraint satisfaction and prove that TCGP-UCB incurs$\tilde{O}(\sqrt{\lambda^*(K)KT\overline{\gamma}_{T}} )$ regret with highprobability, where $\overline{\gamma}_{T}$ is the maximum information gainassociated with the set of base arm contexts that appeared in the first $T$rounds and $K$ is the maximum super arm cardinality over all rounds. We lastlyshow in experiments using synthetic and real-world data and based on afederated learning setup as well as a content-recommendation one that ouralgorithm performs better then the current non-GP state-of-the-artcombinatorial bandit algorithm, while satisfying group constraints.</description><author>Sepehr Elahi, Baran Atalar, Sevda Öğüt, Cem Tekin</author><pubDate>Mon, 10 Jul 2023 16:11:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.14778v2</guid></item><item><title>Measuring Lexical Diversity in Texts: The Twofold Length Problem</title><link>http://arxiv.org/abs/2307.04626v1</link><description>The impact of text length on the estimation of lexical diversity has capturedthe attention of the scientific community for more than a century. Numerousindices have been proposed, and many studies have been conducted to evaluatethem, but the problem remains. This methodological review provides a criticalanalysis not only of the most commonly used indices in language learningstudies, but also of the length problem itself, as well as of the methodologyfor evaluating the proposed solutions. The analysis of three datasets ofEnglish language-learners' texts revealed that indices that reduce all texts tothe same length using a probabilistic or an algorithmic approach solve thelength dependency problem; however, all these indices failed to address thesecond problem, which is their sensitivity to the parameter that determines thelength to which the texts are reduced. The paper concludes with recommendationsfor optimizing lexical diversity analysis.</description><author>Yves Bestgen</author><pubDate>Mon, 10 Jul 2023 16:10:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04626v1</guid></item><item><title>Efficient Data-Driven Optimization with Noisy Data</title><link>http://arxiv.org/abs/2102.04363v3</link><description>Classical Kullback-Leibler or entropic distances are known to enjoy certaindesirable statistical properties in the context of decision-making withnoiseless data. However, in most practical situations the data available to adecision maker is subject to a certain amount of measurement noise. We hencestudy here data-driven prescription problems in which the data is corrupted bya known noise source. We derive efficient data-driven formulations in thisnoisy regime and indicate that they enjoy an entropic optimal transportinterpretation. Finally, we show that these efficient robust formulations aretractable in several interesting settings by exploiting a classicalrepresentation result by Strassen.</description><author>Bart P. G. Van Parys</author><pubDate>Mon, 10 Jul 2023 16:08:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.04363v3</guid></item><item><title>Large Language Models, Natural Language Processing, Domain Specialization</title><link>http://arxiv.org/abs/2305.18703v3</link><description>Large language models (LLMs) have significantly advanced the field of naturallanguage processing (NLP), providing a highly useful, task-agnostic foundationfor a wide range of applications. However, directly applying LLMs to solvesophisticated problems in specific domains meets many hurdles, caused by theheterogeneity of domain data, the sophistication of domain knowledge, theuniqueness of domain objectives, and the diversity of the constraints (e.g.,various social norms, cultural conformity, religious beliefs, and ethicalstandards in the domain applications). Domain specification techniques are keyto make large language models disruptive in many applications. Specifically, tosolve these hurdles, there has been a notable increase in research andpractices conducted in recent years on the domain specialization of LLMs. Thisemerging field of study, with its substantial potential for impact,necessitates a comprehensive and systematic review to better summarize andguide ongoing work in this area. In this article, we present a comprehensivesurvey on domain specification techniques for large language models, anemerging direction critical for large language model applications. First, wepropose a systematic taxonomy that categorizes the LLM domain-specializationtechniques based on the accessibility to LLMs and summarizes the framework forall the subcategories as well as their relations and differences to each other.Second, we present an extensive taxonomy of critical application domains thatcan benefit dramatically from specialized LLMs, discussing their practicalsignificance and open challenges. Last, we offer our insights into the currentresearch status and future trends in this area.</description><author>Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris White, Quanquan Gu, Jian Pei, Carl Yang, Liang Zhao</author><pubDate>Mon, 10 Jul 2023 16:06:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18703v3</guid></item><item><title>Weakly-supervised positional contrastive learning: application to cirrhosis classification</title><link>http://arxiv.org/abs/2307.04617v1</link><description>Large medical imaging datasets can be cheaply and quickly annotated withlow-confidence, weak labels (e.g., radiological scores). Access tohigh-confidence labels, such as histology-based diagnoses, is rare and costly.Pretraining strategies, like contrastive learning (CL) methods, can leverageunlabeled or weakly-annotated datasets. These methods typically require largebatch sizes, which poses a difficulty in the case of large 3D images at fullresolution, due to limited GPU memory. Nevertheless, volumetric positionalinformation about the spatial context of each 2D slice can be very importantfor some medical applications. In this work, we propose an efficientweakly-supervised positional (WSP) contrastive learning strategy where weintegrate both the spatial context of each 2D slice and a weak label via ageneric kernel-based loss function. We illustrate our method on cirrhosisprediction using a large volume of weakly-labeled images, namely radiologicallow-confidence annotations, and small strongly-labeled (i.e., high-confidence)datasets. The proposed model improves the classification AUC by 5% with respectto a baseline model on our internal dataset, and by 26% on the public LIHCdataset from the Cancer Genome Atlas. The code is available at:https://github.com/Guerbet-AI/wsp-contrastive.</description><author>Emma Sarfati, Alexandre Bône, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch</author><pubDate>Mon, 10 Jul 2023 16:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04617v1</guid></item><item><title>MiVOLO: Multi-input Transformer for Age and Gender Estimation</title><link>http://arxiv.org/abs/2307.04616v1</link><description>Age and gender recognition in the wild is a highly challenging task: apartfrom the variability of conditions, pose complexities, and varying imagequality, there are cases where the face is partially or completely occluded. Wepresent MiVOLO (Multi Input VOLO), a straightforward approach for age andgender estimation using the latest vision transformer. Our method integratesboth tasks into a unified dual input/output model, leveraging not only facialinformation but also person image data. This improves the generalizationability of our model and enables it to deliver satisfactory results even whenthe face is not visible in the image. To evaluate our proposed model, weconduct experiments on four popular benchmarks and achieve state-of-the-artperformance, while demonstrating real-time processing capabilities.Additionally, we introduce a novel benchmark based on images from the OpenImages Dataset. The ground truth annotations for this benchmark have beenmeticulously generated by human annotators, resulting in high accuracy answersdue to the smart aggregation of votes. Furthermore, we compare our model's agerecognition performance with human-level accuracy and demonstrate that itsignificantly outperforms humans across a majority of age ranges. Finally, wegrant public access to our models, along with the code for validation andinference. In addition, we provide extra annotations for used datasets andintroduce our new benchmark.</description><author>Maksim Kuprashevich, Irina Tolstykh</author><pubDate>Mon, 10 Jul 2023 15:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04616v1</guid></item><item><title>SPLAL: Similarity-based pseudo-labeling with alignment loss for semi-supervised medical image classification</title><link>http://arxiv.org/abs/2307.04610v1</link><description>Medical image classification is a challenging task due to the scarcity oflabeled samples and class imbalance caused by the high variance in diseaseprevalence. Semi-supervised learning (SSL) methods can mitigate thesechallenges by leveraging both labeled and unlabeled data. However, SSL methodsfor medical image classification need to address two key challenges: (1)estimating reliable pseudo-labels for the images in the unlabeled dataset and(2) reducing biases caused by class imbalance. In this paper, we propose anovel SSL approach, SPLAL, that effectively addresses these challenges. SPLALleverages class prototypes and a weighted combination of classifiers to predictreliable pseudo-labels over a subset of unlabeled images. Additionally, weintroduce alignment loss to mitigate model biases toward majority classes. Toevaluate the performance of our proposed approach, we conduct experiments ontwo publicly available medical image classification benchmark datasets: theskin lesion classification (ISIC 2018) and the blood cell classificationdataset (BCCD). The experimental results empirically demonstrate that ourapproach outperforms several state-of-the-art SSL methods over variousevaluation metrics. Specifically, our proposed approach achieves a significantimprovement over the state-of-the-art approach on the ISIC 2018 dataset in bothAccuracy and F1 score, with relative margins of 2.24\% and 11.40\%,respectively. Finally, we conduct extensive ablation experiments to examine thecontribution of different components of our approach, validating itseffectiveness.</description><author>Md Junaid Mahmood, Pranaw Raj, Divyansh Agarwal, Suruchi Kumari, Pravendra Singh</author><pubDate>Mon, 10 Jul 2023 15:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04610v1</guid></item><item><title>Learning Interpretable Heuristics for WalkSAT</title><link>http://arxiv.org/abs/2307.04608v1</link><description>Local search algorithms are well-known methods for solving large, hardinstances of the satisfiability problem (SAT). The performance of thesealgorithms crucially depends on heuristics for setting noise parameters andscoring variables. The optimal setting for these heuristics varies fordifferent instance distributions. In this paper, we present an approach forlearning effective variable scoring functions and noise parameters by usingreinforcement learning. We consider satisfiability problems from differentinstance distributions and learn specialized heuristics for each of them. Ourexperimental results show improvements with respect to both a WalkSAT baselineand another local search learned heuristic.</description><author>Yannet Interian, Sara Bernardini</author><pubDate>Mon, 10 Jul 2023 15:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04608v1</guid></item><item><title>A Memristor-Inspired Computation for Epileptiform Signals in Spheroids</title><link>http://arxiv.org/abs/2307.04607v1</link><description>In this paper we present a memristor-inspired computational method forobtaining a type of running spectrogram or fingerprint of epileptiform activitygenerated by rodent hippocampal spheroids. It can be used to compute on the flyand with low computational cost an alert-level signal for epileptiform eventsonset. Here, we describe the computational method behind this fingerprinttechnique and illustrate it using epileptiform events recorded from hippocampalspheroids using a microelectrode array system.</description><author>Iván Díez de los Ríos, John Wesley Ephraim, Gemma Palazzolo, Teresa Serrano-Gotarredona, Gabriella Panuccio, Bernabé Linares-Barranco</author><pubDate>Mon, 10 Jul 2023 15:51:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04607v1</guid></item><item><title>Optimal Learning for Structured Bandits</title><link>http://arxiv.org/abs/2007.07302v3</link><description>We study structured multi-armed bandits, which is the problem of onlinedecision-making under uncertainty in the presence of structural information. Inthis problem, the decision-maker needs to discover the best course of actiondespite observing only uncertain rewards over time. The decision-maker is awareof certain convex structural information regarding the reward distributions;that is, the decision-maker knows the reward distributions of the arms belongto a convex compact set. In the presence such structural information, they thenwould like to minimize their regret by exploiting this information, where theregret is its performance difference against a benchmark policy that knows thebest action ahead of time. In the absence of structural information, theclassical upper confidence bound (UCB) and Thomson sampling algorithms are wellknown to suffer minimal regret. As recently pointed out, neither algorithmsare, however, capable of exploiting structural information that is commonlyavailable in practice. We propose a novel learning algorithm that we call"DUSA" whose regret matches the information-theoretic regret lower bound up toa constant factor and can handle a wide range of structural information. Ouralgorithm DUSA solves a dual counterpart of the regret lower bound at theempirical reward distribution and follows its suggested play. We show that thisidea leads to the first computationally viable learning policy with asymptoticminimal regret for various structural information, including well-knownstructured bandits such as linear, Lipschitz, and convex bandits, and novelstructured bandits that have not been studied in the literature due to the lackof a unified and flexible framework.</description><author>Bart P. G. Van Parys, Negin Golrezaei</author><pubDate>Mon, 10 Jul 2023 15:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.07302v3</guid></item><item><title>EchoVest: Real-Time Sound Classification and Depth Perception Expressed through Transcutaneous Electrical Nerve Stimulation</title><link>http://arxiv.org/abs/2307.04604v1</link><description>Over 1.5 billion people worldwide live with hearing impairment. Despitevarious technologies that have been created for individuals with suchdisabilities, most of these technologies are either extremely expensive orinaccessible for everyday use in low-medium income countries. In order tocombat this issue, we have developed a new assistive device, EchoVest, forblind/deaf people to intuitively become more aware of their environment.EchoVest transmits vibrations to the user's body by utilizing transcutaneouselectric nerve stimulation (TENS) based on the source of the sounds. EchoVestalso provides various features, including sound localization, soundclassification, noise reduction, and depth perception. We aimed to outperformCNN-based machine-learning models, the most commonly used machine learningmodel for classification tasks, in accuracy and computational costs. To do so,we developed and employed a novel audio pipeline that adapts the AudioSpectrogram Transformer (AST) model, an attention-based model, for our soundclassification purposes, and Fast Fourier Transforms for noise reduction. Theapplication of Otsu's Method helped us find the optimal thresholds forbackground noise sound filtering and gave us much greater accuracy. In order tocalculate direction and depth accurately, we applied Complex Time Difference ofArrival algorithms and SOTA localization. Our last improvement was to use blindsource separation to make our algorithms applicable to multiple microphoneinputs. The final algorithm achieved state-of-the-art results on numerouscheckpoints, including a 95.7\% accuracy on the ESC-50 dataset forenvironmental sound classification.</description><author>Jesse Choe, Siddhant Sood, Ryan Park</author><pubDate>Mon, 10 Jul 2023 15:43:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04604v1</guid></item><item><title>Model-Driven Engineering for Artificial Intelligence -- A Systematic Literature Review</title><link>http://arxiv.org/abs/2307.04599v1</link><description>Objective: This study aims to investigate the existing body of knowledge inthe field of Model-Driven Engineering MDE in support of AI (MDE4AI) to sharpenfuture research further and define the current state of the art. Method: We conducted a Systemic Literature Review (SLR), collecting papersfrom five major databases resulting in 703 candidate studies, eventuallyretaining 15 primary studies. Each primary study will be evaluated anddiscussed with respect to the adoption of (1) MDE principles and practices and(2) the phases of AI development support aligned with the stages of theCRISP-DM methodology. Results: The study's findings show that the pillar concepts of MDE(metamodel, concrete syntax and model transformation), are leveraged to definedomain-specific languages (DSL) explicitly addressing AI concerns. DifferentMDE technologies are used, leveraging different language workbenches. The mostprominent AI-related concerns are training and modeling of the AI algorithm,while minor emphasis is given to the time-consuming preparation of the datasets. Early project phases that support interdisciplinary communication ofrequirements, such as the CRISP-DM \textit{Business Understanding} phase, arerarely reflected. Conclusion: The study found that the use of MDE for AI is still in its earlystages, and there is no single tool or method that is widely used.Additionally, current approaches tend to focus on specific stages ofdevelopment rather than providing support for the entire development process.As a result, the study suggests several research directions to further improvethe use of MDE for AI and to guide future research in this area.</description><author>Simon Raedler, Luca Berardinelli, Karolin Winter, Abbas Rahimi, Stefanie Rinderle-Ma</author><pubDate>Mon, 10 Jul 2023 15:38:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04599v1</guid></item><item><title>Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks</title><link>http://arxiv.org/abs/2305.16044v4</link><description>-- A theoretical framework that subsumes conventional deterministic spikingneural networks and surrogate gradients, facilitating more efficient andeffective employment of various neuromorphic hardware developments inreal-world applications. -- Scalable spiking neural models that incorporate noisy neuronal dynamicsfor implicit regularization, improved robustness, and computational accounts ofbiological neural computation, revealing that unreliable neural substratesyield reliable computation and learning. Networks of spiking neurons underpin the extraordinary information-processingcapabilities of the brain and have emerged as pillar models in neuromorphicintelligence. Despite extensive research on spiking neural networks (SNNs),most are established on deterministic models. Integrating noise into SNNs leadsto biophysically more realistic neural dynamics and may benefit modelperformance. This work presents the noisy spiking neural network (NSNN) and thenoise-driven learning rule (NDL) by introducing a spiking neuron modelincorporating noisy neuronal dynamics. Our approach shows how noise may serveas a resource for computation and learning and theoretically provides aframework for general SNNs. We show that our method exhibits competitiveperformance and improved robustness against challenging perturbations thandeterministic SNNs and better reproduces probabilistic neural computation inneural coding. This study offers a powerful and easy-to-use tool for machinelearning, neuromorphic intelligence practitioners, and computationalneuroscience researchers.</description><author>Gehua Ma, Rui Yan, Huajin Tang</author><pubDate>Mon, 10 Jul 2023 15:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16044v4</guid></item><item><title>Source-Free Open-Set Domain Adaptation for Histopathological Images via Distilling Self-Supervised Vision Transformer</title><link>http://arxiv.org/abs/2307.04596v1</link><description>There is a strong incentive to develop computational pathology models to i)ease the burden of tissue typology annotation from whole slide histologicalimages; ii) transfer knowledge, e.g., tissue class separability from thewithheld source domain to the distributionally shifted unlabeled target domain,and simultaneously iii) detect Open Set samples, i.e., unseen novel categoriesnot present in the training source domain. This paper proposes a highlypractical setting by addressing the abovementioned challenges in one fellswoop, i.e., source-free Open Set domain adaptation (SF-OSDA), which addressesthe situation where a model pre-trained on the inaccessible source dataset canbe adapted on the unlabeled target dataset containing Open Set samples. Thecentral tenet of our proposed method is distilling knowledge from aself-supervised vision transformer trained in the target domain. We propose anovel style-based data augmentation used as hard positives for self-training avision transformer in the target domain, yielding strongly contextualizedembedding. Subsequently, semantically similar target images are clustered whilethe source model provides their corresponding weak pseudo-labels withunreliable confidence. Furthermore, we propose cluster relative maximum logitscore (CRMLS) to rectify the confidence of the weak pseudo-labels and computeweighted class prototypes in the contextualized embedding space that areutilized for adapting the source model on the target domain. Our methodsignificantly outperforms the previous methods, including open set detection,test-time adaptation, and SF-OSDA methods, setting the new state-of-the-art onthree public histopathological datasets of colorectal cancer (CRC) assessment-Kather-16, Kather-19, and CRCTP. Our code is available athttps://github.com/LTS5/Proto-SF-OSDA.</description><author>Guillaume Vray, Devavrat Tomar, Behzad Bozorgtabar, Jean-Philippe Thiran</author><pubDate>Mon, 10 Jul 2023 15:36:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04596v1</guid></item><item><title>DWA: Differential Wavelet Amplifier for Image Super-Resolution</title><link>http://arxiv.org/abs/2307.04593v1</link><description>This work introduces Differential Wavelet Amplifier (DWA), a drop-in modulefor wavelet-based image Super-Resolution (SR). DWA invigorates an approachrecently receiving less attention, namely Discrete Wavelet Transformation(DWT). DWT enables an efficient image representation for SR and reduces thespatial area of its input by a factor of 4, the overall model size, andcomputation cost, framing it as an attractive approach for sustainable ML. Ourproposed DWA model improves wavelet-based SR models by leveraging thedifference between two convolutional filters to refine relevant featureextraction in the wavelet domain, emphasizing local contrasts and suppressingcommon noise in the input signals. We show its effectiveness by integrating itinto existing SR models, e.g., DWSR and MWCNN, and demonstrate a clearimprovement in classical SR tasks. Moreover, DWA enables a direct applicationof DWSR and MWCNN to input image space, reducing the DWT representationchannel-wise since it omits traditional DWT.</description><author>Brian B. Moser, Stanislav Frolov, Federico Raue, Sebastian Palacio, Andreas Dengel</author><pubDate>Mon, 10 Jul 2023 15:35:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04593v1</guid></item><item><title>A Graph Multi-separator Problem for Image Segmentation</title><link>http://arxiv.org/abs/2307.04592v1</link><description>We propose a novel abstraction of the image segmentation task in the form ofa combinatorial optimization problem that we call the multi-separator problem.Feasible solutions indicate for every pixel whether it belongs to a segment ora segment separator, and indicate for pairs of pixels whether or not the pixelsbelong to the same segment. This is in contrast to the closely related liftedmulticut problem where every pixel is associated to a segment and no pixelexplicitly represents a separating structure. While the multi-separator problemis NP-hard, we identify two special cases for which it can be solvedefficiently. Moreover, we define two local search algorithms for the generalcase and demonstrate their effectiveness in segmenting simulated volume imagesof foam cells and filaments.</description><author>Jannik Irmai, Shengxian Zhao, Jannik Presberger, Bjoern Andres</author><pubDate>Mon, 10 Jul 2023 15:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04592v1</guid></item><item><title>Neural Fields for Interactive Visualization of Statistical Dependencies in 3D Simulation Ensembles</title><link>http://arxiv.org/abs/2307.02203v2</link><description>We present the first neural network that has learned to compactly representand can efficiently reconstruct the statistical dependencies between the valuesof physical variables at different spatial locations in large 3D simulationensembles. Going beyond linear dependencies, we consider mutual information asa measure of non-linear dependence. We demonstrate learning and reconstructionwith a large weather forecast ensemble comprising 1000 members, each storingmultiple physical variables at a 250 x 352 x 20 simulation grid. Bycircumventing compute-intensive statistical estimators at runtime, wedemonstrate significantly reduced memory and computation requirements forreconstructing the major dependence structures. This enables embedding theestimator into a GPU-accelerated direct volume renderer and interactivelyvisualizing all mutual dependencies for a selected domain point.</description><author>Fatemeh Farokhmanesh, Kevin Höhlein, Christoph Neuhauser, Rüdiger Westermann</author><pubDate>Mon, 10 Jul 2023 15:23:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02203v2</guid></item><item><title>Programmable Synthetic Tabular Data Generation</title><link>http://arxiv.org/abs/2307.03577v2</link><description>Large amounts of tabular data remain underutilized due to privacy, dataquality, and data sharing limitations. While training a generative modelproducing synthetic data resembling the original distribution addresses some ofthese issues, most applications require additional constraints from thegenerated data. Existing synthetic data approaches are limited as theytypically only handle specific constraints, e.g., differential privacy (DP) orincreased fairness, and lack an accessible interface for declaring generalspecifications. In this work, we introduce ProgSyn, the first programmablesynthetic tabular data generation algorithm that allows for comprehensivecustomization over the generated data. To ensure high data quality whileadhering to custom specifications, ProgSyn pre-trains a generative model on theoriginal dataset and fine-tunes it on a differentiable loss automaticallyderived from the provided specifications. These can be programmaticallydeclared using statistical and logical expressions, supporting a wide range ofrequirements (e.g., DP or fairness, among others). We conduct an extensiveexperimental evaluation of ProgSyn on a number of constraints, achieving a newstate-of-the-art on some, while remaining general. For instance, at the samefairness level we achieve 2.3% higher downstream accuracy than thestate-of-the-art in fair synthetic data generation on the Adult dataset.Overall, ProgSyn provides a versatile and accessible framework for generatingconstrained synthetic tabular data, allowing for specifications that generalizebeyond the capabilities of prior work.</description><author>Mark Vero, Mislav Balunović, Martin Vechev</author><pubDate>Mon, 10 Jul 2023 15:22:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03577v2</guid></item><item><title>SCORE: Approximating Curvature Information under Self-Concordant Regularization</title><link>http://arxiv.org/abs/2112.07344v3</link><description>Optimization problems that include regularization functions in theirobjectives are regularly solved in many applications. When one seekssecond-order methods for such problems, it may be desirable to exploit specificproperties of some of these regularization functions when accounting forcurvature information in the solution steps to speed up convergence. In thispaper, we propose the SCORE (self-concordant regularization) framework forunconstrained minimization problems which incorporates second-order informationin the Newton-decrement framework for convex optimization. We propose thegeneralized Gauss-Newton with Self-Concordant Regularization (GGN-SCORE)algorithm that updates the minimization variables each time it receives a newinput batch. The proposed algorithm exploits the structure of the second-orderinformation in the Hessian matrix, thereby reducing computational overhead.GGN-SCORE demonstrates how to speed up convergence while also improving modelgeneralization for problems that involve regularized minimization under theproposed SCORE framework. Numerical experiments show the efficiency of ourmethod and its fast convergence, which compare favorably against baselinefirst-order and quasi-Newton methods. Additional experiments involvingnon-convex (overparameterized) neural network training problems show that theproposed method is promising for non-convex optimization.</description><author>Adeyemi D. Adeoye, Alberto Bemporad</author><pubDate>Mon, 10 Jul 2023 15:13:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.07344v3</guid></item><item><title>AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System</title><link>http://arxiv.org/abs/2307.04577v1</link><description>Vision-based teleoperation offers the possibility to endow robots withhuman-level intelligence to physically interact with the environment, whileonly requiring low-cost camera sensors. However, current vision-basedteleoperation systems are designed and engineered towards a particular robotmodel and deploy environment, which scales poorly as the pool of the robotmodels expands and the variety of the operating environment increases. In thispaper, we propose AnyTeleop, a unified and general teleoperation system tosupport multiple different arms, hands, realities, and camera configurationswithin a single system. Although being designed to provide great flexibility tothe choice of simulators and real hardware, our system can still achieve greatperformance. For real-world experiments, AnyTeleop can outperform a previoussystem that was designed for a specific robot hardware with a higher successrate, using the same robot. For teleoperation in simulation, AnyTeleop leads tobetter imitation learning performance, compared with a previous system that isparticularly designed for that simulator. Project page: http://anyteleop.com/.</description><author>Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao, Dietor Fox</author><pubDate>Mon, 10 Jul 2023 15:11:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04577v1</guid></item><item><title>Generalization Bounds with Data-dependent Fractal Dimensions</title><link>http://arxiv.org/abs/2302.02766v2</link><description>Providing generalization guarantees for modern neural networks has been acrucial task in statistical learning. Recently, several studies have attemptedto analyze the generalization error in such settings by using tools fromfractal geometry. While these works have successfully introduced newmathematical tools to apprehend generalization, they heavily rely on aLipschitz continuity assumption, which in general does not hold for neuralnetworks and might make the bounds vacuous. In this work, we address this issueand prove fractal geometry-based generalization bounds without requiring anyLipschitz assumption. To achieve this goal, we build up on a classical coveringargument in learning theory and introduce a data-dependent fractal dimension.Despite introducing a significant amount of technical complications, this newnotion lets us control the generalization error (over either fixed or randomhypothesis spaces) along with certain mutual information (MI) terms. To providea clearer interpretation to the newly introduced MI terms, as a next step, weintroduce a notion of "geometric stability" and link our bounds to the priorart. Finally, we make a rigorous connection between the proposed data-dependentdimension and topological data analysis tools, which then enables us to computethe dimension in a numerically efficient way. We support our theory withexperiments conducted on various settings.</description><author>Benjamin Dupuis, George Deligiannidis, Umut Şimşekli</author><pubDate>Mon, 10 Jul 2023 15:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02766v2</guid></item><item><title>TFR: Texture Defect Detection with Fourier Transform using Normal Reconstructed Template of Simple Autoencoder</title><link>http://arxiv.org/abs/2307.04574v1</link><description>Texture is an essential information in image representation, capturingpatterns and structures. As a result, texture plays a crucial role in themanufacturing industry and is extensively studied in the fields of computervision and pattern recognition. However, real-world textures are susceptible todefects, which can degrade image quality and cause various issues. Therefore,there is a need for accurate and effective methods to detect texture defects.In this study, a simple autoencoder and Fourier transform are employed fortexture defect detection. The proposed method combines Fourier transformanalysis with the reconstructed template obtained from the simple autoencoder.Fourier transform is a powerful tool for analyzing the frequency domain ofimages and signals. Moreover, since texture defects often exhibitcharacteristic changes in specific frequency ranges, analyzing the frequencydomain enables effective defect detection. The proposed method demonstrateseffectiveness and accuracy in detecting texture defects. Experimental resultsare presented to evaluate its performance and compare it with existingapproaches.</description><author>Jongwook Si, Sungyoung Kim</author><pubDate>Mon, 10 Jul 2023 15:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04574v1</guid></item><item><title>A Semi-Automated Solution Approach Selection Tool for Any Use Case via Scopus and OpenAI: a Case Study for AI/ML in Oncology</title><link>http://arxiv.org/abs/2307.04573v1</link><description>In today's vast literature landscape, a manual review is very time-consuming.To address this challenge, this paper proposes a semi-automated tool forsolution method review and selection. It caters to researchers, practitioners,and decision-makers while serving as a benchmark for future work. The toolcomprises three modules: (1) paper selection and scoring, using a keywordselection scheme to query Scopus API and compute relevancy; (2) solution methodextraction in papers utilizing OpenAI API; (3) sensitivity analysis andpost-analyzes. It reveals trends, relevant papers, and methods. AI in theoncology case study and several use cases are presented with promising results,comparing the tool to manual ground truth.</description><author>Deniz Kenan Kılıç, Alex Elkjær Vasegaard, Aurélien Desoeuvres, Peter Nielsen</author><pubDate>Mon, 10 Jul 2023 15:07:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04573v1</guid></item><item><title>Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements</title><link>http://arxiv.org/abs/2307.02329v2</link><description>The advent of novel 5G services and applications with binding latencyrequirements and guaranteed Quality of Service (QoS) hastened the need toincorporate autonomous and proactive decision-making in network managementprocedures. The objective of our study is to provide a thorough analysis ofpredictive latency within 5G networks by utilizing real-world network data thatis accessible to mobile network operators (MNOs). In particular, (i) we presentan analytical formulation of the user-plane latency as a Hypoexponentialdistribution, which is validated by means of a comparative analysis withempirical measurements, and (ii) we conduct experimental results ofprobabilistic regression, anomaly detection, and predictive forecastingleveraging on emerging domains in Machine Learning (ML), such as BayesianLearning (BL) and Machine Learning on Graphs (GML). We test our predictiveframework using data gathered from scenarios of vehicular mobility, dense-urbantraffic, and social gathering events. Our results provide valuable insightsinto the efficacy of predictive algorithms in practical applications.</description><author>Marco Skocaj, Francesca Conserva, Nicol Sarcone Grande, Andrea Orsi, Davide Micheli, Giorgio Ghinamo, Simone Bizzarri, Roberto Verdone</author><pubDate>Mon, 10 Jul 2023 15:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02329v2</guid></item><item><title>Unraveling the Age Estimation Puzzle: Comparative Analysis of Deep Learning Approaches for Facial Age Estimation</title><link>http://arxiv.org/abs/2307.04570v1</link><description>Comparing different age estimation methods poses a challenge due to theunreliability of published results, stemming from inconsistencies in thebenchmarking process. Previous studies have reported continuous performanceimprovements over the past decade using specialized methods; however, ourfindings challenge these claims. We argue that, for age estimation tasksoutside of the low-data regime, designing specialized methods is unnecessary,and the standard approach of utilizing cross-entropy loss is sufficient. Thispaper aims to address the benchmark shortcomings by evaluating state-of-the-artage estimation methods in a unified and comparable setting. We systematicallyanalyze the impact of various factors, including facial alignment, facialcoverage, image resolution, image representation, model architecture, and theamount of data on age estimation results. Surprisingly, these factors oftenexert a more significant influence than the choice of the age estimation methoditself. We assess the generalization capability of each method by evaluatingthe cross-dataset performance for publicly available age estimation datasets.The results emphasize the importance of using consistent data preprocessingpractices and establishing standardized benchmarks to ensure reliable andmeaningful comparisons. The source code is available athttps://github.com/paplhjak/Facial-Age-Estimation-Benchmark.</description><author>Jakub Paplham, Vojtech Franc</author><pubDate>Mon, 10 Jul 2023 15:02:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04570v1</guid></item><item><title>Interpreting and generalizing deep learning in physics-based problems with functional linear models</title><link>http://arxiv.org/abs/2307.04569v1</link><description>Although deep learning has achieved remarkable success in various scientificmachine learning applications, its black-box nature poses concerns regardinginterpretability and generalization capabilities beyond the training data.Interpretability is crucial and often desired in modeling physical systems.Moreover, acquiring extensive datasets that encompass the entire range of inputfeatures is challenging in many physics-based learning tasks, leading toincreased errors when encountering out-of-distribution (OOD) data. In thiswork, motivated by the field of functional data analysis (FDA), we proposegeneralized functional linear models as an interpretable surrogate for atrained deep learning model. We demonstrate that our model could be trainedeither based on a trained neural network (post-hoc interpretation) or directlyfrom training data (interpretable operator learning). A library of generalizedfunctional linear models with different kernel functions is considered andsparse regression is used to discover an interpretable surrogate model thatcould be analytically presented. We present test cases in solid mechanics,fluid mechanics, and transport. Our results demonstrate that our model canachieve comparable accuracy to deep learning and can improve OOD generalizationwhile providing more transparency and interpretability. Our study underscoresthe significance of interpretability in scientific machine learning andshowcases the potential of functional linear models as a tool for interpretingand generalizing deep learning.</description><author>Amirhossein Arzani, Lingxiao Yuan, Pania Newell, Bei Wang</author><pubDate>Mon, 10 Jul 2023 15:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04569v1</guid></item><item><title>Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances</title><link>http://arxiv.org/abs/2306.10006v2</link><description>This paper presents a novel approach for text/speech-driven animation of aphoto-realistic head model based on blend-shape geometry, dynamic textures, andneural rendering. Training a VAE for geometry and texture yields a parametricmodel for accurate capturing and realistic synthesis of facial expressions froma latent feature vector. Our animation method is based on a conditional CNNthat transforms text or speech into a sequence of animation parameters. Incontrast to previous approaches, our animation model learnsdisentangling/synthesizing different acting-styles in an unsupervised manner,requiring only phonetic labels that describe the content of training sequences.For realistic real-time rendering, we train a U-Net that refinesrasterization-based renderings by computing improved pixel colors and aforeground matte. We compare our framework qualitatively/quantitatively againstrecent methods for head modeling as well as facial animation and evaluate theperceived rendering/animation quality in a user-study, which indicates largeimprovements compared to state-of-the-art approaches</description><author>Wolfgang Paier, Anna Hilsmann, Peter Eisert</author><pubDate>Mon, 10 Jul 2023 14:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10006v2</guid></item><item><title>A Survey on Visual Transformer</title><link>http://arxiv.org/abs/2012.12556v6</link><description>Transformer, first applied to the field of natural language processing, is atype of deep neural network mainly based on the self-attention mechanism.Thanks to its strong representation capabilities, researchers are looking atways to apply transformer to computer vision tasks. In a variety of visualbenchmarks, transformer-based models perform similar to or better than othertypes of networks such as convolutional and recurrent neural networks. Givenits high performance and less need for vision-specific inductive bias,transformer is receiving more and more attention from the computer visioncommunity. In this paper, we review these vision transformer models bycategorizing them in different tasks and analyzing their advantages anddisadvantages. The main categories we explore include the backbone network,high/mid-level vision, low-level vision, and video processing. We also includeefficient transformer methods for pushing transformer into real device-basedapplications. Furthermore, we also take a brief look at the self-attentionmechanism in computer vision, as it is the base component in transformer.Toward the end of this paper, we discuss the challenges and provide severalfurther research directions for vision transformers.</description><author>Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, Dacheng Tao</author><pubDate>Mon, 10 Jul 2023 14:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.12556v6</guid></item><item><title>Automatically detecting activities of daily living from in-home sensors as indicators of routine behaviour in an older population</title><link>http://arxiv.org/abs/2307.04563v1</link><description>Objective: The NEX project has developed an integrated Internet of Things(IoT) system coupled with data analytics to offer unobtrusive health andwellness monitoring supporting older adults living independently at home.Monitoring {currently} involves visualising a set of automatically detectedactivities of daily living (ADLs) for each participant. The detection of ADLsis achieved {} to allow the incorporation of additional participants whose ADLsare detected without re-training the system. Methods: Following an extensive User Needs and Requirements study involving426 participants, a pilot trial and a friendly trial of the deployment, anAction Research Cycle (ARC) trial was completed. This involved 23 participantsover a 10-week period each with c.20 IoT sensors in their homes. During the ARCtrial, participants each took part in two data-informed briefings whichpresented visualisations of their own in-home activities. The briefings alsogathered training data on the accuracy of detected activities. Association rulemining was then used on the combination of data from sensors and participantfeedback to improve the automatic detection of ADLs. Results: Association rule mining was used to detect a range of ADLs for eachparticipant independently of others and was then used to detect ADLs acrossparticipants using a single set of rules {for each ADL}. This allows additionalparticipants to be added without the necessity of them providing training data. Conclusions: Additional participants can be added to the NEX system withoutthe necessity to re-train the system for automatic detection of the set oftheir activities of daily living.</description><author>Claire M. Timon, Pamela Hussey, Hyowon Lee, Catriona Murphy, Harsh Vardan Rai, and Alan F. Smeaton</author><pubDate>Mon, 10 Jul 2023 14:52:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04563v1</guid></item><item><title>Art Authentication with Vision Transformers</title><link>http://arxiv.org/abs/2307.03039v2</link><description>In recent years, Transformers, initially developed for language, have beensuccessfully applied to visual tasks. Vision Transformers have been shown topush the state-of-the-art in a wide range of tasks, including imageclassification, object detection, and semantic segmentation. While ampleresearch has shown promising results in art attribution and art authenticationtasks using Convolutional Neural Networks, this paper examines if thesuperiority of Vision Transformers extends to art authentication, improving,thus, the reliability of computer-based authentication of artworks. Using acarefully compiled dataset of authentic paintings by Vincent van Gogh and twocontrast datasets, we compare the art authentication performances of SwinTransformers with those of EfficientNet. Using a standard contrast setcontaining imitations and proxies (works by painters with styles closelyrelated to van Gogh), we find that EfficientNet achieves the best performanceoverall. With a contrast set that only consists of imitations, we find the SwinTransformer to be superior to EfficientNet by achieving an authenticationaccuracy of over 85%. These results lead us to conclude that VisionTransformers represent a strong and promising contender in art authentication,particularly in enhancing the computer-based ability to detect artisticimitations.</description><author>Ludovica Schaerf, Carina Popovici, Eric Postma</author><pubDate>Mon, 10 Jul 2023 14:49:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03039v2</guid></item><item><title>Efficient Large-Scale Visual Representation Learning</title><link>http://arxiv.org/abs/2305.13399v3</link><description>In this article, we present our approach to single-modality visualrepresentation learning. Understanding visual representations of productcontent is vital for recommendations, search, and advertising applications ine-commerce. We detail and contrast techniques used to fine-tune large-scalevisual representation learning models in an efficient manner under low-resourcesettings, including several pretrained backbone architectures, both in theconvolutional neural network as well as the vision transformer family. Wehighlight the challenges for e-commerce applications at-scale and highlight theefforts to more efficiently train, evaluate, and serve visual representations.We present ablation studies evaluating the representation offline performancefor several downstream tasks, including our visually similar adrecommendations. To this end, we present a novel text-to-image generativeoffline evaluation method for visually similar recommendation systems. Finally,we include online results from deployed machine learning systems in productionat Etsy.</description><author>Eden Dolev, Alaa Awad, Denisa Roberts, Zahra Ebrahimzadeh, Marcin Mejran, Vaibhav Malpani, Mahir Yavuz</author><pubDate>Mon, 10 Jul 2023 14:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13399v3</guid></item><item><title>(Un)reasonable Allure of Ante-hoc Interpretability for High-stakes Domains: Transparency Is Necessary but Insufficient for Comprehensibility</title><link>http://arxiv.org/abs/2306.02312v2</link><description>Ante-hoc interpretability has become the holy grail of explainable artificialintelligence for high-stakes domains such as healthcare; however, this notionis elusive, lacks a widely-accepted definition and depends on the operationalcontext. It can refer to predictive models whose structure adheres todomain-specific constraints, or ones that are inherently transparent. Thelatter conceptualisation assumes observers who judge this quality, whereas theformer presupposes them to have technical and domain expertise (thus alienatingother groups of explainees). Additionally, the distinction between ante-hocinterpretability and the less desirable post-hoc explainability, which refersto methods that construct a separate explanatory model, is vague given thattransparent predictive models may still require (post-)processing to yieldsuitable explanatory insights. Ante-hoc interpretability is thus an overloadedconcept that comprises a range of implicit properties, which we unpack in thispaper to better understand what is needed for its safe adoption acrosshigh-stakes domains. To this end, we outline modelling and explainingdesiderata that allow us to navigate its distinct realisations in view of theenvisaged application and audience.</description><author>Kacper Sokol, Julia E. Vogt</author><pubDate>Mon, 10 Jul 2023 14:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02312v2</guid></item><item><title>Auxiliary Functions as Koopman Observables: Data-Driven Analysis of Dynamical Systems via Polynomial Optimization</title><link>http://arxiv.org/abs/2303.01483v3</link><description>We present a flexible data-driven method for dynamical system analysis thatdoes not require explicit model discovery. The method is rooted inwell-established techniques for approximating the Koopman operator from dataand is implemented as a semidefinite program that can be solved numerically.Furthermore, the method is agnostic of whether data is generated through adeterministic or stochastic process, so its implementation requires no prioradjustments by the user to accommodate these different scenarios. Rigorousconvergence results justify the applicability of the method, while alsoextending and uniting similar results from across the literature. Examples ondiscovering Lyapunov functions, performing ergodic optimization, and boundingextrema over attractors for both deterministic and stochastic dynamicsexemplify these convergence results and demonstrate the performance of themethod.</description><author>Jason J. Bramburger, Giovanni Fantuzzi</author><pubDate>Mon, 10 Jul 2023 14:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01483v3</guid></item><item><title>SparseVSR: Lightweight and Noise Robust Visual Speech Recognition</title><link>http://arxiv.org/abs/2307.04552v1</link><description>Recent advances in deep neural networks have achieved unprecedented successin visual speech recognition. However, there remains substantial disparitybetween current methods and their deployment in resource-constrained devices.In this work, we explore different magnitude-based pruning techniques togenerate a lightweight model that achieves higher performance than its densemodel equivalent, especially under the presence of visual noise. Our sparsemodels achieve state-of-the-art results at 10% sparsity on the LRS3 dataset andoutperform the dense equivalent up to 70% sparsity. We evaluate our 50% sparsemodel on 7 different visual noise types and achieve an overall absoluteimprovement of more than 2% WER compared to the dense equivalent. Our resultsconfirm that sparse networks are more resistant to noise than dense networks.</description><author>Adriana Fernandez-Lopez, Honglie Chen, Pingchuan Ma, Alexandros Haliassos, Stavros Petridis, Maja Pantic</author><pubDate>Mon, 10 Jul 2023 14:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04552v1</guid></item><item><title>Gradient Surgery for One-shot Unlearning on Generative Model</title><link>http://arxiv.org/abs/2307.04550v1</link><description>Recent regulation on right-to-be-forgotten emerges tons of interest inunlearning pre-trained machine learning models. While approximating astraightforward yet expensive approach of retrain-from-scratch, recent machineunlearning methods unlearn a sample by updating weights to remove its influenceon the weight parameters. In this paper, we introduce a simple yet effectiveapproach to remove a data influence on the deep generative model. Inspired byworks in multi-task learning, we propose to manipulate gradients to regularizethe interplay of influence among samples by projecting gradients onto thenormal plane of the gradients to be retained. Our work is agnostic tostatistics of the removal samples, outperforming existing baselines whileproviding theoretical analysis for the first time in unlearning a generativemodel.</description><author>Seohui Bae, Seoyoon Kim, Hyemin Jung, Woohyung Lim</author><pubDate>Mon, 10 Jul 2023 14:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04550v1</guid></item><item><title>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI</title><link>http://arxiv.org/abs/2307.02334v3</link><description>Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging(MRI) images from partial measurement is essential to medical imaging research.Benefiting from the diverse and complementary information of multi-contrast MRimages in different imaging modalities, multi-contrast Super-Resolution (SR)reconstruction is promising to yield SR images with higher quality. In themedical scenario, to fully visualize the lesion, radiologists are accustomed tozooming the MR images at arbitrary scales rather than using a fixed scale, asused by most MRI SR methods. In addition, existing multi-contrast MRI SRmethods often require a fixed resolution for the reference image, which makesacquiring reference images difficult and imposes limitations on arbitrary scaleSR tasks. To address these issues, we proposed an implicit neuralrepresentations based dual-arbitrary multi-contrast MRI super-resolutionmethod, called Dual-ArbNet. First, we decouple the resolution of the target andreference images by a feature encoder, enabling the network to input target andreference images at arbitrary scales. Then, an implicit fusion decoder fusesthe multi-contrast features and uses an Implicit Decoding Function~(IDF) toobtain the final MRI SR results. Furthermore, we introduce a curriculumlearning strategy to train our network, which improves the generalization andperformance of our Dual-ArbNet. Extensive experiments in two public MRIdatasets demonstrate that our method outperforms state-of-the-art approachesunder different scale factors and has great potential in clinical practice.</description><author>Jiamiao Zhang, Yichen Chi, Jun Lyu, Wenming Yang, Yapeng Tian</author><pubDate>Mon, 10 Jul 2023 14:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02334v3</guid></item><item><title>Customizing Synthetic Data for Data-Free Student Learning</title><link>http://arxiv.org/abs/2307.04542v1</link><description>Data-free knowledge distillation (DFKD) aims to obtain a lightweight studentmodel without original training data. Existing works generally synthesize datafrom the pre-trained teacher model to replace the original training data forstudent learning. To more effectively train the student model, the syntheticdata shall be customized to the current student learning ability. However, thisis ignored in the existing DFKD methods and thus negatively affects the studenttraining. To address this issue, we propose Customizing Synthetic Data forData-Free Student Learning (CSD) in this paper, which achieves adaptive datasynthesis using a self-supervised augmented auxiliary task to estimate thestudent learning ability. Specifically, data synthesis is dynamically adjustedto enlarge the cross entropy between the labels and the predictions from theself-supervised augmented task, thus generating hard samples for the studentmodel. The experiments on various datasets and teacher-student models show theeffectiveness of our proposed method. Code is available at:$\href{https://github.com/luoshiya/CSD}{https://github.com/luoshiya/CSD}$</description><author>Shiya Luo, Defang Chen, Can Wang</author><pubDate>Mon, 10 Jul 2023 14:17:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04542v1</guid></item><item><title>Syntax and Semantics Meet in the "Middle": Probing the Syntax-Semantics Interface of LMs Through Agentivity</title><link>http://arxiv.org/abs/2305.18185v2</link><description>Recent advances in large language models have prompted researchers to examinetheir abilities across a variety of linguistic tasks, but little has been doneto investigate how models handle the interactions in meaning across words andlarger syntactic forms -- i.e. phenomena at the intersection of syntax andsemantics. We present the semantic notion of agentivity as a case study forprobing such interactions. We created a novel evaluation dataset by utilitizingthe unique linguistic properties of a subset of optionally transitive Englishverbs. This dataset was used to prompt varying sizes of three model classes tosee if they are sensitive to agentivity at the lexical level, and if they canappropriately employ these word-level priors given a specific syntacticcontext. Overall, GPT-3 text-davinci-003 performs extremely well across allexperiments, outperforming all other models tested by far. In fact, the resultsare even better correlated with human judgements than both syntactic andsemantic corpus statistics. This suggests that LMs may potentially serve asmore useful tools for linguistic annotation, theory testing, and discovery thanselect corpora for certain tasks. Code is available athttps://github.com/lindiatjuatja/lm_sem</description><author>Lindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig</author><pubDate>Mon, 10 Jul 2023 14:10:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18185v2</guid></item><item><title>Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis</title><link>http://arxiv.org/abs/2307.04541v1</link><description>Fueled by deep learning, computer-aided diagnosis achieves huge advances.However, out of controlled lab environments, algorithms could face multiplechallenges. Open set recognition (OSR), as an important one, states thatcategories unseen in training could appear in testing. In medical fields, itcould derive from incompletely collected training datasets and the constantlyemerging new or rare diseases. OSR requires an algorithm to not only correctlyclassify known classes, but also recognize unknown classes and forward them toexperts for further diagnosis. To tackle OSR, we assume that known classescould densely occupy small parts of the embedding space and the remainingsparse regions could be recognized as unknowns. Following it, we propose OpenMargin Cosine Loss (OMCL) unifying two mechanisms. The former, called MarginLoss with Adaptive Scale (MLAS), introduces angular margin for reinforcingintra-class compactness and inter-class separability, together with an adaptivescaling factor to strengthen the generalization capacity. The latter, calledOpen-Space Suppression (OSS), opens the classifier by recognizing sparseembedding space as unknowns using proposed feature space descriptors. Besides,since medical OSR is still a nascent field, two publicly available benchmarkdatasets are proposed for comparison. Extensive ablation studies and featurevisualization demonstrate the effectiveness of each design. Compared withstate-of-the-art methods, MLAS achieves superior performances, measured by ACC,AUROC, and OSCR.</description><author>Mingyuan Liu, Lu Xu, Jicong Zhang</author><pubDate>Mon, 10 Jul 2023 14:09:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04541v1</guid></item><item><title>Q-YOLOP: Quantization-aware You Only Look Once for Panoptic Driving Perception</title><link>http://arxiv.org/abs/2307.04537v1</link><description>In this work, we present an efficient and quantization-aware panoptic drivingperception model (Q- YOLOP) for object detection, drivable area segmentation,and lane line segmentation, in the context of autonomous driving. Our modelemploys the Efficient Layer Aggregation Network (ELAN) as its backbone andtask-specific heads for each task. We employ a four-stage training process thatincludes pretraining on the BDD100K dataset, finetuning on both the BDD100K andiVS datasets, and quantization-aware training (QAT) on BDD100K. During thetraining process, we use powerful data augmentation techniques, such as randomperspective and mosaic, and train the model on a combination of the BDD100K andiVS datasets. Both strategies enhance the model's generalization capabilities.The proposed model achieves state-of-the-art performance with an mAP@0.5 of0.622 for object detection and an mIoU of 0.612 for segmentation, whilemaintaining low computational and memory requirements.</description><author>Chi-Chih Chang, Wei-Cheng Lin, Pei-Shuo Wang, Sheng-Feng Yu, Yu-Chen Lu, Kuan-Cheng Lin, Kai-Chiang Wu</author><pubDate>Mon, 10 Jul 2023 14:02:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04537v1</guid></item><item><title>DADO -- Low-Cost Selection Strategies for Deep Active Design Optimization</title><link>http://arxiv.org/abs/2307.04536v1</link><description>In this experience report, we apply deep active learning to the field ofdesign optimization to reduce the number of computationally expensive numericalsimulations. We are interested in optimizing the design of structuralcomponents, where the shape is described by a set of parameters. If we canpredict the performance based on these parameters and consider only thepromising candidates for simulation, there is an enormous potential for savingcomputing power. We present two selection strategies for self-optimization toreduce the computational cost in multi-objective design optimization problems.Our proposed methodology provides an intuitive approach that is easy to apply,offers significant improvements over random sampling, and circumvents the needfor uncertainty estimation. We evaluate our strategies on a large dataset fromthe domain of fluid dynamics and introduce two new evaluation metrics todetermine the model's performance. Findings from our evaluation highlights theeffectiveness of our selection strategies in accelerating design optimization.We believe that the introduced method is easily transferable to otherself-optimization problems.</description><author>Jens Decke, Christian Gruhl, Lukas Rauch, Bernhard Sick</author><pubDate>Mon, 10 Jul 2023 14:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04536v1</guid></item><item><title>QBitOpt: Fast and Accurate Bitwidth Reallocation during Training</title><link>http://arxiv.org/abs/2307.04535v1</link><description>Quantizing neural networks is one of the most effective methods for achievingefficient inference on mobile and embedded devices. In particular, mixedprecision quantized (MPQ) networks, whose layers can be quantized to differentbitwidths, achieve better task performance for the same resource constraintcompared to networks with homogeneous bitwidths. However, finding the optimalbitwidth allocation is a challenging problem as the search space growsexponentially with the number of layers in the network. In this paper, wepropose QBitOpt, a novel algorithm for updating bitwidths duringquantization-aware training (QAT). We formulate the bitwidth allocation problemas a constraint optimization problem. By combining fast-to-computesensitivities with efficient solvers during QAT, QBitOpt can producemixed-precision networks with high task performance guaranteed to satisfystrict resource constraints. This contrasts with existing mixed-precisionmethods that learn bitwidths using gradients and cannot provide suchguarantees. We evaluate QBitOpt on ImageNet and confirm that we outperformexisting fixed and mixed-precision methods under average bitwidth constraintscommonly found in the literature.</description><author>Jorn Peters, Marios Fournarakis, Markus Nagel, Mart van Baalen, Tijmen Blankevoort</author><pubDate>Mon, 10 Jul 2023 14:01:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04535v1</guid></item><item><title>Preventing Errors in Person Detection: A Part-Based Self-Monitoring Framework</title><link>http://arxiv.org/abs/2307.04533v1</link><description>The ability to detect learned objects regardless of their appearance iscrucial for autonomous systems in real-world applications. Especially fordetecting humans, which is often a fundamental task in safety-criticalapplications, it is vital to prevent errors. To address this challenge, wepropose a self-monitoring framework that allows for the perception system toperform plausibility checks at runtime. We show that by incorporating anadditional component for detecting human body parts, we are able tosignificantly reduce the number of missed human detections by factors of up to9 when compared to a baseline setup, which was trained only on holistic personobjects. Additionally, we found that training a model jointly on humans andtheir body parts leads to a substantial reduction in false positive detectionsby up to 50% compared to training on humans alone. We performed comprehensiveexperiments on the publicly available datasets DensePose and Pascal VOC inorder to demonstrate the effectiveness of our framework. Code is available athttps://github.com/ FraunhoferIKS/smf-object-detection.</description><author>Franziska Schwaiger, Andrea Matic, Karsten Roscher, Stephan Günnemann</author><pubDate>Mon, 10 Jul 2023 13:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04533v1</guid></item><item><title>Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis</title><link>http://arxiv.org/abs/2306.09417v2</link><description>With read-aloud speech synthesis achieving high naturalness scores, there isa growing research interest in synthesising spontaneous speech. However, humanspontaneous face-to-face conversation has both spoken and non-verbal aspects(here, co-speech gestures). Only recently has research begun to explore thebenefits of jointly synthesising these two modalities in a single system. Theprevious state of the art used non-probabilistic methods, which fail to capturethe variability of human speech and motion, and risk producing oversmoothingartefacts and sub-optimal synthesis quality. We present the firstdiffusion-based probabilistic model, called Diff-TTSG, that jointly learns tosynthesise speech and gestures together. Our method can be trained on smalldatasets from scratch. Furthermore, we describe a set of careful uni- andmulti-modal subjective tests for evaluating integrated speech and gesturesynthesis systems, and use them to validate our proposed approach. Forsynthesised examples please see https://shivammehta25.github.io/Diff-TTSG</description><author>Shivam Mehta, Siyang Wang, Simon Alexanderson, Jonas Beskow, Éva Székely, Gustav Eje Henter</author><pubDate>Mon, 10 Jul 2023 13:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09417v2</guid></item><item><title>Self Expanding Neural Networks</title><link>http://arxiv.org/abs/2307.04526v1</link><description>The results of training a neural network are heavily dependent on thearchitecture chosen; and even a modification of only the size of the network,however small, typically involves restarting the training process. In contrastto this, we begin training with a small architecture, only increase itscapacity as necessary for the problem, and avoid interfering with previousoptimization while doing so. We thereby introduce a natural gradient basedapproach which intuitively expands both the width and depth of a neural networkwhen this is likely to substantially reduce the hypothetical converged trainingloss. We prove an upper bound on the "rate" at which neurons are added, and acomputationally cheap lower bound on the expansion score. We illustrate thebenefits of such Self-Expanding Neural Networks in both classification andregression problems, including those where the appropriate architecture size issubstantially uncertain a priori.</description><author>Rupert Mitchell, Martin Mundt, Kristian Kersting</author><pubDate>Mon, 10 Jul 2023 13:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04526v1</guid></item><item><title>Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans</title><link>http://arxiv.org/abs/2307.04525v1</link><description>Gastric cancer is the third leading cause of cancer-related mortalityworldwide, but no guideline-recommended screening test exists. Existing methodscan be invasive, expensive, and lack sensitivity to identify early-stagegastric cancer. In this study, we explore the feasibility of using a deeplearning approach on non-contrast CT scans for gastric cancer detection. Wepropose a novel cluster-induced Mask Transformer that jointly segments thetumor and classifies abnormality in a multi-task manner. Our model incorporateslearnable clusters that encode the texture and shape prototypes of gastriccancer, utilizing self- and cross-attention to interact with convolutionalfeatures. In our experiments, the proposed method achieves a sensitivity of85.0% and specificity of 92.6% for detecting gastric tumors on a hold-out testset consisting of 100 patients with cancer and 148 normal. In comparison, tworadiologists have an average sensitivity of 73.5% and specificity of 84.3%. Wealso obtain a specificity of 97.7% on an external test set with 903 normalcases. Our approach performs comparably to established state-of-the-art gastriccancer screening tools like blood testing and endoscopy, while also being moresensitive in detecting early-stage cancer. This demonstrates the potential ofour approach as a novel, non-invasive, low-cost, and accurate method foropportunistic gastric cancer screening.</description><author>Mingze Yuan, Yingda Xia, Xin Chen, Jiawen Yao, Junli Wang, Mingyan Qiu, Hexin Dong, Jingren Zhou, Bin Dong, Le Lu, Li Zhang, Zaiyi Liu, Ling Zhang</author><pubDate>Mon, 10 Jul 2023 13:49:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04525v1</guid></item><item><title>EffLiFe: Efficient Light Field Generation via Hierarchical Sparse Gradient Descent</title><link>http://arxiv.org/abs/2307.03017v2</link><description>With the rise of Extended Reality (XR) technology, there is a growing needfor real-time light field generation from sparse view inputs. Existing methodscan be classified into offline techniques, which can generate high-qualitynovel views but at the cost of long inference/training time, and onlinemethods, which either lack generalizability or produce unsatisfactory results.However, we have observed that the intrinsic sparse manifold of Multi-planeImages (MPI) enables a significant acceleration of light field generation whilemaintaining rendering quality. Based on this insight, we introduce EffLiFe, anovel light field optimization method, which leverages the proposedHierarchical Sparse Gradient Descent (HSGD) to produce high-quality lightfields from sparse view images in real time. Technically, the coarse MPI of ascene is first generated using a 3D CNN, and it is further sparsely optimizedby focusing only on important MPI gradients in a few iterations. Nevertheless,relying solely on optimization can lead to artifacts at occlusion boundaries.Therefore, we propose an occlusion-aware iterative refinement module thatremoves visual artifacts in occluded regions by iteratively filtering theinput. Extensive experiments demonstrate that our method achieves comparablevisual quality while being 100x faster on average than state-of-the-art offlinemethods and delivering better performance (about 2 dB higher in PSNR) comparedto other online approaches.</description><author>Yijie Deng, Lei Han, Tianpeng Lin, Lin Li, Jinzhi Zhang, Lu Fang</author><pubDate>Mon, 10 Jul 2023 13:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03017v2</guid></item><item><title>Efficient Match Pair Retrieval for Large-scale UAV Images via Graph Indexed Global Descriptor</title><link>http://arxiv.org/abs/2307.04520v1</link><description>SfM (Structure from Motion) has been extensively used for UAV (UnmannedAerial Vehicle) image orientation. Its efficiency is directly influenced byfeature matching. Although image retrieval has been extensively used for matchpair selection, high computational costs are consumed due to a large number oflocal features and the large size of the used codebook. Thus, this paperproposes an efficient match pair retrieval method and implements an integratedworkflow for parallel SfM reconstruction. First, an individual codebook istrained online by considering the redundancy of UAV images and local features,which avoids the ambiguity of training codebooks from other datasets. Second,local features of each image are aggregated into a single high-dimension globaldescriptor through the VLAD (Vector of Locally Aggregated Descriptors)aggregation by using the trained codebook, which remarkably reduces the numberof features and the burden of nearest neighbor searching in image indexing.Third, the global descriptors are indexed via the HNSW (Hierarchical NavigableSmall World) based graph structure for the nearest neighbor searching. Matchpairs are then retrieved by using an adaptive threshold selection strategy andutilized to create a view graph for divide-and-conquer based parallel SfMreconstruction. Finally, the performance of the proposed solution has beenverified using three large-scale UAV datasets. The test results demonstratethat the proposed solution accelerates match pair retrieval with a speedupratio ranging from 36 to 108 and improves the efficiency of SfM reconstructionwith competitive accuracy in both relative and absolute orientation.</description><author>San Jiang, Yichen Ma, Qingquan Li, Wanshou Jiang, Bingxuan Guo, Lelin Li, Lizhe Wang</author><pubDate>Mon, 10 Jul 2023 13:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04520v1</guid></item><item><title>NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects</title><link>http://arxiv.org/abs/2303.07308v2</link><description>We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, andillustrate how it supports object SLAM for consistent spatial understandingwith long-term scene changes. NeuSE is a set of latent object embeddingscreated from partial object observations. It serves as a compact point cloudsurrogate for complete object models, encoding full shape information whiletransforming SE(3)-equivariantly in tandem with the object in the physicalworld. With NeuSE, relative frame transforms can be directly derived frominferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shapeand pose characterization, can operate independently or in conjunction withtypical SLAM systems. It directly infers SE(3) camera pose constraints that arecompatible with general SLAM pose graph optimization, while also maintaining alightweight object-centric map that adapts to real-world changes. Our approachis evaluated on synthetic and real-world sequences featuring changed objectsand shows improved localization accuracy and change-aware mapping capability,when working either standalone or jointly with a common SLAM pipeline.</description><author>Jiahui Fu, Yilun Du, Kurran Singh, Joshua B. Tenenbaum, John J. Leonard</author><pubDate>Mon, 10 Jul 2023 13:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07308v2</guid></item><item><title>AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset</title><link>http://arxiv.org/abs/2306.00612v2</link><description>It is a long-term vision for Autonomous Driving (AD) community that theperception models can learn from a large-scale point cloud dataset, to obtainunified representations that can achieve promising results on different tasksor benchmarks. Previous works mainly focus on the self-supervised pre-trainingpipeline, meaning that they perform the pre-training and fine-tuning on thesame benchmark, which is difficult to attain the performance scalability andcross-dataset application for the pre-training checkpoint. In this paper, forthe first time, we are committed to building a large-scale pre-trainingpoint-cloud dataset with diverse data distribution, and meanwhile learninggeneralizable representations from such a diverse pre-training dataset. Weformulate the point-cloud pre-training task as a semi-supervised problem, whichleverages the few-shot labeled and massive unlabeled point-cloud data togenerate the unified backbone representations that can be directly applied tomany baseline models and benchmarks, decoupling the AD-related pre-trainingprocess and downstream fine-tuning task. During the period of backbonepre-training, by enhancing the scene- and instance-level distribution diversityand exploiting the backbone's ability to learn from unknown instances, weachieve significant performance gains on a series of downstream perceptionbenchmarks including Waymo, nuScenes, and KITTI, under different baselinemodels like PV-RCNN++, SECOND, CenterPoint.</description><author>Jiakang Yuan, Bo Zhang, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, Yu Qiao</author><pubDate>Mon, 10 Jul 2023 13:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00612v2</guid></item><item><title>On the Computational Modeling of Meaning: Embodied Cognition Intertwined with Emotion</title><link>http://arxiv.org/abs/2307.04518v1</link><description>This document chronicles this author's attempt to explore how words come tomean what they do, with a particular focus on child language acquisition andwhat that means for models of language understanding.\footnote{I say\emph{historical} because I synthesize the ideas based on when I discoveredthem and how those ideas influenced my later thinking.} I explain the settingfor child language learning, how embodiment -- being able to perceive and enactin the world, including knowledge of concrete and abstract concepts -- iscrucial, and how emotion and cognition relate to each other and the languagelearning process. I end with what I think are some of the requirements for alanguage-learning agent that learns language in a setting similar to that ofchildren. This paper can act as a potential guide for ongoing and future workin modeling language.</description><author>Casey Kennington</author><pubDate>Mon, 10 Jul 2023 13:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04518v1</guid></item><item><title>Hyperspectral Image Super-Resolution via Dual-domain Network Based on Hybrid Convolution</title><link>http://arxiv.org/abs/2304.04589v8</link><description>Since the number of incident energies is limited, it is difficult to directlyacquire hyperspectral images (HSI) with high spatial resolution. Consideringthe high dimensionality and correlation of HSI, super-resolution (SR) of HSIremains a challenge in the absence of auxiliary high-resolution images.Furthermore, it is very important to extract the spatial features effectivelyand make full use of the spectral information. This paper proposes a novel HSIsuper-resolution algorithm, termed dual-domain network based on hybridconvolution (SRDNet). Specifically, a dual-domain network is designed to fullyexploit the spatial-spectral and frequency information among the hyper-spectraldata. To capture inter-spectral self-similarity, a self-attention learningmechanism (HSL) is devised in the spatial domain. Meanwhile the pyramidstructure is applied to increase the acceptance field of attention, whichfurther reinforces the feature representation ability of the network. Moreover,to further improve the perceptual quality of HSI, a frequency loss(HFL) isintroduced to optimize the model in the frequency domain. The dynamic weightingmechanism drives the network to gradually refine the generated frequency andexcessive smoothing caused by spatial loss. Finally, In order to better fullyobtain the mapping relationship between high-resolution space andlow-resolution space, a hybrid module of 2D and 3D units with progressiveupsampling strategy is utilized in our method. Experiments on a widely usedbenchmark dataset illustrate that the proposed SRDNet method enhances thetexture information of HSI and is superior to state-of-the-art methods.</description><author>Tingting Liu, Yuan Liu, Chuncheng Zhang, Yuan Liyin, Xiubao Sui, Qian Chen</author><pubDate>Mon, 10 Jul 2023 13:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04589v8</guid></item><item><title>An Examination of Wearable Sensors and Video Data Capture for Human Exercise Classification</title><link>http://arxiv.org/abs/2307.04516v1</link><description>Wearable sensors such as Inertial Measurement Units (IMUs) are often used toassess the performance of human exercise. Common approaches use handcraftedfeatures based on domain expertise or automatically extracted features usingtime series analysis. Multiple sensors are required to achieve highclassification accuracy, which is not very practical. These sensors requirecalibration and synchronization and may lead to discomfort over longer timeperiods. Recent work utilizing computer vision techniques has shown similarperformance using video, without the need for manual feature engineering, andavoiding some pitfalls such as sensor calibration and placement on the body. Inthis paper, we compare the performance of IMUs to a video-based approach forhuman exercise classification on two real-world datasets consisting of MilitaryPress and Rowing exercises. We compare the performance using a single camerathat captures video in the frontal view versus using 5 IMUs placed on differentparts of the body. We observe that an approach based on a single camera canoutperform a single IMU by 10 percentage points on average. Additionally, aminimum of 3 IMUs are required to outperform a single camera. We observe thatworking with the raw data using multivariate time series classifiersoutperforms traditional approaches based on handcrafted or automaticallyextracted features. Finally, we show that an ensemble model combining the datafrom a single camera with a single IMU outperforms either data modality. Ourwork opens up new and more realistic avenues for this application, where avideo captured using a readily available smartphone camera, combined with asingle sensor, can be used for effective human exercise classification.</description><author>Ashish Singh, Antonio Bevilacqua, Timilehin B. Aderinola, Thach Le Nguyen, Darragh Whelan, Martin O'Reilly, Brian Caulfield, Georgiana Ifrim</author><pubDate>Mon, 10 Jul 2023 13:24:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04516v1</guid></item><item><title>SAGC-A68: a space access graph dataset for the classification of spaces and space elements in apartment buildings</title><link>http://arxiv.org/abs/2307.04515v1</link><description>The analysis of building models for usable area, building safety, and energyuse requires accurate classification data of spaces and space elements. Toreduce input model preparation effort and errors, automated classification ofspaces and space elements is desirable. A barrier hindering the utilization ofGraph Deep Learning (GDL) methods to space function and space elementclassification is a lack of suitable datasets. To bridge this gap, we introducea dataset, SAGC-A68, which comprises access graphs automatically generated from68 digital 3D models of space layouts of apartment buildings. This graph-baseddataset is well-suited for developing GDL models for space function and spaceelement classification. To demonstrate the potential of the dataset, we employit to train and evaluate a graph attention network (GAT) that predicts 22 spacefunction and 6 space element classes. The dataset and code used in theexperiment are available online. https://doi.org/10.5281/zenodo.7805872,https://github.com/A2Amir/SAGC-A68.</description><author>Amir Ziaee, Georg Suter</author><pubDate>Mon, 10 Jul 2023 13:22:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04515v1</guid></item><item><title>Improving Heterogeneous Graph Learning with Weighted Mixed-Curvature Product Manifold</title><link>http://arxiv.org/abs/2307.04514v1</link><description>In graph representation learning, it is important that the complex geometricstructure of the input graph, e.g. hidden relations among nodes, is wellcaptured in embedding space. However, standard Euclidean embedding spaces havea limited capacity in representing graphs of varying structures. A promisingcandidate for the faithful embedding of data with varying structure is productmanifolds of component spaces of different geometries (spherical, hyperbolic,or euclidean). In this paper, we take a closer look at the structure of productmanifold embedding spaces and argue that each component space in a productcontributes differently to expressing structures in the input graph, henceshould be weighted accordingly. This is different from previous works whichconsider the roles of different components equally. We then proposeWEIGHTED-PM, a data-driven method for learning embedding of heterogeneousgraphs in weighted product manifolds. Our method utilizes the topologicalinformation of the input graph to automatically determine the weight of eachcomponent in product spaces. Extensive experiments on synthetic and real-worldgraph datasets demonstrate that WEIGHTED-PM is capable of learning better graphrepresentations with lower geometric distortion from input data, and performsbetter on multiple downstream tasks, such as word similarity learning, top-$k$recommendation, and knowledge graph embedding.</description><author>Tuc Nguyen-Van, Dung D. Le, The-Anh Ta</author><pubDate>Mon, 10 Jul 2023 13:20:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04514v1</guid></item><item><title>CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation</title><link>http://arxiv.org/abs/2307.04513v1</link><description>New lesion segmentation is essential to estimate the disease progression andtherapeutic effects during multiple sclerosis (MS) clinical treatments.However, the expensive data acquisition and expert annotation restrict thefeasibility of applying large-scale deep learning models. Sincesingle-time-point samples with all-lesion labels are relatively easy tocollect, exploiting them to train deep models is highly desirable to improvenew lesion segmentation. Therefore, we proposed a coaction segmentation(CoactSeg) framework to exploit the heterogeneous data (i.e., new-lesionannotated two-time-point data and all-lesion annotated single-time-point data)for new MS lesion segmentation. The CoactSeg model is designed as a unifiedmodel, with the same three inputs (the baseline, follow-up, and theirlongitudinal brain differences) and the same three outputs (the correspondingall-lesion and new-lesion predictions), no matter which type of heterogeneousdata is being used. Moreover, a simple and effective relation regularization isproposed to ensure the longitudinal relations among the three outputs toimprove the model learning. Extensive experiments demonstrate that utilizingthe heterogeneous data and the proposed longitudinal relation constraint cansignificantly improve the performance for both new-lesion and all-lesionsegmentation tasks. Meanwhile, we also introduce an in-house MS-23v1 dataset,including 38 Oceania single-time-point samples with all-lesion labels. Codesand the dataset are released at https://github.com/ycwu1997/CoactSeg.</description><author>Yicheng Wu, Zhonghua Wu, Hengcan Shi, Bjoern Picker, Winston Chong, Jianfei Cai</author><pubDate>Mon, 10 Jul 2023 13:20:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04513v1</guid></item></channel></rss>