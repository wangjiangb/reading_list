<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 02 Oct 2024 13:00:06 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Enhancing GANs with Contrastive Learning-Based Multistage Progressive Finetuning SNN and RL-Based External Optimization</title><link>http://arxiv.org/abs/2409.20340v2</link><description>The application of deep learning in cancer research, particularly in earlydiagnosis, case understanding, and treatment strategy design, emphasizes theneed for high-quality data. Generative AI, especially Generative AdversarialNetworks (GANs), has emerged as a leading solution to challenges like classimbalance, robust learning, and model training, while addressing issuesstemming from patient privacy and the scarcity of real data. Despite theirpromise, GANs face several challenges, both inherent and specific tohistopathology data. Inherent issues include training imbalance, mode collapse,linear learning from insufficient discriminator feedback, and hard boundaryconvergence due to stringent feedback. Histopathology data presents a uniquechallenge with its complex representation, high spatial resolution, andmultiscale features. To address these challenges, we propose a frameworkconsisting of two components. First, we introduce a contrastive learning-basedMultistage Progressive Finetuning Siamese Neural Network (MFT-SNN) forassessing the similarity between histopathology patches. Second, we implement aReinforcement Learning-based External Optimizer (RL-EO) within the GAN trainingloop, serving as a reward signal generator. The modified discriminator lossfunction incorporates a weighted reward, guiding the GAN to maximize thisreward while minimizing loss. This approach offers an external optimizationguide to the discriminator, preventing generator overfitting and ensuringsmooth convergence. Our proposed solution has been benchmarked againststate-of-the-art (SOTA) GANs and a Denoising Diffusion Probabilistic model,outperforming previous SOTA across various metrics, including FID score, KIDscore, Perceptual Path Length, and downstream classification tasks.</description><author>Osama Mustafa</author><pubDate>Tue, 01 Oct 2024 14:14:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20340v2</guid></item><item><title>Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity</title><link>http://arxiv.org/abs/2409.18708v3</link><description>We introduce a novel family of adversarial attacks that exploit the inabilityof language models to interpret ASCII art. To evaluate these attacks, wepropose the ToxASCII benchmark and develop two custom ASCII art fonts: oneleveraging special tokens and another using text-filled letter shapes. Ourattacks achieve a perfect 1.0 Attack Success Rate across ten models, includingOpenAI's o1-preview and LLaMA 3.1. Warning: this paper contains examples of toxic language used for researchpurposes.</description><author>Sergey Berezin, Reza Farahbakhsh, Noel Crespi</author><pubDate>Tue, 01 Oct 2024 08:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18708v3</guid></item><item><title>Instance-adaptive Zero-shot Chain-of-Thought Prompting</title><link>http://arxiv.org/abs/2409.20441v2</link><description>Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effectivestrategy for enhancing the performance of large language models (LLMs) inreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-levelprompt uniformly applied across the whole of instances is inherently limitedsince one prompt cannot be a good partner for all, a more appropriate approachshould consider the interaction between the prompt and each instancemeticulously. This work introduces an instance-adaptive prompting algorithm asan alternative zero-shot CoT reasoning scheme by adaptively differentiatinggood and bad prompts. Concretely, we first employ analysis on LLMs through thelens of information flow to detect the mechanism under zero-shot CoT reasoning,in which we discover that information flows from question to prompt andquestion to rationale jointly influence the reasoning results most. We noticethat a better zero-shot CoT reasoning needs the prompt to obtain semanticinformation from the question then the rationale aggregates sufficientinformation from the question directly and via the prompt indirectly. On thecontrary, lacking any of those would probably lead to a bad one. Stem fromthat, we further propose an instance-adaptive prompting strategy (IAP) forzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwenon math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, CausalJudgement) obtain consistent improvement, demonstrating that theinstance-adaptive zero-shot CoT prompting performs better than other task-levelmethods with some curated prompts or sophisticated procedures, showing thesignificance of our findings in the zero-shot CoT reasoning mechanism.</description><author>Xiaosong Yuan, Chen Shen, Shaotian Yan, Xiaofeng Zhang, Liang Xie, Wenxiao Wang, Renchu Guan, Ying Wang, Jieping Ye</author><pubDate>Tue, 01 Oct 2024 06:03:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20441v2</guid></item><item><title>Continuously Improving Mobile Manipulation with Autonomous Real-World RL</title><link>http://arxiv.org/abs/2409.20568v1</link><description>We present a fully autonomous real-world RL framework for mobile manipulationthat can learn policies without extensive instrumentation or human supervision.This is enabled by 1) task-relevant autonomy, which guides exploration towardsobject interactions and prevents stagnation near goal states, 2) efficientpolicy learning by leveraging basic task knowledge in behavior priors, and 3)formulating generic rewards that combine human-interpretable semanticinformation with low-level, fine-grained observations. We demonstrate that ourapproach allows Spot robots to continually improve their performance on a setof four challenging mobile manipulation tasks, obtaining an average successrate of 80% across tasks, a 3-4 improvement over existing approaches. Videoscan be found at https://continual-mobile-manip.github.io/</description><author>Russell Mendonca, Emmanuel Panov, Bernadette Bucher, Jiuguang Wang, Deepak Pathak</author><pubDate>Mon, 30 Sep 2024 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20568v1</guid></item><item><title>MM1.5: Methods, Analysis &amp; Insights from Multimodal LLM Fine-tuning</title><link>http://arxiv.org/abs/2409.20566v1</link><description>We present MM1.5, a new family of multimodal large language models (MLLMs)designed to enhance capabilities in text-rich image understanding, visualreferring and grounding, and multi-image reasoning. Building upon the MM1architecture, MM1.5 adopts a data-centric approach to model training,systematically exploring the impact of diverse data mixtures across the entiremodel training lifecycle. This includes high-quality OCR data and syntheticcaptions for continual pre-training, as well as an optimized visualinstruction-tuning data mixture for supervised fine-tuning. Our models rangefrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)variants, and demonstrate that careful data curation and training strategiescan yield strong performance even at small scales (1B and 3B). Additionally, weintroduce two specialized variants: MM1.5-Video, designed for videounderstanding, and MM1.5-UI, tailored for mobile UI understanding. Throughextensive empirical studies and ablations, we provide detailed insights intothe training processes and decisions that inform our final designs, offeringvaluable guidance for future research in MLLM development.</description><author>Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev, Mingze Xu, Hong-You Chen, Jean-Philippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, Afshin Dehghan, Peter Grasch, Yinfei Yang</author><pubDate>Mon, 30 Sep 2024 17:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20566v1</guid></item><item><title>Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments</title><link>http://arxiv.org/abs/2409.20565v1</link><description>Evaluating LLM-generated text has become a key challenge, especially indomain-specific contexts like the medical field. This work introduces a novelevaluation methodology for LLM-generated medical explanatory arguments, relyingon Proxy Tasks and rankings to closely align results with human evaluationcriteria, overcoming the biases typically seen in LLMs used as judges. Wedemonstrate that the proposed evaluators are robust against adversarialattacks, including the assessment of non-argumentative text. Additionally, thehuman-crafted arguments needed to train the evaluators are minimized to justone example per Proxy Task. By examining multiple LLM-generated arguments, weestablish a methodology for determining whether a Proxy Task is suitable forevaluating LLM-generated medical explanatory arguments, requiring only fiveexamples and two human experts.</description><author>Iker De la Iglesia, Iakes Goenaga, Johanna Ramirez-Romero, Jose Maria Villa-Gonzalez, Josu Goikoetxea, Ander Barrena</author><pubDate>Mon, 30 Sep 2024 17:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20565v1</guid></item><item><title>DressRecon: Freeform 4D Human Reconstruction from Monocular Video</title><link>http://arxiv.org/abs/2409.20563v1</link><description>We present a method to reconstruct time-consistent human body models frommonocular videos, focusing on extremely loose clothing or handheld objectinteractions. Prior work in human reconstruction is either limited to tightclothing with no object interactions, or requires calibrated multi-viewcaptures or personalized template scans which are costly to collect at scale.Our key insight for high-quality yet flexible reconstruction is the carefulcombination of generic human priors about articulated body shape (learned fromlarge-scale training data) with video-specific articulated "bag-of-bones"deformation (fit to a single video via test-time optimization). We accomplishthis by learning a neural implicit model that disentangles body versus clothingdeformations as separate motion model layers. To capture subtle geometry ofclothing, we leverage image-based priors such as human body pose, surfacenormals, and optical flow during optimization. The resulting neural fields canbe extracted into time-consistent meshes, or further optimized as explicit 3DGaussians for high-fidelity interactive rendering. On datasets with highlychallenging clothing deformations and object interactions, DressRecon yieldshigher-fidelity 3D reconstructions than prior art. Project page:https://jefftan969.github.io/dressrecon/</description><author>Jeff Tan, Donglai Xiang, Shubham Tulsiani, Deva Ramanan, Gengshan Yang</author><pubDate>Mon, 30 Sep 2024 17:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20563v1</guid></item><item><title>SpaceMesh: A Continuous Representation for Learning Manifold Surface Meshes</title><link>http://arxiv.org/abs/2409.20562v1</link><description>Meshes are ubiquitous in visual computing and simulation, yet most existingmachine learning techniques represent meshes only indirectly, e.g. as the levelset of a scalar field or deformation of a template, or as a disordered trianglesoup lacking local structure. This work presents a scheme to directly generatemanifold, polygonal meshes of complex connectivity as the output of a neuralnetwork. Our key innovation is to define a continuous latent connectivity spaceat each mesh vertex, which implies the discrete mesh. In particular, our vertexembeddings generate cyclic neighbor relationships in a halfedge meshrepresentation, which gives a guarantee of edge-manifoldness and the ability torepresent general polygonal meshes. This representation is well-suited tomachine learning and stochastic optimization, without restriction onconnectivity or topology. We first explore the basic properties of thisrepresentation, then use it to fit distributions of meshes from large datasets.The resulting models generate diverse meshes with tessellation structurelearned from the dataset population, with concise details and high-quality meshelements. In applications, this approach not only yields high-quality outputsfrom generative models, but also enables directly learning challenging geometryprocessing tasks such as mesh repair.</description><author>Tianchang Shen, Zhaoshuo Li, Marc Law, Matan Atzmon, Sanja Fidler, James Lucas, Jun Gao, Nicholas Sharp</author><pubDate>Mon, 30 Sep 2024 17:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20562v1</guid></item><item><title>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner</title><link>http://arxiv.org/abs/2409.20560v1</link><description>Language models (LMs) possess a strong capability to comprehend naturallanguage, making them effective in translating human instructions into detailedplans for simple robot tasks. Nevertheless, it remains a significant challengeto handle long-horizon tasks, especially in subtask identification andallocation for cooperative heterogeneous robot teams. To address this issue, wepropose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novelmulti-agent task planning framework that achieves state-of-the-art performanceon long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoningcapability and the traditional heuristic search planner to achieve a highsuccess rate and efficiency while demonstrating strong generalization acrosstasks. Additionally, we create MAT-THOR, a comprehensive benchmark thatfeatures household tasks with two different levels of complexity based on theAI2-THOR environment. The experimental results demonstrate that LaMMA-Pachieves a 105% higher success rate and 36% higher efficiency than existingLM-based multi-agent planners. The experimental videos, code, and datasets ofthis work as well as the detailed prompts used in each module are available athttps://lamma-p.github.io.</description><author>Xiaopan Zhang, Hao Qin, Fuquan Wang, Yue Dong, Jiachen Li</author><pubDate>Mon, 30 Sep 2024 17:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20560v1</guid></item><item><title>Supervised Multi-Modal Fission Learning</title><link>http://arxiv.org/abs/2409.20559v1</link><description>Learning from multimodal datasets can leverage complementary information andimprove performance in prediction tasks. A commonly used strategy to accountfor feature correlations in high-dimensional datasets is the latent variableapproach. Several latent variable methods have been proposed for multimodaldatasets. However, these methods either focus on extracting the sharedcomponent across all modalities or on extracting both a shared component andindividual components specific to each modality. To address this gap, wepropose a Multi-Modal Fission Learning (MMFL) model that simultaneouslyidentifies globally joint, partially joint, and individual componentsunderlying the features of multimodal datasets. Unlike existing latent variablemethods, MMFL uses supervision from the response variable to identifypredictive latent components and has a natural extension for incorporatingincomplete multimodal data. Through simulation studies, we demonstrate thatMMFL outperforms various existing multimodal algorithms in both complete andincomplete modality settings. We applied MMFL to a real-world case study forearly prediction of Alzheimers Disease using multimodal neuroimaging andgenomics data from the Alzheimers Disease Neuroimaging Initiative (ADNI)dataset. MMFL provided more accurate predictions and better insights intowithin- and across-modality correlations compared to existing methods.</description><author>Lingchao Mao, Qi wang, Yi Su, Fleming Lure, Jing Li</author><pubDate>Mon, 30 Sep 2024 17:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20559v1</guid></item><item><title>Uni$^2$Det: Unified and Universal Framework for Prompt-Guided Multi-dataset 3D Detection</title><link>http://arxiv.org/abs/2409.20558v1</link><description>We present Uni$^2$Det, a brand new framework for unified and universalmulti-dataset training on 3D detection, enabling robust performance acrossdiverse domains and generalization to unseen domains. Due to substantialdisparities in data distribution and variations in taxonomy across diversedomains, training such a detector by simply merging datasets poses asignificant challenge. Motivated by this observation, we introduce multi-stageprompting modules for multi-dataset 3D detection, which leverages prompts basedon the characteristics of corresponding datasets to mitigate existingdifferences. This elegant design facilitates seamless plug-and-play integrationwithin various advanced 3D detection frameworks in a unified manner, while alsoallowing straightforward adaptation for universal applicability acrossdatasets. Experiments are conducted across multiple dataset consolidationscenarios involving KITTI, Waymo, and nuScenes, demonstrating that ourUni$^2$Det outperforms existing methods by a large margin in multi-datasettraining. Notably, results on zero-shot cross-dataset transfer validate thegeneralization capability of our proposed method.</description><author>Yubin Wang, Zhikang Zou, Xiaoqing Ye, Xiao Tan, Errui Ding, Cairong Zhao</author><pubDate>Mon, 30 Sep 2024 17:57:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20558v1</guid></item><item><title>Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in Instructional Videos</title><link>http://arxiv.org/abs/2409.20557v1</link><description>Goal-oriented planning, or anticipating a series of actions that transitionan agent from its current state to a predefined objective, is crucial fordeveloping intelligent assistants aiding users in daily procedural tasks. Theproblem presents significant challenges due to the need for comprehensiveknowledge of temporal and hierarchical task structures, as well as strongcapabilities in reasoning and planning. To achieve this, prior work typicallyrelies on extensive training on the target dataset, which often results insignificant dataset bias and a lack of generalization to unseen tasks. In thiswork, we introduce VidAssist, an integrated framework designed forzero/few-shot goal-oriented planning in instructional videos. VidAssistleverages large language models (LLMs) as both the knowledge base and theassessment tool for generating and evaluating action plans, thus overcoming thechallenges of acquiring procedural knowledge from small-scale, low-diversitydatasets. Moreover, VidAssist employs a breadth-first search algorithm foroptimal plan generation, in which a composite of value functions designed forgoal-oriented planning is utilized to assess the predicted actions at eachstep. Extensive experiments demonstrate that VidAssist offers a unifiedframework for different goal-oriented planning setups, e.g., visual planningfor assistance (VPA) and procedural planning (PP), and achieves remarkableperformance in zero-shot and few-shot setups. Specifically, our few-shot modeloutperforms the prior fully supervised state-of-the-art method by +7.7% in VPAand +4.81% PP task on the COIN dataset while predicting 4 future actions. Code,and models are publicly available at https://sites.google.com/view/vidassist.</description><author>Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Fu-Jen Chu, Kris Kitani, Gedas Bertasius, Xitong Yang</author><pubDate>Mon, 30 Sep 2024 17:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20557v1</guid></item><item><title>Inverse Painting: Reconstructing The Painting Process</title><link>http://arxiv.org/abs/2409.20556v1</link><description>Given an input painting, we reconstruct a time-lapse video of how it may havebeen painted. We formulate this as an autoregressive image generation problem,in which an initially blank "canvas" is iteratively updated. The model learnsfrom real artists by training on many painting videos. Our approachincorporates text and region understanding to define a set of painting"instructions" and updates the canvas with a novel diffusion-based renderer.The method extrapolates beyond the limited, acrylic style paintings on which ithas been trained, showing plausible results for a wide range of artistic stylesand genres.</description><author>Bowei Chen, Yifan Wang, Brian Curless, Ira Kemelmacher-Shlizerman, Steven M. Seitz</author><pubDate>Mon, 30 Sep 2024 17:56:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20556v1</guid></item><item><title>Maia-2: A Unified Model for Human-AI Alignment in Chess</title><link>http://arxiv.org/abs/2409.20553v1</link><description>There are an increasing number of domains in which artificial intelligence(AI) systems both surpass human ability and accurately model human behavior.This introduces the possibility of algorithmically-informed teaching in thesedomains through more relatable AI partners and deeper insights into humandecision-making. Critical to achieving this goal, however, is coherentlymodeling human behavior at various skill levels. Chess is an ideal model systemfor conducting research into this kind of human-AI alignment, with its richhistory as a pivotal testbed for AI research, mature superhuman AI systems likeAlphaZero, and precise measurements of skill via chess rating systems. Previouswork in modeling human decision-making in chess uses completely independentmodels to capture human style at different skill levels, meaning they lackcoherence in their ability to adapt to the full spectrum of human improvementand are ultimately limited in their effectiveness as AI partners and teachingtools. In this work, we propose a unified modeling approach for human-AIalignment in chess that coherently captures human style across different skilllevels and directly captures how people improve. Recognizing the complex,non-linear nature of human learning, we introduce a skill-aware attentionmechanism to dynamically integrate players' strengths with encoded chesspositions, enabling our model to be sensitive to evolving player skill. Ourexperimental results demonstrate that this unified framework significantlyenhances the alignment between AI and human players across a diverse range ofexpertise levels, paving the way for deeper insights into human decision-makingand AI-guided teaching tools.</description><author>Zhenwei Tang, Difan Jiao, Reid McIlroy-Young, Jon Kleinberg, Siddhartha Sen, Ashton Anderson</author><pubDate>Mon, 30 Sep 2024 17:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20553v1</guid></item><item><title>LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation</title><link>http://arxiv.org/abs/2409.20550v1</link><description>Code generation aims to automatically generate code from input requirements,significantly enhancing development efficiency. Recent large language models(LLMs) based approaches have shown promising results and revolutionized codegeneration task. Despite the promising performance, LLMs often generatecontents with hallucinations, especially for the code generation scenariorequiring the handling of complex contextual dependencies in practicaldevelopment process. Although previous study has analyzed hallucinations inLLM-powered code generation, the study is limited to standalone functiongeneration. In this paper, we conduct an empirical study to study thephenomena, mechanism, and mitigation of LLM hallucinations within morepractical and complex development contexts in repository-level generationscenario. First, we manually examine the code generation results from sixmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.Next, we elaborate on the phenomenon of hallucinations, analyze theirdistribution across different models. We then analyze causes of hallucinationsand identify four potential factors contributing to hallucinations. Finally, wepropose an RAG-based mitigation method, which demonstrates consistenteffectiveness in all studied LLMs. The replication package including code,data, and experimental results is available athttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination</description><author>Ziyao Zhang, Yanlin Wang, Chong Wang, Jiachi Chen, Zibin Zheng</author><pubDate>Mon, 30 Sep 2024 17:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20550v1</guid></item><item><title>The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance</title><link>http://arxiv.org/abs/2406.11634v2</link><description>Cloze testing is a common method for measuring the behavior of large languagemodels on a number of benchmark tasks. Using the MMLU dataset, we show that thebase-rate probability (BRP) differences across answer tokens are significantand affect task performance ie. guess A if uncertain. We find thatcounterfactual prompting does sufficiently mitigate the BRP effect. The BRPeffect is found to have a similar effect to test taking strategies employed byhumans leading to the conflation of task performance and test-taking ability.We propose the Nvr-X-MMLU task, a variation of MMLU, which helps todisambiguate test-taking ability from task performance and reports the latter.</description><author>Kyle Moore, Jesse Roberts, Thao Pham, Oseremhen Ewaleifoh, Doug Fisher</author><pubDate>Mon, 30 Sep 2024 17:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11634v2</guid></item><item><title>Robi Butler: Remote Multimodal Interactions with Household Robot Assistant</title><link>http://arxiv.org/abs/2409.20548v1</link><description>In this paper, we introduce Robi Butler, a novel household robotic systemthat enables multimodal interactions with remote users. Building on theadvanced communication interfaces, Robi Butler allows users to monitor therobot's status, send text or voice instructions, and select target objects byhand pointing. At the core of our system is a high-level behavior module,powered by Large Language Models (LLMs), that interprets multimodalinstructions to generate action plans. These plans are composed of a set ofopen vocabulary primitives supported by Vision Language Models (VLMs) thathandle both text and pointing queries. The integration of the above componentsallows Robi Butler to ground remote multimodal instructions in the real-worldhome environment in a zero-shot manner. We demonstrate the effectiveness andefficiency of this system using a variety of daily household tasks that involveremote users giving multimodal instructions. Additionally, we conducted a userstudy to analyze how multimodal interactions affect efficiency and userexperience during remote human-robot interaction and discuss the potentialimprovements.</description><author>Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun Yu, David Hsu</author><pubDate>Mon, 30 Sep 2024 17:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20548v1</guid></item><item><title>Annealing Flow Generative Model Towards Sampling High-Dimensional and Multi-Modal Distributions</title><link>http://arxiv.org/abs/2409.20547v1</link><description>Sampling from high-dimensional, multi-modal distributions remains afundamental challenge across domains such as statistical Bayesian inference andphysics-based machine learning. In this paper, we propose Annealing Flow (AF),a continuous normalizing flow-based approach designed to sample fromhigh-dimensional and multi-modal distributions. The key idea is to learn acontinuous normalizing flow-based transport map, guided by annealing, totransition samples from an easy-to-sample distribution to the targetdistribution, facilitating effective exploration of modes in high-dimensionalspaces. Unlike many existing methods, AF training does not rely on samples fromthe target distribution. AF ensures effective and balanced mode exploration,achieves linear complexity in sample size and dimensions, and circumventsinefficient mixing times. We demonstrate the superior performance of AFcompared to state-of-the-art methods through extensive experiments on variouschallenging distributions and real-world datasets, particularly inhigh-dimensional and multi-modal settings. We also highlight the potential ofAF for sampling the least favorable distributions.</description><author>Dongze Wu, Yao Xie</author><pubDate>Mon, 30 Sep 2024 17:48:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20547v1</guid></item><item><title>FABLES: Evaluating faithfulness and content selection in book-length summarization</title><link>http://arxiv.org/abs/2404.01261v2</link><description>While long-context large language models (LLMs) can technically summarizebook-length documents (&gt;100K tokens), the length and complexity of thedocuments have so far prohibited evaluations of input-dependent aspects likefaithfulness. In this paper, we conduct the first large-scale human evaluationof faithfulness and content selection on LLM-generated summaries of fictionalbooks. Our study mitigates the issue of data contamination by focusing onsummaries of books published in 2023 or 2024, and we hire annotators who havefully read each book prior to the annotation task to minimize cost andcognitive burden. We collect FABLES, a dataset of annotations on 3,158 claimsmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, whichallows us to rank LLM summarizers based on faithfulness: Claude-3-Opussignificantly outperforms all closed-source LLMs, while the open-source Mixtralis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that mostunfaithful claims relate to events and character states, and they generallyrequire indirect reasoning over the narrative to invalidate. While LLM-basedauto-raters have proven reliable for factuality and coherence in othersettings, we implement several LLM raters of faithfulness and find that nonecorrelates strongly with human annotations, especially with regard to detectingunfaithful claims. Our experiments suggest that detecting unfaithful claims isan important future direction not only for summarization evaluation but also asa testbed for long-context understanding. Finally, we move beyond faithfulnessby exploring content selection errors in book-length summarization: we developa typology of omission errors related to crucial narrative elements and alsoidentify a systematic over-emphasis on events occurring towards the end of thebook.</description><author>Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, Mohit Iyyer</author><pubDate>Mon, 30 Sep 2024 17:39:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01261v2</guid></item><item><title>Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers</title><link>http://arxiv.org/abs/2409.20537v1</link><description>One of the roadblocks for training generalist robotic models today isheterogeneity. Previous robot learning methods often collect data to train withone specific embodiment for one task, which is expensive and prone tooverfitting. This work studies the problem of learning policy representationsthrough heterogeneous pre-training on robot data across different embodimentsand tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT),which pre-train a large, shareable trunk of a policy neural network to learn atask and embodiment agnostic shared representation. This general architecturealigns the specific proprioception and vision inputs from distinct embodimentsto a short sequence of tokens and then processes such tokens to map to controlrobots for different tasks. Leveraging the recent large-scale multi-embodimentreal-world robotic datasets as well as simulation, deployed robots, and humanvideo datasets, we investigate pre-training policies across heterogeneity. Weconduct experiments to investigate the scaling behaviors of trainingobjectives, to the extent of 52 datasets. HPTs outperform several baselines andenhance the fine-tuned policy performance by over 20% on unseen tasks inmultiple simulator benchmarks and real-world settings. See the project website(https://liruiw.github.io/hpt/) for code and videos.</description><author>Lirui Wang, Xinlei Chen, Jialiang Zhao, Kaiming He</author><pubDate>Mon, 30 Sep 2024 17:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20537v1</guid></item><item><title>Best Practices for Responsible Machine Learning in Credit Scoring</title><link>http://arxiv.org/abs/2409.20536v1</link><description>The widespread use of machine learning in credit scoring has broughtsignificant advancements in risk assessment and decision-making. However, ithas also raised concerns about potential biases, discrimination, and lack oftransparency in these automated systems. This tutorial paper performed anon-systematic literature review to guide best practices for developingresponsible machine learning models in credit scoring, focusing on fairness,reject inference, and explainability. We discuss definitions, metrics, andtechniques for mitigating biases and ensuring equitable outcomes acrossdifferent groups. Additionally, we address the issue of limited datarepresentativeness by exploring reject inference methods that incorporateinformation from rejected loan applications. Finally, we emphasize theimportance of transparency and explainability in credit models, discussingtechniques that provide insights into the decision-making process and enableindividuals to understand and potentially improve their creditworthiness. Byadopting these best practices, financial institutions can harness the power ofmachine learning while upholding ethical and responsible lending practices.</description><author>Giovani Valdrighi, Athyrson M. Ribeiro, Jansen S. B. Pereira, Vitoria Guardieiro, Arthur Hendricks, Décio Miranda Filho, Juan David Nieto Garcia, Felipe F. Bocca, Thalita B. Veronese, Lucas Wanner, Marcos Medeiros Raimundo</author><pubDate>Mon, 30 Sep 2024 17:39:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20536v1</guid></item><item><title>End-to-End Conformal Calibration for Optimization Under Uncertainty</title><link>http://arxiv.org/abs/2409.20534v1</link><description>Machine learning can significantly improve performance for decision-makingunder uncertainty in a wide range of domains. However, ensuring robustnessguarantees requires well-calibrated uncertainty estimates, which can bedifficult to achieve in high-capacity prediction models such as deep neuralnetworks. Moreover, in high-dimensional settings, there may be many validuncertainty estimates, each with their own performance profile - i.e., not alluncertainty is equally valuable for downstream decision-making. To address thisproblem, this paper develops an end-to-end framework to learn the uncertaintyestimates for conditional robust optimization, with robustness and calibrationguarantees provided by conformal prediction. In addition, we propose torepresent arbitrary convex uncertainty sets with partially input-convex neuralnetworks, which are learned as part of our framework. Our approach consistentlyimproves upon two-stage estimate-then-optimize baselines on concreteapplications in energy storage arbitrage and portfolio optimization.</description><author>Christopher Yeh, Nicolas Christianson, Alan Wu, Adam Wierman, Yisong Yue</author><pubDate>Mon, 30 Sep 2024 17:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20534v1</guid></item><item><title>On the Resilience of Multi-Agent Systems with Malicious Agents</title><link>http://arxiv.org/abs/2408.00989v2</link><description>Multi-agent systems, powered by large language models, have shown greatabilities across various tasks due to the collaboration of expert agents, eachfocusing on a specific domain. However, when agents are deployed separately,there is a risk that malicious users may introduce malicious agents whogenerate incorrect or irrelevant results that are too stealthy to be identifiedby other non-specialized agents. Therefore, this paper investigates twoessential questions: (1) What is the resilience of various multi-agent systemstructures (e.g., A$\rightarrow$B$\rightarrow$C,A$\leftrightarrow$B$\leftrightarrow$C) under malicious agents, on differentdownstream tasks? (2) How can we increase system resilience to defend againstmalicious agents? To simulate malicious agents, we devise two methods,AutoTransform and AutoInject, to transform any agent into a malicious one whilepreserving its functional integrity. We run comprehensive experiments on fourdownstream multi-agent systems tasks, namely code generation, math problems,translation, and text evaluation. Results suggest that the "hierarchical"multi-agent structure, i.e., A$\rightarrow$(B$\leftrightarrow$C), exhibitssuperior resilience with the lowest performance drop of $23.6\%$, compared to$46.4\%$ and $49.8\%$ of other two structures. Additionally, we show thepromise of improving multi-agent system resilience by demonstrating that twodefense methods, introducing a mechanism for each agent to challenge others'outputs, or an additional agent to review and correct messages, can enhancesystem resilience. Our code and data are available athttps://github.com/CUHK-ARISE/MAS-Resilience.</description><author>Jen-tse Huang, Jiaxu Zhou, Tailin Jin, Xuhui Zhou, Zixi Chen, Wenxuan Wang, Youliang Yuan, Maarten Sap, Michael R. Lyu</author><pubDate>Mon, 30 Sep 2024 17:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00989v2</guid></item><item><title>Can Large Language Models Address Open-Target Stance Detection?</title><link>http://arxiv.org/abs/2409.00222v4</link><description>Stance detection (SD) identifies a text's position towards a target,typically labeled as favor, against, or none. We introduce Open-Target StanceDetection (OTSD), the most realistic task where targets are neither seen duringtraining nor provided as input. We evaluate Large Language Models (LLMs)GPT-4o, GPT-3.5, Llama-3, and Mistral, comparing their performance to the onlyexisting work, Target-Stance Extraction (TSE), which benefits from predefinedtargets. Unlike TSE, OTSD removes the dependency of a predefined list, makingtarget generation and evaluation more challenging. We also provide a metric forevaluating target quality that correlates well with human judgment. Ourexperiments reveal that LLMs outperform TSE in target generation when the realtarget is explicitly and not explicitly mentioned in the text. Likewise, forstance detection, LLMs excel in explicit cases with comparable performance innon-explicit in general.</description><author>Abu Ubaida Akash, Ahmed Fahmy, Amine Trabelsi</author><pubDate>Mon, 30 Sep 2024 17:37:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00222v4</guid></item><item><title>Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images</title><link>http://arxiv.org/abs/2409.20530v1</link><description>3D GAN inversion aims to project a single image into the latent space of a 3DGenerative Adversarial Network (GAN), thereby achieving 3D geometryreconstruction. While there exist encoders that achieve good results in 3D GANinversion, they are predominantly built on EG3D, which specializes insynthesizing near-frontal views and is limiting in synthesizing comprehensive3D scenes from diverse viewpoints. In contrast to existing approaches, wepropose a novel framework built on PanoHead, which excels in synthesizingimages from a 360-degree perspective. To achieve realistic 3D modeling of theinput image, we introduce a dual encoder system tailored for high-fidelityreconstruction and realistic generation from different viewpoints. Accompanyingthis, we propose a stitching framework on the triplane domain to get the bestpredictions from both. To achieve seamless stitching, both encoders must outputconsistent results despite being specialized for different tasks. For thisreason, we carefully train these encoders using specialized losses, includingan adversarial loss based on our novel occlusion-aware triplane discriminator.Experiments reveal that our approach surpasses the existing encoder trainingmethods qualitatively and quantitatively. Please visit the project page:https://berkegokmen1.github.io/dual-enc-3d-gan-inv.</description><author>Bahri Batuhan Bilecen, Ahmet Berke Gokmen, Aysegul Dundar</author><pubDate>Mon, 30 Sep 2024 17:30:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20530v1</guid></item><item><title>Formally Verified Physics-Informed Neural Control Lyapunov Functions</title><link>http://arxiv.org/abs/2409.20528v1</link><description>Control Lyapunov functions are a central tool in the design and analysis ofstabilizing controllers for nonlinear systems. Constructing such functions,however, remains a significant challenge. In this paper, we investigatephysics-informed learning and formal verification of neural network controlLyapunov functions. These neural networks solve a transformedHamilton-Jacobi-Bellman equation, augmented by data generated usingPontryagin's maximum principle. Similar to how Zubov's equation characterizesthe domain of attraction for autonomous systems, this equation characterizesthe null-controllability set of a controlled system. This principled learningof neural network control Lyapunov functions outperforms alternativeapproaches, such as sum-of-squares and rational control Lyapunov functions, asdemonstrated by numerical examples. As an intermediate step, we also presentresults on the formal verification of quadratic control Lyapunov functions,which, aided by satisfiability modulo theories solvers, can performsurprisingly well compared to more sophisticated approaches and efficientlyproduce global certificates of null-controllability.</description><author>Jun Liu, Maxwell Fitzsimmons, Ruikun Zhou, Yiming Meng</author><pubDate>Mon, 30 Sep 2024 17:27:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20528v1</guid></item><item><title>Word Sense Disambiguation in Native Spanish: A Comprehensive Lexical Evaluation Resource</title><link>http://arxiv.org/abs/2409.20524v1</link><description>Human language, while aimed at conveying meaning, inherently carriesambiguity. It poses challenges for speech and language processing, but alsoserves crucial communicative functions. Efficiently solve ambiguity is both adesired and a necessary characteristic. The lexical meaning of a word incontext can be determined automatically by Word Sense Disambiguation (WSD)algorithms that rely on external knowledge often limited and biased towardEnglish. When adapting content to other languages, automated translations arefrequently inaccurate and a high degree of expert human validation is necessaryto ensure both accuracy and understanding. The current study addresses previouslimitations by introducing a new resource for Spanish WSD. It includes a senseinventory and a lexical dataset sourced from the Diccionario de la LenguaEspa\~nola which is maintained by the Real Academia Espa\~nola. We also reviewcurrent resources for Spanish and report metrics on them by a state-of-the-artsystem.</description><author>Pablo Ortega, Jordi Luque, Luis Lamiable, Rodrigo López, Richard Benjamins</author><pubDate>Mon, 30 Sep 2024 17:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20524v1</guid></item><item><title>Health-LLM: Personalized Retrieval-Augmented Disease Prediction System</title><link>http://arxiv.org/abs/2402.00746v7</link><description>Recent advancements in artificial intelligence (AI), especially largelanguage models (LLMs), have significantly advanced healthcare applications anddemonstrated potentials in intelligent medical treatment. However, there areconspicuous challenges such as vast data volumes and inconsistent symptomcharacterization standards, preventing full integration of healthcare AIsystems with individual patients' needs. To promote professional andpersonalized healthcare, we propose an innovative framework, Heath-LLM, whichcombines large-scale feature extraction and medical knowledge trade-offscoring. Compared to traditional health management applications, our system hasthree main advantages: (1) It integrates health reports and medical knowledgeinto a large model to ask relevant questions to large language model fordisease prediction; (2) It leverages a retrieval augmented generation (RAG)mechanism to enhance feature extraction; (3) It incorporates a semi-automatedfeature updating framework that can merge and delete features to improveaccuracy of disease prediction. We experiment on a large number of healthreports to assess the effectiveness of Health-LLM system. The results indicatethat the proposed system surpasses the existing ones and has the potential tosignificantly advance disease prediction and personalized health management.</description><author>Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua, Suiyuan Zhu, Yanda Meng, Zhenting Wang, Mengnan Du, Yongfeng Zhang</author><pubDate>Mon, 30 Sep 2024 17:22:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00746v7</guid></item><item><title>Upper and Lower Bounds for Distributionally Robust Off-Dynamics Reinforcement Learning</title><link>http://arxiv.org/abs/2409.20521v1</link><description>We study off-dynamics Reinforcement Learning (RL), where the policy trainingand deployment environments are different. To deal with this environmentalperturbation, we focus on learning policies robust to uncertainties intransition dynamics under the framework of distributionally robust Markovdecision processes (DRMDPs), where the nominal and perturbed dynamics arelinear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U thatenjoys an average suboptimality $\widetilde{\mathcal{O}}\big({d H \cdot \min\{1/{\rho}, H\}/\sqrt{K} }\big)$, where $K$ is the number of episodes, $H$ isthe horizon length, $d$ is the feature dimension and $\rho$ is the uncertaintylevel. This result improves the state-of-the-art by$\mathcal{O}(dH/\min\{1/\rho,H\})$. We also construct a novel hard instance andderive the first information-theoretic lower bound in this setting, whichindicates our algorithm is near-optimal up to $\mathcal{O}(\sqrt{H})$ for anyuncertainty level $\rho\in(0,1]$. Our algorithm also enjoys a 'rare-switching'design, and thus only requires $\mathcal{O}(dH\log(1+H^2K))$ policy switchesand $\mathcal{O}(d^2H\log(1+H^2K))$ calls for oracle to solve dual optimizationproblems, which significantly improves the computational efficiency of existingalgorithms for DRMDPs, whose policy switch and oracle complexities are both$\mathcal{O}(K)$.</description><author>Zhishuai Liu, Weixin Wang, Pan Xu</author><pubDate>Mon, 30 Sep 2024 17:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20521v1</guid></item><item><title>Accelerating Non-Maximum Suppression: A Graph Theory Perspective</title><link>http://arxiv.org/abs/2409.20520v1</link><description>Non-maximum suppression (NMS) is an indispensable post-processing step inobject detection. With the continuous optimization of network models, NMS hasbecome the ``last mile'' to enhance the efficiency of object detection. Thispaper systematically analyzes NMS from a graph theory perspective for the firsttime, revealing its intrinsic structure. Consequently, we propose twooptimization methods, namely QSI-NMS and BOE-NMS. The former is a fastrecursive divide-and-conquer algorithm with negligible mAP loss, and itsextended version (eQSI-NMS) achieves optimal complexity of $\mathcal{O}(n\logn)$. The latter, concentrating on the locality of NMS, achieves an optimizationat a constant level without an mAP loss penalty. Moreover, to facilitate rapidevaluation of NMS methods for researchers, we introduce NMS-Bench, the firstbenchmark designed to comprehensively assess various NMS methods. Taking theYOLOv8-N model on MS COCO 2017 as the benchmark setup, our method QSI-NMSprovides $6.2\times$ speed of original NMS on the benchmark, with a $0.1\%$decrease in mAP. The optimal eQSI-NMS, with only a $0.3\%$ mAP decrease,achieves $10.7\times$ speed. Meanwhile, BOE-NMS exhibits $5.1\times$ speed withno compromise in mAP.</description><author>King-Siong Si, Lu Sun, Weizhan Zhang, Tieliang Gong, Jiahao Wang, Jiang Liu, Hao Sun</author><pubDate>Mon, 30 Sep 2024 17:20:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20520v1</guid></item><item><title>SMLE: Safe Machine Learning via Embedded Overapproximation</title><link>http://arxiv.org/abs/2409.20517v1</link><description>Despite the extent of recent advances in Machine Learning (ML) and NeuralNetworks, providing formal guarantees on the behavior of these systems is stillan open problem, and a crucial requirement for their adoption in regulated orsafety-critical scenarios. We consider the task of training differentiable MLmodels guaranteed to satisfy designer-chosen properties, stated as input-outputimplications. This is very challenging, due to the computational complexity ofrigorously verifying and enforcing compliance in modern neural models. Weprovide an innovative approach based on three components: 1) a general, simplearchitecture enabling efficient verification with a conservative semantic; 2) arigorous training algorithm based on the Projected Gradient Method; 3) aformulation of the problem of searching for strong counterexamples. Theproposed framework, being only marginally affected by model complexity, scaleswell to practical applications, and produces models that provide full propertysatisfaction guarantees. We evaluate our approach on properties defined bylinear inequalities in regression, and on mutually exclusive classes inmultilabel classification. Our approach is competitive with a baseline thatincludes property enforcement during preprocessing, i.e. on the training data,as well as during postprocessing, i.e. on the model predictions. Finally, ourcontributions establish a framework that opens up multiple research directionsand potential improvements.</description><author>Matteo Francobaldi, Michele Lombardi</author><pubDate>Mon, 30 Sep 2024 17:19:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20517v1</guid></item><item><title>Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity</title><link>http://arxiv.org/abs/2409.18708v2</link><description>We introduce a novel family of adversarial attacks that exploit the inabilityof language models to interpret ASCII art. To evaluate these attacks, wepropose the ToxASCII benchmark and develop two custom ASCII art fonts: oneleveraging special tokens and another using text-filled letter shapes. Ourattacks achieve a perfect 1.0 Attack Success Rate across ten models, includingOpenAI's o1-preview and LLaMA 3.1. Warning: this paper contains examples of toxic language used for researchpurposes.</description><author>Sergey Berezin, Reza Farahbakhsh, Noel Crespi</author><pubDate>Mon, 30 Sep 2024 17:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18708v2</guid></item><item><title>Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties</title><link>http://arxiv.org/abs/2311.17041v3</link><description>A major reason behind the recent success of large language models (LLMs) istheir \textit{in-context learning} capability, which makes it possible torapidly adapt them to downstream text-based tasks by prompting them with asmall number of relevant demonstrations. While large vision-language models(VLMs) have recently been developed for tasks requiring both text and images,they largely lack in-context learning over visual information, especially inunderstanding and generating text about videos. In this work, we implement\textbf{E}mergent \textbf{I}n-context \textbf{Le}arning on \textbf{V}ideos(\eilev{}), a novel training paradigm that induces in-context learning overvideo and text by capturing key properties of pre-training data found by priorwork to be essential for in-context learning in transformers. In ourexperiments, we show that \eilev-trained models outperform other off-the-shelfVLMs in few-shot video narration for novel, rare actions. Furthermore, wedemonstrate that these key properties of bursty distributions, skewed marginaldistributions, and dynamic meaning each contribute to varying degrees to VLMs'in-context learning capability in narrating procedural videos. Our results,analysis, and \eilev{}-trained models yield numerous insights about theemergence of in-context learning over video and text, creating a foundation forfuture work to optimize and scale VLMs for open-domain video understanding andreasoning. Our code and demo are available at\url{https://github.com/yukw777/EILEV}.</description><author>Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, Shane Storks, Joyce Chai</author><pubDate>Mon, 30 Sep 2024 17:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17041v3</guid></item><item><title>Ensemble WSINDy for Data Driven Discovery of Governing Equations from Laser-based Full-field Measurements</title><link>http://arxiv.org/abs/2409.20510v1</link><description>This work leverages laser vibrometry and the weak form of the sparseidentification of nonlinear dynamics (WSINDy) for partial differentialequations to learn macroscale governing equations from full-field experimentaldata. In the experiments, two beam-like specimens, one aluminum and oneIDOX/Estane composite, are subjected to shear wave excitation in the lowfrequency regime and the response is measured in the form of particle velocityon the specimen surface. The WSINDy for PDEs algorithm is applied to theresulting spatio-temporal data to discover the effective dynamics of thespecimens from a family of potential PDEs. The discovered PDE is of therecognizable Euler-Bernoulli beam model form, from which the Young's modulusfor the two materials are estimated. An ensemble version of the WSINDyalgorithm is also used which results in information about the uncertainty inthe PDE coefficients and Young's moduli. The discovered PDEs are also simulatedwith a finite element code to compare against the experimental data withreasonable accuracy. Using full-field experimental data and WSINDy together isa powerful non-destructive approach for learning unknown governing equationsand gaining insights about mechanical systems in the dynamic regime.</description><author>Abigail C. Schmid, Alireza Doostan, Fatemeh Pourahmadian</author><pubDate>Mon, 30 Sep 2024 17:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20510v1</guid></item><item><title>NUTRIVISION: A System for Automatic Diet Management in Smart Healthcare</title><link>http://arxiv.org/abs/2409.20508v1</link><description>Maintaining health and fitness through a balanced diet is essential forpreventing non communicable diseases such as heart disease, diabetes, andcancer. NutriVision combines smart healthcare with computer vision and machinelearning to address the challenges of nutrition and dietary management. Thispaper introduces a novel system that can identify food items, estimatequantities, and provide comprehensive nutritional information. NutriVisionemploys the Faster Region based Convolutional Neural Network, a deep learningalgorithm that improves object detection by generating region proposals andthen classifying those regions, making it highly effective for accurate andfast food identification even in complex and disorganized meal settings.Through smartphone based image capture, NutriVision delivers instantnutritional data, including macronutrient breakdown, calorie count, andmicronutrient details. One of the standout features of NutriVision is itspersonalized nutritional analysis and diet recommendations, which are tailoredto each user's dietary preferences, nutritional needs, and health history. Byproviding customized advice, NutriVision helps users achieve specific healthand fitness goals, such as managing dietary restrictions or controlling weight.In addition to offering precise food detection and nutritional assessment,NutriVision supports smarter dietary decisions by integrating user data withrecommendations that promote a balanced, healthful diet. This system presents apractical and advanced solution for nutrition management and has the potentialto significantly influence how people approach their dietary choices, promotinghealthier eating habits and overall well being. This paper discusses thedesign, performance evaluation, and prospective applications of the NutriVisionsystem.</description><author>Madhumita Veeramreddy, Ashok Kumar Pradhan, Swetha Ghanta, Laavanya Rachakonda, Saraju P Mohanty</author><pubDate>Mon, 30 Sep 2024 17:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20508v1</guid></item><item><title>What Information Contributes to Log-based Anomaly Detection? Insights from a Configurable Transformer-Based Approach</title><link>http://arxiv.org/abs/2409.20503v1</link><description>Log data are generated from logging statements in the source code, providinginsights into the execution processes of software applications and systems.State-of-the-art log-based anomaly detection approaches typically leverage deeplearning models to capture the semantic or sequential information in the logdata and detect anomalous runtime behaviors. However, the impacts of thesedifferent types of information are not clear. In addition, existing approacheshave not captured the timestamps in the log data, which can potentially providemore fine-grained temporal information than sequential information. In thiswork, we propose a configurable transformer-based anomaly detection model thatcan capture the semantic, sequential, and temporal information in the log dataand allows us to configure the different types of information as the model'sfeatures. Additionally, we train and evaluate the proposed model using logsequences of different lengths, thus overcoming the constraint of existingmethods that rely on fixed-length or time-windowed log sequences as inputs.With the proposed model, we conduct a series of experiments with differentcombinations of input features to evaluate the roles of different types ofinformation in anomaly detection. When presented with log sequences of varyinglengths, the model can attain competitive and consistently stable performancecompared to the baselines. The results indicate that the event occurrenceinformation plays a key role in identifying anomalies, while the impact of thesequential and temporal information is not significant for anomaly detection inthe studied public datasets. On the other hand, the findings also reveal thesimplicity of the studied public datasets and highlight the importance ofconstructing new datasets that contain different types of anomalies to betterevaluate the performance of anomaly detection models.</description><author>Xingfang Wu, Heng Li, Foutse Khomh</author><pubDate>Mon, 30 Sep 2024 17:03:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20503v1</guid></item><item><title>MemBench: Memorized Image Trigger Prompt Dataset for Diffusion Models</title><link>http://arxiv.org/abs/2407.17095v2</link><description>Diffusion models have achieved remarkable success in Text-to-Image generationtasks, leading to the development of many commercial models. However, recentstudies have reported that diffusion models often generate replicated images intrain data when triggered by specific prompts, potentially raising socialissues ranging from copyright to privacy concerns. To sidestep thememorization, there have been recent studies for developing memorizationmitigation methods for diffusion models. Nevertheless, the lack of benchmarksimpedes the assessment of the true effectiveness of these methods. In thiswork, we present MemBench, the first benchmark for evaluating imagememorization mitigation methods. Our benchmark includes a large number ofmemorized image trigger prompts in various Text-to-Image diffusion models.Furthermore, in contrast to the prior work evaluating mitigation performanceonly on trigger prompts, we present metrics evaluating on both trigger promptsand general prompts, so that we can see whether mitigation methods address thememorization issue while maintaining performance for general prompts. This isan important development considering the practical applications which previousworks have overlooked. Through evaluation on MemBench, we verify that theperformance of existing image memorization mitigation methods is stillinsufficient for application to diffusion models. The code and datasets areavailable at https://github.com/chunsanHong/MemBench\_code.</description><author>Chunsan Hong, Tae-Hyun Oh, Minhyuk Sung</author><pubDate>Mon, 30 Sep 2024 17:02:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17095v2</guid></item><item><title>COLLAGE: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models</title><link>http://arxiv.org/abs/2409.20502v1</link><description>We propose a novel framework COLLAGE for generating collaborativeagent-object-agent interactions by leveraging large language models (LLMs) andhierarchical motion-specific vector-quantized variational autoencoders(VQ-VAEs). Our model addresses the lack of rich datasets in this domain byincorporating the knowledge and reasoning abilities of LLMs to guide agenerative diffusion model. The hierarchical VQ-VAE architecture capturesdifferent motion-specific characteristics at multiple levels of abstraction,avoiding redundant concepts and enabling efficient multi-resolutionrepresentation. We introduce a diffusion model that operates in the latentspace and incorporates LLM-generated motion planning cues to guide thedenoising process, resulting in prompt-specific motion generation with greatercontrol and diversity. Experimental results on the CORE-4D, and InterHumandatasets demonstrate the effectiveness of our approach in generating realisticand diverse collaborative human-object-human interactions, outperformingstate-of-the-art methods. Our work opens up new possibilities for modelingcomplex interactions in various domains, such as robotics, graphics andcomputer vision.</description><author>Divyanshu Daiya, Damon Conover, Aniket Bera</author><pubDate>Mon, 30 Sep 2024 17:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20502v1</guid></item><item><title>FreeMask: Rethinking the Importance of Attention Masks for Zero-Shot Video Editing</title><link>http://arxiv.org/abs/2409.20500v1</link><description>Text-to-video diffusion models have made remarkable advancements. Driven bytheir ability to generate temporally coherent videos, research on zero-shotvideo editing using these fundamental models has expanded rapidly. To enhanceediting quality, structural controls are frequently employed in video editing.Among these techniques, cross-attention mask control stands out for itseffectiveness and efficiency. However, when cross-attention masks are naivelyapplied to video editing, they can introduce artifacts such as blurring andflickering. Our experiments uncover a critical factor overlooked in previousvideo editing research: cross-attention masks are not consistently clear butvary with model structure and denoising timestep. To address this issue, wepropose the metric Mask Matching Cost (MMC) that quantifies this variabilityand propose FreeMask, a method for selecting optimal masks tailored to specificvideo editing tasks. Using MMC-selected masks, we further improve the maskedfusion mechanism within comprehensive attention features, e.g., temp, cross,and self-attention modules. Our approach can be seamlessly integrated intoexisting zero-shot video editing frameworks with better performance, requiringno control assistance or parameter fine-tuning but enabling adaptive decouplingof unedited semantic layouts with mask precision control. Extensive experimentsdemonstrate that FreeMask achieves superior semantic fidelity, temporalconsistency, and editing quality compared to state-of-the-art methods.</description><author>Lingling Cai, Kang Zhao, Hangjie Yuan, Yingya Zhang, Shiwei Zhang, Kejie Huang</author><pubDate>Mon, 30 Sep 2024 17:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20500v1</guid></item><item><title>Enhancing Romanian Offensive Language Detection through Knowledge Distillation, Multi-Task Learning, and Data Augmentation</title><link>http://arxiv.org/abs/2409.20498v1</link><description>This paper highlights the significance of natural language processing (NLP)within artificial intelligence, underscoring its pivotal role in comprehendingand modeling human language. Recent advancements in NLP, particularly inconversational bots, have garnered substantial attention and adoption amongdevelopers. This paper explores advanced methodologies for attaining smallerand more efficient NLP models. Specifically, we employ three key approaches:(1) training a Transformer-based neural network to detect offensive language,(2) employing data augmentation and knowledge distillation techniques toincrease performance, and (3) incorporating multi-task learning with knowledgedistillation and teacher annealing using diverse datasets to enhanceefficiency. The culmination of these methods has yielded demonstrably improvedoutcomes.</description><author>Vlad-Cristian Matei, Iulian-Marius Tăiatu, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel</author><pubDate>Mon, 30 Sep 2024 16:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20498v1</guid></item><item><title>Online Decision Deferral under Budget Constraints</title><link>http://arxiv.org/abs/2409.20489v1</link><description>Machine Learning (ML) models are increasingly used to support or substitutedecision making. In applications where skilled experts are a limited resource,it is crucial to reduce their burden and automate decisions when theperformance of an ML model is at least of equal quality. However, models areoften pre-trained and fixed, while tasks arrive sequentially and theirdistribution may shift. In that case, the respective performance of thedecision makers may change, and the deferral algorithm must remain adaptive. Wepropose a contextual bandit model of this online decision making problem. Ourframework includes budget constraints and different types of partial feedbackmodels. Beyond the theoretical guarantees of our algorithm, we proposeefficient extensions that achieve remarkable performance on real-worlddatasets.</description><author>Mirabel Reid, Tom Sühr, Claire Vernade, Samira Samadi</author><pubDate>Mon, 30 Sep 2024 16:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20489v1</guid></item><item><title>"What" x "When" working memory representations using Laplace Neural Manifolds</title><link>http://arxiv.org/abs/2409.20484v1</link><description>Working memory $\unicode{x2013}$ the ability to remember recent events asthey recede continuously into the past $\unicode{x2013}$ requires the abilityto represent any stimulus at any time delay. This property requires neuronscoding working memory to show mixed selectivity, with conjunctive receptivefields (RFs) for stimuli and time, forming a representation of 'what' $\times$'when'. We study the properties of such a working memory in simple experimentswhere a single stimulus must be remembered for a short time. The requirement ofconjunctive receptive fields allows the covariance matrix of the network todecouple neatly, allowing an understanding of the low-dimensional dynamics ofthe population. Different choices of temporal basis functions lead toqualitatively different dynamics. We study a specific choice $\unicode{x2013}$a Laplace space with exponential basis functions for time coupled to an"Inverse Laplace" space with circumscribed basis functions in time. We refer tothis choice with basis functions that evenly tile log time as a Laplace NeuralManifold. Despite the fact that they are related to one another by a linearprojection, the Laplace population shows a stable stimulus-specific subspacewhereas the Inverse Laplace population shows rotational dynamics. The growth ofthe rank of the covariance matrix with time depends on the density of thetemporal basis set; logarithmic tiling shows good agreement with data. Wesketch a continuous attractor CANN that constructs a Laplace Neural Manifold.The attractor in the Laplace space appears as an edge; the attractor for theinverse space appears as a bump. This work provides a map for going from moreabstract cognitive models of WM to circuit-level implementation usingcontinuous attractor neural networks, and places constraints on the types ofneural dynamics that support working memory.</description><author>Aakash Sarkar, Chenyu Wang, Shangfu Zuo, Marc W. Howard</author><pubDate>Mon, 30 Sep 2024 16:47:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20484v1</guid></item><item><title>RecSys Challenge 2024: Balancing Accuracy and Editorial Values in News Recommendations</title><link>http://arxiv.org/abs/2409.20483v1</link><description>The RecSys Challenge 2024 aims to advance news recommendation by addressingboth the technical and normative challenges inherent in designing effective andresponsible recommender systems for news publishing. This paper describes thechallenge, including its objectives, problem setting, and the dataset providedby the Danish news publishers Ekstra Bladet and JP/Politikens Media Group("Ekstra Bladet"). The challenge explores the unique aspects of newsrecommendation, such as modeling user preferences based on behavior, accountingfor the influence of the news agenda on user interests, and managing the rapiddecay of news items. Additionally, the challenge embraces normativecomplexities, investigating the effects of recommender systems on news flow andtheir alignment with editorial values. We summarize the challenge setup,dataset characteristics, and evaluation metrics. Finally, we announce thewinners and highlight their contributions. The dataset is available at:https://recsys.eb.dk.</description><author>Johannes Kruse, Kasper Lindskow, Saikishore Kalloori, Marco Polignano, Claudio Pomo, Abhishek Srivastava, Anshuk Uppal, Michael Riis Andersen, Jes Frellsen</author><pubDate>Mon, 30 Sep 2024 16:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20483v1</guid></item><item><title>The African Woman is Rhythmic and Soulful: An Investigation of Implicit Biases in LLM Open-ended Text Generation</title><link>http://arxiv.org/abs/2407.01270v2</link><description>This paper investigates the subtle and often concealed biases present inLarge Language Models (LLMs), focusing on implicit biases that may remaindespite passing explicit bias tests. Implicit biases are significant becausethey influence the decisions made by these systems, potentially perpetuatingstereotypes and discrimination, even when LLMs appear to function fairly.Traditionally, explicit bias tests or embedding-based methods are employed todetect bias, but these approaches can overlook more nuanced, implicit forms ofbias. To address this, we introduce two novel psychological-inspiredmethodologies: the LLM Implicit Association Test (IAT) Bias and the LLMDecision Bias, designed to reveal and measure implicit biases throughprompt-based and decision-making tasks. Additionally, open-ended generationtasks with thematic analysis of word generations and storytelling providequalitative insights into the model's behavior. Our findings demonstrate thatthe LLM IAT Bias correlates with traditional methods and more effectivelypredicts downstream behaviors, as measured by the LLM Decision Bias, offering amore comprehensive framework for detecting subtle biases in AI systems. Thisresearch advances the field of AI ethics by proposing new methods tocontinually assess and mitigate biases in LLMs, highlighting the importance ofqualitative and decision-focused evaluations to address challenges thatprevious approaches have not fully captured.</description><author>Serene Lim, María Pérez-Ortiz</author><pubDate>Mon, 30 Sep 2024 16:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01270v2</guid></item><item><title>VideoPatchCore: An Effective Method to Memorize Normality for Video Anomaly Detection</title><link>http://arxiv.org/abs/2409.16225v4</link><description>Video anomaly detection (VAD) is a crucial task in video analysis andsurveillance within computer vision. Currently, VAD is gaining attention withmemory techniques that store the features of normal frames. The stored featuresare utilized for frame reconstruction, identifying an abnormality when asignificant difference exists between the reconstructed and input frames.However, this approach faces several challenges due to the simultaneousoptimization required for both the memory and encoder-decoder model. Thesechallenges include increased optimization difficulty, complexity ofimplementation, and performance variability depending on the memory size. Toaddress these challenges,we propose an effective memory method for VAD, calledVideoPatchCore. Inspired by PatchCore, our approach introduces a structure thatprioritizes memory optimization and configures three types of memory tailoredto the characteristics of video data. This method effectively addresses thelimitations of existing memory-based methods, achieving good performancecomparable to state-of-the-art methods. Furthermore, our method requires notraining and is straightforward to implement, making VAD tasks more accessible.Our code is available online at github.com/SkiddieAhn/Paper-VideoPatchCore.</description><author>Sunghyun Ahn, Youngwan Jo, Kijung Lee, Sanghyun Park</author><pubDate>Mon, 30 Sep 2024 16:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16225v4</guid></item><item><title>IRFusionFormer: Enhancing Pavement Crack Segmentation with RGB-T Fusion and Topological-Based Loss</title><link>http://arxiv.org/abs/2409.20474v1</link><description>Crack segmentation is crucial in civil engineering, particularly forassessing pavement integrity and ensuring the durability of infrastructure.While deep learning has advanced RGB-based segmentation, performance degradesunder adverse conditions like low illumination or motion blur. Thermal imagingoffers complementary information by capturing emitted radiation, improvingcrack detection in challenging environments. Combining RGB and thermal images(RGB-T) for crack segmentation shows promise in complex real-world conditions,such as adverse weather, yet research in this area remains limited. CurrentRGB-T segmentation methods often fail to fully exploit the complementaryrelationships between modalities at various levels of interaction. To addressthis, we propose IRFusionFormer, a novel model for crack segmentation thateffectively integrates RGB and thermal data. Our Efficient RGB-T Cross FusionModule captures multi-scale relationships and long-range dependencies betweenmodalities without significant computational overhead. Additionally, weintroduce the Interaction-Hybrid-Branch-Supervision framework, which enhancesinteraction between modalities by distributing fused features across brancheswith joint supervision. To maintain the topological structure of cracks, weintroduce a novel topology-based loss function that preserves connectivityduring training. Our method achieves state-of-the-art performance, with a Dicescore of 90.01% and an IoU of 81.83%, significantly improving robustness andaccuracy in varying environmental conditions. These advancements address keychallenges in pavement crack segmentation, offering a more reliable andefficient solution. For access to the codes, data, and models from this study,visit https://github.com/sheauhuu/IRFusionFormer</description><author>Ruiqiang Xiao, Xiaohu Chen</author><pubDate>Mon, 30 Sep 2024 16:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20474v1</guid></item><item><title>Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey</title><link>http://arxiv.org/abs/2409.18169v2</link><description>Recent research demonstrates that the nascent fine-tuning-as-a-servicebusiness model exposes serious safety concerns -- fine-tuning over a fewharmful data uploaded by the users can compromise the safety alignment of themodel. The attack, known as harmful fine-tuning, has raised a broad researchinterest among the community. However, as the attack is still new, \textbf{weobserve from our miserable submission experience that there are generalmisunderstandings within the research community.} We in this paper aim to clearsome common concerns for the attack setting, and formally establish theresearch problem. Specifically, we first present the threat model of theproblem, and introduce the harmful fine-tuning attack and its variants. Then wesystematically survey the existing literature on attacks/defenses/mechanicalanalysis of the problem. Finally, we outline future research directions thatmight contribute to the development of the field. Additionally, we present alist of questions of interest, which might be useful to refer to when reviewersin the peer review process question the realism of theexperiment/attack/defense setting. A curated list of relevant papers ismaintained and made accessible at:\url{https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers}.</description><author>Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu</author><pubDate>Mon, 30 Sep 2024 16:29:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18169v2</guid></item><item><title>Continual Human Pose Estimation for Incremental Integration of Keypoints and Pose Variations</title><link>http://arxiv.org/abs/2409.20469v1</link><description>This paper reformulates cross-dataset human pose estimation as a continuallearning task, aiming to integrate new keypoints and pose variations intoexisting models without losing accuracy on previously learned datasets. Webenchmark this formulation against established regularization-based methods formitigating catastrophic forgetting, including EWC, LFL, and LwF. Moreover, wepropose a novel regularization method called Importance-Weighted Distillation(IWD), which enhances conventional LwF by introducing a layer-wise distillationpenalty and dynamic temperature adjustment based on layer importance forpreviously learned knowledge. This allows for a controlled adaptation to newtasks that respects the stability-plasticity balance critical in continuallearning. Through extensive experiments across three datasets, we demonstratethat our approach outperforms existing regularization-based continual learningstrategies. IWD shows an average improvement of 3.60\% over thestate-of-the-art LwF method. The results highlight the potential of our methodto serve as a robust framework for real-world applications where models mustevolve with new data without forgetting past knowledge.</description><author>Muhammad Saif Ullah Khan, Muhammad Ahmed Ullah Khan, Muhammad Zeshan Afzal, Didier Stricker</author><pubDate>Mon, 30 Sep 2024 16:29:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20469v1</guid></item><item><title>A Weakly Supervised Data Labeling Framework for Machine Lexical Normalization in Vietnamese Social Media</title><link>http://arxiv.org/abs/2409.20467v1</link><description>This study introduces an innovative automatic labeling framework to addressthe challenges of lexical normalization in social media texts for low-resourcelanguages like Vietnamese. Social media data is rich and diverse, but theevolving and varied language used in these contexts makes manual labelinglabor-intensive and expensive. To tackle these issues, we propose a frameworkthat integrates semi-supervised learning with weak supervision techniques. Thisapproach enhances the quality of training dataset and expands its size whileminimizing manual labeling efforts. Our framework automatically labels rawdata, converting non-standard vocabulary into standardized forms, therebyimproving the accuracy and consistency of the training data. Experimentalresults demonstrate the effectiveness of our weak supervision framework innormalizing Vietnamese text, especially when utilizing Pre-trained LanguageModels. The proposed framework achieves an impressive F1-score of 82.72% andmaintains vocabulary integrity with an accuracy of up to 99.22%. Additionally,it effectively handles undiacritized text under various conditions. Thisframework significantly enhances natural language normalization quality andimproves the accuracy of various NLP tasks, leading to an average accuracyincrease of 1-3%.</description><author>Dung Ha Nguyen, Anh Thi Hoang Nguyen, Kiet Van Nguyen</author><pubDate>Mon, 30 Sep 2024 16:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20467v1</guid></item><item><title>Language Resources in Spanish for Automatic Text Simplification across Domains</title><link>http://arxiv.org/abs/2409.20466v1</link><description>This work describes the language resources and models developed for automaticsimplification of Spanish texts in three domains: Finance, Medicine and Historystudies. We created several corpora in each domain, annotation andsimplification guidelines, a lexicon of technical and simplified medical terms,datasets used in shared tasks for the financial domain, and two simplificationtools. The methodology, resources and companion publications are sharedpublicly on the web-site: https://clara-nlp.uned.es/.</description><author>Antonio Moreno-Sandoval, Leonardo Campillos-Llanos, Ana García-Serrano</author><pubDate>Mon, 30 Sep 2024 16:26:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20466v1</guid></item><item><title>IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training</title><link>http://arxiv.org/abs/2310.07355v4</link><description>In the field of medical Vision-Language Pre-training (VLP), significantefforts have been devoted to deriving text and image features from bothclinical reports and associated medical images. However, most existing methodsmay have overlooked the opportunity in leveraging the inherent hierarchicalstructure of clinical reports, which are generally split into `findings' fordescriptive content and `impressions' for conclusive observation. Instead ofutilizing this rich, structured format, current medical VLP approaches oftensimplify the report into either a unified entity or fragmented tokens. In thiswork, we propose a novel clinical prior guided VLP framework named IMITATE tolearn the structure information from medical reports with hierarchicalvision-language alignment. The framework derives multi-level visual featuresfrom the chest X-ray (CXR) images and separately aligns these features with thedescriptive and the conclusive text encoded in the hierarchical medical report.Furthermore, a new clinical-informed contrastive loss is introduced forcross-modal learning, which accounts for clinical prior knowledge informulating sample correlations in contrastive learning. The proposed model,IMITATE, outperforms baseline VLP methods across six different datasets,spanning five medical imaging downstream tasks. Comprehensive experimentalresults highlight the advantages of integrating the hierarchical structure ofmedical reports for vision-language alignment.</description><author>Che Liu, Sibo Cheng, Miaojing Shi, Anand Shah, Wenjia Bai, Rossella Arcucci</author><pubDate>Mon, 30 Sep 2024 16:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07355v4</guid></item><item><title>Bayesian Event Categorization Matrix Approach for Nuclear Detonations</title><link>http://arxiv.org/abs/2409.18227v2</link><description>Current efforts to detect nuclear detonations and correctly categorizeexplosion sources with ground- and space-collected discriminants presentschallenges that remain unaddressed by the Event Categorization Matrix (ECM)model. Smaller events (lower yield explosions) often include only sparseobservations among few modalities and can therefore lack a complete set ofdiscriminants. The covariance structures can also vary significantly betweensuch observations of event (source-type) categories. Both obstacles areproblematic for ``classic'' ECM. Our work addresses this gap and presents aBayesian update to the previous ECM model, termed B-ECM, which can be trainedon partial observations and does not rely on a pooled covariance structure. Wefurther augment ECM with Bayesian Decision Theory so that false negative orfalse positive rates of an event categorization can be reduced in an intuitivemanner. To demonstrate improved categorization rates with B-ECM, we compare anarray of B-ECM and classic ECM models with multiple performance metrics thatleverage Monte Carlo experiments. We use both synthetic and real data. OurB-ECM models show consistent gains in overall accuracy and a lower falsenegative rates relative to the classic ECM model. We propose future avenues toimprove B-ECM that expand its decision-making and predictive capability.</description><author>Scott Koermer, Joshua D. Carmichael, Brian J. Williams</author><pubDate>Mon, 30 Sep 2024 16:19:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18227v2</guid></item><item><title>POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</title><link>http://arxiv.org/abs/2406.03843v3</link><description>Large language models (LLMs) have exhibited impressive abilities formultimodal content comprehension and reasoning with proper prompting in zero-or few-shot settings. Despite the proliferation of interactive systemsdeveloped to support prompt engineering for LLMs across various tasks, mosthave primarily focused on textual or visual inputs, thus neglecting the complexinterplay between modalities within multimodal inputs. This oversight hindersthe development of effective prompts that guide model multimodal reasoningprocesses by fully exploiting the rich context provided by multiple modalities.In this paper, we present POEM, a visual analytics system to facilitateefficient prompt engineering for enhancing the multimodal reasoning performanceof LLMs. The system enables users to explore the interaction patterns acrossmodalities at varying levels of detail for a comprehensive understanding of themultimodal knowledge elicited by various prompts. Through diverserecommendations of demonstration examples and instructional principles, POEMsupports users in iteratively crafting and refining prompts to better align andenhance model knowledge with human insights. The effectiveness and efficiencyof our system are validated through two case studies and interviews withexperts.</description><author>Jianben He, Xingbo Wang, Shiyi Liu, Guande Wu, Claudio Silva, Huamin Qu</author><pubDate>Mon, 30 Sep 2024 16:16:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03843v3</guid></item><item><title>Linear Projections of Teacher Embeddings for Few-Class Distillation</title><link>http://arxiv.org/abs/2409.20449v1</link><description>Knowledge Distillation (KD) has emerged as a promising approach fortransferring knowledge from a larger, more complex teacher model to a smallerstudent model. Traditionally, KD involves training the student to mimic theteacher's output probabilities, while more advanced techniques have exploredguiding the student to adopt the teacher's internal representations. Despiteits widespread success, the performance of KD in binary classification andfew-class problems has been less satisfactory. This is because the informationabout the teacher model's generalization patterns scales directly with thenumber of classes. Moreover, several sophisticated distillation methods may notbe universally applicable or effective for data types beyond Computer Vision.Consequently, effective distillation techniques remain elusive for a range ofkey real-world applications, such as sentiment analysis, search queryunderstanding, and advertisement-query relevance assessment. Taking theseobservations into account, we introduce a novel method for distilling knowledgefrom the teacher's model representations, which we term Learning EmbeddingLinear Projections (LELP). Inspired by recent findings about the structure offinal-layer representations, LELP works by identifying informative linearsubspaces in the teacher's embedding space, and splitting them intopseudo-subclasses. The student model is then trained to replicate thesepseudo-classes. Our experimental evaluation on large-scale NLP benchmarks likeAmazon Reviews and Sentiment140 demonstrate the LELP is consistentlycompetitive with, and typically superior to, existing state-of-the-artdistillation algorithms for binary and few-class problems, where most KDmethods suffer.</description><author>Noel Loo, Fotis Iliopoulos, Wei Hu, Erik Vee</author><pubDate>Mon, 30 Sep 2024 16:07:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20449v1</guid></item><item><title>POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator</title><link>http://arxiv.org/abs/2409.20447v1</link><description>Neural Architecture Search (NAS) automates neural network design, reducingdependence on human expertise. While NAS methods are computationally intensiveand dataset-specific, auxiliary predictors reduce the models needing training,decreasing search time. This strategy is used to generate architecturessatisfying multiple computational constraints. Recently, Transferable NAS hasemerged, generalizing the search process from dataset-dependent totask-dependent. In this field, DiffusionNAG is a state-of-the-art method. Thisdiffusion-based approach streamlines computation, generating architecturesoptimized for accuracy on unseen datasets without further adaptation. However,by focusing solely on accuracy, DiffusionNAG overlooks other crucial objectiveslike model complexity, computational efficiency, and inference latency --factors essential for deploying models in resource-constrained environments.This paper introduces the Pareto-Optimal Many-Objective Neural ArchitectureGenerator (POMONAG), extending DiffusionNAG via a many-objective diffusionprocess. POMONAG simultaneously considers accuracy, number of parameters,multiply-accumulate operations (MACs), and inference latency. It integratesPerformance Predictor models to estimate these metrics and guide diffusiongradients. POMONAG's optimization is enhanced by expanding its trainingMeta-Dataset, applying Pareto Front Filtering, and refining embeddings forconditional generation. These enhancements enable POMONAG to generatePareto-optimal architectures that outperform the previous state-of-the-art inperformance and efficiency. Results were validated on two search spaces --NASBench201 and MobileNetV3 -- and evaluated across 15 image classificationdatasets.</description><author>Eugenio Lomurno, Samuele Mariani, Matteo Monti, Matteo Matteucci</author><pubDate>Mon, 30 Sep 2024 16:05:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20447v1</guid></item><item><title>Instance-adaptive Zero-shot Chain-of-Thought Prompting</title><link>http://arxiv.org/abs/2409.20441v1</link><description>Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effectivestrategy for enhancing the performance of large language models (LLMs) inreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-levelprompt uniformly applied across the whole of instances is inherently limitedsince one prompt cannot be a good partner for all, a more appropriate approachshould consider the interaction between the prompt and each instancemeticulously. This work introduces an instance-adaptive prompting algorithm asan alternative zero-shot CoT reasoning scheme by adaptively differentiatinggood and bad prompts. Concretely, we first employ analysis on LLMs through thelens of information flow to detect the mechanism under zero-shot CoT reasoning,in which we discover that information flows from question to prompt andquestion to rationale jointly influence the reasoning results most. We noticethat a better zero-shot CoT reasoning needs the prompt to obtain semanticinformation from the question then the rationale aggregates sufficientinformation from the question directly and via the prompt indirectly. On thecontrary, lacking any of those would probably lead to a bad one. Stem fromthat, we further propose an instance-adaptive prompting strategy (IAP) forzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwenon math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, CausalJudgement) obtain consistent improvement, demonstrating that theinstance-adaptive zero-shot CoT prompting performs better than other task-levelmethods with some curated prompts or sophisticated procedures, showing thesignificance of our findings in the zero-shot CoT reasoning mechanism.</description><author>Xiaosong Yuan, Chen Shen, Shaotian Yan, Xiaofeng Zhang, Liang Xie, Wenxiao Wang, Renchu Guan, Ying Wang, Jieping Ye</author><pubDate>Mon, 30 Sep 2024 16:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20441v1</guid></item><item><title>Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits</title><link>http://arxiv.org/abs/2409.20440v1</link><description>Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regretfor adversarial as well as stochastic bandit problems and allow for astreamlined analysis. Nonetheless, FTRL algorithms require the solution of anoptimization problem in every iteration and are thus computationallychallenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achievecomputational efficiency by perturbing the estimates of the rewards of thearms, but their regret analysis is cumbersome. We propose a new FTPL algorithmthat generates optimal policies for both adversarial and stochastic multi-armedbandits. Like FTRL, our algorithm admits a unified regret analysis, and similarto FTPL, it offers low computational costs. Unlike existing FTPL algorithmsthat rely on independent additive disturbances governed by a \textit{known}distribution, we allow for disturbances governed by an \textit{ambiguous}distribution that is only known to belong to a given set and propose aprinciple of optimism in the face of ambiguity. Consequently, our frameworkgeneralizes existing FTPL algorithms. It also encapsulates a broad range ofFTRL methods as special cases, including several optimal ones, which appears tobe impossible with current FTPL methods. Finally, we use techniques fromdiscrete choice theory to devise an efficient bisection algorithm for computingthe optimistic arm sampling probabilities. This algorithm is up to $10^4$ timesfaster than standard FTRL algorithms that solve an optimization problem inevery iteration. Our results not only settle existing conjectures but alsoprovide new insights into the impact of perturbations by mapping FTRL to FTPL.</description><author>Mengmeng Li, Daniel Kuhn, Bahar Taskesen</author><pubDate>Mon, 30 Sep 2024 16:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20440v1</guid></item><item><title>Logarithmic-Regret Quantum Learning Algorithms for Zero-Sum Games</title><link>http://arxiv.org/abs/2304.14197v2</link><description>We propose the first online quantum algorithm for solving zero-sum games with$\widetilde O(1)$ regret under the game setting. Moreover, our quantumalgorithm computes an $\varepsilon$-approximate Nash equilibrium of an $m\times n$ matrix zero-sum game in quantum time $\widetildeO(\sqrt{m+n}/\varepsilon^{2.5})$. Our algorithm uses standard quantum inputsand generates classical outputs with succinct descriptions, facilitatingend-to-end applications. Technically, our online quantum algorithm "quantizes"classical algorithms based on the optimistic multiplicative weight updatemethod. At the heart of our algorithm is a fast quantum multi-samplingprocedure for the Gibbs sampling problem, which may be of independent interest.</description><author>Minbo Gao, Zhengfeng Ji, Tongyang Li, Qisheng Wang</author><pubDate>Mon, 30 Sep 2024 15:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14197v2</guid></item><item><title>QAEncoder: Towards Aligned Representation Learning in Question Answering System</title><link>http://arxiv.org/abs/2409.20434v1</link><description>Modern QA systems entail retrieval-augmented generation (RAG) for accurateand trustworthy responses. However, the inherent gap between user queries andrelevant documents hinders precise matching. Motivated by our conicaldistribution hypothesis, which posits that potential queries and documents forma cone-like structure in the embedding space, we introduce QAEncoder, atraining-free approach to bridge this gap. Specifically, QAEncoder estimatesthe expectation of potential queries in the embedding space as a robustsurrogate for the document embedding, and attaches document fingerprints toeffectively distinguish these embeddings. Extensive experiments on fourteenembedding models across six languages and eight datasets validate QAEncoder'salignment capability, which offers a plug-and-play solution that seamlesslyintegrates with existing RAG architectures and training-based methods.</description><author>Zhengren Wang, Qinhan Yu, Shida Wei, Zhiyu Li, Feiyu Xiong, Xiaoxing Wang, Simin Niu, Hao Liang, Wentao Zhang</author><pubDate>Mon, 30 Sep 2024 15:53:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20434v1</guid></item><item><title>Multilevel Picard approximations and deep neural networks with ReLU, leaky ReLU, and softplus activation overcome the curse of dimensionality when approximating semilinear parabolic partial differential equations in $L^p$-sense</title><link>http://arxiv.org/abs/2409.20431v1</link><description>We prove that multilevel Picard approximations and deep neural networks withReLU, leaky ReLU, and softplus activation are capable of approximatingsolutions of semilinear Kolmogorov PDEs in $L^\mathfrak{p}$-sense,$\mathfrak{p}\in [2,\infty)$, in the case of gradient-independent,Lipschitz-continuous nonlinearities, while the computational effort of themultilevel Picard approximations and the required number of parameters in theneural networks grow at most polynomially in both dimension $d\in \mathbb{N}$and reciprocal of the prescribed accuracy $\epsilon$.</description><author>Ariel Neufeld, Tuan Anh Nguyen</author><pubDate>Mon, 30 Sep 2024 15:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20431v1</guid></item><item><title>HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding</title><link>http://arxiv.org/abs/2409.20429v1</link><description>Large Vision-Language Models (LVLMs) have shown remarkable performance onmany visual-language tasks. However, these models still suffer from multimodalhallucination, which means the generation of objects or content that violatesthe images. Many existing work detects hallucination by directly judgingwhether an object exists in an image, overlooking the association between theobject and semantics. To address this issue, we propose Hierarchical FeedbackLearning with Vision-enhanced Penalty Decoding (HELPD). This frameworkincorporates hallucination feedback at both object and sentence semanticlevels. Remarkably, even with a marginal degree of training, this approach canalleviate over 15% of hallucination. Simultaneously, HELPD penalizes the outputlogits according to the image attention window to avoid being overly affectedby generated text. HELPD can be seamlessly integrated with any LVLMs. Ourexperiments demonstrate that the proposed framework yields favorable resultsacross multiple hallucination benchmarks. It effectively mitigateshallucination for different LVLMs and concurrently improves their textgeneration quality.</description><author>Fan Yuan, Chi Qin, Xiaogang Xu, Piji Li</author><pubDate>Mon, 30 Sep 2024 15:52:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20429v1</guid></item><item><title>Decoding the Echoes of Vision from fMRI: Memory Disentangling for Past Semantic Information</title><link>http://arxiv.org/abs/2409.20428v1</link><description>The human visual system is capable of processing continuous streams of visualinformation, but how the brain encodes and retrieves recent visual memoriesduring continuous visual processing remains unexplored. This study investigatesthe capacity of working memory to retain past information under continuousvisual stimuli. And then we propose a new task Memory Disentangling, which aimsto extract and decode past information from fMRI signals. To address the issueof interference from past memory information, we design a disentangledcontrastive learning method inspired by the phenomenon of proactiveinterference. This method separates the information between adjacent fMRIsignals into current and past components and decodes them into imagedescriptions. Experimental results demonstrate that this method effectivelydisentangles the information within fMRI signals. This research could advancebrain-computer interfaces and mitigate the problem of low temporal resolutionin fMRI.</description><author>Runze Xia, Congchi Yin, Piji Li</author><pubDate>Mon, 30 Sep 2024 15:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20428v1</guid></item><item><title>Sufficient and Necessary Explanations (and What Lies in Between)</title><link>http://arxiv.org/abs/2409.20427v1</link><description>As complex machine learning models continue to find applications inhigh-stakes decision-making scenarios, it is crucial that we can explain andunderstand their predictions. Post-hoc explanation methods provide usefulinsights by identifying important features in an input $\mathbf{x}$ withrespect to the model output $f(\mathbf{x})$. In this work, we formalize andstudy two precise notions of feature importance for general machine learningmodels: sufficiency and necessity. We demonstrate how these two types ofexplanations, albeit intuitive and simple, can fall short in providing acomplete picture of which features a model finds important. To this end, wepropose a unified notion of importance that circumvents these limitations byexploring a continuum along a necessity-sufficiency axis. Our unified notion,we show, has strong ties to other popular definitions of feature importance,like those based on conditional independence and game-theoretic quantities likeShapley values. Crucially, we demonstrate how a unified perspective allows usto detect important features that could be missed by either of the previousapproaches alone.</description><author>Beepul Bharti, Paul Yi, Jeremias Sulam</author><pubDate>Mon, 30 Sep 2024 15:50:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20427v1</guid></item><item><title>Navigating Threats: A Survey of Physical Adversarial Attacks on LiDAR Perception Systems in Autonomous Vehicles</title><link>http://arxiv.org/abs/2409.20426v1</link><description>Autonomous vehicles (AVs) rely heavily on LiDAR (Light Detection and Ranging)systems for accurate perception and navigation, providing high-resolution 3Denvironmental data that is crucial for object detection and classification.However, LiDAR systems are vulnerable to adversarial attacks, which posesignificant challenges to the safety and robustness of AVs. This surveypresents a thorough review of the current research landscape on physicaladversarial attacks targeting LiDAR-based perception systems, covering bothsingle-modality and multi-modality contexts. We categorize and analyze variousattack types, including spoofing and physical adversarial object attacks,detailing their methodologies, impacts, and potential real-world implications.Through detailed case studies and analyses, we identify critical challenges andhighlight gaps in existing attacks for LiDAR-based systems. Additionally, wepropose future research directions to enhance the security and resilience ofthese systems, ultimately contributing to the safer deployment of autonomousvehicles.</description><author>Amira Guesmi, Muhammad Shafique</author><pubDate>Mon, 30 Sep 2024 15:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20426v1</guid></item><item><title>World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering</title><link>http://arxiv.org/abs/2409.20424v1</link><description>Recent advances in Vision-Language Models (VLMs) and the scarcity ofhigh-quality multi-modal alignment data have inspired numerous researches onsynthetic VLM data generation. The conventional norm in VLM data constructionuses a mixture of specialists in caption and OCR, or stronger VLM APIs andexpensive human annotation. In this paper, we present World to Code (W2C), ameticulously curated multi-modal data construction pipeline that organizes thefinal generation output into a Python code format. The pipeline leverages theVLM itself to extract cross-modal information via different prompts and filterthe generated outputs again via a consistency filtering strategy. Experimentshave demonstrated the high quality of W2C by improving various existing visualquestion answering and visual grounding benchmarks across different VLMs.Further analysis also demonstrates that the new code parsing ability of VLMspresents better cross-modal equivalence than the commonly used detail captionability. Our code is available athttps://github.com/foundation-multimodal-models/World2Code.</description><author>Jiacong Wang, Bohong Wu, Haiyong Jiang, Xun Zhou, Xin Xiao, Haoyuan Guo, Jun Xiao</author><pubDate>Mon, 30 Sep 2024 15:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20424v1</guid></item><item><title>Stream-level flow matching from a Bayesian decision theoretic perspective</title><link>http://arxiv.org/abs/2409.20423v1</link><description>Flow matching (FM) is a family of training algorithms for fitting continuousnormalizing flows (CNFs). A standard approach to FM, called conditional flowmatching (CFM), exploits the fact that the marginal vector field of a CNF canbe learned by fitting least-square regression to the so-called conditionalvector field specified given one or both ends of the flow path. We show thatviewing CFM training from a Bayesian decision theoretic perspective onparameter estimation opens the door to generalizations of CFM algorithms. Wepropose one such extension by introducing a CFM algorithm based on definingconditional probability paths given what we refer to as ``streams'', instancesof latent stochastic paths that connect pairs of noise and observed data.Further, we advocates the modeling of these latent streams using Gaussianprocesses (GPs). The unique distributional properties of GPs, and in particularthe fact that the velocities of a GP is still a GP, allows drawing samples fromthe resulting stream-augmented conditional probability path without simulatingthe actual streams, and hence the ``simulation-free" nature of CFM training ispreserved. We show that this generalization of the CFM can substantially reducethe variance in the estimated marginal vector field at a moderate computationalcost, thereby improving the quality of the generated samples under commonmetrics. Additionally, we show that adopting the GP on the streams allows forflexibly linking multiple related training data points (e.g., time series) andincorporating additional prior information. We empirically validate our claimthrough both simulations and applications to two hand-written image datasets.</description><author>Ganchao Wei, Li Ma</author><pubDate>Mon, 30 Sep 2024 15:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20423v1</guid></item><item><title>MAMOC: MRI Motion Correction via Masked Autoencoding</title><link>http://arxiv.org/abs/2405.14590v2</link><description>The presence of motion artifacts in magnetic resonance imaging (MRI) scansposes a significant challenge, where even minor patient movements can lead toartifacts that may compromise the scan's utility.This paper introduces MAskedMOtion Correction (MAMOC), a novel method designed to address the issue ofRetrospective Artifact Correction (RAC) in motion-affected MRI brain scans.MAMOC uses masked autoencoding self-supervision, transfer learning andtest-time prediction to efficiently remove motion artifacts, producinghigh-fidelity, native-resolution scans. Until recently, realistic, openlyavailable paired artifact presentations for training and evaluatingretrospective motion correction methods did not exist, making it necessary tosimulate motion artifacts. Leveraging the MR-ART dataset and bigger unlabeleddatasets (ADNI, OASIS-3, IXI), this work is the first to evaluate motioncorrection in MRI scans using real motion data on a public dataset, showingthat MAMOC achieves improved performance over existing motion correctionmethods.</description><author>Lennart Alexander Van der Goten, Jingyu Guo, Kevin Smith</author><pubDate>Mon, 30 Sep 2024 15:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14590v2</guid></item><item><title>AI-Based Fully Automatic Analysis of Retinal Vascular Morphology in Pediatric High Myopia</title><link>http://arxiv.org/abs/2409.20419v1</link><description>Purpose: To investigate the changes in retinal vascular structures associatedvarious stages of myopia by designing automated software based on an artifintelligencemodel. Methods: The study involved 1324 pediatric participants fromthe National Childr Medical Center in China, and 2366 high-quality retinalimages and correspon refractive parameters were obtained and analyzed.Spherical equivalent refrac(SER) degree was calculated. We proposed a dataanalysis model based c combination of the Convolutional Neural Networks (CNN)model and the atter module to classify images, segment vascular structures, andmeasure vasc parameters, such as main angle (MA), branching angle (BA),bifurcation edge al(BEA) and bifurcation edge coefficient (BEC). One-way ANOVAcompared param measurements betweenthenormalfundus,lowmyopia,moderatemyopia,and high myopia group. Results: There were 279 (12.38%) images in normalgroup and 384 (16.23%) images in the high myopia group. Compared normal fundus,the MA of fundus vessels in different myopic refractive groups significantlyreduced (P = 0.006, P = 0.004, P = 0.019, respectively), and performance of thevenous system was particularly obvious (P&lt;0.001). At the sa time, the BECdecreased disproportionately (P&lt;0.001). Further analysis of fundus vascularparameters at different degrees of myopia showed that there were alsosignificant differences in BA and branching coefficient (BC). The arterial BAvalue of the fundus vessel in the high myopia group was lower than that ofother groups (P : 0.032, 95% confidence interval [Ci], 0.22-4.86), while thevenous BA values increased(P = 0.026). The BEC values of high myopia werehigher than those of low and moderate myopia groups. When the loss function ofour data classification model converged to 0.09,the model accuracy reached94.19%</description><author>Yinzheng Zhao, Zhihao Zhao, Junjie Yang, Li Li, M. Ali Nasseri, Daniel Zapp</author><pubDate>Mon, 30 Sep 2024 15:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20419v1</guid></item><item><title>KANDU-Net:A Dual-Channel U-Net with KAN for Medical Image Segmentation</title><link>http://arxiv.org/abs/2409.20414v1</link><description>The U-Net model has consistently demonstrated strong performance in the fieldof medical image segmentation, with various improvements and enhancements madesince its introduction. This paper presents a novel architecture thatintegrates KAN networks with U-Net, leveraging the powerful nonlinearrepresentation capabilities of KAN networks alongside the established strengthsof U-Net. We introduce a KAN-convolution dual-channel structure that enablesthe model to more effectively capture both local and global features. Weexplore effective methods for fusing features extracted by KAN with thoseobtained through convolutional layers, utilizing an auxiliary network tofacilitate this integration process. Experiments conducted across multipledatasets show that our model performs well in terms of accuracy, indicatingthat the KAN-convolution dual-channel approach has significant potential inmedical image segmentation tasks.</description><author>Chenglin Fang, Kaigui Wu</author><pubDate>Mon, 30 Sep 2024 15:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20414v1</guid></item><item><title>Automatic Classification of Subjective Time Perception Using Multi-modal Physiological Data of Air Traffic Controllers</title><link>http://arxiv.org/abs/2404.15213v3</link><description>In high-pressure environments where human individuals must simultaneouslymonitor multiple entities, communicate effectively, and maintain intense focus,the perception of time becomes a critical factor influencing performance andwell-being. One indicator of well-being can be the person's subjective timeperception. In our project $ChronoPilot$, we aim to develop a device thatmodulates human subjective time perception. In this study, we present a methodto automatically assess the subjective time perception of air trafficcontrollers, a group often faced with demanding conditions, using theirphysiological data and eleven state-of-the-art machine learning classifiers.The physiological data consist of photoplethysmogram, electrodermal activity,and temperature data. We find that the support vector classifier works bestwith an accuracy of 79 % and electrodermal activity provides the mostdescriptive biomarker. These findings are an important step towards closing thefeedback loop of our $ChronoPilot$-device to automatically modulate the user'ssubjective time perception. This technological advancement may promiseimprovements in task management, stress reduction, and overall productivity inhigh-stakes professions.</description><author>Till Aust, Eirini Balta, Argiro Vatakis, Heiko Hamann</author><pubDate>Mon, 30 Sep 2024 15:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15213v3</guid></item><item><title>Novel machine learning applications at the LHC</title><link>http://arxiv.org/abs/2409.20413v1</link><description>Machine learning (ML) is a rapidly growing area of research in the field ofparticle physics, with a vast array of applications at the CERN LHC. ML haschanged the way particle physicists conduct searches and measurements as aversatile tool used to improve existing approaches and enable fundamentally newones. In these proceedings, we describe novel ML techniques and recent resultsfor improved classification, fast simulation, unfolding, and anomaly detectionin LHC experiments.</description><author>Javier M. Duarte</author><pubDate>Mon, 30 Sep 2024 15:40:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20413v1</guid></item><item><title>Conformal Prediction for Dose-Response Models with Continuous Treatments</title><link>http://arxiv.org/abs/2409.20412v1</link><description>Understanding the dose-response relation between a continuous treatment andthe outcome for an individual can greatly drive decision-making, particularlyin areas like personalized drug dosing and personalized healthcareinterventions. Point estimates are often insufficient in these high-riskenvironments, highlighting the need for uncertainty quantification to supportinformed decisions. Conformal prediction, a distribution-free andmodel-agnostic method for uncertainty quantification, has seen limitedapplication in continuous treatments or dose-response models. To address thisgap, we propose a novel methodology that frames the causal dose-responseproblem as a covariate shift, leveraging weighted conformal prediction. Byincorporating propensity estimation, conformal predictive systems, andlikelihood ratios, we present a practical solution for generating predictionintervals for dose-response models. Additionally, our method approximates localcoverage for every treatment value by applying kernel functions as weights inweighted conformal prediction. Finally, we use a new synthetic benchmarkdataset to demonstrate the significance of covariate shift assumptions inachieving robust prediction intervals for dose-response models.</description><author>Jarne Verhaeghe, Jef Jonkers, Sofie Van Hoecke</author><pubDate>Mon, 30 Sep 2024 15:40:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20412v1</guid></item><item><title>LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs</title><link>http://arxiv.org/abs/2409.14744v2</link><description>Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due todiverse potential answers and no objective criterion. The commonly usedautomatic evaluation metrics like ROUGE or BERTScore cannot accurately measuresemantic similarities or answers from different perspectives. Recently, LargeLanguage Models (LLMs) have been resorted to for NFQA evaluation due to theircompelling performance on various NLP tasks. Common approaches includepointwise scoring of each candidate answer and pairwise comparisons betweenanswers. Inspired by the evolution from pointwise to pairwise to listwise inlearning-to-rank methods, we propose a novel listwise NFQA evaluation approach,that utilizes LLMs to rank candidate answers in a list of reference answerssorted by descending quality. Moreover, for NF questions that do not havemulti-grade or any golden answers, we leverage LLMs to generate the referenceanswer list of various quality to facilitate the listwise evaluation. Extensiveexperimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, andWebGLM show that our method has significantly higher correlations with humanannotations compared to automatic scores and common pointwise and pairwiseapproaches.</description><author>Sihui Yang, Keping Bi, Wanqing Cui, Jiafeng Guo, Xueqi Cheng</author><pubDate>Mon, 30 Sep 2024 15:36:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14744v2</guid></item><item><title>Physics-Regularized Multi-Modal Image Assimilation for Brain Tumor Localization</title><link>http://arxiv.org/abs/2409.20409v1</link><description>Physical models in the form of partial differential equations represent animportant prior for many under-constrained problems. One example is tumortreatment planning, which heavily depends on accurate estimates of the spatialdistribution of tumor cells in a patient's anatomy. Medical imaging scans canidentify the bulk of the tumor, but they cannot reveal its full spatialdistribution. Tumor cells at low concentrations remain undetectable, forexample, in the most frequent type of primary brain tumors, glioblastoma.Deep-learning-based approaches fail to estimate the complete tumor celldistribution due to a lack of reliable training data. Most existing workstherefore rely on physics-based simulations to match observed tumors, providinganatomically and physiologically plausible estimations. However, theseapproaches struggle with complex and unknown initial conditions and are limitedby overly rigid physical models. In this work, we present a novel method thatbalances data-driven and physics-based cost functions. In particular, wepropose a unique discretization scheme that quantifies the adherence of ourlearned spatiotemporal tumor and brain tissue distributions to theircorresponding growth and elasticity equations. This quantification, serving asa regularization term rather than a hard constraint, enables greaterflexibility and proficiency in assimilating patient data than existing models.We demonstrate improved coverage of tumor recurrence areas compared to existingtechniques on real-world data from a cohort of patients. The method holds thepotential to enhance clinical adoption of model-driven treatment planning forglioblastoma.</description><author>Michal Balcerak, Tamaz Amiranashvili, Andreas Wagner, Jonas Weidner, Petr Karnakov, Johannes C. Paetzold, Ivan Ezhov, Petros Koumoutsakos, Benedikt Wiestler, Bjoern Menze</author><pubDate>Mon, 30 Sep 2024 15:36:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20409v1</guid></item><item><title>Open-Source Periorbital Segmentation Dataset for Ophthalmic Applications</title><link>http://arxiv.org/abs/2409.20407v1</link><description>Periorbital segmentation and distance prediction using deep learning allowsfor the objective quantification of disease state, treatment monitoring, andremote medicine. However, there are currently no reports of segmentationdatasets for the purposes of training deep learning models with sub mm accuracyon the regions around the eyes. All images (n=2842) had the iris, sclera, lid,caruncle, and brow segmented by five trained annotators. Here, we validate thisdataset through intra and intergrader reliability tests and show the utility ofthe data in training periorbital segmentation networks. All the annotations arepublicly available for free download. Having access to segmentation datasetsdesigned specifically for oculoplastic surgery will permit more rapiddevelopment of clinically useful segmentation networks which can be leveragedfor periorbital distance prediction and disease classification. In addition tothe annotations, we also provide an open-source toolkit for periorbitaldistance prediction from segmentation masks. The weights of all models havealso been open-sourced and are publicly available for use by the community.</description><author>George R. Nahass, Emma Koehler, Nicholas Tomaras, Danny Lopez, Madison Cheung, Alexander Palacios, Jefferey Peterson, Sacha Hubschman, Kelsey Green, Chad A. Purnell, Pete Setabutr, Ann Q. Tran, Darvin Yi</author><pubDate>Mon, 30 Sep 2024 15:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20407v1</guid></item><item><title>Accelerating PoT Quantization on Edge Devices</title><link>http://arxiv.org/abs/2409.20403v1</link><description>Non-uniform quantization, such as power-of-two (PoT) quantization, matchesdata distributions better than uniform quantization, which reduces thequantization error of Deep Neural Networks (DNNs). PoT quantization also allowsbit-shift operations to replace multiplications, but there are limited studieson the efficiency of shift-based accelerators for PoT quantization.Furthermore, existing pipelines for accelerating PoT-quantized DNNs on edgedevices are not open-source. In this paper, we first design shift-basedprocessing elements (shift-PE) for different PoT quantization methods andevaluate their efficiency using synthetic benchmarks. Then we design ashift-based accelerator using our most efficient shift-PE and propose PoTAcc,an open-source pipeline for end-to-end acceleration of PoT-quantized DNNs onresource-constrained edge devices. Using PoTAcc, we evaluate the performance ofour shift-based accelerator across three DNNs. On average, it achieves a 1.23xspeedup and 1.24x energy reduction compared to a multiplier-based accelerator,and a 2.46x speedup and 1.83x energy reduction compared to CPU-only execution.Our code is available at https://github.com/gicLAB/PoTAcc</description><author>Rappy Saha, Jude Haris, José Cano</author><pubDate>Mon, 30 Sep 2024 15:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20403v1</guid></item><item><title>AUCSeg: AUC-oriented Pixel-level Long-tail Semantic Segmentation</title><link>http://arxiv.org/abs/2409.20398v1</link><description>The Area Under the ROC Curve (AUC) is a well-known metric for evaluatinginstance-level long-tail learning problems. In the past two decades, many AUCoptimization methods have been proposed to improve model performance underlong-tail distributions. In this paper, we explore AUC optimization methods inthe context of pixel-level long-tail semantic segmentation, a much morecomplicated scenario. This task introduces two major challenges for AUCoptimization techniques. On one hand, AUC optimization in a pixel-level taskinvolves complex coupling across loss terms, with structured inner-image andpairwise inter-image dependencies, complicating theoretical analysis. On theother hand, we find that mini-batch estimation of AUC loss in this caserequires a larger batch size, resulting in an unaffordable space complexity. Toaddress these issues, we develop a pixel-level AUC loss function and conduct adependency-graph-based theoretical analysis of the algorithm's generalizationability. Additionally, we design a Tail-Classes Memory Bank (T-Memory Bank) tomanage the significant memory demand. Finally, comprehensive experiments acrossvarious benchmarks confirm the effectiveness of our proposed AUCSeg method. Thecode is available at https://github.com/boyuh/AUCSeg.</description><author>Boyu Han, Qianqian Xu, Zhiyong Yang, Shilong Bao, Peisong Wen, Yangbangyan Jiang, Qingming Huang</author><pubDate>Mon, 30 Sep 2024 15:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20398v1</guid></item><item><title>Physics-Informed Neural Networks for Dynamic Process Operations with Limited Physical Knowledge and Data</title><link>http://arxiv.org/abs/2406.01528v3</link><description>In chemical engineering, process data are expensive to acquire, and complexphenomena are difficult to fully model. We explore the use of physics-informedneural networks (PINNs) for modeling dynamic processes with incompletemechanistic semi-explicit differential-algebraic equation systems and scarceprocess data. In particular, we focus on estimating states for which neitherdirect observational data nor constitutive equations are available. We proposean easy-to-apply heuristic to assess whether estimation of such states may bepossible. As numerical examples, we consider a continuously stirred tankreactor and a liquid-liquid separator. We find that PINNs can inferimmeasurable states with reasonable accuracy, even if respective constitutiveequations are unknown. We thus show that PINNs are capable of modelingprocesses when relatively few experimental data and only partially knownmechanistic descriptions are available, and conclude that they constitute apromising avenue that warrants further investigation.</description><author>Mehmet Velioglu, Song Zhai, Sophia Rupprecht, Alexander Mitsos, Andreas Jupke, Manuel Dahmen</author><pubDate>Mon, 30 Sep 2024 15:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01528v3</guid></item><item><title>Pessimistic Iterative Planning for Robust POMDPs</title><link>http://arxiv.org/abs/2408.08770v2</link><description>Robust partially observable Markov decision processes (robust POMDPs) extendclassical POMDPs to handle additional uncertainty on the transition andobservation probabilities via so-called uncertainty sets. Policies for robustPOMDPs must not only be memory-based to account for partial observability butalso robust against model uncertainty to account for the worst-case instancesfrom the uncertainty sets. We propose the pessimistic iterative planning (PIP)framework, which finds robust memory-based policies for robust POMDPs. PIPalternates between two main steps: (1) selecting an adversarial (non-robust)POMDP via worst-case probability instances from the uncertainty sets; and (2)computing a finite-state controller (FSC) for this adversarial POMDP. Weevaluate the performance of this FSC on the original robust POMDP and use thisevaluation in step (1) to select the next adversarial POMDP. Within PIP, wepropose the rFSCNet algorithm. In each iteration, rFSCNet finds an FSC througha recurrent neural network by using supervision policies optimized for theadversarial POMDP. The empirical evaluation in four benchmark environmentsshowcases improved robustness against several baseline methods and competitiveperformance compared to a state-of-the-art robust POMDP solver.</description><author>Maris F. L. Galesloot, Marnix Suilen, Thiago D. Simão, Steven Carr, Matthijs T. J. Spaan, Ufuk Topcu, Nils Jansen</author><pubDate>Mon, 30 Sep 2024 15:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.08770v2</guid></item><item><title>RLocator: Reinforcement Learning for Bug Localization</title><link>http://arxiv.org/abs/2305.05586v3</link><description>Software developers spend a significant portion of time fixing bugs in theirprojects. To streamline this process, bug localization approaches have beenproposed to identify the source code files that are likely responsible for aparticular bug. Prior work proposed several similarity-based machine-learningtechniques for bug localization. Despite significant advances in thesetechniques, they do not directly optimize the evaluation measures. We arguethat directly optimizing evaluation measures can positively contribute to theperformance of bug localization approaches. Therefore, In this paper, weutilize Reinforcement Learning (RL) techniques to directly optimize the rankingmetrics. We propose RLocator, a Reinforcement Learning-based bug localizationapproach. We formulate RLocator using a Markov Decision Process (MDP) tooptimize the evaluation measures directly. We present the technique andexperimentally evaluate it based on a benchmark dataset of 8,316 bug reportsfrom six highly popular Apache projects. The results of our evaluation revealthat RLocator achieves a Mean Reciprocal Rank (MRR) of 0.62, a Mean AveragePrecision (MAP) of 0.59, and a Top 1 score of 0.46. We compare RLocator withtwo state-of-the-art bug localization tools, FLIM and BugLocator. Ourevaluation reveals that RLocator outperforms both approaches by a substantialmargin, with improvements of 38.3% in MAP, 36.73% in MRR, and 23.68% in the TopK metric. These findings highlight that directly optimizing evaluation measuresconsiderably contributes to performance improvement of the bug localizationproblem.</description><author>Partha Chakraborty, Mahmoud Alfadel, Meiyappan Nagappan</author><pubDate>Mon, 30 Sep 2024 15:29:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05586v3</guid></item><item><title>Anti-stereotypical Predictive Text Suggestions Do Not Reliably Yield Anti-stereotypical Writing</title><link>http://arxiv.org/abs/2409.20390v1</link><description>AI-based systems such as language models can replicate and amplify socialbiases reflected in their training data. Among other questionable behavior,this can lead to LM-generated text--and text suggestions--that containnormatively inappropriate stereotypical associations. In this paper, weconsider the question of how "debiasing" a language model impacts stories thatpeople write using that language model in a predictive text scenario. We findthat (n=414), in certain scenarios, language model suggestions that align withcommon social stereotypes are more likely to be accepted by human authors.Conversely, although anti-stereotypical language model suggestions sometimeslead to an increased rate of anti-stereotypical stories, this influence is farfrom sufficient to lead to "fully debiased" stories.</description><author>Connor Baumler, Hal Daumé III</author><pubDate>Mon, 30 Sep 2024 15:21:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20390v1</guid></item><item><title>Wait, but Tylenol is Acetaminophen... Investigating and Improving Language Models' Ability to Resist Requests for Misinformation</title><link>http://arxiv.org/abs/2409.20385v1</link><description>Background: Large language models (LLMs) are trained to follow directions,but this introduces a vulnerability to blindly comply with user requests evenif they generate wrong information. In medicine, this could accelerate thegeneration of misinformation that impacts human well-being. Objectives/Methods: We analyzed compliance to requests to generate misleadingcontent about medications in settings where models know the request isillogical. We investigated whether in-context directions and instruction-tuningof LLMs to prioritize logical reasoning over compliance reduced misinformationrisk. Results: While all frontier LLMs complied with misinformation requests, bothprompt-based and parameter-based approaches can improve the detection of logicflaws in requests and prevent the dissemination of medical misinformation. Conclusion: Shifting LLMs to prioritize logic over compliance could reducerisks of exploitation for medical misinformation.</description><author>Shan Chen, Mingye Gao, Kuleen Sasse, Thomas Hartvigsen, Brian Anthony, Lizhou Fan, Hugo Aerts, Jack Gallifant, Danielle Bitterman</author><pubDate>Mon, 30 Sep 2024 15:20:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20385v1</guid></item><item><title>FireLite: Leveraging Transfer Learning for Efficient Fire Detection in Resource-Constrained Environments</title><link>http://arxiv.org/abs/2409.20384v1</link><description>Fire hazards are extremely dangerous, particularly in sectors such as thetransportation industry, where political unrest increases the likelihood oftheir occurrence. By employing IP cameras to facilitate the setup of firedetection systems on transport vehicles, losses from fire events may beprevented proactively. However, the development of lightweight fire detectionmodels is required due to the computational constraints of the embedded systemswithin these cameras. We introduce FireLite, a low-parameter convolutionalneural network (CNN) designed for quick fire detection in contexts with limitedresources, in response to this difficulty. With an accuracy of 98.77\%, ourmodel -- which has just 34,978 trainable parameters achieves remarkableperformance numbers. It also shows a validation loss of 8.74 and peaks at 98.77for precision, recall, and F1-score measures. Because of its precision andefficiency, FireLite is a promising solution for fire detection inresource-constrained environments.</description><author>Mahamudul Hasan, Md Maruf Al Hossain Prince, Mohammad Samar Ansari, Sabrina Jahan, Abu Saleh Musa Miah, Jungpil Shin</author><pubDate>Mon, 30 Sep 2024 15:20:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20384v1</guid></item><item><title>Beyond Derivative Pathology of PINNs: Variable Splitting Strategy with Convergence Analysis</title><link>http://arxiv.org/abs/2409.20383v1</link><description>Physics-informed neural networks (PINNs) have recently emerged as effectivemethods for solving partial differential equations (PDEs) in various problems.Substantial research focuses on the failure modes of PINNs due to theirfrequent inaccuracies in predictions. However, most are based on the premisethat minimizing the loss function to zero causes the network to converge to asolution of the governing PDE. In this study, we prove that PINNs encounter afundamental issue that the premise is invalid. We also reveal that this issuestems from the inability to regulate the behavior of the derivatives of thepredicted solution. Inspired by the \textit{derivative pathology} of PINNs, wepropose a \textit{variable splitting} strategy that addresses this issue byparameterizing the gradient of the solution as an auxiliary variable. Wedemonstrate that using the auxiliary variable eludes derivative pathology byenabling direct monitoring and regulation of the gradient of the predictedsolution. Moreover, we prove that the proposed method guarantees convergence toa generalized solution for second-order linear PDEs, indicating itsapplicability to various problems.</description><author>Yesom Park, Changhoon Song, Myungjoo Kang</author><pubDate>Mon, 30 Sep 2024 15:20:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20383v1</guid></item><item><title>Word-wise intonation model for cross-language TTS systems</title><link>http://arxiv.org/abs/2409.20374v1</link><description>In this paper we propose a word-wise intonation model for Russian languageand show how it can be generalized for other languages. The proposed model issuitable for automatic data markup and its extended application totext-to-speech systems. It can also be implemented for an intonation contourmodeling by using rule-based algorithms or by predicting contours with languagemodels. The key idea is a partial elimination of the variability connected withdifferent placements of a stressed syllable in a word. It is achieved withsimultaneous applying of pitch simplification with a dynamic time warpingclustering. The proposed model could be used as a tool for intonation researchor as a backbone for prosody description in text-to-speech systems. As theadvantage of the model, we show its relations with the existing intonationsystems as well as the possibility of using language models for prosodyprediction. Finally, we demonstrate some practical evidence of the systemrobustness to parameter variations.</description><author>Tomilov A. A., Gromova A. Y., Svischev A. N</author><pubDate>Mon, 30 Sep 2024 15:09:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20374v1</guid></item><item><title>Manifold-Constrained Nucleus-Level Denoising Diffusion Model for Structure-Based Drug Design</title><link>http://arxiv.org/abs/2409.10584v2</link><description>Artificial intelligence models have shown great potential in structure-baseddrug design, generating ligands with high binding affinities. However, existingmodels have often overlooked a crucial physical constraint: atoms must maintaina minimum pairwise distance to avoid separation violation, a phenomenongoverned by the balance of attractive and repulsive forces. To mitigate suchseparation violations, we propose NucleusDiff. It models the interactionsbetween atomic nuclei and their surrounding electron clouds by enforcing thedistance constraint between the nuclei and manifolds. We quantitativelyevaluate NucleusDiff using the CrossDocked2020 dataset and a COVID-19therapeutic target, demonstrating that NucleusDiff reduces violation rate by upto 100.00% and enhances binding affinity by up to 22.16%, surpassingstate-of-the-art models for structure-based drug design. We also providequalitative analysis through manifold sampling, visually confirming theeffectiveness of NucleusDiff in reducing separation violations and improvingbinding affinities.</description><author>Shengchao Liu, Divin Yan, Weitao Du, Weiyang Liu, Zhuoxinran Li, Hongyu Guo, Christian Borgs, Jennifer Chayes, Anima Anandkumar</author><pubDate>Mon, 30 Sep 2024 15:09:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10584v2</guid></item><item><title>Subgraph Clustering and Atom Learning for Improved Image Classification</title><link>http://arxiv.org/abs/2407.14772v2</link><description>In this study, we present the Graph Sub-Graph Network (GSN), a novel hybridimage classification model merging the strengths of Convolutional NeuralNetworks (CNNs) for feature extraction and Graph Neural Networks (GNNs) forstructural modeling. GSN employs k-means clustering to group graph nodes intoclusters, facilitating the creation of subgraphs. These subgraphs are thenutilized to learn representative `atoms` for dictionary learning, enabling theidentification of sparse, class-distinguishable features. This integratedapproach is particularly relevant in domains like medical imaging, wherediscerning subtle feature differences is crucial for accurate classification. To evaluate the performance of our proposed GSN, we conducted experiments onbenchmark datasets, including PascalVOC and HAM10000. Our results demonstratethe efficacy of our model in optimizing dictionary configurations across variedclasses, which contributes to its effectiveness in medical classificationtasks. This performance enhancement is primarily attributed to the integrationof CNNs, GNNs, and graph learning techniques, which collectively improve thehandling of datasets with limited labeled examples. Specifically, ourexperiments show that the model achieves a higher accuracy on benchmarkdatasets such as Pascal VOC and HAM10000 compared to conventional CNNapproaches.</description><author>Aryan Singh, Pepijn Van de Ven, Ciarán Eising, Patrick Denny</author><pubDate>Mon, 30 Sep 2024 15:08:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14772v2</guid></item><item><title>Frequency Adaptive Normalization For Non-stationary Time Series Forecasting</title><link>http://arxiv.org/abs/2409.20371v1</link><description>Time series forecasting typically needs to address non-stationary data withevolving trend and seasonal patterns. To address the non-stationarity,reversible instance normalization has been recently proposed to alleviateimpacts from the trend with certain statistical measures, e.g., mean andvariance. Although they demonstrate improved predictive accuracy, they arelimited to expressing basic trends and are incapable of handling seasonalpatterns. To address this limitation, this paper proposes a new instancenormalization solution, called frequency adaptive normalization (FAN), whichextends instance normalization in handling both dynamic trend and seasonalpatterns. Specifically, we employ the Fourier transform to identifyinstance-wise predominant frequent components that cover most non-stationaryfactors. Furthermore, the discrepancy of those frequency components betweeninputs and outputs is explicitly modeled as a prediction task with a simple MLPmodel. FAN is a model-agnostic method that can be applied to arbitrarypredictive backbones. We instantiate FAN on four widely used forecasting modelsas the backbone and evaluate their prediction performance improvements on eightbenchmark datasets. FAN demonstrates significant performance advancement,achieving 7.76% ~ 37.90% average improvements in MSE.</description><author>Weiwei Ye, Songgaojun Deng, Qiaosha Zou, Ning Gui</author><pubDate>Mon, 30 Sep 2024 15:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20371v1</guid></item><item><title>The Perfect Blend: Redefining RLHF with Mixture of Judges</title><link>http://arxiv.org/abs/2409.20370v1</link><description>Reinforcement learning from human feedback (RLHF) has become the leadingapproach for fine-tuning large language models (LLM). However, RLHF haslimitations in multi-task learning (MTL) due to challenges of reward hackingand extreme multi-objective optimization (i.e., trade-off of multiple and/orsometimes conflicting objectives). Applying RLHF for MTL currently requirescareful tuning of the weights for reward model and data combinations. This isoften done via human intuition and does not generalize. In this work, weintroduce a novel post-training paradigm which we called Constrained GenerativePolicy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) withcost-efficient constrained policy optimization with stratification, which canidentify the perfect blend in RLHF in a principled manner. It shows strongempirical results with theoretical guarantees, does not require extensivehyper-parameter tuning, and is plug-and-play in common post-training pipelines.Together, this can detect and mitigate reward hacking behaviors while reachinga pareto-optimal point across an extremely large number of objectives. Our empirical evaluations demonstrate that CGPO significantly outperformsstandard RLHF algorithms like PPO and DPO across various tasks includinggeneral chat, STEM questions, instruction following, and coding. Specifically,CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% inArena-Hard (STEM &amp; reasoning), and consistent gains in other domains like mathand coding. Notably, PPO, while commonly used, is prone to severe rewardhacking in popular coding benchmarks, which CGPO successfully addresses. Thisbreakthrough in RLHF not only tackles reward hacking and extrememulti-objective optimization challenges but also advances the state-of-the-artin aligning general-purpose LLMs for diverse applications.</description><author>Tengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen Zhu, Hejia Zhang, Wenxuan Zhou, Zhouhao Zeng, Yun He, Karishma Mandyam, Arya Talabzadeh, Madian Khabsa, Gabriel Cohen, Yuandong Tian, Hao Ma, Sinong Wang, Han Fang</author><pubDate>Mon, 30 Sep 2024 15:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20370v1</guid></item><item><title>Disentangling Singlish Discourse Particles with Task-Driven Representation</title><link>http://arxiv.org/abs/2409.20366v1</link><description>Singlish, or formally Colloquial Singapore English, is an English-basedcreole language originating from the SouthEast Asian country Singapore. Thelanguage contains influences from Sinitic languages such as Chinese dialects,Malay, Tamil and so forth. A fundamental task to understanding Singlish is tofirst understand the pragmatic functions of its discourse particles, upon whichSinglish relies heavily to convey meaning. This work offers a preliminaryeffort to disentangle the Singlish discourse particles (lah, meh and hor) withtask-driven representation learning. After disentanglement, we cluster thesediscourse particles to differentiate their pragmatic functions, and performSinglish-to-English machine translation. Our work provides a computationalmethod to understanding Singlish discourse particles, and opens avenues towardsa deeper comprehension of the language and its usage.</description><author>Linus Tze En Foo, Lynnette Hui Xian Ng</author><pubDate>Mon, 30 Sep 2024 15:04:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20366v1</guid></item><item><title>VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs</title><link>http://arxiv.org/abs/2409.20365v1</link><description>In the video-language domain, recent works in leveraging zero-shot LargeLanguage Model-based reasoning for video understanding have become competitivechallengers to previous end-to-end models. However, long video understandingpresents unique challenges due to the complexity of reasoning over extendedtimespans, even for zero-shot LLM-based approaches. The challenge ofinformation redundancy in long videos prompts the question of what specificinformation is essential for large language models (LLMs) and how to leveragethem for complex spatial-temporal reasoning in long-form video analysis. Wepropose a framework VideoINSTA, i.e. INformative Spatial-TemporAl Reasoning forzero-shot long-form video understanding. VideoINSTA contributes (1) a zero-shotframework for long video understanding using LLMs; (2) an event-based temporalreasoning and content-based spatial reasoning approach for LLMs to reason overspatial-temporal information in videos; (3) a self-reflective informationreasoning scheme balancing temporal factors based on information sufficiencyand prediction confidence. Our model significantly improves thestate-of-the-art on three long video question-answering benchmarks: EgoSchema,NextQA, and IntentQA, and the open question answering dataset ActivityNetQA.The code is released here: https://github.com/mayhugotong/VideoINSTA.</description><author>Ruotong Liao, Max Erler, Huiyu Wang, Guangyao Zhai, Gengyuan Zhang, Yunpu Ma, Volker Tresp</author><pubDate>Mon, 30 Sep 2024 15:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20365v1</guid></item><item><title>Efficient Driving Behavior Narration and Reasoning on Edge Device Using Large Language Models</title><link>http://arxiv.org/abs/2409.20364v1</link><description>Deep learning architectures with powerful reasoning capabilities have drivensignificant advancements in autonomous driving technology. Large languagemodels (LLMs) applied in this field can describe driving scenes and behaviorswith a level of accuracy similar to human perception, particularly in visualtasks. Meanwhile, the rapid development of edge computing, with its advantageof proximity to data sources, has made edge devices increasingly important inautonomous driving. Edge devices process data locally, reducing transmissiondelays and bandwidth usage, and achieving faster response times. In this work,we propose a driving behavior narration and reasoning framework that appliesLLMs to edge devices. The framework consists of multiple roadside units, withLLMs deployed on each unit. These roadside units collect road data andcommunicate via 5G NSR/NR networks. Our experiments show that LLMs deployed onedge devices can achieve satisfactory response speeds. Additionally, we proposea prompt strategy to enhance the narration and reasoning performance of thesystem. This strategy integrates multi-modal information, includingenvironmental, agent, and motion data. Experiments conducted on theOpenDV-Youtube dataset demonstrate that our approach significantly improvesperformance across both tasks.</description><author>Yizhou Huang, Yihua Cheng, Kezhi Wang</author><pubDate>Mon, 30 Sep 2024 15:03:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20364v1</guid></item><item><title>Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4 inference</title><link>http://arxiv.org/abs/2409.20361v1</link><description>Large language models have demonstrated promising capabilities upon scalingup parameters. However, serving large language models incurs substantialcomputation and memory movement costs due to their large scale. Quantizationmethods have been employed to reduce service costs and latency. Nevertheless,outliers in activations hinder the development of INT4 weight-activationquantization. Existing approaches separate outliers and normal values into twomatrices or migrate outliers from activations to weights, suffering from highlatency or accuracy degradation. Based on observing activations from largelanguage models, outliers can be classified into channel-wise and spikeoutliers. In this work, we propose Rotated Runtime Smooth (RRS), aplug-and-play activation smoother for quantization, consisting of RuntimeSmooth and the Rotation operation. Runtime Smooth (RS) is introduced toeliminate channel-wise outliers by smoothing activations with channel-wisemaximums during runtime. The rotation operation can narrow the gap betweenspike outliers and normal values, alleviating the effect of victims caused bychannel-wise smoothing. The proposed method outperforms the state-of-the-artmethod in the LLaMA and Qwen families and improves WikiText-2 perplexity from57.33 to 6.66 for INT4 inference.</description><author>Ke Yi, Zengke Liu, Jianwei Zhang, Chengyuan Li, Tong Zhang, Junyang Lin, Jingren Zhou</author><pubDate>Mon, 30 Sep 2024 14:59:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20361v1</guid></item><item><title>Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence</title><link>http://arxiv.org/abs/2406.10957v3</link><description>Direct Preference Optimization (DPO) has emerged as a prominent algorithm forthe direct and robust alignment of Large Language Models (LLMs) with humanpreferences, offering a more straightforward alternative to the complexReinforcement Learning from Human Feedback (RLHF). Despite its promisingefficacy, DPO faces a notable drawback: "verbosity", a common over-optimizationphenomenon also observed in RLHF. While previous studies mainly attributedverbosity to biased labels within the data, we propose that the issue alsostems from an inherent algorithmic length reliance in DPO. Specifically, wesuggest that the discrepancy between sequence-level Kullback-Leibler (KL)divergences between chosen and rejected sequences, used in DPO, results inoverestimated or underestimated rewards due to varying token lengths.Empirically, we utilize datasets with different label lengths to demonstratethe presence of biased rewards. We then introduce an effective downsamplingapproach, named SamPO, to eliminate potential length reliance. Our experimentalevaluations, conducted across three LLMs of varying scales and a diverse arrayof conditional and open-ended benchmarks, highlight the efficacy of SamPO inmitigating verbosity, achieving improvements of 5% to 12% over DPO throughdebaised rewards. Our codes can be accessed at:https://github.com/LuJunru/SamPO/.</description><author>Junru Lu, Jiazheng Li, Siyu An, Meng Zhao, Yulan He, Di Yin, Xing Sun</author><pubDate>Mon, 30 Sep 2024 14:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10957v3</guid></item><item><title>CableInspect-AD: An Expert-Annotated Anomaly Detection Dataset</title><link>http://arxiv.org/abs/2409.20353v1</link><description>Machine learning models are increasingly being deployed in real-worldcontexts. However, systematic studies on their transferability to specific andcritical applications are underrepresented in the research literature. Animportant example is visual anomaly detection (VAD) for robotic power lineinspection. While existing VAD methods perform well in controlled environments,real-world scenarios present diverse and unexpected anomalies that currentdatasets fail to capture. To address this gap, we introduce$\textit{CableInspect-AD}$, a high-quality, publicly available dataset createdand annotated by domain experts from Hydro-Qu\'ebec, a Canadian public utility.This dataset includes high-resolution images with challenging real-worldanomalies, covering defects with varying severity levels. To address thechallenges of collecting diverse anomalous and nominal examples for setting adetection threshold, we propose an enhancement to the celebrated PatchCorealgorithm. This enhancement enables its use in scenarios with limited labeleddata. We also present a comprehensive evaluation protocol based oncross-validation to assess models' performances. We evaluate our$\textit{Enhanced-PatchCore}$ for few-shot and many-shot detection, andVision-Language Models for zero-shot detection. While promising, these modelsstruggle to detect all anomalies, highlighting the dataset's value as achallenging benchmark for the broader research community. Project page:https://mila-iqia.github.io/cableinspect-ad/.</description><author>Akshatha Arodi, Margaux Luck, Jean-Luc Bedwani, Aldo Zaimi, Ge Li, Nicolas Pouliot, Julien Beaudry, Gaétan Marceau Caron</author><pubDate>Mon, 30 Sep 2024 14:50:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20353v1</guid></item><item><title>AI generated annotations for Breast, Brain, Liver, Lungs and Prostate cancer collections in National Cancer Institute Imaging Data Commons</title><link>http://arxiv.org/abs/2409.20342v1</link><description>AI in Medical Imaging project aims to enhance the National Cancer Institute's(NCI) Image Data Commons (IDC) by developing nnU-Net models and providingAI-assisted segmentations for cancer radiology images. We created high-quality,AI-annotated imaging datasets for 11 IDC collections. These datasets includeimages from various modalities, such as computed tomography (CT) and magneticresonance imaging (MRI), covering the lungs, breast, brain, kidneys, prostate,and liver. The nnU-Net models were trained using open-source datasets. Aportion of the AI-generated annotations was reviewed and corrected byradiologists. Both the AI and radiologist annotations were encoded incompliance with the the Digital Imaging and Communications in Medicine (DICOM)standard, ensuring seamless integration into the IDC collections. All models,images, and annotations are publicly accessible, facilitating further researchand development in cancer imaging. This work supports the advancement ofimaging tools and algorithms by providing comprehensive and accurate annotateddatasets.</description><author>Gowtham Krishnan Murugesan, Diana McCrumb, Rahul Soni, Jithendra Kumar, Leonard Nuernberg, Linmin Pei, Ulrike Wagner, Sutton Granger, Andrey Y. Fedorov, Stephen Moore, Jeff Van Oss</author><pubDate>Mon, 30 Sep 2024 14:43:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20342v1</guid></item><item><title>Gromov-Wasserstein-like Distances in the Gaussian Mixture Models Space</title><link>http://arxiv.org/abs/2310.11256v3</link><description>The Gromov-Wasserstein (GW) distance is frequently used in machine learningto compare distributions across distinct metric spaces. Despite its utility, itremains computationally intensive, especially for large-scale problems.Recently, a novel Wasserstein distance specifically tailored for Gaussianmixture models (GMMs) and known as MW2 (mixture Wasserstein) has beenintroduced by several authors. In scenarios where data exhibit clustering, thisapproach simplifies to a small-scale discrete optimal transport problem, whichcomplexity depends solely on the number of Gaussian components in the GMMs.This paper aims to incorporate invariance properties into MW2. This is done byintroducing new Gromov-type distances, designed to be isometry-invariant inEuclidean spaces and applicable for comparing GMMs across different dimensionalspaces. Our first contribution is the Mixture Gromov Wasserstein distance(MGW2), which can be viewed as a "Gromovized" version of MW2. This new distancehas a straightforward discrete formulation, making it highly efficient forestimating distances between GMMs in practical applications. To facilitate thederivation of a transport plan between GMMs, we present a second distance, theEmbedded Wasserstein distance (EW2). This distance turns out to be closelyrelated to several recent alternatives to Gromov-Wasserstein. We show that EW2can be adapted to derive a distance as well as optimal transportation plansbetween GMMs. We demonstrate the efficiency of these newly proposed distanceson medium to large-scale problems, including shape matching and hyperspectralimage color transfer.</description><author>Antoine Salmona, Julie Delon, Agnès Desolneux</author><pubDate>Mon, 30 Sep 2024 14:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11256v3</guid></item><item><title>Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection</title><link>http://arxiv.org/abs/2401.13551v2</link><description>Video Anomaly Detection (VAD) has been extensively studied under the settingsof One-Class Classification (OCC) and Weakly-Supervised learning (WS), whichhowever both require laborious human-annotated normal/abnormal labels. In thispaper, we study Unsupervised VAD (UVAD) that does not depend on any label bycombining OCC and WS into a unified training framework. Specifically, we extendOCC to weighted OCC (wOCC) and propose a wOCC-WS interleaving training module,where the two models automatically generate pseudo-labels for each other. Weface two challenges to make the combination effective: (1) Models' performancefluctuates occasionally during the training process due to the inevitablerandomness of the pseudo labels. (2) Thresholds are needed to divide pseudolabels, making the training depend on the accuracy of user intervention. Forthe first problem, we propose to use wOCC requiring soft labels instead of OCCtrained with hard zero/one labels, as soft labels exhibit high consistencythroughout different training cycles while hard labels are prone to suddenchanges. For the second problem, we repeat the interleaving training modulemultiple times, during which we propose an adaptive thresholding strategy thatcan progressively refine a rough threshold to a relatively optimal threshold,which reduces the influence of user interaction. A benefit of employing OCC andWS methods to compose a UVAD method is that we can incorporate the most recentOCC or WS model into our framework. Experiments demonstrate the effectivenessof the proposed UVAD framework.</description><author>Yongwei Nie, Hao Huang, Chengjiang Long, Qing Zhang, Pradipta Maji, Hongmin Cai</author><pubDate>Mon, 30 Sep 2024 14:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13551v2</guid></item><item><title>Enhancing GANs with Contrastive Learning-Based Multistage Progressive Finetuning SNN and RL-Based External Optimization</title><link>http://arxiv.org/abs/2409.20340v1</link><description>The application of deep learning in cancer research, particularly in earlydiagnosis, case understanding, and treatment strategy design, emphasizes theneed for high-quality data. Generative AI, especially Generative AdversarialNetworks (GANs), has emerged as a leading solution to challenges like classimbalance, robust learning, and model training, while addressing issuesstemming from patient privacy and the scarcity of real data. Despite theirpromise, GANs face several challenges, both inherent and specific tohistopathology data. Inherent issues include training imbalance, mode collapse,linear learning from insufficient discriminator feedback, and hard boundaryconvergence due to stringent feedback. Histopathology data presents a uniquechallenge with its complex representation, high spatial resolution, andmultiscale features. To address these challenges, we propose a frameworkconsisting of two components. First, we introduce a contrastive learning-basedMultistage Progressive Finetuning Siamese Neural Network (MFT-SNN) forassessing the similarity between histopathology patches. Second, we implement aReinforcement Learning-based External Optimizer (RL-EO) within the GAN trainingloop, serving as a reward signal generator. The modified discriminator lossfunction incorporates a weighted reward, guiding the GAN to maximize thisreward while minimizing loss. This approach offers an external optimizationguide to the discriminator, preventing generator overfitting and ensuringsmooth convergence. Our proposed solution has been benchmarked againststate-of-the-art (SOTA) GANs and a Denoising Diffusion Probabilistic model,outperforming previous SOTA across various metrics, including FID score, KIDscore, Perceptual Path Length, and downstream classification tasks.</description><author>Osama Mustafa</author><pubDate>Mon, 30 Sep 2024 14:39:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20340v1</guid></item><item><title>Devil is in Details: Locality-Aware 3D Abdominal CT Volume Generation for Self-Supervised Organ Segmentation</title><link>http://arxiv.org/abs/2409.20332v1</link><description>In the realm of medical image analysis, self-supervised learning (SSL)techniques have emerged to alleviate labeling demands, while still facing thechallenge of training data scarcity owing to escalating resource requirementsand privacy constraints. Numerous efforts employ generative models to generatehigh-fidelity, unlabeled 3D volumes across diverse modalities and anatomicalregions. However, the intricate and indistinguishable anatomical structureswithin the abdomen pose a unique challenge to abdominal CT volume generationcompared to other anatomical regions. To address the overlooked challenge, weintroduce the Locality-Aware Diffusion (Lad), a novel method tailored forexquisite 3D abdominal CT volume generation. We design a locality loss torefine crucial anatomical regions and devise a condition extractor to integrateabdominal priori into generation, thereby enabling the generation of largequantities of high-quality abdominal CT volumes essential for SSL tasks withoutthe need for additional data such as labels or radiology reports. Volumesgenerated through our method demonstrate remarkable fidelity in reproducingabdominal structures, achieving a decrease in FID score from 0.0034 to 0.0002on AbdomenCT-1K dataset, closely mirroring authentic data and surpassingcurrent methods. Extensive experiments demonstrate the effectiveness of ourmethod in self-supervised organ segmentation tasks, resulting in an improvementin mean Dice scores on two abdominal datasets effectively. These resultsunderscore the potential of synthetic data to advance self-supervised learningin medical image analysis.</description><author>Yuran Wang, Zhijing Wan, Yansheng Qiu, Zheng Wang</author><pubDate>Mon, 30 Sep 2024 14:35:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20332v1</guid></item></channel></rss>