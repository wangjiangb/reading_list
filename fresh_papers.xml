<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 18 Apr 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Factorized Diffusion: Perceptual Illusions by Noise Decomposition</title><link>http://arxiv.org/abs/2404.11615v1</link><description>Given a factorization of an image into a sum of linear components, we presenta zero-shot method to control each individual component through diffusion modelsampling. For example, we can decompose an image into low and high spatialfrequencies and condition these components on different text prompts. Thisproduces hybrid images, which change appearance depending on viewing distance.By decomposing an image into three frequency subbands, we can generate hybridimages with three prompts. We also use a decomposition into grayscale and colorcomponents to produce images whose appearance changes when they are viewed ingrayscale, a phenomena that naturally occurs under dim lighting. And we explorea decomposition by a motion blur kernel, which produces images that changeappearance under motion blurring. Our method works by denoising with acomposite noise estimate, built from the components of noise estimatesconditioned on different prompts. We also show that for certain decompositions,our method recovers prior approaches to compositional generation and spatialcontrol. Finally, we show that we can extend our approach to generate hybridimages from real images. We do this by holding one component fixed andgenerating the remaining components, effectively solving an inverse problem.</description><author>Daniel Geng, Inbum Park, Andrew Owens</author><pubDate>Wed, 17 Apr 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11615v1</guid></item><item><title>Dynamic Typography: Bringing Words to Life</title><link>http://arxiv.org/abs/2404.11614v1</link><description>Text animation serves as an expressive medium, transforming staticcommunication into dynamic experiences by infusing words with motion to evokeemotions, emphasize meanings, and construct compelling narratives. Craftinganimations that are semantically aware poses significant challenges, demandingexpertise in graphic design and animation. We present an automated textanimation scheme, termed "Dynamic Typography", which combines two challengingtasks. It deforms letters to convey semantic meaning and infuses them withvibrant movements based on user prompts. Our technique harnesses vectorgraphics representations and an end-to-end optimization-based framework. Thisframework employs neural displacement fields to convert letters into baseshapes and applies per-frame motion, encouraging coherence with the intendedtextual concept. Shape preservation techniques and perceptual lossregularization are employed to maintain legibility and structural integritythroughout the animation process. We demonstrate the generalizability of ourapproach across various text-to-video models and highlight the superiority ofour end-to-end methodology over baseline methods, which might comprise separatetasks. Through quantitative and qualitative evaluations, we demonstrate theeffectiveness of our framework in generating coherent text animations thatfaithfully interpret user prompts while maintaining readability. Our code isavailable at: https://animate-your-word.github.io/demo/.</description><author>Zichen Liu, Yihao Meng, Hao Ouyang, Yue Yu, Bolin Zhao, Daniel Cohen-Or, Huamin Qu</author><pubDate>Wed, 17 Apr 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11614v1</guid></item><item><title>InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior</title><link>http://arxiv.org/abs/2404.11613v1</link><description>3D Gaussians have recently emerged as an efficient representation for novelview synthesis. This work studies its editability with a particular focus onthe inpainting task, which aims to supplement an incomplete set of 3D Gaussianswith additional points for visually harmonious rendering. Compared to 2Dinpainting, the crux of inpainting 3D Gaussians is to figure out therendering-relevant properties of the introduced points, whose optimizationlargely benefits from their initial 3D positions. To this end, we propose toguide the point initialization with an image-conditioned depth completionmodel, which learns to directly restore the depth map based on the observedimage. Such a design allows our model to fill in depth values at an alignedscale with the original depth, and also to harness strong generalizability fromlargescale diffusion prior. Thanks to the more accurate depth completion, ourapproach, dubbed InFusion, surpasses existing alternatives with sufficientlybetter fidelity and efficiency under various complex scenarios. We furtherdemonstrate the effectiveness of InFusion with several practical applications,such as inpainting with user-specific texture or with novel object insertion.</description><author>Zhiheng Liu, Hao Ouyang, Qiuyu Wang, Ka Leong Cheng, Jie Xiao, Kai Zhu, Nan Xue, Yu Liu, Yujun Shen, Yang Cao</author><pubDate>Wed, 17 Apr 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11613v1</guid></item><item><title>VehicleGAN: Pair-flexible Pose Guided Image Synthesis for Vehicle Re-identification</title><link>http://arxiv.org/abs/2311.16278v3</link><description>Vehicle Re-identification (Re-ID) has been broadly studied in the lastdecade; however, the different camera view angle leading to confuseddiscrimination in the feature subspace for the vehicles of various poses, isstill challenging for the Vehicle Re-ID models in the real world. To promotethe Vehicle Re-ID models, this paper proposes to synthesize a large number ofvehicle images in the target pose, whose idea is to project the vehicles ofdiverse poses into the unified target pose so as to enhance featurediscrimination. Considering that the paired data of the same vehicles indifferent traffic surveillance cameras might be not available in the realworld, we propose the first Pair-flexible Pose Guided Image Synthesis methodfor Vehicle Re-ID, named as VehicleGAN in this paper, which works for bothsupervised and unsupervised settings without the knowledge of geometric 3Dmodels. Because of the feature distribution difference between real andsynthetic data, simply training a traditional metric learning based Re-ID modelwith data-level fusion (i.e., data augmentation) is not satisfactory, thereforewe propose a new Joint Metric Learning (JML) via effective feature-level fusionfrom both real and synthetic data. Intensive experimental results on the publicVeRi-776 and VehicleID datasets prove the accuracy and effectiveness of ourproposed VehicleGAN and JML.</description><author>Baolu Li, Ping Liu, Lan Fu, Jinlong Li, Jianwu Fang, Zhigang Xu, Hongkai Yu</author><pubDate>Wed, 17 Apr 2024 18:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16278v3</guid></item><item><title>Towards White Box Deep Learning</title><link>http://arxiv.org/abs/2403.09863v5</link><description>Deep neural networks learn fragile "shortcut" features, rendering themdifficult to interpret (black box) and vulnerable to adversarial attacks. Thispaper proposes semantic features as a general architectural solution to thisproblem. The main idea is to make features locality-sensitive in the adequatesemantic topology of the domain, thus introducing a strong regularization. Theproof of concept network is lightweight, inherently interpretable and achievesalmost human-level adversarial test metrics - with no adversarial training!These results and the general nature of the approach warrant further researchon semantic features. The code is available athttps://github.com/314-Foundation/white-box-nn</description><author>Maciej Satkiewicz</author><pubDate>Wed, 17 Apr 2024 18:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09863v5</guid></item><item><title>Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning</title><link>http://arxiv.org/abs/2403.02333v2</link><description>Large language models (LLMs) have shown great potential in complex reasoningtasks, yet their performance is often hampered by the scarcity of high-qualityand reasoning-focused training datasets. Addressing this challenge, we proposeKey-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework thatsynthesizes question-answer pairs by leveraging key points and exemplarpractices from authentic data sources. KPDDS ensures the generation of novelquestions with rigorous quality control and substantial scalability. As aresult, we present KPMath, an extensive synthetic dataset tailored formathematical reasoning, comprising over 800K question-answer pairs. UtilizingKPMath and augmenting it with additional reasoning-intensive corpora, we createthe comprehensive KPMath-Plus dataset. The fine-tuned DeepSeekMath model onKPMath-Plus achieves zero-shot PASS@1 accuracies of 83.9% on GSM8K and 48.8% onMATH, and also reaches promising performance on other math reasoning datasets,outperforming competitors in the 7B to 70B range.</description><author>Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, Weizhu Chen</author><pubDate>Wed, 17 Apr 2024 18:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02333v2</guid></item><item><title>Learning to Solve the Constrained Most Probable Explanation Task in Probabilistic Graphical Models</title><link>http://arxiv.org/abs/2404.11606v1</link><description>We propose a self-supervised learning approach for solving the followingconstrained optimization task in log-linear models or Markov networks. Let $f$and $g$ be two log-linear models defined over the sets $\mathbf{X}$ and$\mathbf{Y}$ of random variables respectively. Given an assignment $\mathbf{x}$to all variables in $\mathbf{X}$ (evidence) and a real number $q$, theconstrained most-probable explanation (CMPE) task seeks to find an assignment$\mathbf{y}$ to all variables in $\mathbf{Y}$ such that $f(\mathbf{x},\mathbf{y})$ is maximized and $g(\mathbf{x}, \mathbf{y})\leq q$. In ourproposed self-supervised approach, given assignments $\mathbf{x}$ to$\mathbf{X}$ (data), we train a deep neural network that learns to outputnear-optimal solutions to the CMPE problem without requiring access to anypre-computed solutions. The key idea in our approach is to use first principlesand approximate inference methods for CMPE to derive novel loss functions thatseek to push infeasible solutions towards feasible ones and feasible solutionstowards optimal ones. We analyze the properties of our proposed method andexperimentally demonstrate its efficacy on several benchmark problems.</description><author>Shivvrat Arya, Tahrima Rahman, Vibhav Gogate</author><pubDate>Wed, 17 Apr 2024 18:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11606v1</guid></item><item><title>VG4D: Vision-Language Model Goes 4D Video Recognition</title><link>http://arxiv.org/abs/2404.11605v1</link><description>Understanding the real world through point cloud video is a crucial aspect ofrobotics and autonomous driving systems. However, prevailing methods for 4Dpoint cloud recognition have limitations due to sensor resolution, which leadsto a lack of detailed information. Recent advances have shown thatVision-Language Models (VLM) pre-trained on web-scale text-image datasets canlearn fine-grained visual concepts that can be transferred to variousdownstream tasks. However, effectively integrating VLM into the domain of 4Dpoint clouds remains an unresolved problem. In this work, we propose theVision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge fromvisual-text pre-trained models to a 4D point cloud network. Our approachinvolves aligning the 4D encoder's representation with a VLM to learn a sharedvisual and text space from training on large-scale image-text pairs. Bytransferring the knowledge of the VLM to the 4D encoder and combining the VLM,our VG4D achieves improved recognition performance. To enhance the 4D encoder,we modernize the classic dynamic point cloud backbone and propose an improvedversion of PSTNet, im-PSTNet, which can efficiently model point cloud videos.Experiments demonstrate that our method achieves state-of-the-art performancefor action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120dataset. Code is available at \url{https://github.com/Shark0-0/VG4D}.</description><author>Zhichao Deng, Xiangtai Li, Xia Li, Yunhai Tong, Shen Zhao, Mengyuan Liu</author><pubDate>Wed, 17 Apr 2024 18:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11605v1</guid></item><item><title>Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering</title><link>http://arxiv.org/abs/2307.16877v2</link><description>Retriever-augmented instruction-following models are attractive alternativesto fine-tuned approaches for information-seeking tasks such as questionanswering (QA). By simply prepending retrieved documents in its input alongwith an instruction, these models can be adapted to various information domainsand tasks without additional fine-tuning. While the model responses tend to benatural and fluent, the additional verbosity makes traditional QA evaluationmetrics such as exact match (EM) and F1 unreliable for accurately quantifyingmodel performance. In this work, we investigate the performance of instruction-following modelsacross three information-seeking QA tasks. We use both automatic and humanevaluation to evaluate these models along two dimensions: 1) how well theysatisfy the user's information need (correctness), and 2) whether they producea response based on the provided knowledge (faithfulness). Guided by humanevaluation and analysis, we highlight the shortcomings of traditional metricsfor both correctness and faithfulness. We then propose simple token-overlapbased and model-based metrics that reflect the true performance of thesemodels. Our analysis reveals that instruction-following models are competitive,and sometimes even outperform fine-tuned models for correctness. However, thesemodels struggle to stick to the provided knowledge and often hallucinate intheir responses. We hope our work encourages a more holistic evaluation ofinstruction-following models for QA. Our code and data is available athttps://github.com/McGill-NLP/instruct-qa</description><author>Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, Siva Reddy</author><pubDate>Wed, 17 Apr 2024 18:52:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16877v2</guid></item><item><title>The Brain Tumor Sequence Registration (BraTS-Reg) Challenge: Establishing Correspondence Between Pre-Operative and Follow-up MRI Scans of Diffuse Glioma Patients</title><link>http://arxiv.org/abs/2112.06979v2</link><description>Registration of longitudinal brain MRI scans containing pathologies ischallenging due to dramatic changes in tissue appearance. Although there hasbeen progress in developing general-purpose medical image registrationtechniques, they have not yet attained the requisite precision and reliabilityfor this task, highlighting its inherent complexity. Here we describe the BrainTumor Sequence Registration (BraTS-Reg) challenge, as the first publicbenchmark environment for deformable registration algorithms focusing onestimating correspondences between pre-operative and follow-up scans of thesame patient diagnosed with a diffuse brain glioma. The BraTS-Reg data comprisede-identified multi-institutional multi-parametric MRI (mpMRI) scans, curatedfor size and resolution according to a canonical anatomical template, anddivided into training, validation, and testing sets. Clinical experts annotatedground truth (GT) landmark points of anatomical locations distinct across thetemporal domain. Quantitative evaluation and ranking were based on the MedianEuclidean Error (MEE), Robustness, and the determinant of the Jacobian of thedisplacement field. The top-ranked methodologies yielded similar performanceacross all evaluation metrics and shared several methodological commonalities,including pre-alignment, deep neural networks, inverse consistency analysis,and test-time instance optimization per-case basis as a post-processing step.The top-ranked method attained the MEE at or below that of the inter-ratervariability for approximately 60% of the evaluated landmarks, underscoring thescope for further accuracy and robustness improvements, especially relative tohuman experts. The aim of BraTS-Reg is to continue to serve as an activeresource for research, with the data and online evaluation tools accessible athttps://bratsreg.github.io/.</description><author>Bhakti Baheti, Satrajit Chakrabarty, Hamed Akbari, Michel Bilello, Benedikt Wiestler, Julian Schwarting, Evan Calabrese, Jeffrey Rudie, Syed Abidi, Mina Mousa, Javier Villanueva-Meyer, Brandon K. K. Fields, Florian Kofler, Russell Takeshi Shinohara, Juan Eugenio Iglesias, Tony C. W. Mok, Albert C. S. Chung, Marek Wodzinski, Artur Jurgas, Niccolo Marini, Manfredo Atzori, Henning Muller, Christoph Grobroehmer, Hanna Siebert, Lasse Hansen, Mattias P. Heinrich, Luca Canalini, Jan Klein, Annika Gerken, Stefan Heldmann, Alessa Hering, Horst K. Hahn, Mingyuan Meng, Lei Bi, Dagan Feng, Jinman Kim, Ramy A. Zeineldin, Mohamed E. Karar, Franziska Mathis-Ullrich, Oliver Burgert, Javid Abderezaei, Aymeric Pionteck, Agamdeep Chopra, Mehmet Kurt, Kewei Yan, Yonghong Yan, Zhe Tang, Jianqiang Ma, Sahar Alm</author><pubDate>Wed, 17 Apr 2024 18:50:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.06979v2</guid></item><item><title>Variational Bayesian Last Layers</title><link>http://arxiv.org/abs/2404.11599v1</link><description>We introduce a deterministic variational formulation for training Bayesianlast layer neural networks. This yields a sampling-free, single-pass model andloss that effectively improves uncertainty estimation. Our variational Bayesianlast layer (VBLL) can be trained and evaluated with only quadratic complexityin last layer width, and is thus (nearly) computationally free to add tostandard architectures. We experimentally investigate VBLLs, and show that theyimprove predictive accuracy, calibration, and out of distribution detectionover baselines across both regression and classification. Finally, weinvestigate combining VBLL layers with variational Bayesian feature learning,yielding a lower variance collapsed variational inference method for Bayesianneural networks.</description><author>James Harrison, John Willes, Jasper Snoek</author><pubDate>Wed, 17 Apr 2024 18:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11599v1</guid></item><item><title>Explainable Artificial Intelligence Techniques for Accurate Fault Detection and Diagnosis: A Review</title><link>http://arxiv.org/abs/2404.11597v1</link><description>As the manufacturing industry advances with sensor integration andautomation, the opaque nature of deep learning models in machine learning posesa significant challenge for fault detection and diagnosis. And despite therelated predictive insights Artificial Intelligence (AI) can deliver, advancedmachine learning engines often remain a black box. This paper reviews theeXplainable AI (XAI) tools and techniques in this context. We explore variousXAI methodologies, focusing on their role in making AI decision-makingtransparent, particularly in critical scenarios where humans are involved. Wealso discuss current limitations and potential future research that aims tobalance explainability with model performance while improving trustworthinessin the context of AI applications for critical industrial use cases.</description><author>Ahmed Maged, Salah Haridy, Herman Shen</author><pubDate>Wed, 17 Apr 2024 18:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11597v1</guid></item><item><title>IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination</title><link>http://arxiv.org/abs/2404.11593v1</link><description>This paper aims to recover object materials from posed images captured underan unknown static lighting condition. Recent methods solve this task byoptimizing material parameters through differentiable physically basedrendering. However, due to the coupling between object geometry, materials, andenvironment lighting, there is inherent ambiguity during the inverse renderingprocess, preventing previous methods from obtaining accurate results. Toovercome this ill-posed problem, our key idea is to learn the material priorwith a generative model for regularizing the optimization process. We observethat the general rendering equation can be split into diffuse and specularshading terms, and thus formulate the material prior as diffusion models ofalbedo and specular. Thanks to this design, our model can be trained using theexisting abundant 3D object data, and naturally acts as a versatile tool toresolve the ambiguity when recovering material representations from RGB images.In addition, we develop a coarse-to-fine training strategy that leveragesestimated materials to guide diffusion models to satisfy multi-view consistentconstraints, leading to more stable and accurate results. Extensive experimentson real-world and synthetic datasets demonstrate that our approach achievesstate-of-the-art performance on material recovery. The code will be availableat https://zju3dv.github.io/IntrinsicAnything.</description><author>Xi Chen, Sida Peng, Dongchen Yang, Yuan Liu, Bowen Pan, Chengfei Lv, Xiaowei Zhou</author><pubDate>Wed, 17 Apr 2024 18:45:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11593v1</guid></item><item><title>Re-Nerfing: Improving Novel Views Synthesis through Novel Views Synthesis</title><link>http://arxiv.org/abs/2312.02255v2</link><description>Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesiscapabilities even in large-scale, unbounded scenes, albeit requiring hundredsof views or introducing artifacts in sparser settings. Their optimizationsuffers from shape-radiance ambiguities wherever only a small visual overlap isavailable. This leads to erroneous scene geometry and artifacts. In this paper,we propose Re-Nerfing, a simple and general multi-stage data augmentationapproach that leverages NeRF's own view synthesis ability to address theselimitations. With Re-Nerfing, we enhance the geometric consistency of novelviews as follows: First, we train a NeRF with the available views. Then, we usethe optimized NeRF to synthesize pseudo-views around the original ones with aview selection strategy to improve coverage and preserve view quality. Finally,we train a second NeRF with both the original images and the pseudo viewsmasking out uncertain regions. Extensive experiments applying Re-Nerfing onvarious pipelines on the mip-NeRF 360 dataset, including Gaussian Splatting,provide valuable insights into the improvements achievable without externaldata or supervision, on denser and sparser input scenarios. Project page:https://renerfing.github.io</description><author>Felix Tristram, Stefano Gasperini, Nassir Navab, Federico Tombari</author><pubDate>Wed, 17 Apr 2024 18:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02255v2</guid></item><item><title>Segmenting the motion components of a video: A long-term unsupervised model</title><link>http://arxiv.org/abs/2310.01040v3</link><description>Human beings have the ability to continuously analyze a video and immediatelyextract the motion components. We want to adopt this paradigm to provide acoherent and stable motion segmentation over the video sequence. In thisperspective, we propose a novel long-term spatio-temporal model operating in atotally unsupervised way. It takes as input the volume of consecutive opticalflow (OF) fields, and delivers a volume of segments of coherent motion over thevideo. More specifically, we have designed a transformer-based network, wherewe leverage a mathematically well-founded framework, the Evidence Lower Bound(ELBO), to derive the loss function. The loss function combines a flowreconstruction term involving spatio-temporal parametric motion modelscombining, in a novel way, polynomial (quadratic) motion models for the spatialdimensions and B-splines for the time dimension of the video sequence, and aregularization term enforcing temporal consistency on the segments. We reportexperiments on four VOS benchmarks, demonstrating competitive quantitativeresults, while performing motion segmentation on a whole sequence in one go. Wealso highlight through visual results the key contributions on temporalconsistency brought by our method.</description><author>Etienne Meunier, Patrick Bouthemy</author><pubDate>Wed, 17 Apr 2024 18:44:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01040v3</guid></item><item><title>Mastering Diverse Domains through World Models</title><link>http://arxiv.org/abs/2301.04104v2</link><description>Developing a general algorithm that learns to solve tasks across a wide rangeof applications has been a fundamental challenge in artificial intelligence.Although current reinforcement learning algorithms can be readily applied totasks similar to what they have been developed for, configuring them for newapplication domains requires significant human expertise and experimentation.We present DreamerV3, a general algorithm that outperforms specialized methodsacross over 150 diverse tasks, with a single configuration. Dreamer learns amodel of the environment and improves its behavior by imagining futurescenarios. Robustness techniques based on normalization, balancing, andtransformations enable stable learning across domains. Applied out of the box,Dreamer is the first algorithm to collect diamonds in Minecraft from scratchwithout human data or curricula. This achievement has been posed as asignificant challenge in artificial intelligence that requires exploringfarsighted strategies from pixels and sparse rewards in an open world. Our workallows solving challenging control problems without extensive experimentation,making reinforcement learning broadly applicable.</description><author>Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap</author><pubDate>Wed, 17 Apr 2024 18:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.04104v2</guid></item><item><title>A Subspace-Constrained Tyler's Estimator and its Applications to Structure from Motion</title><link>http://arxiv.org/abs/2404.11590v1</link><description>We present the subspace-constrained Tyler's estimator (STE) designed forrecovering a low-dimensional subspace within a dataset that may be highlycorrupted with outliers. STE is a fusion of the Tyler's M-estimator (TME) and avariant of the fast median subspace. Our theoretical analysis suggests that,under a common inlier-outlier model, STE can effectively recover the underlyingsubspace, even when it contains a smaller fraction of inliers relative to othermethods in the field of robust subspace recovery. We apply STE in the contextof Structure from Motion (SfM) in two ways: for robust estimation of thefundamental matrix and for the removal of outlying cameras, enhancing therobustness of the SfM pipeline. Numerical experiments confirm thestate-of-the-art performance of our method in these applications. This researchmakes significant contributions to the field of robust subspace recovery,particularly in the context of computer vision and 3D reconstruction.</description><author>Feng Yu, Teng Zhang, Gilad Lerman</author><pubDate>Wed, 17 Apr 2024 18:39:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11590v1</guid></item><item><title>Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept Understanding</title><link>http://arxiv.org/abs/2404.11589v1</link><description>The rapid evolution of text-to-image diffusion models has opened the door ofgenerative AI, enabling the translation of textual descriptions into visuallycompelling images with remarkable quality. However, a persistent challengewithin this domain is the optimization of prompts to effectively conveyabstract concepts into concrete objects. For example, text encoders can hardlyexpress "peace", while can easily illustrate olive branches and white doves.This paper introduces a novel approach named Prompt Optimizer for AbstractConcepts (POAC) specifically designed to enhance the performance oftext-to-image diffusion models in interpreting and generating images fromabstract concepts. We propose a Prompt Language Model (PLM), which isinitialized from a pre-trained language model, and then fine-tuned with acurated dataset of abstract concept prompts. The dataset is created with GPT-4to extend the abstract concept to a scene and concrete objects. Our frameworkemploys a Reinforcement Learning (RL)-based optimization strategy, focusing onthe alignment between the generated images by a stable diffusion model andoptimized prompts. Through extensive experiments, we demonstrate that ourproposed POAC significantly improves the accuracy and aesthetic quality ofgenerated images, particularly in the description of abstract concepts andalignment with optimized prompts. We also present a comprehensive analysis ofour model's performance across diffusion models under different settings,showcasing its versatility and effectiveness in enhancing abstract conceptrepresentation.</description><author>Zezhong Fan, Xiaohan Li, Chenhao Fang, Topojoy Biswas, Kaushiki Nag, Jianpeng Xu, Kannan Achan</author><pubDate>Wed, 17 Apr 2024 18:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11589v1</guid></item><item><title>Related Work and Citation Text Generation: A Survey</title><link>http://arxiv.org/abs/2404.11588v1</link><description>To convince readers of the novelty of their research paper, authors mustperform a literature review and compose a coherent story that connects andrelates prior works to the current work. This challenging nature of literaturereview writing makes automatic related work generation (RWG) academically andcomputationally interesting, and also makes it an excellent test bed forexamining the capability of SOTA natural language processing (NLP) models.Since the initial proposal of the RWG task, its popularity has waxed and waned,following the capabilities of mainstream NLP approaches. In this work, wesurvey the zoo of RWG historical works, summarizing the key approaches and taskdefinitions and discussing the ongoing challenges of RWG.</description><author>Xiangci Li, Jessica Ouyang</author><pubDate>Wed, 17 Apr 2024 18:37:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11588v1</guid></item><item><title>TCJA-SNN: Temporal-Channel Joint Attention for Spiking Neural Networks</title><link>http://arxiv.org/abs/2206.10177v3</link><description>Spiking Neural Networks (SNNs) are attracting widespread interest due totheir biological plausibility, energy efficiency, and powerful spatio-temporalinformation representation ability. Given the critical role of attentionmechanisms in enhancing neural network performance, the integration of SNNs andattention mechanisms exhibits potential to deliver energy-efficient andhigh-performance computing paradigms. We present a novel Temporal-Channel JointAttention mechanism for SNNs, referred to as TCJA-SNN. The proposed TCJA-SNNframework can effectively assess the significance of spike sequence from bothspatial and temporal dimensions. More specifically, our essential technicalcontribution lies on: 1) We employ the squeeze operation to compress the spikestream into an average matrix. Then, we leverage two local attention mechanismsbased on efficient 1D convolutions to facilitate comprehensive featureextraction at the temporal and channel levels independently. 2) We introducethe Cross Convolutional Fusion (CCF) layer as a novel approach to model theinter-dependencies between the temporal and channel scopes. This layer breaksthe independence of these two dimensions and enables the interaction betweenfeatures. Experimental results demonstrate that the proposed TCJA-SNNoutperforms SOTA by up to 15.7% accuracy on standard static and neuromorphicdatasets, including Fashion-MNIST, CIFAR10-DVS, N-Caltech 101, and DVS128Gesture. Furthermore, we apply the TCJA-SNN framework to image generation tasksby leveraging a variation autoencoder. To the best of our knowledge, this studyis the first instance where the SNN-attention mechanism has been employed forimage classification and generation tasks. Notably, our approach has achievedSOTA performance in both domains, establishing a significant advancement in thefield. Codes are available at https://github.com/ridgerchu/TCJA.</description><author>Rui-Jie Zhu, Malu Zhang, Qihang Zhao, Haoyu Deng, Yule Duan, Liang-Jian Deng</author><pubDate>Wed, 17 Apr 2024 18:36:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.10177v3</guid></item><item><title>Spatial Context-based Self-Supervised Learning for Handwritten Text Recognition</title><link>http://arxiv.org/abs/2404.11585v1</link><description>Handwritten Text Recognition (HTR) is a relevant problem in computer vision,and implies unique challenges owing to its inherent variability and the richcontextualization required for its interpretation. Despite the success ofSelf-Supervised Learning (SSL) in computer vision, its application to HTR hasbeen rather scattered, leaving key SSL methodologies unexplored. This workfocuses on one of them, namely Spatial Context-based SSL. We investigate howthis family of approaches can be adapted and optimized for HTR and propose newworkflows that leverage the unique features of handwritten text. Ourexperiments demonstrate that the methods considered lead to advancements in thestate-of-the-art of SSL for HTR in a number of benchmark cases.</description><author>Carlos Penarrubia, Carlos Garrido-Munoz, Jose J. Valero-Mas, Jorge Calvo-Zaragoza</author><pubDate>Wed, 17 Apr 2024 18:33:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11585v1</guid></item><item><title>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</title><link>http://arxiv.org/abs/2404.11584v1</link><description>This survey paper examines the recent advancements in AI agentimplementations, with a focus on their ability to achieve complex goals thatrequire enhanced reasoning, planning, and tool execution capabilities. Theprimary objectives of this work are to a) communicate the current capabilitiesand limitations of existing AI agent implementations, b) share insights gainedfrom our observations of these systems in action, and c) suggest importantconsiderations for future developments in AI agent design. We achieve this byproviding overviews of single-agent and multi-agent architectures, identifyingkey patterns and divergences in design choices, and evaluating their overallimpact on accomplishing a provided goal. Our contribution outlines key themeswhen selecting an agentic architecture, the impact of leadership on agentsystems, agent communication styles, and key phases for planning, execution,and reflection that enable robust AI agent systems.</description><author>Tula Masterman, Sandi Besen, Mason Sawtell, Alex Chao</author><pubDate>Wed, 17 Apr 2024 18:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11584v1</guid></item><item><title>LLMTune: Accelerate Database Knob Tuning with Large Language Models</title><link>http://arxiv.org/abs/2404.11581v1</link><description>Database knob tuning is a critical challenge in the database community,aiming to optimize knob values to enhance database performance for specificworkloads. DBMS often feature hundreds of tunable knobs, posing a significantchallenge for DBAs to recommend optimal configurations. Consequently, manymachine learning-based tuning methods have been developed to automate thisprocess. Despite the introduction of various optimizers, practical applicationshave unveiled a new problem: they typically require numerous workload runs toachieve satisfactory performance, a process that is both time-consuming andresource-intensive. This inefficiency largely stems from the optimalconfiguration often being substantially different from the default setting,necessitating multiple iterations during tuning. Recognizing this, we arguethat an effective starting point could significantly reduce redundantexploration in less efficient areas, thereby potentially speeding up the tuningprocess for the optimizers. Based on this assumption, we introduce LLMTune, alarge language model-based configuration generator designed to produce aninitial, high-quality configuration for new workloads. These generatedconfigurations can then serve as starting points for various base optimizers,accelerating their tuning processes. To obtain training data for LLMTune'ssupervised fine-tuning, we have devised a new automatic data generationframework capable of efficiently creating a large number of &lt;workload,configuration&gt; pairs. We have conducted thorough experiments to evaluateLLMTune's effectiveness with different workloads, such as TPC-H and JOB. Incomparison to leading methods, LLMTune demonstrates a quicker ability toidentify superior configurations. For instance, with the challenging TPC-Hworkload, our LLMTune achieves a significant 15.6x speed-up ratio in findingthe best-performing configurations.</description><author>Xinmei Huang, Haoyang Li, Jing Zhang, Xinxin Zhao, Zhiming Yao, Yiyan Li, Zhuohao Yu, Tieying Zhang, Hong Chen, Cuiping Li</author><pubDate>Wed, 17 Apr 2024 18:28:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11581v1</guid></item><item><title>Deep Policy Optimization with Temporal Logic Constraints</title><link>http://arxiv.org/abs/2404.11578v1</link><description>Temporal logics, such as linear temporal logic (LTL), offer a precise meansof specifying tasks for (deep) reinforcement learning (RL) agents. In our work,we consider the setting where the task is specified by an LTL objective andthere is an additional scalar reward that we need to optimize. Previous worksfocus either on learning a LTL task-satisfying policy alone or are restrictedto finite state spaces. We make two contributions: First, we introduce anRL-friendly approach to this setting by formulating this problem as a singleoptimization objective. Our formulation guarantees that an optimal policy willbe reward-maximal from the set of policies that maximize the likelihood ofsatisfying the LTL specification. Second, we address a sparsity issue thatoften arises for LTL-guided Deep RL policies by introducing Cycle ExperienceReplay (CyclER), a technique that automatically guides RL agents towards thesatisfaction of an LTL specification. Our experiments demonstrate the efficacyof CyclER in finding performant deep RL policies in both continuous anddiscrete experimental domains.</description><author>Ameesh Shah, Cameron Voloshin, Chenxi Yang, Abhinav Verma, Swarat Chaudhuri, Sanjit A. Seshia</author><pubDate>Wed, 17 Apr 2024 18:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11578v1</guid></item><item><title>Towards Reliable Empirical Machine Unlearning Evaluation: A Game-Theoretic View</title><link>http://arxiv.org/abs/2404.11577v1</link><description>Machine unlearning is the process of updating machine learning models toremove the information of specific training data samples, in order to complywith data protection regulations that allow individuals to request the removalof their personal data. Despite the recent development of numerous unlearningalgorithms, reliable evaluation of these algorithms remains an open researchquestion. In this work, we focus on membership inference attack (MIA) basedevaluation, one of the most common approaches for evaluating unlearningalgorithms, and address various pitfalls of existing evaluation metrics thatlack reliability. Specifically, we propose a game-theoretic framework thatformalizes the evaluation process as a game between unlearning algorithms andMIA adversaries, measuring the data removal efficacy of unlearning algorithmsby the capability of the MIA adversaries. Through careful design of the game,we demonstrate that the natural evaluation metric induced from the game enjoysprovable guarantees that the existing evaluation metrics fail to satisfy.Furthermore, we propose a practical and efficient algorithm to estimate theevaluation metric induced from the game, and demonstrate its effectivenessthrough both theoretical analysis and empirical experiments. This work presentsa novel and reliable approach to empirically evaluating unlearning algorithms,paving the way for the development of more effective unlearning techniques.</description><author>Yiwen Tu, Pingbang Hu, Jiaqi Ma</author><pubDate>Wed, 17 Apr 2024 18:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11577v1</guid></item><item><title>Predicting Traffic Congestion at Urban Intersections Using Data-Driven Modeling</title><link>http://arxiv.org/abs/2404.08838v3</link><description>Traffic congestion at intersections is a significant issue in urban areas,leading to increased commute times, safety hazards, and operationalinefficiencies. This study aims to develop a predictive model for congestion atintersections in major U.S. cities, utilizing a dataset of trip-logging metricsfrom commercial vehicles across 4,800 intersections. The dataset encompasses 27features, including intersection coordinates, street names, time of day, andtraffic metrics (Kashyap et al., 2019). Additional features, such asrainfall/snowfall percentage, distance from downtown and outskirts, and roadtypes, were incorporated to enhance the model's predictive power. Themethodology involves data exploration, feature transformation, and handlingmissing values through low-rank models and label encoding. The proposed modelhas the potential to assist city planners and governments in anticipatingtraffic hot spots, optimizing operations, and identifying infrastructurechallenges.</description><author>Tara Kelly, Jessica Gupta</author><pubDate>Wed, 17 Apr 2024 18:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08838v3</guid></item><item><title>State-space Decomposition Model for Video Prediction Considering Long-term Motion Trend</title><link>http://arxiv.org/abs/2404.11576v1</link><description>Stochastic video prediction enables the consideration of uncertainty infuture motion, thereby providing a better reflection of the dynamic nature ofthe environment. Stochastic video prediction methods based on imageauto-regressive recurrent models need to feed their predictions back into thelatent space. Conversely, the state-space models, which decouple framesynthesis and temporal prediction, proves to be more efficient. However,inferring long-term temporal information about motion and generalizing todynamic scenarios under non-stationary assumptions remains an unresolvedchallenge. In this paper, we propose a state-space decomposition stochasticvideo prediction model that decomposes the overall video frame generation intodeterministic appearance prediction and stochastic motion prediction. Throughadaptive decomposition, the model's generalization capability to dynamicscenarios is enhanced. In the context of motion prediction, obtaining a prioron the long-term trend of future motion is crucial. Thus, in the stochasticmotion prediction branch, we infer the long-term motion trend from conditionalframes to guide the generation of future frames that exhibit high consistencywith the conditional frames. Experimental results demonstrate that our modeloutperforms baselines on multiple datasets.</description><author>Fei Cui, Jiaojiao Fang, Xiaojiang Wu, Zelong Lai, Mengke Yang, Menghan Jia, Guizhong Liu</author><pubDate>Wed, 17 Apr 2024 18:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11576v1</guid></item><item><title>Decentralized Personalized Federated Learning for Min-Max Problems</title><link>http://arxiv.org/abs/2106.07289v6</link><description>Personalized Federated Learning (PFL) has witnessed remarkable advancements,enabling the development of innovative machine learning applications thatpreserve the privacy of training data. However, existing theoretical researchin this field has primarily focused on distributed optimization forminimization problems. This paper is the first to study PFL for saddle pointproblems encompassing a broader range of optimization problems, that requiremore than just solving minimization problems. In this work, we consider arecently proposed PFL setting with the mixing objective function, an approachcombining the learning of a global model together with locally distributedlearners. Unlike most previous work, which considered only the centralizedsetting, we work in a more general and decentralized setup that allows us todesign and analyze more practical and federated ways to connect devices to thenetwork. We proposed new algorithms to address this problem and provide atheoretical analysis of the smooth (strongly) convex-(strongly) concave saddlepoint problems in stochastic and deterministic cases. Numerical experiments forbilinear problems and neural networks with adversarial noise demonstrate theeffectiveness of the proposed methods.</description><author>Ekaterina Borodich, Aleksandr Beznosikov, Abdurakhmon Sadiev, Vadim Sushko, Nikolay Savelyev, Martin Takáč, Alexander Gasnikov</author><pubDate>Wed, 17 Apr 2024 18:16:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.07289v6</guid></item><item><title>ML-Bench: Evaluating Large Language Models for Code Generation in Repository-Level Machine Learning Tasks</title><link>http://arxiv.org/abs/2311.09835v2</link><description>While Large Language Models (LLMs) have demonstrated proficiency in codegeneration benchmarks, translating these results into practical developmentscenarios - where leveraging existing repository-level libraries is the norm -remains challenging. To bridge the gap between lab-scale benchmarks andreal-world coding practices, we introduce ML-Bench: a novel benchmark designedto assess LLMs' ability to integrate and utilize repository-level open-sourcelibraries to complete machine learning tasks. ML-Bench comprises a diverse setof 9,641 samples across 169 distinct tasks derived from 18 GitHub repositories.Our findings reveal that while GPT-4 outshines other LLMs, it successfullyaddresses only 33.82% of the tasks, highlighting the complexity of thechallenge. Complementarily, we introduce a baseline agent, ML-Agent, capable ofskillful codebase navigation and precise generation of functional codesegments. This groundwork aims at catalyzing the development of moresophisticated LLM agents that can handle the intricacies of real-worldprogramming. Our code, data, and models are available athttps://github.com/gersteinlab/ML-bench.</description><author>Yuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Liang Chen, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, Mark Gerstein</author><pubDate>Wed, 17 Apr 2024 18:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09835v2</guid></item><item><title>Generative Representational Instruction Tuning</title><link>http://arxiv.org/abs/2402.09906v2</link><description>All text-based language problems can be reduced to either generation orembedding. Current models only perform well at one or the other. We introducegenerative representational instruction tuning (GRIT) whereby a large languagemodel is trained to handle both generative and embedding tasks bydistinguishing between them through instructions. Compared to other openmodels, our resulting GritLM 7B sets a new state of the art on the Massive TextEmbedding Benchmark (MTEB) and outperforms all models up to its size on a rangeof generative tasks. By scaling up further, GritLM 8x7B outperforms all opengenerative language models that we tried while still being among the bestembedding models. Notably, we find that GRIT matches training on onlygenerative or embedding data, thus we can unify both at no performance loss.Among other benefits, the unification via GRIT speeds up Retrieval-AugmentedGeneration (RAG) by &gt; 60% for long documents, by no longer requiring separateretrieval and generation models. Models, code, etc. are freely available athttps://github.com/ContextualAI/gritlm.</description><author>Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela</author><pubDate>Wed, 17 Apr 2024 18:12:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09906v2</guid></item><item><title>Simple Image Signal Processing using Global Context Guidance</title><link>http://arxiv.org/abs/2404.11569v1</link><description>In modern smartphone cameras, the Image Signal Processor (ISP) is the coreelement that converts the RAW readings from the sensor into perceptuallypleasant RGB images for the end users. The ISP is typically proprietary andhandcrafted and consists of several blocks such as white balance, colorcorrection, and tone mapping. Deep learning-based ISPs aim to transform RAWimages into DSLR-like RGB images using deep neural networks. However, mostlearned ISPs are trained using patches (small regions) due to computationallimitations. Such methods lack global context, which limits their efficacy onfull-resolution images and harms their ability to capture global propertiessuch as color constancy or illumination. First, we propose a novel module thatcan be integrated into any neural ISP to capture the global context informationfrom the full RAW images. Second, we propose an efficient and simple neural ISPthat utilizes our proposed module. Our model achieves state-of-the-art resultson different benchmarks using diverse and real smartphone images.</description><author>Omar Elezabi, Marcos V. Conde, Radu Timofte</author><pubDate>Wed, 17 Apr 2024 18:11:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11569v1</guid></item><item><title>On the Scalability of GNNs for Molecular Graphs</title><link>http://arxiv.org/abs/2404.11568v1</link><description>Scaling deep learning models has been at the heart of recent revolutions inlanguage modelling and image generation. Practitioners have observed a strongrelationship between model size, dataset size, and performance. However,structure-based architectures such as Graph Neural Networks (GNNs) are yet toshow the benefits of scale mainly due to the lower efficiency of sparseoperations, large data requirements, and lack of clarity about theeffectiveness of various architectures. We address this drawback of GNNs bystudying their scaling behavior. Specifically, we analyze message-passingnetworks, graph Transformers, and hybrid architectures on the largest publiccollection of 2D molecular graphs. For the first time, we observe that GNNsbenefit tremendously from the increasing scale of depth, width, number ofmolecules, number of labels, and the diversity in the pretraining datasets,resulting in a 30.25% improvement when scaling to 1 billion parameters and28.98% improvement when increasing size of dataset to eightfold. We furtherdemonstrate strong finetuning scaling behavior on 38 tasks, outclassingprevious large models. We hope that our work paves the way for an era wherefoundational GNNs drive pharmaceutical drug discovery.</description><author>Maciej Sypetkowski, Frederik Wenkel, Farimah Poursafaei, Nia Dickson, Karush Suri, Philip Fradkin, Dominique Beaini</author><pubDate>Wed, 17 Apr 2024 18:11:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11568v1</guid></item><item><title>MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation</title><link>http://arxiv.org/abs/2404.11565v1</link><description>We introduce a new architecture for personalization of text-to-imagediffusion models, coined Mixture-of-Attention (MoA). Inspired by theMixture-of-Experts mechanism utilized in large language models (LLMs), MoAdistributes the generation workload between two attention pathways: apersonalized branch and a non-personalized prior branch. MoA is designed toretain the original model's prior by fixing its attention layers in the priorbranch, while minimally intervening in the generation process with thepersonalized branch that learns to embed subjects in the layout and contextgenerated by the prior branch. A novel routing mechanism manages thedistribution of pixels in each layer across these branches to optimize theblend of personalized and generic content creation. Once trained, MoAfacilitates the creation of high-quality, personalized images featuringmultiple subjects with compositions and interactions as diverse as thosegenerated by the original model. Crucially, MoA enhances the distinctionbetween the model's pre-existing capability and the newly augmentedpersonalized intervention, thereby offering a more disentangled subject-contextcontrol that was previously unattainable. Project page:https://snap-research.github.io/mixture-of-attention</description><author>Kuan-Chieh, Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, Kfir Aberman</author><pubDate>Wed, 17 Apr 2024 18:08:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11565v1</guid></item><item><title>Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding</title><link>http://arxiv.org/abs/2401.12798v3</link><description>Entity alignment (EA), a pivotal process in integrating multi-sourceKnowledge Graphs (KGs), seeks to identify equivalent entity pairs across thesegraphs. Most existing approaches regard EA as a graph representation learningtask, concentrating on enhancing graph encoders. However, the decoding processin EA - essential for effective operation and alignment accuracy - has receivedlimited attention and remains tailored to specific datasets and modelarchitectures, necessitating both entity and additional explicit relationembeddings. This specificity limits its applicability, particularly inGNN-based models. To address this gap, we introduce a novel, generalized, andefficient decoding approach for EA, relying solely on entity embeddings. Ourmethod optimizes the decoding process by minimizing Dirichlet energy, leadingto the gradient flow within the graph, to maximize graph homophily. Thediscretization of the gradient flow produces a fast and scalable approach,termed Triple Feature Propagation (TFP). TFP innovatively generalizes adjacencymatrices to multi-views matrices:entity-to-entity, entity-to-relation,relation-to-entity, and relation-to-triple. The gradient flow throughgeneralized matrices enables TFP to harness the multi-view structuralinformation of KGs. Rigorous experimentation on diverse public datasetsdemonstrates that our approach significantly enhances various EA methods.Notably, the approach achieves these advancements with less than 6 seconds ofadditional computational time, establishing a new benchmark in efficiency andadaptability for future EA methods.</description><author>Yuanyi Wang, Haifeng Sun, Jingyu Wang, Qi Qi, Shaoling Sun, Jianxin Liao</author><pubDate>Wed, 17 Apr 2024 18:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12798v3</guid></item><item><title>HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models</title><link>http://arxiv.org/abs/2403.11456v2</link><description>The ubiquitousness of social media has led to the need for reliable andefficient detection of offensive content to limit harmful effects. This has ledto a proliferation of datasets and models related to detecting offensivecontent. While sophisticated models have attained strong performance onindividual datasets, these models often do not generalize due to differencesbetween how "offensive content" is conceptualized, and the resultingdifferences in how these datasets are labeled. In this paper, we introduceHateCOT, a dataset of 52,000 samples drawn from diverse existing sources withexplanations generated by GPT-3.5-Turbo and human-curated. We show thatpre-training models for the detection of offensive content on HateCOTsignificantly boots open-sourced Language Models on three benchmark datasets inboth zero and few-shot settings, despite differences in domain and task.} Wefurther find that HateCOT enables effective K-shot fine-tuning in thelow-resource settings.</description><author>Huy Nghiem, Hal Daumé III</author><pubDate>Wed, 17 Apr 2024 17:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11456v2</guid></item><item><title>mEdIT: Multilingual Text Editing via Instruction Tuning</title><link>http://arxiv.org/abs/2402.16472v2</link><description>We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recentstate-of-the-art text editing models for writing assistance. mEdIT models aretrained by fine-tuning multi-lingual large, pre-trained language models (LLMs)via instruction tuning. They are designed to take instructions from the userspecifying the attributes of the desired text in the form of natural languageinstructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\'on(Spanish). We build mEdIT by curating data from multiple publicly availablehuman-annotated text editing datasets for three text editing tasks (GrammaticalError Correction (GEC), Text Simplification, and Paraphrasing) across diverselanguages belonging to six different language families. We detail the designand training of mEdIT models and demonstrate their strong performance on manymulti-lingual text editing benchmarks against other multilingual LLMs. We alsofind that mEdIT generalizes effectively to new languages over multilingualbaselines. We publicly release our data, code, and trained models athttps://github.com/vipulraheja/medit.</description><author>Vipul Raheja, Dimitris Alikaniotis, Vivek Kulkarni, Bashar Alhafni, Dhruv Kumar</author><pubDate>Wed, 17 Apr 2024 17:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16472v2</guid></item><item><title>Information theory for data-driven model reduction in physics and biology</title><link>http://arxiv.org/abs/2312.06608v2</link><description>Model reduction is the construction of simple yet predictive descriptions ofthe dynamics of many-body systems in terms of a few relevant variables. Aprerequisite to model reduction is the identification of these relevantvariables, a task for which no general method exists. Here, we develop asystematic approach based on the information bottleneck to identify therelevant variables, defined as those most predictive of the future. Weelucidate analytically the relation between these relevant variables and theeigenfunctions of the transfer operator describing the dynamics. Further, weshow that in the limit of high compression, the relevant variables are directlydetermined by the slowest-decaying eigenfunctions. Our information-basedapproach indicates when to optimally stop increasing the complexity of thereduced model. Furthermore, it provides a firm foundation to constructinterpretable deep learning tools that perform model reduction. We illustratehow these tools work in practice by considering uncurated videos of atmosphericflows from which our algorithms automatically extract the dominant slowcollective variables, as well as experimental videos of cyanobacteria coloniesin which we discover an emergent synchronization order parameter.</description><author>Matthew S. Schmitt, Maciej Koch-Janusz, Michel Fruchart, Daniel S. Seara, Michael Rust, Vincenzo Vitelli</author><pubDate>Wed, 17 Apr 2024 17:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06608v2</guid></item><item><title>Predicting Long-horizon Futures by Conditioning on Geometry and Time</title><link>http://arxiv.org/abs/2404.11554v1</link><description>Our work explores the task of generating future sensor observationsconditioned on the past. We are motivated by `predictive coding' concepts fromneuroscience as well as robotic applications such as self-driving vehicles.Predictive video modeling is challenging because the future may be multi-modaland learning at scale remains computationally expensive for video processing.To address both challenges, our key insight is to leverage the large-scalepretraining of image diffusion models which can handle multi-modality. Werepurpose image models for video prediction by conditioning on new frametimestamps. Such models can be trained with videos of both static and dynamicscenes. To allow them to be trained with modestly-sized datasets, we introduceinvariances by factoring out illumination and texture by forcing the model topredict (pseudo) depth, readily obtained for in-the-wild videos viaoff-the-shelf monocular depth networks. In fact, we show that simply modifyingnetworks to predict grayscale pixels already improves the accuracy of videoprediction. Given the extra controllability with timestamp conditioning, wepropose sampling schedules that work better than the traditional autoregressiveand hierarchical sampling strategies. Motivated by probabilistic metrics fromthe object forecasting literature, we create a benchmark for video predictionon a diverse set of videos spanning indoor and outdoor scenes and a largevocabulary of objects. Our experiments illustrate the effectiveness of learningto condition on timestamps, and show the importance of predicting the futurewith invariant modalities.</description><author>Tarasha Khurana, Deva Ramanan</author><pubDate>Wed, 17 Apr 2024 17:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11554v1</guid></item><item><title>Quantifying Multilingual Performance of Large Language Models Across Languages</title><link>http://arxiv.org/abs/2404.11553v1</link><description>The training process of Large Language Models (LLMs) requires extensive textcorpus. However, these data are often unevenly distributed in differentlanguages. As a result, LLMs perform well on common languages, such as English,German, and French, but perform poorly on low-resource languages. However,currently there is no work to quantitatively measure the performance of LLMs inlow-resource languages. To fill this gap, we proposed the Language Ranker thataims to benchmark and rank different languages according to the performance ofLLMs on those languages. We employ the LLM's performance on the English corpusas a baseline to compare the performances of different languages and English.We have the following three findings: 1. The performance rankings of differentLLMs in all languages are roughly the same. 2. LLMs with different sizes havethe same partial order of performance. 3. There is a strong correlation betweenLlaMa2's performance in different languages and the proportion of thepre-training corpus. These findings illustrate that the Language Ranker can beused as an indicator to measure the language performance of LLMs.</description><author>Zihao Li, Yucheng Shi, Zirui Liu, Fan Yang, Ninghao Liu, Mengnan Du</author><pubDate>Wed, 17 Apr 2024 17:53:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11553v1</guid></item><item><title>ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal Instance Segmentation</title><link>http://arxiv.org/abs/2403.11376v4</link><description>Amodal Instance Segmentation (AIS) presents a challenging task as it involvespredicting both visible and occluded parts of objects within images. ExistingAIS methods rely on a bidirectional approach, encompassing both the transitionfrom amodal features to visible features (amodal-to-visible) and from visiblefeatures to amodal features (visible-to-amodal). Our observation shows that theutilization of amodal features through the amodal-to-visible can confuse thevisible features due to the extra information of occluded/hidden segments notpresented in visible display. Consequently, this compromised quality of visiblefeatures during the subsequent visible-to-amodal transition. To tackle thisissue, we introduce ShapeFormer, a decoupled Transformer-based model with avisible-to-amodal transition. It facilitates the explicit relationship betweenoutput segmentations and avoids the need for amodal-to-visible transitions.ShapeFormer comprises three key modules: (i) Visible-Occluding Mask Head forpredicting visible segmentation with occlusion awareness, (ii) Shape-PriorAmodal Mask Head for predicting amodal and occluded masks, and (iii)Category-Specific Shape Prior Retriever aims to provide shape prior knowledge.Comprehensive experiments and extensive ablation studies across various AISbenchmarks demonstrate the effectiveness of our ShapeFormer. The code isavailable at: \url{https://github.com/UARK-AICV/ShapeFormer}</description><author>Minh Tran, Winston Bounsavy, Khoa Vo, Anh Nguyen, Tri Nguyen, Ngan Le</author><pubDate>Wed, 17 Apr 2024 17:46:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11376v4</guid></item><item><title>Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching</title><link>http://arxiv.org/abs/2312.03678v2</link><description>Non-isometric shape correspondence remains a fundamental challenge incomputer vision. Traditional methods using Laplace-Beltrami operator (LBO)eigenmodes face limitations in characterizing high-frequency extrinsic shapechanges like bending and creases. We propose a novel approach of combining thenon-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shellhessian with the intrinsic ones of the LBO, creating a hybrid spectral space inwhich we construct functional maps. To this end, we present a theoreticalframework to effectively integrate non-orthogonal basis functions intodescriptor- and learning-based functional map methods. Our approach can beincorporated easily into existing functional map pipelines across varyingapplications and is able to handle complex deformations beyond isometries. Weshow extensive evaluations across various supervised and unsupervised settingsand demonstrate significant improvements. Notably, our approach achieves up to15% better mean geodesic error for non-isometric correspondence settings and upto 45% improvement in scenarios with topological noise.</description><author>Lennart Bastian, Yizheng Xie, Nassir Navab, Zorah Lähner</author><pubDate>Wed, 17 Apr 2024 17:37:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03678v2</guid></item><item><title>Precise Asymptotics for Spectral Methods in Mixed Generalized Linear Models</title><link>http://arxiv.org/abs/2211.11368v3</link><description>In a mixed generalized linear model, the objective is to learn multiplesignals from unlabeled observations: each sample comes from exactly one signal,but it is not known which one. We consider the prototypical problem ofestimating two statistically independent signals in a mixed generalized linearmodel with Gaussian covariates. Spectral methods are a popular class ofestimators which output the top two eigenvectors of a suitable data-dependentmatrix. However, despite the wide applicability, their design is still obtainedvia heuristic considerations, and the number of samples $n$ needed to guaranteerecovery is super-linear in the signal dimension $d$. In this paper, we developexact asymptotics on spectral methods in the challenging proportional regime inwhich $n, d$ grow large and their ratio converges to a finite constant. Bydoing so, we are able to optimize the design of the spectral method, andcombine it with a simple linear estimator, in order to minimize the estimationerror. Our characterization exploits a mix of tools from random matrices, freeprobability and the theory of approximate message passing algorithms. Numericalsimulations for mixed linear regression and phase retrieval demonstrate theadvantage enabled by our analysis over existing designs of spectral methods.</description><author>Yihan Zhang, Marco Mondelli, Ramji Venkataramanan</author><pubDate>Wed, 17 Apr 2024 17:36:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11368v3</guid></item><item><title>Evaluating Span Extraction in Generative Paradigm: A Reflection on Aspect-Based Sentiment Analysis</title><link>http://arxiv.org/abs/2404.11539v1</link><description>In the era of rapid evolution of generative language models within the realmof natural language processing, there is an imperative call to revisit andreformulate evaluation methodologies, especially in the domain of aspect-basedsentiment analysis (ABSA). This paper addresses the emerging challengesintroduced by the generative paradigm, which has moderately blurred traditionalboundaries between understanding and generation tasks. Building upon prevailingpractices in the field, we analyze the advantages and shortcomings associatedwith the prevalent ABSA evaluation paradigms. Through an in-depth examination,supplemented by illustrative examples, we highlight the intricacies involved inaligning generative outputs with other evaluative metrics, specifically thosederived from other tasks, including question answering. While we steer clear ofadvocating for a singular and definitive metric, our contribution lies inpaving the path for a comprehensive guideline tailored for ABSA evaluations inthis generative paradigm. In this position paper, we aim to providepractitioners with profound reflections, offering insights and directions thatcan aid in navigating this evolving landscape, ensuring evaluations that areboth accurate and reflective of generative capabilities.</description><author>Soyoung Yang, Won Ik Cho</author><pubDate>Wed, 17 Apr 2024 17:33:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11539v1</guid></item><item><title>GenFighter: A Generative and Evolutive Textual Attack Removal</title><link>http://arxiv.org/abs/2404.11538v1</link><description>Adversarial attacks pose significant challenges to deep neural networks(DNNs) such as Transformer models in natural language processing (NLP). Thispaper introduces a novel defense strategy, called GenFighter, which enhancesadversarial robustness by learning and reasoning on the training classificationdistribution. GenFighter identifies potentially malicious instances deviatingfrom the distribution, transforms them into semantically equivalent instancesaligned with the training data, and employs ensemble techniques for a unifiedand robust response. By conducting extensive experiments, we show thatGenFighter outperforms state-of-the-art defenses in accuracy under attack andattack success rate metrics. Additionally, it requires a high number of queriesper attack, making the attack more challenging in real scenarios. The ablationstudy shows that our approach integrates transfer learning, agenerative/evolutive procedure, and an ensemble method, providing an effectivedefense against NLP adversarial attacks.</description><author>Md Athikul Islam, Edoardo Serra, Sushil Jajodia</author><pubDate>Wed, 17 Apr 2024 17:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11538v1</guid></item><item><title>DiscDiff: Latent Diffusion Model for DNA Sequence Generation</title><link>http://arxiv.org/abs/2402.06079v2</link><description>This paper introduces a novel framework for DNA sequence generation,comprising two key components: DiscDiff, a Latent Diffusion Model (LDM)tailored for generating discrete DNA sequences, and Absorb-Escape, apost-training algorithm designed to refine these sequences. Absorb-Escapeenhances the realism of the generated sequences by correcting `round errors'inherent in the conversion process between latent and input spaces. Ourapproach not only sets new standards in DNA sequence generation but alsodemonstrates superior performance over existing diffusion models, in generatingboth short and long DNA sequences. Additionally, we introduce EPD-GenDNA, thefirst comprehensive, multi-species dataset for DNA generation, encompassing160,000 unique sequences from 15 species. We hope this study will advance thegenerative modelling of DNA, with potential implications for gene therapy andprotein production.</description><author>Zehui Li, Yuhao Ni, William A V Beardall, Guoxuan Xia, Akashaditya Das, Guy-Bart Stan, Yiren Zhao</author><pubDate>Wed, 17 Apr 2024 17:31:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06079v2</guid></item><item><title>SSDiff: Spatial-spectral Integrated Diffusion Model for Remote Sensing Pansharpening</title><link>http://arxiv.org/abs/2404.11537v1</link><description>Pansharpening is a significant image fusion technique that merges the spatialcontent and spectral characteristics of remote sensing images to generatehigh-resolution multispectral images. Recently, denoising diffusionprobabilistic models have been gradually applied to visual tasks, enhancingcontrollable image generation through low-rank adaptation (LoRA). In thispaper, we introduce a spatial-spectral integrated diffusion model for theremote sensing pansharpening task, called SSDiff, which considers thepansharpening process as the fusion process of spatial and spectral componentsfrom the perspective of subspace decomposition. Specifically, SSDiff utilizesspatial and spectral branches to learn spatial details and spectral featuresseparately, then employs a designed alternating projection fusion module (APFM)to accomplish the fusion. Furthermore, we propose a frequency modulationinter-branch module (FMIM) to modulate the frequency distribution betweenbranches. The two components of SSDiff can perform favorably against the APFMwhen utilizing a LoRA-like branch-wise alternative fine-tuning method. Itrefines SSDiff to capture component-discriminating features more sufficiently.Finally, extensive experiments on four commonly used datasets, i.e.,WorldView-3, WorldView-2, GaoFen-2, and QuickBird, demonstrate the superiorityof SSDiff both visually and quantitatively. The code will be made open sourceafter possible acceptance.</description><author>Yu Zhong, Xiao Wu, Liang-Jian Deng, Zihan Cao</author><pubDate>Wed, 17 Apr 2024 17:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11537v1</guid></item><item><title>FedPFT: Federated Proxy Fine-Tuning of Foundation Models</title><link>http://arxiv.org/abs/2404.11536v1</link><description>Adapting Foundation Models (FMs) for downstream tasks through FederatedLearning (FL) emerges a promising strategy for protecting data privacy andvaluable FMs. Existing methods fine-tune FM by allocating sub-FM to clients inFL, however, leading to suboptimal performance due to insufficient tuning andinevitable error accumulations of gradients. In this paper, we proposeFederated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptationin downstream tasks through FL by two key modules. First, the sub-FMconstruction module employs a layer-wise compression approach, facilitatingcomprehensive FM fine-tuning across all layers by emphasizing those crucialneurons. Second, the sub-FM alignment module conducts a two-stepdistillations-layer-level and neuron-level-before and during FL fine-tuningrespectively, to reduce error of gradient by accurately aligning sub-FM with FMunder theoretical guarantees. Experimental results on seven commonly useddatasets (i.e., four text and three vision) demonstrate the superiority ofFedPFT.</description><author>Zhaopeng Peng, Xiaoliang Fan, Yufan Chen, Zheng Wang, Shirui Pan, Chenglu Wen, Ruisheng Zhang, Cheng Wang</author><pubDate>Wed, 17 Apr 2024 17:30:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11536v1</guid></item><item><title>Decomposing and Editing Predictions by Modeling Model Computation</title><link>http://arxiv.org/abs/2404.11534v1</link><description>How does the internal computation of a machine learning model transforminputs into predictions? In this paper, we introduce a task called componentmodeling that aims to address this question. The goal of component modeling isto decompose an ML model's prediction in terms of its components -- simplefunctions (e.g., convolution filters, attention heads) that are the "buildingblocks" of model computation. We focus on a special case of this task,component attribution, where the goal is to estimate the counterfactual impactof individual components on a given prediction. We then present COAR, ascalable algorithm for estimating component attributions; we demonstrate itseffectiveness across models, datasets, and modalities. Finally, we show thatcomponent attributions estimated with COAR directly enable model editing acrossfive tasks, namely: fixing model errors, ``forgetting'' specific classes,boosting subpopulation robustness, localizing backdoor attacks, and improvingrobustness to typographic attacks. We provide code for COAR athttps://github.com/MadryLab/modelcomponents .</description><author>Harshay Shah, Andrew Ilyas, Aleksander Madry</author><pubDate>Wed, 17 Apr 2024 17:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11534v1</guid></item><item><title>What are human values, and how do we align AI to them?</title><link>http://arxiv.org/abs/2404.10636v2</link><description>There is an emerging consensus that we need to align AI systems with humanvalues (Gabriel, 2020; Ji et al., 2024), but it remains unclear how to applythis to language models in practice. We split the problem of "aligning to humanvalues" into three parts: first, eliciting values from people; second,reconciling those values into an alignment target for training ML models; andthird, actually training the model. In this paper, we focus on the first twoparts, and ask the question: what are "good" ways to synthesize diverse humaninputs about values into a target for aligning language models? To answer thisquestion, we first define a set of 6 criteria that we believe must be satisfiedfor an alignment target to shape model behavior in accordance with humanvalues. We then propose a process for eliciting and reconciling values calledMoral Graph Elicitation (MGE), which uses a large language model to interviewparticipants about their values in particular contexts; our approach isinspired by the philosophy of values advanced by Taylor (1977), Chang (2004),and others. We trial MGE with a representative sample of 500 Americans, on 3intentionally divisive prompts (e.g. advice about abortion). Our resultsdemonstrate that MGE is promising for improving model alignment across all 6criteria. For example, almost all participants (89.1%) felt well represented bythe process, and (89%) thought the final moral graph was fair, even if theirvalue wasn't voted as the wisest. Our process often results in "expert" values(e.g. values from women who have solicited abortion advice) rising to the topof the moral graph, without defining who is considered an expert in advance.</description><author>Oliver Klingefjord, Ryan Lowe, Joe Edelman</author><pubDate>Wed, 17 Apr 2024 17:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10636v2</guid></item><item><title>Select and Reorder: A Novel Approach for Neural Sign Language Production</title><link>http://arxiv.org/abs/2404.11532v1</link><description>Sign languages, often categorised as low-resource languages, face significantchallenges in achieving accurate translation due to the scarcity of parallelannotated datasets. This paper introduces Select and Reorder (S&amp;R), a novelapproach that addresses data scarcity by breaking down the translation processinto two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Ourmethod leverages large spoken language models and the substantial lexicaloverlap between source spoken languages and target sign languages to establishan initial alignment. Both steps make use of Non-AutoRegressive (NAR) decodingfor reduced computation and faster inference speeds. Through thisdisentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores onthe Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1improvement of 37.88% in Text to Gloss (T2G) Translation. This innovativeapproach paves the way for more effective translation models for signlanguages, even in resource-constrained settings.</description><author>Harry Walsh, Ben Saunders, Richard Bowden</author><pubDate>Wed, 17 Apr 2024 17:25:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11532v1</guid></item><item><title>Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization</title><link>http://arxiv.org/abs/2404.11531v1</link><description>Fusing knowledge from multiple Large Language Models (LLMs) can combine theirdiverse strengths to achieve improved performance on a given task. However,current fusion approaches either rely on learning-based fusers that do notgeneralize to new LLMs, or do not take into account how well each LLMunderstands the input. In this work, we study LLM fusion at test-time, whichenables leveraging knowledge from arbitrary user-specified LLMs duringinference. We introduce Pack of LLMs (PackLLM), an effective method fortest-time fusion that leverages each LLM's expertise, given an input prompt.PackLLM performs model fusion by solving an optimization problem fordetermining each LLM's importance, so that perplexity over the input prompt isminimized. First, our simple PackLLM-sim variant validates that perplexity is agood indicator for measuring each LLM's expertise. Second, our PackLLM-optvariant approximately solves the perplexity minimization problem via a greedyalgorithm. The derived importance weights are used to combine the LLMs duringinference. We conduct experiments with over 100 total LLMs on a diverse set oftasks. Experimental results show that (i) perplexity is a reliable measure forLLM fusion, (ii) PackLLM outperforms test-time fusion baselines by 1.89%accuracy points, and (iii) PackLLM can leverage new LLMs to improve performanceover learning-based fusion approaches by 3.92-11.94% accuracy points.</description><author>Costas Mavromatis, Petros Karypis, George Karypis</author><pubDate>Wed, 17 Apr 2024 17:24:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11531v1</guid></item><item><title>Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</title><link>http://arxiv.org/abs/2403.14608v3</link><description>Large models represent a groundbreaking advancement in multiple applicationfields, enabling remarkable achievements across various tasks. However, theirunprecedented scale comes with significant computational costs. These models,often consisting of billions of parameters, require vast amounts ofcomputational resources for execution. Especially, the expansive scale andcomputational demands pose considerable challenges when customizing them forparticular downstream tasks, particularly over the hardware platformsconstrained by computational capabilities. Parameter Efficient Fine-Tuning(PEFT) provides a practical solution by efficiently adapt the large models overthe various downstream tasks. In particular, PEFT refers to the process ofadjusting the parameters of a pre-trained large models to adapt it to aspecific task while minimizing the number of additional parameters introducedor computational resources required. This approach is particularly importantwhen dealing with large language models with high parameter counts, asfine-tuning these models from scratch can be computationally expensive andresource-intensive, posing considerable challenges in the supporting systemplatform design. In this survey, we present comprehensive studies of variousPEFT algorithms, examining their performance and computational overhead.Moreover, we provide an overview of applications developed using different PEFTalgorithms and discuss common techniques employed to mitigate computation costsfor PEFT. In addition to the algorithmic perspective, we overview variousreal-world system designs to investigate the implementation costs associatedwith different PEFT algorithms. This survey serves as an indispensable resourcefor researchers aiming to understand both the PEFT algorithm and its systemimplementation, offering detailed insights into recent advancements andpractical applications.</description><author>Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang</author><pubDate>Wed, 17 Apr 2024 17:23:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14608v3</guid></item><item><title>Landmark Guided Active Exploration with State-specific Balance Coefficient</title><link>http://arxiv.org/abs/2306.17484v2</link><description>Goal-conditioned hierarchical reinforcement learning (GCHRL) decomposeslong-horizon tasks into sub-tasks through a hierarchical framework and it hasdemonstrated promising results across a variety of domains. However, thehigh-level policy's action space is often excessively large, presenting asignificant challenge to effective exploration and resulting in potentiallyinefficient training. In this paper, we design a measure of prospect forsub-goals by planning in the goal space based on the goal-conditioned valuefunction. Building upon the measure of prospect, we propose a landmark-guidedexploration strategy by integrating the measures of prospect and novelty whichaims to guide the agent to explore efficiently and improve sample efficiency.In order to dynamically consider the impact of prospect and novelty onexploration, we introduce a state-specific balance coefficient to balance thesignificance of prospect and novelty. The experimental results demonstrate thatour proposed exploration strategy significantly outperforms the baselinemethods across multiple tasks.</description><author>Fei Cui, Jiaojiao Fang, Mengke Yang, Guizhong Liu</author><pubDate>Wed, 17 Apr 2024 17:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17484v2</guid></item><item><title>StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning</title><link>http://arxiv.org/abs/2312.04865v2</link><description>Graph contrastive learning (GCL) has become a powerful tool for learninggraph data, but its scalability remains a significant challenge. In this work,we propose a simple yet effective training framework called StructuralCompression (StructComp) to address this issue. Inspired by a sparse low-rankapproximation on the diffusion matrix, StructComp trains the encoder with thecompressed nodes. This allows the encoder not to perform any message passingduring the training stage, and significantly reduces the number of sample pairsin the contrastive loss. We theoretically prove that the original GCL loss canbe approximated with the contrastive loss computed by StructComp. Moreover,StructComp can be regarded as an additional regularization term for GCL models,resulting in a more robust encoder. Empirical studies on various datasets showthat StructComp greatly reduces the time and memory consumption while improvingmodel performance compared to the vanilla GCL models and scalable trainingmethods.</description><author>Shengzhong Zhang, Wenjie Yang, Xinyuan Cao, Hongwei Zhang, Zengfeng Huang</author><pubDate>Wed, 17 Apr 2024 17:19:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04865v2</guid></item><item><title>PAC Privacy Preserving Diffusion Models</title><link>http://arxiv.org/abs/2312.01201v3</link><description>Data privacy protection is garnering increased attention among researchers.Diffusion models (DMs), particularly with strict differential privacy, canpotentially produce images with both high privacy and visual quality. However,challenges arise such as in ensuring robust protection in privatizing specificdata attributes, areas where current models often fall short. To address thesechallenges, we introduce the PAC Privacy Preserving Diffusion Model, a modelleverages diffusion principles and ensure Probably Approximately Correct (PAC)privacy. We enhance privacy protection by integrating a private classifierguidance into the Langevin Sampling Process. Additionally, recognizing the gapin measuring the privacy of models, we have developed a novel metric to gaugeprivacy levels. Our model, assessed with this new metric and supported byGaussian matrix computations for the PAC bound, has shown superior performancein privacy protection over existing leading private generative models accordingto benchmark tests.</description><author>Qipan Xu, Youlong Ding, Xinxi Zhang, Jie Gao, Hao Wang</author><pubDate>Wed, 17 Apr 2024 17:18:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01201v3</guid></item><item><title>A Comparison of Traditional and Deep Learning Methods for Parameter Estimation of the Ornstein-Uhlenbeck Process</title><link>http://arxiv.org/abs/2404.11526v1</link><description>We consider the Ornstein-Uhlenbeck (OU) process, a stochastic process widelyused in finance, physics, and biology. Parameter estimation of the OU processis a challenging problem. Thus, we review traditional tracking methods andcompare them with novel applications of deep learning to estimate theparameters of the OU process. We use a multi-layer perceptron to estimate theparameters of the OU process and compare its performance with traditionalparameter estimation methods, such as the Kalman filter and maximum likelihoodestimation. We find that the multi-layer perceptron can accurately estimate theparameters of the OU process given a large dataset of observed trajectories;however, traditional parameter estimation methods may be more suitable forsmaller datasets.</description><author>Jacob Fein-Ashley</author><pubDate>Wed, 17 Apr 2024 17:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11526v1</guid></item><item><title>JointViT: Modeling Oxygen Saturation Levels with Joint Supervision on Long-Tailed OCTA</title><link>http://arxiv.org/abs/2404.11525v1</link><description>The oxygen saturation level in the blood (SaO2) is crucial for health,particularly in relation to sleep-related breathing disorders. However,continuous monitoring of SaO2 is time-consuming and highly variable dependingon patients' conditions. Recently, optical coherence tomography angiography(OCTA) has shown promising development in rapidly and effectively screeningeye-related lesions, offering the potential for diagnosing sleep-relateddisorders. To bridge this gap, our paper presents three key contributions.Firstly, we propose JointViT, a novel model based on the Vision Transformerarchitecture, incorporating a joint loss function for supervision. Secondly, weintroduce a balancing augmentation technique during data preprocessing toimprove the model's performance, particularly on the long-tail distributionwithin the OCTA dataset. Lastly, through comprehensive experiments on the OCTAdataset, our proposed method significantly outperforms other state-of-the-artmethods, achieving improvements of up to 12.28% in overall accuracy. Thisadvancement lays the groundwork for the future utilization of OCTA indiagnosing sleep-related disorders. See project websitehttps://steve-zeyu-zhang.github.io/JointViT</description><author>Zeyu Zhang, Xuyin Qi, Mingxi Chen, Guangxi Li, Ryan Pham, Ayub Zuhair, Ella Berry, Zhibin Liao, Owen Siggs, Robert Mclaughlin, Jamie Craig, Minh-Son To</author><pubDate>Wed, 17 Apr 2024 17:16:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11525v1</guid></item><item><title>Provable Reward-Agnostic Preference-Based Reinforcement Learning</title><link>http://arxiv.org/abs/2305.18505v3</link><description>Preference-based Reinforcement Learning (PbRL) is a paradigm in which an RLagent learns to optimize a task using pair-wise preference-based feedback overtrajectories, rather than explicit reward signals. While PbRL has demonstratedpractical success in fine-tuning language models, existing theoretical workfocuses on regret minimization and fails to capture most of the practicalframeworks. In this study, we fill in such a gap between theoretical PbRL andpractical algorithms by proposing a theoretical reward-agnostic PbRL frameworkwhere exploratory trajectories that enable accurate learning of hidden rewardfunctions are acquired before collecting any human feedback. Theoreticalanalysis demonstrates that our algorithm requires less human feedback forlearning the optimal policy under preference-based models with linearparameterization and unknown transitions, compared to the existing theoreticalliterature. Specifically, our framework can incorporate linear and low-rankMDPs with efficient sample complexity. Additionally, we investigatereward-agnostic RL with action-based comparison feedback and introduce anefficient querying algorithm tailored to this scenario.</description><author>Wenhao Zhan, Masatoshi Uehara, Wen Sun, Jason D. Lee</author><pubDate>Wed, 17 Apr 2024 17:13:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18505v3</guid></item><item><title>SuperPrimitive: Scene Reconstruction at a Primitive Level</title><link>http://arxiv.org/abs/2312.05889v2</link><description>Joint camera pose and dense geometry estimation from a set of images or amonocular video remains a challenging problem due to its computationalcomplexity and inherent visual ambiguities. Most dense incrementalreconstruction systems operate directly on image pixels and solve for their 3Dpositions using multi-view geometry cues. Such pixel-level approaches sufferfrom ambiguities or violations of multi-view consistency (e.g. caused bytextureless or specular surfaces). We address this issue with a new image representation which we call aSuperPrimitive. SuperPrimitives are obtained by splitting images intosemantically correlated local regions and enhancing them with estimated surfacenormal directions, both of which are predicted by state-of-the-art single imageneural networks. This provides a local geometry estimate per SuperPrimitive,while their relative positions are adjusted based on multi-view observations. We demonstrate the versatility of our new representation by addressing three3D reconstruction tasks: depth completion, few-view structure from motion, andmonocular dense visual odometry.</description><author>Kirill Mazur, Gwangbin Bae, Andrew J. Davison</author><pubDate>Wed, 17 Apr 2024 17:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05889v2</guid></item><item><title>Embedding Privacy in Computational Social Science and Artificial Intelligence Research</title><link>http://arxiv.org/abs/2404.11515v1</link><description>Privacy is a human right. It ensures that individuals are free to engage indiscussions, participate in groups, and form relationships online or offlinewithout fear of their data being inappropriately harvested, analyzed, orotherwise used to harm them. Preserving privacy has emerged as a criticalfactor in research, particularly in the computational social science (CSS),artificial intelligence (AI) and data science domains, given their reliance onindividuals' data for novel insights. The increasing use of advancedcomputational models stands to exacerbate privacy concerns because, ifinappropriately used, they can quickly infringe privacy rights and lead toadverse effects for individuals - especially vulnerable groups - and society.We have already witnessed a host of privacy issues emerge with the advent oflarge language models (LLMs), such as ChatGPT, which further demonstrate theimportance of embedding privacy from the start. This article contributes to thefield by discussing the role of privacy and the primary issues that researchersworking in CSS, AI, data science and related domains are likely to face. Itthen presents several key considerations for researchers to ensure participantprivacy is best preserved in their research design, data collection and use,analysis, and dissemination of research results.</description><author>Keenan Jones, Fatima Zahrah, Jason R. C. Nurse</author><pubDate>Wed, 17 Apr 2024 17:07:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11515v1</guid></item><item><title>Event Cameras Meet SPADs for High-Speed, Low-Bandwidth Imaging</title><link>http://arxiv.org/abs/2404.11511v1</link><description>Traditional cameras face a trade-off between low-light performance andhigh-speed imaging: longer exposure times to capture sufficient light resultsin motion blur, whereas shorter exposures result in Poisson-corrupted noisyimages. While burst photography techniques help mitigate this tradeoff,conventional cameras are fundamentally limited in their sensor noisecharacteristics. Event cameras and single-photon avalanche diode (SPAD) sensorshave emerged as promising alternatives to conventional cameras due to theirdesirable properties. SPADs are capable of single-photon sensitivity withmicrosecond temporal resolution, and event cameras can measure brightnesschanges up to 1 MHz with low bandwidth requirements. We show that theseproperties are complementary, and can help achieve low-light, high-speed imagereconstruction with low bandwidth requirements. We introduce a sensor fusionframework to combine SPADs with event cameras to improves the reconstruction ofhigh-speed, low-light scenes while reducing the high bandwidth cost associatedwith using every SPAD frame. Our evaluation, on both synthetic and real sensordata, demonstrates significant enhancements ( &gt; 5 dB PSNR) in reconstructinglow-light scenes at high temporal resolution (100 kHz) compared to conventionalcameras. Event-SPAD fusion shows great promise for real-world applications,such as robotics or medical imaging.</description><author>Manasi Muglikar, Siddharth Somasundaram, Akshat Dave, Edoardo Charbon, Ramesh Raskar, Davide Scaramuzza</author><pubDate>Wed, 17 Apr 2024 17:06:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11511v1</guid></item><item><title>VC Theory for Inventory Policies</title><link>http://arxiv.org/abs/2404.11509v1</link><description>Advances in computational power and AI have increased interest inreinforcement learning approaches to inventory management. This paper providesa theoretical foundation for these approaches and investigates the benefits ofrestricting to policy structures that are well-established by decades ofinventory theory. In particular, we prove generalization guarantees forlearning several well-known classes of inventory policies, including base-stockand (s, S) policies, by leveraging the celebrated Vapnik-Chervonenkis (VC)theory. We apply the concepts of the Pseudo-dimension and Fat-shatteringdimension from VC theory to determine the generalizability of inventorypolicies, that is, the difference between an inventory policy's performance ontraining data and its expected performance on unseen data. We focus on aclassical setting without contexts, but allow for an arbitrary distributionover demand sequences and do not make any assumptions such as independence overtime. We corroborate our supervised learning results using numericalsimulations. Managerially, our theory and simulations translate to the following insights.First, there is a principle of "learning less is more" in inventory management:depending on the amount of data available, it may be beneficial to restrictoneself to a simpler, albeit suboptimal, class of inventory policies tominimize overfitting errors. Second, the number of parameters in a policy classmay not be the correct measure of overfitting error: in fact, the class ofpolicies defined by T time-varying base-stock levels exhibits a generalizationerror comparable to that of the two-parameter (s, S) policy class. Finally, ourresearch suggests situations in which it could be beneficial to incorporate theconcepts of base-stock and inventory position into black-box learning machines,instead of having these machines directly learn the order quantity actions.</description><author>Yaqi Xie, Will Ma, Linwei Xin</author><pubDate>Wed, 17 Apr 2024 17:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11509v1</guid></item><item><title>Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology</title><link>http://arxiv.org/abs/2403.06567v3</link><description>Content-based image retrieval (CBIR) has the potential to significantlyimprove diagnostic aid and medical research in radiology. Current CBIR systemsface limitations due to their specialization to certain pathologies, limitingtheir utility. In response, we propose using vision foundation models aspowerful and versatile off-the-shelf feature extractors for content-basedmedical image retrieval. By benchmarking these models on a comprehensivedataset of 1.6 million 2D radiological images spanning four modalities and 161pathologies, we identify weakly-supervised models as superior, achieving a P@1of up to 0.594. This performance not only competes with a specialized model butdoes so without the need for fine-tuning. Our analysis further explores thechallenges in retrieving pathological versus anatomical structures, indicatingthat accurate retrieval of pathological features presents greater difficulty.Despite these challenges, our research underscores the vast potential offoundation models for CBIR in radiology, proposing a shift towards versatile,general-purpose medical image retrieval systems that do not require specifictuning.</description><author>Stefan Denner, David Zimmerer, Dimitrios Bounias, Markus Bujotzek, Shuhan Xiao, Lisa Kausch, Philipp Schader, Tobias Penzkofer, Paul F. Jäger, Klaus Maier-Hein</author><pubDate>Wed, 17 Apr 2024 16:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06567v3</guid></item><item><title>Towards Coarse-to-Fine Evaluation of Inference Efficiency for Large Language Models</title><link>http://arxiv.org/abs/2404.11502v1</link><description>In real world, large language models (LLMs) can serve as the assistant tohelp users accomplish their jobs, and also support the development of advancedapplications. For the wide application of LLMs, the inference efficiency is anessential concern, which has been widely studied in existing work, and numerousoptimization algorithms and code libraries have been proposed to improve it.Nonetheless, users still find it challenging to compare the effectiveness ofall the above methods and understand the underlying mechanisms. In this work,we perform a detailed coarse-to-fine analysis of the inference performance ofvarious code libraries. To evaluate the overall effectiveness, we examine fourusage scenarios within two practical applications. We further provide boththeoretical and empirical fine-grained analyses of each module in theTransformer architecture. Our experiments yield comprehensive results that areinvaluable for researchers to evaluate code libraries and improve inferencestrategies.</description><author>Yushuo Chen, Tianyi Tang, Erge Xiang, Linjiang Li, Wayne Xin Zhao, Jing Wang, Yunpeng Chai, Ji-Rong Wen</author><pubDate>Wed, 17 Apr 2024 16:57:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11502v1</guid></item><item><title>Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2404.11500v1</link><description>This paper studies the relationship between the surface form of amathematical problem and its solvability by large language models. We find thatsubtle alterations in the surface form can significantly impact the answerdistribution and the solve rate, exposing the language model's lack ofrobustness and sensitivity to the surface form in reasoning through complexproblems. To improve mathematical reasoning performance, we proposeSelf-Consistency-over-Paraphrases (SCoP), which diversifies reasoning pathsfrom specific surface forms of the problem. We evaluate our approach on fourmathematics reasoning benchmarks over three large language models and show thatSCoP improves mathematical reasoning performance over vanilla self-consistency,particularly for problems initially deemed unsolvable. Finally, we provideadditional experiments and discussion regarding problem difficulty and surfaceforms, including cross-model difficulty agreement and paraphrasingtransferability, and Variance of Variations (VOV) for language modelevaluation.</description><author>Yue Zhou, Yada Zhu, Diego Antognini, Yoon Kim, Yang Zhang</author><pubDate>Wed, 17 Apr 2024 16:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11500v1</guid></item><item><title>A Data-Driven Representation for Sign Language Production</title><link>http://arxiv.org/abs/2404.11499v1</link><description>Phonetic representations are used when recording spoken languages, but noequivalent exists for recording signed languages. As a result, linguists haveproposed several annotation systems that operate on the gloss or sub-unitlevel; however, these resources are notably irregular and scarce. Sign Language Production (SLP) aims to automatically translate spokenlanguage sentences into continuous sequences of sign language. However, currentstate-of-the-art approaches rely on scarce linguistic resources to work. Thishas limited progress in the field. This paper introduces an innovative solutionby transforming the continuous pose generation problem into a discrete sequencegeneration problem. Thus, overcoming the need for costly annotation. Although,if available, we leverage the additional information to enhance our approach. By applying Vector Quantisation (VQ) to sign language data, we first learn acodebook of short motions that can be combined to create a natural sequence ofsign. Where each token in the codebook can be thought of as the lexicon of ourrepresentation. Then using a transformer we perform a translation from spokenlanguage text to a sequence of codebook tokens. Each token can be directlymapped to a sequence of poses allowing the translation to be performed by asingle network. Furthermore, we present a sign stitching method to effectivelyjoin tokens together. We evaluate on the RWTH-PHOENIX-Weather-2014T(PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets. Anextensive evaluation shows our approach outperforms previous methods,increasing the BLEU-1 back translation score by up to 72%.</description><author>Harry Walsh, Abolfazl Ravanshad, Mariam Rahmani, Richard Bowden</author><pubDate>Wed, 17 Apr 2024 16:52:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11499v1</guid></item><item><title>Runtime Analysis of Evolutionary Diversity Optimization on the Multi-objective (LeadingOnes, TrailingZeros) Problem</title><link>http://arxiv.org/abs/2404.11496v1</link><description>The diversity optimization is the class of optimization problems, in which weaim at finding a diverse set of good solutions. One of the frequently usedapproaches to solve such problems is to use evolutionary algorithms whichevolve a desired diverse population. This approach is called evolutionarydiversity optimization (EDO). In this paper, we analyse EDO on a 3-objective function LOTZ$_k$, which is amodification of the 2-objective benchmark function (LeadingOnes,TrailingZeros). We prove that the GSEMO computes a set of all Pareto-optimalsolutions in $O(kn^3)$ expected iterations. We also analyze the runtime of theGSEMO$_D$ (a modification of the GSEMO for diversity optimization) until itfinds a population with the best possible diversity for two different diversitymeasures, the total imbalance and the sorted imbalances vector. For the firstmeasure we show that the GSEMO$_D$ optimizes it asymptotically faster than itfinds a Pareto-optimal population, in $O(kn^2\log(n))$ expected iterations, andfor the second measure we show an upper bound of $O(k^2n^3\log(n))$ expectediterations. We complement our theoretical analysis with an empirical study,which shows a very similar behavior for both diversity measures that is closeto the theory predictions.</description><author>Denis Antipov, Aneta Neumann, Frank Neumann. Andrew M. Sutton</author><pubDate>Wed, 17 Apr 2024 16:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11496v1</guid></item><item><title>arcjetCV: an open-source software to analyze material ablation</title><link>http://arxiv.org/abs/2404.11492v1</link><description>arcjetCV is an open-source Python software designed to automate time-resolvedmeasurements of heatshield material recession and recession rates from arcjettest video footage. This new automated and accessible capability greatlyexceeds previous manual extraction methods, enabling rapid and detailedcharacterization of material recession for any sample with a profile video.arcjetCV automates the video segmentation process using machine learningmodels, including a one-dimensional (1D) Convolutional Neural Network (CNN) toinfer the time-window of interest, a two-dimensional (2D) CNN for image andedge segmentation, and a Local Outlier Factor (LOF) for outlier filtering. Agraphical user interface (GUI) simplifies the user experience and anapplication programming interface (API) allows users to call the core functionsfrom scripts, enabling video batch processing. arcjetCV's capability to measuretime-resolved recession in turn enables characterization of non-linearprocesses (shrinkage, swelling, melt flows, etc.), contributing to higherfidelity validation and improved modeling of heatshield material performance.The source code associated with this article can be found athttps://github.com/magnus-haw/arcjetCV.</description><author>Alexandre Quintart, Magnus Haw, Federico Semeraro</author><pubDate>Wed, 17 Apr 2024 16:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11492v1</guid></item><item><title>Multi-resolution Rescored ByteTrack for Video Object Detection on Ultra-low-power Embedded Systems</title><link>http://arxiv.org/abs/2404.11488v1</link><description>This paper introduces Multi-Resolution Rescored Byte-Track (MR2-ByteTrack), anovel video object detection framework for ultra-low-power embedded processors.This method reduces the average compute load of an off-the-shelf Deep NeuralNetwork (DNN) based object detector by up to 2.25$\times$ by alternating theprocessing of high-resolution images (320$\times$320 pixels) with multipledown-sized frames (192$\times$192 pixels). To tackle the accuracy degradationdue to the reduced image input size, MR2-ByteTrack correlates the outputdetections over time using the ByteTrack tracker and corrects potentialmisclassification using a novel probabilistic Rescore algorithm. Byinterleaving two down-sized images for every high-resolution one as the inputof different state-of-the-art DNN object detectors with our MR2-ByteTrack, wedemonstrate an average accuracy increase of 2.16% and a latency reduction of43% on the GAP9 microcontroller compared to a baseline frame-by-frame inferencescheme using exclusively full-resolution images. Code available at:https://github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack</description><author>Luca Bompani, Manuele Rusci, Daniele Palossi, Francesco Conti, Luca Benini</author><pubDate>Wed, 17 Apr 2024 16:45:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11488v1</guid></item><item><title>Randomly Pivoted Partial Cholesky: Random How?</title><link>http://arxiv.org/abs/2404.11487v1</link><description>We consider the problem of finding good low rank approximations of symmetric,positive-definite $A \in \mathbb{R}^{n \times n}$. Chen-Epperly-Tropp-Webbershowed, among many other things, that the randomly pivoted partial Choleskyalgorithm that chooses the $i-$th row with probability proportional to thediagonal entry $A_{ii}$ leads to a universal contraction of the trace norm (theSchatten 1-norm) in expectation for each step. We show that if one chooses the$i-$th row with likelihood proportional to $A_{ii}^2$ one obtains the sameresult in the Frobenius norm (the Schatten 2-norm). Implications for the greedypivoting rule and pivot selection strategies are discussed.</description><author>Stefan Steinerberger</author><pubDate>Wed, 17 Apr 2024 16:45:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11487v1</guid></item><item><title>Visibility into AI Agents</title><link>http://arxiv.org/abs/2401.13138v5</link><description>Increased delegation of commercial, scientific, governmental, and personalactivities to AI agents -- systems capable of pursuing complex goals withlimited supervision -- may exacerbate existing societal risks and introduce newrisks. Understanding and mitigating these risks involves critically evaluatingexisting governance structures, revising and adapting these structures whereneeded, and ensuring accountability of key stakeholders. Information aboutwhere, why, how, and by whom certain AI agents are used, which we refer to asvisibility, is critical to these objectives. In this paper, we assess threecategories of measures to increase visibility into AI agents: agentidentifiers, real-time monitoring, and activity logging. For each, we outlinepotential implementations that vary in intrusiveness and informativeness. Weanalyze how the measures apply across a spectrum of centralized throughdecentralized deployment contexts, accounting for various actors in the supplychain including hardware and software service providers. Finally, we discussthe implications of our measures for privacy and concentration of power.Further work into understanding the measures and mitigating their negativeimpacts can help to build a foundation for the governance of AI agents.</description><author>Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond, Herbie Bradley, Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt, Lennart Heim, Markus Anderljung</author><pubDate>Wed, 17 Apr 2024 16:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13138v5</guid></item><item><title>From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection and Model Querying</title><link>http://arxiv.org/abs/2310.04145v2</link><description>Safeguarding the Intellectual Property (IP) of data has become criticallyimportant as machine learning applications continue to proliferate, and theirsuccess heavily relies on the quality of training data. While variousmechanisms exist to secure data during storage, transmission, and consumption,fewer studies have been developed to detect whether they are already leaked formodel training without authorization. This issue is particularly challengingdue to the absence of information and control over the training processconducted by potential attackers. In this paper, we concentrate on the domain of tabular data and introduce anovel methodology, Local Distribution Shifting Synthesis (\textsc{LDSS}), todetect leaked data that are used to train classification models. The coreconcept behind \textsc{LDSS} involves injecting a small volume of syntheticdata--characterized by local shifts in class distribution--into the owner'sdataset. This enables the effective identification of models trained on leakeddata through model querying alone, as the synthetic data injection results in apronounced disparity in the predictions of models trained on leaked andmodified datasets. \textsc{LDSS} is \emph{model-oblivious} and hence compatiblewith a diverse range of classification models. We have conducted extensiveexperiments on seven types of classification models across five real-worlddatasets. The comprehensive results affirm the reliability, robustness,fidelity, security, and efficiency of \textsc{LDSS}. Extending \textsc{LDSS} toregression tasks further highlights its versatility and efficacy compared withbaseline methods.</description><author>Biao Wu, Qiang Huang, Anthony K. H. Tung</author><pubDate>Wed, 17 Apr 2024 16:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04145v2</guid></item><item><title>AgentKit: Flow Engineering with Graphs, not Coding</title><link>http://arxiv.org/abs/2404.11483v1</link><description>We propose an intuitive LLM prompting framework (AgentKit) formultifunctional agents. AgentKit offers a unified framework for explicitlyconstructing a complex "thought process" from simple natural language prompts.The basic building block in AgentKit is a node, containing a natural languageprompt for a specific subtask. The user then puts together chains of nodes,like stacking LEGO pieces. The chains of nodes can be designed to explicitlyenforce a naturally structured "thought process". For example, for the task ofwriting a paper, one may start with the thought process of 1) identify a coremessage, 2) identify prior research gaps, etc. The nodes in AgentKit can bedesigned and combined in different ways to implement multiple advancedcapabilities including on-the-fly hierarchical planning, reflection, andlearning from interactions. In addition, due to the modular nature and theintuitive design to simulate explicit human thought process, a basic agentcould be implemented as simple as a list of prompts for the subtasks andtherefore could be designed and tuned by someone without any programmingexperience. Quantitatively, we show that agents designed through AgentKitachieve SOTA performance on WebShop and Crafter. These advances underscoreAgentKit's potential in making LLM agents effective and accessible for a widerrange of applications. https://github.com/holmeswww/AgentKit</description><author>Yue Wu, Yewen Fan, So Yeon Min, Shrimai Prabhumoye, Stephen McAleer, Yonatan Bisk, Ruslan Salakhutdinov, Yuanzhi Li, Tom Mitchell</author><pubDate>Wed, 17 Apr 2024 16:40:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11483v1</guid></item><item><title>Discovering Nuclear Models from Symbolic Machine Learning</title><link>http://arxiv.org/abs/2404.11477v1</link><description>Numerous phenomenological nuclear models have been proposed to describespecific observables within different regions of the nuclear chart. However,developing a unified model that describes the complex behavior of all nucleiremains an open challenge. Here, we explore whether novel symbolic MachineLearning (ML) can rediscover traditional nuclear physics models or identifyalternatives with improved simplicity, fidelity, and predictive power. Toaddress this challenge, we developed a Multi-objective Iterated SymbolicRegression approach that handles symbolic regressions over multiple targetobservables, accounts for experimental uncertainties and is robust againsthigh-dimensional problems. As a proof of principle, we applied this method todescribe the nuclear binding energies and charge radii of light and medium massnuclei. Our approach identified simple analytical relationships based on thenumber of protons and neutrons, providing interpretable models with precisioncomparable to state-of-the-art nuclear models. Additionally, we integrated thisML-discovered model with an existing complementary model to estimate the limitsof nuclear stability. These results highlight the potential of symbolic ML todevelop accurate nuclear models and guide our description of complex many-bodyproblems.</description><author>Jose M. Munoz, Silviu M. Udrescu, Ronald F. Garcia Ruiz</author><pubDate>Wed, 17 Apr 2024 16:32:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11477v1</guid></item><item><title>Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and Regulatory Measures in the EU AI Act</title><link>http://arxiv.org/abs/2404.11476v1</link><description>Technological innovations have shown remarkable capabilities to benefit andharm society alike. AI constitutes a democratized sophisticated technologyaccessible to large parts of society, including malicious actors. This workproposes a taxonomy focusing on on (geo)political risks associated with AI. Itidentifies 12 risks in total divided into four categories: (1) GeopoliticalPressures, (2) Malicious Usage, (3) Environmental, Social, and Ethical Risks,and (4) Privacy and Trust Violations. Incorporating a regulatory side, thispaper conducts a policy assessment of the EU AI Act. Adopted in March 2023, thelandmark regulation has the potential to have a positive top-down impactconcerning AI risk reduction but needs regulatory adjustments to mitigate risksmore comprehensively. Regulatory exceptions for open-source models, excessivelyhigh parameters for the classification of GPAI models as a systemic risk, andthe exclusion of systems designed exclusively for military purposes from theregulation's obligations leave room for future action.</description><author>Sinan Arda</author><pubDate>Wed, 17 Apr 2024 16:32:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11476v1</guid></item><item><title>AdaIR: Exploiting Underlying Similarities of Image Restoration Tasks with Adapters</title><link>http://arxiv.org/abs/2404.11475v1</link><description>Existing image restoration approaches typically employ extensive networksspecifically trained for designated degradations. Despite being effective, suchmethods inevitably entail considerable storage costs and computationaloverheads due to the reliance on task-specific networks. In this work, we gobeyond this well-established framework and exploit the inherent commonalitiesamong image restoration tasks. The primary objective is to identify componentsthat are shareable across restoration tasks and augment the shared componentswith modules specifically trained for individual tasks. Towards this goal, wepropose AdaIR, a novel framework that enables low storage cost and efficienttraining without sacrificing performance. Specifically, a generic restorationnetwork is first constructed through self-supervised pre-training usingsynthetic degradations. Subsequent to the pre-training phase, adapters aretrained to adapt the pre-trained network to specific degradations. AdaIRrequires solely the training of lightweight, task-specific modules, ensuring amore efficient storage and training regimen. We have conducted extensiveexperiments to validate the effectiveness of AdaIR and analyze the influence ofthe pre-training strategy on discovering shareable components. Extensiveexperimental results show that AdaIR achieves outstanding results on multi-taskrestoration while utilizing significantly fewer parameters (1.9 MB) and lesstraining time (7 hours) for each restoration task. The source codes and trainedmodels will be released.</description><author>Hao-Wei Chen, Yu-Syuan Xu, Kelvin C. K. Chan, Hsien-Kai Kuo, Chun-Yi Lee, Ming-Hsuan Yang</author><pubDate>Wed, 17 Apr 2024 16:31:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11475v1</guid></item><item><title>Towards Highly Realistic Artistic Style Transfer via Stable Diffusion with Step-aware and Layer-aware Prompt</title><link>http://arxiv.org/abs/2404.11474v1</link><description>Artistic style transfer aims to transfer the learned artistic style onto anarbitrary content image, generating artistic stylized images. Existinggenerative adversarial network-based methods fail to generate highly realisticstylized images and always introduce obvious artifacts and disharmoniouspatterns. Recently, large-scale pre-trained diffusion models opened up a newway for generating highly realistic artistic stylized images. However,diffusion model-based methods generally fail to preserve the content structureof input content images well, introducing some undesired content structure andstyle patterns. To address the above problems, we propose a novel pre-traineddiffusion-based artistic style transfer method, called LSAST, which cangenerate highly realistic artistic stylized images while preserving the contentstructure of input content images well, without bringing obvious artifacts anddisharmonious style patterns. Specifically, we introduce a Step-aware andLayer-aware Prompt Space, a set of learnable prompts, which can learn the styleinformation from the collection of artworks and dynamically adjusts the inputimages' content structure and style pattern. To train our prompt space, wepropose a novel inversion method, called Step-ware and Layer-aware PromptInversion, which allows the prompt space to learn the style information of theartworks collection. In addition, we inject a pre-trained conditional branch ofControlNet into our LSAST, which further improved our framework's ability tomaintain content structure. Extensive experiments demonstrate that our proposedmethod can generate more highly realistic artistic stylized images than thestate-of-the-art artistic style transfer methods.</description><author>Zhanjie Zhang, Quanwei Zhang, Huaizhong Lin, Wei Xing, Juncheng Mo, Shuaicheng Huang, Jinheng Xie, Guangyuan Li, Junsheng Luan, Lei Zhao, Dalong Zhang, Lixia Chen</author><pubDate>Wed, 17 Apr 2024 16:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11474v1</guid></item><item><title>A Federated Learning Approach to Privacy Preserving Offensive Language Identification</title><link>http://arxiv.org/abs/2404.11470v1</link><description>The spread of various forms of offensive speech online is an importantconcern in social media. While platforms have been investing heavily in ways ofcoping with this problem, the question of privacy remains largely unaddressed.Models trained to detect offensive language on social media are trained and/orfine-tuned using large amounts of data often stored in centralized servers.Since most social media data originates from end users, we propose a privacypreserving decentralized architecture for identifying offensive language onlineby introducing Federated Learning (FL) in the context of offensive languageidentification. FL is a decentralized architecture that allows multiple modelsto be trained locally without the need for data sharing hence preserving users'privacy. We propose a model fusion approach to perform FL. We trained multipledeep learning models on four publicly available English benchmark datasets(AHSD, HASOC, HateXplain, OLID) and evaluated their performance in detail. Wealso present initial cross-lingual experiments in English and Spanish. We showthat the proposed model fusion approach outperforms baselines in all thedatasets while preserving privacy.</description><author>Marcos Zampieri, Damith Premasiri, Tharindu Ranasinghe</author><pubDate>Wed, 17 Apr 2024 16:23:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11470v1</guid></item><item><title>Solving morphological analogies: from retrieval to generation</title><link>http://arxiv.org/abs/2303.18062v2</link><description>Analogical inference is a remarkable capability of human reasoning, and hasbeen used to solve hard reasoning tasks. Analogy based reasoning (AR) hasgained increasing interest from the artificial intelligence community and hasshown its potential in multiple machine learning tasks such as classification,decision making and recommendation with competitive results. We propose a deeplearning (DL) framework to address and tackle two key tasks in AR: analogydetection and solving. The framework is thoroughly tested on the Siganalogiesdataset of morphological analogical proportions (APs) between words, and shownto outperform symbolic approaches in many languages. Previous work haveexplored the behavior of the Analogy Neural Network for classification (ANNc)on analogy detection and of the Analogy Neural Network for retrieval (ANNr) onanalogy solving by retrieval, as well as the potential of an autoencoder (AE)for analogy solving by generating the solution word. In this article wesummarize these findings and we extend them by combining ANNr and the AEembedding model, and checking the performance of ANNc as an retrieval method.The combination of ANNr and AE outperforms the other approaches in almost allcases, and ANNc as a retrieval method achieves competitive or betterperformance than 3CosMul. We conclude with general guidelines on using ourframework to tackle APs with DL.</description><author>Esteban Marquer, Miguel Couceiro</author><pubDate>Wed, 17 Apr 2024 16:23:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.18062v2</guid></item><item><title>Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model</title><link>http://arxiv.org/abs/2310.09089v2</link><description>Integrating large language models (LLMs) into healthcare holds greatpotential but faces challenges. Pre-training LLMs from scratch for domains likemedicine is resource-heavy and often unfeasible. On the other hand, solereliance on Supervised Fine-tuning (SFT) can result in overconfidentpredictions and may not tap into domain-specific insights. In response, wepresent a multi-stage training method combining Domain-specific ContinuedPre-training (DCPT), SFT, and Direct Preference Optimization (DPO). Inaddition, we publish a 3Gb Chinese Medicine (ChiMed) dataset, encompassingmedical question answering, plain texts, knowledge graphs, and dialogues,segmented into three training stages. The medical LLM trained with ourpipeline, Qilin-Med, shows substantial performance improvement. In the CPT andSFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set,respectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by7.5%. In the DPO phase, it scored 16.66 in BLEU-1 and 27.44 in ROUGE-1 on theHuatuo-26M test set, bringing further improvement to the SFT phase (12.69 inBLEU-1 and 24.21 in ROUGE-1). Additionally, we have further enhanced themodel's performance through the Retrieval Augmented Generation (RAG) approach.Experiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8%on CMExam. These results highlight the contribution of our novel trainingapproach in building LLMs for medical applications.</description><author>Qichen Ye, Junling Liu, Dading Chong, Peilin Zhou, Yining Hua, Fenglin Liu, Meng Cao, Ziming Wang, Xuxin Cheng, Zhu Lei, Zhenhua Guo</author><pubDate>Wed, 17 Apr 2024 16:18:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09089v2</guid></item><item><title>Interpreting and generalizing deep learning in physics-based problems with functional linear models</title><link>http://arxiv.org/abs/2307.04569v2</link><description>Although deep learning has achieved remarkable success in various scientificmachine learning applications, its opaque nature poses concerns regardinginterpretability and generalization capabilities beyond the training data.Interpretability is crucial and often desired in modeling physical systems.Moreover, acquiring extensive datasets that encompass the entire range of inputfeatures is challenging in many physics-based learning tasks, leading toincreased errors when encountering out-of-distribution (OOD) data. In thiswork, motivated by the field of functional data analysis (FDA), we proposegeneralized functional linear models as an interpretable surrogate for atrained deep learning model. We demonstrate that our model could be trainedeither based on a trained neural network (post-hoc interpretation) or directlyfrom training data (interpretable operator learning). A library of generalizedfunctional linear models with different kernel functions is considered andsparse regression is used to discover an interpretable surrogate model thatcould be analytically presented. We present test cases in solid mechanics,fluid mechanics, and transport. Our results demonstrate that our model canachieve comparable accuracy to deep learning and can improve OOD generalizationwhile providing more transparency and interpretability. Our study underscoresthe significance of interpretable representation in scientific machine learningand showcases the potential of functional linear models as a tool forinterpreting and generalizing deep learning.</description><author>Amirhossein Arzani, Lingxiao Yuan, Pania Newell, Bei Wang</author><pubDate>Wed, 17 Apr 2024 16:16:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04569v2</guid></item><item><title>Predictive representations: building blocks of intelligence</title><link>http://arxiv.org/abs/2402.06590v2</link><description>Adaptive behavior often requires predicting future events. The theory ofreinforcement learning prescribes what kinds of predictive representations areuseful and how to compute them. This paper integrates these theoretical ideaswith work on cognition and neuroscience. We pay special attention to thesuccessor representation (SR) and its generalizations, which have been widelyapplied both as engineering tools and models of brain function. Thisconvergence suggests that particular kinds of predictive representations mayfunction as versatile building blocks of intelligence.</description><author>Wilka Carvalho, Momchil S. Tomov, William de Cothi, Caswell Barry, Samuel J. Gershman</author><pubDate>Wed, 17 Apr 2024 16:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06590v2</guid></item><item><title>Influencer Backdoor Attack on Semantic Segmentation</title><link>http://arxiv.org/abs/2303.12054v5</link><description>When a small number of poisoned samples are injected into the trainingdataset of a deep neural network, the network can be induced to exhibitmalicious behavior during inferences, which poses potential threats toreal-world applications. While they have been intensively studied inclassification, backdoor attacks on semantic segmentation have been largelyoverlooked. Unlike classification, semantic segmentation aims to classify everypixel within a given image. In this work, we explore backdoor attacks onsegmentation models to misclassify all pixels of a victim class by injecting aspecific trigger on non-victim pixels during inferences, which is dubbedInfluencer Backdoor Attack (IBA). IBA is expected to maintain theclassification accuracy of non-victim pixels and mislead classifications of allvictim pixels in every single inference and could be easily applied toreal-world scenes. Based on the context aggregation ability of segmentationmodels, we proposed a simple, yet effective, Nearest-Neighbor trigger injectionstrategy. We also introduce an innovative Pixel Random Labeling strategy whichmaintains optimal performance even when the trigger is placed far from thevictim pixels. Our extensive experiments reveal that current segmentationmodels do suffer from backdoor attacks, demonstrate IBA real-worldapplicability, and show that our proposed techniques can further increaseattack performance.</description><author>Haoheng Lan, Jindong Gu, Philip Torr, Hengshuang Zhao</author><pubDate>Wed, 17 Apr 2024 16:12:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12054v5</guid></item><item><title>Leave No One Behind: Online Self-Supervised Self-Distillation for Sequential Recommendation</title><link>http://arxiv.org/abs/2404.07219v2</link><description>Sequential recommendation methods play a pivotal role in modernrecommendation systems. A key challenge lies in accurately modeling userpreferences in the face of data sparsity. To tackle this challenge, recentmethods leverage contrastive learning (CL) to derive self-supervision signalsby maximizing the mutual information of two augmented views of the originaluser behavior sequence. Despite their effectiveness, CL-based methods encountera limitation in fully exploiting self-supervision signals for users withlimited behavior data, as users with extensive behaviors naturally offer moreinformation. To address this problem, we introduce a novel learning paradigm,named Online Self-Supervised Self-distillation for Sequential Recommendation($S^4$Rec), effectively bridging the gap between self-supervised learning andself-distillation methods. Specifically, we employ online clustering toproficiently group users by their distinct latent intents. Additionally, anadversarial learning strategy is utilized to ensure that the clusteringprocedure is not affected by the behavior length factor. Subsequently, weemploy self-distillation to facilitate the transfer of knowledge from userswith extensive behaviors (teachers) to users with limited behaviors (students).Experiments conducted on four real-world datasets validate the effectiveness ofthe proposed method.</description><author>Shaowei Wei, Zhengwei Wu, Xin Li, Qintong Wu, Zhiqiang Zhang, Jun Zhou, Lihong Gu, Jinjie Gu</author><pubDate>Wed, 17 Apr 2024 16:10:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07219v2</guid></item><item><title>Using Game Engines and Machine Learning to Create Synthetic Satellite Imagery for a Tabletop Verification Exercise</title><link>http://arxiv.org/abs/2404.11461v1</link><description>Satellite imagery is regarded as a great opportunity for citizen-basedmonitoring of activities of interest. Relevant imagery may however not beavailable at sufficiently high resolution, quality, or cadence -- let alone beuniformly accessible to open-source analysts. This limits an assessment of thetrue long-term potential of citizen-based monitoring of nuclear activitiesusing publicly available satellite imagery. In this article, we demonstrate howmodern game engines combined with advanced machine-learning techniques can beused to generate synthetic imagery of sites of interest with the ability tochoose relevant parameters upon request; these include time of day, cloudcover, season, or level of activity onsite. At the same time, resolution andoff-nadir angle can be adjusted to simulate different characteristics of thesatellite. While there are several possible use-cases for synthetic imagery,here we focus on its usefulness to support tabletop exercises in which simplemonitoring scenarios can be examined to better understand verificationcapabilities enabled by new satellite constellations and very short revisittimes.</description><author>Johannes Hoster, Sara Al-Sayed, Felix Biessmann, Alexander Glaser, Kristian Hildebrand, Igor Moric, Tuong Vy Nguyen</author><pubDate>Wed, 17 Apr 2024 16:09:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11461v1</guid></item><item><title>Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent</title><link>http://arxiv.org/abs/2404.11459v1</link><description>A multimodal AI agent is characterized by its ability to process and learnfrom various types of data, including natural language, visual, and audioinputs, to inform its actions. Despite advancements in large language modelsthat incorporate visual data, such as GPT-4V, effectively translatingimage-based data into actionable outcomes for AI agents continues to bechallenging. In this paper, we introduce a multimodal model that incorporatesthe concept of functional token specifically designed for AI agentapplications. To ensure compatibility with edge devices, our model is optimizedto a compact size of less than 1B parameters. Like GPT-4, our model can processboth English and Chinese. We demonstrate that this model is capable ofoperating efficiently on a wide range of edge devices, including as constrainedas a Raspberry Pi.</description><author>Wei Chen, Zhiyuan Li</author><pubDate>Wed, 17 Apr 2024 16:07:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11459v1</guid></item><item><title>Learn to Tour: Operator Design For Solution Feasibility Mapping in Pickup-and-delivery Traveling Salesman Problem</title><link>http://arxiv.org/abs/2404.11458v1</link><description>This paper aims to develop a learning method for a special class of travelingsalesman problems (TSP), namely, the pickup-and-delivery TSP (PDTSP), whichfinds the shortest tour along a sequence of one-to-one pickup-and-deliverynodes. One-to-one here means that the transported people or goods areassociated with designated pairs of pickup and delivery nodes, in contrast tothat indistinguishable goods can be delivered to any nodes. In PDTSP,precedence constraints need to be satisfied that each pickup node must bevisited before its corresponding delivery node. Classic operations research(OR) algorithms for PDTSP are difficult to scale to large-sized problems.Recently, reinforcement learning (RL) has been applied to TSPs. The basic ideais to explore and evaluate visiting sequences in a solution space. However,this approach could be less computationally efficient, as it has to potentiallyevaluate many infeasible solutions of which precedence constraints areviolated. To restrict solution search within a feasible space, we utilizeoperators that always map one feasible solution to another, without spendingtime exploring the infeasible solution space. Such operators are evaluated andselected as policies to solve PDTSPs in an RL framework. We make a comparisonof our method and baselines, including classic OR algorithms and existinglearning methods. Results show that our approach can find tours shorter thanbaselines.</description><author>Bowen Fang, Xu Chen, Xuan Di</author><pubDate>Wed, 17 Apr 2024 16:05:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11458v1</guid></item><item><title>Unifying Bias and Unfairness in Information Retrieval: A Survey of Challenges and Opportunities with Large Language Models</title><link>http://arxiv.org/abs/2404.11457v1</link><description>With the rapid advancement of large language models (LLMs), informationretrieval (IR) systems, such as search engines and recommender systems, haveundergone a significant paradigm shift. This evolution, while heralding newopportunities, introduces emerging challenges, particularly in terms of biasesand unfairness, which may threaten the information ecosystem. In this paper, wepresent a comprehensive survey of existing works on emerging and pressing biasand unfairness issues in IR systems when the integration of LLMs. We firstunify bias and unfairness issues as distribution mismatch problems, providing agroundwork for categorizing various mitigation strategies through distributionalignment. Subsequently, we systematically delve into the specific bias andunfairness issues arising from three critical stages of LLMs integration intoIR systems: data collection, model development, and result evaluation. In doingso, we meticulously review and analyze recent literature, focusing on thedefinitions, characteristics, and corresponding mitigation strategiesassociated with these issues. Finally, we identify and highlight some openproblems and challenges for future work, aiming to inspire researchers andstakeholders in the IR field and beyond to better understand and mitigate biasand unfairness issues of IR in this LLM era. We also consistently maintain aGitHub repository for the relevant papers and resources in this risingdirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.</description><author>Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, Jun Xu</author><pubDate>Wed, 17 Apr 2024 16:05:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11457v1</guid></item><item><title>The LuViRA Dataset: Measurement Description</title><link>http://arxiv.org/abs/2302.05309v2</link><description>We present a dataset to evaluate localization algorithms, which utilizesvision, audio, and radio sensors: the Lund University Vision, Radio, and Audio(LuViRA) Dataset. The dataset includes RGB images, corresponding depth maps,IMU readings, channel response between a massive MIMO channel sounder and auser equipment, audio recorded by 12 microphones, and 0.5 mm accurate 6DoF poseground truth. We synchronize these sensors to make sure that all data arerecorded simultaneously. A camera, speaker, and transmit antenna are placed ontop of a slowly moving service robot and 88 trajectories are recorded. Eachtrajectory includes 20 to 50 seconds of recorded sensor data and ground truthlabels. The data from different sensors can be used separately or jointly toconduct localization tasks and a motion capture system is used to verify theresults obtained by the localization algorithms. The main aim of this datasetis to enable research on fusing the most commonly used sensors for localizationtasks. However, the full dataset or some parts of it can also be used for otherresearch areas such as channel estimation, image classification, etc. Fusingsensor data can lead to increased localization accuracy and reliability, aswell as decreased latency and power consumption. The created dataset will bemade public at a later date.</description><author>Ilayda Yaman, Guoda Tian, Martin Larsson, Patrik Persson, Michiel Sandra, Alexander Dürr, Erik Tegler, Nikhil Challa, Henrik Garde, Fredrik Tufvesson, Kalle Åström, Ove Edfors, Steffen Malkowsky, Liang Liu</author><pubDate>Wed, 17 Apr 2024 16:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05309v2</guid></item><item><title>Reformatted Alignment</title><link>http://arxiv.org/abs/2402.12219v2</link><description>The quality of finetuning data is crucial for aligning large language models(LLMs) with human values. Current methods to improve data quality are eitherlabor-intensive or prone to factual errors caused by LLM hallucinations. Thispaper explores elevating the quality of existing instruction data to betteralign with human values, introducing a simple and effective approach namedReAlign, which reformats the responses of instruction data into a format thatbetter aligns with pre-established criteria and the collated evidence. Thisapproach minimizes human annotation, hallucination, and the difficulty inscaling, remaining orthogonal to existing alignment techniques. Experimentally,ReAlign significantly boosts the general alignment ability, math reasoning,factuality, and readability of the LLMs. Encouragingly, without introducing any additional data or advanced trainingtechniques, and merely by reformatting the response, LLaMA-2-13B's mathematicalreasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignmentability measured by the Alpaca dataset. This work highlights the need forfurther research into the science and mechanistic interpretability of LLMs. Wehave made the associated code and data publicly accessible to support futurestudies at https://github.com/GAIR-NLP/ReAlign.</description><author>Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu</author><pubDate>Wed, 17 Apr 2024 16:03:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12219v2</guid></item><item><title>ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs</title><link>http://arxiv.org/abs/2305.15964v5</link><description>The integration of Computer-Aided Diagnosis (CAD) with Large Language Models(LLMs) presents a promising frontier in clinical applications, notably inautomating diagnostic processes akin to those performed by radiologists andproviding consultations similar to a virtual family doctor. Despite thepromising potential of this integration, current works face at least twolimitations: (1) From the perspective of a radiologist, existing studiestypically have a restricted scope of applicable imaging domains, failing tomeet the diagnostic needs of different patients. Also, the insufficientdiagnostic capability of LLMs further undermine the quality and reliability ofthe generated medical reports. (2) Current LLMs lack the requisite depth inmedical expertise, rendering them less effective as virtual family doctors dueto the potential unreliability of the advice provided during patientconsultations. To address these limitations, we introduce ChatCAD+, to beuniversal and reliable. Specifically, it is featured by two main modules: (1)Reliable Report Generation and (2) Reliable Interaction. The Reliable ReportGeneration module is capable of interpreting medical images from diversedomains and generate high-quality medical reports via our proposed hierarchicalin-context learning. Concurrently, the interaction module leverages up-to-dateinformation from reputable medical websites to provide reliable medical advice.Together, these designed modules synergize to closely align with the expertiseof human medical professionals, offering enhanced consistency and reliabilityfor interpretation and advice. The source code is available athttps://github.com/zhaozh10/ChatCAD.</description><author>Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu, Lanzhuju Mei, Zixu Zhuang, Zhiming Cui, Qian Wang, Dinggang Shen</author><pubDate>Wed, 17 Apr 2024 16:01:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15964v5</guid></item><item><title>GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration</title><link>http://arxiv.org/abs/2403.17089v2</link><description>The advent of ChatGPT and similar large language models (LLMs) hasrevolutionized the human-AI interaction and information-seeking process.Leveraging LLMs as an alternative to search engines, users can now accesssummarized information tailored to their queries, significantly reducing thecognitive load associated with navigating vast information resources. Thisshift underscores the potential of LLMs in redefining information accessparadigms. Drawing on the foundation of task-focused information retrieval andLLMs' task planning ability, this research extends the scope of LLMcapabilities beyond routine task automation to support users in navigatinglong-term and significant life tasks. It introduces the GOLF framework(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' abilityto assist in significant life decisions through goal orientation and long-termplanning. The methodology encompasses a comprehensive simulation study to testthe framework's efficacy, followed by model and human evaluations to develop adataset benchmark for long-term life tasks, and experiments across differentmodels and settings. By shifting the focus from short-term tasks to the broaderspectrum of long-term life goals, this research underscores the transformativepotential of LLMs in enhancing human decision-making processes and taskmanagement, marking a significant step forward in the evolution of human-AIcollaboration.</description><author>Ben Wang</author><pubDate>Wed, 17 Apr 2024 16:00:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17089v2</guid></item><item><title>ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation</title><link>http://arxiv.org/abs/2403.18807v4</link><description>In the absence of parallax cues, a learning-based single image depthestimation (SIDE) model relies heavily on shading and contextual cues in theimage. While this simplicity is attractive, it is necessary to train suchmodels on large and varied datasets, which are difficult to capture. It hasbeen shown that using embeddings from pre-trained foundational models, such asCLIP, improves zero shot transfer in several applications. Taking inspirationfrom this, in our paper we explore the use of global image priors generatedfrom a pre-trained ViT model to provide more detailed contextual information.We argue that the embedding vector from a ViT model, pre-trained on a largedataset, captures greater relevant information for SIDE than the usual route ofgenerating pseudo image captions, followed by CLIP based text embeddings. Basedon this idea, we propose a new SIDE model using a diffusion backbone which isconditioned on ViT embeddings. Our proposed design establishes a newstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of0.059 (14% improvement) compared to 0.069 by the current SOTA (VPD). And onKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to0.142 by the current SOTA (GEDepth). For zero-shot transfer with a modeltrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,18%, 45%, 9%) by ZoeDepth. The project page is available athttps://ecodepth-iitd.github.io</description><author>Suraj Patni, Aradhye Agarwal, Chetan Arora</author><pubDate>Wed, 17 Apr 2024 15:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18807v4</guid></item><item><title>AI-Enhanced Cognitive Behavioral Therapy: Deep Learning and Large Language Models for Extracting Cognitive Pathways from Social Media Texts</title><link>http://arxiv.org/abs/2404.11449v1</link><description>Cognitive Behavioral Therapy (CBT) is an effective technique for addressingthe irrational thoughts stemming from mental illnesses, but it necessitatesprecise identification of cognitive pathways to be successfully implemented inpatient care. In current society, individuals frequently express negativeemotions on social media on specific topics, often exhibiting cognitivedistortions, including suicidal behaviors in extreme cases. Yet, there is anotable absence of methodologies for analyzing cognitive pathways that couldaid psychotherapists in conducting effective interventions online. In thisstudy, we gathered data from social media and established the task ofextracting cognitive pathways, annotating the data based on a cognitivetheoretical framework. We initially categorized the task of extractingcognitive pathways as a hierarchical text classification with four maincategories and nineteen subcategories. Following this, we structured a textsummarization task to help psychotherapists quickly grasp the essentialinformation. Our experiments evaluate the performance of deep learning andlarge language models (LLMs) on these tasks. The results demonstrate that ourdeep learning method achieved a micro-F1 score of 62.34% in the hierarchicaltext classification task. Meanwhile, in the text summarization task, GPT-4attained a Rouge-1 score of 54.92 and a Rouge-2 score of 30.86, surpassing theexperimental deep learning model's performance. However, it may suffer from anissue of hallucination. We have made all models and codes publicly available tosupport further research in this field.</description><author>Meng Jiang, Yi Jing Yu, Qing Zhao, Jianqiang Li, Changwei Song, Hongzhi Qi, Wei Zhai, Dan Luo, Xiaoqin Wang, Guanghui Fu, Bing Xiang Yang</author><pubDate>Wed, 17 Apr 2024 15:55:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11449v1</guid></item><item><title>Research on emotionally intelligent dialogue generation based on automatic dialogue system</title><link>http://arxiv.org/abs/2404.11447v1</link><description>Automated dialogue systems are important applications of artificialintelligence, and traditional systems struggle to understand user emotions andprovide empathetic feedback. This study integrates emotional intelligencetechnology into automated dialogue systems and creates a dialogue generationmodel with emotional intelligence through deep learning and natural languageprocessing techniques. The model can detect and understand a wide range ofemotions and specific pain signals in real time, enabling the system to provideempathetic interaction. By integrating the results of the study "Can artificialintelligence detect pain and express pain empathy?", the model's ability tounderstand the subtle elements of pain empathy has been enhanced, settinghigher standards for emotional intelligence dialogue systems. The project aimsto provide theoretical understanding and practical suggestions to integrateadvanced emotional intelligence capabilities into dialogue systems, therebyimproving user experience and interaction quality.</description><author>Jin Wang, JinFei Wang, Shuying Dai, Jiqiang Yu, Keqin Li</author><pubDate>Wed, 17 Apr 2024 15:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11447v1</guid></item><item><title>Open-Ended Wargames with Large Language Models</title><link>http://arxiv.org/abs/2404.11446v1</link><description>Wargames are a powerful tool for understanding and rehearsing real-worlddecision making. Automated play of wargames using artificial intelligence (AI)enables possibilities beyond those of human-conducted games, such as playingthe game many times over to see a range of possible outcomes. There are twocategories of wargames: quantitative games, with discrete types of moves, andqualitative games, which revolve around open-ended responses. Historically,automation efforts have focused on quantitative games, but large languagemodels (LLMs) make it possible to automate qualitative wargames. We introduce"Snow Globe," an LLM-powered multi-agent system for playing qualitativewargames. With Snow Globe, every stage of a text-based qualitative wargame fromscenario preparation to post-game analysis can be optionally carried out by AI,humans, or a combination thereof. We describe its software architectureconceptually and release an open-source implementation alongside thispublication. As case studies, we simulate a tabletop exercise about an AIincident response and a political wargame about a geopolitical crisis. Wediscuss potential applications of the approach and how it fits into the broaderwargaming ecosystem.</description><author>Daniel P. Hogan, Andrea Brennen</author><pubDate>Wed, 17 Apr 2024 15:54:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11446v1</guid></item><item><title>Distance and Collision Probability Estimation from Gaussian Surface Models</title><link>http://arxiv.org/abs/2402.00186v2</link><description>This paper describes continuous-space methodologies to estimate the collisionprobability, Euclidean distance and gradient between an ellipsoidal robot modeland an environment surface modeled as a set of Gaussian distributions.Continuous-space collision probability estimation is critical foruncertainty-aware motion planning. Most collision detection and avoidanceapproaches assume the robot is modeled as a sphere, but ellipsoidalrepresentations provide tighter approximations and enable navigation incluttered and narrow spaces. State-of-the-art methods derive the Euclideandistance and gradient by processing raw point clouds, which is computationallyexpensive for large workspaces. Recent advances in Gaussian surface modeling(e.g. mixture models, splatting) enable compressed and high-fidelity surfacerepresentations. Few methods exist to estimate continuous-space occupancy fromsuch models. They require Gaussians to model free space and are unable toestimate the collision probability, Euclidean distance and gradient for anellipsoidal robot. The proposed methods bridge this gap by extending prior workin ellipsoid-to-ellipsoid Euclidean distance and collision probabilityestimation to Gaussian surface models. A geometric blending approach is alsoproposed to improve collision probability estimation. The approaches areevaluated with numerical 2D and 3D experiments using real-world point clouddata. Methods for efficient calculation of these quantities are demonstrated toexecute within a few microseconds per ellipsoid pair using a single-thread onlow-power CPUs of modern embedded computers</description><author>Kshitij Goel, Wennie Tabib</author><pubDate>Wed, 17 Apr 2024 15:54:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00186v2</guid></item><item><title>Prediction of Unmanned Surface Vessel Motion Attitude Based on CEEMDAN-PSO-SVM</title><link>http://arxiv.org/abs/2404.11443v1</link><description>Unmanned boats, while navigating at sea, utilize active compensation systemsto mitigate wave disturbances experienced by onboard instruments and equipment.However, there exists a lag in the measurement of unmanned boat attitudes, thusintroducing unmanned boat motion attitude prediction to compensate for the lagin the signal acquisition process. This paper, based on the basic principles ofwaves, derives the disturbance patterns of waves on unmanned boats from thewave energy spectrum. Through simulation analysis of unmanned boat motionattitudes, motion attitude data is obtained, providing experimental data forsubsequent work. A combined prediction model based on Complete EnsembleEmpirical Mode Decomposition with Adaptive Noise (CEEMDAN), Particle SwarmOptimization (PSO), and Support Vector Machine (SVM) is designed to predict themotion attitude of unmanned boats. Simulation results validate its superiorprediction accuracy compared to traditional prediction models. For example, interms of mean absolute error, it improves by 17% compared to the EMD-PSO-SVMmodel.</description><author>Zhuoya Geng, Jianmei Chen, Wanqiang Zhu</author><pubDate>Wed, 17 Apr 2024 15:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11443v1</guid></item><item><title>Additive Covariance Matrix Models: Modelling Regional Electricity Net-Demand in Great Britain</title><link>http://arxiv.org/abs/2211.07451v3</link><description>Forecasts of regional electricity net-demand, consumption minus embeddedgeneration, are an essential input for reliable and economic power systemoperation, and energy trading. While such forecasts are typically performedregion by region, operations such as managing power flows require spatiallycoherent joint forecasts, which account for cross-regional dependencies. Here,we forecast the joint distribution of net-demand across the 14 regionsconstituting Great Britain's electricity network. Joint modelling iscomplicated by the fact that the net-demand variability within each region, andthe dependencies between regions, vary with temporal, socio-economical andweather-related factors. We accommodate for these characteristics by proposinga multivariate Gaussian model based on a modified Cholesky parametrisation,which allows us to model each unconstrained parameter via an additive model.Given that the number of model parameters and covariates is large, we adopt asemi-automated approach to model selection, based on gradient boosting. Inaddition to comparing the forecasting performance of several versions of theproposed model with that of two non-Gaussian copula-based models, we visuallyexplore the model output to interpret how the covariates affect net-demandvariability and dependencies. The code for reproducing the results in this paper is available athttps://doi.org/10.5281/zenodo.7315105, while methods for building and fittingmultivariate Gaussian additive models are provided by the SCM R package,available at https://github.com/VinGioia90/SCM.</description><author>V. Gioia, M. Fasiolo, J. Browell, R. Bellio</author><pubDate>Wed, 17 Apr 2024 15:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07451v3</guid></item><item><title>Neuroevolving Electronic Dynamical Networks</title><link>http://arxiv.org/abs/2404.04587v2</link><description>Neuroevolution is a powerful method of applying an evolutionary algorithm torefine the performance of artificial neural networks through natural selection;however, the fitness evaluation of these networks can be time-consuming andcomputationally expensive, particularly for continuous time recurrent neuralnetworks (CTRNNs) that necessitate the simulation of differential equations. Toovercome this challenge, field programmable gate arrays (FPGAs) have emerged asan increasingly popular solution, due to their high performance and low powerconsumption. Further, their ability to undergo dynamic and partialreconfiguration enables the extremely rapid evaluation of the fitness ofCTRNNs, effectively addressing the bottleneck associated with conventionalmethods of evolvable hardware. By incorporating fitness evaluation directlyupon the programmable logic of the FPGA, hyper-parallel evaluation becomesfeasible, dramatically reducing the time required for assessment. This inherentparallelism of FPGAs accelerates the entire neuroevolutionary process byseveral orders of magnitude, facilitating faster convergence to an optimalsolution. The work presented in this study demonstrates the potential ofutilizing dynamic and partial reconfiguration on capable FPGAs as a powerfulplatform for neuroevolving dynamic neural networks.</description><author>Derek Whitley</author><pubDate>Wed, 17 Apr 2024 15:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04587v2</guid></item></channel></rss>