<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 12 Jun 2024 06:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Blur-aware Spatio-temporal Sparse Transformer for Video Deblurring</title><link>http://arxiv.org/abs/2406.07551v1</link><description>Video deblurring relies on leveraging information from other frames in thevideo sequence to restore the blurred regions in the current frame. Mainstreamapproaches employ bidirectional feature propagation, spatio-temporaltransformers, or a combination of both to extract information from the videosequence. However, limitations in memory and computational resourcesconstraints the temporal window length of the spatio-temporal transformer,preventing the extraction of longer temporal contextual information from thevideo sequence. Additionally, bidirectional feature propagation is highlysensitive to inaccurate optical flow in blurry frames, leading to erroraccumulation during the propagation process. To address these issues, wepropose \textbf{BSSTNet}, \textbf{B}lur-aware \textbf{S}patio-temporal\textbf{S}parse \textbf{T}ransformer Network. It introduces the blur map, whichconverts the originally dense attention into a sparse form, enabling a moreextensive utilization of information throughout the entire video sequence.Specifically, BSSTNet (1) uses a longer temporal window in the transformer,leveraging information from more distant frames to restore the blurry pixels inthe current frame. (2) introduces bidirectional feature propagation guided byblur maps, which reduces error accumulation caused by the blur frame. Theexperimental results demonstrate the proposed BSSTNet outperforms thestate-of-the-art methods on the GoPro and DVD datasets.</description><author>Huicong Zhang, Haozhe Xie, Hongxun Yao</author><pubDate>Tue, 11 Jun 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07551v1</guid></item><item><title>An Image is Worth 32 Tokens for Reconstruction and Generation</title><link>http://arxiv.org/abs/2406.07550v1</link><description>Recent advancements in generative models have highlighted the crucial role ofimage tokenization in the efficient synthesis of high-resolution images.Tokenization, which transforms images into latent representations, reducescomputational demands compared to directly processing pixels and enhances theeffectiveness and efficiency of the generation process. Prior methods, such asVQGAN, typically utilize 2D latent grids with fixed downsampling factors.However, these 2D tokenizations face challenges in managing the inherentredundancies present in images, where adjacent regions frequently displaysimilarities. To overcome this issue, we introduce Transformer-based1-Dimensional Tokenizer (TiTok), an innovative approach that tokenizes imagesinto 1D latent sequences. TiTok provides a more compact latent representation,yielding substantially more efficient and effective representations thanconventional techniques. For example, a 256 x 256 x 3 image can be reduced tojust 32 discrete tokens, a significant reduction from the 256 or 1024 tokensobtained by prior methods. Despite its compact nature, TiTok achievescompetitive performance to state-of-the-art approaches. Specifically, using thesame generator framework, TiTok attains 1.97 gFID, outperforming MaskGITbaseline significantly by 4.21 at ImageNet 256 x 256 benchmark. The advantagesof TiTok become even more significant when it comes to higher resolution. AtImageNet 512 x 512 benchmark, TiTok not only outperforms state-of-the-artdiffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the imagetokens by 64x, leading to 410x faster generation process. Our best-performingvariant can significantly surpasses DiT-XL/2 (gFID 2.13 vs. 3.04) while stillgenerating high-quality samples 74x faster.</description><author>Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, Liang-Chieh Chen</author><pubDate>Tue, 11 Jun 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07550v1</guid></item><item><title>Image and Video Tokenization with Binary Spherical Quantization</title><link>http://arxiv.org/abs/2406.07548v1</link><description>We propose a new transformer-based image and video tokenizer with BinarySpherical Quantization (BSQ). BSQ projects the high-dimensional visualembedding to a lower-dimensional hypersphere and then applies binaryquantization. BSQ is (1) parameter-efficient without an explicit codebook, (2)scalable to arbitrary token dimensions, and (3) compact: compressing visualdata by up to 100$\times$ with minimal distortion. Our tokenizer uses atransformer encoder and decoder with simple block-wise causal masking tosupport variable-length videos as input. The resulting BSQ-ViT achievesstate-of-the-art visual reconstruction quality on image and videoreconstruction benchmarks with 2.4$\times$ throughput compared to the bestprior methods. Furthermore, by learning an autoregressive prior for adaptivearithmetic coding, BSQ-ViT achieves comparable results on video compressionwith state-of-the-art video compression standards. BSQ-ViT also enables maskedlanguage models to achieve competitive image synthesis quality to GAN- anddiffusion-based methods.</description><author>Yue Zhao, Yuanjun Xiong, Philipp Krähenbühl</author><pubDate>Tue, 11 Jun 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07548v1</guid></item><item><title>Zero-shot Image Editing with Reference Imitation</title><link>http://arxiv.org/abs/2406.07547v1</link><description>Image editing serves as a practical yet challenging task considering thediverse demands from users, where one of the hardest parts is to preciselydescribe how the edited image should look like. In this work, we present a newform of editing, termed imitative editing, to help users exercise theircreativity more conveniently. Concretely, to edit an image region of interest,users are free to directly draw inspiration from some in-the-wild references(e.g., some relative pictures come across online), without having to cope withthe fit between the reference and the source. Such a design requires the systemto automatically figure out what to expect from the reference to perform theediting. For this purpose, we propose a generative training framework, dubbedMimicBrush, which randomly selects two frames from a video clip, masks someregions of one frame, and learns to recover the masked regions using theinformation from the other frame. That way, our model, developed from adiffusion prior, is able to capture the semantic correspondence betweenseparate images in a self-supervised manner. We experimentally show theeffectiveness of our method under various test cases as well as its superiorityover existing alternatives. We also construct a benchmark to facilitate furtherresearch.</description><author>Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, Hengshuang Zhao</author><pubDate>Tue, 11 Jun 2024 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07547v1</guid></item><item><title>Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?</title><link>http://arxiv.org/abs/2406.07546v1</link><description>We present a novel task and benchmark for evaluating the ability oftext-to-image(T2I) generation models to produce images that fit commonsense inreal life, which we call Commonsense-T2I. Given two adversarial text promptscontaining an identical set of action words with minor differences, such as "alightbulb without electricity" v.s. "a lightbulb with electricity", we evaluatewhether T2I models can conduct visual-commonsense reasoning, e.g. produceimages that fit "the lightbulb is unlit" vs. "the lightbulb is lit"correspondingly. Commonsense-T2I presents an adversarial challenge, providingpairwise text prompts along with expected outputs. The dataset is carefullyhand-curated by experts and annotated with fine-grained labels, such ascommonsense type and likelihood of the expected outputs, to assist analyzingmodel behavior. We benchmark a variety of state-of-the-art (sota) T2I modelsand surprisingly find that, there is still a large gap between image synthesisand real life photos--even the DALL-E 3 model could only achieve 48.92% onCommonsense-T2I, and the stable diffusion XL model only achieves 24.92%accuracy. Our experiments show that GPT-enriched prompts cannot solve thischallenge, and we include a detailed analysis about possible reasons for suchdeficiency. We aim for Commonsense-T2I to serve as a high-quality evaluationbenchmark for T2I commonsense checking, fostering advancements in real lifeimage generation.</description><author>Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, Dan Roth</author><pubDate>Tue, 11 Jun 2024 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07546v1</guid></item><item><title>Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena</title><link>http://arxiv.org/abs/2406.07545v1</link><description>Multiple-choice questions (MCQ) are frequently used to assess large languagemodels (LLMs). Typically, an LLM is given a question and selects the answerdeemed most probable after adjustments for factors like length. Unfortunately,LLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due toinherent biases of priori unbalanced probabilities, influencing the predictionof answers based on these IDs. Previous research has introduced methods toreduce this ''selection bias'' by simply permutating options on a few testsamples and applying to new ones. Another problem of MCQ is the lottery ticketchoice by ''random guessing''. The LLM does not learn particular knowledge, butthe option is guessed correctly. This situation is especially serious for thosesmall-scale LLMs. To address them, a more thorough approach involves shiftingfrom MCQ to open-style questions, which can fundamentally eliminate selectionbias and random guessing issues. However, transitioning causes its own set ofchallenges in (1) identifying suitable open-style questions and (2) validatingthe correctness of LLM open-style responses against human-annotatedground-truths. This work aims to tackle these significant difficulties, andestablish a new LLM evaluation benchmark through entirely open-style questions.Consequently, we introduce the Open-LLM-Leaderboard to track various LLMs'performance and reflect true capability of them, such as GPT-4o/4/3.5, Claude3, Gemini, etc. Our code and dataset are available athttps://github.com/VILA-Lab/Open-LLM-Leaderboard.</description><author>Aidar Myrzakhan, Sondos Mahmoud Bsharat, Zhiqiang Shen</author><pubDate>Tue, 11 Jun 2024 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07545v1</guid></item><item><title>Situational Awareness Matters in 3D Vision Language Reasoning</title><link>http://arxiv.org/abs/2406.07544v1</link><description>Being able to carry out complicated vision language reasoning tasks in 3Dspace represents a significant milestone in developing household robots andhuman-centered embodied AI. In this work, we demonstrate that a critical anddistinct challenge in 3D vision language reasoning is situational awareness,which incorporates two key components: (1) The autonomous agent grounds itsself-location based on a language prompt. (2) The agent answers open-endedquestions from the perspective of its calculated position. To address thischallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3Dvision language reasoning. We tokenize the 3D scene into sparse voxelrepresentation and propose a language-grounded situation estimator, followed bya situated question answering module. Experiments on the SQA3D and ScanQAdatasets show that SIG3D outperforms state-of-the-art models in situationestimation and question answering by a large margin (e.g., an enhancement ofover 30% on situation estimation accuracy). Subsequent analysis corroboratesour architectural design choices, explores the distinct functions of visual andtextual tokens, and highlights the importance of situational awareness in thedomain of 3D question answering.</description><author>Yunze Man, Liang-Yan Gui, Yu-Xiong Wang</author><pubDate>Tue, 11 Jun 2024 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07544v1</guid></item><item><title>Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning</title><link>http://arxiv.org/abs/2406.07543v1</link><description>Recently, vision model pre-training has evolved from relying on manuallyannotated datasets to leveraging large-scale, web-crawled image-text data.Despite these advances, there is no pre-training method that effectivelyexploits the interleaved image-text data, which is very prevalent on theInternet. Inspired by the recent success of compression learning in naturallanguage processing, we propose a novel vision model pre-training method calledLatent Compression Learning (LCL) for interleaved image-text data. This methodperforms latent compression learning by maximizing the mutual informationbetween the inputs and outputs of a causal attention model. The trainingobjective can be decomposed into two basic tasks: 1) contrastive learningbetween visual representation and preceding context, and 2) generatingsubsequent text based on visual representation. Our experiments demonstratethat our method not only matches the performance of CLIP on paired pre-trainingdatasets (e.g., LAION), but can also leverage interleaved pre-training data(e.g., MMC4) to learn robust visual representation from scratch, showcasing thepotential of vision model pre-training with interleaved image-text data. Codeis released at https://github.com/OpenGVLab/LCL.</description><author>Chenyu Yang, Xizhou Zhu, Jinguo Zhu, Weijie Su, Junjie Wang, Xuan Dong, Wenhai Wang, Lewei Lu, Bin Li, Jie Zhou, Yu Qiao, Jifeng Dai</author><pubDate>Tue, 11 Jun 2024 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07543v1</guid></item><item><title>Cognitive Insights Across Languages: Enhancing Multimodal Interview Analysis</title><link>http://arxiv.org/abs/2406.07542v1</link><description>Cognitive decline is a natural process that occurs as individuals age. Earlydiagnosis of anomalous decline is crucial for initiating professional treatmentthat can enhance the quality of life of those affected. To address this issue,we propose a multimodal model capable of predicting Mild Cognitive Impairmentand cognitive scores. The TAUKADIAL dataset is used to conduct the evaluation,which comprises audio recordings of clinical interviews. The proposed modeldemonstrates the ability to transcribe and differentiate between languages usedin the interviews. Subsequently, the model extracts audio and text features,combining them into a multimodal architecture to achieve robust and generalizedresults. Our approach involves in-depth research to implement various featuresobtained from the proposed modalities.</description><author>David Ortiz-Perez, Jose Garcia-Rodriguez, David Tomás</author><pubDate>Tue, 11 Jun 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07542v1</guid></item><item><title>CDSA: Conservative Denoising Score-based Algorithm for Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2406.07541v1</link><description>Distribution shift is a major obstacle in offline reinforcement learning,which necessitates minimizing the discrepancy between the learned policy andthe behavior policy to avoid overestimating rare or unseen actions. Previousconservative offline RL algorithms struggle to generalize to unseen actions,despite their success in learning good in-distribution policy. In contrast, wepropose to use the gradient fields of the dataset density generated from apre-trained offline RL algorithm to adjust the original actions. We decouplethe conservatism constraints from the policy, thus can benefit wide offline RLalgorithms. As a consequence, we propose the Conservative Denoising Score-basedAlgorithm (CDSA) which utilizes the denoising score-based model to model thegradient of the dataset density, rather than the dataset density itself, andfacilitates a more accurate and efficient method to adjust the action generatedby the pre-trained policy in a deterministic and continuous MDP environment. Inexperiments, we show that our approach significantly improves the performanceof baseline algorithms in D4RL datasets, and demonstrate the generalizabilityand plug-and-play capability of our model across different pre-trained offlineRL policy in different tasks. We also validate that the agent exhibits greaterrisk aversion after employing our method while showcasing its ability togeneralize effectively across diverse tasks.</description><author>Zeyuan Liu, Kai Yang, Xiu Li</author><pubDate>Tue, 11 Jun 2024 18:59:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07541v1</guid></item><item><title>Ctrl-X: Controlling Structure and Appearance for Text-To-Image Generation Without Guidance</title><link>http://arxiv.org/abs/2406.07540v1</link><description>Recent controllable generation approaches such as FreeControl and DiffusionSelf-guidance bring fine-grained spatial and appearance control totext-to-image (T2I) diffusion models without training auxiliary modules.However, these methods optimize the latent embedding for each type of scorefunction with longer diffusion steps, making the generation processtime-consuming and limiting their flexibility and use. This work presentsCtrl-X, a simple framework for T2I diffusion controlling structure andappearance without additional training or guidance. Ctrl-X designs feed-forwardstructure control to enable the structure alignment with a structure image andsemantic-aware appearance transfer to facilitate the appearance transfer from auser-input image. Extensive qualitative and quantitative experiments illustratethe superior performance of Ctrl-X on various condition inputs and modelcheckpoints. In particular, Ctrl-X supports novel structure and appearancecontrol with arbitrary condition images of any modality, exhibits superiorimage quality and appearance transfer compared to existing works, and providesinstant plug-and-play functionality to any T2I and text-to-video (T2V)diffusion model. See our project page for an overview of the results:https://genforce.github.io/ctrl-x</description><author>Kuan Heng Lin, Sicheng Mo, Ben Klingher, Fangzhou Mu, Bolei Zhou</author><pubDate>Tue, 11 Jun 2024 18:59:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07540v1</guid></item><item><title>Autoregressive Pretraining with Mamba in Vision</title><link>http://arxiv.org/abs/2406.07537v1</link><description>The vision community has started to build with the recently developed statespace model, Mamba, as the new backbone for a range of tasks. This paper showsthat Mamba's visual capability can be significantly enhanced throughautoregressive pretraining, a direction not previously explored.Efficiency-wise, the autoregressive nature can well capitalize on the Mamba'sunidirectional recurrent structure, enabling faster overall training speedcompared to other training strategies like mask modeling. Performance-wise,autoregressive pretraining equips the Mamba architecture with markedly higheraccuracy over its supervised-trained counterparts and, more importantly,successfully unlocks its scaling potential to large and even huge model sizes.For example, with autoregressive pretraining, a base-size Mamba attains 83.2\%ImageNet accuracy, outperforming its supervised counterpart by 2.0\%; ourhuge-size Mamba, the largest Vision Mamba to date, attains 85.0\% ImageNetaccuracy (85.5\% when finetuned with $384\times384$ inputs), notably surpassingall other Mamba variants in vision. The code is available at\url{https://github.com/OliverRensu/ARM}.</description><author>Sucheng Ren, Xianhang Li, Haoqin Tu, Feng Wang, Fangxun Shu, Lei Zhang, Jieru Mei, Linjie Yang, Peng Wang, Heng Wang, Alan Yuille, Cihang Xie</author><pubDate>Tue, 11 Jun 2024 18:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07537v1</guid></item><item><title>Methods for Recovering Conditional Independence Graphs: A Survey</title><link>http://arxiv.org/abs/2211.06829v2</link><description>Conditional Independence (CI) graphs are a type of probabilistic graphicalmodels that are primarily used to gain insights about feature relationships.Each edge represents the partial correlation between the connected featureswhich gives information about their direct dependence. In this survey, we listout different methods and study the advances in techniques developed to recoverCI graphs. We cover traditional optimization methods as well as recentlydeveloped deep learning architectures along with their recommendedimplementations. To facilitate wider adoption, we include preliminaries thatconsolidate associated operations, for example techniques to obtain covariancematrix for mixed datatypes.</description><author>Harsh Shrivastava, Urszula Chajewska</author><pubDate>Tue, 11 Jun 2024 18:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06829v2</guid></item><item><title>Towards Fundamentally Scalable Model Selection: Asymptotically Fast Update and Selection</title><link>http://arxiv.org/abs/2406.07536v1</link><description>The advancement of deep learning technologies is bringing new models everyday, motivating the study of scalable model selection. An ideal model selectionscheme should minimally support two operations efficiently over a large pool ofcandidate models: update, which involves either adding a new candidate model orremoving an existing candidate model, and selection, which involves locatinghighly performing models for a given task. However, previous solutions to modelselection require high computational complexity for at least one of these twooperations. In this work, we target fundamentally (more) scalable modelselection that supports asymptotically fast update and asymptotically fastselection at the same time. Firstly, we define isolated model embedding, afamily of model selection schemes supporting asymptotically fast update andselection: With respect to the number of candidate models $m$, the updatecomplexity is O(1) and the selection consists of a single sweep over $m$vectors in addition to O(1) model operations. Isolated model embedding alsoimplies several desirable properties for applications. Secondly, we presentStandardized Embedder, an empirical realization of isolated model embedding. Weassess its effectiveness by using it to select representations from a pool of100 pre-trained vision models for classification tasks and measuring theperformance gaps between the selected models and the best candidates with alinear probing protocol. Experiments suggest our realization is effective inselecting models with competitive performances and highlight isolated modelembedding as a promising direction towards model selection that isfundamentally (more) scalable.</description><author>Wenxiao Wang, Weiming Zhuang, Lingjuan Lyu</author><pubDate>Tue, 11 Jun 2024 18:57:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07536v1</guid></item><item><title>diff History for Neural Language Agents</title><link>http://arxiv.org/abs/2312.07540v3</link><description>Neural Language Models (LMs) offer an exciting solution for general-purposeembodied control. However, a key technical issue arises when using an LM-basedcontroller: environment observations must be converted to text, which coupledwith history, results in long and verbose textual prompts. As a result, priorwork in LM agents is limited to restricted domains with small observation sizeas well as minimal needs for interaction history or instruction tuning. In thispaper, we introduce diff history, a simple and highly effective solution tothese issues. By applying the Unix diff command on consecutive textobservations in the interaction histories used to prompt LM policies, we canboth abstract away redundant information and focus the content of textualinputs on the salient changes in the environment. On NetHack, an unsolved videogame that requires long-horizon reasoning for decision-making, LMs tuned withdiff history match state-of-the-art performance for neural agents while needing1800x fewer training examples compared to prior work. Even on the simplerBabyAI-Text environment with concise text observations, we find that althoughdiff history increases the length of prompts, the representation it providesoffers a 25% improvement in the efficiency of low-sample instruction tuning.Further, we show that diff history scales favorably across different tuningdataset sizes. We open-source our code and data tohttps://diffhistory.github.io.</description><author>Ulyana Piterbarg, Lerrel Pinto, Rob Fergus</author><pubDate>Tue, 11 Jun 2024 18:57:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07540v3</guid></item><item><title>Hearing Anything Anywhere</title><link>http://arxiv.org/abs/2406.07532v1</link><description>Recent years have seen immense progress in 3D computer vision and computergraphics, with emerging tools that can virtualize real-world 3D environmentsfor numerous Mixed Reality (XR) applications. However, alongside immersivevisual experiences, immersive auditory experiences are equally vital to ourholistic perception of an environment. In this paper, we aim to reconstruct thespatial acoustic characteristics of an arbitrary environment given only asparse set of (roughly 12) room impulse response (RIR) recordings and a planarreconstruction of the scene, a setup that is easily achievable by ordinaryusers. To this end, we introduce DiffRIR, a differentiable RIR renderingframework with interpretable parametric models of salient acoustic features ofthe scene, including sound source directivity and surface reflectivity. Thisallows us to synthesize novel auditory experiences through the space with anysource audio. To evaluate our method, we collect a dataset of RIR recordingsand music in four diverse, real environments. We show that our modeloutperforms state-ofthe-art baselines on rendering monaural and binaural RIRsand music at unseen locations, and learns physically interpretable parameterscharacterizing acoustic properties of the sound source and surfaces in thescene.</description><author>Mason Wang, Ryosuke Sawata, Samuel Clarke, Ruohan Gao, Shangzhe Wu, Jiajun Wu</author><pubDate>Tue, 11 Jun 2024 18:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07532v1</guid></item><item><title>MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation</title><link>http://arxiv.org/abs/2406.07529v1</link><description>Model merging has emerged as an effective approach to combine multiplesingle-task models, fine-tuned from the same pre-trained model, into amultitask model. This process typically involves computing a weighted averageof the model parameters without any additional training. Existing model-mergingmethods focus on enhancing average task accuracy. However, interference andconflicts between the objectives of different tasks can lead to trade-offsduring model merging. In real-world applications, a set of solutions withvarious trade-offs can be more informative, helping practitioners makedecisions based on diverse preferences. In this paper, we introduce a novellow-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAPidentifies a Pareto set of scaling coefficients for merging multiple models toreflect the trade-offs. The core component of MAP is approximating theevaluation metrics of the various tasks using a quadratic approximationsurrogate model derived from a pre-selected set of scaling coefficients,enabling amortized inference. Experimental results on vision and naturallanguage processing tasks show that MAP can accurately identify the Paretofront. To further reduce the required computation of MAP, we propose (1) aBayesian adaptive sampling algorithm and (2) a nested merging scheme withmultiple stages.</description><author>Lu Li, Tianyu Zhang, Zhiqi Bu, Suyuchen Wang, Huan He, Jie Fu, Yonghui Wu, Jiang Bian, Yong Chen, Yoshua Bengio</author><pubDate>Tue, 11 Jun 2024 18:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07529v1</guid></item><item><title>QuickLLaMA: Query-aware Inference Acceleration for Large Language Models</title><link>http://arxiv.org/abs/2406.07528v1</link><description>The capacity of Large Language Models (LLMs) to comprehend and reason overlong contexts is pivotal for advancements in diverse fields. Yet, they stillstuggle with capturing long-distance dependencies within sequences to deeplyunderstand semantics. To address this issue, we introduce Query-aware Inferencefor LLMs (Q-LLM), a system designed to process extensive sequences akin tohuman cognition. By focusing on memory data relevant to a given query, Q-LLMcan accurately capture pertinent information within a fixed window size andprovide precise answers to queries. It doesn't require extra training and canbe seamlessly integrated with any LLMs. Q-LLM using LLaMA3 (QuickLLaMA) canread Harry Potter within 30s and accurately answer the questions. Q-LLMimproved by 7.17% compared to the current state-of-the-art on LLaMA3, and by3.26% on Mistral on the $\infty$-bench. In the Needle-in-a-Haystack task, Onwidely recognized benchmarks, Q-LLM improved upon the current SOTA by 7.0% onMistral and achieves 100% on LLaMA3. Our code can be found inhttps://github.com/dvlab-research/Q-LLM.</description><author>Jingyao Li, Han Shi, Xin Jiang, Zhenguo Li, Hong Xu, Jiaya Jia</author><pubDate>Tue, 11 Jun 2024 18:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07528v1</guid></item><item><title>On the Recoverability of Causal Relations from Temporally Aggregated I.I.D. Data</title><link>http://arxiv.org/abs/2406.02191v2</link><description>We consider the effect of temporal aggregation on instantaneous(non-temporal) causal discovery in general setting. This is motivated by theobservation that the true causal time lag is often considerably shorter thanthe observational interval. This discrepancy leads to high aggregation, causingtime-delay causality to vanish and instantaneous dependence to manifest.Although we expect such instantaneous dependence has consistency with the truecausal relation in certain sense to make the discovery results meaningful, itremains unclear what type of consistency we need and when will such consistencybe satisfied. We proposed functional consistency and conditional independenceconsistency in formal way correspond functional causal model-based methods andconditional independence-based methods respectively and provide the conditionsunder which these consistencies will hold. We show theoretically andexperimentally that causal discovery results may be seriously distorted byaggregation especially in complete nonlinear case and we also find causalrelationship still recoverable from aggregated data if we have partiallinearity or appropriate prior. Our findings suggest community should take acautious and meticulous approach when interpreting causal discovery resultsfrom such data and show why and when aggregation will distort the performanceof causal discovery methods.</description><author>Shunxing Fan, Mingming Gong, Kun Zhang</author><pubDate>Tue, 11 Jun 2024 18:53:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02191v2</guid></item><item><title>Simple and Effective Masked Diffusion Language Models</title><link>http://arxiv.org/abs/2406.07524v1</link><description>While diffusion models excel at generating high-quality images, prior workreports a significant performance gap between diffusion and autoregressive (AR)methods in language modeling. In this work, we show that simple masked discretediffusion is more performant than previously thought. We apply an effectivetraining recipe that improves the performance of masked diffusion models andderive a simplified, Rao-Blackwellized objective that results in additionalimprovements. Our objective has a simple form -- it is a mixture of classicalmasked language modeling losses -- and can be used to train encoder-onlylanguage models that admit efficient samplers, including ones that can generatearbitrary lengths of text semi-autoregressively like a traditional languagemodel. On language modeling benchmarks, a range of masked diffusion modelstrained with modern engineering practices achieves a new state-of-the-art amongdiffusion models, and approaches AR perplexity. We release our code at:https://github.com/kuleshov-group/mdlm</description><author>Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T Chiu, Alexander Rush, Volodymyr Kuleshov</author><pubDate>Tue, 11 Jun 2024 18:51:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07524v1</guid></item><item><title>Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</title><link>http://arxiv.org/abs/2406.07522v1</link><description>Efficiently modeling sequences with infinite context length has been along-standing problem. Past works suffer from either the quadratic computationcomplexity or the limited extrapolation ability on length generalization. Inthis work, we present Samba, a simple hybrid architecture that layer-wisecombines Mamba, a selective State Space Model (SSM), with Sliding WindowAttention (SWA). Samba selectively compresses a given sequence into recurrenthidden states while still maintaining the ability to precisely recall memorieswith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2Ttraining tokens and show that Samba substantially outperforms thestate-of-the-art models based on pure attention or SSMs on a wide range ofbenchmarks. When trained on 4K length sequences, Samba can be efficientlyextrapolated to 256K context length with perfect memory recall and showimproved token predictions up to 1M context length. As a linear-time sequencemodel, Samba enjoys a 3.73x higher throughput compared to Transformers withgrouped-query attention when processing user prompts of 128K length, and 3.64xspeedup when generating 64K tokens with unlimited streaming. A sampleimplementation of Samba is publicly available inhttps://github.com/microsoft/Samba.</description><author>Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen</author><pubDate>Tue, 11 Jun 2024 18:50:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07522v1</guid></item><item><title>Faster Spectral Density Estimation and Sparsification in the Nuclear Norm</title><link>http://arxiv.org/abs/2406.07521v1</link><description>We consider the problem of estimating the spectral density of the normalizedadjacency matrix of an $n$-node undirected graph. We provide a randomizedalgorithm that, with $O(n\epsilon^{-2})$ queries to a degree and neighbororacle and in $O(n\epsilon^{-3})$ time, estimates the spectrum up to $\epsilon$accuracy in the Wasserstein-1 metric. This improves on previousstate-of-the-art methods, including an $O(n\epsilon^{-7})$ time algorithm from[Braverman et al., STOC 2022] and, for sufficiently small $\epsilon$, a$2^{O(\epsilon^{-1})}$ time method from [Cohen-Steiner et al., KDD 2018]. Toachieve this result, we introduce a new notion of graph sparsification, whichwe call nuclear sparsification. We provide an $O(n\epsilon^{-2})$-query and$O(n\epsilon^{-2})$-time algorithm for computing $O(n\epsilon^{-2})$-sparsenuclear sparsifiers. We show that this bound is optimal in both its sparsityand query complexity, and we separate our results from the related notion ofadditive spectral sparsification. Of independent interest, we show that oursparsification method also yields the first deterministic algorithm forspectral density estimation that scales linearly with $n$ (sublinear in therepresentation size of the graph).</description><author>Yujia Jin, Ishani Karmarkar, Christopher Musco, Aaron Sidford, Apoorv Vikram Singh</author><pubDate>Tue, 11 Jun 2024 18:50:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07521v1</guid></item><item><title>Neural Gaffer: Relighting Any Object via Diffusion</title><link>http://arxiv.org/abs/2406.07520v1</link><description>Single-image relighting is a challenging task that involves reasoning aboutthe complex interplay between geometry, materials, and lighting. Many priormethods either support only specific categories of images, such as portraits,or require special capture conditions, like using a flashlight. Alternatively,some methods explicitly decompose a scene into intrinsic components, such asnormals and BRDFs, which can be inaccurate or under-expressive. In this work,we propose a novel end-to-end 2D relighting diffusion model, called NeuralGaffer, that takes a single image of any object and can synthesize an accurate,high-quality relit image under any novel environmental lighting condition,simply by conditioning an image generator on a target environment map, withoutan explicit scene decomposition. Our method builds on a pre-trained diffusionmodel, and fine-tunes it on a synthetic relighting dataset, revealing andharnessing the inherent understanding of lighting present in the diffusionmodel. We evaluate our model on both synthetic and in-the-wild Internet imageryand demonstrate its advantages in terms of generalization and accuracy.Moreover, by combining with other generative methods, our model enables manydownstream 2D tasks, such as text-based relighting and object insertion. Ourmodel can also operate as a strong relighting prior for 3D tasks, such asrelighting a radiance field.</description><author>Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, Noah Snavely</author><pubDate>Tue, 11 Jun 2024 18:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07520v1</guid></item><item><title>Physics-guided weak-form discovery of reduced-order models for trapped ultracold hydrodynamics</title><link>http://arxiv.org/abs/2406.07519v1</link><description>We study the relaxation of a highly collisional, ultracold but nondegenerategas of polar molecules. Confined within a harmonic trap, the gas is subject tofluid-gaseous coupled dynamics that lead to a breakdown of first-orderhydrodynamics. An attempt to treat these higher-order hydrodynamic effects waspreviously made with a Gaussian ansatz and coarse-graining model parameter [R.R. W. Wang &amp; J. L. Bohn, Phys. Rev. A 108, 013322 (2023)], leading to anapproximate set of equations for a few collective observables accessible toexperiments. Here we present substantially improved reduced-order models forthese same observables, admissible beyond previous parameter regimes,discovered directly from particle simulations using the WSINDy algorithm(Weak-form Sparse Identification of Nonlinear Dynamics). The interpretablenature of the learning algorithm enables estimation of previously unknownphysical quantities and discovery of model terms with candidate physicalmechanisms, revealing new physics in mixed collisional regimes. Our approachconstitutes a general framework for data-driven model identification leveragingknown physics.</description><author>Reuben R. W. Wang, Daniel Messenger</author><pubDate>Tue, 11 Jun 2024 18:50:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07519v1</guid></item><item><title>Instant 3D Human Avatar Generation using Image Diffusion Models</title><link>http://arxiv.org/abs/2406.07516v1</link><description>We present AvatarPopUp, a method for fast, high quality 3D human avatargeneration from different input modalities, such as images and text prompts andwith control over the generated pose and shape. The common theme is the use ofdiffusion-based image generation networks that are specialized for eachparticular task, followed by a 3D lifting network. We purposefully decouple thegeneration from the 3D modeling which allow us to leverage powerful imagesynthesis priors, trained on billions of text-image pairs. We fine-tune latentdiffusion networks with additional image conditioning to solve tasks such asimage generation and back-view prediction, and to support qualitativelydifferent multiple 3D hypotheses. Our partial fine-tuning approach allows toadapt the networks for each task without inducing catastrophic forgetting. Inour experiments, we demonstrate that our method produces accurate, high-quality3D avatars with diverse appearance that respect the multimodal text, image, andbody control signals. Our approach can produce a 3D model in as few as 2seconds, a four orders of magnitude speedup w.r.t. the vast majority ofexisting methods, most of which solve only a subset of our tasks, and withfewer controls, thus enabling applications that require the controlled 3Dgeneration of human avatars at scale. The project website can be found athttps://www.nikoskolot.com/avatarpopup/.</description><author>Nikos Kolotouros, Thiemo Alldieck, Enric Corona, Eduard Gabriel Bazavan, Cristian Sminchisescu</author><pubDate>Tue, 11 Jun 2024 18:47:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07516v1</guid></item><item><title>RudolfV: A Foundation Model by Pathologists for Pathologists</title><link>http://arxiv.org/abs/2401.04079v4</link><description>Artificial intelligence has started to transform histopathology impactingclinical diagnostics and biomedical research. However, while many computationalpathology approaches have been proposed, most current AI models are limitedwith respect to generalization, application variety, and handling rarediseases. Recent efforts introduced self-supervised foundation models toaddress these challenges, yet existing approaches do not leverage pathologistknowledge by design. In this study, we present a novel approach to designingfoundation models for computational pathology, incorporating pathologistexpertise, semi-automated data curation, and a diverse dataset from over 15laboratories, including 58 tissue types, and encompassing 129 differenthistochemical and immunohistochemical staining modalities. We demonstrate thatour model "RudolfV" surpasses existing state-of-the-art foundation modelsacross different benchmarks focused on tumor microenvironment profiling,biomarker evaluation, and reference case search while exhibiting favorablerobustness properties. Our study shows how domain-specific knowledge canincrease the efficiency and performance of pathology foundation models andenable novel application areas.</description><author>Jonas Dippel, Barbara Feulner, Tobias Winterhoff, Timo Milbich, Stephan Tietz, Simon Schallenberg, Gabriel Dernbach, Andreas Kunft, Simon Heinke, Marie-Lisa Eich, Julika Ribbat-Idel, Rosemarie Krupar, Philipp Anders, Niklas Prenißl, Philipp Jurmeister, David Horst, Lukas Ruff, Klaus-Robert Müller, Frederick Klauschen, Maximilian Alber</author><pubDate>Tue, 11 Jun 2024 18:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04079v4</guid></item><item><title>Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement</title><link>http://arxiv.org/abs/2406.07515v1</link><description>Synthesized data from generative models is increasingly considered as analternative to human-annotated data for fine-tuning Large Language Models. Thisraises concerns about model collapse: a drop in performance of modelsfine-tuned on generated data. Considering that it is easier for both humans andmachines to tell between good and bad examples than to generate high-qualitysamples, we investigate the use of feedback on synthesized data to preventmodel collapse. We derive theoretical conditions under which a Gaussian mixtureclassification model can achieve asymptotically optimal performance whentrained on feedback-augmented synthesized data, and provide supportingsimulations for finite regimes. We illustrate our theoretical predictions ontwo practical problems: computing matrix eigenvalues with transformers and newssummarization with large language models, which both undergo model collapsewhen trained on model-generated data. We show that training fromfeedback-augmented synthesized data, either by pruning incorrect predictions orby selecting the best of several guesses, can prevent model collapse,validating popular approaches like RLHF.</description><author>Yunzhen Feng, Elvis Dohmatob, Pu Yang, Francois Charton, Julia Kempe</author><pubDate>Tue, 11 Jun 2024 18:46:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07515v1</guid></item><item><title>Mercury: A Code Efficiency Benchmark for Code Large Language Models</title><link>http://arxiv.org/abs/2402.07844v4</link><description>Amidst the recent strides in evaluating Large Language Models for Code (CodeLLMs), existing benchmarks have mainly focused on the functional correctness ofgenerated code, neglecting the importance of their computational efficiency. Tofill the gap, we present Mercury, the first code efficiency benchmark for CodeLLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutionsthat serve as real-world efficiency baselines, enabling a comprehensiveanalysis of the runtime distribution. Based on the distribution, we introduce anew metric Beyond, which computes a runtime-percentile-weighted Pass score toreflect functional correctness and code efficiency simultaneously. On Mercury,leading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Giventhat an ideal Beyond score would be aligned with the Pass score, it indicatesthat while Code LLMs exhibit impressive capabilities in generating functionallycorrect code, there remains a notable gap in their efficiency. Finally, ourempirical experiments reveal that Direct Preference Optimization (DPO) servesas a robust baseline for enhancing code efficiency compared with SupervisedFine Tuning (SFT), which paves a promising avenue for future exploration ofefficient code generation. Our code and data are available on GitHub:https://github.com/Elfsong/Mercury.</description><author>Mingzhe Du, Anh Tuan Luu, Bin Ji, Qian Liu, See-Kiong Ng</author><pubDate>Tue, 11 Jun 2024 18:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07844v4</guid></item><item><title>Toward efficient resource utilization at edge nodes in federated learning</title><link>http://arxiv.org/abs/2309.10367v2</link><description>Federated learning (FL) enables edge nodes to collaboratively contribute toconstructing a global model without sharing their data. This is accomplished bydevices computing local, private model updates that are then aggregated by aserver. However, computational resource constraints and network communicationcan become a severe bottleneck for larger model sizes typical for deep learningapplications. Edge nodes tend to have limited hardware resources (RAM, CPU),and the network bandwidth and reliability at the edge is a concern for scalingfederated fleet applications. In this paper, we propose and evaluate a FLstrategy inspired by transfer learning in order to reduce resource utilizationon devices, as well as the load on the server and network in each globaltraining round. For each local model update, we randomly select layers totrain, freezing the remaining part of the model. In doing so, we can reduceboth server load and communication costs per round by excluding all untrainedlayer weights from being transferred to the server. The goal of this study isto empirically explore the potential trade-off between resource utilization ondevices and global model convergence under the proposed strategy. We implementthe approach using the federated learning framework FEDn. A number ofexperiments were carried out over different datasets (CIFAR-10, CASA, andIMDB), performing different tasks using different deep-learning modelarchitectures. Our results show that training the model partially canaccelerate the training process, efficiently utilizes resources on-device, andreduce the data transmission by around 75% and 53% when we train 25%, and 50%of the model layers, respectively, without harming the resulting global modelaccuracy.</description><author>Sadi Alawadi, Addi Ait-Mlouk, Salman Toor, Andreas Hellander</author><pubDate>Tue, 11 Jun 2024 18:44:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10367v2</guid></item><item><title>Flow Map Matching</title><link>http://arxiv.org/abs/2406.07507v1</link><description>Generative models based on dynamical transport of measure, such as diffusionmodels, flow matching models, and stochastic interpolants, learn an ordinary orstochastic differential equation whose trajectories push initial conditionsfrom a known base distribution onto the target. While training is cheap,samples are generated via simulation, which is more expensive than one-stepmodels like GANs. To close this gap, we introduce flow map matching -- analgorithm that learns the two-time flow map of an underlying ordinarydifferential equation. The approach leads to an efficient few-step generativemodel whose step count can be chosen a-posteriori to smoothly trade offaccuracy for computational expense. Leveraging the stochastic interpolantframework, we introduce losses for both direct training of flow maps anddistillation from pre-trained (or otherwise known) velocity fields.Theoretically, we show that our approach unifies many existing few-stepgenerative models, including consistency models, consistency trajectory models,progressive distillation, and neural operator approaches, which can be obtainedas particular cases of our formalism. With experiments on CIFAR-10 and ImageNet32x32, we show that flow map matching leads to high-quality samples withsignificantly reduced sampling cost compared to diffusion or stochasticinterpolant methods.</description><author>Nicholas M. Boffi, Michael S. Albergo, Eric Vanden-Eijnden</author><pubDate>Tue, 11 Jun 2024 18:41:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07507v1</guid></item><item><title>Understanding Visual Concepts Across Models</title><link>http://arxiv.org/abs/2406.07506v1</link><description>Large multimodal models such as Stable Diffusion can generate, detect, andclassify new visual concepts after fine-tuning just a single word embedding. Domodels learn similar words for the same concepts (i.e. &lt;orange-cat&gt; = orange +cat)? We conduct a large-scale analysis on three state-of-the-art models intext-to-image generation, open-set object detection, and zero-shotclassification, and find that new word embeddings are model-specific andnon-transferable. Across 4,800 new embeddings trained for 40 diverse visualconcepts on four standard datasets, we find perturbations within an$\epsilon$-ball to any prior embedding that generate, detect, and classify anarbitrary concept. When these new embeddings are spliced into new models,fine-tuning that targets the original model is lost. We show popular softprompt-tuning approaches find these perturbative solutions when applied tovisual concept learning tasks, and embeddings for visual concepts are nottransferable. Code for reproducing our work is available at:https://visual-words.github.io.</description><author>Brandon Trabucco, Max Gurinas, Kyle Doherty, Ruslan Salakhutdinov</author><pubDate>Tue, 11 Jun 2024 18:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07506v1</guid></item><item><title>THaLLE: Text Hyperlocally Augmented Large Language Extension -- Technical Report</title><link>http://arxiv.org/abs/2406.07505v1</link><description>Recent advancements in Large Language Models (LLMs) have revealed newcapabilities and opportunities across the technological landscape. However, thepracticality of very large LLMs is challenged by their high compute cost, whichdoes not justify the benefits given their limited capability compared tohumans. While smaller, more practical LLMs have shown potential in financialanalysis, though they are not yet fully proficient, as evidenced by theirnear-passing performance on the Chartered Financial Analyst (CFA) exam. In thiswork, we present Financial Analyst Extension to our Text Hyperlocally AugmentedLarge Language Extension (THaLLE), a series of 8B LLMs consistently achievinghighest performance on mock CFA exams against models of comparable size. Wethoroughly document the fine-tuning techniques used to facilitate futureresearch. Additionally, we introduce the use of Flare CFA, a publicly availabledataset for evaluating LLMs as a financial advisor.</description><author>KBTG Labs, Danupat Khamnuansin, Atthakorn Petchsod, Anuruth Lertpiya, Pornchanan Balee, Thanawat Lodkaew, Tawunrat Chalothorn, Thadpong Pongthawornkamol, Monchai Lertsutthiwong</author><pubDate>Tue, 11 Jun 2024 18:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07505v1</guid></item><item><title>Just Because We Camp, Doesn't Mean We Should: The Ethics of Modelling Queer Voices</title><link>http://arxiv.org/abs/2406.07504v1</link><description>Modern voice cloning models claim to be able to capture a diverse range ofvoices. We test the ability of a typical pipeline to capture the style knowncolloquially as "gay voice" and notice a homogenisation effect: synthesisedspeech is rated as sounding significantly "less gay" (by LGBTQ+ participants)than its corresponding ground-truth for speakers with "gay voice", but ratingsactually increase for control speakers. Loss of "gay voice" has implicationsfor accessibility. We also find that for speakers with "gay voice", loss of"gay voice" corresponds to lower similarity ratings. However, we caution that improving the ability of such models to synthesise``gay voice'' comes with a great number of risks. We use this pipeline as astarting point for a discussion on the ethics of modelling queer voices morebroadly. Collecting "clean" queer data has safety and fairness ramifications,and the resulting technology may cause harms from mockery to death.</description><author>Atli Sigurgeirsson, Eddie L. Ungless</author><pubDate>Tue, 11 Jun 2024 18:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07504v1</guid></item><item><title>Image Textualization: An Automatic Framework for Creating Accurate and Detailed Image Descriptions</title><link>http://arxiv.org/abs/2406.07502v1</link><description>Image description datasets play a crucial role in the advancement of variousapplications such as image understanding, text-to-image generation, andtext-image retrieval. Currently, image description datasets primarily originatefrom two sources. One source is the scraping of image-text pairs from the web.Despite their abundance, these descriptions are often of low quality and noisy.Another is through human labeling. Datasets such as COCO are generally veryshort and lack details. Although detailed image descriptions can be annotatedby humans, the high annotation cost limits the feasibility. These limitationsunderscore the need for more efficient and scalable methods to generateaccurate and detailed image descriptions. In this paper, we propose aninnovative framework termed Image Textualization (IT), which automaticallyproduces high-quality image descriptions by leveraging existing multi-modallarge language models (MLLMs) and multiple vision expert models in acollaborative manner, which maximally convert the visual information into text.To address the current lack of benchmarks for detailed descriptions, we proposeseveral benchmarks for comprehensive evaluation, which verifies the quality ofimage descriptions created by our framework. Furthermore, we show thatLLaVA-7B, benefiting from training on IT-curated descriptions, acquire improvedcapability to generate richer image descriptions, substantially increasing thelength and detail of their output with less hallucination.</description><author>Renjie Pi, Jianshu Zhang, Jipeng Zhang, Rui Pan, Zhekai Chen, Tong Zhang</author><pubDate>Tue, 11 Jun 2024 18:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07502v1</guid></item><item><title>SPIN: Spacecraft Imagery for Navigation</title><link>http://arxiv.org/abs/2406.07500v1</link><description>Data acquired in space operational conditions is scarce due to the costs andcomplexity of space operations. This poses a challenge to learning-basedvisual-based navigation algorithms employed in autonomous spacecraftnavigation. Existing datasets, which largely depend on computer-simulated data,have partially filled this gap. However, the image generation tools they useare proprietary, which limits the evaluation of methods to unseen scenarios.Furthermore, these datasets provide limited ground-truth data, primarilyfocusing on the spacecraft's translation and rotation relative to the camera.To address these limitations, we present SPIN (SPacecraft Imagery forNavigation), an open-source realistic spacecraft image generation tool forrelative navigation between two spacecrafts. SPIN provides a wide variety ofground-truth data and allows researchers to employ custom 3D models ofsatellites, define specific camera-relative poses, and adjust various settingssuch as camera parameters and environmental illumination conditions. For thetask of spacecraft pose estimation, we compare the results of training with aSPIN-generated dataset against existing synthetic datasets. We show a %50average error reduction in common testbed data (that simulates realistic spaceconditions). Both the SPIN tool (and source code) and our enhanced version ofthe synthetic datasets will be publicly released upon paper acceptance onGitHub https://github.com/vpulab/SPIN.</description><author>Javier Montalvo, Juan Ignacio Bravo Pérez-Villar, Álvaro García-Martín, Pablo Carballeira, Jesús Besc'os</author><pubDate>Tue, 11 Jun 2024 18:35:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07500v1</guid></item><item><title>Trim 3D Gaussian Splatting for Accurate Geometry Representation</title><link>http://arxiv.org/abs/2406.07499v1</link><description>In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) toreconstruct accurate 3D geometry from images. Previous arts for geometryreconstruction from 3D Gaussians mainly focus on exploring strong geometryregularization. Instead, from a fresh perspective, we propose to obtainaccurate 3D geometry of a scene by Gaussian trimming, which selectively removesthe inaccurate geometry while preserving accurate structures. To achieve this,we analyze the contributions of individual 3D Gaussians and propose acontribution-based trimming strategy to remove the redundant or inaccurateGaussians. Furthermore, our experimental and theoretical analyses reveal that arelatively small Gaussian scale is a non-negligible factor in representing andoptimizing the intricate details. Therefore the proposed TrimGS maintainsrelatively small Gaussian scales. In addition, TrimGS is also compatible withthe effective geometry regularization strategies in previous arts. Whencombined with the original 3DGS and the state-of-the-art 2DGS, TrimGSconsistently yields more accurate geometry and higher perceptual quality. Ourproject page is https://trimgs.github.io</description><author>Lue Fan, Yuxue Yang, Minxing Li, Hongsheng Li, Zhaoxiang Zhang</author><pubDate>Tue, 11 Jun 2024 18:34:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07499v1</guid></item><item><title>Understanding Cross-Lingual Alignment -- A Survey</title><link>http://arxiv.org/abs/2404.06228v2</link><description>Cross-lingual alignment, the meaningful similarity of representations acrosslanguages in multilingual language models, has been an active field of researchin recent years. We survey the literature of techniques to improvecross-lingual alignment, providing a taxonomy of methods and summarisinginsights from throughout the field. We present different understandings ofcross-lingual alignment and their limitations. We provide a qualitative summaryof results from a large number of surveyed papers. Finally, we discuss howthese insights may be applied not only to encoder models, where this topic hasbeen heavily studied, but also to encoder-decoder or even decoder-only models,and argue that an effective trade-off between language-neutral andlanguage-specific information is key.</description><author>Katharina Hämmerl, Jindřich Libovický, Alexander Fraser</author><pubDate>Tue, 11 Jun 2024 18:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06228v2</guid></item><item><title>TextGrad: Automatic "Differentiation" via Text</title><link>http://arxiv.org/abs/2406.07496v1</link><description>AI is undergoing a paradigm shift, with breakthroughs achieved by systemsorchestrating multiple large language models (LLMs) and other complexcomponents. As a result, developing principled and automated optimizationmethods for compound AI systems is one of the most important new challenges.Neural networks faced a similar challenge in its early days untilbackpropagation and automatic differentiation transformed the field by makingoptimization turn-key. Inspired by this, we introduce TextGrad, a powerfulframework performing automatic ``differentiation'' via text. TextGradbackpropagates textual feedback provided by LLMs to improve individualcomponents of a compound AI system. In our framework, LLMs provide rich,general, natural language suggestions to optimize variables in computationgraphs, ranging from code snippets to molecular structures. TextGrad followsPyTorch's syntax and abstraction and is flexible and easy-to-use. It worksout-of-the-box for a variety of tasks, where the users only provide theobjective function without tuning components or prompts of the framework. Weshowcase TextGrad's effectiveness and generality across a diverse range ofapplications, from question answering and molecule optimization to radiotherapytreatment planning. Without modifying the framework, TextGrad improves thezero-shot accuracy of GPT-4o in Google-Proof Question Answering from $51\%$ to$55\%$, yields $20\%$ relative performance gain in optimizing LeetCode-Hardcoding problem solutions, improves prompts for reasoning, designs new druglikesmall molecules with desirable in silico binding, and designs radiationoncology treatment plans with high specificity. TextGrad lays a foundation toaccelerate the development of the next-generation of AI systems.</description><author>Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, James Zou</author><pubDate>Tue, 11 Jun 2024 18:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07496v1</guid></item><item><title>CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization</title><link>http://arxiv.org/abs/2406.07494v1</link><description>Abstractive dialogue summarization is the task of distilling conversationsinto informative and concise summaries. Although reviews have been conducted onthis topic, there is a lack of comprehensive work detailing the challenges ofdialogue summarization, unifying the differing understanding of the task, andaligning proposed techniques, datasets, and evaluation metrics with thechallenges. This article summarizes the research on Transformer-basedabstractive summarization for English dialogues by systematically reviewing1262 unique research papers published between 2019 and 2024, relying on theSemantic Scholar and DBLP databases. We cover the main challenges present indialog summarization (i.e., language, structure, comprehension, speaker,salience, and factuality) and link them to corresponding techniques such asgraph-based approaches, additional training tasks, and planning strategies,which typically overly rely on BART-based encoder-decoder models. We find thatwhile some challenges, like language, have seen considerable progress, mainlydue to training methods, others, such as comprehension, factuality, andsalience, remain difficult and hold significant research opportunities. Weinvestigate how these approaches are typically assessed, covering the datasetsfor the subdomains of dialogue (e.g., meeting, medical), the establishedautomatic metrics and human evaluation approaches for assessing scores andannotator agreement. We observe that only a few datasets span across allsubdomains. The ROUGE metric is the most used, while human evaluation isfrequently reported without sufficient detail on inner-annotator agreement andannotation guidelines. Additionally, we discuss the possible implications ofthe recently explored large language models and conclude that despite apotential shift in relevance and difficulty, our described challenge taxonomyremains relevant.</description><author>Frederic Kirstein, Jan Philip Wahle, Bela Gipp, Terry Ruas</author><pubDate>Tue, 11 Jun 2024 18:30:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07494v1</guid></item><item><title>Paraphrasing in Affirmative Terms Improves Negation Understanding</title><link>http://arxiv.org/abs/2406.07492v1</link><description>Negation is a common linguistic phenomenon. Yet language models facechallenges with negation in many natural language understanding tasks such asquestion answering and natural language inference. In this paper, we experimentwith seamless strategies that incorporate affirmative interpretations (i.e.,paraphrases without negation) to make models more robust against negation.Crucially, our affirmative interpretations are obtained automatically. We showimprovements with CondaQA, a large corpus requiring reasoning with negation,and five natural language understanding tasks.</description><author>MohammadHossein Rezaei, Eduardo Blanco</author><pubDate>Tue, 11 Jun 2024 18:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07492v1</guid></item><item><title>Exploring Meta Information for Audio-based Zero-shot Bird Classification</title><link>http://arxiv.org/abs/2309.08398v2</link><description>Advances in passive acoustic monitoring and machine learning have led to theprocurement of vast datasets for computational bioacoustic research.Nevertheless, data scarcity is still an issue for rare and underrepresentedspecies. This study investigates how meta-information can improve zero-shotaudio classification, utilising bird species as an example case study due tothe availability of rich and diverse meta-data. We investigate three differentsources of metadata: textual bird sound descriptions encoded via (S)BERT,functional traits (AVONET), and bird life-history (BLH) characteristics. Asaudio features, we extract audio spectrogram transformer (AST) embeddings andproject them to the dimension of the auxiliary information by adopting a singlelinear layer. Then, we employ the dot product as compatibility function and astandard zero-shot learning ranking hinge loss to determine the correct class.The best results are achieved by concatenating the AVONET and BLH featuresattaining a mean unweighted F1-score of .233 over five different test sets with8 to 10 classes.</description><author>Alexander Gebhard, Andreas Triantafyllopoulos, Teresa Bez, Lukas Christ, Alexander Kathan, Björn W. Schuller</author><pubDate>Tue, 11 Jun 2024 18:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08398v2</guid></item><item><title>Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks</title><link>http://arxiv.org/abs/2305.01713v3</link><description>Disentangled latent spaces usually have better semantic separability andgeometrical properties, which leads to better interpretability and morecontrollable data generation. While this has been well investigated in ComputerVision, in tasks such as image disentanglement, in the NLP domain sentencedisentanglement is still comparatively under-investigated. Most previous workhave concentrated on disentangling task-specific generative factors, such assentiment, within the context of style transfer. In this work, we focus on amore general form of sentence disentanglement, targeting the localisedmodification and control of more general sentence semantic features. To achievethis, we contribute to a novel notion of sentence semantic disentanglement andintroduce a flow-based invertible neural network (INN) mechanism integratedwith a transformer-based language Autoencoder (AE) in order to deliver latentspaces with better separability properties. Experimental results demonstratethat the model can conform the distributed latent space into a bettersemantically disentangled sentence space, leading to improved languageinterpretability and controlled generation when compared to the recentstate-of-the-art language VAE models.</description><author>Yingji Zhang, Danilo S. Carvalho, André Freitas</author><pubDate>Tue, 11 Jun 2024 18:29:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01713v3</guid></item><item><title>ReduceFormer: Attention with Tensor Reduction by Summation</title><link>http://arxiv.org/abs/2406.07488v1</link><description>Transformers have excelled in many tasks including vision. However, efficientdeployment of transformer models in low-latency or high-throughput applicationsis hindered by the computation in the attention mechanism which involvesexpensive operations such as matrix multiplication and Softmax. To addressthis, we introduce ReduceFormer, a family of models optimized for efficiencywith the spirit of attention. ReduceFormer leverages only simple operationssuch as reduction and element-wise multiplication, leading to greatlysimplified architecture and improved inference performance, with up to 37%reduction in latency and 44% improvement in throughput, while maintainingcompetitive accuracy comparable to other recent methods. The proposed modelfamily is suitable for edge devices where compute resource and memory bandwidthare limited, as well as for cloud computing where high throughput is soughtafter.</description><author>John Yang, Le An, Su Inn Park</author><pubDate>Tue, 11 Jun 2024 18:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07488v1</guid></item><item><title>GLAD: Towards Better Reconstruction with Global and Local Adaptive Diffusion Models for Unsupervised Anomaly Detection</title><link>http://arxiv.org/abs/2406.07487v1</link><description>Diffusion models have shown superior performance on unsupervised anomalydetection tasks. Since trained with normal data only, diffusion models tend toreconstruct normal counterparts of test images with certain noises added.However, these methods treat all potential anomalies equally, which may causetwo main problems. From the global perspective, the difficulty ofreconstructing images with different anomalies is uneven. Therefore, instead ofutilizing the same setting for all samples, we propose to predict a particulardenoising step for each sample by evaluating the difference between imagecontents and the priors extracted from diffusion models. From the localperspective, reconstructing abnormal regions differs from normal areas even inthe same image. Theoretically, the diffusion model predicts a noise for eachstep, typically following a standard Gaussian distribution. However, due to thedifference between the anomaly and its potential normal counterpart, thepredicted noise in abnormal regions will inevitably deviate from the standardGaussian distribution. To this end, we propose introducing synthetic abnormalsamples in training to encourage the diffusion models to break through thelimitation of standard Gaussian distribution, and a spatial-adaptive featurefusion scheme is utilized during inference. With the above modifications, wepropose a global and local adaptive diffusion model (abbreviated to GLAD) forunsupervised anomaly detection, which introduces appealing flexibility andachieves anomaly-free reconstruction while retaining as much normal informationas possible. Extensive experiments are conducted on three commonly used anomalydetection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit boarddataset (PCB-Bank) we integrated, showing the effectiveness of the proposedmethod.</description><author>Hang Yao, Ming Liu, Haolin Wang, Zhicun Yin, Zifei Yan, Xiaopeng Hong, Wangmeng Zuo</author><pubDate>Tue, 11 Jun 2024 18:27:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07487v1</guid></item><item><title>3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos</title><link>http://arxiv.org/abs/2403.01444v4</link><description>Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenesfrom multi-view videos remains a challenging endeavor. Despite the remarkableadvancements achieved by current neural rendering techniques, these methodsgenerally require complete video sequences for offline training and are notcapable of real-time rendering. To address these constraints, we introduce3DGStream, a method designed for efficient FVV streaming of real-world dynamicscenes. Our method achieves fast on-the-fly per-frame reconstruction within 12seconds and real-time rendering at 200 FPS. Specifically, we utilize 3DGaussians (3DGs) to represent the scene. Instead of the na\"ive approach ofdirectly optimizing 3DGs per-frame, we employ a compact Neural TransformationCache (NTC) to model the translations and rotations of 3DGs, markedly reducingthe training time and storage required for each FVV frame. Furthermore, wepropose an adaptive 3DG addition strategy to handle emerging objects in dynamicscenes. Experiments demonstrate that 3DGStream achieves competitive performancein terms of rendering speed, image quality, training time, and model storagewhen compared with state-of-the-art methods.</description><author>Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing</author><pubDate>Tue, 11 Jun 2024 18:26:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01444v4</guid></item><item><title>Towards Generalized Hydrological Forecasting using Transformer Models for 120-Hour Streamflow Prediction</title><link>http://arxiv.org/abs/2406.07484v1</link><description>This study explores the efficacy of a Transformer model for 120-hourstreamflow prediction across 125 diverse locations in Iowa, US. Utilizing datafrom the preceding 72 hours, including precipitation, evapotranspiration, anddischarge values, we developed a generalized model to predict futurestreamflow. Our approach contrasts with traditional methods that typically relyon location-specific models. We benchmarked the Transformer model's performanceagainst three deep learning models (LSTM, GRU, and Seq2Seq) and the Persistenceapproach, employing Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency(KGE), Pearson's r, and Normalized Root Mean Square Error (NRMSE) as metrics.The study reveals the Transformer model's superior performance, maintaininghigher median NSE and KGE scores and exhibiting the lowest NRMSE values. Thisindicates its capability to accurately simulate and predict streamflow,adapting effectively to varying hydrological conditions and geographicalvariances. Our findings underscore the Transformer model's potential as anadvanced tool in hydrological modeling, offering significant improvements overtraditional and contemporary approaches.</description><author>Bekir Z. Demiray, Ibrahim Demir</author><pubDate>Tue, 11 Jun 2024 18:26:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07484v1</guid></item><item><title>Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing</title><link>http://arxiv.org/abs/2406.07483v1</link><description>In the rapidly evolving landscape of Natural Language Processing (NLP), theuse of Large Language Models (LLMs) for automated text annotation in socialmedia posts has garnered significant interest. Despite the impressiveinnovations in developing LLMs like ChatGPT, their efficacy, and accuracy asannotation tools are not well understood. In this paper, we analyze theperformance of eight open-source and proprietary LLMs for annotating the stanceexpressed in social media posts, benchmarking their performance against humanannotators' (i.e., crowd-sourced) judgments. Additionally, we investigate theconditions under which LLMs are likely to disagree with human judgment. Asignificant finding of our study is that the explicitness of text expressing astance plays a critical role in how faithfully LLMs' stance judgments matchhumans'. We argue that LLMs perform well when human annotators do, and whenLLMs fail, it often corresponds to situations in which human annotatorsstruggle to reach an agreement. We conclude with recommendations for acomprehensive approach that combines the precision of human expertise with thescalability of LLM predictions. This study highlights the importance ofimproving the accuracy and comprehensiveness of automated stance detection,aiming to advance these technologies for more efficient and unbiased analysisof social media.</description><author>Mao Li, Frederick Conrad</author><pubDate>Tue, 11 Jun 2024 18:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07483v1</guid></item><item><title>Comparing Deep Learning Models for Rice Mapping in Bhutan Using High Resolution Satellite Imagery</title><link>http://arxiv.org/abs/2406.07482v1</link><description>The Bhutanese government is increasing its utilization of technologicalapproaches such as including Remote Sensing-based knowledge in theirdecision-making process. This study focuses on crop type and crop extent inParo, one of the top rice-yielding districts in Bhutan, and employs publiclyavailable NICFI high-resolution satellite imagery from Planet. Two DeepLearning (DL) approaches, point-based (DNN) and patch-based (U-Net), modelswere used in conjunction with cloud-computing platforms. Three different modelsper DL approaches (DNN and U-Net) were trained: 1) RGBN channels from Planet;2) RGBN and elevation data (RGBNE); 3) RGBN and Sentinel-1 (S1) data (RGBNS),and RGBN with E and S1 data (RGBNES). From this comprehensive analysis, theU-Net displayed higher performance metrics across both model training and modelvalidation efforts. Among the U-Net model sets, the RGBN, RGBNE, RGBNS, andRGBNES models had an F1-score of 0.8546, 0.8563, 0.8467, and 0.8500respectively. An independent model evaluation was performed and found a highlevel of performance variation across all the metrics. For this independentmodel evaluation, the U-Net RGBN, RGBNE, RGBNES, and RGBN models displayed theF1-scores of 0.5935, 0.6154, 0.5882, and 0.6582, suggesting U-Net RGBNES as thebest model. The study shows that the DL approaches can predict rice. Also, DLmethods can be used with the survey-based approaches currently utilized by theBhutan Department of Agriculture. Further, this study demonstrated the usage ofregional land cover products such as SERVIR's RLCMS as a weak label approach tocapture different strata addressing the class imbalance problem and improvingthe sampling design for DL application. Finally, through preliminary modeltesting and comparisons outlined it was shown that using additional featuressuch as NDVI, EVI, and NDWI did not drastically improve model performance.</description><author>Biplov Bhandari, Timothy Mayer</author><pubDate>Tue, 11 Jun 2024 18:25:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07482v1</guid></item><item><title>Image Neural Field Diffusion Models</title><link>http://arxiv.org/abs/2406.07480v1</link><description>Diffusion models have shown an impressive ability to model complex datadistributions, with several key advantages over GANs, such as stable training,better coverage of the training distribution's modes, and the ability to solveinverse problems without extra training. However, most diffusion models learnthe distribution of fixed-resolution images. We propose to learn thedistribution of continuous images by training diffusion models on image neuralfields, which can be rendered at any resolution, and show its advantages overfixed-resolution models. To achieve this, a key challenge is to obtain a latentspace that represents photorealistic image neural fields. We propose a simpleand effective method, inspired by several recent techniques but with keychanges to make the image neural fields photorealistic. Our method can be usedto convert existing latent diffusion autoencoders into image neural fieldautoencoders. We show that image neural field diffusion models can be trainedusing mixed-resolution image datasets, outperform fixed-resolution diffusionmodels followed by super-resolution models, and can solve inverse problems withconditions applied at different scales efficiently.</description><author>Yinbo Chen, Oliver Wang, Richard Zhang, Eli Shechtman, Xiaolong Wang, Michael Gharbi</author><pubDate>Tue, 11 Jun 2024 18:24:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07480v1</guid></item><item><title>Learning from Integral Losses in Physics Informed Neural Networks</title><link>http://arxiv.org/abs/2305.17387v2</link><description>This work proposes a solution for the problem of training physics-informednetworks under partial integro-differential equations. These equations requirean infinite or a large number of neural evaluations to construct a singleresidual for training. As a result, accurate evaluation may be impractical, andwe show that naive approximations at replacing these integrals with unbiasedestimates lead to biased loss functions and solutions. To overcome this bias,we investigate three types of potential solutions: the deterministic samplingapproaches, the double-sampling trick, and the delayed target method. Weconsider three classes of PDEs for benchmarking; one defining Poisson problemswith singular charges and weak solutions of up to 10 dimensions, anotherinvolving weak solutions on electro-magnetic fields and a Maxwell equation, anda third one defining a Smoluchowski coagulation problem. Our numerical resultsconfirm the existence of the aforementioned bias in practice and also show thatour proposed delayed target approach can lead to accurate solutions withcomparable quality to ones estimated with a large sample size integral. Ourimplementation is open-source and available athttps://github.com/ehsansaleh/btspinn.</description><author>Ehsan Saleh, Saba Ghaffari, Timothy Bretl, Luke Olson, Matthew West</author><pubDate>Tue, 11 Jun 2024 18:22:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17387v2</guid></item><item><title>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</title><link>http://arxiv.org/abs/2406.07476v1</link><description>In this paper, we present the VideoLLaMA 2, a set of Video Large LanguageModels (Video-LLMs) designed to enhance spatial-temporal modeling and audiounderstanding in video and audio-oriented tasks. Building upon its predecessor,VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC)connector, which effectively captures the intricate spatial and temporaldynamics of video data. Additionally, we integrate an Audio Branch into themodel through joint training, thereby enriching the multimodal understandingcapabilities of the model by seamlessly incorporating audio cues. Comprehensiveevaluations on multiple-choice video question answering (MC-VQA), open-endedvideo question answering (OE-VQA), and video captioning (VC) tasks demonstratethat VideoLLaMA 2 consistently achieves competitive results among open-sourcemodels and even gets close to some proprietary models on several benchmarks.Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only andaudio-video question-answering (AQA &amp; OE-AVQA) benchmarks over existing models.These advancements underline VideoLLaMA 2's superior performance in multimodalcomprehension, setting a new standard for intelligent video analysis systems.All models are public to facilitate further research.</description><author>Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, Lidong Bing</author><pubDate>Tue, 11 Jun 2024 18:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07476v1</guid></item><item><title>Partially Observed Trajectory Inference using Optimal Transport and a Dynamics Prior</title><link>http://arxiv.org/abs/2406.07475v1</link><description>Trajectory inference seeks to recover the temporal dynamics of a populationfrom snapshots of its (uncoupled) temporal marginals, i.e. where observedparticles are not tracked over time. Lavenant et al. arXiv:2102.09204 addressedthis challenging problem under a stochastic differential equation (SDE) modelwith a gradient-driven drift in the observed space, introducing a minimumentropy estimator relative to the Wiener measure. Chizat et al.arXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL)algorithm using Schr\"odinger bridges. Motivated by the overwhelming success ofobservable state space models in the traditional paired trajectory inferenceproblem (e.g. target tracking), we extend the above framework to a class oflatent SDEs in the form of observable state space models. In this setting, weuse partial observations to infer trajectories in the latent space under aspecified dynamics model (e.g. the constant velocity/acceleration models fromtarget tracking). We introduce PO-MFL to solve this latent trajectory inferenceproblem and provide theoretical guarantees by extending the results ofarXiv:2102.09204 to the partially observed setting. We leverage the MFLframework of arXiv:2205.07146, yielding an algorithm based on entropic OTbetween dynamics-adjusted adjacent time marginals. Experiments validate therobustness of our method and the exponential convergence of the MFL dynamics,and demonstrate significant outperformance over the latent-free method ofarXiv:2205.07146 in key scenarios.</description><author>Anming Gu, Edward Chien, Kristjan Greenewald</author><pubDate>Tue, 11 Jun 2024 18:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07475v1</guid></item><item><title>Quantifying Local Model Validity using Active Learning</title><link>http://arxiv.org/abs/2406.07474v1</link><description>Real-world applications of machine learning models are often subject to legalor policy-based regulations. Some of these regulations require ensuring thevalidity of the model, i.e., the approximation error being smaller than athreshold. A global metric is generally too insensitive to determine thevalidity of a specific prediction, whereas evaluating local validity is costlysince it requires gathering additional data.We propose learning the model errorto acquire a local validity estimate while reducing the amount of required datathrough active learning. Using model validation benchmarks, we provideempirical evidence that the proposed method can lead to an error model withsufficient discriminative properties using a relatively small amount of data.Furthermore, an increased sensitivity to local changes of the validity boundscompared to alternative approaches is demonstrated.</description><author>Sven Lämmle, Can Bogoclu, Robert Voßhall, Anselm Haselhoff, Dirk Roos</author><pubDate>Tue, 11 Jun 2024 18:20:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07474v1</guid></item><item><title>4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models</title><link>http://arxiv.org/abs/2406.07472v1</link><description>Existing dynamic scene generation methods mostly rely on distilling knowledgefrom pre-trained 3D generative models, which are typically fine-tuned onsynthetic object datasets. As a result, the generated scenes are oftenobject-centric and lack photorealism. To address these limitations, weintroduce a novel pipeline designed for photorealistic text-to-4D scenegeneration, discarding the dependency on multi-view generative models andinstead fully utilizing video generative models trained on diverse real-worlddatasets. Our method begins by generating a reference video using the videogeneration model. We then learn the canonical 3D representation of the videousing a freeze-time video, delicately generated from the reference video. Tohandle inconsistencies in the freeze-time video, we jointly learn a per-framedeformation to model these imperfections. We then learn the temporaldeformation based on the canonical representation to capture dynamicinteractions in the reference video. The pipeline facilitates the generation ofdynamic scenes with enhanced photorealism and structural integrity, viewablefrom multiple perspectives, thereby setting a new standard in 4D scenegeneration.</description><author>Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo A Jeni, Sergey Tulyakov, Hsin-Ying Lee</author><pubDate>Tue, 11 Jun 2024 18:19:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07472v1</guid></item><item><title>Robust Inverse Graphics via Probabilistic Inference</title><link>http://arxiv.org/abs/2402.01915v2</link><description>How do we infer a 3D scene from a single image in the presence of corruptionslike rain, snow or fog? Straightforward domain randomization relies on knowingthe family of corruptions ahead of time. Here, we propose a Bayesianapproach-dubbed robust inverse graphics (RIG)-that relies on a strong sceneprior and an uninformative uniform corruption prior, making it applicable to awide range of corruptions. Given a single image, RIG performs posteriorinference jointly over the scene and the corruption. We demonstrate this ideaby training a neural radiance field (NeRF) scene prior and using a secondaryNeRF to represent the corruptions over which we place an uninformative prior.RIG, trained only on clean data, outperforms depth estimators and alternativeNeRF approaches that perform point estimation instead of full inference. Theresults hold for a number of scene prior architectures based on normalizingflows and diffusion models. For the latter, we develop reconstruction-guidancewith auxiliary latents (ReGAL)-a diffusion conditioning algorithm that isapplicable in the presence of auxiliary latent variables such as thecorruption. RIG demonstrates how scene priors can be used beyond generationtasks.</description><author>Tuan Anh Le, Pavel Sountsov, Matthew D. Hoffman, Ben Lee, Brian Patton, Rif A. Saurous</author><pubDate>Tue, 11 Jun 2024 18:19:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01915v2</guid></item><item><title>OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow Understanding</title><link>http://arxiv.org/abs/2406.07471v1</link><description>Surgical scene perception via videos are critical for advancing roboticsurgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology.However, the scarcity of diverse and richly annotated video datasets hashindered the development of intelligent systems for surgical workflow analysis.Existing datasets for surgical workflow analysis, which typically facechallenges such as small scale, a lack of diversity in surgery and phasecategories, and the absence of time-localized annotations, limit therequirements for action understanding and model generalization validation incomplex and diverse real-world surgical scenarios. To address this gap, weintroduce OphNet, a large-scale, expert-annotated video benchmark forophthalmic surgical workflow understanding. OphNet features: 1) A diversecollection of 2,278 surgical videos spanning 66 types of cataract, glaucoma,and corneal surgeries, with detailed annotations for 102 unique surgical phasesand 150 granular operations; 2) It offers sequential and hierarchicalannotations for each surgery, phase, and operation, enabling comprehensiveunderstanding and improved interpretability; 3) Moreover, OphNet providestime-localized annotations, facilitating temporal localization and predictiontasks within surgical workflows. With approximately 205 hours of surgicalvideos, OphNet is about 20 times larger than the largest existing surgicalworkflow analysis benchmark. Our dataset and code have been made available at:\url{https://github.com/minghu0830/OphNet-benchmark}.</description><author>Ming Hu, Peng Xia, Lin Wang, Siyuan Yan, Feilong Tang, Zhongxing Xu, Yimin Luo, Kaimin Song, Jurgen Leitner, Xuelian Cheng, Jun Cheng, Chi Liu, Kaijing Zhou, Zongyuan Ge</author><pubDate>Tue, 11 Jun 2024 18:18:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07471v1</guid></item><item><title>Minimum discrepancy principle strategy for choosing $k$ in $k$-NN regression</title><link>http://arxiv.org/abs/2008.08718v6</link><description>We present a novel data-driven strategy to choose the hyperparameter $k$ inthe $k$-NN regression estimator without using any hold-out data. We treat theproblem of choosing the hyperparameter as an iterative procedure (over $k$) andpropose using an easily implemented in practice strategy based on the idea ofearly stopping and the minimum discrepancy principle. This model selectionstrategy is proven to be minimax-optimal, under the fixed-design assumption oncovariates, over some smoothness function classes, for instance, the Lipschitzfunctions class on a bounded domain. The novel method often improvesstatistical performance on artificial and real-world data sets in comparison toother model selection strategies, such as the Hold-out method, 5-foldcross-validation, and AIC criterion. The novelty of the strategy comes fromreducing the computational time of the model selection procedure whilepreserving the statistical (minimax) optimality of the resulting estimator.More precisely, given a sample of size $n$, if one should choose $k$ among$\left\{ 1, \ldots, n \right\}$, and $\left\{ f^1, \ldots, f^n \right\}$ arethe estimators of the regression function, the minimum discrepancy principlerequires calculation of a fraction of the estimators, while this is not thecase for the generalized cross-validation, Akaike's AIC criteria or Lepskiiprinciple.</description><author>Yaroslav Averyanov, Alain Celisse</author><pubDate>Tue, 11 Jun 2024 18:15:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2008.08718v6</guid></item><item><title>Formal Semantic Geometry over Transformer-based Variational AutoEncoder</title><link>http://arxiv.org/abs/2210.06230v2</link><description>Formal/symbolic semantics can provide canonical, rigid controllability andinterpretability to sentence representations due to their \textit{localisation}or \textit{composition} property. How can we deliver such property to thecurrent distributional sentence representations to control and interpret thegeneration of language models (LMs)? In this work, we theoretically frame thesentence semantics as the composition of \textit{semantic role - word content}features and propose the formal semantic geometry. To inject such geometry intoTransformer-based LMs (i.e. GPT2), we deploy Transformer-based VariationalAutoEncoder with a supervision approach, where the sentence generation can bemanipulated and explained over low-dimensional latent Gaussian space. Inaddition, we propose a new probing algorithm to guide the movement of sentencevectors over such geometry. Experimental results reveal that the formalsemantic geometry can potentially deliver better control and interpretation tosentence generation.</description><author>Yingji Zhang, Danilo S. Carvalho, Ian Pratt-Hartmann, André Freitas</author><pubDate>Tue, 11 Jun 2024 18:15:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06230v2</guid></item><item><title>Multimodal Belief Prediction</title><link>http://arxiv.org/abs/2406.07466v1</link><description>Recognizing a speaker's level of commitment to a belief is a difficult task;humans do not only interpret the meaning of the words in context, but alsounderstand cues from intonation and other aspects of the audio signal. Manypapers and corpora in the NLP community have approached the belief predictiontask using text-only approaches. We are the first to frame and present resultson the multimodal belief prediction task. We use the CB-Prosody corpus (CBP),containing aligned text and audio with speaker belief annotations. We firstreport baselines and significant features using acoustic-prosodic features andtraditional machine learning methods. We then present text and audio baselinesfor the CBP corpus fine-tuning on BERT and Whisper respectively. Finally, wepresent our multimodal architecture which fine-tunes on BERT and Whisper anduses multiple fusion methods, improving on both modalities alone.</description><author>John Murzaku, Adil Soubki, Owen Rambow</author><pubDate>Tue, 11 Jun 2024 18:12:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07466v1</guid></item><item><title>Generated Contents Enrichment</title><link>http://arxiv.org/abs/2405.03650v2</link><description>In this paper, we investigate a novel artificial intelligence generationtask, termed as generated contents enrichment (GCE). Different fromconventional artificial intelligence contents generation task that enriches thegiven textual description implicitly with limited semantics for generatingvisually real content, our proposed GCE strives to perform content enrichmentexplicitly on both the visual and textual domain, from which the enrichedcontents are visually real, structurally reasonable, and semantically abundant.Towards to solve GCE, we propose a deep end-to-end method that explicitlyexplores the semantics and inter-semantic relationships during the enrichment.Specifically, we first model the input description as a semantic graph, whereineach node represents an object and each edge corresponds to the inter-objectrelationship. We then adopt Graph Convolutional Networks on top of the inputscene description to predict the enriching objects and their relationships withthe input objects. Finally, the enriched description is fed into an imagesynthesis model to carry out the visual contents generation. Our experimentsconducted on the Visual Genome dataset exhibit promising and visually plausibleresults.</description><author>Mahdi Naseri, Jiayan Qiu, Zhou Wang</author><pubDate>Tue, 11 Jun 2024 18:12:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03650v2</guid></item><item><title>Data-dependent Generalization Bounds via Variable-Size Compressibility</title><link>http://arxiv.org/abs/2303.05369v3</link><description>In this paper, we establish novel data-dependent upper bounds on thegeneralization error through the lens of a "variable-size compressibility"framework that we introduce newly here. In this framework, the generalizationerror of an algorithm is linked to a variable-size 'compression rate' of itsinput data. This is shown to yield bounds that depend on the empirical measureof the given input data at hand, rather than its unknown distribution. Our newgeneralization bounds that we establish are tail bounds, tail bounds on theexpectation, and in-expectations bounds. Moreover, it is shown that ourframework also allows to derive general bounds on any function of the inputdata and output hypothesis random variables. In particular, these generalbounds are shown to subsume and possibly improve over several existingPAC-Bayes and data-dependent intrinsic dimension-based bounds that arerecovered as special cases, thus unveiling a unifying character of ourapproach. For instance, a new data-dependent intrinsic dimension-based bound isestablished, which connects the generalization error to the optimizationtrajectories and reveals various interesting connections with therate-distortion dimension of a process, the R\'enyi information dimension of aprocess, and the metric mean dimension.</description><author>Milad Sefidgaran, Abdellatif Zaidi</author><pubDate>Tue, 11 Jun 2024 18:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05369v3</guid></item><item><title>Estimating the Hallucination Rate of Generative AI</title><link>http://arxiv.org/abs/2406.07457v1</link><description>This work is about estimating the hallucination rate for in-context learning(ICL) with Generative AI. In ICL, a conditional generative model (CGM) isprompted with a dataset and asked to make a prediction based on that dataset.The Bayesian interpretation of ICL assumes that the CGM is calculating aposterior predictive distribution over an unknown Bayesian model of a latentparameter and data. With this perspective, we define a \textit{hallucination}as a generated prediction that has low-probability under the true latentparameter. We develop a new method that takes an ICL problem -- that is, a CGM,a dataset, and a prediction question -- and estimates the probability that aCGM will generate a hallucination. Our method only requires generating queriesand responses from the model and evaluating its response log probability. Weempirically evaluate our method on synthetic regression and natural languageICL tasks using large language models.</description><author>Andrew Jesson, Nicolas Beltran-Velez, Quentin Chu, Sweta Karlekar, Jannik Kossen, Yarin Gal, John P. Cunningham, David Blei</author><pubDate>Tue, 11 Jun 2024 18:01:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07457v1</guid></item><item><title>fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis functions</title><link>http://arxiv.org/abs/2406.07456v1</link><description>Recent advancements in neural network design have given rise to thedevelopment of Kolmogorov-Arnold Networks (KANs), which enhance speed,interpretability, and precision. This paper presents the FractionalKolmogorov-Arnold Network (fKAN), a novel neural network architecture thatincorporates the distinctive attributes of KANs with a trainable adaptivefractional-orthogonal Jacobi function as its basis function. By leveraging theunique mathematical properties of fractional Jacobi functions, including simplederivative formulas, non-polynomial behavior, and activity for both positiveand negative input values, this approach ensures efficient learning andenhanced accuracy. The proposed architecture is evaluated across a range oftasks in deep learning and physics-informed deep learning. Precision is testedon synthetic regression data, image classification, image denoising, andsentiment analysis. Additionally, the performance is measured on variousdifferential equations, including ordinary, partial, and fractional delaydifferential equations. The results demonstrate that integrating fractionalJacobi functions into KANs significantly improves training speed andperformance across diverse fields and applications.</description><author>Alireza Afzal Aghaei</author><pubDate>Tue, 11 Jun 2024 18:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07456v1</guid></item><item><title>Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis</title><link>http://arxiv.org/abs/2406.07455v1</link><description>In this paper, we study reinforcement learning from human feedback (RLHF)under an episodic Markov decision process with a general trajectory-wise rewardmodel. We developed a model-free RLHF best policy identification algorithm,called $\mathsf{BSAD}$, without explicit reward model inference, which is acritical intermediate step in the contemporary RLHF paradigms for traininglarge language models (LLM). The algorithm identifies the optimal policydirectly from human preference information in a backward manner, employing adueling bandit sub-routine that constantly duels actions to identify thesuperior one. $\mathsf{BSAD}$ adopts a reward-free exploration andbest-arm-identification-like adaptive stopping criteria to equalize thevisitation among all states in the same decision step while moving to theprevious step as soon as the optimal action is identifiable, leading to aprovable, instance-dependent sample complexity$\tilde{\mathcal{O}}(c_{\mathcal{M}}SA^3H^3M\log\frac{1}{\delta})$ whichresembles the result in classic RL, where $c_{\mathcal{M}}$ is theinstance-dependent constant and $M$ is the batch size. Moreover,$\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm withlogarithmic regret and generalized to discounted MDPs using a frame-basedapproach. Our results show: (i) sample-complexity-wise, RLHF is notsignificantly harder than classic RL and (ii) end-to-end RLHF may deliverimproved performance by avoiding pitfalls in reward inferring such as overfitand distribution shift.</description><author>Qining Zhang, Honghao Wei, Lei Ying</author><pubDate>Tue, 11 Jun 2024 18:01:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07455v1</guid></item><item><title>Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision</title><link>http://arxiv.org/abs/2402.08960v2</link><description>Current state-of-the-art open-vocabulary segmentation methods typically relyon image-mask-text triplet annotations for supervision. However, acquiring suchdetailed annotations is labour-intensive and poses scalability challenges incomplex real-world scenarios. While existing weakly-supervised approachesleverage image-text pairs to reduce the expansive annotation cost, the lack ofmask supervision makes it difficult for the model to locate multiple instancesand accurately group pixels with similar semantics, significantly hamperingversatility and performance. In this paper, we introduce Unpair-Seg, a novelweakly-supervised open-vocabulary segmentation framework that learns fromunpaired image-mask and image-text pairs, which can be independently andefficiently collected. Unpair-Seg initially predicts a set of binary masks andgenerates pseudo labels by identifying confident pairs of masks and textentities. We then train a feature adapter to align region embeddings with textembeddings based on these pseudo labels, achieving open-vocabularysegmentation. However, the inherent noise in the mask-entity correspondenceposes a challenge to obtaining reliable pairs. To address this, we employ avision-language large model to re-caption the input images and extract preciseentities, and we design a multi-scale matching strategy to reduce noisymask-entity pairs. Our Unpair-Seg framework demonstrates impressiveperformance, achieving 14.6\% and 19.5\% mIoU on the ADE-847 and PASCALContext-459 datasets, significantly narrowing the gap between fully-supervisedand weakly-supervised methods.</description><author>Zhaoqing Wang, Xiaobo Xia, Ziye Chen, Xiao He, Yandong Guo, Mingming Gong, Tongliang Liu</author><pubDate>Tue, 11 Jun 2024 18:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08960v2</guid></item><item><title>An Optimism-based Approach to Online Evaluation of Generative Models</title><link>http://arxiv.org/abs/2406.07451v1</link><description>Existing frameworks for evaluating and comparing generative models typicallytarget an offline setting, where the evaluator has access to full batches ofdata produced by the models. However, in many practical scenarios, the goal isto identify the best model using the fewest generated samples to minimize thecosts of querying data from the models. Such an online comparison ischallenging with current offline assessment methods. In this work, we proposean online evaluation framework to find the generative model that maximizes astandard assessment score among a group of available models. Our method uses anoptimism-based multi-armed bandit framework to identify the model producingdata with the highest evaluation score, quantifying the quality and diversityof generated data. Specifically, we study the online assessment of generativemodels based on the Fr\'echet Inception Distance (FID) and Inception Score (IS)metrics and propose the FID-UCB and IS-UCB algorithms leveraging the upperconfidence bound approach in online learning. We prove sub-linear regret boundsfor these algorithms and present numerical results on standard image datasets,demonstrating their effectiveness in identifying the score-maximizinggenerative model.</description><author>Xiaoyan Hu, Ho-fung Leung, Farzan Farnia</author><pubDate>Tue, 11 Jun 2024 17:57:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07451v1</guid></item><item><title>Benchmarking Vision-Language Contrastive Methods for Medical Representation Learning</title><link>http://arxiv.org/abs/2406.07450v1</link><description>We perform a comprehensive benchmarking of contrastive frameworks forlearning multimodal representations in the medical domain. Through this study,we aim to answer the following research questions: (i) How transferable aregeneral-domain representations to the medical domain? (ii) Is multimodalcontrastive training sufficient, or does it benefit from unimodal training aswell? (iii) What is the impact of feature granularity on the effectiveness ofmultimodal medical representation learning? To answer these questions, weinvestigate eight contrastive learning approaches under identical trainingsetups, and train them on 2.8 million image-text pairs from four datasets, andevaluate them on 25 downstream tasks, including classification (zero-shot andlinear probing), image-to-text and text-to-image retrieval, and visualquestion-answering. Our findings suggest a positive answer to the firstquestion, a negative answer to the second question, and the benefit of learningfine-grained features. Finally, we make our code publicly available.</description><author>Shuvendu Roy, Yasaman Parhizkar, Franklin Ogidi, Vahid Reza Khazaie, Michael Colacci, Ali Etemad, Elham Dolatabadi, Arash Afkanpour</author><pubDate>Tue, 11 Jun 2024 17:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07450v1</guid></item><item><title>Boosted Conformal Prediction Intervals</title><link>http://arxiv.org/abs/2406.07449v1</link><description>This paper introduces a boosted conformal procedure designed to tailorconformalized prediction intervals toward specific desired properties, such asenhanced conditional coverage or reduced interval length. We employ machinelearning techniques, notably gradient boosting, to systematically improve upona predefined conformity score function. This process is guided by carefullyconstructed loss functions that measure the deviation of prediction intervalsfrom the targeted properties. The procedure operates post-training, relyingsolely on model predictions and without modifying the trained model (e.g., thedeep network). Systematic experiments demonstrate that starting fromconventional conformal methods, our boosted procedure achieves substantialimprovements in reducing interval length and decreasing deviation from targetconditional coverage.</description><author>Ran Xie, Rina Foygel Barber, Emmanuel J. Candès</author><pubDate>Tue, 11 Jun 2024 17:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07449v1</guid></item><item><title>On the Robustness of Document-Level Relation Extraction Models to Entity Name Variations</title><link>http://arxiv.org/abs/2406.07444v1</link><description>Driven by the demand for cross-sentence and large-scale relation extraction,document-level relation extraction (DocRE) has attracted increasing researchinterest. Despite the continuous improvement in performance, we find thatexisting DocRE models which initially perform well may make more mistakes whenmerely changing the entity names in the document, hindering the generalizationto novel entity names. To this end, we systematically investigate therobustness of DocRE models to entity name variations in this work. We firstpropose a principled pipeline to generate entity-renamed documents by replacingthe original entity names with names from Wikidata. By applying the pipeline toDocRED and Re-DocRED datasets, we construct two novel benchmarks namedEnv-DocRED and Env-Re-DocRED for robustness evaluation. Experimental resultsshow that both three representative DocRE models and two in-context learnedlarge language models consistently lack sufficient robustness to entity namevariations, particularly on cross-sentence relation instances and documentswith more entities. Finally, we propose an entity variation robust trainingmethod which not only improves the robustness of DocRE models but also enhancestheir understanding and reasoning capabilities. We further verify that thebasic idea of this method can be effectively transferred to in-context learningfor DocRE as well.</description><author>Shiao Meng, Xuming Hu, Aiwei Liu, Fukun Ma, Yawen Yang, Shuang Li, Lijie Wen</author><pubDate>Tue, 11 Jun 2024 17:51:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07444v1</guid></item><item><title>Textual Similarity as a Key Metric in Machine Translation Quality Estimation</title><link>http://arxiv.org/abs/2406.07440v1</link><description>Machine Translation (MT) Quality Estimation (QE) assesses translationreliability without reference texts. This study introduces "textual similarity"as a new metric for QE, using sentence transformers and cosine similarity tomeasure semantic closeness. Analyzing data from the MLQE-PE dataset, we foundthat textual similarity exhibits stronger correlations with human scores thantraditional metrics (hter, model evaluation etc.). Employing GAMMs as astatistical tool, we demonstrated that textual similarity consistentlyoutperforms other metrics across multiple language pairs in predicting humanscores. We also found that "hter" actually failed to predict human scores inQE. Our findings highlight the effectiveness of textual similarity as a robustQE metric, recommending its integration with other metrics into QE frameworksand MT system training for improved accuracy and usability.</description><author>Kun Sun, Rong Wang</author><pubDate>Tue, 11 Jun 2024 17:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07440v1</guid></item><item><title>DeformTime: Capturing Variable Dependencies with Deformable Attention for Time Series Forecasting</title><link>http://arxiv.org/abs/2406.07438v1</link><description>In multivariate time series (MTS) forecasting, existing state-of-the-art deeplearning approaches tend to focus on autoregressive formulations and overlookthe information within exogenous indicators. To address this limitation, wepresent DeformTime, a neural network architecture that attempts to capturecorrelated temporal patterns from the input space, and hence, improveforecasting accuracy. It deploys two core operations performed by deformableattention blocks (DABs): learning dependencies across variables from differenttime steps (variable DAB), and preserving temporal dependencies in data fromprevious time steps (temporal DAB). Input data transformation is explicitlydesigned to enhance learning from the deformed series of information whilepassing through a DAB. We conduct extensive experiments on 6 MTS data sets,using previously established benchmarks as well as challenging infectiousdisease modelling tasks with more exogenous variables. The results demonstratethat DeformTime improves accuracy against previous competitive methods acrossthe vast majority of MTS forecasting tasks, reducing the mean absolute error by10% on average. Notably, performance gains remain consistent across longerforecasting horizons.</description><author>Yuxuan Shu, Vasileios Lampos</author><pubDate>Tue, 11 Jun 2024 17:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07438v1</guid></item><item><title>Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of SSWP</title><link>http://arxiv.org/abs/2309.05423v2</link><description>In expressive and controllable Text-to-Speech (TTS), explicit prosodicfeatures significantly improve the naturalness and controllability ofsynthesised speech. However, manual prosody annotation is labor-intensive andinconsistent. To address this issue, a two-stage automatic annotation pipelineis novelly proposed in this paper. In the first stage, we use contrastivepretraining of Speech-Silence and Word-Punctuation (SSWP) pairs to enhanceprosodic information in latent representations. In the second stage, we build amulti-modal prosody annotator, comprising pretrained encoders, a text-speechfusing scheme, and a sequence classifier. Experiments on English prosodicboundaries demonstrate that our method achieves state-of-the-art (SOTA)performance with 0.72 and 0.93 f1 score for Prosodic Word and Prosodic Phraseboundary respectively, while bearing remarkable robustness to data scarcity.</description><author>Jinzuomu Zhong, Yang Li, Hui Huang, Korin Richmond, Jie Liu, Zhiba Su, Jing Guo, Benlai Tang, Fengjie Zhu</author><pubDate>Tue, 11 Jun 2024 17:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05423v2</guid></item><item><title>Beware of Aliases -- Signal Preservation is Crucial for Robust Image Restoration</title><link>http://arxiv.org/abs/2406.07435v1</link><description>Image restoration networks are usually comprised of an encoder and a decoder,responsible for aggregating image content from noisy, distorted data and torestore clean, undistorted images, respectively. Data aggregation as well ashigh-resolution image generation both usually come at the risk of involvingaliases, i.e.~standard architectures put their ability to reconstruct the modelinput in jeopardy to reach high PSNR values on validation data. The price to bepaid is low model robustness. In this work, we show that simply providingalias-free paths in state-of-the-art reconstruction transformers supportsimproved model robustness at low costs on the restoration performance. We do soby proposing BOA-Restormer, a transformer-based image restoration model thatexecutes downsampling and upsampling operations partly in the frequency domainto ensure alias-free paths along the entire model while potentially preservingall relevant high-frequency information.</description><author>Shashank Agnihotri, Julia Grabinski, Janis Keuper, Margret Keuper</author><pubDate>Tue, 11 Jun 2024 17:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07435v1</guid></item><item><title>Improving Logits-based Detector without Logits from Black-box LLMs</title><link>http://arxiv.org/abs/2406.05232v2</link><description>The advent of Large Language Models (LLMs) has revolutionized textgeneration, producing outputs that closely mimic human writing. This blurringof lines between machine- and human-written text presents new challenges indistinguishing one from the other a task further complicated by the frequentupdates and closed nature of leading proprietary LLMs. Traditional logits-baseddetection methods leverage surrogate models for identifying LLM-generatedcontent when the exact logits are unavailable from black-box LLMs. However,these methods grapple with the misalignment between the distributions of thesurrogate and the often undisclosed target models, leading to performancedegradation, particularly with the introduction of new, closed-source models.Furthermore, while current methodologies are generally effective when thesource model is identified, they falter in scenarios where the model versionremains unknown, or the test set comprises outputs from various source models.To address these limitations, we present Distribution-Aligned LLMs Detection(DALD), an innovative framework that redefines the state-of-the-art performancein black-box text detection even without logits from source LLMs. DALD isdesigned to align the surrogate model's distribution with that of unknowntarget LLMs, ensuring enhanced detection capability and resilience againstrapid model iterations with minimal training investment. By leveraging corpussamples from publicly accessible outputs of advanced models such as ChatGPT,GPT-4 and Claude-3, DALD fine-tunes surrogate models to synchronize withunknown source model distributions effectively.</description><author>Cong Zeng, Shengkun Tang, Xianjun Yang, Yuanzhou Chen, Yiyou Sun, zhiqiang xu, Yao Li, Haifeng Chen, Wei Cheng, Dongkuan Xu</author><pubDate>Tue, 11 Jun 2024 17:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05232v2</guid></item><item><title>Label Alignment Regularization for Distribution Shift</title><link>http://arxiv.org/abs/2211.14960v4</link><description>Recent work has highlighted the label alignment property (LAP) in supervisedlearning, where the vector of all labels in the dataset is mostly in the spanof the top few singular vectors of the data matrix. Drawing inspiration fromthis observation, we propose a regularization method for unsupervised domainadaptation that encourages alignment between the predictions in the targetdomain and its top singular vectors. Unlike conventional domain adaptationapproaches that focus on regularizing representations, we instead regularizethe classifier to align with the unsupervised target data, guided by the LAP inboth the source and target domains. Theoretical analysis demonstrates that,under certain assumptions, our solution resides within the span of the topright singular vectors of the target domain data and aligns with the optimalsolution. By removing the reliance on the commonly used optimal joint riskassumption found in classic domain adaptation theory, we showcase theeffectiveness of our method on addressing problems where traditional domainadaptation methods often fall short due to high joint error. Additionally, wereport improved performance over domain adaptation baselines in well-knowntasks such as MNIST-USPS domain adaptation and cross-lingual sentimentanalysis.</description><author>Ehsan Imani, Guojun Zhang, Runjia Li, Jun Luo, Pascal Poupart, Philip H. S. Torr, Yangchen Pan</author><pubDate>Tue, 11 Jun 2024 17:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14960v4</guid></item><item><title>Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes</title><link>http://arxiv.org/abs/2402.18477v2</link><description>Inferring the causal structure underlying stochastic dynamical systems fromobservational data holds great promise in domains ranging from science andhealth to finance. Such processes can often be accurately modeled viastochastic differential equations (SDEs), which naturally imply causalrelationships via "which variables enter the differential of which othervariables". In this paper, we develop a kernel-based test of conditionalindependence (CI) on "path-space" -- e.g., solutions to SDEs, but applicablebeyond that -- by leveraging recent advances in signature kernels. Wedemonstrate strictly superior performance of our proposed CI test compared toexisting approaches on path-space and provide theoretical consistency results.Then, we develop constraint-based causal discovery algorithms for acyclicstochastic dynamical systems (allowing for self-loops) that leverage temporalinformation to recover the entire directed acyclic graph. Assuming faithfulnessand a CI oracle, we show that our algorithms are sound and complete. Weempirically verify that our developed CI test in conjunction with the causaldiscovery algorithms outperform baselines across a range of settings.</description><author>Georg Manten, Cecilia Casolo, Emilio Ferrucci, Søren Wengel Mogensen, Cristopher Salvi, Niki Kilbertus</author><pubDate>Tue, 11 Jun 2024 17:37:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18477v2</guid></item><item><title>Active Scout: Multi-Target Tracking Using Neural Radiance Fields in Dense Urban Environments</title><link>http://arxiv.org/abs/2406.07431v1</link><description>We study pursuit-evasion games in highly occluded urban environments, e.g.tall buildings in a city, where a scout (quadrotor) tracks multiple dynamictargets on the ground. We show that we can build a neural radiance field (NeRF)representation of the city -- online -- using RGB and depth images fromdifferent vantage points. This representation is used to calculate theinformation gain to both explore unknown parts of the city and track thetargets -- thereby giving a completely first-principles approach to activelytracking dynamic targets. We demonstrate, using a custom-built simulator usingOpen Street Maps data of Philadelphia and New York City, that we can exploreand locate 20 stationary targets within 300 steps. This is slower than a greedybaseline which which does not use active perception. But for dynamic targetsthat actively hide behind occlusions, we show that our approach maintains, atworst, a tracking error of 200m; the greedy baseline can have a tracking erroras large as 600m. We observe a number of interesting properties in the scout'spolicies, e.g., it switches its attention to track a different targetperiodically, as the quality of the NeRF representation improves over time, thescout also becomes better in terms of target tracking.</description><author>Christopher D. Hsu, Pratik Chaudhari</author><pubDate>Tue, 11 Jun 2024 17:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07431v1</guid></item><item><title>Learning Domain-Invariant Features for Out-of-Context News Detection</title><link>http://arxiv.org/abs/2406.07430v1</link><description>Multimodal out-of-context news is a common type of misinformation on onlinemedia platforms. This involves posting a caption, alongside an invalidout-of-context news image. Reflecting its importance, researchers havedeveloped models to detect such misinformation. However, a common limitation ofthese models is that they only consider the scenario where pre-labeled data isavailable for each domain, failing to address the out-of-context news detectionon unlabeled domains (e.g., unverified news on new topics or agencies). In thiswork, we therefore focus on domain adaptive out-of-context news detection. Inorder to effectively adapt the detection model to unlabeled news topics oragencies, we propose ConDA-TTA (Contrastive Domain Adaptation with Test-TimeAdaptation) which applies contrastive learning and maximum mean discrepancy(MMD) to learn the domain-invariant feature. In addition, it leverages targetdomain statistics during test-time to further assist domain adaptation.Experimental results show that our approach outperforms baselines in 5 out of 7domain adaptation settings on two public datasets, by as much as 2.93% in F1and 2.08% in accuracy.</description><author>Yimeng Gu, Mengqi Zhang, Ignacio Castro, Shu Wu, Gareth Tyson</author><pubDate>Tue, 11 Jun 2024 17:34:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07430v1</guid></item><item><title>GemNet: Menu-Based, Strategy-Proof Multi-Bidder Auctions Through Deep Learning</title><link>http://arxiv.org/abs/2406.07428v1</link><description>Differentiable economics uses deep learning for automated mechanism design.Despite strong progress, it has remained an open problem to learn multi-bidder,general, and fully strategy-proof (SP) auctions. We introduce GEneralMenu-based NETwork (GemNet), which significantly extends the menu-basedapproach of RochetNet [D\"utting et al., 2023] to the multi-bidder setting. Thechallenge in achieving SP is to learn bidder-independent menus that arefeasible, so that the optimal menu choices for each bidder do not over-allocateitems when taken together (we call this menu compatibility). GemNet penalizesthe failure of menu compatibility during training, and transforms learned menusafter training through price changes, by considering a set of discretizedbidder values and reasoning about Lipschitz smoothness to guarantee menucompatibility on the entire value space. This approach is general, leavingundisturbed trained menus that already satisfy menu compatibility and reducingto RochetNet for a single bidder. Mixed-integer linear programs are used formenu transforms and through a number of optimizations, including adaptive gridsand methods to skip menu elements, we scale to large auction design problems.GemNet learns auctions with better revenue than affine maximization methods,achieves exact SP whereas previous general multi-bidder methods areapproximately SP, and offers greatly enhanced interpretability.</description><author>Tonghan Wang, Yanchen Jiang, David C. Parkes</author><pubDate>Tue, 11 Jun 2024 17:30:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07428v1</guid></item><item><title>Errors are Robustly Tamed in Cumulative Knowledge Processes</title><link>http://arxiv.org/abs/2309.05638v3</link><description>We study processes of societal knowledge accumulation, where the validity ofa new unit of knowledge depends both on the correctness of its derivation andon the validity of the units it depends on. A fundamental question in thissetting is: If a constant fraction of the new derivations is wrong, caninvesting a constant fraction, bounded away from one, of effort ensure that aconstant fraction of knowledge in society is valid? Ben-Eliezer, Mikulincer,Mossel, and Sudan (ITCS 2023) introduced a concrete probabilistic model toanalyze such questions and showed an affirmative answer to this question. Theirstudy, however, focuses on the simple case where each new unit depends on justone existing unit, and units attach according to a $\textit{preferentialattachment rule}$. In this work, we consider much more general families of cumulative knowledgeprocesses, where new units may attach according to varied attachment mechanismsand depend on multiple existing units. We also allow a (random) fraction ofinsertions of adversarial nodes. We give a robust affirmative answer to the above question by showing that for$\textit{all}$ of these models, as long as many of the units follow simpleheuristics for checking a bounded number of units they depend on, all errorswill be eventually eliminated. Our results indicate that preserving the qualityof large interdependent collections of units of knowledge is feasible, as longas careful but not too costly checks are performed when new units arederived/deposited.</description><author>Anna Brandenberger, Cassandra Marcussen, Elchanan Mossel, Madhu Sudan</author><pubDate>Tue, 11 Jun 2024 17:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05638v3</guid></item><item><title>DERM12345: A Large, Multisource Dermatoscopic Skin Lesion Dataset with 38 Subclasses</title><link>http://arxiv.org/abs/2406.07426v1</link><description>Skin lesion datasets provide essential information for understanding variousskin conditions and developing effective diagnostic tools. They aid theartificial intelligence-based early detection of skin cancer, facilitatetreatment planning, and contribute to medical education and research. Publishedlarge datasets have partially coverage the subclassifications of the skinlesions. This limitation highlights the need for more expansive and varieddatasets to reduce false predictions and help improve the failure analysis forskin lesions. This study presents a diverse dataset comprising 12,345dermatoscopic images with 38 subclasses of skin lesions collected in Turkiyewhich comprises different skin types in the transition zone between Europe andAsia. Each subgroup contains high-resolution photos and expert annotations,providing a strong and reliable basis for future research. The detailedanalysis of each subgroup provided in this study facilitates targeted researchendeavors and enhances the depth of understanding regarding the skin lesions.This dataset distinguishes itself through a diverse structure with 5 superclasses, 15 main classes, 38 subclasses and its 12,345 high-resolutiondermatoscopic images.</description><author>Abdurrahim Yilmaz, Sirin Pekcan Yasar, Gulsum Gencoglan, Burak Temelkuran</author><pubDate>Tue, 11 Jun 2024 17:27:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07426v1</guid></item><item><title>MINERS: Multilingual Language Models as Semantic Retrievers</title><link>http://arxiv.org/abs/2406.07424v1</link><description>Words have been represented in a high-dimensional vector space that encodestheir semantic similarities, enabling downstream applications such asretrieving synonyms, antonyms, and relevant contexts. However, despite recentadvances in multilingual language models (LMs), the effectiveness of thesemodels' representations in semantic retrieval contexts has not beencomprehensively explored. To fill this gap, this paper introduces the MINERS, abenchmark designed to evaluate the ability of multilingual LMs in semanticretrieval tasks, including bitext mining and classification viaretrieval-augmented contexts. We create a comprehensive framework to assess therobustness of LMs in retrieving samples across over 200 diverse languages,including extremely low-resource languages in challenging cross-lingual andcode-switching settings. Our results demonstrate that by solely retrievingsemantically similar embeddings yields performance competitive withstate-of-the-art approaches, without requiring any fine-tuning.</description><author>Genta Indra Winata, Ruochen Zhang, David Ifeoluwa Adelani</author><pubDate>Tue, 11 Jun 2024 17:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07424v1</guid></item><item><title>Nash Learning from Human Feedback</title><link>http://arxiv.org/abs/2312.00886v4</link><description>Reinforcement learning from human feedback (RLHF) has emerged as the mainparadigm for aligning large language models (LLMs) with human preferences.Typically, RLHF involves the initial step of learning a reward model from humanfeedback, often expressed as preferences between pairs of text generationsproduced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned byoptimizing it to maximize the reward model through a reinforcement learningalgorithm. However, an inherent limitation of current reward models is theirinability to fully represent the richness of human preferences and theirdependency on the sampling distribution. In this study, we introduce an alternative pipeline for the fine-tuning ofLLMs using pairwise human feedback. Our approach entails the initial learningof a preference model, which is conditioned on two inputs given a prompt,followed by the pursuit of a policy that consistently generates responsespreferred over those generated by any competing policy, thus defining the Nashequilibrium of this preference model. We term this approach Nash learning fromhuman feedback (NLHF). In the context of a tabular policy representation, we present a novelalgorithmic solution, Nash-MD, founded on the principles of mirror descent.This algorithm produces a sequence of policies, with the last iterationconverging to the regularized Nash equilibrium. Additionally, we exploreparametric representations of policies and introduce gradient descentalgorithms for deep-learning architectures. To demonstrate the effectiveness ofour approach, we present experimental results involving the fine-tuning of aLLM for a text summarization task. We believe NLHF offers a compelling avenuefor preference learning and policy optimization with the potential of advancingthe field of aligning LLMs with human preferences.</description><author>Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, Bilal Piot</author><pubDate>Tue, 11 Jun 2024 17:25:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00886v4</guid></item><item><title>Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics</title><link>http://arxiv.org/abs/2403.14077v4</link><description>DeepFakes, which refer to AI-generated media content, have become anincreasing concern due to their use as a means for disinformation. DetectingDeepFakes is currently solved with programmed machine learning algorithms. Inthis work, we investigate the capabilities of multimodal large language models(LLMs) in DeepFake detection. We conducted qualitative and quantitativeexperiments to demonstrate multimodal LLMs and show that they can exposeAI-generated images through careful experimental design and prompt engineering.This is interesting, considering that LLMs are not inherently tailored formedia forensic tasks, and the process does not require programming. We discussthe limitations of multimodal LLMs for these tasks and suggest possibleimprovements.</description><author>Shan Jia, Reilin Lyu, Kangran Zhao, Yize Chen, Zhiyuan Yan, Yan Ju, Chuanbo Hu, Xin Li, Baoyuan Wu, Siwei Lyu</author><pubDate>Tue, 11 Jun 2024 17:24:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14077v4</guid></item><item><title>Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for Sampling</title><link>http://arxiv.org/abs/2406.07423v1</link><description>Monte Carlo methods, Variational Inference, and their combinations play apivotal role in sampling from intractable probability distributions. However,current studies lack a unified evaluation framework, relying on disparateperformance measures and limited method comparisons across diverse tasks,complicating the assessment of progress and hindering the decision-making ofpractitioners. In response to these challenges, our work introduces a benchmarkthat evaluates sampling methods using a standardized task suite and a broadrange of performance criteria. Moreover, we study existing metrics forquantifying mode collapse and introduce novel metrics for this purpose. Ourfindings provide insights into strengths and weaknesses of existing samplingmethods, serving as a valuable reference for future developments. The code ispublicly available here.</description><author>Denis Blessing, Xiaogang Jia, Johannes Esslinger, Francisco Vargas, Gerhard Neumann</author><pubDate>Tue, 11 Jun 2024 17:23:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07423v1</guid></item><item><title>Enhanced Gene Selection in Single-Cell Genomics: Pre-Filtering Synergy and Reinforced Optimization</title><link>http://arxiv.org/abs/2406.07418v1</link><description>Recent advancements in single-cell genomics necessitate precision in genepanel selection to interpret complex biological data effectively. Those methodsaim to streamline the analysis of scRNA-seq data by focusing on the mostinformative genes that contribute significantly to the specific analysis task.Traditional selection methods, which often rely on expert domain knowledge,embedded machine learning models, or heuristic-based iterative optimization,are prone to biases and inefficiencies that may obscure critical genomicsignals. Recognizing the limitations of traditional methods, we aim totranscend these constraints with a refined strategy. In this study, weintroduce an iterative gene panel selection strategy that is applicable toclustering tasks in single-cell genomics. Our method uniquely integratesresults from other gene selection algorithms, providing valuable preliminaryboundaries or prior knowledge as initial guides in the search space to enhancethe efficiency of our framework. Furthermore, we incorporate the stochasticnature of the exploration process in reinforcement learning (RL) and itscapability for continuous optimization through reward-based feedback. Thiscombination mitigates the biases inherent in the initial boundaries andharnesses RL's adaptability to refine and target gene panel selectiondynamically. To illustrate the effectiveness of our method, we conducteddetailed comparative experiments, case studies, and visualization analysis.</description><author>Weiliang Zhang, Zhen Meng, Dongjie Wang, Min Wu, Kunpeng Liu, Yuanchun Zhou, Meng Xiao</author><pubDate>Tue, 11 Jun 2024 17:21:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07418v1</guid></item><item><title>Holistic Memory Diversification for Incremental Learning in Growing Graphs</title><link>http://arxiv.org/abs/2406.07413v1</link><description>This paper addresses the challenge of incremental learning in growing graphswith increasingly complex tasks. The goal is to continually train a graph modelto handle new tasks while retaining its inference ability on previous tasks.Existing methods usually neglect the importance of memory diversity, limitingin effectively selecting high-quality memory from previous tasks andremembering broad previous knowledge within the scarce memory on graphs. Toaddress that, we introduce a novel holistic Diversified Memory Selection andGeneration (DMSG) framework for incremental learning in graphs, which firstintroduces a buffer selection strategy that considers both intra-class andinter-class diversities, employing an efficient greedy algorithm for samplingrepresentative training nodes from graphs into memory buffers after learningeach new task. Then, to adequately rememorize the knowledge preserved in thememory buffer when learning new tasks, we propose a diversified memorygeneration replay method. This method first utilizes a variational layer togenerate the distribution of buffer node embeddings and sample synthesized onesfor replaying. Furthermore, an adversarial variational embedding learningmethod and a reconstruction-based decoder are proposed to maintain theintegrity and consolidate the generalization of the synthesized nodeembeddings, respectively. Finally, we evaluate our model on node classificationtasks involving increasing class numbers. Extensive experimental results onpublicly accessible datasets demonstrate the superiority of DMSG overstate-of-the-art methods.</description><author>Ziyue Qiao, Junren Xiao, Qingqiang Sun, Meng Xiao, Hui Xiong</author><pubDate>Tue, 11 Jun 2024 17:18:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07413v1</guid></item><item><title>On the Convergence of Loss and Uncertainty-based Active Learning Algorithms</title><link>http://arxiv.org/abs/2312.13927v3</link><description>We investigate the convergence rates and data sample sizes required fortraining a machine learning model using a stochastic gradient descent (SGD)algorithm, where data points are sampled based on either their loss value oruncertainty value. These training methods are particularly relevant for activelearning and data subset selection problems. For SGD with a constant step sizeupdate, we present convergence results for linear classifiers and linearlyseparable datasets using squared hinge loss and similar training lossfunctions. Additionally, we extend our analysis to more general classifiers anddatasets, considering a wide range of loss-based sampling strategies and smoothconvex training loss functions. We propose a novel algorithm calledAdaptive-Weight Sampling (AWS) that utilizes SGD with an adaptive step sizethat achieves stochastic Polyak's step size in expectation. We establishconvergence rate results for AWS for smooth convex training loss functions. Ournumerical experiments demonstrate the efficiency of AWS on various datasets byusing either exact or estimated loss values.</description><author>Daniel Haimovich, Dima Karamshuk, Fridolin Linder, Niek Tax, Milan Vojnovic</author><pubDate>Tue, 11 Jun 2024 17:17:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13927v3</guid></item><item><title>OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing</title><link>http://arxiv.org/abs/2305.12307v3</link><description>Fine-grained entity typing (FET), which assigns entities in text withcontext-sensitive, fine-grained semantic types, is a basic but important taskfor knowledge extraction from unstructured text. FET has been studiedextensively in natural language processing and typically relies onhuman-annotated corpora for training, which is costly and difficult to scale.Recent studies explore the utilization of pre-trained language models (PLMs) asa knowledge base to generate rich and context-aware weak supervision for FET.However, a PLM still requires direction and guidance to serve as a knowledgebase as they often generate a mixture of rough and fine-grained types, ortokens unsuitable for typing. In this study, we vision that an ontologyprovides a semantics-rich, hierarchical structure, which will help select thebest results generated by multiple PLM models and head words. Specifically, wepropose a novel annotation-free, ontology-guided FET method, OntoType, whichfollows a type ontological structure, from coarse to fine, ensembles multiplePLM prompting results to generate a set of type candidates, and refines itstype resolution, under the local context with a natural language inferencemodel. Our experiments on the Ontonotes, FIGER, and NYT datasets using theirassociated ontological structures demonstrate that our method outperforms thestate-of-the-art zero-shot fine-grained entity typing methods as well as atypical LLM method, ChatGPT. Our error analysis shows that refinement of theexisting ontology structures will further improve fine-grained entity typing.</description><author>Tanay Komarlu, Minhao Jiang, Xuan Wang, Jiawei Han</author><pubDate>Tue, 11 Jun 2024 17:16:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12307v3</guid></item><item><title>VersiCode: Towards Version-controllable Code Generation</title><link>http://arxiv.org/abs/2406.07411v1</link><description>Significant research has focused on improving the performance of largelanguage model on code-related tasks due to their practical importance.Although performance is typically evaluated using public benchmark datasets,the existing datasets do not account for the concept of \emph{version}, whichis crucial in professional software development. In this paper, we introduceVersiCode, the first comprehensive dataset designed to assess the ability oflarge language models to generate verifiable code for specific libraryversions. VersiCode encompasses 300 libraries across more than 2,000 versionsspanning 9 years. We design two dedicated evaluation tasks: version-specificcode completion (VSCC) and version-aware code editing (VACE). Comprehensiveexperiments are conducted to benchmark the performance of LLMs, revealing thechallenging nature of these tasks and VersiCode, that even state-of-the-artLLMs struggle to generate version-correct code. This dataset, together with theproposed tasks, sheds light on LLMs' capabilities and limitations in handlingversion-specific code generation, and opens up an important new area ofresearch for further investigation. The resources can be found athttps://github.com/wutong8023/VersiCode.</description><author>Tongtong Wu, Weigang Wu, Xingyu Wang, Kang Xu, Suyu Ma, Bo Jiang, Ping Yang, Zhenchang Xing, Yuan-Fang Li, Gholamreza Haffari</author><pubDate>Tue, 11 Jun 2024 17:15:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07411v1</guid></item><item><title>Accelerating Ill-conditioned Hankel Matrix Recovery via Structured Newton-like Descent</title><link>http://arxiv.org/abs/2406.07409v1</link><description>This paper studies the robust Hankel recovery problem, which simultaneouslyremoves the sparse outliers and fulfills missing entries from the partialobservation. We propose a novel non-convex algorithm, coined Hankel StructuredNewton-Like Descent (HSNLD), to tackle the robust Hankel recovery problem.HSNLD is highly efficient with linear convergence, and its convergence rate isindependent of the condition number of the underlying Hankel matrix. Therecovery guarantee has been established under some mild conditions. Numericalexperiments on both synthetic and real datasets show the superior performanceof HSNLD against state-of-the-art algorithms.</description><author>HanQin Cai, Longxiu Huang, Xiliang Lu, Juntao You</author><pubDate>Tue, 11 Jun 2024 17:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07409v1</guid></item><item><title>Private Geometric Median</title><link>http://arxiv.org/abs/2406.07407v1</link><description>In this paper, we study differentially private (DP) algorithms for computingthe geometric median (GM) of a dataset: Given $n$ points, $x_1,\dots,x_n$ in$\mathbb{R}^d$, the goal is to find a point $\theta$ that minimizes the sum ofthe Euclidean distances to these points, i.e., $\sum_{i=1}^{n} \|\theta -x_i\|_2$. Off-the-shelf methods, such as DP-GD, require strong a prioriknowledge locating the data within a ball of radius $R$, and the excess risk ofthe algorithm depends linearly on $R$. In this paper, we ask: can we design anefficient and private algorithm with an excess error guarantee that scales withthe (unknown) radius containing the majority of the datapoints? Our maincontribution is a pair of polynomial-time DP algorithms for the task of privateGM with an excess error guarantee that scales with the effective diameter ofthe datapoints. Additionally, we propose an inefficient algorithm based on theinverse smooth sensitivity mechanism, which satisfies the more restrictivenotion of pure DP. We complement our results with a lower bound and demonstratethe optimality of our polynomial-time algorithms in terms of sample complexity.</description><author>Mahdi Haghifam, Thomas Steinke, Jonathan Ullman</author><pubDate>Tue, 11 Jun 2024 17:13:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07407v1</guid></item><item><title>Enhancing Tabular Data Optimization with a Flexible Graph-based Reinforced Exploration Strategy</title><link>http://arxiv.org/abs/2406.07404v1</link><description>Tabular data optimization methods aim to automatically find an optimalfeature transformation process that generates high-value features and improvesthe performance of downstream machine learning tasks. Current frameworks forautomated feature transformation rely on iterative sequence generation tasks,optimizing decision strategies through performance feedback from downstreamtasks. However, these approaches fail to effectively utilize historicaldecision-making experiences and overlook potential relationships amonggenerated features, thus limiting the depth of knowledge extraction. Moreover,the granularity of the decision-making process lacks dynamic backtrackingcapabilities for individual features, leading to insufficient adaptability whenencountering inefficient pathways, adversely affecting overall robustness andexploration efficiency. To address the limitations observed in currentautomatic feature engineering frameworks, we introduce a novel method thatutilizes a feature-state transformation graph to effectively preserve theentire feature transformation journey, where each node represents a specifictransformation state. During exploration, three cascading agents iterativelyselect nodes and idea mathematical operations to generate new transformationstates. This strategy leverages the inherent properties of the graph structure,allowing for the preservation and reuse of valuable transformations. It alsoenables backtracking capabilities through graph pruning techniques, which canrectify inefficient transformation paths. To validate the efficacy andflexibility of our approach, we conducted comprehensive experiments anddetailed case studies, demonstrating superior performance in diverse scenarios.</description><author>Xiaohan Huang, Dongjie Wang, Zhiyuan Ning, Ziyue Qiao, Qingqing Long, Haowei Zhu, Min Wu, Yuanchun Zhou, Meng Xiao</author><pubDate>Tue, 11 Jun 2024 17:10:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07404v1</guid></item><item><title>A Survey on Recent Random Walk-based Methods for Embedding Knowledge Graphs</title><link>http://arxiv.org/abs/2406.07402v1</link><description>Machine learning, deep learning, and NLP methods on knowledge graphs arepresent in different fields and have important roles in various domains fromself-driving cars to friend recommendations on social media platforms. However,to apply these methods to knowledge graphs, the data usually needs to be in anacceptable size and format. In fact, knowledge graphs normally have highdimensions and therefore we need to transform them to a low-dimensional vectorspace. An embedding is a low-dimensional space into which you can translatehigh dimensional vectors in a way that intrinsic features of the input data arepreserved. In this review, we first explain knowledge graphs and theirembedding and then review some of the random walk-based embedding methods thathave been developed recently.</description><author>Elika Bozorgi, Sakher Khalil Alqaiidi, Afsaneh Shams, Hamid Reza Arabnia, Krzysztof Kochut</author><pubDate>Tue, 11 Jun 2024 17:08:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07402v1</guid></item><item><title>Guiding LLM Temporal Logic Generation with Explicit Separation of Data and Control</title><link>http://arxiv.org/abs/2406.07400v1</link><description>Temporal logics are powerful tools that are widely used for the synthesis andverification of reactive systems. The recent progress on Large Language Models(LLMs) has the potential to make the process of writing such specificationsmore accessible. However, writing specifications in temporal logics remainschallenging for all but the most expert users. A key question in using LLMs fortemporal logic specification engineering is to understand what kind of guidanceis most helpful to the LLM and the users to easily produce specifications.Looking specifically at the problem of reactive program synthesis, we explorethe impact of providing an LLM with guidance on the separation of control anddata--making explicit for the LLM what functionality is relevant for thespecification, and treating the remaining functionality as an implementationdetail for a series of pre-defined functions and predicates. We present abenchmark set and find that this separation of concerns improves specificationgeneration. Our benchmark provides a test set against which to verify futurework in LLM generation of temporal logic specifications.</description><author>William Murphy, Nikolaus Holzer, Nathan Koenig, Leyi Cui, Raven Rothkopf, Feitong Qiao, Mark Santolucito</author><pubDate>Tue, 11 Jun 2024 17:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07400v1</guid></item><item><title>Redefining Automotive Radar Imaging: A Domain-Informed 1D Deep Learning Approach for High-Resolution and Efficient Performance</title><link>http://arxiv.org/abs/2406.07399v1</link><description>Millimeter-wave (mmWave) radars are indispensable for perception tasks ofautonomous vehicles, thanks to their resilience in challenging weatherconditions. Yet, their deployment is often limited by insufficient spatialresolution for precise semantic scene interpretation. Classicalsuper-resolution techniques adapted from optical imaging inadequately addressthe distinct characteristics of radar signal data. In response, our studyredefines radar imaging super-resolution as a one-dimensional (1D) signalsuper-resolution spectra estimation problem by harnessing the radar signalprocessing domain knowledge, introducing innovative data normalization and adomain-informed signal-to-noise ratio (SNR)-guided loss function. Our tailoreddeep learning network for automotive radar imaging exhibits remarkablescalability, parameter efficiency and fast inference speed, alongside enhancedperformance in terms of radar imaging quality and resolution. Extensive testingconfirms that our SR-SPECNet sets a new benchmark in producing high-resolutionradar range-azimuth images, outperforming existing methods across variedantenna configurations and dataset sizes. Source code and new radar datasetwill be made publicly available online.</description><author>Ruxin Zheng, Shunqiao Sun, Holger Caesar, Honglei Chen, Jian Li</author><pubDate>Tue, 11 Jun 2024 17:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07399v1</guid></item><item><title>Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving</title><link>http://arxiv.org/abs/2406.03877v2</link><description>In an era marked by the rapid scaling of foundation models, autonomousdriving technologies are approaching a transformative threshold whereend-to-end autonomous driving (E2E-AD) emerges due to its potential of scalingup in the data-driven manner. However, existing E2E-AD methods are mostlyevaluated under the open-loop log-replay manner with L2 errors and collisionrate as metrics (e.g., in nuScenes), which could not fully reflect the drivingperformance of algorithms as recently acknowledged in the community. For thoseE2E-AD methods evaluated under the closed-loop protocol, they are tested infixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score asmetrics, which is known for high variance due to the unsmoothed metric functionand large randomness in the long route. Besides, these methods usually collecttheir own data for training, which makes algorithm-level fair comparisoninfeasible. To fulfill the paramount need of comprehensive, realistic, and fair testingenvironments for Full Self-Driving (FSD), we present Bench2Drive, the firstbenchmark for evaluating E2E-AD systems' multiple abilities in a closed-loopmanner. Bench2Drive's official training data consists of 2 million fullyannotated frames, collected from 10000 short clips uniformly distributed under44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny,foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2.Its evaluation protocol requires E2E-AD models to pass 44 interactive scenariosunder different locations and weathers which sums up to 220 routes and thusprovides a comprehensive and disentangled assessment about their drivingcapability under different situations. We implement state-of-the-art E2E-ADmodels and evaluate them in Bench2Drive, providing insights regarding currentstatus and future directions.</description><author>Xiaosong Jia, Zhenjie Yang, Qifeng Li, Zhiyuan Zhang, Junchi Yan</author><pubDate>Tue, 11 Jun 2024 17:06:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03877v2</guid></item><item><title>Visual Representation Learning with Stochastic Frame Prediction</title><link>http://arxiv.org/abs/2406.07398v1</link><description>Self-supervised learning of image representations by predicting future framesis a promising direction but still remains a challenge. This is because of theunder-determined nature of frame prediction; multiple potential futures canarise from a single current frame. To tackle this challenge, in this paper, werevisit the idea of stochastic video generation that learns to captureuncertainty in frame prediction and explore its effectiveness forrepresentation learning. Specifically, we design a framework that trains astochastic frame prediction model to learn temporal information between frames.Moreover, to learn dense information within each frame, we introduce anauxiliary masked image modeling objective along with a shared decoderarchitecture. We find this architecture allows for combining both objectives ina synergistic and compute-efficient manner. We demonstrate the effectiveness ofour framework on a variety of tasks from video label propagation andvision-based robot learning domains, such as video segmentation, pose tracking,vision-based robotic locomotion, and manipulation tasks. Code is available onthe project webpage: https://sites.google.com/view/2024rsp.</description><author>Huiwon Jang, Dongyoung Kim, Junsu Kim, Jinwoo Shin, Pieter Abbeel, Younggyo Seo</author><pubDate>Tue, 11 Jun 2024 17:05:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07398v1</guid></item><item><title>Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B</title><link>http://arxiv.org/abs/2406.07394v1</link><description>This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovativeintegration of Large Language Models (LLMs) with Monte Carlo Tree Search(MCTS), designed to enhance performance in complex mathematical reasoningtasks. Addressing the challenges of accuracy and reliability in LLMs,particularly in strategic and mathematical reasoning, MCTSr leveragessystematic exploration and heuristic self-refine mechanisms to improvedecision-making frameworks within LLMs. The algorithm constructs a Monte Carlosearch tree through iterative processes of Selection, self-refine,self-evaluation, and Backpropagation, utilizing an improved Upper ConfidenceBound (UCB) formula to optimize the exploration-exploitation balance. Extensiveexperiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematicalproblems, significantly improving success rates across multiple datasets,including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including MathOdyssey, AIME, and OlympiadBench. The study advances the application of LLMs incomplex reasoning tasks and sets a foundation for future AI integration,enhancing decision-making accuracy and reliability in LLM-driven applications.</description><author>Di Zhang, Jiatong Li, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang</author><pubDate>Tue, 11 Jun 2024 17:01:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07394v1</guid></item><item><title>Limited Out-of-Context Knowledge Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2406.07393v1</link><description>Large Language Models (LLMs) have demonstrated strong capabilities asknowledge bases and significant in-context reasoning capabilities. However,previous work challenges their out-of-context reasoning ability, i.e., theability to infer information from their training data, instead of from thecontext or prompt. This paper focuses on a significant facet of out-of-contextreasoning: Out-of-Context Knowledge Reasoning (OCKR), which is to combinemultiple knowledge to infer new knowledge. We designed a synthetic dataset withseven representative OCKR tasks to systematically assess the OCKR capabilitiesof LLMs. Using this dataset, we evaluated the LLaMA2-13B-chat model anddiscovered that its proficiency in this aspect is limited, regardless ofwhether the knowledge is trained in a separate or adjacent training settings.Moreover, training the model to reason with complete reasoning data did notresult in significant improvement. Training the model to perform explicitknowledge retrieval helps in only one of the tasks, indicating that the model'slimited OCKR capabilities are due to difficulties in retrieving relevantknowledge. Furthermore, we treat cross-lingual knowledge transfer as a distinctform of OCKR, and evaluate this ability. Our results show that the evaluatedmodel also exhibits limited ability in transferring knowledge across languages.The dataset used in this study is available athttps://github.com/NJUNLP/ID-OCKR.</description><author>Peng Hu, Changjiang Gao, Ruiqi Gao, Jiajun Chen, Shujian Huang</author><pubDate>Tue, 11 Jun 2024 16:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07393v1</guid></item></channel></rss>