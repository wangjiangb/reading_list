<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 28 Sep 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations</title><link>http://arxiv.org/abs/2309.15848v1</link><description>Implicit Neural Representations (INR) or neural fields have emerged as apopular framework to encode multimedia signals such as images and radiancefields while retaining high-quality. Recently, learnable feature grids proposedby Instant-NGP have allowed significant speed-up in the training as well as thesampling of INRs by replacing a large neural network with a multi-resolutionlook-up table of feature vectors and a much smaller neural network. However,these feature grids come at the expense of large memory consumption which canbe a bottleneck for storage and streaming applications. In this work, wepropose SHACIRA, a simple yet effective task-agnostic framework for compressingsuch feature grids with no additional post-hoc pruning/quantization stages. Wereparameterize feature grids with quantized latent weights and apply entropyregularization in the latent space to achieve high levels of compression acrossvarious domains. Quantitative and qualitative results on diverse datasetsconsisting of images, videos, and radiance fields, show that our approachoutperforms existing INR approaches without the need for any large datasets ordomain-specific heuristics. Our project page is available athttp://shacira.github.io .</description><author>Sharath Girish, Abhinav Shrivastava, Kamal Gupta</author><pubDate>Wed, 27 Sep 2023 18:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15848v1</guid></item><item><title>Exploiting the Signal-Leak Bias in Diffusion Models</title><link>http://arxiv.org/abs/2309.15842v1</link><description>There is a bias in the inference pipeline of most diffusion models. This biasarises from a signal leak whose distribution deviates from the noisedistribution, creating a discrepancy between training and inference processes.We demonstrate that this signal-leak bias is particularly significant whenmodels are tuned to a specific style, causing sub-optimal style matching.Recent research tries to avoid the signal leakage during training. We insteadshow how we can exploit this signal-leak bias in existing diffusion models toallow more control over the generated images. This enables us to generateimages with more varied brightness, and images that better match a desiredstyle or color. By modeling the distribution of the signal leak in the spatialfrequency and pixel domains, and including a signal leak in the initial latent,we generate images that better match expected results without any additionaltraining.</description><author>Martin Nicolas Everaert, Athanasios Fitsios, Marco Bocchio, Sami Arpa, Sabine Süsstrunk, Radhakrishna Achanta</author><pubDate>Wed, 27 Sep 2023 18:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15842v1</guid></item><item><title>On Exploiting Hitting Sets for Model Reconciliation</title><link>http://arxiv.org/abs/2012.09274v3</link><description>In human-aware planning, a planning agent may need to provide an explanationto a human user on why its plan is optimal. A popular approach to do this iscalled model reconciliation, where the agent tries to reconcile the differencesin its model and the human's model such that the plan is also optimal in thehuman's model. In this paper, we present a logic-based framework for modelreconciliation that extends beyond the realm of planning. More specifically,given a knowledge base $KB_1$ entailing a formula $\varphi$ and a secondknowledge base $KB_2$ not entailing it, model reconciliation seeks anexplanation, in the form of a cardinality-minimal subset of $KB_1$, whoseintegration into $KB_2$ makes the entailment possible. Our approach, based onideas originating in the context of analysis of inconsistencies, exploits theexisting hitting set duality between minimal correction sets (MCSes) andminimal unsatisfiable sets (MUSes) in order to identify an appropriateexplanation. However, differently from those works targeting inconsistentformulas, which assume a single knowledge base, MCSes and MUSes are computedover two distinct knowledge bases. We conclude our paper with an empiricalevaluation of the newly introduced approach on planning instances, where weshow how it outperforms an existing state-of-the-art solver, and genericnon-planning instances from recent SAT competitions, for which no other solverexists.</description><author>Stylianos Loukas Vasileiou, Alessandro Previti, William Yeoh</author><pubDate>Wed, 27 Sep 2023 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2012.09274v3</guid></item><item><title>Examining the Values Reflected by Children during AI Problem Formulation</title><link>http://arxiv.org/abs/2309.15839v1</link><description>Understanding how children design and what they value in AI interfaces thatallow them to explicitly train their models such as teachable machines, couldhelp increase such activities' impact and guide the design of futuretechnologies. In a co-design session using a modified storyboard, a team of 5children (aged 7-13 years) and adult co-designers, engaged in AI problemformulation activities where they imagine their own teachable machines. Ourfindings, leveraging an established psychological value framework (the RokeachValue Survey), illuminate how children conceptualize and embed their values inAI systems that they themselves devise to support their everyday activities.Specifically, we find that children's proposed ideas require advanced systemintelligence, e.g. emotion detection and understanding the social relationshipsof a user. The underlying models could be trained under multiple modalities andany errors would be fixed by adding more data or by anticipating negativeexamples. Children's ideas showed they cared about family and expected machinesto understand their social context before making decisions.</description><author>Utkarsh Dwivedi, Salma Elsayed-ali, Elizabeth Bonsignore, Hernisa Kacorri</author><pubDate>Wed, 27 Sep 2023 18:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15839v1</guid></item><item><title>OrthoPlanes: A Novel Representation for Better 3D-Awareness of GANs</title><link>http://arxiv.org/abs/2309.15830v1</link><description>We present a new method for generating realistic and view-consistent imageswith fine geometry from 2D image collections. Our method proposes a hybridexplicit-implicit representation called \textbf{OrthoPlanes}, which encodesfine-grained 3D information in feature maps that can be efficiently generatedby modifying 2D StyleGANs. Compared to previous representations, our method hasbetter scalability and expressiveness with clear and explicit information. As aresult, our method can handle more challenging view-angles and synthesizearticulated objects with high spatial degree of freedom. Experimentsdemonstrate that our method achieves state-of-the-art results on FFHQ and SHHQdatasets, both quantitatively and qualitatively. Project page:\url{https://orthoplanes.github.io/}.</description><author>Honglin He, Zhuoqian Yang, Shikai Li, Bo Dai, Wayne Wu</author><pubDate>Wed, 27 Sep 2023 18:52:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15830v1</guid></item><item><title>Multi-unit soft sensing permits few-shot learning</title><link>http://arxiv.org/abs/2309.15828v1</link><description>Recent literature has explored various ways to improve soft sensors usinglearning algorithms with transferability. Broadly put, the performance of asoft sensor may be strengthened when it is learned by solving multiple tasks.The usefulness of transferability depends on how strongly related the devisedlearning tasks are. A particularly relevant case for transferability, is when asoft sensor is to be developed for a process of which there are manyrealizations, e.g. system or device with many implementations from which datais available. Then, each realization presents a soft sensor learning task, andit is reasonable to expect that the different tasks are strongly related.Applying transferability in this setting leads to what we call multi-unit softsensing, where a soft sensor models a process by learning from data from all ofits realizations. This paper explores the learning abilities of a multi-unit soft sensor, whichis formulated as a hierarchical model and implemented using a deep neuralnetwork. In particular, we investigate how well the soft sensor generalizes asthe number of units increase. Using a large industrial dataset, we demonstratethat, when the soft sensor is learned from a sufficient number of tasks, itpermits few-shot learning on data from new units. Surprisingly, regarding thedifficulty of the task, few-shot learning on 1-3 data points often leads to ahigh performance on new units.</description><author>Bjarne Grimstad, Kristian Løvland, Lars S. Imsland</author><pubDate>Wed, 27 Sep 2023 18:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15828v1</guid></item><item><title>Accelerating Deep Neural Networks via Semi-Structured Activation Sparsity</title><link>http://arxiv.org/abs/2309.06626v2</link><description>The demand for efficient processing of deep neural networks (DNNs) onembedded devices is a significant challenge limiting their deployment.Exploiting sparsity in the network's feature maps is one of the ways to reduceits inference latency. It is known that unstructured sparsity results in loweraccuracy degradation with respect to structured sparsity but the former needsextensive inference engine changes to get latency benefits. To tackle thischallenge, we propose a solution to induce semi-structured activation sparsityexploitable through minor runtime modifications. To attain high speedup levelsat inference time, we design a sparse training procedure with awareness of thefinal position of the activations while computing the General MatrixMultiplication (GEMM). We extensively evaluate the proposed solution acrossvarious models for image classification and object detection tasks. Remarkably,our approach yields a speed improvement of $1.25 \times$ with a minimalaccuracy drop of $1.1\%$ for the ResNet18 model on the ImageNet dataset.Furthermore, when combined with a state-of-the-art structured pruning method,the resulting models provide a good latency-accuracy trade-off, outperformingmodels that solely employ structured pruning techniques.</description><author>Matteo Grimaldi, Darshan C. Ganji, Ivan Lazarevich, Sudhakar Sah</author><pubDate>Wed, 27 Sep 2023 18:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06626v2</guid></item><item><title>Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing</title><link>http://arxiv.org/abs/2309.15826v1</link><description>Recent works in end-to-end speech-to-text translation (ST) have proposedmulti-tasking methods with soft parameter sharing which leverage machinetranslation (MT) data via secondary encoders that map text inputs to aneventual cross-modal representation. In this work, we instead propose a ST/MTmulti-tasking framework with hard parameter sharing in which all modelparameters are shared cross-modally. Our method reduces the speech-textmodality gap via a pre-processing stage which converts speech and text inputsinto two discrete token sequences of similar length -- this allows models toindiscriminately process both modalities simply using a joint vocabulary. Withexperiments on MuST-C, we demonstrate that our multi-tasking framework improvesattentional encoder-decoder, Connectionist Temporal Classification (CTC),transducer, and joint CTC/attention models by an average of +0.5 BLEU withoutany external MT data. Further, we show that this framework incorporatesexternal MT data, yielding +0.8 BLEU, and also improves transfer learning frompre-trained textual models, yielding +1.8 BLEU.</description><author>Brian Yan, Xuankai Chang, Antonios Anastasopoulos, Yuya Fujita, Shinji Watanabe</author><pubDate>Wed, 27 Sep 2023 18:48:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15826v1</guid></item><item><title>Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</title><link>http://arxiv.org/abs/2309.15818v1</link><description>Significant advancements have been achieved in the realm of large-scalepre-trained text-to-video Diffusion Models (VDMs). However, previous methodseither rely solely on pixel-based VDMs, which come with high computationalcosts, or on latent-based VDMs, which often struggle with precise text-videoalignment. In this paper, we are the first to propose a hybrid model, dubbed asShow-1, which marries pixel-based and latent-based VDMs for text-to-videogeneration. Our model first uses pixel-based VDMs to produce a low-resolutionvideo of strong text-video correlation. After that, we propose a novel experttranslation method that employs the latent-based VDMs to further upsample thelow-resolution video to high resolution. Compared to latent VDMs, Show-1 canproduce high-quality videos of precise text-video alignment; Compared to pixelVDMs, Show-1 is much more efficient (GPU memory usage during inference is 15Gvs 72G). We also validate our model on standard video generation benchmarks.Our code and model weights are publicly available at\url{https://github.com/showlab/Show-1}.</description><author>David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou</author><pubDate>Wed, 27 Sep 2023 18:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15818v1</guid></item><item><title>AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers</title><link>http://arxiv.org/abs/2306.06531v2</link><description>For effective human-robot interaction, robots need to understand, plan, andexecute complex, long-horizon tasks described by natural language. Recentadvances in large language models (LLMs) have shown promise for translatingnatural language into robot action sequences for complex tasks. However,existing approaches either translate the natural language directly into robottrajectories or factor the inference process by decomposing language into tasksub-goals and relying on a motion planner to execute each sub-goal. Whencomplex environmental and temporal constraints are involved, inference overplanning tasks must be performed jointly with motion plans using traditionaltask-and-motion planning (TAMP) algorithms, making factorization into subgoalsuntenable. Rather than using LLMs to directly plan task sub-goals, we insteadperform few-shot translation from natural language task descriptions to anintermediate task representation that can then be consumed by a TAMP algorithmto jointly solve the task and motion plan. To improve translation, weautomatically detect and correct both syntactic and semantic errors viaautoregressive re-prompting, resulting in significant improvements in taskcompletion. We show that our approach outperforms several methods using LLMs asplanners in complex task domains. See our project websitehttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.</description><author>Yongchao Chen, Jacob Arkin, Charles Dawson, Yang Zhang, Nicholas Roy, Chuchu Fan</author><pubDate>Wed, 27 Sep 2023 18:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06531v2</guid></item><item><title>Detecting Objects with Context-Likelihood Graphs and Graph Refinement</title><link>http://arxiv.org/abs/2212.12395v3</link><description>The goal of this paper is to detect objects by exploiting theirinterrelationships. Contrary to existing methods, which learn objects andrelations separately, our key idea is to learn the object-relation distributionjointly. We first propose a novel way of creating a graphical representation ofan image from inter-object relation priors and initial class predictions, wecall a context-likelihood graph. We then learn the joint distribution with anenergy-based modeling technique which allows to sample and refine thecontext-likelihood graph iteratively for a given image. Our formulation ofjointly learning the distribution enables us to generate a more accurate graphrepresentation of an image which leads to a better object detectionperformance. We demonstrate the benefits of our context-likelihood graphformulation and the energy-based graph refinement via experiments on the VisualGenome and MS-COCO datasets where we achieve a consistent improvement overobject detectors like DETR and Faster-RCNN, as well as alternative methodsmodeling object interrelationships separately. Our method is detector agnostic,end-to-end trainable, and especially beneficial for rare object classes.</description><author>Aritra Bhowmik, Yu Wang, Nora Baka, Martin R. Oswald, Cees G. M. Snoek</author><pubDate>Wed, 27 Sep 2023 18:43:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12395v3</guid></item><item><title>Convolutional Networks with Oriented 1D Kernels</title><link>http://arxiv.org/abs/2309.15812v1</link><description>In computer vision, 2D convolution is arguably the most important operationperformed by a ConvNet. Unsurprisingly, it has been the focus of intensesoftware and hardware optimization and enjoys highly efficient implementations.In this work, we ask an intriguing question: can we make a ConvNet work without2D convolutions? Surprisingly, we find that the answer is yes -- we show that aConvNet consisting entirely of 1D convolutions can do just as well as 2D onImageNet classification. Specifically, we find that one key ingredient to ahigh-performing 1D ConvNet is oriented 1D kernels: 1D kernels that are orientednot just horizontally or vertically, but also at other angles. Our experimentsshow that oriented 1D convolutions can not only replace 2D convolutions butalso augment existing architectures with large kernels, leading to improvedaccuracy with minimal FLOPs increase. A key contribution of this work is ahighly-optimized custom CUDA implementation of oriented 1D kernels, specializedto the depthwise convolution setting. Our benchmarks demonstrate that ourcustom CUDA implementation almost perfectly realizes the theoretical advantageof 1D convolution: it is faster than a native horizontal convolution for anyarbitrary angle. Code is available athttps://github.com/princeton-vl/Oriented1D.</description><author>Alexandre Kirchmeyer, Jia Deng</author><pubDate>Wed, 27 Sep 2023 18:36:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15812v1</guid></item><item><title>Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics</title><link>http://arxiv.org/abs/2304.09123v2</link><description>This paper provides a finite-sample analysis of a passive stochastic gradientLangevin dynamics algorithm (PSGLD) designed to achieve adaptive inversereinforcement learning (IRL). By passive, we mean that the noisy gradientsavailable to the PSGLD algorithm (inverse learning process) are evaluated atrandomly chosen points by an external stochastic gradient algorithm (forwardlearner) that aims to optimize a cost function. The PSGLD algorithm acts as arandomized sampler to achieve adaptive IRL by reconstructing this cost functionnonparametrically from the stationary measure of a Langevin diffusion. Previouswork has analyzed the asymptotic performance of this passive algorithm usingweak convergence techniques. This paper analyzes the non-asymptotic(finite-sample) performance using a logarithmic-Sobolev inequality and theOtto-Villani Theorem. We obtain finite-sample bounds on the 2-Wassersteindistance between the estimates generated by the PSGLD algorithm and the costfunction. Apart from achieving finite-sample guarantees for adaptive IRL, thiswork extends a line of research in analysis of passive stochastic gradientalgorithms to the finite-sample regime for Langevin dynamics.</description><author>Luke Snow, Vikram Krishnamurthy</author><pubDate>Wed, 27 Sep 2023 18:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09123v2</guid></item><item><title>Fair Canonical Correlation Analysis</title><link>http://arxiv.org/abs/2309.15809v1</link><description>This paper investigates fairness and bias in Canonical Correlation Analysis(CCA), a widely used statistical technique for examining the relationshipbetween two sets of variables. We present a framework that alleviatesunfairness by minimizing the correlation disparity error associated withprotected attributes. Our approach enables CCA to learn global projectionmatrices from all data points while ensuring that these matrices yieldcomparable correlation levels to group-specific projection matrices.Experimental evaluation on both synthetic and real-world datasets demonstratesthe efficacy of our method in reducing correlation disparity error withoutcompromising CCA accuracy.</description><author>Zhuoping Zhou, Davoud Ataee Tarzanagh, Bojian Hou, Boning Tong, Jia Xu, Yanbo Feng, Qi Long, Li Shen</author><pubDate>Wed, 27 Sep 2023 18:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15809v1</guid></item><item><title>A Deep Learning System for Domain-specific Speech Recognition</title><link>http://arxiv.org/abs/2303.10510v2</link><description>As human-machine voice interfaces provide easy access to increasinglyintelligent machines, many state-of-the-art automatic speech recognition (ASR)systems are proposed. However, commercial ASR systems usually have poorperformance on domain-specific speech especially under low-resource settings.The author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models todevelop benefit-specific ASR systems. The domain-specific data are collectedusing proposed semi-supervised learning annotation with little humanintervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60acoustic model with an external KenLM, which surpasses the Google and AWS ASRsystems on benefit-specific speech. The viability of using error prone ASRtranscriptions as part of spoken language understanding (SLU) is alsoinvestigated. Results of a benefit-specific natural language understanding(NLU) task show that the domain-specific fine-tuned ASR system can outperformthe commercial ASR systems even when its transcriptions have higher word errorrate (WER), and the results between fine-tuned ASR and human transcriptions aresimilar.</description><author>Yanan Jia</author><pubDate>Wed, 27 Sep 2023 18:32:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10510v2</guid></item><item><title>Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack</title><link>http://arxiv.org/abs/2309.15807v1</link><description>Training text-to-image models with web scale image-text pairs enables thegeneration of a wide range of visual concepts from text. However, thesepre-trained models often face challenges when it comes to generating highlyaesthetic images. This creates the need for aesthetic alignment postpre-training. In this paper, we propose quality-tuning to effectively guide apre-trained model to exclusively generate highly visually appealing images,while maintaining generality across visual concepts. Our key insight is thatsupervised fine-tuning with a set of surprisingly small but extremely visuallyappealing images can significantly improve the generation quality. We pre-traina latent diffusion model on $1.1$ billion image-text pairs and fine-tune itwith only a few thousand carefully selected high-quality images. The resultingmodel, Emu, achieves a win rate of $82.9\%$ compared with its pre-trained onlycounterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred$68.4\%$ and $71.3\%$ of the time on visual appeal on the standard PartiPromptsand our Open User Input benchmark based on the real-world usage oftext-to-image models. In addition, we show that quality-tuning is a genericapproach that is also effective for other architectures, including pixeldiffusion and masked generative transformer models.</description><author>Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, Devi Parikh</author><pubDate>Wed, 27 Sep 2023 18:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15807v1</guid></item><item><title>Lyra: Orchestrating Dual Correction in Automated Theorem Proving</title><link>http://arxiv.org/abs/2309.15806v1</link><description>Large Language Models (LLMs) present an intriguing avenue for exploration inthe field of formal theorem proving. Nevertheless, their full potential,particularly concerning the mitigation of hallucinations and refinement throughprover error messages, remains an area that has yet to be thoroughlyinvestigated. To enhance the effectiveness of LLMs in the field, we introducethe Lyra, a new framework that employs two distinct correction mechanisms: ToolCorrection (TC) and Conjecture Correction (CC). To implement Tool Correction inthe post-processing of formal proofs, we leverage prior knowledge to utilizepredefined prover tools (e.g., Sledgehammer) for guiding the replacement ofincorrect tools. Tool Correction significantly contributes to mitigatinghallucinations, thereby improving the overall accuracy of the proof. Inaddition, we introduce Conjecture Correction, an error feedback mechanismdesigned to interact with prover to refine formal proof conjectures with provererror messages. Compared to the previous refinement framework, the proposedConjecture Correction refines generation with instruction but does not collectpaired (generation, error &amp; refinement) prompts. Our method has achievedstate-of-the-art (SOTA) performance on both miniF2F validation (48.0% -&gt; 55.3%)and test (45.5% -&gt; 51.2%). We also present 3 IMO problems solved by Lyra. Webelieve Tool Correction (post-process for hallucination mitigation) andConjecture Correction (subgoal adjustment from interaction with environment)could provide a promising avenue for future research in this field.</description><author>Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, Yu Li</author><pubDate>Wed, 27 Sep 2023 18:29:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15806v1</guid></item><item><title>Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study</title><link>http://arxiv.org/abs/2309.15800v1</link><description>Speech signals, typically sampled at rates in the tens of thousands persecond, contain redundancies, evoking inefficiencies in sequence modeling.High-dimensional speech features such as spectrograms are often used as theinput for the subsequent model. However, they can still be redundant. Recentinvestigations proposed the use of discrete speech units derived fromself-supervised learning representations, which significantly compresses thesize of speech data. Applying various methods, such as de-duplication andsubword modeling, can further compress the speech sequence length. Hence,training time is significantly reduced while retaining notable performance. Inthis study, we undertake a comprehensive and systematic exploration into theapplication of discrete units within end-to-end speech processing models.Experiments on 12 automatic speech recognition, 3 speech translation, and 1spoken language understanding corpora demonstrate that discrete units achievereasonably good results in almost all the settings. We intend to release ourconfigurations and trained models to foster future research efforts.</description><author>Xuankai Chang, Brian Yan, Kwanghee Choi, Jeeweon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang</author><pubDate>Wed, 27 Sep 2023 18:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15800v1</guid></item><item><title>Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction</title><link>http://arxiv.org/abs/2309.15798v1</link><description>Single-step retrosynthesis is a crucial task in organic chemistry and drugdesign, requiring the identification of required reactants to synthesize aspecific compound. with the advent of computer-aided synthesis planning, thereis growing interest in using machine-learning techniques to facilitate theprocess. Existing template-free machine learning-based models typically utilizetransformer structures and represent molecules as ID sequences. However, thesemethods often face challenges in fully leveraging the extensive topologicalinformation of the molecule and aligning atoms between the production andreactants, leading to results that are not as competitive as those ofsemi-template models. Our proposed method, Node-Aligned Graph-to-Graph (NAG2G),also serves as a transformer-based template-free model but utilizes 2Dmolecular graphs and 3D conformation information. Furthermore, our approachsimplifies the incorporation of production-reactant atom mapping alignment byleveraging node alignment to determine a specific order for node generation andgenerating molecular graphs in an auto-regressive manner node-by-node. Thismethod ensures that the node generation order coincides with the node order inthe input graph, overcoming the difficulty of determining a specific nodegeneration order in an auto-regressive manner. Our extensive benchmarkingresults demonstrate that the proposed NAG2G can outperform the previousstate-of-the-art baselines in various metrics.</description><author>Lin Yao, Zhen Wang, Wentao Guo, Shang Xiang, Wentan Liu, Guolin Ke</author><pubDate>Wed, 27 Sep 2023 18:16:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15798v1</guid></item><item><title>A Quantum-Classical Hybrid Block-Matching Algorithm in Noisy Environment using Dissimilarity Measure</title><link>http://arxiv.org/abs/2309.15792v1</link><description>A block-matching algorithm finds a group of similar image patches inside asearch area. Similarity/dissimilarity measures can help to solve this problem.In different practical applications, finding groups of similar image blockswithin an ample search area is often necessary, such as video compression,image clustering, vector quantization, and nonlocal noise reduction. In thiswork, classical image processing is performed using Gaussian noise and imagesize reduction with a fit of a Low-Pass Filter or Domain Transform. Ahierarchical search technique is implemented to encode the images by phaseoperator. Using phase image coding with the quantum Fourier transform and theSwap test, we propose a dissimilarity measure. Results were obtained withperfect and noisy simulations and in the case of the Swap test with the IBM andIonq quantum devices.</description><author>M. Martínez-Felipe, J. Montiel-Pérez, V. Onofre-González, A. Maldonado-Romo, Ricky Young</author><pubDate>Wed, 27 Sep 2023 18:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15792v1</guid></item><item><title>Large Language Model Routing with Benchmark Datasets</title><link>http://arxiv.org/abs/2309.15789v1</link><description>There is a rapidly growing number of open-source Large Language Models (LLMs)and benchmark datasets to compare them. While some models dominate thesebenchmarks, no single model typically achieves the best accuracy in all tasksand use cases. In this work, we address the challenge of selecting the best LLMout of a collection of models for new tasks. We propose a new formulation forthe problem, in which benchmark datasets are repurposed to learn a "router"model for this LLM selection, and we show that this problem can be reduced to acollection of binary classification tasks. We demonstrate the utility andlimitations of learning model routers from various benchmark datasets, where weconsistently improve performance upon using any single model for all tasks.</description><author>Tal Shnitzer, Anthony Ou, Mírian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, Mikhail Yurochkin</author><pubDate>Wed, 27 Sep 2023 18:08:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15789v1</guid></item><item><title>Partial Transport for Point-Cloud Registration</title><link>http://arxiv.org/abs/2309.15787v1</link><description>Point cloud registration plays a crucial role in various fields, includingrobotics, computer graphics, and medical imaging. This process involvesdetermining spatial relationships between different sets of points, typicallywithin a 3D space. In real-world scenarios, complexities arise from non-rigidmovements and partial visibility, such as occlusions or sensor noise, makingnon-rigid registration a challenging problem. Classic non-rigid registrationmethods are often computationally demanding, suffer from unstable performance,and, importantly, have limited theoretical guarantees. The optimal transportproblem and its unbalanced variations (e.g., the optimal partial transportproblem) have emerged as powerful tools for point-cloud registration,establishing a strong benchmark in this field. These methods view point cloudsas empirical measures and provide a mathematically rigorous way to quantify the`correspondence' between (the transformed) source and target points. In thispaper, we approach the point-cloud registration problem through the lens ofoptimal transport theory and first propose a comprehensive set of non-rigidregistration methods based on the optimal partial transportation problem.Subsequently, leveraging the emerging work on efficient solutions to theone-dimensional optimal partial transport problem, we extend our proposedalgorithms via slicing to gain significant computational efficiency, resultingin fast and robust non-rigid registration algorithms. We demonstrate theeffectiveness of our proposed methods and compare them against baselines onvarious 3D and 2D non-rigid registration problems where the source and targetpoint clouds are corrupted by random noise.</description><author>Yikun Bai, Huy Tran, Steven B. Damelin, Soheil Kolouri</author><pubDate>Wed, 27 Sep 2023 18:04:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15787v1</guid></item><item><title>One For All: Video Conversation is Feasible Without Video Instruction Tuning</title><link>http://arxiv.org/abs/2309.15785v1</link><description>The recent progress in Large Language Models (LLM) has spurred variousadvancements in image-language conversation agents, while how to build aproficient video-based dialogue system is still under exploration. Consideringthe extensive scale of LLM and visual backbone, minimal GPU memory is left forfacilitating effective temporal modeling, which is crucial for comprehendingand providing feedback on videos. To this end, we propose Branching TemporalAdapter (BT-Adapter), a novel method for extending image-language pretrainedmodels into the video domain. Specifically, BT-Adapter serves as a plug-and-usetemporal modeling branch alongside the pretrained visual encoder, which istuned while keeping the backbone frozen. Just pretrained once, BT-Adapter canbe seamlessly integrated into all image conversation models using this versionof CLIP, enabling video conversations without the need for video instructions.Besides, we develop a unique asymmetric token masking strategy inside thebranch with tailor-made training tasks for BT-Adapter, facilitating fasterconvergence and better results. Thanks to BT-Adapter, we are able to empowerexisting multimodal dialogue models with strong video understandingcapabilities without incurring excessive GPU costs. Without bells and whistles,BT-Adapter achieves (1) state-of-the-art zero-shot results on various videotasks using thousands of fewer GPU hours. (2) better performance than currentvideo chatbots without any video instruction tuning. (3) state-of-the-artresults of video chatting using video instruction tuning, outperformingprevious SOTAs by a large margin.</description><author>Ruyang Liu, Chen Li, Yixiao Ge, Ying Shan, Thomas H. Li, Ge Li</author><pubDate>Wed, 27 Sep 2023 17:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15785v1</guid></item><item><title>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</title><link>http://arxiv.org/abs/2309.15112v2</link><description>We propose InternLM-XComposer, a vision-language large model that enablesadvanced image-text comprehension and composition. The innovative nature of ourmodel is highlighted by three appealing properties: 1) Interleaved Text-ImageComposition: InternLM-XComposer can effortlessly generate coherent andcontextual articles that seamlessly integrate images, providing a more engagingand immersive reading experience. Simply provide a title, and our system willgenerate the corresponding manuscript. It can intelligently identify the areasin the text where images would enhance the content and automatically insert themost appropriate visual candidates. 2) Comprehension with Rich MultilingualKnowledge: The text-image comprehension is empowered by training on extensivemulti-modal multilingual concepts with carefully crafted strategies, resultingin a deep understanding of visual content. 3) State-of-the-art Performance: Ourmodel consistently achieves state-of-the-art results across various mainstreambenchmarks for vision-language foundational models, including MME Benchmark,MMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).Collectively, InternLM-XComposer seamlessly blends advanced text-imagecomprehension and composition, revolutionizing vision-language interaction andoffering new insights and opportunities. The InternLM-XComposer models with 7Bparameters are publicly available athttps://github.com/InternLM/InternLM-XComposer.</description><author>Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang</author><pubDate>Wed, 27 Sep 2023 17:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15112v2</guid></item><item><title>Joint-YODNet: A Light-weight Object Detector for UAVs to Achieve Above 100fps</title><link>http://arxiv.org/abs/2309.15782v1</link><description>Small object detection via UAV (Unmanned Aerial Vehicle) images captured fromdrones and radar is a complex task with several formidable challenges. Thisdomain encompasses numerous complexities that impede the accurate detection andlocalization of small objects. To address these challenges, we propose a novelmethod called JointYODNet for UAVs to detect small objects, leveraging a jointloss function specifically designed for this task. Our method revolves aroundthe development of a joint loss function tailored to enhance the detectionperformance of small objects. Through extensive experimentation on a diversedataset of UAV images captured under varying environmental conditions, weevaluated different variations of the loss function and determined the mosteffective formulation. The results demonstrate that our proposed joint lossfunction outperforms existing methods in accurately localizing small objects.Specifically, our method achieves a recall of 0.971, and a F1Score of 0.975,surpassing state-of-the-art techniques. Additionally, our method achieves amAP@.5(%) of 98.6, indicating its robustness in detecting small objects acrossvarying scales</description><author>Vipin Gautam, Shitala Prasad, Sharad Sinha</author><pubDate>Wed, 27 Sep 2023 17:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15782v1</guid></item><item><title>AaP-ReID: Improved Attention-Aware Person Re-identification</title><link>http://arxiv.org/abs/2309.15780v1</link><description>Person re-identification (ReID) is a well-known problem in the field ofcomputer vision. The primary objective is to identify a specific individualwithin a gallery of images. However, this task is challenging due to variousfactors, such as pose variations, illumination changes, obstructions, and thepresence ofconfusing backgrounds. Existing ReID methods often fail to capturediscriminative features (e.g., head, shoes, backpacks) and instead captureirrelevant features when the target is occluded. Motivated by the success ofpart-based and attention-based ReID methods, we improve AlignedReID++ andpresent AaP-ReID, a more effective method for person ReID that incorporateschannel-wise attention into a ResNet-based architecture. Our methodincorporates the Channel-Wise Attention Bottleneck (CWAbottleneck) block andcan learn discriminating features by dynamically adjusting the importanceofeach channel in the feature maps. We evaluated Aap-ReID on three benchmarkdatasets: Market-1501, DukeMTMC-reID, and CUHK03. When compared withstate-of-the-art person ReID methods, we achieve competitive results withrank-1 accuracies of 95.6% on Market-1501, 90.6% on DukeMTMC-reID, and 82.4% onCUHK03.</description><author>Vipin Gautam, Shitala Prasad, Sharad Sinha</author><pubDate>Wed, 27 Sep 2023 17:54:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15780v1</guid></item><item><title>Question answering using deep learning in low resource Indian language Marathi</title><link>http://arxiv.org/abs/2309.15779v1</link><description>Precise answers are extracted from a text for a given input question in aquestion answering system. Marathi question answering system is created inrecent studies by using ontology, rule base and machine learning basedapproaches. Recently transformer models and transfer learning approaches areused to solve question answering challenges. In this paper we investigatedifferent transformer models for creating a reading comprehension-based Marathiquestion answering system. We have experimented on different pretrained Marathilanguage multilingual and monolingual models like Multilingual Representationsfor Indian Languages (MuRIL), MahaBERT, Indic Bidirectional EncoderRepresentations from Transformers (IndicBERT) and fine-tuned it on a Marathireading comprehension-based data set. We got the best accuracy in a MuRILmultilingual model with an EM score of 0.64 and F1 score of 0.74 by fine tuningthe model on the Marathi dataset.</description><author>Dhiraj Amin, Sharvari Govilkar, Sagar Kulkarni</author><pubDate>Wed, 27 Sep 2023 17:53:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15779v1</guid></item><item><title>MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods</title><link>http://arxiv.org/abs/2309.10966v3</link><description>Recent research in decoding methods for Natural Language Generation (NLG)tasks has shown that MAP decoding is not optimal, because model probabilitiesdo not always align with human preferences. Stronger decoding methods,including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)decoding, have since been proposed to mitigate the model-perplexity-vs-qualitymismatch. While these decoding methods achieve state-of-the-art performance,they are prohibitively expensive to compute. In this work, we propose MBRfinetuning and QE finetuning which distill the quality gains from thesedecoding methods at training time, while using an efficient decoding algorithmat inference time. Using the canonical NLG task of Neural Machine Translation(NMT), we show that even with self-training, these finetuning methodssignificantly outperform the base model. Moreover, when using an external LLMas a teacher model, these finetuning methods outperform finetuning onhuman-generated references. These findings suggest new ways to leveragemonolingual data to achieve improvements in model quality that are on par with,or even exceed, improvements from human-curated data, while maintaining maximumefficiency during decoding.</description><author>Mara Finkelstein, Markus Freitag</author><pubDate>Wed, 27 Sep 2023 17:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10966v3</guid></item><item><title>ssVERDICT: Self-Supervised VERDICT-MRI for Enhanced Prostate Tumour Characterisation</title><link>http://arxiv.org/abs/2309.06268v2</link><description>Purpose: Demonstrating and assessing self-supervised machine learning fittingof the VERDICT (Vascular, Extracellular and Restricted DIffusion for Cytometryin Tumours) model for prostate. Methods: We derive a self-supervised neuralnetwork for fitting VERDICT (ssVERDICT) that estimates parameter maps withouttraining data. We compare the performance of ssVERDICT to two establishedbaseline methods for fitting diffusion MRI models: conventional nonlinear leastsquares (NLLS) and supervised deep learning. We do this quantitatively onsimulated data, by comparing the Pearson's correlation coefficient,mean-squared error (MSE), bias, and variance with respect to the simulatedground truth. We also calculate in vivo parameter maps on a cohort of 20prostate cancer patients and compare the methods' performance in discriminatingbenign from cancerous tissue via Wilcoxon's signed-rank test. Results: Insimulations, ssVERDICT outperforms the baseline methods (NLLS and supervisedDL) in estimating all the parameters from the VERDICT prostate model in termsof Pearson's correlation coefficient, bias, and MSE. In vivo, ssVERDICT showsstronger lesion conspicuity across all parameter maps, and improvesdiscrimination between benign and cancerous tissue over the baseline methods.Conclusion: ssVERDICT significantly outperforms state-of-the-art methods forVERDICT model fitting, and shows for the first time, fitting of a complexthree-compartment biophysical model with machine learning without therequirement of explicit training labels.</description><author>Snigdha Sen, Saurabh Singh, Hayley Pye, Caroline M. Moore, Hayley Whitaker, Shonit Punwani, David Atkinson, Eleftheria Panagiotaki, Paddy J. Slator</author><pubDate>Wed, 27 Sep 2023 17:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06268v2</guid></item><item><title>Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features</title><link>http://arxiv.org/abs/2307.07683v2</link><description>Synthetic-voice cloning technologies have seen significant advances in recentyears, giving rise to a range of potential harms. From small- and large-scalefinancial fraud to disinformation campaigns, the need for reliable methods todifferentiate real and synthesized voices is imperative. We describe threetechniques for differentiating a real from a cloned voice designed toimpersonate a specific person. These three approaches differ in their featureextraction stage with low-dimensional perceptual features offering highinterpretability but lower accuracy, to generic spectral features, andend-to-end learned features offering less interpretability but higher accuracy.We show the efficacy of these approaches when trained on a single speaker'svoice and when trained on multiple voices. The learned features consistentlyyield an equal error rate between 0% and 4%, and are reasonably robust toadversarial laundering.</description><author>Sarah Barrington, Romit Barua, Gautham Koorma, Hany Farid</author><pubDate>Wed, 27 Sep 2023 17:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07683v2</guid></item><item><title>Learning the Efficient Frontier</title><link>http://arxiv.org/abs/2309.15775v1</link><description>The efficient frontier (EF) is a fundamental resource allocation problemwhere one has to find an optimal portfolio maximizing a reward at a given levelof risk. This optimal solution is traditionally found by solving a convexoptimization problem. In this paper, we introduce NeuralEF: a fast neuralapproximation framework that robustly forecasts the result of the EF convexoptimization problem with respect to heterogeneous linear constraints andvariable number of optimization inputs. By reformulating an optimizationproblem as a sequence to sequence problem, we show that NeuralEF is a viablesolution to accelerate large-scale simulation while handling discontinuousbehavior.</description><author>Philippe Chatigny, Ivan Sergienko, Ryan Ferguson, Jordan Weir, Maxime Bergeron</author><pubDate>Wed, 27 Sep 2023 17:49:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15775v1</guid></item><item><title>Replay Buffer with Local Forgetting for Adapting to Local Environment Changes in Deep Model-Based Reinforcement Learning</title><link>http://arxiv.org/abs/2303.08690v2</link><description>One of the key behavioral characteristics used in neuroscience to determinewhether the subject of study -- be it a rodent or a human -- exhibitsmodel-based learning is effective adaptation to local changes in theenvironment, a particular form of adaptivity that is the focus of this work. Inreinforcement learning, however, recent work has shown that modern deepmodel-based reinforcement-learning (MBRL) methods adapt poorly to localenvironment changes. An explanation for this mismatch is that MBRL methods aretypically designed with sample-efficiency on a single task in mind and therequirements for effective adaptation are substantially higher, both in termsof the learned world model and the planning routine. One particularlychallenging requirement is that the learned world model has to be sufficientlyaccurate throughout relevant parts of the state-space. This is challenging fordeep-learning-based world models due to catastrophic forgetting. And while areplay buffer can mitigate the effects of catastrophic forgetting, thetraditional first-in-first-out replay buffer precludes effective adaptation dueto maintaining stale data. In this work, we show that a conceptually simplevariation of this traditional replay buffer is able to overcome thislimitation. By removing only samples from the buffer from the localneighbourhood of the newly observed samples, deep world models can be builtthat maintain their accuracy across the state-space, while also being able toeffectively adapt to local changes in the reward function. We demonstrate thisby applying our replay-buffer variation to a deep version of the classical Dynamethod, as well as to recent methods such as PlaNet and DreamerV2,demonstrating that deep model-based methods can adapt effectively as well tolocal changes in the environment.</description><author>Ali Rahimi-Kalahroudi, Janarthanan Rajendran, Ida Momennejad, Harm van Seijen, Sarath Chandar</author><pubDate>Wed, 27 Sep 2023 17:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08690v2</guid></item><item><title>Novel and flexible parameter estimation methods for data-consistent inversion in mechanistic modeling</title><link>http://arxiv.org/abs/2009.08267v3</link><description>Predictions for physical systems often rely upon knowledge acquired fromensembles of entities, e.g., ensembles of cells in biological sciences. Forqualitative and quantitative analysis, these ensembles are simulated withparametric families of mechanistic models (MM). Two classes of methodologies,based on Bayesian inference and Population of Models, currently prevail inparameter estimation for physical systems. However, in Bayesian analysis,uninformative priors for MM parameters introduce undesirable bias. Here, wepropose how to infer parameters within the framework of stochastic inverseproblems (SIP), also termed data-consistent inversion, wherein the priortargets only uncertainties that arise due to MM non-invertibility. Todemonstrate, we introduce new methods to solve SIP based on rejection sampling,Markov chain Monte Carlo, and generative adversarial networks (GANs). Inaddition, to overcome limitations of SIP, we reformulate SIP based onconstrained optimization and present a novel GAN to solve the constrainedoptimization problem.</description><author>Timothy Rumbell, Jaimit Parikh, James Kozloski, Viatcheslav Gurev</author><pubDate>Wed, 27 Sep 2023 17:45:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2009.08267v3</guid></item><item><title>Importance-Weighted Offline Learning Done Right</title><link>http://arxiv.org/abs/2309.15771v1</link><description>We study the problem of offline policy optimization in stochastic contextualbandit problems, where the goal is to learn a near-optimal policy based on adataset of decision data collected by a suboptimal behavior policy. Rather thanmaking any structural assumptions on the reward function, we assume access to agiven policy class and aim to compete with the best comparator policy withinthis class. In this setting, a standard approach is to computeimportance-weighted estimators of the value of each policy, and select a policythat minimizes the estimated value up to a "pessimistic" adjustment subtractedfrom the estimates to reduce their random fluctuations. In this paper, we showthat a simple alternative approach based on the "implicit exploration"estimator of \citet{Neu2015} yields performance guarantees that are superior innearly all possible terms to all previous results. Most notably, we remove anextremely restrictive "uniform coverage" assumption made in all previous works.These improvements are made possible by the observation that the upper andlower tails importance-weighted estimators behave very differently from eachother, and their careful control can massively improve on previous results thatwere all based on symmetric two-sided concentration inequalities. We alsoextend our results to infinite policy classes in a PAC-Bayesian fashion, andshowcase the robustness of our algorithm to the choice of hyper-parameters bymeans of numerical simulations.</description><author>Germano Gabbianelli, Gergely Neu, Matteo Papini</author><pubDate>Wed, 27 Sep 2023 17:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15771v1</guid></item><item><title>Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator</title><link>http://arxiv.org/abs/2309.15769v1</link><description>Deep learning research has uncovered the phenomenon of benign overfitting forover-parameterized statistical models, which has drawn significant theoreticalinterest in recent years. Given its simplicity and practicality, the ordinaryleast squares (OLS) interpolator has become essential to gain foundationalinsights into this phenomenon. While properties of OLS are well established inclassical settings, its behavior in high-dimensional settings is less explored(unlike for ridge or lasso regression) though significant progress has beenmade of late. We contribute to this growing literature by providing fundamentalalgebraic and statistical results for the minimum $\ell_2$-norm OLSinterpolator. In particular, we provide high-dimensional algebraic equivalentsof (i) the leave-$k$-out residual formula, (ii) Cochran's formula, and (iii)the Frisch-Waugh-Lovell theorem. These results aid in understanding the OLSinterpolator's ability to generalize and have substantive implications forcausal inference. Additionally, under the Gauss-Markov model, we presentstatistical results such as a high-dimensional extension of the Gauss-Markovtheorem and an analysis of variance estimation under homoskedastic errors. Tosubstantiate our theoretical contributions, we conduct simulation studies thatfurther explore the stochastic properties of the OLS interpolator.</description><author>Dennis Shen, Dogyoon Song, Peng Ding, Jasjeet S. Sekhon</author><pubDate>Wed, 27 Sep 2023 17:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15769v1</guid></item><item><title>AI in Software Engineering: Case Studies and Prospects</title><link>http://arxiv.org/abs/2309.15768v1</link><description>Artificial intelligence (AI) and software engineering (SE) are two importantareas in computer science. In recent years, researchers are trying to apply AItechniques in various stages of software development to improve the overallquality of software products. Moreover, there are also some researchers focuson the intersection between SE and AI. In fact, the relationship between SE andAI is very weak; however, methods and techniques in one area have been adoptedin another area. More and more software products are capable of performingintelligent behaviour like human beings. In this paper, two cases studies whichare IBM Watson and Google AlphaGo that use different AI techniques in solvingreal world challenging problems have been analysed, evaluated and compared.Based on the analysis of both case studies, using AI techniques such as deeplearning and machine learning in software systems contributes to intelligentsystems. Watson adopts 'decision making support' strategy to help human makedecisions; whereas AlphaGo uses 'self-decision making' to choose operationsthat contribute to the best outcome. In addition, Watson learns from man-maderesources such as paper; AlphaGo, on the other hand, learns from massive onlineresources such as photos. AlphaGo uses neural networks and reinforcementlearning to mimic human brain, which might be very useful in medical researchfor diagnosis and treatment. However, there is still a long way to go if wewant to reproduce human brain in machine and view computers as thinkers,because human brain and machines are intrinsically different. It would be morepromising to see whether computers and software systems will become more andmore intelligent to help with real world challenging problems that human beingscannot do.</description><author>Lei Wang</author><pubDate>Wed, 27 Sep 2023 17:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15768v1</guid></item><item><title>A Robust and Constrained Multi-Agent Reinforcement Learning Electric Vehicle Rebalancing Method in AMoD Systems</title><link>http://arxiv.org/abs/2209.08230v2</link><description>Electric vehicles (EVs) play critical roles in autonomous mobility-on-demand(AMoD) systems, but their unique charging patterns increase the modeluncertainties in AMoD systems (e.g. state transition probability). Since thereusually exists a mismatch between the training and test/true environments,incorporating model uncertainty into system design is of critical importance inreal-world applications. However, model uncertainties have not been consideredexplicitly in EV AMoD system rebalancing by existing literature yet, and thecoexistence of model uncertainties and constraints that the decision shouldsatisfy makes the problem even more challenging. In this work, we design arobust and constrained multi-agent reinforcement learning (MARL) framework withstate transition kernel uncertainty for EV AMoD systems. We then propose arobust and constrained MARL algorithm (ROCOMA) with robust natural policygradients (RNPG) that trains a robust EV rebalancing policy to balance thesupply-demand ratio and the charging utilization rate across the city undermodel uncertainty. Experiments show that the ROCOMA can learn an effective androbust rebalancing policy. It outperforms non-robust MARL methods in thepresence of model uncertainties. It increases the system fairness by 19.6% anddecreases the rebalancing costs by 75.8%.</description><author>Sihong He, Yue Wang, Shuo Han, Shaofeng Zou, Fei Miao</author><pubDate>Wed, 27 Sep 2023 17:25:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08230v2</guid></item><item><title>Studying Drowsiness Detection Performance while Driving through Scalable Machine Learning Models using Electroencephalography</title><link>http://arxiv.org/abs/2209.04048v2</link><description>- Background / Introduction: Driver drowsiness is a significant concern andone of the leading causes of traffic accidents. Advances in cognitiveneuroscience and computer science have enabled the detection of drivers'drowsiness using Brain-Computer Interfaces (BCIs) and Machine Learning (ML).However, the literature lacks a comprehensive evaluation of drowsinessdetection performance using a heterogeneous set of ML algorithms, and it isnecessary to study the performance of scalable ML models suitable for groups ofsubjects. - Methods: To address these limitations, this work presents anintelligent framework employing BCIs and features based onelectroencephalography for detecting drowsiness in driving scenarios. TheSEED-VIG dataset is used to evaluate the best-performing models for individualsubjects and groups. - Results: Results show that Random Forest (RF)outperformed other models used in the literature, such as Support VectorMachine (SVM), with a 78% f1-score for individual models. Regarding scalablemodels, RF reached a 79% f1-score, demonstrating the effectiveness of theseapproaches. This publication highlights the relevance of exploring a diverseset of ML algorithms and scalable approaches suitable for groups of subjects toimprove drowsiness detection systems and ultimately reduce the number ofaccidents caused by driver fatigue. - Conclusions: The lessons learned fromthis study show that not only SVM but also other models not sufficientlyexplored in the literature are relevant for drowsiness detection. Additionally,scalable approaches are effective in detecting drowsiness, even when newsubjects are evaluated. Thus, the proposed framework presents a novel approachfor detecting drowsiness in driving scenarios using BCIs and ML.</description><author>José Manuel Hidalgo Rogel, Enrique Tomás Martínez Beltrán, Mario Quiles Pérez, Sergio López Bernal, Gregorio Martínez Pérez, Alberto Huertas Celdrán</author><pubDate>Wed, 27 Sep 2023 17:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.04048v2</guid></item><item><title>Rapid Network Adaptation: Learning to Adapt Neural Networks Using Test-Time Feedback</title><link>http://arxiv.org/abs/2309.15762v1</link><description>We propose a method for adapting neural networks to distribution shifts attest-time. In contrast to training-time robustness mechanisms that attempt toanticipate and counter the shift, we create a closed-loop system and make useof a test-time feedback signal to adapt a network on the fly. We show that thisloop can be effectively implemented using a learning-based function, whichrealizes an amortized optimizer for the network. This leads to an adaptationmethod, named Rapid Network Adaptation (RNA), that is notably more flexible andorders of magnitude faster than the baselines. Through a broad set ofexperiments using various adaptation signals and target tasks, we study theefficiency and flexibility of this method. We perform the evaluations usingvarious datasets (Taskonomy, Replica, ScanNet, Hypersim, COCO, ImageNet), tasks(depth, optical flow, semantic segmentation, classification), and distributionshifts (Cross-datasets, 2D and 3D Common Corruptions) with promising results.We end with a discussion on general formulations for handling distributionshifts and our observations from comparing with similar approaches from otherdomains.</description><author>Teresa Yeo, Oğuzhan Fatih Kar, Zahra Sodagar, Amir Zamir</author><pubDate>Wed, 27 Sep 2023 17:20:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15762v1</guid></item><item><title>RepViT: Revisiting Mobile CNN From ViT Perspective</title><link>http://arxiv.org/abs/2307.09283v5</link><description>Recently, lightweight Vision Transformers (ViTs) demonstrate superiorperformance and lower latency compared with lightweight Convolutional NeuralNetworks (CNNs) on resource-constrained mobile devices. This improvement isusually attributed to the multi-head self-attention module, which enables themodel to learn global representations. However, the architectural disparitiesbetween lightweight ViTs and lightweight CNNs have not been adequatelyexamined. In this study, we revisit the efficient design of lightweight CNNsand emphasize their potential for mobile devices. We incrementally enhance themobile-friendliness of a standard lightweight CNN, specifically MobileNetV3, byintegrating the efficient architectural choices of lightweight ViTs. This endsup with a new family of pure lightweight CNNs, namely RepViT. Extensiveexperiments show that RepViT outperforms existing state-of-the-art lightweightViTs and exhibits favorable latency in various vision tasks. On ImageNet,RepViT achieves over 80\% top-1 accuracy with 1ms latency on an iPhone 12,which is the first time for a lightweight model, to the best of our knowledge.Our largest model, RepViT-M2.3, obtains 83.7\% accuracy with only 2.3mslatency. The code and trained models are available at\url{https://github.com/jameslahm/RepViT}.</description><author>Ao Wang, Hui Chen, Zijia Lin, Hengjun Pu, Guiguang Ding</author><pubDate>Wed, 27 Sep 2023 17:15:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09283v5</guid></item><item><title>Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe</title><link>http://arxiv.org/abs/2209.05324v4</link><description>Learning powerful representations in bird's-eye-view (BEV) for perceptiontasks is trending and drawing extensive attention both from industry andacademia. Conventional approaches for most autonomous driving algorithmsperform detection, segmentation, tracking, etc., in a front or perspectiveview. As sensor configurations get more complex, integrating multi-sourceinformation from different sensors and representing features in a unified viewcome of vital importance. BEV perception inherits several advantages, asrepresenting surrounding scenes in BEV is intuitive and fusion-friendly; andrepresenting objects in BEV is most desirable for subsequent modules as inplanning and/or control. The core problems for BEV perception lie in (a) how toreconstruct the lost 3D information via view transformation from perspectiveview to BEV; (b) how to acquire ground truth annotations in BEV grid; (c) howto formulate the pipeline to incorporate features from different sources andviews; and (d) how to adapt and generalize algorithms as sensor configurationsvary across different scenarios. In this survey, we review the most recentworks on BEV perception and provide an in-depth analysis of differentsolutions. Moreover, several systematic designs of BEV approach from theindustry are depicted as well. Furthermore, we introduce a full suite ofpractical guidebook to improve the performance of BEV perception tasks,including camera, LiDAR and fusion inputs. At last, we point out the futureresearch directions in this area. We hope this report will shed some light onthe community and encourage more research effort on BEV perception. We keep anactive repository to collect the most recent work and provide a toolbox for bagof tricks at https://github.com/OpenDriveLab/Birds-eye-view-Perception</description><author>Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Jia Zeng, Zhiqi Li, Jiazhi Yang, Hanming Deng, Hao Tian, Enze Xie, Jiangwei Xie, Li Chen, Tianyu Li, Yang Li, Yulu Gao, Xiaosong Jia, Si Liu, Jianping Shi, Dahua Lin, Yu Qiao</author><pubDate>Wed, 27 Sep 2023 17:15:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05324v4</guid></item><item><title>Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data</title><link>http://arxiv.org/abs/2309.15757v1</link><description>In the domain of semi-supervised learning, the current approachesinsufficiently exploit the potential of considering inter-instancerelationships among (un)labeled data. In this work, we address this limitationby providing an approach for inferring latent graphs that capture the intrinsicdata relationships. By leveraging graph-based representations, our approachfacilitates the seamless propagation of information throughout the graph,enabling the effective incorporation of global and local knowledge. Throughevaluations on biomedical tabular datasets, we compare the capabilities of ourapproach to other contemporary methods. Our work demonstrates the significanceof inter-instance relationship discovery as practical means for constructingrobust latent graphs to enhance semi-supervised learning techniques. Our methodachieves state-of-the-art results on three biomedical datasets.</description><author>Boshko Koloski, Blaž Škrlj, Senja Pollak ınst{1}, Nada Lavrač</author><pubDate>Wed, 27 Sep 2023 17:13:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15757v1</guid></item><item><title>CAIT: Triple-Win Compression towards High Accuracy, Fast Inference, and Favorable Transferability For ViTs</title><link>http://arxiv.org/abs/2309.15755v1</link><description>Vision Transformers (ViTs) have emerged as state-of-the-art models forvarious vision tasks recently. However, their heavy computation costs remaindaunting for resource-limited devices. Consequently, researchers have dedicatedthemselves to compressing redundant information in ViTs for acceleration.However, they generally sparsely drop redundant image tokens by token pruningor brutally remove channels by channel pruning, leading to a sub-optimalbalance between model performance and inference speed. They are alsodisadvantageous in transferring compressed models to downstream vision tasksthat require the spatial structure of images, such as semantic segmentation. Totackle these issues, we propose a joint compression method for ViTs that offersboth high accuracy and fast inference speed, while also maintaining favorabletransferability to downstream tasks (CAIT). Specifically, we introduce anasymmetric token merging (ATME) strategy to effectively integrate neighboringtokens. It can successfully compress redundant token information whilepreserving the spatial structure of images. We further employ a consistentdynamic channel pruning (CDCP) strategy to dynamically prune unimportantchannels in ViTs. Thanks to CDCP, insignificant channels in multi-headself-attention modules of ViTs can be pruned uniformly, greatly enhancing themodel compression. Extensive experiments on benchmark datasets demonstrate thatour proposed method can achieve state-of-the-art performance across variousViTs. For example, our pruned DeiT-Tiny and DeiT-Small achieve speedups of1.7$\times$ and 1.9$\times$, respectively, without accuracy drops on ImageNet.On the ADE20k segmentation dataset, our method can enjoy up to 1.31$\times$speedups with comparable mIoU. Our code will be publicly available.</description><author>Ao Wang, Hui Chen, Zijia Lin, Sicheng Zhao, Jungong Han, Guiguang Ding</author><pubDate>Wed, 27 Sep 2023 17:12:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15755v1</guid></item><item><title>InfraParis: A multi-modal and multi-task autonomous driving dataset</title><link>http://arxiv.org/abs/2309.15751v1</link><description>Current deep neural networks (DNNs) for autonomous driving computer visionare typically trained on specific datasets that only involve a single type ofdata and urban scenes. Consequently, these models struggle to handle newobjects, noise, nighttime conditions, and diverse scenarios, which is essentialfor safety-critical applications. Despite ongoing efforts to enhance theresilience of computer vision DNNs, progress has been sluggish, partly due tothe absence of benchmarks featuring multiple modalities. We introduce a noveland versatile dataset named InfraParis that supports multiple tasks acrossthree modalities: RGB, depth, and infrared. We assess various state-of-the-artbaseline techniques, encompassing models for the tasks of semanticsegmentation, object detection, and depth estimation.</description><author>Gianni Franchi, Marwane Hariat, Xuanlong Yu, Nacim Belkhir, Antoine Manzanera, David Filliat</author><pubDate>Wed, 27 Sep 2023 17:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15751v1</guid></item><item><title>Group-invariant tensor train networks for supervised learning</title><link>http://arxiv.org/abs/2206.15051v2</link><description>Invariance has recently proven to be a powerful inductive bias in machinelearning models. One such class of predictive or generative models are tensornetworks. We introduce a new numerical algorithm to construct a basis oftensors that are invariant under the action of normal matrix representations ofan arbitrary discrete group. This method can be up to several orders ofmagnitude faster than previous approaches. The group-invariant tensors are thencombined into a group-invariant tensor train network, which can be used as asupervised machine learning model. We applied this model to a protein bindingclassification problem, taking into account problem-specific invariances, andobtained prediction accuracy in line with state-of-the-art deep learningapproaches.</description><author>Brent Sprangers, Nick Vannieuwenhoven</author><pubDate>Wed, 27 Sep 2023 17:07:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.15051v2</guid></item><item><title>Automated CT Lung Cancer Screening Workflow using 3D Camera</title><link>http://arxiv.org/abs/2309.15750v1</link><description>Despite recent developments in CT planning that enabled automation in patientpositioning, time-consuming scout scans are still needed to compute doseprofile and ensure the patient is properly positioned. In this paper, wepresent a novel method which eliminates the need for scout scans in CT lungcancer screening by estimating patient scan range, isocenter, and WaterEquivalent Diameter (WED) from 3D camera images. We achieve this task bytraining an implicit generative model on over 60,000 CT scans and introduce anovel approach for updating the prediction using real-time scan data. Wedemonstrate the effectiveness of our method on a testing set of 110 pairs ofdepth data and CT scan, resulting in an average error of 5mm in estimating theisocenter, 13mm in determining the scan range, 10mm and 16mm in estimating theAP and lateral WED respectively. The relative WED error of our method is 4%,which is well within the International Electrotechnical Commission (IEC)acceptance criteria of 10%.</description><author>Brian Teixeira, Vivek Singh, Birgi Tamersoy, Andreas Prokein, Ankur Kapoor</author><pubDate>Wed, 27 Sep 2023 17:06:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15750v1</guid></item><item><title>Era Splitting -- Invariant Learning for Decision Trees</title><link>http://arxiv.org/abs/2309.14496v2</link><description>Real life machine learning problems exhibit distributional shifts in the datafrom one time to another or from on place to another. This behavior is beyondthe scope of the traditional empirical risk minimization paradigm, whichassumes i.i.d. distribution of data over time and across locations. Theemerging field of out-of-distribution (OOD) generalization addresses thisreality with new theory and algorithms which incorporate environmental, orera-wise information into the algorithms. So far, most research has beenfocused on linear models and/or neural networks. In this research we developtwo new splitting criteria for decision trees, which allow us to apply ideasfrom OOD generalization research to decision tree models, including randomforest and gradient-boosting decision trees. The new splitting criteria useera-wise information associated with each data point to allow tree-based modelsto find split points that are optimal across all disjoint eras in the data,instead of optimal over the entire data set pooled together, which is thedefault setting. We describe the new splitting criteria in detail and developunique experiments to showcase the benefits of these new criteria, whichimprove metrics in our experiments out-of-sample. The new criteria areincorporated into the a state-of-the-art gradient boosted decision tree modelin the Scikit-Learn code base, which is made freely available.</description><author>Timothy DeLise</author><pubDate>Wed, 27 Sep 2023 16:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14496v2</guid></item><item><title>Metalearning generalizable dynamics from trajectories</title><link>http://arxiv.org/abs/2301.00957v2</link><description>We present the interpretable meta neural ordinary differential equation(iMODE) method to rapidly learn generalizable (i.e., not parameter-specific)dynamics from trajectories of multiple dynamical systems that vary in theirphysical parameters. The iMODE method learns meta-knowledge, the functionalvariations of the force field of dynamical system instances without knowing thephysical parameters, by adopting a bi-level optimization framework: an outerlevel capturing the common force field form among studied dynamical systeminstances and an inner level adapting to individual system instances. A prioriphysical knowledge can be conveniently embedded in the neural networkarchitecture as inductive bias, such as conservative force field and Euclideansymmetry. With the learned meta-knowledge, iMODE can model an unseen systemwithin seconds, and inversely reveal knowledge on the physical parameters of asystem, or as a Neural Gauge to "measure" the physical parameters of an unseensystem with observed trajectories. We test the validity of the iMODE method onbistable, double pendulum, Van der Pol, Slinky, and reaction-diffusion systems.</description><author>Qiaofeng Li, Tianyi Wang, Vwani Roychowdhury, M. Khalid Jawed</author><pubDate>Wed, 27 Sep 2023 16:54:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00957v2</guid></item><item><title>Experience and Evidence are the eyes of an excellent summarizer! Towards Knowledge Infused Multi-modal Clinical Conversation Summarization</title><link>http://arxiv.org/abs/2309.15739v1</link><description>With the advancement of telemedicine, both researchers and medicalpractitioners are working hand-in-hand to develop various techniques toautomate various medical operations, such as diagnosis report generation. Inthis paper, we first present a multi-modal clinical conversation summarygeneration task that takes a clinician-patient interaction (both textual andvisual information) and generates a succinct synopsis of the conversation. Wepropose a knowledge-infused, multi-modal, multi-tasking medical domainidentification and clinical conversation summary generation(MM-CliConSummation) framework. It leverages an adapter to infuse knowledge andvisual features and unify the fused feature vector using a gated mechanism.Furthermore, we developed a multi-modal, multi-intent clinical conversationsummarization corpus annotated with intent, symptom, and summary. The extensiveset of experiments, both quantitatively and qualitatively, led to the followingfindings: (a) critical significance of visuals, (b) more precise and medicalentity preserving summary with additional knowledge infusion, and (c) acorrelation between medical department identification and clinical synopsisgeneration. Furthermore, the dataset and source code are available athttps://github.com/NLP-RL/MM-CliConSummation.</description><author>Abhisek Tiwari, Anisha Saha, Sriparna Saha, Pushpak Bhattacharyya, Minakshi Dhar</author><pubDate>Wed, 27 Sep 2023 16:49:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15739v1</guid></item><item><title>Provably Efficient Exploration in Constrained Reinforcement Learning:Posterior Sampling Is All You Need</title><link>http://arxiv.org/abs/2309.15737v1</link><description>We present a new algorithm based on posterior sampling for learning inconstrained Markov decision processes (CMDP) in the infinite-horizonundiscounted setting. The algorithm achieves near-optimal regret bounds whilebeing advantageous empirically compared to the existing algorithms. Our maintheoretical result is a Bayesian regret bound for each cost component of\tilde{O} (HS \sqrt{AT}) for any communicating CMDP with S states, A actions,and bound on the hitting time H. This regret bound matches the lower bound inorder of time horizon T and is the best-known regret bound for communicatingCMDPs in the infinite-horizon undiscounted setting. Empirical results showthat, despite its simplicity, our posterior sampling algorithm outperforms theexisting algorithms for constrained reinforcement learning.</description><author>Danil Provodin, Pratik Gajane, Mykola Pechenizkiy, Maurits Kaptein</author><pubDate>Wed, 27 Sep 2023 16:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15737v1</guid></item><item><title>InvKA: Gait Recognition via Invertible Koopman Autoencoder</title><link>http://arxiv.org/abs/2309.14764v2</link><description>Most current gait recognition methods suffer from poor interpretability andhigh computational cost. To improve interpretability, we investigate gaitfeatures in the embedding space based on Koopman operator theory. Thetransition matrix in this space captures complex kinematic features of gaitcycles, namely the Koopman operator. The diagonal elements of the operatormatrix can represent the overall motion trend, providing a physicallymeaningful descriptor. To reduce the computational cost of our algorithm, weuse a reversible autoencoder to reduce the model size and eliminateconvolutional layers to compress its depth, resulting in fewer floating-pointoperations. Experimental results on multiple datasets show that our methodreduces computational cost to 1% compared to state-of-the-art methods whileachieving competitive recognition accuracy 98% on non-occlusion datasets.</description><author>Fan Li, Dong Liang, Jing Lian, Qidong Liu, Hegui Zhu, Jizhao Liu</author><pubDate>Wed, 27 Sep 2023 16:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14764v2</guid></item><item><title>Synthetic Latent Fingerprint Generation Using Style Transfer</title><link>http://arxiv.org/abs/2309.15734v1</link><description>Limited data availability is a challenging problem in the latent fingerprintdomain. Synthetically generated fingerprints are vital for training data-hungryneural network-based algorithms. Conventional methods distort cleanfingerprints to generate synthetic latent fingerprints. We propose a simple andeffective approach using style transfer and image blending to synthesizerealistic latent fingerprints. Our evaluation criteria and experimentsdemonstrate that the generated synthetic latent fingerprints preserve theidentity information from the input contact-based fingerprints while possessingsimilar characteristics as real latent fingerprints. Additionally, we show thatthe generated fingerprints exhibit several qualities and styles, suggestingthat the proposed method can generate multiple samples from a singlefingerprint.</description><author>Amol S. Joshi, Ali Dabouei, Nasser Nasrabadi, Jeremy Dawson</author><pubDate>Wed, 27 Sep 2023 16:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15734v1</guid></item><item><title>Enhancing data efficiency in reinforcement learning: a novel imagination mechanism based on mesh information propagation</title><link>http://arxiv.org/abs/2309.14243v2</link><description>Reinforcement learning(RL) algorithms face the challenge of limited dataefficiency, particularly when dealing with high-dimensional state spaces andlarge-scale problems. Most of RL methods often rely solely on state transitioninformation within the same episode when updating the agent's Critic, which canlead to low data efficiency and sub-optimal training time consumption. Inspiredby human-like analogical reasoning abilities, we introduce a novel meshinformation propagation mechanism, termed the 'Imagination Mechanism (IM)',designed to significantly enhance the data efficiency of RL algorithms.Specifically, IM enables information generated by a single sample to beeffectively broadcasted to different states across episodes, instead of simplytransmitting in the same episode. This capability enhances the model'scomprehension of state interdependencies and facilitates more efficientlearning of limited sample information. To promote versatility, we extend theIM to function as a plug-and-play module that can be seamlessly and fluidlyintegrated into other widely adopted RL algorithms. Our experiments demonstratethat IM consistently boosts four mainstream SOTA RL algorithms, such as SAC,PPO, DDPG, and DQN, by a considerable margin, ultimately leading to superiorperformance than before across various tasks. For access to our code and data,please visit https://github.com/OuAzusaKou/imagination_mechanism</description><author>Zihang Wang, Maowei Jiang</author><pubDate>Wed, 27 Sep 2023 16:43:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14243v2</guid></item><item><title>Robust Distortion-free Watermarks for Language Models</title><link>http://arxiv.org/abs/2307.15593v2</link><description>We propose a methodology for planting watermarks in text from anautoregressive language model that are robust to perturbations without changingthe distribution over text up to a certain maximum generation budget. Wegenerate watermarked text by mapping a sequence of random numbers -- which wecompute using a randomized watermark key -- to a sample from the languagemodel. To detect watermarked text, any party who knows the key can align thetext to the random number sequence. We instantiate our watermark methodologywith two sampling schemes: inverse transform sampling and exponential minimumsampling. We apply these watermarks to three language models -- OPT-1.3B,LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical powerand robustness to various paraphrasing attacks. Notably, for both the OPT-1.3Band LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq0.01$) from $35$ tokens even after corrupting between $40$-$50\%$ of the tokensvia random edits (i.e., substitutions, insertions or deletions). For theAlpaca-7B model, we conduct a case study on the feasibility of watermarkingresponses to typical user instructions. Due to the lower entropy of theresponses, detection is more difficult: around $25\%$ of the responses -- whosemedian length is around $100$ tokens -- are detectable with $p \leq 0.01$, andthe watermark is also less robust to certain automated paraphrasing attacks weimplement.</description><author>Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, Percy Liang</author><pubDate>Wed, 27 Sep 2023 16:42:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15593v2</guid></item><item><title>Deep Learning-based Analysis of Basins of Attraction</title><link>http://arxiv.org/abs/2309.15732v1</link><description>This study showcases the effectiveness of convolutional neural networks(CNNs) in characterizing the complexity and unpredictability of basins ofattraction for diverse dynamical systems. This novel method is optimal forexploring different parameters of dynamical systems since the conventionalmethods are computationally expensive for characterizing multiple basins ofattraction. Additionally, our research includes a comparison of different CNNarchitectures for this task showing the superiority of our proposedcharacterization method over the conventional methods, even with obsoletearchitectures.</description><author>David Valle, Alexandre Wagemakers, Miguel A. F. Sanjuán</author><pubDate>Wed, 27 Sep 2023 16:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15732v1</guid></item><item><title>Surface Normal Clustering for Implicit Representation of Manhattan Scenes</title><link>http://arxiv.org/abs/2212.01331v4</link><description>Novel view synthesis and 3D modeling using implicit neural fieldrepresentation are shown to be very effective for calibrated multi-viewcameras. Such representations are known to benefit from additional geometricand semantic supervision. Most existing methods that exploit additionalsupervision require dense pixel-wise labels or localized scene priors. Thesemethods cannot benefit from high-level vague scene priors provided in terms ofscenes' descriptions. In this work, we aim to leverage the geometric prior ofManhattan scenes to improve the implicit neural radiance field representations.More precisely, we assume that only the knowledge of the indoor scene (underinvestigation) being Manhattan is known -- with no additional informationwhatsoever -- with an unknown Manhattan coordinate frame. Such high-level prioris used to self-supervise the surface normals derived explicitly in theimplicit neural fields. Our modeling allows us to cluster the derived normalsand exploit their orthogonality constraints for self-supervision. Ourexhaustive experiments on datasets of diverse indoor scenes demonstrate thesignificant benefit of the proposed method over the established baselines. Thesource code is available athttps://github.com/nikola3794/normal-clustering-nerf.</description><author>Nikola Popovic, Danda Pani Paudel, Luc Van Gool</author><pubDate>Wed, 27 Sep 2023 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01331v4</guid></item><item><title>Temporal graph models fail to capture global temporal dynamics</title><link>http://arxiv.org/abs/2309.15730v1</link><description>A recently released Temporal Graph Benchmark is analyzed in the context ofDynamic Link Property Prediction. We outline our observations and propose atrivial optimization-free baseline of "recently popular nodes" outperformingother methods on all medium and large-size datasets in the Temporal GraphBenchmark. We propose two measures based on Wasserstein distance which canquantify the strength of short-term and long-term global dynamics of datasets.By analyzing our unexpectedly strong baseline, we show how standard negativesampling evaluation can be unsuitable for datasets with strong temporaldynamics. We also show how simple negative-sampling can lead to modeldegeneration during training, resulting in impossible to rank, fully saturatedpredictions of temporal graph networks. We propose improved negative samplingschemes for both training and evaluation and prove their usefulness. We conducta comparison with a model trained non-contrastively without negative sampling.Our results provide a challenging baseline and indicate that temporal graphnetwork architectures need deep rethinking for usage in problems withsignificant global dynamics, such as social media, cryptocurrency markets ore-commerce. We open-source the code for baselines, measures and proposednegative sampling schemes.</description><author>Michał Daniluk, Jacek Dąbrowski</author><pubDate>Wed, 27 Sep 2023 16:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15730v1</guid></item><item><title>MindGPT: Interpreting What You See with Non-invasive Brain Recordings</title><link>http://arxiv.org/abs/2309.15729v1</link><description>Decoding of seen visual contents with non-invasive brain recordings hasimportant scientific and practical values. Efforts have been made to recoverthe seen images from brain signals. However, most existing approaches cannotfaithfully reflect the visual contents due to insufficient image quality orsemantic mismatches. Compared with reconstructing pixel-level visual images,speaking is a more efficient and effective way to explain visual information.Here we introduce a non-invasive neural decoder, termed as MindGPT, whichinterprets perceived visual stimuli into natural languages from fMRI signals.Specifically, our model builds upon a visually guided neural encoder with across-attention mechanism, which permits us to guide latent neuralrepresentations towards a desired language semantic direction in an end-to-endmanner by the collaborative use of the large language model GPT. By doing so,we found that the neural representations of the MindGPT are explainable, whichcan be used to evaluate the contributions of visual properties to languagesemantics. Our experiments show that the generated word sequences truthfullyrepresented the visual information (with essential details) conveyed in theseen stimuli. The results also suggested that with respect to language decodingtasks, the higher visual cortex (HVC) is more semantically informative than thelower visual cortex (LVC), and using only the HVC can recover most of thesemantic information. The code of the MindGPT model will be publicly availableat https://github.com/JxuanC/MindGPT.</description><author>Jiaxuan Chen, Yu Qi, Yueming Wang, Gang Pan</author><pubDate>Wed, 27 Sep 2023 16:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15729v1</guid></item><item><title>Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation</title><link>http://arxiv.org/abs/2309.15726v1</link><description>We develop a neural network architecture which, trained in an unsupervisedmanner as a denoising diffusion model, simultaneously learns to both generateand segment images. Learning is driven entirely by the denoising diffusionobjective, without any annotation or prior knowledge about regions duringtraining. A computational bottleneck, built into the neural architecture,encourages the denoising network to partition an input into regions, denoisethem in parallel, and combine the results. Our trained model generates bothsynthetic images and, by simple examination of its internal predictedpartitions, a semantic segmentation of those images. Without any finetuning, wedirectly apply our unsupervised model to the downstream task of segmenting realimages via noising and subsequently denoising them. Experiments demonstratethat our model achieves accurate unsupervised image segmentation andhigh-quality synthetic image generation across multiple datasets.</description><author>Xin Yuan, Michael Maire</author><pubDate>Wed, 27 Sep 2023 16:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15726v1</guid></item><item><title>Where Are We So Far? Understanding Data Storytelling Tools from the Perspective of Human-AI Collaboration</title><link>http://arxiv.org/abs/2309.15723v1</link><description>Data storytelling is powerful for communicating data insights, but itrequires diverse skills and considerable effort from human creators. Recentresearch has widely explored the potential for artificial intelligence (AI) tosupport and augment humans in data storytelling. However, there lacks asystematic review to understand data storytelling tools from the perspective ofhuman-AI collaboration, which hinders researchers from reflecting on theexisting collaborative tool designs that promote humans' and AI's advantagesand mitigate their shortcomings. This paper investigated existing tools with aframework from two perspectives: the stages in the storytelling workflow wherea tool serves, including analysis, planning, implementation, and communication,and the roles of humans and AI in each stage, such as creators, assistants,optimizers, and reviewers. Through our analysis, we recognize the commoncollaboration patterns in existing tools, summarize lessons learned from thesepatterns, and further illustrate research opportunities for human-AIcollaboration in data storytelling.</description><author>Haotian Li, Yun Wang, Huamin Qu</author><pubDate>Wed, 27 Sep 2023 16:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15723v1</guid></item><item><title>Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness</title><link>http://arxiv.org/abs/2308.03666v2</link><description>As researchers strive to narrow the gap between machine intelligence andhuman through the development of artificial intelligence technologies, it isimperative that we recognize the critical importance of trustworthiness inopen-world, which has become ubiquitous in all aspects of daily life foreveryone. However, several challenges may create a crisis of trust in currentartificial intelligence systems that need to be bridged: 1) Insufficientexplanation of predictive results; 2) Inadequate generalization for learningmodels; 3) Poor adaptability to uncertain environments. Consequently, weexplore a neural program to bridge trustworthiness and open-world learning,extending from single-modal to multi-modal scenarios for readers. 1) To enhancedesign-level interpretability, we first customize trustworthy networks withspecific physical meanings; 2) We then design environmental well-beingtask-interfaces via flexible learning regularizers for improving thegeneralization of trustworthy learning; 3) We propose to increase therobustness of trustworthy learning by integrating open-world recognition losseswith agent mechanisms. Eventually, we enhance various trustworthy propertiesthrough the establishment of design-level explainability, environmentalwell-being task-interfaces and open-world recognition programs. These designedopen-world protocols are applicable across a wide range of surroundings, underopen-world multimedia recognition scenarios with significant performanceimprovements observed.</description><author>Shide Du, Zihan Fang, Shiyang Lan, Yanchao Tan, Manuel Günther, Shiping Wang, Wenzhong Guo</author><pubDate>Wed, 27 Sep 2023 16:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03666v2</guid></item><item><title>Cognitive Architectures for Language Agents</title><link>http://arxiv.org/abs/2309.02427v2</link><description>Recent efforts have augmented large language models (LLMs) with externalresources (e.g., the Internet) or internal control flows (e.g., promptchaining) for tasks requiring grounding or reasoning, leading to a new class oflanguage agents. While these agents have achieved substantial empiricalsuccess, we lack a systematic framework to organize existing agents and planfuture developments. In this paper, we draw on the rich history of cognitivescience and symbolic artificial intelligence to propose Cognitive Architecturesfor Language Agents (CoALA). CoALA describes a language agent with modularmemory components, a structured action space to interact with internal memoryand external environments, and a generalized decision-making process to chooseactions. We use CoALA to retrospectively survey and organize a large body ofrecent work, and prospectively identify actionable directions towards morecapable agents. Taken together, CoALA contextualizes today's language agentswithin the broader history of AI and outlines a path towards language-basedgeneral intelligence.</description><author>Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths</author><pubDate>Wed, 27 Sep 2023 16:27:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02427v2</guid></item><item><title>Analysis and Evaluation of Kinect-based Action Recognition Algorithms</title><link>http://arxiv.org/abs/2112.08626v2</link><description>Human action recognition still exists many challenging problems such asdifferent viewpoints, occlusion, lighting conditions, human body size and thespeed of action execution, although it has been widely used in different areas.To tackle these challenges, the Kinect depth sensor has been developed torecord real time depth sequences, which are insensitive to the color of humanclothes and illumination conditions. Many methods on recognizing human actionhave been reported in the literature such as HON4D, HOPC, RBD and HDG, whichuse the 4D surface normals, pointclouds, skeleton-based model and depthgradients respectively to capture discriminative information from depth videosor skeleton data. In this research project, the performance of fouraforementioned algorithms will be analyzed and evaluated using five benchmarkdatasets, which cover challenging issues such as noise, change of viewpoints,background clutters and occlusions. We also implemented and improved the HDGalgorithm, and applied it in cross-view action recognition using the UWA3DMultiview Activity dataset. Moreover, we used different combinations ofindividual feature vectors in HDG for performance evaluation. The experimentalresults show that our improvement of HDG outperforms other threestate-of-the-art algorithms for cross-view action recognition.</description><author>Lei Wang</author><pubDate>Wed, 27 Sep 2023 16:27:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.08626v2</guid></item><item><title>Model Share AI: An Integrated Toolkit for Collaborative Machine Learning Model Development, Provenance Tracking, and Deployment in Python</title><link>http://arxiv.org/abs/2309.15719v1</link><description>Machine learning (ML) has the potential to revolutionize a wide range ofresearch areas and industries, but many ML projects never progress past theproof-of-concept stage. To address this issue, we introduce Model Share AI(AIMS), an easy-to-use MLOps platform designed to streamline collaborativemodel development, model provenance tracking, and model deployment, as well asa host of other functions aiming to maximize the real-world impact of MLresearch. AIMS features collaborative project spaces and a standardized modelevaluation process that ranks model submissions based on their performance onunseen evaluation data, enabling collaborative model development andcrowd-sourcing. Model performance and various model metadata are automaticallycaptured to facilitate provenance tracking and allow users to learn from andbuild on previous submissions. Additionally, AIMS allows users to deploy MLmodels built in Scikit-Learn, TensorFlow Keras, PyTorch, and ONNX into liveREST APIs and automatically generated web apps with minimal code. The abilityto deploy models with minimal effort and to make them accessible tonon-technical end-users through web apps has the potential to make ML researchmore applicable to real-world challenges.</description><author>Heinrich Peters, Michael Parrott</author><pubDate>Wed, 27 Sep 2023 16:24:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15719v1</guid></item><item><title>Taming Contrast Maximization for Learning Sequential, Low-latency, Event-based Optical Flow</title><link>http://arxiv.org/abs/2303.05214v2</link><description>Event cameras have recently gained significant traction since they open upnew avenues for low-latency and low-power solutions to complex computer visionproblems. To unlock these solutions, it is necessary to develop algorithms thatcan leverage the unique nature of event data. However, the currentstate-of-the-art is still highly influenced by the frame-based literature, andusually fails to deliver on these promises. In this work, we take this intoconsideration and propose a novel self-supervised learning pipeline for thesequential estimation of event-based optical flow that allows for the scalingof the models to high inference frequencies. At its core, we have acontinuously-running stateful neural model that is trained using a novelformulation of contrast maximization that makes it robust to nonlinearities andvarying statistics in the input events. Results across multiple datasetsconfirm the effectiveness of our method, which establishes a new state of theart in terms of accuracy for approaches trained or optimized without groundtruth.</description><author>Federico Paredes-Vallés, Kirk Y. W. Scheper, Christophe De Wagter, Guido C. H. E. de Croon</author><pubDate>Wed, 27 Sep 2023 16:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05214v2</guid></item><item><title>Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription</title><link>http://arxiv.org/abs/2309.15717v1</link><description>In recent years, research on music transcription has focused mainly onarchitecture design and instrument-specific data acquisition. With the lack ofavailability of diverse datasets, progress is often limited to solo-instrumenttasks such as piano transcription. Several works have explored multi-instrumenttranscription as a means to bolster the performance of models on low-resourcetasks, but these methods face the same data availability issues. We proposeTimbre-Trap, a novel framework which unifies music transcription and audioreconstruction by exploiting the strong separability between pitch and timbre.We train a single U-Net to simultaneously estimate pitch salience andreconstruct complex spectral coefficients, selecting between either outputduring the decoding stage via a simple switch mechanism. In this way, the modellearns to produce coefficients corresponding to timbre-less audio, which can beinterpreted as pitch salience. We demonstrate that the framework leads toperformance comparable to state-of-the-art instrument-agnostic transcriptionmethods, while only requiring a small amount of annotated data.</description><author>Frank Cwitkowitz, Kin Wai Cheuk, Woosung Choi, Marco A. Martínez-Ramírez, Keisuke Toyama, Wei-Hsiang Liao, Yuki Mitsufuji</author><pubDate>Wed, 27 Sep 2023 16:19:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15717v1</guid></item><item><title>Impact of architecture on robustness and interpretability of multispectral deep neural networks</title><link>http://arxiv.org/abs/2309.12463v2</link><description>Including information from additional spectral bands (e.g., near-infrared)can improve deep learning model performance for many vision-oriented tasks.There are many possible ways to incorporate this additional information into adeep learning model, but the optimal fusion strategy has not yet beendetermined and can vary between applications. At one extreme, known as "earlyfusion," additional bands are stacked as extra channels to obtain an inputimage with more than three channels. At the other extreme, known as "latefusion," RGB and non-RGB bands are passed through separate branches of a deeplearning model and merged immediately before a final classification orsegmentation layer. In this work, we characterize the performance of a suite ofmultispectral deep learning models with different fusion approaches, quantifytheir relative reliance on different input bands and evaluate their robustnessto naturalistic image corruptions affecting one or more input channels.</description><author>Charles Godfrey, Elise Bishoff, Myles McKay, Eleanor Byler</author><pubDate>Wed, 27 Sep 2023 16:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12463v2</guid></item><item><title>ChatGPT-BCI: Word-Level Neural State Classification Using GPT, EEG, and Eye-Tracking Biomarkers in Semantic Inference Reading Comprehension</title><link>http://arxiv.org/abs/2309.15714v1</link><description>With the recent explosion of large language models (LLMs), such as GenerativePretrained Transformers (GPT), the need to understand the ability of humans andmachines to comprehend semantic language meaning has entered a new phase. Thisrequires interdisciplinary research that bridges the fields of cognitivescience and natural language processing (NLP). This pilot study aims to provideinsights into individuals' neural states during a semantic relationreading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, andelectroencephalographic (EEG) data to study how the brain processes words withvarying degrees of relevance to a keyword during reading. We also use a featureengineering approach to improve the fixation-related EEG data classificationwhile participants read words with high versus low relevance to the keyword.The best validation accuracy in this word-level classification is over 60\%across 12 subjects. Words of high relevance to the inference keyword hadsignificantly more eye fixations per word: 1.0584 compared to 0.6576 whenexcluding no-fixation words, and 1.5126 compared to 1.4026 when including them.This study represents the first attempt to classify brain states at a wordlevel using LLM knowledge. It provides valuable insights into human cognitiveabilities and the realm of Artificial General Intelligence (AGI), and offersguidance for developing potential reading-assisted technologies.</description><author>Yuhong Zhang, Qin Li, Sujal Nahata, Tasnia Jamal, Shih-kuen Cheng, Gert Cauwenberghs, Tzyy-Ping Jung</author><pubDate>Wed, 27 Sep 2023 16:12:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15714v1</guid></item><item><title>Deep network series for large-scale high-dynamic range imaging</title><link>http://arxiv.org/abs/2210.16060v3</link><description>We propose a new approach for large-scale high-dynamic range computationalimaging. Deep Neural Networks (DNNs) trained end-to-end can solve linearinverse imaging problems almost instantaneously. While unfolded architecturesprovide robustness to measurement setting variations, embedding large-scalemeasurement operators in DNN architectures is impractical. AlternativePlug-and-Play (PnP) approaches, where the denoising DNNs are blind to themeasurement setting, have proven effective to address scalability andhigh-dynamic range challenges, but rely on highly iterative algorithms. Wepropose a residual DNN series approach, also interpretable as a learned versionof matching pursuit, where the reconstructed image is a sum of residual imagesprogressively increasing the dynamic range, and estimated iteratively by DNNstaking the back-projected data residual of the previous iteration as input. Wedemonstrate on radio-astronomical imaging simulations that a series of only fewterms provides a reconstruction quality competitive with PnP, at a fraction ofthe cost.</description><author>Amir Aghabiglou, Matthieu Terris, Adrian Jackson, Yves Wiaux</author><pubDate>Wed, 27 Sep 2023 16:05:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16060v3</guid></item><item><title>An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning</title><link>http://arxiv.org/abs/2308.11677v2</link><description>Class-Incremental Learning (CIL) aims to build classification models fromdata streams. At each step of the CIL process, new classes must be integratedinto the model. Due to catastrophic forgetting, CIL is particularly challengingwhen examples from past classes cannot be stored, the case on which we focushere. To date, most approaches are based exclusively on the target dataset ofthe CIL process. However, the use of models pre-trained in a self-supervisedway on large amounts of data has recently gained momentum. The initial model ofthe CIL process may only use the first batch of the target dataset, or also usepre-trained weights obtained on an auxiliary dataset. The choice between thesetwo initial learning strategies can significantly influence the performance ofthe incremental learning model, but has not yet been studied in depth.Performance is also influenced by the choice of the CIL algorithm, the neuralarchitecture, the nature of the target task, the distribution of classes in thestream and the number of examples available for learning. We conduct acomprehensive experimental study to assess the roles of these factors. Wepresent a statistical analysis framework that quantifies the relativecontribution of each factor to incremental performance. Our main finding isthat the initial training strategy is the dominant factor influencing theaverage incremental accuracy, but that the choice of CIL algorithm is moreimportant in preventing forgetting. Based on this analysis, we proposepractical recommendations for choosing the right initial training strategy fora given incremental learning use case. These recommendations are intended tofacilitate the practical deployment of incremental learning.</description><author>Grégoire Petit, Michael Soumm, Eva Feillet, Adrian Popescu, Bertrand Delezoide, David Picard, Céline Hudelot</author><pubDate>Wed, 27 Sep 2023 15:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11677v2</guid></item><item><title>Maximum Weight Entropy</title><link>http://arxiv.org/abs/2309.15704v1</link><description>This paper deals with uncertainty quantification and out-of-distributiondetection in deep learning using Bayesian and ensemble methods. It proposes apractical solution to the lack of prediction diversity observed recently forstandard approaches when used out-of-distribution (Ovadia et al., 2019; Liu etal., 2021). Considering that this issue is mainly related to a lack of weightdiversity, we claim that standard methods sample in "over-restricted" regionsof the weight space due to the use of "over-regularization" processes, such asweight decay and zero-mean centered Gaussian priors. We propose to solve theproblem by adopting the maximum entropy principle for the weight distribution,with the underlying idea to maximize the weight diversity. Under this paradigm,the epistemic uncertainty is described by the weight distribution of maximalentropy that produces neural networks "consistent" with the trainingobservations. Considering stochastic neural networks, a practical optimizationis derived to build such a distribution, defined as a trade-off between theaverage empirical risk and the weight distribution entropy. We develop a novelweight parameterization for the stochastic model, based on the singular valuedecomposition of the neural network's hidden representations, which enables alarge increase of the weight entropy for a small empirical risk penalization.We provide both theoretical and numerical results to assess the efficiency ofthe approach. In particular, the proposed algorithm appears in the top threebest methods in all configurations of an extensive out-of-distributiondetection benchmark including more than thirty competitors.</description><author>Antoine de Mathelin, François Deheeger, Mathilde Mougeot, Nicolas Vayatis</author><pubDate>Wed, 27 Sep 2023 15:46:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15704v1</guid></item><item><title>Physics-Based Rigid Body Object Tracking and Friction Filtering From RGB-D Videos</title><link>http://arxiv.org/abs/2309.15703v1</link><description>Physics-based understanding of object interactions from sensory observationsis an essential capability in augmented reality and robotics. It enablescapturing the properties of a scene for simulation and control. In this paper,we propose a novel approach for real-to-sim which tracks rigid objects in 3Dfrom RGB-D images and infers physical properties of the objects. We use adifferentiable physics simulation as state-transition model in an ExtendedKalman Filter which can model contact and friction for arbitrary mesh-basedshapes and in this way estimate physically plausible trajectories. Wedemonstrate that our approach can filter position, orientation, velocities, andconcurrently can estimate the coefficient of friction of the objects. Weanalyse our approach on various sliding scenarios in synthetic image sequencesof single objects and colliding objects. We also demonstrate and evaluate ourapproach on a real-world dataset. We will make our novel benchmark datasetspublicly available to foster future research in this novel problem setting andcomparison with our method.</description><author>Rama Krishna Kandukuri, Michael Strecke, Joerg Stueckler</author><pubDate>Wed, 27 Sep 2023 15:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15703v1</guid></item><item><title>SGRec3D: Self-Supervised 3D Scene Graph Learning via Object-Level Scene Reconstruction</title><link>http://arxiv.org/abs/2309.15702v1</link><description>In the field of 3D scene understanding, 3D scene graphs have emerged as a newscene representation that combines geometric and semantic information aboutobjects and their relationships. However, learning semantic 3D scene graphs ina fully supervised manner is inherently difficult as it requires not onlyobject-level annotations but also relationship labels. While pre-trainingapproaches have helped to boost the performance of many methods in variousfields, pre-training for 3D scene graph prediction has received littleattention. Furthermore, we find in this paper that classical contrastive pointcloud-based pre-training approaches are ineffective for 3D scene graphlearning. To this end, we present SGRec3D, a novel self-supervised pre-trainingmethod for 3D scene graph prediction. We propose to reconstruct the 3D inputscene from a graph bottleneck as a pretext task. Pre-training SGRec3D does notrequire object relationship labels, making it possible to exploit large-scale3D scene understanding datasets, which were off-limits for 3D scene graphlearning before. Our experiments demonstrate that in contrast to recent pointcloud-based pre-training approaches, our proposed pre-training improves the 3Dscene graph prediction considerably, which results in SOTA performance,outperforming other 3D scene graph models by +10% on object prediction and +4%on relationship prediction. Additionally, we show that only using a smallsubset of 10% labeled data during fine-tuning is sufficient to outperform thesame model without pre-training.</description><author>Sebastian Koch, Pedro Hermosilla, Narunas Vaskevicius, Mirco Colosi, Timo Ropinski</author><pubDate>Wed, 27 Sep 2023 15:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15702v1</guid></item><item><title>HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models</title><link>http://arxiv.org/abs/2309.15701v1</link><description>Advancements in deep neural networks have allowed automatic speechrecognition (ASR) systems to attain human parity on several publicly availableclean speech datasets. However, even state-of-the-art ASR systems experienceperformance degradation when confronted with adverse conditions, as awell-trained acoustic model is sensitive to variations in the speech domain,e.g., background noise. Intuitively, humans address this issue by relying ontheir linguistic knowledge: the meaning of ambiguous spoken terms is usuallyinferred from contextual cues thereby reducing the dependency on the auditorysystem. Inspired by this observation, we introduce the first open-sourcebenchmark to utilize external large language models (LLMs) for ASR errorcorrection, where N-best decoding hypotheses provide informative elements fortrue transcription prediction. This approach is a paradigm shift from thetraditional language model rescoring strategy that can only select onecandidate hypothesis as the output transcription. The proposed benchmarkcontains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairsof N-best hypotheses and corresponding accurate transcriptions across prevalentspeech domains. Given this dataset, we examine three types of error correctiontechniques based on LLMs with varying amounts of labeledhypotheses-transcription pairs, which gains a significant word error rate (WER)reduction. Experimental evidence demonstrates the proposed technique achieves abreakthrough by surpassing the upper bound of traditional re-ranking basedmethods. More surprisingly, LLM with reasonable prompt and its generativecapability can even correct those tokens that are missing in N-best list. Wemake our results publicly accessible for reproducible pipelines with releasedpre-trained models, thus providing a new evaluation paradigm for ASR errorcorrection with LLMs.</description><author>Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Sabato Macro Siniscalchi, Pin-Yu Chen, Eng Siong Chng</author><pubDate>Wed, 27 Sep 2023 15:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15701v1</guid></item><item><title>Deep Model Fusion: A Survey</title><link>http://arxiv.org/abs/2309.15698v1</link><description>Deep model fusion/merging is an emerging technique that merges the parametersor predictions of multiple deep learning models into a single one. It combinesthe abilities of different models to make up for the biases and errors of asingle model to achieve better performance. However, deep model fusion onlarge-scale deep learning models (e.g., LLMs and foundation models) facesseveral challenges, including high computational cost, high-dimensionalparameter space, interference between different heterogeneous models, etc.Although model fusion has attracted widespread attention due to its potentialto solve complex real-world tasks, there is still a lack of complete anddetailed survey research on this technique. Accordingly, in order to understandthe model fusion method better and promote its development, we present acomprehensive survey to summarize the recent progress. Specifically, wecategorize existing deep model fusion methods as four-fold: (1) "Modeconnectivity", which connects the solutions in weight space via a path ofnon-increasing loss, in order to obtain better initialization for model fusion;(2) "Alignment" matches units between neural networks to create betterconditions for fusion; (3) "Weight average", a classical model fusion method,averages the weights of multiple models to obtain more accurate results closerto the optimal solution; (4) "Ensemble learning" combines the outputs ofdiverse models, which is a foundational technique for improving the accuracyand robustness of the final model. In addition, we analyze the challenges facedby deep model fusion and propose possible research directions for model fusionin the future. Our review is helpful in deeply understanding the correlationbetween different model fusion methods and practical application methods, whichcan enlighten the research in the field of deep model fusion.</description><author>Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, Li Shen</author><pubDate>Wed, 27 Sep 2023 15:40:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15698v1</guid></item><item><title>Physics Inspired Hybrid Attention for SAR Target Recognition</title><link>http://arxiv.org/abs/2309.15697v1</link><description>There has been a recent emphasis on integrating physical models and deepneural networks (DNNs) for SAR target recognition, to improve performance andachieve a higher level of physical interpretability. The attributed scatteringcenter (ASC) parameters garnered the most interest, being considered asadditional input data or features for fusion in most methods. However, theperformance greatly depends on the ASC optimization result, and the fusionstrategy is not adaptable to different types of physical information.Meanwhile, the current evaluation scheme is inadequate to assess the model'srobustness and generalizability. Thus, we propose a physics inspired hybridattention (PIHA) mechanism and the once-for-all (OFA) evaluation protocol toaddress the above issues. PIHA leverages the high-level semantics of physicalinformation to activate and guide the feature group aware of local semantics oftarget, so as to re-weight the feature importance based on knowledge prior. Itis flexible and generally applicable to various physical models, and can beintegrated into arbitrary DNNs without modifying the original architecture. Theexperiments involve a rigorous assessment using the proposed OFA, which entailstraining and validating a model on either sufficient or limited data andevaluating on multiple test sets with different data distributions. Our methodoutperforms other state-of-the-art approaches in 12 test scenarios with sameASC parameters. Moreover, we analyze the working mechanism of PIHA and evaluatevarious PIHA enabled DNNs. The experiments also show PIHA is effective fordifferent physical information. The source code together with the adoptedphysical information is available at https://github.com/XAI4SAR.</description><author>Zhongling Huang, Chong Wu, Xiwen Yao, Zhicheng Zhao, Xiankai Huang, Junwei Han</author><pubDate>Wed, 27 Sep 2023 15:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15697v1</guid></item><item><title>A Unified View of Differentially Private Deep Generative Modeling</title><link>http://arxiv.org/abs/2309.15696v1</link><description>The availability of rich and vast data sources has greatly advanced machinelearning applications in various domains. However, data with privacy concernscomes with stringent regulations that frequently prohibited data access anddata sharing. Overcoming these obstacles in compliance with privacyconsiderations is key for technological progress in many real-world applicationscenarios that involve privacy sensitive data. Differentially private (DP) datapublishing provides a compelling solution, where only a sanitized form of thedata is publicly released, enabling privacy-preserving downstream analysis andreproducible research in sensitive domains. In recent years, various approacheshave been proposed for achieving privacy-preserving high-dimensional datageneration by private training on top of deep neural networks. In this paper,we present a novel unified view that systematizes these approaches. Our viewprovides a joint design space for systematically deriving methods that cater todifferent use cases. We then discuss the strengths, limitations, and inherentcorrelations between different approaches, aiming to shed light on crucialaspects and inspire future research. We conclude by presenting potential pathsforward for the field of DP data generation, with the aim of steering thecommunity toward making the next important steps in advancingprivacy-preserving learning.</description><author>Dingfan Chen, Raouf Kerkouche, Mario Fritz</author><pubDate>Wed, 27 Sep 2023 15:38:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15696v1</guid></item><item><title>Face Generation and Editing with StyleGAN: A Survey</title><link>http://arxiv.org/abs/2212.09102v3</link><description>Our goal with this survey is to provide an overview of the state of the artdeep learning methods for face generation and editing using StyleGAN. Thesurvey covers the evolution of StyleGAN, from PGGAN to StyleGAN3, and exploresrelevant topics such as suitable metrics for training, different latentrepresentations, GAN inversion to latent spaces of StyleGAN, face imageediting, cross-domain face stylization, face restoration, and even Deepfakeapplications. We aim to provide an entry point into the field for readers thathave basic knowledge about the field of deep learning and are looking for anaccessible introduction and overview.</description><author>Andrew Melnik, Maksim Miasayedzenkau, Dzianis Makarovets, Dzianis Pirshtuk, Eren Akbulut, Dennis Holzmann, Tarek Renusch, Gustav Reichert, Helge Ritter</author><pubDate>Wed, 27 Sep 2023 15:34:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09102v3</guid></item><item><title>Breaking NoC Anonymity using Flow Correlation Attack</title><link>http://arxiv.org/abs/2309.15687v1</link><description>Network-on-Chip (NoC) is widely used as the internal communication fabric intoday's multicore System-on-Chip (SoC) designs. Security of the on-chipcommunication is crucial because exploiting any vulnerability in shared NoCwould be a goldmine for an attacker. NoC security relies on effectivecountermeasures against diverse attacks. We investigate the security strengthof existing anonymous routing protocols in NoC architectures. Specifically,this paper makes two important contributions. We show that the existinganonymous routing is vulnerable to machine learning (ML) based flow correlationattacks on NoCs. We propose a lightweight anonymous routing that use trafficobfuscation techniques which can defend against ML-based flow correlationattacks. Experimental studies using both real and synthetic traffic reveal thatour proposed attack is successful against state-of-the-art anonymous routing inNoC architectures with a high accuracy (up to 99%) for diverse trafficpatterns, while our lightweight countermeasure can defend against ML-basedattacks with minor hardware and performance overhead.</description><author>Hansika Weerasena, Pan Zhixin, Khushboo Rani, Prabhat Mishra</author><pubDate>Wed, 27 Sep 2023 15:32:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15687v1</guid></item><item><title>Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization</title><link>http://arxiv.org/abs/2309.15686v1</link><description>Incorporating longer context has been shown to benefit machine translation,but the inclusion of context in end-to-end speech translation (E2E-ST) remainsunder-studied. To bridge this gap, we introduce target language context inE2E-ST, enhancing coherence and overcoming memory constraints of extended audiosegments. Additionally, we propose context dropout to ensure robustness to theabsence of context, and further improve performance by adding speakerinformation. Our proposed contextual E2E-ST outperforms the isolatedutterance-based E2E-ST approach. Lastly, we demonstrate that in conversationalspeech, contextual information primarily contributes to capturing contextstyle, as well as resolving anaphora and named entities.</description><author>Amir Hussein, Brian Yan, Antonios Anastasopoulos, Shinji Watanabe, Sanjeev Khudanpur</author><pubDate>Wed, 27 Sep 2023 15:32:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15686v1</guid></item><item><title>End-to-End Streaming Video Temporal Action Segmentation with Reinforce Learning</title><link>http://arxiv.org/abs/2309.15683v1</link><description>Temporal Action Segmentation (TAS) from video is a kind of frame recognitiontask for long video with multiple action classes. As an video understandingtask for long videos, current methods typically combine multi-modality actionrecognition models with temporal models to convert feature sequences to labelsequences. This approach can only be applied to offline scenarios, whichseverely limits the TAS application. Therefore, this paper proposes anend-to-end Streaming Video Temporal Action Segmentation with Reinforce Learning(SVTAS-RL). The end-to-end SVTAS which regard TAS as an action segmentclustering task can expand the application scenarios of TAS; and RL is used toalleviate the problem of inconsistent optimization objective and direction.Through extensive experiments, the SVTAS-RL model achieves a competitiveperformance to the state-of-the-art model of TAS on multiple datasets, andshows greater advantages on the ultra-long video dataset EGTEA. This indicatesthat our method can replace all current TAS models end-to-end and SVTAS-RL ismore suitable for long video TAS. Code is availabel athttps://github.com/Thinksky5124/SVTAS.</description><author>Wujun Wen, Jinrong Zhang, Shenglan Liu, Yunheng Li, Qifeng Li, Lin Feng</author><pubDate>Wed, 27 Sep 2023 15:30:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15683v1</guid></item><item><title>Quantum-Noise-driven Generative Diffusion Models</title><link>http://arxiv.org/abs/2308.12013v2</link><description>Generative models realized with machine learning techniques are powerfultools to infer complex and unknown data distributions from a finite number oftraining samples in order to produce new synthetic data. Diffusion models arean emerging framework that have recently overcome the performance of thegenerative adversarial networks in creating synthetic text and high-qualityimages. Here, we propose and discuss the quantum generalization of diffusionmodels, i.e., three quantum-noise-driven generative diffusion models that couldbe experimentally tested on real quantum systems. The idea is to harness uniquequantum features, in particular the non-trivial interplay among coherence,entanglement and noise that the currently available noisy quantum processors dounavoidably suffer from, in order to overcome the main computational burdens ofclassical diffusion models during inference. Hence, we suggest to exploitquantum noise not as an issue to be detected and solved but instead as a veryremarkably beneficial key ingredient to generate much more complex probabilitydistributions that would be difficult or even impossible to expressclassically, and from which a quantum processor might sample more efficientlythan a classical one. An example of numerical simulations for an hybridclassical-quantum generative diffusion model is also included. Therefore, ourresults are expected to pave the way for new quantum-inspired or quantum-basedgenerative diffusion algorithms addressing more powerfully classical tasks asdata generation/prediction with widespread real-world applications ranging fromclimate forecasting to neuroscience, from traffic flow analysis to financialforecasting.</description><author>Marco Parigi, Stefano Martina, Filippo Caruso</author><pubDate>Wed, 27 Sep 2023 15:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12013v2</guid></item><item><title>Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs</title><link>http://arxiv.org/abs/2309.07311v2</link><description>Most interpretability research in NLP focuses on understanding the behaviorand features of a fully trained model. However, certain insights into modelbehavior may only be accessible by observing the trajectory of the trainingprocess. In this paper, we present a case study of syntax acquisition in maskedlanguage models (MLMs). Our findings demonstrate how analyzing the evolution ofinterpretable artifacts throughout training deepens our understanding ofemergent behavior. In particular, we study Syntactic Attention Structure (SAS),a naturally emerging property of MLMs wherein specific Transformer heads tendto focus on specific syntactic relations. We identify a brief window intraining when models abruptly acquire SAS and find that this window isconcurrent with a steep drop in loss. Moreover, SAS precipitates the subsequentacquisition of linguistic capabilities. We then examine the causal role of SASby introducing a regularizer to manipulate SAS during training, and demonstratethat SAS is necessary for the development of grammatical capabilities. Wefurther find that SAS competes with other beneficial traits and capabilitiesduring training, and that briefly suppressing SAS can improve model quality.These findings reveal a real-world example of the relationship betweendisadvantageous simplicity bias and interpretable breakthrough trainingdynamics.</description><author>Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, Naomi Saphra</author><pubDate>Wed, 27 Sep 2023 15:25:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07311v2</guid></item><item><title>Joint Sampling and Optimisation for Inverse Rendering</title><link>http://arxiv.org/abs/2309.15676v1</link><description>When dealing with difficult inverse problems such as inverse rendering, usingMonte Carlo estimated gradients to optimise parameters can slow downconvergence due to variance. Averaging many gradient samples in each iterationreduces this variance trivially. However, for problems that require thousandsof optimisation iterations, the computational cost of this approach risesquickly. We derive a theoretical framework for interleaving sampling and optimisation.We update and reuse past samples with low-variance finite-difference estimatorsthat describe the change in the estimated gradients between each iteration. Bycombining proportional and finite-difference samples, we continuously reducethe variance of our novel gradient meta-estimators throughout the optimisationprocess. We investigate how our estimator interlinks with Adam and derive astable combination. We implement our method for inverse path tracing and demonstrate how ourestimator speeds up convergence on difficult optimisation tasks.</description><author>Martin Balint, Karol Myszkowski, Hans-Peter Seidel, Gurprit Singh</author><pubDate>Wed, 27 Sep 2023 15:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15676v1</guid></item><item><title>SJTU-TMQA: A quality assessment database for static mesh with texture map</title><link>http://arxiv.org/abs/2309.15675v1</link><description>In recent years, static meshes with texture maps have become one of the mostprevalent digital representations of 3D shapes in various applications, such asanimation, gaming, medical imaging, and cultural heritage applications.However, little research has been done on the quality assessment of texturedmeshes, which hinders the development of quality-oriented applications, such asmesh compression and enhancement. In this paper, we create a large-scaletextured mesh quality assessment database, namely SJTU-TMQA, which includes 21reference meshes and 945 distorted samples. The meshes are rendered intoprocessed video sequences and then conduct subjective experiments to obtainmean opinion scores (MOS). The diversity of content and accuracy of MOS hasbeen shown to validate its heterogeneity and reliability. The impact of varioustypes of distortion on human perception is demonstrated. 13 state-of-the-artobjective metrics are evaluated on SJTU-TMQA. The results report the highestcorrelation of around 0.6, indicating the need for more effective objectivemetrics. The SJTU-TMQA is available at https://ccccby.github.io</description><author>Bingyang Cui, Qi Yang, Kaifa Yang, Yiling Xu, Xiaozhong Xu, Shan Liu</author><pubDate>Wed, 27 Sep 2023 15:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15675v1</guid></item><item><title>Speech collage: code-switched audio generation by collaging monolingual corpora</title><link>http://arxiv.org/abs/2309.15674v1</link><description>Designing effective automatic speech recognition (ASR) systems forCode-Switching (CS) often depends on the availability of the transcribed CSresources. To address data scarcity, this paper introduces Speech Collage, amethod that synthesizes CS data from monolingual corpora by splicing audiosegments. We further improve the smoothness quality of audio generation usingan overlap-add approach. We investigate the impact of generated data on speechrecognition in two scenarios: using in-domain CS text and a zero-shot approachwith synthesized CS text. Empirical results highlight up to 34.4% and 16.2%relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain andzero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentationbolsters the model's code-switching inclination and reduces its monolingualbias.</description><author>Amir Hussein, Dorsa Zeinali, Ondřej Klejch, Matthew Wiesner, Brian Yan, Shammur Chowdhury, Ahmed Ali, Shinji Watanabe, Sanjeev Khudanpur</author><pubDate>Wed, 27 Sep 2023 15:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15674v1</guid></item><item><title>MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection</title><link>http://arxiv.org/abs/2309.15670v1</link><description>In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) havebeen increasingly popular in the Bangla language, which is the seventh mostspoken language throughout the entire world. However, the language isstructurally complicated, which makes this field arduous to extract emotions inan accurate manner. Several distinct approaches such as the extraction ofpositive and negative sentiments as well as multiclass emotions, have beenimplemented in this field of study. Nevertheless, the extraction of multiplesentiments is an almost untouched area in this language. Which involvesidentifying several feelings based on a single piece of text. Therefore, thisstudy demonstrates a thorough method for constructing an annotated corpus basedon scrapped data from Facebook to bridge the gaps in this subject area toovercome the challenges. To make this annotation more fruitful, thecontext-based approach has been used. Bidirectional Encoder Representationsfrom Transformers (BERT), a well-known methodology of transformers, have beenshown the best results of all methods implemented. Finally, a web applicationhas been developed to demonstrate the performance of the pre-trainedtop-performer model (BERT) for multi-label ER in Bangla.</description><author>Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty</author><pubDate>Wed, 27 Sep 2023 15:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15670v1</guid></item><item><title>On Computational Entanglement and Its Interpretation in Adversarial Machine Learning</title><link>http://arxiv.org/abs/2309.15669v1</link><description>Adversarial examples in machine learning has emerged as a focal point ofresearch due to their remarkable ability to deceive models with seeminglyinconspicuous input perturbations, potentially resulting in severeconsequences. In this study, we embark on a comprehensive exploration ofadversarial machine learning models, shedding light on their intrinsiccomplexity and interpretability. Our investigation reveals intriguing linksbetween machine learning model complexity and Einstein's theory of specialrelativity, through the concept of entanglement. More specific, we defineentanglement computationally and demonstrate that distant feature samples canexhibit strong correlations, akin to entanglement in quantum realm. Thisrevelation challenges conventional perspectives in describing the phenomenon ofadversarial transferability observed in contemporary machine learning models.By drawing parallels with the relativistic effects of time dilation and lengthcontraction during computation, we gain deeper insights into adversarialmachine learning, paving the way for more robust and interpretable models inthis rapidly evolving field.</description><author>YenLung Lai, Xingbo Dong, Zhe Jin</author><pubDate>Wed, 27 Sep 2023 15:09:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15669v1</guid></item><item><title>Improving the convergence of SGD through adaptive batch sizes</title><link>http://arxiv.org/abs/1910.08222v4</link><description>Mini-batch stochastic gradient descent (SGD) and variants thereof approximatethe objective function's gradient with a small number of training examples, akathe batch size. Small batch sizes require little computation for each modelupdate but can yield high-variance gradient estimates, which poses somechallenges for optimization. Conversely, large batches require more computationbut can yield higher precision gradient estimates. This work presents a methodto adapt the batch size to the model's training loss. For various functionclasses, we show that our method requires the same order of model updates asgradient descent while requiring the same order of gradient computations asSGD. This method requires evaluating the model's loss on the entire datasetevery model update. However, the required computation is greatly reduced byapproximating the training loss. We provide experiments that illustrate ourmethods require fewer model updates without increasing the total amount ofcomputation.</description><author>Scott Sievert, Shrey Shah</author><pubDate>Wed, 27 Sep 2023 15:05:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1910.08222v4</guid></item><item><title>Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection</title><link>http://arxiv.org/abs/2308.12885v2</link><description>The rapid entry of machine learning approaches in our daily activities andhigh-stakes domains demands transparency and scrutiny of their fairness andreliability. To help gauge machine learning models' robustness, researchtypically focuses on the massive datasets used for their deployment, e.g.,creating and maintaining documentation for understanding their origin, processof development, and ethical considerations. However, data collection for AI isstill typically a one-off practice, and oftentimes datasets collected for acertain purpose or application are reused for a different problem.Additionally, dataset annotations may not be representative over time, containambiguous or erroneous annotations, or be unable to generalize across issues ordomains. Recent research has shown these practices might lead to unfair,biased, or inaccurate outcomes. We argue that data collection for AI should beperformed in a responsible manner where the quality of the data is thoroughlyscrutinized and measured through a systematic set of appropriate metrics. Inthis paper, we propose a Responsible AI (RAI) methodology designed to guide thedata collection with a set of metrics for an iterative in-depth analysis of thefactors influencing the quality and reliability} of the generated data. Wepropose a granular set of measurements to inform on the internal reliability ofa dataset and its external stability over time. We validate our approach acrossnine existing datasets and annotation tasks and four content modalities. Thisapproach impacts the assessment of data robustness used for AI applied in thereal world, where diversity of users and content is eminent. Furthermore, itdeals with fairness and accountability aspects in data collection by providingsystematic and transparent quality analysis for data collections.</description><author>Oana Inel, Tim Draws, Lora Aroyo</author><pubDate>Wed, 27 Sep 2023 15:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12885v2</guid></item><item><title>MoCaE: Mixture of Calibrated Experts Significantly Improves Object Detection</title><link>http://arxiv.org/abs/2309.14976v2</link><description>We propose an extremely simple and highly effective approach to faithfullycombine different object detectors to obtain a Mixture of Experts (MoE) thathas a superior accuracy to the individual experts in the mixture. We find thatnaively combining these experts in a similar way to the well-known DeepEnsembles (DEs), does not result in an effective MoE. We identify theincompatibility between the confidence score distribution of differentdetectors to be the primary reason for such failure cases. Therefore, toconstruct the MoE, our proposal is to first calibrate each individual detectoragainst a target calibration function. Then, filter and refine all thepredictions from different detectors in the mixture. We term this approach asMoCaE and demonstrate its effectiveness through extensive experiments on objectdetection, instance segmentation and rotated object detection tasks.Specifically, MoCaE improves (i) three strong object detectors on COCO test-devby $2.4$ $\mathrm{AP}$ by reaching $59.0$ $\mathrm{AP}$; (ii) instancesegmentation methods on the challenging long-tailed LVIS dataset by $2.3$$\mathrm{AP}$; and (iii) all existing rotated object detectors by reaching$82.62$ $\mathrm{AP_{50}}$ on DOTA dataset, establishing a new state-of-the-art(SOTA). Code will be made public.</description><author>Kemal Oksuz, Selim Kuzucu, Tom Joy, Puneet K. Dokania</author><pubDate>Wed, 27 Sep 2023 14:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14976v2</guid></item><item><title>Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data</title><link>http://arxiv.org/abs/2304.05874v3</link><description>Graph neural network (GNN) models are increasingly being used for theclassification of electroencephalography (EEG) data. However, GNN-baseddiagnosis of neurological disorders, such as Alzheimer's disease (AD), remainsa relatively unexplored area of research. Previous studies have relied onfunctional connectivity methods to infer brain graph structures and used simpleGNN architectures for the diagnosis of AD. In this work, we propose a noveladaptive gated graph convolutional network (AGGCN) that can provide explainablepredictions. AGGCN adaptively learns graph structures by combiningconvolution-based node feature enhancement with a correlation-based measure ofpower spectral density similarity. Furthermore, the gated graph convolution candynamically weigh the contribution of various spatial scales. The proposedmodel achieves high accuracy in both eyes-closed and eyes-open conditions,indicating the stability of learned representations. Finally, we demonstratethat the proposed AGGCN model generates consistent explanations of itspredictions that might be relevant for further study of AD-related alterationsof brain networks.</description><author>Dominik Klepl, Fei He, Min Wu, Daniel J. Blackburn, Ptolemaios G. Sarrigiannis</author><pubDate>Wed, 27 Sep 2023 14:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05874v3</guid></item><item><title>Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing</title><link>http://arxiv.org/abs/2309.15664v1</link><description>Large-scale text-to-image generative models have been a ground-breakingdevelopment in generative AI, with diffusion models showing their astoundingability to synthesize convincing images following an input text prompt. Thegoal of image editing research is to give users control over the generatedimages by modifying the text prompt. Current image editing techniques aresusceptible to unintended modifications of regions outside the targeted area,such as on the background or on distractor objects which have some semantic orvisual relationship with the targeted object. According to our experimentalfindings, inaccurate cross-attention maps are at the root of this problem.Based on this observation, we propose Dynamic Prompt Learning (DPL) to forcecross-attention maps to focus on correct noun words in the text prompt. Byupdating the dynamic tokens for nouns in the textual input with the proposedleakage repairment losses, we achieve fine-grained image editing overparticular objects while preventing undesired changes to other image regions.Our method DPL, based on the publicly available Stable Diffusion, isextensively evaluated on a wide range of images, and consistently obtainssuperior results both quantitatively (CLIP score, Structure-Dist) andqualitatively (on user-evaluation). We show improved prompt editing results forWord-Swap, Prompt Refinement, and Attention Re-weighting, especially forcomplex multi-object scenes.</description><author>Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, Joost van de Weijer</author><pubDate>Wed, 27 Sep 2023 14:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15664v1</guid></item><item><title>Human Kinematics-inspired Skeleton-based Video Anomaly Detection</title><link>http://arxiv.org/abs/2309.15662v1</link><description>Previous approaches to detecting human anomalies in videos have typicallyrelied on implicit modeling by directly applying the model to video or skeletondata, potentially resulting in inaccurate modeling of motion information. Inthis paper, we conduct an exploratory study and introduce a new idea calledHKVAD (Human Kinematic-inspired Video Anomaly Detection) for video anomalydetection, which involves the explicit use of human kinematic features todetect anomalies. To validate the effectiveness and potential of thisperspective, we propose a pilot method that leverages the kinematic features ofthe skeleton pose, with a specific focus on the walking stride, skeletondisplacement at feet level, and neck level. Following this, the method employsa normalizing flow model to estimate density and detect anomalies based on theestimated density. Based on the number of kinematic features used, we havedevised three straightforward variant methods and conducted experiments on twohighly challenging public datasets, ShanghaiTech and UBnormal. Our methodachieves good results with minimal computational resources, validating itseffectiveness and potential.</description><author>Jian Xiao, Tianyuan Liu, Genlin Ji</author><pubDate>Wed, 27 Sep 2023 14:52:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15662v1</guid></item><item><title>Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency</title><link>http://arxiv.org/abs/2309.15659v1</link><description>Federated Learning (FL) is a prominent distributed learning paradigmfacilitating collaboration among nodes within an edge network to co-train aglobal model without centralizing data. By shifting computation to the networkedge, FL offers robust and responsive edge-AI solutions and enhanceprivacy-preservation. However, deploying deep FL models within edgeenvironments is often hindered by communication bottlenecks, dataheterogeneity, and memory limitations. To address these challenges jointly, weintroduce FeDEQ, a pioneering FL framework that effectively employs deepequilibrium learning and consensus optimization to exploit a compact shareddata representation across edge nodes, allowing the derivation of personalizedmodels specific to each node. We delve into a unique model structure composedof an equilibrium layer followed by traditional neural network layers. Here,the equilibrium layer functions as a global feature representation that edgenodes can adapt to personalize their local layers. Capitalizing on FeDEQ'scompactness and representation power, we present a novel distributed algorithmrooted in the alternating direction method of multipliers (ADMM) consensusoptimization and theoretically establish its convergence for smooth objectives.Experiments across various benchmarks demonstrate that FeDEQ achievesperformance comparable to state-of-the-art personalized methods while employingmodels of up to 4 times smaller in communication size and 1.5 times lowermemory footprint during training.</description><author>Long Tan Le, Tuan Dung Nguyen, Tung-Anh Nguyen, Choong Seon Hong, Nguyen H. Tran</author><pubDate>Wed, 27 Sep 2023 14:48:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15659v1</guid></item><item><title>Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis</title><link>http://arxiv.org/abs/2309.15656v1</link><description>Scripted dialogues such as movie and TV subtitles constitute a widespreadsource of training data for conversational NLP models. However, the linguisticcharacteristics of those dialogues are notably different from those observed incorpora of spontaneous interactions. This difference is particularly marked forcommunicative feedback and grounding phenomena such as backchannels,acknowledgments, or clarification requests. Such signals are known toconstitute a key part of the conversation flow and are used by the dialogueparticipants to provide feedback to one another on their perception of theongoing interaction. This paper presents a quantitative analysis of suchcommunicative feedback phenomena in both subtitles and spontaneousconversations. Based on dialogue data in English, French, German, Hungarian,Italian, Japanese, Norwegian and Chinese, we extract both lexical statisticsand classification outputs obtained with a neural dialogue act tagger. Two mainfindings of this empirical study are that (1) conversational feedback ismarkedly less frequent in subtitles than in spontaneous dialogues and (2)subtitles contain a higher proportion of negative feedback. Furthermore, weshow that dialogue responses generated by large language models also follow thesame underlying trends and include comparatively few occurrences ofcommunicative feedback, except when those models are explicitly fine-tuned onspontaneous dialogues.</description><author>Ildikó Pilán, Laurent Prévot, Hendrik Buschmeier, Pierre Lison</author><pubDate>Wed, 27 Sep 2023 14:45:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15656v1</guid></item><item><title>DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion</title><link>http://arxiv.org/abs/2308.12877v2</link><description>This paper outlines the performance evaluation of a system for adverse drugevent normalization, developed by the Data Science for Digital Health (DS4DH)group for the Social Media Mining for Health Applications (SMM4H) 2023 sharedtask 5. Shared task 5 targeted the normalization of adverse drug event mentionsin Twitter to standard concepts of the Medical Dictionary for RegulatoryActivities terminology. Our system hinges on a two-stage approach: BERTfine-tuning for entity recognition, followed by zero-shot normalization usingsentence transformers and reciprocal-rank fusion. The approach yielded aprecision of 44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformedthe median performance in shared task 5 by 10% and demonstrated the highestperformance among all participants. These results substantiate theeffectiveness of our approach and its potential application for adverse drugevent normalization in the realm of social media text mining.</description><author>Anthony Yazdani, Hossein Rouhizadeh, David Vicente Alvarez, Douglas Teodoro</author><pubDate>Wed, 27 Sep 2023 14:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12877v2</guid></item><item><title>TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins</title><link>http://arxiv.org/abs/2303.15954v4</link><description>Road network digital twins (RNDTs) play a critical role in the development ofnext-generation intelligent transportation systems, enabling more precisetraffic planning and control. To support just-in-time (JIT) decision making,RNDTs require a model that dynamically learns the traffic patterns from onlinesensor data and generates high-fidelity simulation results. Although currenttraffic prediction techniques based on graph neural networks have achievedstate-of-the-art performance, these techniques only predict future traffic bymining correlations in historical traffic data, disregarding the causes oftraffic generation, such as traffic demands and route selection. Therefore,their performance is unreliable for JIT decision making. To fill this gap, weintroduce a novel deep learning framework called TraffNet that learns thecausality of traffic volumes from vehicle trajectory data. First, we use aheterogeneous graph to represent the road network, allowing the model toincorporate causal features of traffic volumes. Next, motivated by the trafficdomain knowledge, we propose a traffic causality learning method to learn anembedding vector that encodes travel demands and path-level dependencies foreach road segment. Then, we model temporal dependencies to match the underlyingprocess of traffic generation. Finally, the experiments verify the utility ofTraffNet. The code of TraffNet is available athttps://github.com/mayunyi-1999/TraffNet_code.git.</description><author>Ming Xu, Yunyi Ma, Ruimin Li, Geqi Qi, Xiangfu Meng, Haibo Jin</author><pubDate>Wed, 27 Sep 2023 14:40:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.15954v4</guid></item><item><title>Generative Speech Recognition Error Correction with Large Language Models</title><link>http://arxiv.org/abs/2309.15649v1</link><description>We explore the ability of large language models (LLMs) to act as ASRpost-processors that perform rescoring and error correction. Our focus is oninstruction prompting to let LLMs perform these task without fine-tuning, forwhich we evaluate different prompting schemes, both zero- and few-shotin-context learning, and a novel task-activating prompting (TAP) method thatcombines instruction and demonstration. Using a pre-trained first-pass systemand rescoring output on two out-of-domain tasks (ATIS and WSJ), we show thatrescoring only by in-context learning with frozen LLMs achieves results thatare competitive with rescoring by domain-tuned LMs. By combining promptingtechniques with fine-tuning we achieve error rates below the N-best oraclelevel, showcasing the generalization power of the LLMs.</description><author>Chao-Han Huck Yang, Yile Gu, Yi-Chieh Liu, Shalini Ghosh, Ivan Bulyko, Andreas Stolcke</author><pubDate>Wed, 27 Sep 2023 14:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15649v1</guid></item><item><title>SANGEA: Scalable and Attributed Network Generation</title><link>http://arxiv.org/abs/2309.15648v1</link><description>The topic of synthetic graph generators (SGGs) has recently received muchattention due to the wave of the latest breakthroughs in generative modelling.However, many state-of-the-art SGGs do not scale well with the graph size.Indeed, in the generation process, all the possible edges for a fixed number ofnodes must often be considered, which scales in $\mathcal{O}(N^2)$, with $N$being the number of nodes in the graph. For this reason, many state-of-the-artSGGs are not applicable to large graphs. In this paper, we present SANGEA, asizeable synthetic graph generation framework which extends the applicabilityof any SGG to large graphs. By first splitting the large graph intocommunities, SANGEA trains one SGG per community, then links the communitygraphs back together to create a synthetic large graph. Our experiments showthat the graphs generated by SANGEA have high similarity to the original graph,in terms of both topology and node feature distribution. Additionally, thesegenerated graphs achieve high utility on downstream tasks such as linkprediction. Finally, we provide a privacy assessment of the generated graphs toshow that, even though they have excellent utility, they also achievereasonable privacy scores.</description><author>Valentin Lemaire, Youssef Achenchabe, Lucas Ody, Houssem Eddine Souid, Gianmarco Aversano, Nicolas Posocco, Sabri Skhiri</author><pubDate>Wed, 27 Sep 2023 14:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15648v1</guid></item></channel></rss>