<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 10 Mar 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed</title><link>http://arxiv.org/abs/2403.04765v1</link><description>We present a novel method for efficiently producing semi-dense matches acrossimages. Previous detector-free matcher LoFTR has shown remarkable matchingcapability in handling large-viewpoint change and texture-poor scenarios butsuffers from low efficiency. We revisit its design choices and derive multipleimprovements for both efficiency and accuracy. One key observation is thatperforming the transformer over the entire feature map is redundant due toshared local information, therefore we propose an aggregated attentionmechanism with adaptive token selection for efficiency. Furthermore, we findspatial variance exists in LoFTR's fine correlation module, which is adverse tomatching accuracy. A novel two-stage correlation layer is proposed to achieveaccurate subpixel correspondences for accuracy improvement. Our efficiencyoptimized model is $\sim 2.5\times$ faster than LoFTR which can even surpassstate-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue.Moreover, extensive experiments show that our method can achieve higheraccuracy compared with competitive semi-dense matchers, with considerableefficiency benefits. This opens up exciting prospects for large-scale orlatency-sensitive applications such as image retrieval and 3D reconstruction.Project page: https://zju3dv.github.io/efficientloftr.</description><author>Yifan Wang, Xingyi He, Sida Peng, Dongli Tan, Xiaowei Zhou</author><pubDate>Thu, 07 Mar 2024 18:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04765v1</guid></item><item><title>Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization</title><link>http://arxiv.org/abs/2403.04764v1</link><description>This paper presents a new approach for batch Bayesian Optimization (BO),where the sampling takes place by minimizing a Thompson Sampling approximationof a regret to uncertainty ratio. Our objective is able to coordinate theactions chosen in each batch in a way that minimizes redundancy between pointswhilst focusing on points with high predictive means or high uncertainty. Weprovide high-probability theoretical guarantees on the regret of our algorithm.Finally, numerically, we demonstrate that our method attains state-of-the-artperformance on a range of nonconvex test functions, where it outperformsseveral competitive benchmark batch BO algorithms by an order of magnitude onaverage.</description><author>Zhaolin Ren, Na Li</author><pubDate>Thu, 07 Mar 2024 18:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04764v1</guid></item><item><title>BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization</title><link>http://arxiv.org/abs/2403.04763v1</link><description>Bilevel optimization refers to scenarios whereby the optimal solution of alower-level energy function serves as input features to an upper-levelobjective of interest. These optimal features typically depend on tunableparameters of the lower-level energy in such a way that the entire bilevelpipeline can be trained end-to-end. Although not generally presented as such,this paper demonstrates how a variety of graph learning techniques can berecast as special cases of bilevel optimization or simplifications thereof. Inbrief, building on prior work we first derive a more flexible class of energyfunctions that, when paired with various descent steps (e.g., gradient descent,proximal methods, momentum, etc.), form graph neural network (GNN)message-passing layers; critically, we also carefully unpack where any residualapproximation error lies with respect to the underlying constituentmessage-passing functions. We then probe several simplifications of thisframework to derive close connections with non-GNN-based graph learningapproaches, including knowledge graph embeddings, various forms of labelpropagation, and efficient graph-regularized MLP models. And finally, wepresent supporting empirical results that demonstrate the versatility of theproposed bilevel lens, which we refer to as BloomGML, referencing that BiLevelOptimization Offers More Graph Machine Learning. Our code is available athttps://github.com/amberyzheng/BloomGML. Let graph ML bloom.</description><author>Amber Yijia Zheng, Tong He, Yixuan Qiu, Minjie Wang, David Wipf</author><pubDate>Thu, 07 Mar 2024 18:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04763v1</guid></item><item><title>iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries</title><link>http://arxiv.org/abs/2403.04760v1</link><description>The recent explosion in popularity of large language models (LLMs) hasinspired learning engineers to incorporate them into adaptive educational toolsthat automatically score summary writing. Understanding and evaluating LLMs isvital before deploying them in critical learning environments, yet theirunprecedented size and expanding number of parameters inhibits transparency andimpedes trust when they underperform. Through a collaborative user-centereddesign process with several learning engineers building and deploying summaryscoring LLMs, we characterized fundamental design challenges and goals aroundinterpreting their models, including aggregating large text inputs, trackingscore provenance, and scaling LLM interpretability methods. To address theirconcerns, we developed iScore, an interactive visual analytics tool forlearning engineers to upload, score, and compare multiple summariessimultaneously. Tightly integrated views allow users to iteratively revise thelanguage in summaries, track changes in the resulting LLM scores, and visualizemodel weights at multiple levels of abstraction. To validate our approach, wedeployed iScore with three learning engineers over the course of a month. Wepresent a case study where interacting with iScore led a learning engineer toimprove their LLM's score accuracy by three percentage points. Finally, weconducted qualitative interviews with the learning engineers that revealed howiScore enabled them to understand, evaluate, and build trust in their LLMsduring deployment.</description><author>Adam Coscia, Langdon Holmes, Wesley Morris, Joon Suh Choi, Scott Crossley, Alex Endert</author><pubDate>Thu, 07 Mar 2024 18:56:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04760v1</guid></item><item><title>Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing</title><link>http://arxiv.org/abs/2403.04759v1</link><description>On-device learning has emerged as a prevailing trend that avoids the slowresponse time and costly communication of cloud-based learning. The ability tolearn continuously and indefinitely in a changing environment, and withresource constraints, is critical for real sensor deployments. However,existing designs are inadequate for practical scenarios with (i) streaming datainput, (ii) lack of supervision and (iii) limited on-board resources. In thispaper, we design and deploy the first on-device lifelong learning system calledLifeHD for general IoT applications with limited supervision. LifeHD isdesigned based on a novel neurally-inspired and lightweight learning paradigmcalled Hyperdimensional Computing (HDC). We utilize a two-tier associativememory organization to intelligently store and manage high-dimensional,low-precision vectors, which represent the historical patterns as clustercentroids. We additionally propose two variants of LifeHD to cope with scarcelabeled inputs and power constraints. We implement LifeHD on off-the-shelf edgeplatforms and perform extensive evaluations across three scenarios. Ourmeasurements show that LifeHD improves the unsupervised clustering accuracy byup to 74.8% compared to the state-of-the-art NN-based unsupervised lifelonglearning baselines with as much as 34.3x better energy efficiency. Our code isavailable at https://github.com/Orienfish/LifeHD.</description><author>Xiaofan Yu, Anthony Thomas, Ivannia Gomez Moreno, Louis Gutierrez, Tajana Rosing</author><pubDate>Thu, 07 Mar 2024 18:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04759v1</guid></item><item><title>KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts</title><link>http://arxiv.org/abs/2403.04758v1</link><description>Recent growth in the popularity of large language models has led to theirincreased usage for summarizing, predicting, and generating text, making itvital to help researchers and engineers understand how and why they work. Wepresent KnowledgeVis, a human-in-the-loop visual analytics system forinterpreting language models using fill-in-the-blank sentences as prompts. Bycomparing predictions between sentences, KnowledgeVis reveals learnedassociations that intuitively connect what language models learn duringtraining to natural language tasks downstream, helping users create and testmultiple prompt variations, analyze predicted words using a novel semanticclustering technique, and discover insights using interactive visualizations.Collectively, these visualizations help users identify the likelihood anduniqueness of individual predictions, compare sets of predictions betweenprompts, and summarize patterns and relationships between predictions acrossall prompts. We demonstrate the capabilities of KnowledgeVis with feedback fromsix NLP experts as well as three different use cases: (1) probing biomedicalknowledge in two domain-adapted models; and (2) evaluating harmful identitystereotypes and (3) discovering facts and relationships between threegeneral-purpose models.</description><author>Adam Coscia, Alex Endert</author><pubDate>Thu, 07 Mar 2024 18:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04758v1</guid></item><item><title>That's My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation</title><link>http://arxiv.org/abs/2403.04755v1</link><description>This paper is about 3D pose estimation on LiDAR scans with extremely minimalstorage requirements to enable scalable mapping and localisation. We achievethis by clustering all points of segmented scans into semantic objects andrepresenting them only with their respective centroid and semantic class. Inthis way, each LiDAR scan is reduced to a compact collection of four-numbervectors. This abstracts away important structural information from the scenes,which is crucial for traditional registration approaches. To mitigate this, weintroduce an object-matching network based on self- and cross-correlation thatcaptures geometric and semantic relationships between entities. The respectivematches allow us to recover the relative transformation between scans throughweighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus(RANSAC). We demonstrate that such representation is sufficient for metriclocalisation by registering point clouds taken under different viewpoints onthe KITTI dataset, and at different periods of time localising between KITTIand KITTI-360. We achieve accurate metric estimates comparable withstate-of-the-art methods with almost half the representation size, specifically1.33 kB on average.</description><author>Georgi Pramatarov, Matthew Gadd, Paul Newman, Daniele De Martini</author><pubDate>Thu, 07 Mar 2024 18:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04755v1</guid></item><item><title>JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework</title><link>http://arxiv.org/abs/2403.04750v1</link><description>Particle-based fluid simulations have emerged as a powerful tool for solvingthe Navier-Stokes equations, especially in cases that include intricate physicsand free surfaces. The recent addition of machine learning methods to thetoolbox for solving such problems is pushing the boundary of the quality vs.speed tradeoff of such numerical simulations. In this work, we lead the way toLagrangian fluid simulators compatible with deep learning frameworks, andpropose JAX-SPH - a Smoothed Particle Hydrodynamics (SPH) framework implementedin JAX. JAX-SPH builds on the code for dataset generation from theLagrangeBench project (Toshev et al., 2023) and extends this code in multipleways: (a) integration of further key SPH algorithms, (b) restructuring the codetoward a Python library, (c) verification of the gradients through the solver,and (d) demonstration of the utility of the gradients for solving inverseproblems as well as a Solver-in-the-Loop application. Our code is available athttps://github.com/tumaer/jax-sph.</description><author>Artur P. Toshev, Harish Ramachandran, Jonas A. Erbesdobler, Gianluca Galletti, Johannes Brandstetter, Nikolaus A. Adams</author><pubDate>Thu, 07 Mar 2024 18:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04750v1</guid></item><item><title>GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks</title><link>http://arxiv.org/abs/2403.04747v1</link><description>Graph neural networks (GNNs), and especially message-passing neural networks,excel in various domains such as physics, drug discovery, and molecularmodeling. The expressivity of GNNs with respect to their ability todiscriminate non-isomorphic graphs critically depends on the functions employedfor message aggregation and graph-level readout. By applying signal propagationtheory, we propose a variance-preserving aggregation function (VPA) thatmaintains expressivity, but yields improved forward and backward dynamics.Experiments demonstrate that VPA leads to increased predictive performance forpopular GNN architectures as well as improved learning dynamics. Our resultscould pave the way towards normalizer-free or self-normalizing GNNs.</description><author>Lisa Schneckenreiter, Richard Freinschlag, Florian Sestak, Johannes Brandstetter, Günter Klambauer, Andreas Mayr</author><pubDate>Thu, 07 Mar 2024 18:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04747v1</guid></item><item><title>LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error</title><link>http://arxiv.org/abs/2403.04746v1</link><description>Tools are essential for large language models (LLMs) to acquire up-to-dateinformation and take consequential actions in external environments. Existingwork on tool-augmented LLMs primarily focuses on the broad coverage of toolsand the flexibility of adding new tools. However, a critical aspect that hassurprisingly been understudied is simply how accurately an LLM uses tools forwhich it has been trained. We find that existing LLMs, including GPT-4 andopen-source LLMs specifically fine-tuned for tool use, only reach a correctnessrate in the range of 30% to 60%, far from reliable use in practice. We proposea biologically inspired method for tool-augmented LLMs, simulated trial anderror (STE), that orchestrates three key mechanisms for successful tool usebehaviors in the biological system: trial and error, imagination, and memory.Specifically, STE leverages an LLM's 'imagination' to simulate plausiblescenarios for using a tool, after which the LLM interacts with the tool tolearn from its execution feedback. Both short-term and long-term memory areemployed to improve the depth and breadth of the exploration, respectively.Comprehensive experiments on ToolBench show that STE substantially improvestool learning for LLMs under both in-context learning and fine-tuning settings,bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperformGPT-4. We also show effective continual learning of tools via a simpleexperience replay strategy.</description><author>Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, Yu Su</author><pubDate>Thu, 07 Mar 2024 18:50:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04746v1</guid></item><item><title>SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions</title><link>http://arxiv.org/abs/2403.04744v1</link><description>We study the complexity of Non-Gaussian Component Analysis (NGCA) in theStatistical Query (SQ) model. Prior work developed a general methodology toprove SQ lower bounds for this task that have been applicable to a wide rangeof contexts. In particular, it was known that for any univariate distribution$A$ satisfying certain conditions, distinguishing between a standardmultivariate Gaussian and a distribution that behaves like $A$ in a randomhidden direction and like a standard Gaussian in the orthogonal complement, isSQ-hard. The required conditions were that (1) $A$ matches many low-ordermoments with the standard univariate Gaussian, and (2) the chi-squared norm of$A$ with respect to the standard Gaussian is finite. While the moment-matchingcondition is necessary for hardness, the chi-squared condition was onlyrequired for technical reasons. In this work, we establish that the lattercondition is indeed not necessary. In particular, we prove near-optimal SQlower bounds for NGCA under the moment-matching condition only. Our resultnaturally generalizes to the setting of a hidden subspace. Leveraging ourgeneral SQ lower bound, we obtain near-optimal SQ lower bounds for a range ofconcrete estimation tasks where existing techniques provide sub-optimal or evenvacuous guarantees.</description><author>Ilias Diakonikolas, Daniel Kane, Lisheng Ren, Yuxin Sun</author><pubDate>Thu, 07 Mar 2024 18:49:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04744v1</guid></item><item><title>I Can't Believe It's Not Scene Flow!</title><link>http://arxiv.org/abs/2403.04739v1</link><description>Current scene flow methods broadly fail to describe motion on small objects,and current scene flow evaluation protocols hide this failure by averaging overmany points, with most drawn larger objects. To fix this evaluation failure, wepropose a new evaluation protocol, Bucket Normalized EPE, which is class-awareand speed-normalized, enabling contextualized error comparisons between objecttypes that move at vastly different speeds. To highlight current methodfailures, we propose a frustratingly simple supervised scene flow baseline,TrackFlow, built by bolting a high-quality pretrained detector (trained usingmany class rebalancing techniques) onto a simple tracker, that producesstate-of-the-art performance on current standard evaluations and largeimprovements over prior art on our new evaluation. Our results make it clearthat all scene flow evaluations must be class and speed aware, and supervisedscene flow methods must address point class imbalances. We release theevaluation code publicly athttps://github.com/kylevedder/BucketedSceneFlowEval.</description><author>Ishan Khatri, Kyle Vedder, Neehar Peri, Deva Ramanan, James Hays</author><pubDate>Thu, 07 Mar 2024 18:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04739v1</guid></item><item><title>Merging Text Transformer Models from Different Initializations</title><link>http://arxiv.org/abs/2403.00986v2</link><description>Recent work on one-shot permutation-based model merging has shown impressivelow- or zero-barrier mode connectivity between models from completely differentinitializations. However, this line of work has not yet extended to theTransformer architecture, despite its dominant popularity in the languagedomain. Therefore, in this work, we investigate the extent to which separateTransformer minima learn similar features, and propose a model mergingtechnique to investigate the relationship between these minima in the losslandscape. The specifics of the architecture, like its residual connections,multi-headed attention, and discrete, sequential input, require specificinterventions in order to compute model permutations that remain within thesame functional equivalence class. In merging these models with our method, weconsistently find lower loss barriers between minima compared to modelaveraging for several models trained on a masked-language modeling task orfine-tuned on a language understanding benchmark. Our results show that theminima of these models are less sharp and isolated than previously understood,and provide a basis for future work on merging separately trained Transformermodels.</description><author>Neha Verma, Maha Elbayad</author><pubDate>Thu, 07 Mar 2024 18:45:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00986v2</guid></item><item><title>SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM</title><link>http://arxiv.org/abs/2403.04735v1</link><description>Vision-extended LLMs have made significant strides in Visual QuestionAnswering (VQA). Despite these advancements, VLLMs still encounter substantialdifficulties in handling queries involving long-tail entities, with a tendencyto produce erroneous or hallucinated responses. In this work, we introduce anovel evaluative benchmark named \textbf{SnapNTell}, specifically tailored forentity-centric VQA. This task aims to test the models' capabilities inidentifying entities and providing detailed, entity-specific knowledge. We havedeveloped the \textbf{SnapNTell Dataset}, distinct from traditional VQAdatasets: (1) It encompasses a wide range of categorized entities, eachrepresented by images and explicitly named in the answers; (2) It features QApairs that require extensive knowledge for accurate responses. The dataset isorganized into 22 major categories, containing 7,568 unique entities in total.For each entity, we curated 10 illustrative images and crafted 10knowledge-intensive QA pairs. To address this novel task, we devised ascalable, efficient, and transparent retrieval-augmented multimodal LLM. Ourapproach markedly outperforms existing methods on the SnapNTell dataset,achieving a 66.5\% improvement in the BELURT score. We will soon make thedataset and the source code publicly accessible.</description><author>Jielin Qiu, Andrea Madotto, Zhaojiang Lin, Paul A. Crook, Yifan Ethan Xu, Xin Luna Dong, Christos Faloutsos, Lei Li, Babak Damavandi, Seungwhan Moon</author><pubDate>Thu, 07 Mar 2024 18:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04735v1</guid></item><item><title>How Far Are We from Intelligent Visual Deductive Reasoning?</title><link>http://arxiv.org/abs/2403.04732v1</link><description>Vision-Language Models (VLMs) such as GPT-4V have recently demonstratedincredible strides on diverse vision language tasks. We dig into vision-baseddeductive reasoning, a more sophisticated but less explored realm, and findpreviously unexposed blindspots in the current SOTA VLMs. Specifically, weleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities toperform multi-hop relational and deductive reasoning relying solely on visualclues. We perform comprehensive evaluations of several popular VLMs employingstandard strategies such as in-context learning, self-consistency, andChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,IntelligenceTest, and RAVEN. The results reveal that despite the impressivecapabilities of LLMs in text-based reasoning, we are still far from achievingcomparable proficiency in visual deductive reasoning. We found that certainstandard strategies that are effective when applied to LLMs do not seamlesslytranslate to the challenges presented by visual reasoning tasks. Moreover, adetailed analysis reveals that VLMs struggle to solve these tasks mainlybecause they are unable to perceive and comprehend multiple, confoundingabstract patterns in RPM examples.</description><author>Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh Susskind, Navdeep Jaitly</author><pubDate>Thu, 07 Mar 2024 18:35:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04732v1</guid></item><item><title>Player Pressure Map -- A Novel Representation of Pressure in Soccer for Evaluating Player Performance in Different Game Contexts</title><link>http://arxiv.org/abs/2401.16235v2</link><description>In soccer, contextual player performance metrics are invaluable to coaches.For example, the ability to perform under pressure during matches distinguishesthe elite from the average. Appropriate pressure metric enables teams to assessplayers' performance accurately under pressure and design targeted trainingscenarios to address their weaknesses. The primary objective of this paper isto leverage both tracking and event data and game footage to capture thepressure experienced by the possession team in a soccer game scene. We proposea player pressure map to represent a given game scene, which lowers thedimension of raw data and still contains rich contextual information. Not onlydoes it serve as an effective tool for visualizing and evaluating the pressureon the team and each individual, but it can also be utilized as a backbone foraccessing players' performance. Overall, our model provides coaches andanalysts with a deeper understanding of players' performance under pressure sothat they make data-oriented tactical decisions.</description><author>Chaoyi Gu, Jiaming Na, Yisheng Pei, Varuna De Silva</author><pubDate>Thu, 07 Mar 2024 18:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16235v2</guid></item><item><title>VeCLIP: Improving CLIP Training via Visual-enriched Captions</title><link>http://arxiv.org/abs/2310.07699v2</link><description>Large-scale web-crawled datasets are fundamental for the success ofpre-training vision-language models, such as CLIP. However, the inherent noiseand potential irrelevance of web-crawled AltTexts pose challenges in achievingprecise image-text alignment. Existing methods utilizing large language models(LLMs) for caption rewriting have shown promise on small, curated datasets likeCC3M and CC12M. This study introduces a scalable pipeline for noisy captionrewriting. Unlike recent LLM rewriting techniques, we emphasize theincorporation of visual concepts into captions, termed as Visual-enrichedCaptions (VeCap). To ensure data diversity, we propose a novel mixed trainingscheme that optimizes the utilization of AltTexts alongside newly generatedVeCap. We showcase the adaptation of this method for training CLIP onlarge-scale web-crawled datasets, termed VeCLIP. Employing this cost-effectivepipeline, we effortlessly scale our dataset up to 300 million samples namedVeCap dataset. Our results show significant advantages in image-text alignmentand overall model performance. For example, VeCLIP achieves up to +25.2% gainin COCO and Flickr30k retrieval tasks under the 12M setting. For dataefficiency, VeCLIP achieves +3% gain while only using 14% of the data employedin the vanilla CLIP and 11% in ALIGN. We also note the VeCap data iscomplementary with other well curated datasets good for zero-shotclassification tasks. When combining VeCap and DFN, our model can achievestrong performance on both of image-text retrieval and zero-shot classificationtasks, e.g. 83.1% accuracy@1 on ImageNet zero-shot for a H/14 model. We releasethe pre-trained models at https://github.com/apple/ml-veclip.</description><author>Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, Meng Cao</author><pubDate>Thu, 07 Mar 2024 18:25:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07699v2</guid></item><item><title>A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation</title><link>http://arxiv.org/abs/2403.04726v1</link><description>We study the algorithmic problem of sparse mean estimation in the presence ofadversarial outliers. Specifically, the algorithm observes a \emph{corrupted}set of samples from $\mathcal{N}(\mu,\mathbf{I}_d)$, where the unknown mean$\mu \in \mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior workshas developed efficient algorithms for robust sparse mean estimation withsample complexity $\mathrm{poly}(k,\log d, 1/\epsilon)$ and runtime $d^2\mathrm{poly}(k,\log d,1/\epsilon)$, where $\epsilon$ is the fraction ofcontamination. In particular, the fastest runtime of existing algorithms isquadratic ($\Omega(d^2)$), which can be prohibitive in high dimensions. Thisquadratic barrier in the runtime stems from the reliance of these algorithms onthe sample covariance matrix, which is of size $d^2$. Our main contribution isan algorithm for robust sparse mean estimation which runs in\emph{subquadratic} time using $\mathrm{poly}(k,\log d,1/\epsilon)$ samples. Wealso provide analogous results for robust sparse PCA. Our results build onalgorithmic advances in detecting weak correlations, a generalized version ofthe light-bulb problem by Valiant.</description><author>Ankit Pensia</author><pubDate>Thu, 07 Mar 2024 18:23:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04726v1</guid></item><item><title>Masked Capsule Autoencoders</title><link>http://arxiv.org/abs/2403.04724v1</link><description>We propose Masked Capsule Autoencoders (MCAE), the first Capsule Network thatutilises pretraining in a self-supervised manner. Capsule Networks have emergedas a powerful alternative to Convolutional Neural Networks (CNNs), and haveshown favourable properties when compared to Vision Transformers (ViT), buthave struggled to effectively learn when presented with more complex data,leading to Capsule Network models that do not scale to modern tasks. Ourproposed MCAE model alleviates this issue by reformulating the Capsule Networkto use masked image modelling as a pretraining stage before finetuning in asupervised manner. Across several experiments and ablations studies wedemonstrate that similarly to CNNs and ViTs, Capsule Networks can also benefitfrom self-supervised pretraining, paving the way for further advancements inthis neural network domain. For instance, pretraining on the Imagenettedataset, a dataset of 10 classes of Imagenet-sized images, we achieve not onlystate-of-the-art results for Capsule Networks but also a 9% improvementcompared to purely supervised training. Thus we propose that Capsule Networksbenefit from and should be trained within a masked image modelling framework,with a novel capsule decoder, to improve a Capsule Network's performance onrealistic-sized images.</description><author>Miles Everett, Mingjun Zhong, Georgios Leontidis</author><pubDate>Thu, 07 Mar 2024 18:22:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04724v1</guid></item><item><title>Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices</title><link>http://arxiv.org/abs/2308.06528v2</link><description>Learning to perform abstract reasoning often requires decomposing the task inquestion into intermediate subgoals that are not specified upfront, but need tobe autonomously devised by the learner. In Raven Progressive Matrices (RPM),the task is to choose one of the available answers given a context, where boththe context and answers are composite images featuring multiple objects invarious spatial arrangements. As this high-level goal is the only guidanceavailable, learning to solve RPMs is challenging. In this study, we propose adeep learning architecture based on the transformer blueprint which, ratherthan directly making the above choice, addresses the subgoal of predicting thevisual properties of individual objects and their arrangements. Themultidimensional predictions obtained in this way are then directly juxtaposedto choose the answer. We consider a few ways in which the model parses thevisual input into tokens and several regimes of masking parts of the input inself-supervised training. In experimental assessment, the models not onlyoutperform state-of-the-art methods but also provide interesting insights andpartial explanations about the inference. The design of the method also makesit immune to biases that are known to be present in some RPM benchmarks.</description><author>Jakub Kwiatkowski, Krzysztof Krawiec</author><pubDate>Thu, 07 Mar 2024 18:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06528v2</guid></item><item><title>Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization</title><link>http://arxiv.org/abs/2403.04720v1</link><description>Effectively representing heterogeneous tabular datasets for meta-learningremains an open problem. Previous approaches rely on predefined meta-features,for example, statistical measures or landmarkers. Encoder-based models, such asDataset2Vec, allow us to extract significant meta-features automaticallywithout human intervention. This research introduces a novel encoder-basedrepresentation of tabular datasets implemented within the liltab packageavailable on GitHub https://github.com/azoz01/liltab. Our package is based onan established model for heterogeneous tabular data proposed in [Iwata andKumagai, 2020]. The proposed approach employs a different model for encodingfeature relationships, generating alternative representations compared toexisting methods like Dataset2Vec. Both of them leverage the fundamentalassumption of dataset similarity learning. In this work, we evaluateDataset2Vec and liltab on two common meta-tasks - representing entire datasetsand hyperparameter optimization warm-start. However, validation on anindependent metaMIMIC dataset highlights the nuanced challenges inrepresentation learning. We show that general representations may not sufficefor some meta-tasks where requirements are not explicitly considered duringextraction. [Iwata and Kumagai, 2020] Tomoharu Iwata and Atsutoshi Kumagai. Meta-learningfrom Tasks with Heterogeneous Attribute Spaces. In Advances in NeuralInformation Processing Systems, 2020.</description><author>Dawid Płudowski, Antoni Zajko, Anna Kozak, Katarzyna Woźnica</author><pubDate>Thu, 07 Mar 2024 18:16:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04720v1</guid></item><item><title>Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months</title><link>http://arxiv.org/abs/2310.18191v2</link><description>We analyze VeLO (versatile learned optimizer), the largest scale attempt totrain a general purpose "foundational" optimizer to date. VeLO was trained onthousands of machine learning tasks using over 4000 TPU months with the goal ofproducing an optimizer capable of generalizing to new problems while beinghyperparameter free, and outperforming industry standards such as Adam. Weindependently evaluate VeLO on the MLCommons optimizer benchmark suite. We findthat, contrary to initial claims: (1) VeLO has a critical hyperparameter thatneeds problem-specific tuning, (2) VeLO does not necessarily outperformcompetitors in quality of solution found, and (3) VeLO is not faster thancompeting optimizers at reducing the training loss. These observations callinto question VeLO's generality and the value of the investment in training it.</description><author>Fady Rezk, Antreas Antoniou, Henry Gouk, Timothy Hospedales</author><pubDate>Thu, 07 Mar 2024 18:10:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18191v2</guid></item><item><title>When Machine Learning Models Leak: An Exploration of Synthetic Training Data</title><link>http://arxiv.org/abs/2310.08775v2</link><description>We investigate an attack on a machine learning model that predicts whether aperson or household will relocate in the next two years, i.e., apropensity-to-move classifier. The attack assumes that the attacker can querythe model to obtain predictions and that the marginal distribution of the dataon which the model was trained is publicly available. The attack also assumesthat the attacker has obtained the values of non-sensitive attributes for acertain number of target individuals. The objective of the attack is to inferthe values of sensitive attributes for these target individuals. We explore howreplacing the original data with synthetic data when training the model impactshow successfully the attacker can infer sensitive attributes.</description><author>Manel Slokom, Peter-Paul de Wolf, Martha Larson</author><pubDate>Thu, 07 Mar 2024 18:09:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08775v2</guid></item><item><title>Common 7B Language Models Already Possess Strong Math Capabilities</title><link>http://arxiv.org/abs/2403.04706v1</link><description>Mathematical capabilities were previously believed to emerge in commonlanguage models only at a very large scale or require extensive math-relatedpre-training. This paper shows that the LLaMA-2 7B model with commonpre-training already exhibits strong mathematical abilities, as evidenced byits impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks,respectively, when selecting the best response from 256 random generations. Theprimary issue with the current base model is the difficulty in consistentlyeliciting its inherent mathematical capabilities. Notably, the accuracy for thefirst answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks,respectively. We find that simply scaling up the SFT data can significantlyenhance the reliability of generating correct answers. However, the potentialfor extensive scaling is constrained by the scarcity of publicly available mathquestions. To overcome this limitation, we employ synthetic data, which provesto be nearly as effective as real data and shows no clear saturation whenscaled up to approximately one million samples. This straightforward approachachieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7Bmodels, surpassing previous models by 14.2% and 20.8%, respectively. We alsoprovide insights into scaling behaviors across different reasoning complexitiesand error types.</description><author>Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, Houwen Peng</author><pubDate>Thu, 07 Mar 2024 18:00:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04706v1</guid></item><item><title>Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework</title><link>http://arxiv.org/abs/2307.01715v3</link><description>Connectionist Temporal Classification (CTC) is a widely used criterion fortraining supervised sequence-to-sequence (seq2seq) models. It enables learningthe relations between input and output sequences, termed alignments, bymarginalizing over perfect alignments (that yield the ground truth), at theexpense of imperfect alignments. This binary differentiation of perfect andimperfect alignments falls short of capturing other essential alignmentproperties that hold significance in other real-world applications. Here wepropose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Playframework}$ for enhancing a desired property in models trained with the CTCcriterion. We do that by complementing the CTC with an additional loss termthat prioritizes alignments according to a desired property. Our method doesnot require any intervention in the CTC loss function, enables easyoptimization of a variety of properties, and allows differentiation betweenboth perfect and imperfect alignments. We apply our framework in the domain ofAutomatic Speech Recognition (ASR) and show its generality in terms of propertyselection, architectural choice, and scale of training dataset (up to 280,000hours). To demonstrate the effectiveness of our framework, we apply it to twounrelated properties: emission time and word error rate (WER). For the former,we report an improvement of up to 570ms in latency optimization with a minorreduction in WER, and for the latter, we report a relative improvement of 4.5%WER over the baseline models. To the best of our knowledge, these applicationshave never been demonstrated to work on a scale of data as large as ours.Notably, our method can be implemented using only a few lines of code, and canbe extended to other alignment-free loss functions and to domains other thanASR.</description><author>Eliya Segev, Maya Alroy, Ronen Katsir, Noam Wies, Ayana Shenhav, Yael Ben-Oren, David Zar, Oren Tadmor, Jacob Bitterman, Amnon Shashua, Tal Rosenwein</author><pubDate>Thu, 07 Mar 2024 17:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01715v3</guid></item><item><title>On the expressivity of bi-Lipschitz normalizing flows</title><link>http://arxiv.org/abs/2107.07232v3</link><description>An invertible function is bi-Lipschitz if both the function and its inversehave bounded Lipschitz constants. Nowadays, most Normalizing Flows arebi-Lipschitz by design or by training to limit numerical errors (among otherthings). In this paper, we discuss the expressivity of bi-Lipschitz NormalizingFlows and identify several target distributions that are difficult toapproximate using such models. Then, we characterize the expressivity ofbi-Lipschitz Normalizing Flows by giving several lower bounds on the TotalVariation distance between these particularly unfavorable distributions andtheir best possible approximation. Finally, we discuss potential remedies whichinclude using more complex latent distributions.</description><author>Alexandre Verine, Benjamin Negrevergne, Fabrice Rossi, Yann Chevaleyre</author><pubDate>Thu, 07 Mar 2024 17:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.07232v3</guid></item><item><title>Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks</title><link>http://arxiv.org/abs/2309.16347v2</link><description>Current reinforcement learning algorithms struggle in sparse and complexenvironments, most notably in long-horizon manipulation tasks entailing aplethora of different sequences. In this work, we propose the IntrinsicallyGuided Exploration from Large Language Models (IGE-LLMs) framework. Byleveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides theexploratory process in reinforcement learning to address intricate long-horizonwith sparse rewards robotic manipulation tasks. We evaluate our framework andrelated intrinsic learning methods in an environment challenged withexploration, and a complex robotic manipulation task challenged by bothexploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higherperformance over related intrinsic methods and the direct use of LLMs indecision-making, (ii) can be combined and complement existing learning methodshighlighting its modularity, (iii) are fairly insensitive to differentintrinsic scaling parameters, and (iv) maintain robustness against increasedlevels of uncertainty and horizons.</description><author>Eleftherios Triantafyllidis, Filippos Christianos, Zhibin Li</author><pubDate>Thu, 07 Mar 2024 17:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16347v2</guid></item><item><title>ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes</title><link>http://arxiv.org/abs/2403.04701v1</link><description>Given the large-scale multi-modal training of recent vision-based models andtheir generalization capabilities, understanding the extent of their robustnessis critical for their real-world deployment. In this work, we evaluate theresilience of current vision-based models against diverse object-to-backgroundcontext variations. The majority of robustness evaluation methods haveintroduced synthetic datasets to induce changes to object characteristics(viewpoints, scale, color) or utilized image transformation techniques(adversarial changes, common corruptions) on real images to simulate shifts indistributions. Recent works have explored leveraging large language models anddiffusion models to generate changes in the background. However, these methodseither lack in offering control over the changes to be made or distort theobject semantics, making them unsuitable for the task. Our method, on the otherhand, can induce diverse object-to-background changes while preserving theoriginal semantics and appearance of the object. To achieve this goal, weharness the generative capabilities of text-to-image, image-to-text, andimage-to-segment models to automatically generate a broad spectrum ofobject-to-background changes. We induce both natural and adversarial backgroundchanges by either modifying the textual prompts or optimizing the latents andtextual embedding of text-to-image models. This allows us to quantify the roleof background context in understanding the robustness and generalization ofdeep neural networks. We produce various versions of standard vision datasets(ImageNet, COCO), incorporating either diverse and realistic backgrounds intothe images or introducing color, texture, and adversarial changes in thebackground. We conduct extensive experiment to analyze the robustness ofvision-based models against object-to-background context variations acrossdiverse tasks.</description><author>Hashmat Shadab Malik, Muhammad Huzaifa, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan</author><pubDate>Thu, 07 Mar 2024 17:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04701v1</guid></item><item><title>Delving into the Trajectory Long-tail Distribution for Muti-object Tracking</title><link>http://arxiv.org/abs/2403.04700v1</link><description>Multiple Object Tracking (MOT) is a critical area within computer vision,with a broad spectrum of practical implementations. Current research hasprimarily focused on the development of tracking algorithms and enhancement ofpost-processing techniques. Yet, there has been a lack of thorough examinationconcerning the nature of tracking data it self. In this study, we pioneer anexploration into the distribution patterns of tracking data and identify apronounced long-tail distribution issue within existing MOT datasets. We note asignificant imbalance in the distribution of trajectory lengths acrossdifferent pedestrians, a phenomenon we refer to as "pedestrians trajectorylong-tail distribution". Addressing this challenge, we introduce a bespokestrategy designed to mitigate the effects of this skewed distribution.Specifically, we propose two data augmentation strategies, including StationaryCamera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation(DVA) , designed for viewpoint states and the Group Softmax (GS) module forRe-ID. SVA is to backtrack and predict the pedestrian trajectory of tailclasses, and DVA is to use diffusion model to change the background of thescene. GS divides the pedestrians into unrelated groups and performs softmaxoperation on each group individually. Our proposed strategies can be integratedinto numerous existing tracking systems, and extensive experimentationvalidates the efficacy of our method in reducing the influence of long-taildistribution on multi-object tracking performance. The code is available athttps://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.</description><author>Sijia Chen, En Yu, Jinyang Li, Wenbing Tao</author><pubDate>Thu, 07 Mar 2024 17:48:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04700v1</guid></item><item><title>Implicit regularization of multi-task learning and finetuning in overparameterized neural networks</title><link>http://arxiv.org/abs/2310.02396v2</link><description>In this work, we investigate the inductive biases that result from learningmultiple tasks, either simultaneously (multi-task learning, MTL) orsequentially (pretraining and subsequent finetuning, PT+FT). In the simplifiedsetting of two-layer diagonal linear networks trained with gradient descent, weapply prior theoretical results to describe novel implicit regularizationpenalties associated with MTL and PT+FT, both of which incentivize featuresharing between tasks and sparsity in learned task-specific features. Notably,these results imply that during finetuning, networks operate in a hybrid of thekernel (or "lazy") regime and the feature learning ("rich") regime identifiedin prior work. Moreover, we show that PT+FT can exhibit a novel "nested featureselection" behavior not captured by either regime, which biases it to extract asparse subset of the features learned during pretraining. In ReLU networks, wereproduce all of these qualitative behaviors empirically, in particularverifying that analogues of the sparsity biases predicted by the linear theoryhold in the nonlinear case. Our findings hold qualitatively for a deeparchitecture trained on image classification tasks, and our characterization ofthe nested feature selection regime motivates a modification to PT+FT that wefind empirically improves performance. We also observe that PT+FT (but not MTL)is biased to learn features that are correlated with (but distinct from) thoseneeded for the auxiliary task, while MTL is biased toward using identicalfeatures for both tasks, which can lead to a tradeoff in performance as afunction of the number of finetuning samples. Our results shed light on theimpact of auxiliary task learning and suggest ways to leverage it moreeffectively.</description><author>Jack W. Lindsey, Samuel Lippl</author><pubDate>Thu, 07 Mar 2024 17:47:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02396v2</guid></item><item><title>AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors</title><link>http://arxiv.org/abs/2403.04697v1</link><description>Facial Action Units (AU) is a vital concept in the realm of affectivecomputing, and AU detection has always been a hot research topic. Existingmethods suffer from overfitting issues due to the utilization of a large numberof learnable parameters on scarce AU-annotated datasets or heavy reliance onsubstantial additional relevant data. Parameter-Efficient Transfer Learning(PETL) provides a promising paradigm to address these challenges, whereas itsexisting methods lack design for AU characteristics. Therefore, we innovativelyinvestigate PETL paradigm to AU detection, introducing AUFormer and proposing anovel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individualMoKE specific to a certain AU with minimal learnable parameters firstintegrates personalized multi-scale and correlation knowledge. Then the MoKEcollaborates with other MoKEs in the expert group to obtain aggregatedinformation and inject it into the frozen Vision Transformer (ViT) to achieveparameter-efficient AU detection. Additionally, we design a Margin-truncatedDifficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage themodel to focus more on activated AUs, differentiate the difficulty ofunactivated AUs, and discard potential mislabeled samples. Extensiveexperiments from various perspectives, including within-domain, cross-domain,data efficiency, and micro-expression domain, demonstrate AUFormer'sstate-of-the-art performance and robust generalization abilities withoutrelying on additional relevant data. The code for AUFormer is available athttps://github.com/yuankaishen2001/AUFormer.</description><author>Kaishen Yuan, Zitong Yu, Xin Liu, Weicheng Xie, Huanjing Yue, Jingyu Yang</author><pubDate>Thu, 07 Mar 2024 17:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04697v1</guid></item><item><title>Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population</title><link>http://arxiv.org/abs/2401.14512v2</link><description>Randomized controlled trials (RCTs) serve as the cornerstone forunderstanding causal effects, yet extending inferences to target populationspresents challenges due to effect heterogeneity and underrepresentation. Ourpaper addresses the critical issue of identifying and characterizingunderrepresented subgroups in RCTs, proposing a novel framework for refiningtarget populations to improve generalizability. We introduce anoptimization-based approach, Rashomon Set of Optimal Trees (ROOT), tocharacterize underrepresented groups. ROOT optimizes the target subpopulationdistribution by minimizing the variance of the target average treatment effectestimate, ensuring more precise treatment effect estimations. Notably, ROOTgenerates interpretable characteristics of the underrepresented population,aiding researchers in effective communication. Our approach demonstratesimproved precision and interpretability compared to alternatives, asillustrated with synthetic data experiments. We apply our methodology to extendinferences from the Starting Treatment with Agonist Replacement Therapies(START) trial -- investigating the effectiveness of medication for opioid usedisorder -- to the real-world population represented by the Treatment EpisodeDataset: Admissions (TEDS-A). By refining target populations using ROOT, ourframework offers a systematic approach to enhance decision-making accuracy andinform future trials in diverse populations.</description><author>Harsh Parikh, Rachael Ross, Elizabeth Stuart, Kara Rudolph</author><pubDate>Thu, 07 Mar 2024 17:45:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14512v2</guid></item><item><title>Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification</title><link>http://arxiv.org/abs/2403.04696v1</link><description>Large language models (LLMs) are notorious for hallucinating, i.e., producingerroneous claims in their output. Such hallucinations can be dangerous, asoccasional factual inaccuracies in the generated text might be obscured by therest of the output being generally factual, making it extremely hard for theusers to spot them. Current services that leverage LLMs usually do not provideany means for detecting unreliable generations. Here, we aim to bridge thisgap. In particular, we propose a novel fact-checking and hallucinationdetection pipeline based on token-level uncertainty quantification. Uncertaintyscores leverage information encapsulated in the output of a neural network orits layers to detect unreliable predictions, and we show that they can be usedto fact-check the atomic claims in the LLM output. Moreover, we present a noveltoken-level uncertainty quantification method that removes the impact ofuncertainty about what claim to generate on the current step and what surfaceform to use. Our method Claim Conditioned Probability (CCP) measures only theuncertainty of particular claim value expressed by the model. Experiments onthe task of biography generation demonstrate strong improvements for CCPcompared to the baselines for six different LLMs and three languages. Humanevaluation reveals that the fact-checking pipeline based on uncertaintyquantification is competitive with a fact-checking tool that leverages externalknowledge.</description><author>Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov</author><pubDate>Thu, 07 Mar 2024 17:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04696v1</guid></item><item><title>Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data</title><link>http://arxiv.org/abs/2403.03309v2</link><description>Visual understanding and segmentation of materials and their states isfundamental for understanding the physical world. The infinite textures, shapesand often blurry boundaries formed by material make this task particularly hardto generalize. Whether it's identifying wet regions of a surface, minerals inrocks, infected regions in plants, or pollution in water, each material statehas its own unique form. For neural nets to learn class-agnostic materialssegmentation it is necessary to first collect and annotate data that capturethis complexity. Collecting real-world images and manually annotating islimited both by the cost and limited precision of manual labor. In contrast,synthetic data is highly accurate and almost cost-free but fails to replicatethe vast diversity of the material world. In this work, we suggest a method tobridge this crucial gap, by implanting patterns extracted from real-worldimages, in synthetic data. Hence, patterns automatically collected from naturalimages are used to map materials into synthetic scenes. This unsupervisedapproach allows the generated data to capture the vast complexity of the realworld while maintaining the precision and scale of synthetic data. We alsopresent the first general benchmark for class-agnostic material statesegmentation. The benchmark images contain a wide range of real-world images ofmaterial states, from cooking, food, rocks, construction, plants, and liquidseach in various states(wet/dry/stained/cooked/burned/worned/rusted/sediment/foam...). The annotationincludes both partial similarity between regions with similar but not identicalmaterials, and hard segmentation of only points of the exact same materialstate. We show that net trains on MatSeg significantly outperform existingstate-of-the-art methods on this task.</description><author>Sagi Eppel, Jolina Li, Manuel Drehwald, Alan Aspuru-Guzik</author><pubDate>Thu, 07 Mar 2024 17:43:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03309v2</guid></item><item><title>GenTKG: Generative Forecasting on Temporal Knowledge Graph</title><link>http://arxiv.org/abs/2310.07793v3</link><description>The rapid advancements in large language models (LLMs) have ignited interestin the temporal knowledge graph (tKG) domain, where conventionalembedding-based and rule-based methods dominate. The question remains open ofwhether pre-trained LLMs can understand structured temporal relational data andreplace them as the foundation model for temporal relational forecasting.Therefore, we bring temporal knowledge forecasting into the generative setting.However, challenges occur in the huge chasms between complex temporal graphdata structure and sequential natural expressions LLMs can handle, and betweenthe enormous data sizes of tKGs and heavy computation costs of finetuning LLMs.To address these challenges, we propose a novel retrieval-augmented generationframework named GenTKG combining a temporal logical rule-based retrievalstrategy and few-shot parameter-efficient instruction tuning to solve the abovechallenges, respectively. Extensive experiments have shown that GenTKGoutperforms conventional methods of temporal relational forecasting with lowcomputation resources using extremely limited training data as few as 16samples. GenTKG also highlights remarkable cross-domain generalizability withoutperforming performance on unseen datasets without re-training, and in-domaingeneralizability regardless of time split in the same dataset. Our work revealsthe huge potential of LLMs in the tKG domain and opens a new frontier forgenerative forecasting on tKGs.</description><author>Ruotong Liao, Xu Jia, Yunpu Ma, Yangzhe Li, Volker Tresp</author><pubDate>Thu, 07 Mar 2024 17:43:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07793v3</guid></item><item><title>Analysis of Systems' Performance in Natural Language Processing Competitions</title><link>http://arxiv.org/abs/2403.04693v1</link><description>Collaborative competitions have gained popularity in the scientific andtechnological fields. These competitions involve defining tasks, selectingevaluation scores, and devising result verification methods. In the standardscenario, participants receive a training set and are expected to provide asolution for a held-out dataset kept by organizers. An essential challenge fororganizers arises when comparing algorithms' performance, assessing multipleparticipants, and ranking them. Statistical tools are often used for thispurpose; however, traditional statistical methods often fail to capturedecisive differences between systems' performance. This manuscript describes anevaluation methodology for statistically analyzing competition results andcompetition. The methodology is designed to be universally applicable; however,it is illustrated using eight natural language competitions as case studiesinvolving classification and regression problems. The proposed methodologyoffers several advantages, including off-the-shell comparisons with correctionmechanisms and the inclusion of confidence intervals. Furthermore, we introducemetrics that allow organizers to assess the difficulty of competitions. Ouranalysis shows the potential usefulness of our methodology for effectivelyevaluating competition results.</description><author>Sergio Nava-Muñoz, Mario Graff, Hugo Jair Escalante</author><pubDate>Thu, 07 Mar 2024 17:42:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04693v1</guid></item><item><title>PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation</title><link>http://arxiv.org/abs/2403.04692v1</link><description>In this paper, we introduce PixArt-\Sigma, a Diffusion Transformermodel~(DiT) capable of directly generating images at 4K resolution.PixArt-\Sigma represents a significant advancement over its predecessor,PixArt-\alpha, offering images of markedly higher fidelity and improvedalignment with text prompts. A key feature of PixArt-\Sigma is its trainingefficiency. Leveraging the foundational pre-training of PixArt-\alpha, itevolves from the `weaker' baseline to a `stronger' model via incorporatinghigher quality data, a process we term "weak-to-strong training". Theadvancements in PixArt-\Sigma are twofold: (1) High-Quality Training Data:PixArt-\Sigma incorporates superior-quality image data, paired with moreprecise and detailed image captions. (2) Efficient Token Compression: wepropose a novel attention module within the DiT framework that compresses bothkeys and values, significantly improving efficiency and facilitatingultra-high-resolution image generation. Thanks to these improvements,PixArt-\Sigma achieves superior image quality and user prompt adherencecapabilities with significantly smaller model size (0.6B parameters) thanexisting text-to-image diffusion models, such as SDXL (2.6B parameters) and SDCascade (5.1B parameters). Moreover, PixArt-\Sigma's capability to generate 4Kimages supports the creation of high-resolution posters and wallpapers,efficiently bolstering the production of high-quality visual content inindustries such as film and gaming.</description><author>Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, Zhenguo Li</author><pubDate>Thu, 07 Mar 2024 17:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04692v1</guid></item><item><title>Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level</title><link>http://arxiv.org/abs/2403.04690v1</link><description>Neighborhood attention reduces the cost of self attention by restricting eachtoken's attention span to its nearest neighbors. This restriction,parameterized by a window size and dilation factor, draws a spectrum ofpossible attention patterns between linear projection and self attention.Neighborhood attention, and more generally sliding window attention patterns,have long been bounded by infrastructure, particularly in higher-rank spaces(2-D and 3-D), calling for the development of custom kernels, which have beenlimited in either functionality, or performance, if not both. In this work, wefirst show that neighborhood attention can be represented as a batched GEMMproblem, similar to standard attention, and implement it for 1-D and 2-Dneighborhood attention. These kernels on average provide 895% and 272%improvement in full precision latency compared to existing naive kernels for1-D and 2-D neighborhood attention respectively. We find certain inherentinefficiencies in all unfused neighborhood attention kernels that bound theirperformance and lower-precision scalability. We also developed fusedneighborhood attention; an adaptation of fused dot-product attention kernelsthat allow fine-grained control over attention across different spatial axes.Known for reducing the quadratic time complexity of self attention to a linearcomplexity, neighborhood attention can now enjoy a reduced and constant memoryfootprint, and record-breaking half precision latency. We observe that ourfused kernels successfully circumvent some of the unavoidable inefficiencies inunfused implementations. While our unfused GEMM-based kernels only improve halfprecision performance compared to naive kernels by an average of 496% and 113%in 1-D and 2-D problems respectively, our fused kernels improve naive kernelsby an average of 1607% and 581% in 1-D and 2-D problems respectively.</description><author>Ali Hassani, Wen-Mei Hwu, Humphrey Shi</author><pubDate>Thu, 07 Mar 2024 17:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04690v1</guid></item><item><title>RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations</title><link>http://arxiv.org/abs/2309.17182v2</link><description>COMpression with Bayesian Implicit NEural Representations (COMBINER) is arecent data compression method that addresses a key inefficiency of previousImplicit Neural Representation (INR)-based approaches: it avoids quantizationand enables direct optimization of the rate-distortion performance. However,COMBINER still has significant limitations: 1) it uses factorized priors andposterior approximations that lack flexibility; 2) it cannot effectively adaptto local deviations from global patterns in the data; and 3) its performancecan be susceptible to modeling choices and the variational parameters'initializations. Our proposed method, Robust and Enhanced COMBINER(RECOMBINER), addresses these issues by 1) enriching the variationalapproximation while retaining a low computational cost via a linearreparameterization of the INR weights, 2) augmenting our INRs with learnablepositional encodings that enable them to adapt to local details and 3)splitting high-resolution data into patches to increase robustness andutilizing expressive hierarchical priors to capture dependency across patches.We conduct extensive experiments across several data modalities, showcasingthat RECOMBINER achieves competitive results with the best INR-based methodsand even outperforms autoencoder-based codecs on low-resolution images at lowbitrates. Our PyTorch implementation is available athttps://github.com/cambridge-mlg/RECOMBINER/.</description><author>Jiajun He, Gergely Flamich, Zongyu Guo, José Miguel Hernández-Lobato</author><pubDate>Thu, 07 Mar 2024 17:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17182v2</guid></item><item><title>Policy Gradient Methods in the Presence of Symmetries and State Abstractions</title><link>http://arxiv.org/abs/2305.05666v2</link><description>Reinforcement learning (RL) on high-dimensional and complex problems relieson abstraction for improved efficiency and generalization. In this paper, westudy abstraction in the continuous-control setting, and extend the definitionof Markov decision process (MDP) homomorphisms to the setting of continuousstate and action spaces. We derive a policy gradient theorem on the abstractMDP for both stochastic and deterministic policies. Our policy gradient resultsallow for leveraging approximate symmetries of the environment for policyoptimization. Based on these theorems, we propose a family of actor-criticalgorithms that are able to learn the policy and the MDP homomorphism mapsimultaneously, using the lax bisimulation metric. Finally, we introduce aseries of environments with continuous symmetries to further demonstrate theability of our algorithm for action abstraction in the presence of suchsymmetries. We demonstrate the effectiveness of our method on our environments,as well as on challenging visual control tasks from the DeepMind Control Suite.Our method's ability to utilize MDP homomorphisms for representation learningleads to improved performance, and the visualizations of the latent spaceclearly demonstrate the structure of the learned abstraction.</description><author>Prakash Panangaden, Sahand Rezaei-Shoshtari, Rosie Zhao, David Meger, Doina Precup</author><pubDate>Thu, 07 Mar 2024 17:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05666v2</guid></item><item><title>Regression modelling of spatiotemporal extreme U.S. wildfires via partially-interpretable neural networks</title><link>http://arxiv.org/abs/2208.07581v4</link><description>Risk management in many environmental settings requires an understanding ofthe mechanisms that drive extreme events. Useful metrics for quantifying suchrisk are extreme quantiles of response variables conditioned on predictorvariables that describe, e.g., climate, biosphere and environmental states.Typically these quantiles lie outside the range of observable data and so, forestimation, require specification of parametric extreme value models within aregression framework. Classical approaches in this context utilise linear oradditive relationships between predictor and response variables and suffer ineither their predictive capabilities or computational efficiency; moreover,their simplicity is unlikely to capture the truly complex structures that leadto the creation of extreme wildfires. In this paper, we propose a newmethodological framework for performing extreme quantile regression usingartificial neutral networks, which are able to capture complex non-linearrelationships and scale well to high-dimensional data. The "black box" natureof neural networks means that they lack the desirable trait of interpretabilityoften favoured by practitioners; thus, we unify linear, and additive,regression methodology with deep learning to create partially-interpretableneural networks that can be used for statistical inference but retain highprediction accuracy. To complement this methodology, we further propose a novelpoint process model for extreme values which overcomes the finitelower-endpoint problem associated with the generalised extreme value class ofdistributions. Efficacy of our unified framework is illustrated on U.S.wildfire data with a high-dimensional predictor set and we illustrate vastimprovements in predictive performance over linear and spline-based regressiontechniques.</description><author>Jordan Richards, Raphaël Huser</author><pubDate>Thu, 07 Mar 2024 17:23:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.07581v4</guid></item><item><title>English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts</title><link>http://arxiv.org/abs/2402.03223v4</link><description>Emotion classification in text is a challenging task due to the processesinvolved when interpreting a textual description of a potential emotionstimulus. In addition, the set of emotion categories is highly domain-specific.For instance, literature analysis might require the use of aesthetic emotions(e.g., finding something beautiful), and social media analysis could benefitfrom fine-grained sets (e.g., separating anger from annoyance) than only thosethat represent basic categories as they have been proposed by Paul Ekman(anger, disgust, fear, joy, surprise, sadness). This renders the task aninteresting field for zero-shot classifications, in which the label set is notknown at model development time. Unfortunately, most resources for emotionanalysis are English, and therefore, most studies on emotion analysis have beenperformed in English, including those that involve prompting language modelsfor text labels. This leaves us with a research gap that we address in thispaper: In which language should we prompt for emotion labels on non-Englishtexts? This is particularly of interest when we have access to a multilinguallarge language model, because we could request labels with English prompts evenfor non-English data. Our experiments with natural language inference-basedlanguage models show that it is consistently better to use English prompts evenif the data is in a different language.</description><author>Patrick Bareiß, Roman Klinger, Jeremy Barnes</author><pubDate>Thu, 07 Mar 2024 17:18:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03223v4</guid></item><item><title>Greater than the sum of its parts: The role of minority and majority status in collaborative problem-solving communication</title><link>http://arxiv.org/abs/2403.04671v1</link><description>Collaborative problem-solving (CPS) is a vital skill used both in theworkplace and in educational environments. CPS is useful in tacklingincreasingly complex global, economic, and political issues and is considered acentral 21st century skill. The increasingly connected global communitypresents a fruitful opportunity for creative and collaborative problem-solvinginteractions and solutions that involve diverse perspectives. Unfortunately,women and underrepresented minorities (URMs) often face obstacles duringcollaborative interactions that hinder their key participation in theseproblem-solving conversations. Here, we explored the communication patterns ofminority and non-minority individuals working together in a CPS task. GroupCommunication Analysis (GCA), a temporally-sensitive computational linguistictool, was used to examine how URM status impacts individuals' sociocognitivelinguistic patterns. Results show differences across racial/ethnic groups inkey sociocognitive features that indicate fruitful collaborative interactions.We also investigated how the groups' racial/ethnic composition impacts bothindividual and group communication patterns. In general, individuals in moredemographically diverse groups displayed more productive communicationbehaviors than individuals who were in majority-dominated groups. We discussthe implications of individual and group diversity on communication patternsthat emerge during CPS and how these patterns can impact collaborativeoutcomes.</description><author>Jacqueline G. Cavazos, Nia Nixon</author><pubDate>Thu, 07 Mar 2024 17:17:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04671v1</guid></item><item><title>End-to-end Conditional Robust Optimization</title><link>http://arxiv.org/abs/2403.04670v1</link><description>The field of Contextual Optimization (CO) integrates machine learning andoptimization to solve decision making problems under uncertainty. Recently, arisk sensitive variant of CO, known as Conditional Robust Optimization (CRO),combines uncertainty quantification with robust optimization in order topromote safety and reliability in high stake applications. Exploiting moderndifferentiable optimization methods, we propose a novel end-to-end approach totrain a CRO model in a way that accounts for both the empirical risk of theprescribed decisions and the quality of conditional coverage of the contextualuncertainty set that supports them. While guarantees of success for the latterobjective are impossible to obtain from the point of view of conformalprediction theory, high quality conditional coverage is achieved empirically byingeniously employing a logistic regression differentiable layer within thecalculation of coverage quality in our training loss. We show that the proposedtraining algorithms produce decisions that outperform the traditional estimatethen optimize approaches.</description><author>Abhilash Chenreddy, Erick Delage</author><pubDate>Thu, 07 Mar 2024 17:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04670v1</guid></item><item><title>The Social Impact of Generative AI: An Analysis on ChatGPT</title><link>http://arxiv.org/abs/2403.04667v1</link><description>In recent months, the social impact of Artificial Intelligence (AI) hasgained considerable public interest, driven by the emergence of Generative AImodels, ChatGPT in particular. The rapid development of these models hassparked heated discussions regarding their benefits, limitations, andassociated risks. Generative models hold immense promise across multipledomains, such as healthcare, finance, and education, to cite a few, presentingdiverse practical applications. Nevertheless, concerns about potential adverseeffects have elicited divergent perspectives, ranging from privacy risks toescalating social inequality. This paper adopts a methodology to delve into thesocietal implications of Generative AI tools, focusing primarily on the case ofChatGPT. It evaluates the potential impact on several social sectors andillustrates the findings of a comprehensive literature review of both positiveand negative effects, emerging trends, and areas of opportunity of GenerativeAI models. This analysis aims to facilitate an in-depth discussion by providinginsights that can inspire policy, regulation, and responsible developmentpractices to foster a human-centered AI.</description><author>Maria T. Baldassarre, Danilo Caivano, Berenice Fernandez Nieto, Domenico Gigante, Azzurra Ragone</author><pubDate>Thu, 07 Mar 2024 17:14:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04667v1</guid></item><item><title>Telecom Language Models: Must They Be Large?</title><link>http://arxiv.org/abs/2403.04666v1</link><description>The increasing interest in Large Language Models (LLMs) within thetelecommunications sector underscores their potential to revolutionizeoperational efficiency. However, the deployment of these sophisticated modelsis often hampered by their substantial size and computational demands, raisingconcerns about their viability in resource-constrained environments. Addressingthis challenge, recent advancements have seen the emergence of small languagemodels that surprisingly exhibit performance comparable to their largercounterparts in many tasks, such as coding and common-sense reasoning. Phi-2, acompact yet powerful model, exemplifies this new wave of efficient smalllanguage models. This paper conducts a comprehensive evaluation of Phi-2'sintrinsic understanding of the telecommunications domain. Recognizing thescale-related limitations, we enhance Phi-2's capabilities through aRetrieval-Augmented Generation approach, meticulously integrating an extensiveknowledge base specifically curated with telecom standard specifications. Theenhanced Phi-2 model demonstrates a profound improvement in accuracy, answeringquestions about telecom standards with a precision that closely rivals the moreresource-intensive GPT-3.5. The paper further explores the refined capabilitiesof Phi-2 in addressing problem-solving scenarios within the telecom sector,highlighting its potential and limitations.</description><author>Nicola Piovesan, Antonio De Domenico, Fadhel Ayed</author><pubDate>Thu, 07 Mar 2024 17:13:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04666v1</guid></item><item><title>Dynamic Cross Attention for Audio-Visual Person Verification</title><link>http://arxiv.org/abs/2403.04661v1</link><description>Although person or identity verification has been predominantly exploredusing individual modalities such as face and voice, audio-visual fusion hasrecently shown immense potential to outperform unimodal approaches. Audio andvisual modalities are often expected to pose strong complementaryrelationships, which plays a crucial role in effective audio-visual fusion.However, they may not always strongly complement each other, they may alsoexhibit weak complementary relationships, resulting in poor audio-visualfeature representations. In this paper, we propose a Dynamic Cross-Attention(DCA) model that can dynamically select the cross-attended or unattendedfeatures on the fly based on the strong or weak complementary relationships,respectively, across audio and visual modalities. In particular, a conditionalgating layer is designed to evaluate the contribution of the cross-attentionmechanism and choose cross-attended features only when they exhibit strongcomplementary relationships, otherwise unattended features. Extensiveexperiments are conducted on the Voxceleb1 dataset to demonstrate therobustness of the proposed model. Results indicate that the proposed modelconsistently improves the performance on multiple variants of cross-attentionwhile outperforming the state-of-the-art methods.</description><author>R. Gnana Praveen, Jahangir Alam</author><pubDate>Thu, 07 Mar 2024 17:07:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04661v1</guid></item><item><title>On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning</title><link>http://arxiv.org/abs/2311.12688v2</link><description>Bayesian deep learning and conformal prediction are two methods that havebeen used to convey uncertainty and increase safety in machine learningsystems. We focus on combining Bayesian deep learning with split conformalprediction and how this combination effects out-of-distribution coverage;particularly in the case of multiclass image classification. We suggest that ifthe model is generally underconfident on the calibration set, then theresultant conformal sets may exhibit worse out-of-distribution coveragecompared to simple predictive credible sets. Conversely, if the model isoverconfident on the calibration set, the use of conformal prediction mayimprove out-of-distribution coverage. We evaluate prediction sets as a resultof combining split conformal methods and neural networks trained with (i)stochastic gradient descent, (ii) deep ensembles, and (iii) mean-fieldvariational inference. Our results suggest that combining Bayesian deeplearning models with split conformal prediction can, in some cases, causeunintended consequences such as reducing out-of-distribution coverage.</description><author>Paul Scemama, Ariel Kapusta</author><pubDate>Thu, 07 Mar 2024 17:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12688v2</guid></item><item><title>Chain of Thought Explanation for Dialogue State Tracking</title><link>http://arxiv.org/abs/2403.04656v1</link><description>Dialogue state tracking (DST) aims to record user queries and goals during aconversational interaction achieved by maintaining a prede- fined set of slotsand their corresponding values. Current approaches decide slot values opaquely,while humans usually adopt a more deliberate approach by collecting informationfrom relevant dialogue turns and then reasoning the appropriate values. In thiswork, we focus on the steps needed to figure out slot values by proposing amodel named Chain-of-Thought-Explanation (CoTE) for the DST task. CoTE, whichis built on the generative DST framework, is designed to create detailedexplanations step by step after determining the slot values. This process leadsto more accurate and reliable slot values. More-over, to improve the reasoningability of the CoTE, we further construct more fluent and high-qualityexplanations with automatic paraphrasing, leading the method CoTE-refined.Experimental results on three widely recognized DST benchmarks-MultiWOZ 2.2,WoZ 2.0, and M2M-demonstrate the remarkable effectiveness of the CoTE.Furthermore, through a meticulous fine-grained analysis, we observe significantbenefits of our CoTE on samples characterized by longer dialogue turns, userresponses, and reasoning steps.</description><author>Lin Xu, Ningxin Peng, Daquan Zhou, See-Kiong Ng, Jinlan Fu</author><pubDate>Thu, 07 Mar 2024 16:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04656v1</guid></item><item><title>Audio-Visual Person Verification based on Recursive Fusion of Joint Cross-Attention</title><link>http://arxiv.org/abs/2403.04654v1</link><description>Person or identity verification has been recently gaining a lot of attentionusing audio-visual fusion as faces and voices share close associations witheach other. Conventional approaches based on audio-visual fusion rely onscore-level or early feature-level fusion techniques. Though existingapproaches showed improvement over unimodal systems, the potential ofaudio-visual fusion for person verification is not fully exploited. In thispaper, we have investigated the prospect of effectively capturing both theintra- and inter-modal relationships across audio and visual modalities, whichcan play a crucial role in significantly improving the fusion performance overunimodal systems. In particular, we introduce a recursive fusion of a jointcross-attentional model, where a joint audio-visual feature representation isemployed in the cross-attention framework in a recursive fashion toprogressively refine the feature representations that can efficiently capturethe intra-and inter-modal relationships. To further enhance the audio-visualfeature representations, we have also explored BLSTMs to improve the temporalmodeling of audio-visual feature representations. Extensive experiments areconducted on the Voxceleb1 dataset to evaluate the proposed model. Resultsindicate that the proposed model shows promising improvement in fusionperformance by adeptly capturing the intra-and inter-modal relationships acrossaudio and visual modalities.</description><author>R. Gnana Praveen, Jahangir Alam</author><pubDate>Thu, 07 Mar 2024 16:57:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04654v1</guid></item><item><title>Yi: Open Foundation Models by 01.AI</title><link>http://arxiv.org/abs/2403.04652v1</link><description>We introduce the Yi model family, a series of language and multimodal modelsthat demonstrate strong multi-dimensional capabilities. The Yi model family isbased on 6B and 34B pretrained language models, then we extend them to chatmodels, 200K long context models, depth-upscaled models, and vision-languagemodels. Our base models achieve strong performance on a wide range ofbenchmarks like MMLU, and our finetuned chat models deliver strong humanpreference rate on major evaluation platforms like AlpacaEval and ChatbotArena. Building upon our scalable super-computing infrastructure and theclassical transformer architecture, we attribute the performance of Yi modelsprimarily to its data quality resulting from our data-engineering efforts. Forpretraining, we construct 3.1 trillion tokens of English and Chinese corporausing a cascaded data deduplication and quality filtering pipeline. Forfinetuning, we polish a small scale (less than 10K) instruction dataset overmultiple iterations such that every single instance has been verified directlyby our machine learning engineers. For vision-language, we combine the chatlanguage model with a vision transformer encoder and train the model to alignvisual representations to the semantic space of the language model. We furtherextend the context length to 200K through lightweight continual pretraining anddemonstrate strong needle-in-a-haystack retrieval performance. We show thatextending the depth of the pretrained checkpoint through continual pretrainingfurther improves performance. We believe that given our current results,continuing to scale up model parameters using thoroughly optimized data willlead to even stronger frontier models.</description><author>01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai</author><pubDate>Thu, 07 Mar 2024 16:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04652v1</guid></item><item><title>Context-Based Multimodal Fusion</title><link>http://arxiv.org/abs/2403.04650v1</link><description>The fusion models, which effectively combine information from differentsources, are widely used in solving multimodal tasks. However, they havesignificant limitations related to aligning data distributions across differentmodalities. This challenge can lead to inconsistencies and difficulties inlearning robust representations. Alignment models, while specificallyaddressing this issue, often require training "from scratch" with largedatasets to achieve optimal results, which can be costly in terms of resourcesand time. To overcome these limitations, we propose an innovative model calledContext-Based Multimodal Fusion (CBMF), which combines both modality fusion anddata distribution alignment. In CBMF, each modality is represented by aspecific context vector, fused with the embedding of each modality. Thisenables the use of large pre-trained models that can be frozen, reducing thecomputational and training data requirements. Additionally, the network learnsto differentiate embeddings of different modalities through fusion with contextand aligns data distributions using a contrastive approach for self-supervisedlearning. Thus, CBMF offers an effective and economical solution for solvingcomplex multimodal tasks.</description><author>Bilal Faye, Hanane Azzag, Mustapha Lebbah, Djamel Bouchaffra</author><pubDate>Thu, 07 Mar 2024 16:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04650v1</guid></item><item><title>Interpreting User Requests in the Context of Natural Language Standing Instructions</title><link>http://arxiv.org/abs/2311.09796v2</link><description>Users of natural language interfaces, generally powered by Large LanguageModels (LLMs),often must repeat their preferences each time they make a similarrequest. We describe an approach to LLM-based dialogue modeling in whichpersistent user constraints and preferences -- collectively termed standinginstructions -- as additional context for such interfaces. For example, when auser states "I'm hungry", a previously expressed preference for Persian foodcan be automatically added to the LLM prompt, influencing the search forrelevant restaurants. We develop NLSI, a language-to-program dataset consistingof over 2.4K dialogues spanning 17 domains, where each dialogue is paired witha user profile (a set of users specific standing instructions) andcorresponding structured representations (API calls). A key challenge in NLSIis to identify which subset of the standing instructions is applicable to agiven dialogue. NLSI contains diverse phenomena, from simple preferences tointerdependent instructions such as triggering a hotel search whenever the useris booking tickets to an event. We conduct experiments on NLSI using promptingwith large language models and various retrieval approaches, achieving amaximum of 44.7% exact match on API prediction. Our results demonstrate thechallenges in identifying the relevant standing instructions and theirinterpretation into API calls.</description><author>Nikita Moghe, Patrick Xia, Jacob Andreas, Jason Eisner, Benjamin Van Durme, Harsh Jhamtani</author><pubDate>Thu, 07 Mar 2024 16:49:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09796v2</guid></item><item><title>Can Large Language Models Replace Economic Choice Prediction Labs?</title><link>http://arxiv.org/abs/2401.17435v3</link><description>Economic choice prediction is an essential challenging task, oftenconstrained by the difficulties in acquiring human choice data. Indeed,experimental economics studies had focused mostly on simple choice settings.The AI community has recently contributed to that effort in two ways:considering whether LLMs can substitute for humans in the above-mentionedsimple choice prediction settings, and the study through ML lens of moreelaborated but still rigorous experimental economics settings, employingincomplete information, repetitive play, and natural language communication,notably language-based persuasion games. This leaves us with a majorinspiration: can LLMs be used to fully simulate the economic environment andgenerate data for efficient human choice prediction, substituting for theelaborated economic lab studies? We pioneer the study of this subject,demonstrating its feasibility. In particular, we show that a model trainedsolely on LLM-generated data can effectively predict human behavior in alanguage-based persuasion game, and can even outperform models trained onactual human data.</description><author>Eilam Shapira, Omer Madmon, Roi Reichart, Moshe Tennenholtz</author><pubDate>Thu, 07 Mar 2024 16:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17435v3</guid></item><item><title>QAQ: Quality Adaptive Quantization for LLM KV Cache</title><link>http://arxiv.org/abs/2403.04643v1</link><description>The emergence of LLMs has ignited a fresh surge of breakthroughs in NLPapplications, particularly in domains such as question-answering systems andtext generation. As the need for longer context grows, a significant bottleneckin model deployment emerges due to the linear expansion of the Key-Value (KV)cache with the context length. Existing methods primarily rely on varioushypotheses, such as sorting the KV cache based on attention scores forreplacement or eviction, to compress the KV cache and improve model throughput.However, heuristics used by these strategies may wrongly evict essential KVcache, which can significantly degrade model performance. In this paper, wepropose QAQ, a Quality Adaptive Quantization scheme for the KV cache. Wetheoretically demonstrate that key cache and value cache exhibit distinctsensitivities to quantization, leading to the formulation of separatequantization strategies for their non-uniform quantization. Through theintegration of dedicated outlier handling, as well as an improvedattention-aware approach, QAQ achieves up to 10x the compression ratio of theKV cache size with a neglectable impact on model performance. QAQ significantlyreduces the practical hurdles of deploying LLMs, opening up new possibilitiesfor longer-context applications. The code is available atgithub.com/ClubieDong/KVCacheQuantization.</description><author>Shichen Dong, Wen Cheng, Jiayu Qin, Wei Wang</author><pubDate>Thu, 07 Mar 2024 16:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04643v1</guid></item><item><title>LibCity: A Unified Library Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction</title><link>http://arxiv.org/abs/2304.14343v7</link><description>As deep learning technology advances and more urban spatial-temporal dataaccumulates, an increasing number of deep learning models are being proposed tosolve urban spatial-temporal prediction problems. However, there arelimitations in the existing field, including open-source data being in variousformats and difficult to use, few papers making their code and data openlyavailable, and open-source models often using different frameworks andplatforms, making comparisons challenging. A standardized framework is urgentlyneeded to implement and evaluate these methods. To address these issues, wepropose LibCity, an open-source library that offers researchers a credibleexperimental tool and a convenient development framework. In this library, wehave reproduced 65 spatial-temporal prediction models and collected 55spatial-temporal datasets, allowing researchers to conduct comprehensiveexperiments conveniently. By enabling fair model comparisons, designing aunified data storage format, and simplifying the process of developing newmodels, LibCity is poised to make significant contributions to thespatial-temporal prediction field.</description><author>Jiawei Jiang, Chengkai Han, Wenjun Jiang, Wayne Xin Zhao, Jingyuan Wang</author><pubDate>Thu, 07 Mar 2024 16:41:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14343v7</guid></item><item><title>Teaching Large Language Models to Reason with Reinforcement Learning</title><link>http://arxiv.org/abs/2403.04642v1</link><description>Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as adominant approach for aligning LLM outputs with human preferences. Inspired bythe success of RLHF, we study the performance of multiple algorithms that learnfrom feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}),Return-Conditioned RL) on improving LLM reasoning capabilities. We investigateboth sparse and dense rewards provided to the LLM both heuristically and via alearned reward model. We additionally start from multiple model sizes andinitializations both with and without supervised fine-tuning (\textbf{SFT})data. Overall, we find all algorithms perform comparably, with Expert Iterationperforming best in most cases. Surprisingly, we find the sample complexity ofExpert Iteration is similar to that of PPO, requiring at most on the order of$10^6$ samples to converge from a pretrained checkpoint. We investigate whythis is the case, concluding that during RL training models fail to exploresignificantly beyond solutions already produced by SFT models. Additionally, wediscuss a trade off between maj@1 and pass@96 metric performance during SFTtraining and how conversely RL training improves both simultaneously. We thenconclude by discussing the implications of our findings for RLHF and the futurerole of RL in LLM fine-tuning.</description><author>Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu</author><pubDate>Thu, 07 Mar 2024 16:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04642v1</guid></item><item><title>Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with Sparse Views</title><link>http://arxiv.org/abs/2310.05483v4</link><description>In this paper, we propose a novel method for 3D scene and objectreconstruction from sparse multi-view images. Different from previous methodsthat leverage extra information such as depth or generalizable features acrossscenes, our approach leverages the scene properties embedded in the multi-viewinputs to create precise pseudo-labels for optimization without any priortraining. Specifically, we introduce a geometry-guided approach that improvessurface reconstruction accuracy from sparse views by leveraging sphericalharmonics to predict the novel radiance while holistically considering allcolor observations for a point in the scene. Also, our pipeline exploits proxygeometry and correctly handles the occlusion in generating the pseudo-labels ofradiance, which previous image-warping methods fail to avoid. Our method,dubbed Ray Augmentation (RayAug), achieves superior results on DTU and Blenderdatasets without requiring prior training, demonstrating its effectiveness inaddressing the problem of sparse view reconstruction. Our pipeline is flexibleand can be integrated into other implicit neural reconstruction methods forsparse views.</description><author>Jiawei Yao, Chen Wang, Tong Wu, Chuming Li</author><pubDate>Thu, 07 Mar 2024 16:35:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05483v4</guid></item><item><title>CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios</title><link>http://arxiv.org/abs/2403.04640v1</link><description>This paper focuses on the challenge of answering questions in scenarios thatare composed of rich and complex dynamic audio-visual components. Althoughexisting Multimodal Large Language Models (MLLMs) can respond to audio-visualcontent, these responses are sometimes ambiguous and fail to describe specificaudio-visual events. To overcome this limitation, we introduce the CAT, whichenhances MLLM in three ways: 1) besides straightforwardly bridging audio andvideo, we design a clue aggregator that aggregates question-related clues indynamic audio-visual scenarios to enrich the detailed knowledge required forlarge language models. 2) CAT is trained on a mixed multimodal dataset,allowing direct application in audio-visual scenarios. Notably, we collect anaudio-visual joint instruction dataset named AVinstruct, to further enhance thecapacity of CAT to model cross-semantic correlations. 3) we propose AI-assistedambiguity-aware direct preference optimization, a strategy specialized inretraining the model to favor the non-ambiguity response and improve theability to localize specific audio-visual objects. Extensive experimentalresults demonstrate that CAT outperforms existing methods on multimodal tasks,especially in Audio-Visual Question Answering (AVQA) tasks. The codes and thecollected instructions are released at https://github.com/rikeilong/Bay-CAT.</description><author>Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, Xiaochun Cao</author><pubDate>Thu, 07 Mar 2024 16:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04640v1</guid></item><item><title>MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis</title><link>http://arxiv.org/abs/2403.04639v1</link><description>The present paper introduces new sentiment data, MaCMS, forMagahi-Hindi-English (MHE) code-mixed language, where Magahi is aless-resourced minority language. This dataset is the firstMagahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further,we also provide a linguistics analysis of the dataset to understand thestructure of code-mixing and a statistical study to understand the languagepreferences of speakers with different polarities. With these analyses, we alsotrain baseline models to evaluate the dataset's quality.</description><author>Priya Rani, Gaurav Negi, Theodorus Fransen, John P. McCrae</author><pubDate>Thu, 07 Mar 2024 16:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04639v1</guid></item><item><title>Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]</title><link>http://arxiv.org/abs/2308.12899v3</link><description>The field of urban spatial-temporal prediction is advancing rapidly with thedevelopment of deep learning techniques and the availability of large-scaledatasets. However, challenges persist in accessing and utilizing diverse urbanspatial-temporal datasets from different sources and stored in differentformats, as well as determining effective model structures and components withthe proliferation of deep learning models. This work addresses these challengesand provides three significant contributions. Firstly, we introduce "atomicfiles", a unified storage format designed for urban spatial-temporal big data,and validate its effectiveness on 40 diverse datasets, simplifying datamanagement. Secondly, we present a comprehensive overview of technologicaladvances in urban spatial-temporal prediction models, guiding the developmentof robust models. Thirdly, we conduct extensive experiments using diversemodels and datasets, establishing a performance leaderboard and identifyingpromising research directions. Overall, this work effectively manages urbanspatial-temporal data, guides future efforts, and facilitates the developmentof accurate and efficient urban spatial-temporal prediction models. It canpotentially make long-term contributions to urban spatial-temporal datamanagement and prediction, ultimately leading to improved urban livingstandards.</description><author>Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, Jingyuan Wang</author><pubDate>Thu, 07 Mar 2024 16:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12899v3</guid></item><item><title>Entropy Aware Message Passing in Graph Neural Networks</title><link>http://arxiv.org/abs/2403.04636v1</link><description>Deep Graph Neural Networks struggle with oversmoothing. This paper introducesa novel, physics-inspired GNN model designed to mitigate this issue. Ourapproach integrates with existing GNN architectures, introducing anentropy-aware message passing term. This term performs gradient ascent on theentropy during node aggregation, thereby preserving a certain degree of entropyin the embeddings. We conduct a comparative analysis of our model againststate-of-the-art GNNs across various common datasets.</description><author>Philipp Nazari, Oliver Lemke, Davide Guidobene, Artiom Gesp</author><pubDate>Thu, 07 Mar 2024 16:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04636v1</guid></item><item><title>ShortGPT: Layers in Large Language Models are More Redundant Than You Expect</title><link>http://arxiv.org/abs/2403.03853v2</link><description>As Large Language Models (LLMs) continue to advance in performance, theirsize has escalated significantly, with current LLMs containing billions or eventrillions of parameters. However, in this study, we discovered that many layersof LLMs exhibit high similarity, and some layers play a negligible role innetwork functionality. Based on this observation, we define a metric calledBlock Influence (BI) to gauge the significance of each layer in LLMs. We thenpropose a straightforward pruning approach: layer removal, in which we directlydelete the redundant layers in LLMs based on their BI scores. Experimentsdemonstrate that our method, which we call ShortGPT, significantly outperformsprevious state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPTis orthogonal to quantization-like methods, enabling further reduction inparameters and computation. The ability to achieve better results throughsimple layer removal, as opposed to more complex pruning techniques, suggests ahigh degree of redundancy in the model architecture.</description><author>Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, Weipeng Chen</author><pubDate>Thu, 07 Mar 2024 16:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03853v2</guid></item><item><title>Pix2Gif: Motion-Guided Diffusion for GIF Generation</title><link>http://arxiv.org/abs/2403.04634v1</link><description>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)generation. We tackle this problem differently by formulating the task as animage translation problem steered by text and motion magnitude prompts, asshown in teaser fig. To ensure that the model adheres to motion guidance, wepropose a new motion-guided warping module to spatially transform the featuresof the source image conditioned on the two types of prompts. Furthermore, weintroduce a perceptual loss to ensure the transformed feature map remainswithin the same space as the target image, ensuring content consistency andcoherence. In preparation for the model training, we meticulously curated databy extracting coherent image frames from the TGIF video-caption dataset, whichprovides rich information about the temporal changes of subjects. Afterpretraining, we apply our model in a zero-shot manner to a number of videodatasets. Extensive qualitative and quantitative experiments demonstrate theeffectiveness of our model -- it not only captures the semantic prompt fromtext but also the spatial ones from motion guidance. We train all our modelsusing a single node of 16xV100 GPUs. Code, dataset and models are made publicat: https://hiteshk03.github.io/Pix2Gif/.</description><author>Hitesh Kandala, Jianfeng Gao, Jianwei Yang</author><pubDate>Thu, 07 Mar 2024 16:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04634v1</guid></item><item><title>Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics</title><link>http://arxiv.org/abs/2211.09510v4</link><description>Trajectory Representation Learning (TRL) is a powerful tool forspatial-temporal data analysis and management. TRL aims to convert complicatedraw trajectories into low-dimensional representation vectors, which can beapplied to various downstream tasks, such as trajectory classification,clustering, and similarity computation. Existing TRL works usually treattrajectories as ordinary sequence data, while some important spatial-temporalcharacteristics, such as temporal regularities and travel semantics, are notfully exploited. To fill this gap, we propose a novel Self-supervisedtrajectory representation learning framework with TemporAl Regularities andTravel semantics, namely START. The proposed method consists of two stages. Thefirst stage is a Trajectory Pattern-Enhanced Graph Attention Network (TPE-GAT),which converts the road network features and travel semantics intorepresentation vectors of road segments. The second stage is a Time-AwareTrajectory Encoder (TAT-Enc), which encodes representation vectors of roadsegments in the same trajectory as a trajectory representation vector,meanwhile incorporating temporal regularities with the trajectoryrepresentation. Moreover, we also design two self-supervised tasks, i.e.,span-masked trajectory recovery and trajectory contrastive learning, tointroduce spatial-temporal characteristics of trajectories into the trainingprocess of our START framework. The effectiveness of the proposed method isverified by extensive experiments on two large-scale real-world datasets forthree downstream tasks. The experiments also demonstrate that our method can betransferred across different cities to adapt heterogeneous trajectory datasets.</description><author>Jiawei Jiang, Dayan Pan, Houxing Ren, Xiaohan Jiang, Chao Li, Jingyuan Wang</author><pubDate>Thu, 07 Mar 2024 16:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09510v4</guid></item><item><title>Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration</title><link>http://arxiv.org/abs/2403.04629v1</link><description>Bayesian optimization (BO) with Gaussian processes (GP) has become anindispensable algorithm for black box optimization problems. Not without a dashof irony, BO is often considered a black box itself, lacking ways to providereasons as to why certain parameters are proposed to be evaluated. This isparticularly relevant in human-in-the-loop applications of BO, such as inrobotics. We address this issue by proposing ShapleyBO, a framework forinterpreting BO's proposals by game-theoretic Shapley values.They quantify eachparameter's contribution to BO's acquisition function. Exploiting the linearityof Shapley values, we are further able to identify how strongly each parameterdrives BO's exploration and exploitation for additive acquisition functionslike the confidence bound. We also show that ShapleyBO can disentangle thecontributions to exploration into those that explore aleatoric and epistemicuncertainty. Moreover, our method gives rise to a ShapleyBO-assisted humanmachine interface (HMI), allowing users to interfere with BO in case proposalsdo not align with human reasoning. We demonstrate this HMI's benefits for theuse case of personalizing wearable robotic devices (assistive back exosuits) byhuman-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBOcan achieve lower regret than teams without.</description><author>Julian Rodemann, Federico Croppi, Philipp Arens, Yusuf Sale, Julia Herbinger, Bernd Bischl, Eyke Hüllermeier, Thomas Augustin, Conor J. Walsh, Giuseppe Casalicchio</author><pubDate>Thu, 07 Mar 2024 16:13:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04629v1</guid></item><item><title>Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL</title><link>http://arxiv.org/abs/2309.06553v4</link><description>In this study, we aim to enhance the arithmetic reasoning ability of LargeLanguage Models (LLMs) through zero-shot prompt optimization. We identify apreviously overlooked objective of query dependency in such optimization andelucidate two ensuing challenges that impede the successful and economicaldesign of prompt optimization techniques. One primary issue is the absence ofan effective method to evaluate prompts during inference when the golden answeris unavailable. Concurrently, learning via interactions with the LLMs tonavigate the expansive natural language prompting space proves to beresource-intensive. To address this, we introduce Prompt-OIRL, which harnessesoffline inverse reinforcement learning to draw insights from offline promptingdemonstration data. Such data exists as by-products when diverse prompts arebenchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependentprompt optimization objective is achieved by first learning an offline rewardmodel. This model can evaluate any query-prompt pairs without accessing LLMs.Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt.Our experimental evaluations across various LLM scales and arithmetic reasoningdatasets underscore both the efficacy and economic viability of the proposedapproach.</description><author>Hao Sun, Alihan Hüyük, Mihaela van der Schaar</author><pubDate>Thu, 07 Mar 2024 16:12:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06553v4</guid></item><item><title>MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder</title><link>http://arxiv.org/abs/2403.04626v1</link><description>Within the domain of medical analysis, extensive research has explored thepotential of mutual learning between Masked Autoencoders(MAEs) and multimodaldata. However, the impact of MAEs on intermodality remains a key challenge. Weintroduce MedFLIP, a Fast Language-Image Pre-training method for Medicalanalysis. We explore MAEs for zero-shot learning with crossed domains, whichenhances the model ability to learn from limited data, a common scenario inmedical diagnostics. We verify that masking an image does not affect intermodallearning. Furthermore, we propose the SVD loss to enhance the representationlearning for characteristics of medical images, aiming to improveclassification accuracy by leveraging the structural intricacies of such data.Lastly, we validate using language will improve the zero-shot performance forthe medical image analysis. MedFLIP scaling of the masking process marks anadvancement in the field, offering a pathway to rapid and precise medical imageanalysis without the traditional computational bottlenecks. Through experimentsand validation, MedFLIP demonstrates efficient performance improvements,setting an explored standard for future research and application in medicaldiagnostics.</description><author>Lei Li, Tianfang Zhang, Xinglin Zhang, Jiaqi Liu, Bingqi Ma, Yan Luo, Tao Chen</author><pubDate>Thu, 07 Mar 2024 16:11:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04626v1</guid></item><item><title>Label Alignment Regularization for Distribution Shift</title><link>http://arxiv.org/abs/2211.14960v3</link><description>Recent work has highlighted the label alignment property (LAP) in supervisedlearning, where the vector of all labels in the dataset is mostly in the spanof the top few singular vectors of the data matrix. Drawing inspiration fromthis observation, we propose a regularization method for unsupervised domainadaptation that encourages alignment between the predictions in the targetdomain and its top singular vectors. Unlike conventional domain adaptationapproaches that focus on regularizing representations, we instead regularizethe classifier to align with the unsupervised target data, guided by the LAP inboth the source and target domains. Theoretical analysis demonstrates that,under certain assumptions, our solution resides within the span of the topright singular vectors of the target domain data and aligns with the optimalsolution. By removing the reliance on the commonly used optimal joint riskassumption found in classic domain adaptation theory, we showcase theeffectiveness of our method on addressing problems where traditional domainadaptation methods often fall short due to high joint error. Additionally, wereport improved performance over domain adaptation baselines in well-knowntasks such as MNIST-USPS domain adaptation and cross-lingual sentimentanalysis.</description><author>Ehsan Imani, Guojun Zhang, Runjia Li, Jun Luo, Pascal Poupart, Philip H. S. Torr, Yangchen Pan</author><pubDate>Thu, 07 Mar 2024 16:04:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14960v3</guid></item><item><title>Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces</title><link>http://arxiv.org/abs/2305.12584v2</link><description>Sparsity of a learning solution is a desirable feature in machine learning.Certain reproducing kernel Banach spaces (RKBSs) are appropriate hypothesisspaces for sparse learning methods. The goal of this paper is to understandwhat kind of RKBSs can promote sparsity for learning solutions. We consider twotypical learning models in an RKBS: the minimum norm interpolation (MNI)problem and the regularization problem. We first establish an explicitrepresenter theorem for solutions of these problems, which represents theextreme points of the solution set by a linear combination of the extremepoints of the subdifferential set, of the norm function, which isdata-dependent. We then propose sufficient conditions on the RKBS that cantransform the explicit representation of the solutions to a sparse kernelrepresentation having fewer terms than the number of the observed data. Underthe proposed sufficient conditions, we investigate the role of theregularization parameter on sparsity of the regularized solutions. We furthershow that two specific RKBSs: the sequence space $\ell_1(\mathbb{N})$ and themeasure space can have sparse representer theorems for both MNI andregularization models.</description><author>Rui Wang, Yuesheng Xu, Mingsong Yan</author><pubDate>Thu, 07 Mar 2024 16:04:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12584v2</guid></item><item><title>High-Level Parallelism and Nested Features for Dynamic Inference Cost and Top-Down Attention</title><link>http://arxiv.org/abs/2308.05128v2</link><description>This paper introduces a novel network topology that seamlessly integratesdynamic inference cost with a top-down attention mechanism, addressing twosignificant gaps in traditional deep learning models. Drawing inspiration fromhuman perception, we combine sequential processing of generic low-levelfeatures with parallelism and nesting of high-level features. This design notonly reflects a finding from recent neuroscience research regarding - spatiallyand contextually distinct neural activations - in human cortex, but alsointroduces a novel "cutout" technique: the ability to selectively activate%segments of the network for task-relevant only network segments oftask-relevant categories to optimize inference cost and eliminate the need forre-training. We believe this paves the way for future network designs that arelightweight and adaptable, making them suitable for a wide range ofapplications, from compact edge devices to large-scale clouds. Our proposedtopology also comes with a built-in top-down attention mechanism, which allowsprocessing to be directly influenced by either enhancing or inhibitingcategory-specific high-level features, drawing parallels to the selectiveattention mechanism observed in human cognition. Using targeted externalsignals, we experimentally enhanced predictions across all tested models. Interms of dynamic inference cost our methodology can achieve an exclusion of upto $73.48\,\%$ of parameters and $84.41\,\%$ fewer giga-multiply-accumulate(GMAC) operations, analysis against comparative baselines show an averagereduction of $40\,\%$ in parameters and $8\,\%$ in GMACs across the cases weevaluated.</description><author>André Peter Kelm, Niels Hannemann, Bruno Heberle, Lucas Schmidt, Tim Rolff, Christian Wilms, Ehsan Yaghoubi, Simone Frintrop</author><pubDate>Thu, 07 Mar 2024 16:03:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05128v2</guid></item><item><title>Strong Priority and Determinacy in Timed CCS</title><link>http://arxiv.org/abs/2403.04618v1</link><description>Building on the classical theory of process algebra with priorities, weidentify a new scheduling mechanism, called "sequentially constructivereduction" which is designed to capture the essence of synchronous programming.The distinctive property of this evaluation strategy is to achievedeterminism-by-construction for multi-cast concurrent communication. Inparticular, it permits us to model shared memory multi-threading with reactionto absence as it lies at the core of the programming language Esterel. In thetechnical setting of CCS extended by clocks and priorities, we prove for alarge class of processes, which we call "structurally coherent" the confluenceproperty for constructive reductions. We further show that under some syntacticrestrictions, called "pivotable" the operators of prefix, summation, parallelcomposition, restriction and hiding preserve structural coherence. This coversa strictly larger class of processes compared to those that are confluent inMilner's classical theory of CCS without priorities.</description><author>Luigi Liquori, Michael Mendler</author><pubDate>Thu, 07 Mar 2024 16:02:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04618v1</guid></item><item><title>PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction</title><link>http://arxiv.org/abs/2301.07945v3</link><description>As a core technology of Intelligent Transportation System, traffic flowprediction has a wide range of applications. The fundamental challenge intraffic flow prediction is to effectively model the complex spatial-temporaldependencies in traffic data. Spatial-temporal Graph Neural Network (GNN)models have emerged as one of the most promising methods to solve this problem.However, GNN-based models have three major limitations for traffic prediction:i) Most methods model spatial dependencies in a static manner, which limits theability to learn dynamic urban traffic patterns; ii) Most methods only considershort-range spatial information and are unable to capture long-range spatialdependencies; iii) These methods ignore the fact that the propagation oftraffic conditions between locations has a time delay in traffic systems. Tothis end, we propose a novel Propagation Delay-aware dynamic long-rangetransFormer, namely PDFormer, for accurate traffic flow prediction.Specifically, we design a spatial self-attention module to capture the dynamicspatial dependencies. Then, two graph masking matrices are introduced tohighlight spatial dependencies from short- and long-range views. Moreover, atraffic delay-aware feature transformation module is proposed to empowerPDFormer with the capability of explicitly modeling the time delay of spatialinformation propagation. Extensive experimental results on six real-worldpublic traffic datasets show that our method can not only achievestate-of-the-art performance but also exhibit competitive computationalefficiency. Moreover, we visualize the learned spatial-temporal attention mapto make our model highly interpretable.</description><author>Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, Jingyuan Wang</author><pubDate>Thu, 07 Mar 2024 16:00:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07945v3</guid></item><item><title>On Trojan Signatures in Large Language Models of Code</title><link>http://arxiv.org/abs/2402.16896v2</link><description>Trojan signatures, as described by Fields et al. (2021), are noticeabledifferences in the distribution of the trojaned class parameters (weights) andthe non-trojaned class parameters of the trojaned model, that can be used todetect the trojaned model. Fields et al. (2021) found trojan signatures incomputer vision classification tasks with image models, such as, Resnet,WideResnet, Densenet, and VGG. In this paper, we investigate such signatures inthe classifier layer parameters of large language models of source code. Our results suggest that trojan signatures could not generalize to LLMs ofcode. We found that trojaned code models are stubborn, even when the modelswere poisoned under more explicit settings (finetuned with pre-trained weightsfrozen). We analyzed nine trojaned models for two binary classification tasks:clone and defect detection. To the best of our knowledge, this is the firstwork to examine weight-based trojan signature revelation techniques forlarge-language models of code and furthermore to demonstrate that detectingtrojans only from the weights in such models is a hard problem.</description><author>Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour</author><pubDate>Thu, 07 Mar 2024 15:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16896v2</guid></item><item><title>A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images</title><link>http://arxiv.org/abs/2403.04612v1</link><description>Currently, medical image domain translation operations show a high demandfrom researchers and clinicians. Amongst other capabilities, this task allowsthe generation of new medical images with sufficiently high image quality,making them clinically relevant. Deep Learning (DL) architectures, mostspecifically deep generative models, are widely used to generate and translateimages from one domain to another. The proposed framework relies on anadversarial Denoising Diffusion Model (DDM) to synthesize echocardiographyimages and perform domain translation. Contrary to Generative AdversarialNetworks (GANs), DDMs are able to generate high quality image samples with alarge diversity. If a DDM is combined with a GAN, this ability to generate newdata is completed at an even faster sampling time. In this work we trained anadversarial DDM combined with a GAN to learn the reverse denoising process,relying on a guide image, making sure relevant anatomical structures of eachechocardiography image were kept and represented on the generated imagesamples. For several domain translation operations, the results verified thatsuch generative model was able to synthesize high quality image samples: MSE:11.50 +/- 3.69, PSNR (dB): 30.48 +/- 0.09, SSIM: 0.47 +/- 0.03. The proposedmethod showed high generalization ability, introducing a framework to createechocardiography images suitable to be used for clinical research purposes.</description><author>Cristiana Tiago, Sten Roar Snare, Jurica Sprem, Kristin McLeod</author><pubDate>Thu, 07 Mar 2024 15:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04612v1</guid></item><item><title>Repelling-Attracting Hamiltonian Monte Carlo</title><link>http://arxiv.org/abs/2403.04607v1</link><description>We propose a variant of Hamiltonian Monte Carlo (HMC), called theRepelling-Attracting Hamiltonian Monte Carlo (RAHMC), for sampling frommultimodal distributions. The key idea that underpins RAHMC is a departure fromthe conservative dynamics of Hamiltonian systems, which form the basis oftraditional HMC, and turning instead to the dissipative dynamics of conformalHamiltonian systems. In particular, RAHMC involves two stages: a mode-repellingstage to encourage the sampler to move away from regions of high probabilitydensity; and, a mode-attracting stage, which facilitates the sampler to findand settle near alternative modes. We achieve this by introducing just oneadditional tuning parameter -- the coefficient of friction. The proposed methodadapts to the geometry of the target distribution, e.g., modes and densityridges, and can generate proposals that cross low-probability barriers withlittle to no computational overhead in comparison to traditional HMC. Notably,RAHMC requires no additional information about the target distribution ormemory of previously visited modes. We establish the theoretical basis forRAHMC, and we discuss repelling-attracting extensions to several variants ofHMC in literature. Finally, we provide a tuning-free implementation viadual-averaging, and we demonstrate its effectiveness in sampling from, both,multimodal and unimodal distributions in high dimensions.</description><author>Siddharth Vishwanath, Hyungsuk Tak</author><pubDate>Thu, 07 Mar 2024 15:54:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04607v1</guid></item><item><title>In-n-Out: Calibrating Graph Neural Networks for Link Prediction</title><link>http://arxiv.org/abs/2403.04605v1</link><description>Deep neural networks are notoriously miscalibrated, i.e., their outputs donot reflect the true probability of the event we aim to predict. While networksfor tabular or image data are usually overconfident, recent works have shownthat graph neural networks (GNNs) show the opposite behavior for node-levelclassification. But what happens when we are predicting links? We show that, inthis case, GNNs often exhibit a mixed behavior. More specifically, they may beoverconfident in negative predictions while being underconfident in positiveones. Based on this observation, we propose IN-N-OUT, the first-ever method tocalibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions:i) attributing true/false labels to an edge while respecting a GNNs predictionshould cause but small fluctuations in that edge's embedding; and, conversely,ii) if we label that same edge contradicting our GNN, embeddings should changemore substantially. An extensive experimental campaign shows that IN-N-OUTsignificantly improves the calibration of GNNs in link prediction, consistentlyoutperforming the baselines available -- which are not designed for thisspecific task.</description><author>Erik Nascimento, Diego Mesquita, Samuel Kaskio, Amauri H Souza</author><pubDate>Thu, 07 Mar 2024 15:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04605v1</guid></item><item><title>Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation</title><link>http://arxiv.org/abs/2403.04599v1</link><description>Recently, because of the high-quality representations of contrastive learningmethods, rehearsal-based contrastive continual learning has been proposed toexplore how to continually learn transferable representation embeddings toavoid the catastrophic forgetting issue in traditional continual settings.Based on this framework, we propose Contrastive Continual Learning viaImportance Sampling (CCLIS) to preserve knowledge by recovering previous datadistributions with a new strategy for Replay Buffer Selection (RBS), whichminimize estimated variance to save hard negative samples for representationlearning with high quality. Furthermore, we present the Prototype-instanceRelation Distillation (PRD) loss, a technique designed to maintain therelationship between prototypes and sample representations using aself-distillation process. Experiments on standard continual learningbenchmarks reveal that our method notably outperforms existing baselines interms of knowledge preservation and thereby effectively counteractscatastrophic forgetting in online contexts. The code is available athttps://github.com/lijy373/CCLIS.</description><author>Jiyong Li, Dilshod Azizov, Yang Li, Shangsong Liang</author><pubDate>Thu, 07 Mar 2024 15:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04599v1</guid></item><item><title>Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization</title><link>http://arxiv.org/abs/2402.02746v2</link><description>There has been a long-standing and widespread belief that BayesianOptimization (BO) with standard Gaussian process (GP), referred to as standardBO, is ineffective in high-dimensional optimization problems. This perceptionmay partly stem from the intuition that GPs struggle with high-dimensionalinputs for covariance modeling and function estimation. While these concernsseem reasonable, empirical evidence supporting this belief is lacking. In thispaper, we systematically investigated BO with standard GP regression across avariety of synthetic and real-world benchmark problems for high-dimensionaloptimization. Surprisingly, the performance with standard GP consistently ranksamong the best, often outperforming existing BO methods specifically designedfor high-dimensional optimization by a large margin. Contrary to thestereotype, we found that standard GP can serve as a capable surrogate forlearning high-dimensional target functions. Without strong structuralassumptions, BO with standard GP not only excels in high-dimensionaloptimization but also proves robust in accommodating various structures withinthe target functions. Furthermore, with standard GP, achieving promisingoptimization performance is possible by only using maximum likelihoodestimation, eliminating the need for expensive Markov-Chain Monte Carlo (MCMC)sampling that might be required by more complex surrogate models. We thusadvocate for a re-evaluation and in-depth study of the potential of standard BOin addressing high-dimensional problems.</description><author>Zhitong Xu, Shandian Zhe</author><pubDate>Thu, 07 Mar 2024 15:47:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02746v2</guid></item><item><title>Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures</title><link>http://arxiv.org/abs/2403.02308v2</link><description>Transformers have revolutionized computer vision and natural languageprocessing, but their high computational complexity limits their application inhigh-resolution image processing and long-context analysis. This paperintroduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in theNLP field with necessary modifications for vision tasks. Similar to the VisionTransformer (ViT), our model is designed to efficiently handle sparse inputsand demonstrate robust global processing capabilities, while also scaling upeffectively, accommodating both large-scale parameters and extensive datasets.Its distinctive advantage lies in its reduced spatial aggregation complexity,which renders it exceptionally adept at processing high-resolution imagesseamlessly, eliminating the necessity for windowing operations. Our evaluationsdemonstrate that VRWKV surpasses ViT's performance in image classification andhas significantly faster speeds and lower memory usage processinghigh-resolution inputs. In dense prediction tasks, it outperforms window-basedmodels, maintaining comparable speeds. These results highlight VRWKV'spotential as a more efficient alternative for visual perception tasks. Code isreleased at \url{https://github.com/OpenGVLab/Vision-RWKV}.</description><author>Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Hongsheng Li, Jifeng Dai, Wenhai Wang</author><pubDate>Thu, 07 Mar 2024 15:43:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02308v2</guid></item><item><title>First-order penalty methods for bilevel optimization</title><link>http://arxiv.org/abs/2301.01716v2</link><description>In this paper we study a class of unconstrained and constrained bileveloptimization problems in which the lower level is a possibly nonsmooth convexoptimization problem, while the upper level is a possibly nonconvexoptimization problem. We introduce a notion of $\varepsilon$-KKT solution forthem and show that an $\varepsilon$-KKT solution leads to an$O(\sqrt{\varepsilon})$- or $O(\varepsilon)$-hypergradient based stionary pointunder suitable assumptions. We also propose first-order penalty methods forfinding an $\varepsilon$-KKT solution of them, whose subproblems turn out to bea structured minimax problem and can be suitably solved by a first-order methodrecently developed by the authors. Under suitable assumptions, an\emph{operation complexity} of $O(\varepsilon^{-4}\log\varepsilon^{-1})$ and$O(\varepsilon^{-7}\log\varepsilon^{-1})$, measured by their fundamentaloperations, is established for the proposed penalty methods for finding an$\varepsilon$-KKT solution of the unconstrained and constrained bileveloptimization problems, respectively. Preliminary numerical results arepresented to illustrate the performance of our proposed methods. To the best ofour knowledge, this paper is the first work to demonstrate that bileveloptimization can be approximately solved as minimax optimization, and moreover,it provides the first implementable method with complexity guarantees for suchsophisticated bilevel optimization.</description><author>Zhaosong Lu, Sanyou Mei</author><pubDate>Thu, 07 Mar 2024 15:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.01716v2</guid></item><item><title>Embodied Understanding of Driving Scenarios</title><link>http://arxiv.org/abs/2403.04593v1</link><description>Embodied scene understanding serves as the cornerstone for autonomous agentsto perceive, interpret, and respond to open driving scenarios. Suchunderstanding is typically founded upon Vision-Language Models (VLMs).Nevertheless, existing VLMs are restricted to the 2D domain, devoid of spatialawareness and long-horizon extrapolation proficiencies. We revisit the keyaspects of autonomous driving and formulate appropriate rubrics. Hereby, weintroduce the Embodied Language Model (ELM), a comprehensive framework tailoredfor agents' understanding of driving scenes with large spatial and temporalspans. ELM incorporates space-aware pre-training to endow the agent with robustspatial localization capabilities. Besides, the model employs time-aware tokenselection to accurately inquire about temporal cues. We instantiate ELM on thereformulated multi-faced benchmark, and it surpasses previous state-of-the-artapproaches in all aspects. All code, data, and models will be publicly shared.</description><author>Yunsong Zhou, Linyan Huang, Qingwen Bu, Jia Zeng, Tianyu Li, Hang Qiu, Hongzi Zhu, Minyi Guo, Yu Qiao, Hongyang Li</author><pubDate>Thu, 07 Mar 2024 15:39:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04593v1</guid></item><item><title>FOSI: Hybrid First and Second Order Optimization</title><link>http://arxiv.org/abs/2302.08484v4</link><description>Popular machine learning approaches forgo second-order information due to thedifficulty of computing curvature in high dimensions. We present FOSI, a novelmeta-algorithm that improves the performance of any base first-order optimizerby efficiently incorporating second-order information during the optimizationprocess. In each iteration, FOSI implicitly splits the function into twoquadratic functions defined on orthogonal subspaces, then uses a second-ordermethod to minimize the first, and the base optimizer to minimize the other. Weformally analyze FOSI's convergence and the conditions under which it improvesa base optimizer. Our empirical evaluation demonstrates that FOSI improves theconvergence rate and optimization time of first-order methods such asHeavy-Ball and Adam, and outperforms second-order methods (K-FAC and L-BFGS).</description><author>Hadar Sivan, Moshe Gabel, Assaf Schuster</author><pubDate>Thu, 07 Mar 2024 15:37:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08484v4</guid></item><item><title>Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace</title><link>http://arxiv.org/abs/2403.04588v1</link><description>Humans perceive the world through multiple senses, enabling them to create acomprehensive representation of their surroundings and to generalizeinformation across domains. For instance, when a textual description of a sceneis given, humans can mentally visualize it. In fields like robotics andReinforcement Learning (RL), agents can also access information about theenvironment through multiple sensors; yet redundancy and complementaritybetween sensors is difficult to exploit as a source of robustness (e.g. againstsensor failure) or generalization (e.g. transfer across domains). Priorresearch demonstrated that a robust and flexible multimodal representation canbe efficiently constructed based on the cognitive science notion of a 'GlobalWorkspace': a unique representation trained to combine information acrossmodalities, and to broadcast its signal back to each modality. Here, we explorewhether such a brain-inspired multimodal representation could be advantageousfor RL agents. First, we train a 'Global Workspace' to exploit informationcollected about the environment via two input modalities (a visual input, or anattribute vector representing the state of the agent and/or its environment).Then, we train a RL agent policy using this frozen Global Workspace. In twodistinct environments and tasks, our results reveal the model's ability toperform zero-shot cross-modal transfer between input modalities, i.e. to applyto image inputs a policy previously trained on attribute vectors (andvice-versa), without additional training or fine-tuning. Variants and ablationsof the full Global Workspace (including a CLIP-like multimodal representationtrained via contrastive learning) did not display the same generalizationabilities.</description><author>Léopold Maytié, Benjamin Devillers, Alexandre Arnold, Rufin VanRullen</author><pubDate>Thu, 07 Mar 2024 15:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04588v1</guid></item><item><title>EasyPortrait -- Face Parsing and Portrait Segmentation Dataset</title><link>http://arxiv.org/abs/2304.13509v3</link><description>Recently, video conferencing apps have become functional by accomplishingsuch computer vision-based features as real-time background removal and facebeautification. Limited variability in existing portrait segmentation and faceparsing datasets, including head poses, ethnicity, scenes, and occlusionsspecific to video conferencing, motivated us to create a new dataset,EasyPortrait, for these tasks simultaneously. It contains 40,000 primarilyindoor photos repeating video meeting scenarios with 13,705 unique users andfine-grained segmentation masks separated into 9 classes. Inappropriateannotation masks from other datasets caused a revision of annotator guidelines,resulting in EasyPortrait's ability to process cases, such as teeth whiteningand skin smoothing. The pipeline for data mining and high-quality maskannotation via crowdsourcing is also proposed in this paper. In the ablationstudy experiments, we proved the importance of data quantity and diversity inhead poses in our dataset for the effective learning of the model. Thecross-dataset evaluation experiments confirmed the best domain generalizationability among portrait segmentation datasets. Moreover, we demonstrate thesimplicity of training segmentation models on EasyPortrait without extratraining tricks. The proposed dataset and trained models are publiclyavailable.</description><author>Karina Kvanchiani, Elizaveta Petrova, Karen Efremyan, Alexander Sautin, Alexander Kapitanov</author><pubDate>Thu, 07 Mar 2024 15:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13509v3</guid></item><item><title>Learning Agility Adaptation for Flight in Clutter</title><link>http://arxiv.org/abs/2403.04586v1</link><description>Animals learn to adapt agility of their movements to their capabilities andthe environment they operate in. Mobile robots should also demonstrate thisability to combine agility and safety. The aim of this work is to endow flightvehicles with the ability of agility adaptation in prior unknown and partiallyobservable cluttered environments. We propose a hierarchical learning andplanning framework where we utilize both trial and error to comprehensivelylearn an agility policy with the vehicle's observation as the input, andwell-established methods of model-based trajectory generation. Technically, weuse online model-free reinforcement learning and a pre-training-fine-tuningreward scheme to obtain the deployable policy. The statistical results insimulation demonstrate the advantages of our method over the constant agilitybaselines and an alternative method in terms of flight efficiency and safety.In particular, the policy leads to intelligent behaviors, such as perceptionawareness, which distinguish it from other approaches. By deploying the policyto hardware, we verify that these advantages can be brought to the real world.</description><author>Guangyu Zhao, Tianyue Wu, Yeke Chen, Fei Gao</author><pubDate>Thu, 07 Mar 2024 15:30:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04586v1</guid></item><item><title>Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds</title><link>http://arxiv.org/abs/2306.06836v3</link><description>While numerous works have focused on devising efficient algorithms forreinforcement learning (RL) with uniformly bounded rewards, it remains an openquestion whether sample or time-efficient algorithms for RL with largestate-action space exist when the rewards are \emph{heavy-tailed}, i.e., withonly finite $(1+\epsilon)$-th moments for some $\epsilon\in(0,1]$. In thiswork, we address the challenge of such rewards in RL with linear functionapproximation. We first design an algorithm, \textsc{Heavy-OFUL}, forheavy-tailed linear bandits, achieving an \emph{instance-dependent} $T$-roundregret of $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$, the\emph{first} of this kind. Here, $d$ is the feature dimension, and$\nu_t^{1+\epsilon}$ is the $(1+\epsilon)$-th central moment of the reward atthe $t$-th round. We further show the above bound is minimax optimal whenapplied to the worst-case instances in stochastic and deterministic linearbandits. We then extend this algorithm to the RL settings with linear functionapproximation. Our algorithm, termed as \textsc{Heavy-LSVI-UCB}, achieves the\emph{first} computationally efficient \emph{instance-dependent} $K$-episoderegret of $\tilde{O}(d \sqrt{H \mathcal{U}^*} K^\frac{1}{1+\epsilon} + d\sqrt{H \mathcal{V}^* K})$. Here, $H$ is length of the episode, and$\mathcal{U}^*, \mathcal{V}^*$ are instance-dependent quantities scaling withthe central moment of reward and value functions, respectively. We also providea matching minimax lower bound $\Omega(d H K^{\frac{1}{1+\epsilon}} + d\sqrt{H^3 K})$ to demonstrate the optimality of our algorithm in the worstcase. Our result is achieved via a novel robust self-normalized concentrationinequality that may be of independent interest in handling heavy-tailed noisein general online regression problems.</description><author>Jiayi Huang, Han Zhong, Liwei Wang, Lin F. Yang</author><pubDate>Thu, 07 Mar 2024 15:29:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06836v3</guid></item><item><title>Unbiased Estimator for Distorted Conics in Camera Calibration</title><link>http://arxiv.org/abs/2403.04583v1</link><description>In the literature, points and conics have been major features for camerageometric calibration. Although conics are more informative features thanpoints, the loss of the conic property under distortion has critically limitedthe utility of conic features in camera calibration. Many existing approachesaddressed conic-based calibration by ignoring distortion or introducing 3Dspherical targets to circumvent this limitation. In this paper, we present anovel formulation for conic-based calibration using moments. Our derivation isbased on the mathematical finding that the first moment can be estimatedwithout bias even under distortion. This allows us to track moment changesduring projection and distortion, ensuring the preservation of the first momentof the distorted conic. With an unbiased estimator, the circular patterns canbe accurately detected at the sub-pixel level and can now be fully exploitedfor an entire calibration pipeline, resulting in significantly improvedcalibration. The entire code is readily available fromgithub.com/ChaehyeonSong/discocal.</description><author>Chaehyeon Song, Jaeho Shin, Myung-Hwan Jeon, Jongwoo Lim, Ayoung Kim</author><pubDate>Thu, 07 Mar 2024 15:29:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04583v1</guid></item><item><title>Beyond Major Product Prediction: Reproducing Reaction Mechanisms with Machine Learning Models Trained on a Large-Scale Mechanistic Dataset</title><link>http://arxiv.org/abs/2403.04580v1</link><description>Mechanistic understanding of organic reactions can facilitate reactiondevelopment, impurity prediction, and in principle, reaction discovery. Whileseveral machine learning models have sought to address the task of predictingreaction products, their extension to predicting reaction mechanisms has beenimpeded by the lack of a corresponding mechanistic dataset. In this study, weconstruct such a dataset by imputing intermediates between experimentallyreported reactants and products using expert reaction templates and trainseveral machine learning models on the resulting dataset of 5,184,184elementary steps. We explore the performance and capabilities of these models,focusing on their ability to predict reaction pathways and recapitulate theroles of catalysts and reagents. Additionally, we demonstrate the potential ofmechanistic models in predicting impurities, often overlooked by conventionalmodels. We conclude by evaluating the generalizability of mechanistic models tonew reaction types, revealing challenges related to dataset diversity,consecutive predictions, and violations of atom conservation.</description><author>Joonyoung F. Joung, Mun Hong Fong, Jihye Roh, Zhengkai Tu, John Bradshaw, Connor W. Coley</author><pubDate>Thu, 07 Mar 2024 15:26:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04580v1</guid></item><item><title>Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition</title><link>http://arxiv.org/abs/2403.04577v1</link><description>Web tables contain a large amount of valuable knowledge and have inspiredtabular language models aimed at tackling table interpretation (TI) tasks. Inthis paper, we analyse a widely used benchmark dataset for evaluation of TItasks, particularly focusing on the entity linking task. Our analysis revealsthat this dataset is overly simplified, potentially reducing its effectivenessfor thorough evaluation and failing to accurately represent tables as theyappear in the real-world. To overcome this drawback, we construct and annotatea new more challenging dataset. In addition to introducing the new dataset, wealso introduce a novel problem aimed at addressing the entity linking task:named entity recognition within cells. Finally, we propose a promptingframework for evaluating the newly developed large language models (LLMs) onthis novel TI task. We conduct experiments on prompting LLMs under varioussettings, where we use both random and similarity-based selection to choose theexamples presented to the models. Our ablation study helps us gain insightsinto the impact of the few-shot examples. Additionally, we perform qualitativeanalysis to gain insights into the challenges encountered by the models and tounderstand the limitations of the proposed dataset.</description><author>Aneta Koleva, Martin Ringsquandl, Ahmed Hatem, Thomas Runkler, Volker Tresp</author><pubDate>Thu, 07 Mar 2024 15:22:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04577v1</guid></item><item><title>Machine learning and information theory concepts towards an AI Mathematician</title><link>http://arxiv.org/abs/2403.04571v1</link><description>The current state-of-the-art in artificial intelligence is impressive,especially in terms of mastery of language, but not so much in terms ofmathematical reasoning. What could be missing? Can we learn something usefulabout that gap from how the brains of mathematicians go about their craft? Thisessay builds on the idea that current deep learning mostly succeeds at system 1abilities -- which correspond to our intuition and habitual behaviors -- butstill lacks something important regarding system 2 abilities -- which includereasoning and robust uncertainty estimation. It takes aninformation-theoretical posture to ask questions about what constitutes aninteresting mathematical statement, which could guide future work in craftingan AI mathematician. The focus is not on proving a given theorem but ondiscovering new and interesting conjectures. The central hypothesis is that adesirable body of theorems better summarizes the set of all provablestatements, for example by having a small description length while at the sametime being close (in terms of number of derivation steps) to many provablestatements.</description><author>Yoshua Bengio, Nikolay Malkin</author><pubDate>Thu, 07 Mar 2024 15:12:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04571v1</guid></item><item><title>Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition</title><link>http://arxiv.org/abs/2403.04568v1</link><description>We study reinforcement learning with linear function approximation, unknowntransition, and adversarial losses in the bandit feedback setting.Specifically, we focus on linear mixture MDPs whose transition kernel is alinear mixture model. We propose a new algorithm that attains an$\widetilde{O}(d\sqrt{HS^3K} + \sqrt{HSAK})$ regret with high probability,where $d$ is the dimension of feature mappings, $S$ is the size of state space,$A$ is the size of action space, $H$ is the episode length and $K$ is thenumber of episodes. Our result strictly improves the previous best-known$\widetilde{O}(dS^2 \sqrt{K} + \sqrt{HSAK})$ result in Zhao et al. (2023a)since $H \leq S$ holds by the layered MDP structure. Our advancements areprimarily attributed to (i) a new least square estimator for the transitionparameter that leverages the visit information of all states, as opposed toonly one state in prior work, and (ii) a new self-normalized concentrationtailored specifically to handle non-independent noises, originally proposed inthe dynamic assortment area and firstly applied in reinforcement learning tohandle correlations between different states.</description><author>Long-Fei Li, Peng Zhao, Zhi-Hua Zhou</author><pubDate>Thu, 07 Mar 2024 15:03:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04568v1</guid></item><item><title>Out of the Room: Generalizing Event-Based Dynamic Motion Segmentation for Complex Scenes</title><link>http://arxiv.org/abs/2403.04562v1</link><description>Rapid and reliable identification of dynamic scene parts, also known asmotion segmentation, is a key challenge for mobile sensors. Contemporary RGBcamera-based methods rely on modeling camera and scene properties however, areoften under-constrained and fall short in unknown categories. Event camerashave the potential to overcome these limitations, but corresponding methodshave only been demonstrated in smaller-scale indoor environments withsimplified dynamic objects. This work presents an event-based method forclass-agnostic motion segmentation that can successfully be deployed acrosscomplex large-scale outdoor environments too. To this end, we introduce a noveldivide-and-conquer pipeline that combines: (a) ego-motion compensated events,computed via a scene understanding module that predicts monocular depth andcamera pose as auxiliary tasks, and (b) optical flow from a dedicated opticalflow module. These intermediate representations are then fed into asegmentation module that predicts motion segmentation masks. A noveltransformer-based temporal attention module in the segmentation module buildscorrelations across adjacent 'frames' to get temporally consistent segmentationmasks. Our method sets the new state-of-the-art on the classic EV-IMO benchmark(indoors), where we achieve improvements of 2.19 moving object IoU (2.22 mIoU)and 4.52 point IoU respectively, as well as on a newly-generated motionsegmentation and tracking benchmark (outdoors) based on the DSEC event dataset,termed DSEC-MOTS, where we show improvement of 12.91 moving object IoU.</description><author>Stamatios Georgoulis, Weining Ren, Alfredo Bochicchio, Daniel Eckert, Yuanyou Li, Abel Gawel</author><pubDate>Thu, 07 Mar 2024 14:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04562v1</guid></item><item><title>Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization</title><link>http://arxiv.org/abs/2309.06692v2</link><description>Federated learning (FL) is a privacy-preserving paradigm for collaborativelytraining a global model from decentralized clients. However, the performance ofFL is hindered by non-independent and identically distributed (non-IID) dataand device heterogeneity. In this work, we revisit this key challenge throughthe lens of gradient conflicts on the server side. Specifically, we firstinvestigate the gradient conflict phenomenon among multiple clients and revealthat stronger heterogeneity leads to more severe gradient conflicts. To tacklethis issue, we propose FedGH, a simple yet effective method that mitigateslocal drifts through Gradient Harmonization. This technique projects onegradient vector onto the orthogonal plane of the other within conflictingclient pairs. Extensive experiments demonstrate that FedGH consistentlyenhances multiple state-of-the-art FL baselines across diverse benchmarks andnon-IID scenarios. Notably, FedGH yields more significant improvements inscenarios with stronger heterogeneity. As a plug-and-play module, FedGH can beseamlessly integrated into any FL framework without requiring hyperparametertuning.</description><author>Xinyu Zhang, Weiyu Sun, Ying Chen</author><pubDate>Thu, 07 Mar 2024 14:57:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06692v2</guid></item><item><title>Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology</title><link>http://arxiv.org/abs/2403.04558v1</link><description>Deep Learning models have been successfully utilized to extract clinicallyactionable insights from routinely available histology data. Generally, thesemodels require annotations performed by clinicians, which are scarce and costlyto generate. The emergence of self-supervised learning (SSL) methods removethis barrier, allowing for large-scale analyses on non-annotated data. However,recent SSL approaches apply increasingly expansive model architectures andlarger datasets, causing the rapid escalation of data volumes, hardwareprerequisites, and overall expenses, limiting access to these resources to fewinstitutions. Therefore, we investigated the complexity of contrastive SSL incomputational pathology in relation to classification performance with theutilization of consumer-grade hardware. Specifically, we analyzed the effectsof adaptations in data volume, architecture, and algorithms on downstream clas-sification tasks, emphasizing their impact on computational resources. Wetrained breast cancer foundation models on a large public patient cohort andvalidated them on various downstream classification tasks in a weaklysupervised manner on two external public patient cohorts. Our experimentsdemonstrate that we can improve downstream classification performance whilstreducing SSL training duration by 90%. In summary, we propose a set ofadaptations which enable the utilization of SSL in computational pathology innon-resource abundant environments.</description><author>Tim Lenz, Omar S. M. El Nahhas, Marta Ligero, Jakob Nikolas Kather</author><pubDate>Thu, 07 Mar 2024 14:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04558v1</guid></item><item><title>Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation with a Unified Model</title><link>http://arxiv.org/abs/2303.06965v5</link><description>Chemical reactions are the fundamental building blocks of drug design andorganic chemistry research. In recent years, there has been a growing need fora large-scale deep-learning framework that can efficiently capture the basicrules of chemical reactions. In this paper, we have proposed a unifiedframework that addresses both the reaction representation learning and moleculegeneration tasks, which allows for a more holistic approach. Inspired by theorganic chemistry mechanism, we develop a novel pretraining framework thatenables us to incorporate inductive biases into the model. Our frameworkachieves state-of-the-art results on challenging downstream tasks. Bypossessing chemical knowledge, our generative framework overcome thelimitations of current molecule generation models that rely on a small numberof reaction templates. In the extensive experiments, our model generatessynthesizable drug-like structures of high quality. Overall, our work presentsa significant step toward a large-scale deep-learning framework for a varietyof reaction-based applications.</description><author>Bo Qiang, Yiran Zhou, Yuheng Ding, Ningfeng Liu, Song Song, Liangren Zhang, Bo Huang, Zhenming Liu</author><pubDate>Thu, 07 Mar 2024 14:51:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06965v5</guid></item><item><title>Improvements &amp; Evaluations on the MLCommons CloudMask Benchmark</title><link>http://arxiv.org/abs/2403.04553v1</link><description>In this paper, we report the performance benchmarking results of deeplearning models on MLCommons' Science cloud-masking benchmark using ahigh-performance computing cluster at New York University (NYU): NYU Greene.MLCommons is a consortium that develops and maintains several scientificbenchmarks that can benefit from developments in AI. We provide a descriptionof the cloud-masking benchmark task, updated code, and the best model for thisbenchmark when using our selected hyperparameter settings. Our benchmarkingresults include the highest accuracy achieved on the NYU system as well as theaverage time taken for both training and inference on the benchmark acrossseveral runs/seeds. Our code can be found on GitHub. MLCommons team has beenkept informed about our progress and may use the developed code for theirfuture work.</description><author>Varshitha Chennamsetti, Laiba Mehnaz, Dan Zhao, Banani Ghosh, Sergey V. Samsonau</author><pubDate>Thu, 07 Mar 2024 14:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04553v1</guid></item><item><title>Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI</title><link>http://arxiv.org/abs/2403.04551v1</link><description>Characterizing samples that are difficult to learn from is crucial todeveloping highly performant ML models. This has led to numerous HardnessCharacterization Methods (HCMs) that aim to identify "hard" samples. However,there is a lack of consensus regarding the definition and evaluation of"hardness". Unfortunately, current HCMs have only been evaluated on specifictypes of hardness and often only qualitatively or with respect to downstreamperformance, overlooking the fundamental quantitative identification task. Weaddress this gap by presenting a fine-grained taxonomy of hardness types.Additionally, we propose the Hardness Characterization Analysis Toolkit(H-CAT), which supports comprehensive and quantitative benchmarking of HCMsacross the hardness taxonomy and can easily be extended to new HCMs, hardnesstypes, and datasets. We use H-CAT to evaluate 13 different HCMs across 8hardness types. This comprehensive evaluation encompassing over 14K setupsuncovers strengths and weaknesses of different HCMs, leading to practical tipsto guide HCM selection and future development. Our findings highlight the needfor more comprehensive HCM evaluation, while we hope our hardness taxonomy andtoolkit will advance the principled evaluation and uptake of data-centric AImethods.</description><author>Nabeel Seedat, Fergus Imrie, Mihaela van der Schaar</author><pubDate>Thu, 07 Mar 2024 14:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04551v1</guid></item><item><title>Explainable Face Verification via Feature-Guided Gradient Backpropagation</title><link>http://arxiv.org/abs/2403.04549v1</link><description>Recent years have witnessed significant advancement in face recognition (FR)techniques, with their applications widely spread in people's lives andsecurity-sensitive areas. There is a growing need for reliable interpretationsof decisions of such systems. Existing studies relying on various mechanismshave investigated the usage of saliency maps as an explanation approach, butsuffer from different limitations. This paper first explores the spatialrelationship between face image and its deep representation via gradientbackpropagation. Then a new explanation approach FGGB has been conceived, whichprovides precise and insightful similarity and dissimilarity saliency maps toexplain the "Accept" and "Reject" decision of an FR system. Extensive visualpresentation and quantitative measurement have shown that FGGB achievessuperior performance in both similarity and dissimilarity maps when compared tocurrent state-of-the-art explainable face verification approaches.</description><author>Yuhang Lu, Zewei Xu, Touradj Ebrahimi</author><pubDate>Thu, 07 Mar 2024 14:43:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04549v1</guid></item><item><title>CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?</title><link>http://arxiv.org/abs/2403.04547v1</link><description>We study the effectiveness of data-balancing for mitigating biases incontrastive language-image pretraining (CLIP), identifying areas of strengthand limitation. First, we reaffirm prior conclusions that CLIP models caninadvertently absorb societal stereotypes. To counter this, we present a novelalgorithm, called Multi-Modal Moment Matching (M4), designed to reduce bothrepresentation and association biases (i.e. in first- and second-orderstatistics) in multimodal data. We use M4 to conduct an in-depth analysistaking into account various factors, such as the model, representation, anddata size. Our study also explores the dynamic nature of how CLIP learns andunlearns biases. In particular, we find that fine-tuning is effective incountering representation biases, though its impact diminishes for associationbiases. Also, data balancing has a mixed impact on quality: it tends to improveclassification but can hurt retrieval. Interestingly, data and architecturalimprovements seem to mitigate the negative impact of data balancing onperformance; e.g. applying M4 to SigLIP-B/16 with data quality filters improvesCOCO image-to-text retrieval @5 from 86% (without data balancing) to 87% andImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude withrecommendations for improving the efficacy of data balancing in multimodalsystems.</description><author>Ibrahim Alabdulmohsin, Xiao Wang, Andreas Steiner, Priya Goyal, Alexander D'Amour, Xiaohua Zhai</author><pubDate>Thu, 07 Mar 2024 14:43:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04547v1</guid></item></channel></rss>