<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 31 Jan 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>A simple, strong baseline for building damage detection on the xBD dataset</title><link>http://arxiv.org/abs/2401.17271v1</link><description>We construct a strong baseline method for building damage detection bystarting with the highly-engineered winning solution of the xView2 competition,and gradually stripping away components. This way, we obtain a much simplermethod, while retaining adequate performance. We expect the simplified solutionto be more widely and easily applicable. This expectation is based on thereduced complexity, as well as the fact that we choose hyperparameters based onsimple heuristics, that transfer to other datasets. We then re-arrange thexView2 dataset splits such that the test locations are not seen duringtraining, contrary to the competition setup. In this setting, we find that boththe complex and the simplified model fail to generalize to unseen locations.Analyzing the dataset indicates that this failure to generalize is not only amodel-based problem, but that the difficulty might also be influenced by theunequal class distributions between events. Code, including the baseline model, is available underhttps://github.com/PaulBorneP/Xview2_Strong_Baseline</description><author>Sebastian Gerard, Paul Borne-Pons, Josephine Sullivan</author><pubDate>Tue, 30 Jan 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17271v1</guid></item><item><title>YOLO-World: Real-Time Open-Vocabulary Object Detection</title><link>http://arxiv.org/abs/2401.17270v1</link><description>The You Only Look Once (YOLO) series of detectors have established themselvesas efficient and practical tools. However, their reliance on predefined andtrained object categories limits their applicability in open scenarios.Addressing this limitation, we introduce YOLO-World, an innovative approachthat enhances YOLO with open-vocabulary detection capabilities throughvision-language modeling and pre-training on large-scale datasets.Specifically, we propose a new Re-parameterizable Vision-Language PathAggregation Network (RepVL-PAN) and region-text contrastive loss to facilitatethe interaction between visual and linguistic information. Our method excels indetecting a wide range of objects in a zero-shot manner with high efficiency.On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS onV100, which outperforms many state-of-the-art methods in terms of both accuracyand speed. Furthermore, the fine-tuned YOLO-World achieves remarkableperformance on several downstream tasks, including object detection andopen-vocabulary instance segmentation.</description><author>Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying Shan</author><pubDate>Tue, 30 Jan 2024 18:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17270v1</guid></item><item><title>In-Context Language Learning: Architectures and Algorithms</title><link>http://arxiv.org/abs/2401.12973v2</link><description>Large-scale neural language models exhibit a remarkable capacity forin-context learning (ICL): they can infer novel functions from datasetsprovided as input. Most of our current understanding of when and how ICL arisescomes from LMs trained on extremely simple learning problems like linearregression and associative recall. There remains a significant gap betweenthese model problems and the "real" ICL exhibited by LMs trained on large textcorpora, which involves not just retrieval and function approximation butfree-form generation of language and other structured outputs. In this paper,we study ICL through the lens of a new family of model problems we term incontext language learning (ICLL). In ICLL, LMs are presented with a set ofstrings from a formal language, and must generate additional strings from thesame language. We focus on in-context learning of regular languages generatedby random finite automata. We evaluate a diverse set of neural sequence models(including several RNNs, Transformers, and state-space model variants) onregular ICLL tasks, aiming to answer three questions: (1) Which model classesare empirically capable of ICLL? (2) What algorithmic solutions do successfulmodels implement to perform ICLL? (3) What architectural changes can improveICLL in less performant models? We first show that Transformers significantlyoutperform neural sequence models with recurrent or convolutionalrepresentations on ICLL tasks. Next, we provide evidence that their ability todo so relies on specialized "n-gram heads" (higher-order variants of inductionheads) that compute input-conditional next-token distributions. Finally, weshow that hard-wiring these heads into neural models improves performance notjust on ICLL, but natural language modeling -- improving the perplexity of340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.</description><author>Ekin Aky√ºrek, Bailin Wang, Yoon Kim, Jacob Andreas</author><pubDate>Tue, 30 Jan 2024 18:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12973v2</guid></item><item><title>Effect of Weight Quantization on Learning Models by Typical Case Analysis</title><link>http://arxiv.org/abs/2401.17269v1</link><description>This paper examines the quantization methods used in large-scale dataanalysis models and their hyperparameter choices. The recent surge in dataanalysis scale has significantly increased computational resource requirements.To address this, quantizing model weights has become a prevalent practice indata analysis applications such as deep learning. Quantization is particularlyvital for deploying large models on devices with limited computationalresources. However, the selection of quantization hyperparameters, like thenumber of bits and value range for weight quantization, remains anunderexplored area. In this study, we employ the typical case analysis fromstatistical physics, specifically the replica method, to explore the impact ofhyperparameters on the quantization of simple learning models. Our analysisyields three key findings: (i) an unstable hyperparameter phase, known asreplica symmetry breaking, occurs with a small number of bits and a largequantization width; (ii) there is an optimal quantization width that minimizeserror; and (iii) quantization delays the onset of overparameterization, helpingto mitigate overfitting as indicated by the double descent phenomenon. We alsodiscover that non-uniform quantization can enhance stability. Additionally, wedevelop an approximate message-passing algorithm to validate our theoreticalresults.</description><author>Shuhei Kashiwamura, Ayaka Sakata, Masaaki Imaizumi</author><pubDate>Tue, 30 Jan 2024 18:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17269v1</guid></item><item><title>Weaver: Foundation Models for Creative Writing</title><link>http://arxiv.org/abs/2401.17268v1</link><description>This work introduces Weaver, our first family of large language models (LLMs)dedicated to content creation. Weaver is pre-trained on a carefully selectedcorpus that focuses on improving the writing capabilities of large languagemodels. We then fine-tune Weaver for creative and professional writing purposesand align it to the preference of professional writers using a suit of novelmethods for instruction data synthesis and LLM alignment, making it able toproduce more human-like texts and follow more diverse instructions for contentcreation. The Weaver family consists of models of Weaver Mini (1.8B), WeaverBase (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable fordifferent applications and can be dynamically dispatched by a routing agentaccording to query complexity to balance response quality and computation cost.Evaluation on a carefully curated benchmark for assessing the writingcapabilities of LLMs shows Weaver models of all sizes outperform generalistLLMs several times larger than them. Notably, our most-capable Weaver Ultramodel surpasses GPT-4, a state-of-the-art generalist LLM, on various writingscenarios, demonstrating the advantage of training specialized LLMs for writingpurposes. Moreover, Weaver natively supports retrieval-augmented generation(RAG) and function calling (tool usage). We present various use cases of theseabilities for improving AI-assisted writing systems, including integration ofexternal knowledge bases, tools, or APIs, and providing personalized writingassistance. Furthermore, we discuss and summarize a guideline and bestpractices for pre-training and fine-tuning domain-specific LLMs.</description><author>Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang, Wangchunshu Zhou</author><pubDate>Tue, 30 Jan 2024 18:58:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17268v1</guid></item><item><title>ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models</title><link>http://arxiv.org/abs/2401.17267v1</link><description>Chemical reactivity models are developed to predict chemical reactionoutcomes in the form of classification (success/failure) or regression (productyield) tasks. The vast majority of the reported models are trained solely onchemical information such as reactants, products, reagents, and solvents, butnot on the details of a synthetic protocol. Herein incorporation of proceduraltext with the aim to augment the Graphormer reactivity model and improve itsaccuracy is presented. Two major approaches are used: training an adapterGraphormer model that is provided with a GPT-2-derived latent representation ofthe text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of adataset with the LLaMA 2 model followed by training the Graphormer on anextended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance thediscernment of unpromising reactions, thereby providing more accurate modelswith improved specificity.</description><author>Aline Hartgers, Ramil Nugmanov, Kostiantyn Chernichenko, Joerg Kurt Wegner</author><pubDate>Tue, 30 Jan 2024 18:57:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17267v1</guid></item><item><title>Proactive Detection of Voice Cloning with Localized Watermarking</title><link>http://arxiv.org/abs/2401.17264v1</link><description>In the rapidly evolving field of speech generative models, there is apressing need to ensure audio authenticity against the risks of voice cloning.We present AudioSeal, the first audio watermarking technique designedspecifically for localized detection of AI-generated speech. AudioSeal employsa generator/detector architecture trained jointly with a localization loss toenable localized watermark detection up to the sample level, and a novelperceptual loss inspired by auditory masking, that enables AudioSeal to achievebetter imperceptibility. AudioSeal achieves state-of-the-art performance interms of robustness to real life audio manipulations and imperceptibility basedon automatic and human evaluation metrics. Additionally, AudioSeal is designedwith a fast, single-pass detector, that significantly surpasses existing modelsin speed - achieving detection up to two orders of magnitude faster, making itideal for large-scale and real-time applications.</description><author>Robin San Roman, Pierre Fernandez, Alexandre D√©fossez, Teddy Furon, Tuan Tran, Hady Elsahar</author><pubDate>Tue, 30 Jan 2024 18:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17264v1</guid></item><item><title>Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks</title><link>http://arxiv.org/abs/2401.17263v1</link><description>Despite advances in AI alignment, language models (LM) remain vulnerable toadversarial attacks or jailbreaking, in which adversaries modify input promptsto induce harmful behavior. While some defenses have been proposed, they focuson narrow threat models and fall short of a strong defense, which we positshould be effective, universal, and practical. To achieve this, we propose thefirst adversarial objective for defending LMs against jailbreaking attacks andan algorithm, robust prompt optimization (RPO), that uses gradient-based tokenoptimization to enforce harmless outputs. This results in an easily accessiblesuffix that significantly improves robustness to both jailbreaks seen duringoptimization and unknown, held-out jailbreaks, reducing the attack success rateon Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we findthat RPO has a minor effect on normal LM use, is successful under adaptiveattacks, and can transfer to black-box models, reducing the success rate of thestrongest attack on GPT-4 from 92% to 6%.</description><author>Andy Zhou, Bo Li, Haohan Wang</author><pubDate>Tue, 30 Jan 2024 18:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17263v1</guid></item><item><title>War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars</title><link>http://arxiv.org/abs/2311.17227v2</link><description>Can we avoid wars at the crossroads of history? This question has beenpursued by individuals, scholars, policymakers, and organizations throughouthuman history. In this research, we attempt to answer the question based on therecent advances of Artificial Intelligence (AI) and Large Language Models(LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, tosimulate the participating countries, their decisions, and the consequences, inhistorical international conflicts, including the World War I (WWI), the WorldWar II (WWII), and the Warring States Period (WSP) in Ancient China. Byevaluating the simulation effectiveness, we examine the advancements andlimitations of cutting-edge AI systems' abilities in studying complexcollective human behaviors such as international conflicts under diversesettings. In these simulations, the emergent interactions among agents alsooffer a novel perspective for examining the triggers and conditions that leadto war. Our findings offer data-driven and AI-augmented insights that canredefine how we approach conflict resolution and peacekeeping strategies. Theimplications stretch beyond historical analysis, offering a blueprint for usingAI to understand human history and possibly prevent future internationalconflicts. Code and data are available at\url{https://github.com/agiresearch/WarAgent}.</description><author>Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang</author><pubDate>Tue, 30 Jan 2024 18:53:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17227v2</guid></item><item><title>You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation</title><link>http://arxiv.org/abs/2401.17258v1</link><description>In this paper, we introduce YONOS-SR, a novel stable diffusion-based approachfor image super-resolution that yields state-of-the-art results using only asingle DDIM step. We propose a novel scale distillation approach to train ourSR model. Instead of directly training our SR model on the scale factor ofinterest, we start by training a teacher model on a smaller magnificationscale, thereby making the SR problem simpler for the teacher. We then train astudent model for a higher magnification scale, using the predictions of theteacher as a target during the training. This process is repeated iterativelyuntil we reach the target scale factor of the final model. The rationale behindour scale distillation is that the teacher aids the student diffusion modeltraining by i) providing a target adapted to the current noise level ratherthan using the same target coming from ground truth data for all noise levelsand ii) providing an accurate target as the teacher has a simpler task tosolve. We empirically show that the distilled model significantly outperformsthe model trained for high scales directly, specifically with few steps duringinference. Having a strong diffusion model that requires only one step allowsus to freeze the U-Net and fine-tune the decoder on top of it. We show that thecombination of spatially distilled U-Net and fine-tuned decoder outperformsstate-of-the-art methods requiring 200 steps with only one single step.</description><author>Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos</author><pubDate>Tue, 30 Jan 2024 18:49:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17258v1</guid></item><item><title>Weak-to-Strong Jailbreaking on Large Language Models</title><link>http://arxiv.org/abs/2401.17256v1</link><description>Although significant efforts have been dedicated to aligning large languagemodels (LLMs), red-teaming reports suggest that these carefully aligned LLMscould still be jailbroken through adversarial prompts, tuning, or decoding.Upon examining the jailbreaking vulnerability of aligned LLMs, we observe thatthe decoding distributions of jailbroken and aligned models differ only in theinitial generations. This observation motivates us to propose theweak-to-strong jailbreaking attack, where adversaries can utilize smallerunsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantlylarger aligned LLMs (e.g., 70B). To jailbreak, one only needs to additionallydecode two smaller LLMs once, which involves minimal computation and latencycompared to decoding the larger LLMs. The efficacy of this attack isdemonstrated through experiments conducted on five models from three differentorganizations. Our study reveals a previously unnoticed yet efficient way ofjailbreaking, exposing an urgent safety issue that needs to be considered whenaligning LLMs. As an initial attempt, we propose a defense strategy to protectagainst such attacks, but creating more advanced defenses remains challenging.The code for replicating the method is available athttps://github.com/XuandongZhao/weak-to-strong</description><author>Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang</author><pubDate>Tue, 30 Jan 2024 18:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17256v1</guid></item><item><title>SLIC: A Learned Image Codec Using Structure and Color</title><link>http://arxiv.org/abs/2401.17246v1</link><description>We propose the structure and color based learned image codec (SLIC) in whichthe task of compression is split into that of luminance and chrominance. Thedeep learning model is built with a novel multi-scale architecture for Y and UVchannels in the encoder, where the features from various stages are combined toobtain the latent representation. An autoregressive context model is employedfor backward adaptation and a hyperprior block for forward adaptation. Variousexperiments are carried out to study and analyze the performance of theproposed model, and to compare it with other image codecs. We also illustratethe advantages of our method through the visualization of channel impulseresponses, latent channels and various ablation studies. The model achievesBj{\o}ntegaard delta bitrate gains of 7.5% and 4.66% in terms of MS-SSIM andCIEDE2000 metrics with respect to other state-of-the-art reference codecs.</description><author>Srivatsa Prativadibhayankaram, Mahadev Prasad Panda, Thomas Richter, Heiko Sparenberg, Siegfried F√∂√üel, Andr√© Kaup</author><pubDate>Tue, 30 Jan 2024 18:39:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17246v1</guid></item><item><title>LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation</title><link>http://arxiv.org/abs/2401.17244v1</link><description>Reducing hallucination of Large Language Models (LLMs) is imperative for usein the sciences where reproducibility is crucial. However, LLMs inherently lacklong-term memory, making it a nontrivial, ad hoc, and inevitably biased task tofine-tune them on domain-specific literature and data. Here we introduce LLaMP,a multimodal retrieval-augmented generation (RAG) framework of multipledata-aware reasoning-and-acting (ReAct) agents that dynamically interact withcomputational and experimental data on Materials Project (MP). Withoutfine-tuning, LLaMP demonstrates an ability to comprehend and integrate variousmodalities of materials science concepts, fetch relevant data stores on thefly, process higher-order data (such as crystal structures and elastictensors), and summarize multi-step procedures for solid-state synthesis. Weshow that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge,reducing a 5.21% MAPE on frequently-documented bandgaps and a significant1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive frommixed data sources. Additionally, LLaMP substantially reduces the hallucinatedvolumetric strain in a diamond cubic silicon structure from 66.3% to 0. Theproposed framework offers an intuitive and nearly hallucination-free approachto exploring materials informatics and establishes a pathway for knowledgedistillation and fine-tuning other language models. We envision the frameworkas a valuable component for scientific hypotheses and a foundation for futureautonomous laboratories where multiple LLM agents communicate and cooperatewith robotics to drive material synthesis and chemical reactions withouthard-coded human logic and intervention.</description><author>Yuan Chiang, Chia-Hong Chou, Janosh Riebesell</author><pubDate>Tue, 30 Jan 2024 18:37:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17244v1</guid></item><item><title>Enhancing Low-Order Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier--Stokes Equations</title><link>http://arxiv.org/abs/2310.18897v2</link><description>The growing computing power over the years has enabled simulations to becomemore complex and accurate. While immensely valuable for scientific discoveryand problem-solving, however, high-fidelity simulations come with significantcomputational demands. As a result, it is common to run a low-fidelity modelwith a subgrid-scale model to reduce the computational cost, but selecting theappropriate subgrid-scale models and tuning them are challenging. We propose anovel method for learning the subgrid-scale model effects when simulatingpartial differential equations augmented by neural ordinary differentialoperators in the context of discontinuous Galerkin (DG) spatial discretization.Our approach learns the missing scales of the low-order DG solver at acontinuous level and hence improves the accuracy of the low-order DGapproximations as well as accelerates the filtered high-order DG simulationswith a certain degree of precision. We demonstrate the performance of ourapproach through multidimensional Taylor-Green vortex examples at differentReynolds numbers and times, which cover laminar, transitional, and turbulentregimes. The proposed method not only reconstructs the subgrid-scale from thelow-order (1st-order) approximation but also speeds up the filtered high-orderDG (6th-order) simulation by two orders of magnitude.</description><author>Shinhoo Kang, Emil M. Constantinescu</author><pubDate>Tue, 30 Jan 2024 18:35:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18897v2</guid></item><item><title>Scaling laws for language encoding models in fMRI</title><link>http://arxiv.org/abs/2305.11863v4</link><description>Representations from transformer-based unidirectional language models areknown to be effective at predicting brain responses to natural language.However, most studies comparing language models to brains have used GPT-2 orsimilarly sized language models. Here we tested whether larger open-sourcemodels such as those from the OPT and LLaMA families are better at predictingbrain responses recorded using fMRI. Mirroring scaling results from othercontexts, we found that brain prediction performance scales logarithmicallywith model size from 125M to 30B parameter models, with ~15% increased encodingperformance as measured by correlation with a held-out test set across 3subjects. Similar logarithmic behavior was observed when scaling the size ofthe fMRI training set. We also characterized scaling for acoustic encodingmodels that use HuBERT, WavLM, and Whisper, and we found comparableimprovements with model size. A noise ceiling analysis of these large,high-performance encoding models showed that performance is nearing thetheoretical maximum for brain areas such as the precuneus and higher auditorycortex. These results suggest that increasing scale in both models and datawill yield incredibly effective models of language processing in the brain,enabling better scientific understanding as well as applications such asdecoding.</description><author>Richard Antonello, Aditya Vaidya, Alexander G. Huth</author><pubDate>Tue, 30 Jan 2024 18:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11863v4</guid></item><item><title>Ambiguity-Aware In-Context Learning with Large Language Models</title><link>http://arxiv.org/abs/2309.07900v2</link><description>In-context learning (ICL) i.e. showing LLMs only a few task-specificdemonstrations has led to downstream gains with no task-specific fine-tuningrequired. However, LLMs are sensitive to the choice of prompts, and therefore acrucial research question is how to select good demonstrations for ICL. Oneeffective strategy is leveraging semantic similarity between the ICLdemonstrations and test inputs by using a text retriever, which however issub-optimal as that does not consider the LLM's existing knowledge about thattask. From prior work (Lyu et al., 2023), we already know that labels pairedwith the demonstrations bias the model predictions. This leads us to ourhypothesis whether considering LLM's existing knowledge about the task,especially with respect to the output label space can help in a betterdemonstration selection strategy. Through extensive experimentation on threetext classification tasks, we find that it is beneficial to not only choosesemantically similar ICL demonstrations but also to choose those demonstrationsthat help resolve the inherent label ambiguity surrounding the test example.Interestingly, we find that including demonstrations that the LLM previouslymis-classified and also fall on the test example's decision boundary, bringsthe most performance gain.</description><author>Lingyu Gao, Aditi Chaudhary, Krishna Srinivasan, Kazuma Hashimoto, Karthik Raman, Michael Bendersky</author><pubDate>Tue, 30 Jan 2024 18:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07900v2</guid></item><item><title>Asynchronous Distributed Genetic Algorithms with Javascript and JSON</title><link>http://arxiv.org/abs/2401.17234v1</link><description>In a connected world, spare CPU cycles are up for grabs, if you only make itsobtention easy enough. In this paper we present a distributed evolutionarycomputation system that uses the computational capabilities of the ubiquituousweb browser. Using Asynchronous Javascript and JSON (Javascript ObjectNotation, a serialization protocol) allows anybody with a web browser (that is,mostly everybody connected to the Internet) to participate in a geneticalgorithm experiment with little effort, or none at all. Since, in this case,computing becomes a social activity and is inherently impredictable, in thispaper we will explore the performance of this kind of virtual computer bysolving simple problems such as the Royal Road function and analyzing how manymachines and evaluations it yields. We will also examine possible performancebottlenecks and how to solve them, and, finally, issue some advice on how toset up this kind of experiments to maximize turnout and, thus, performance.</description><author>Juan Juli√°n Merelo, Pedro A. Castillo, Juan Luis Jim√©nez Laredo, Antonio M. Mora, Alberto Prieto</author><pubDate>Tue, 30 Jan 2024 18:23:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17234v1</guid></item><item><title>ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment</title><link>http://arxiv.org/abs/2401.17231v1</link><description>Despite the remarkable strides made in artificial intelligence, currentobject recognition models still lag behind in emulating the mechanism of visualinformation processing in human brains. Recent studies have highlighted thepotential of using neural data to mimic brain processing; however, these oftenreply on invasive neural recordings from non-human subjects, leaving a criticalgap in our understanding of human visual perception and the development of morehuman brain-like vision models. Addressing this gap, we present, for the firsttime, "Re(presentational)Al(ignment)net", a vision model aligned with humanbrain activity based on non-invasive EEG recordings, demonstrating asignificantly higher similarity to human brain representations. Our innovativeimage-to-brain multi-layer encoding alignment framework not only optimizesmultiple layers of the model, marking a substantial leap in neural alignment,but also enables the model to efficiently learn and mimic human brain's visualrepresentational patterns across object categories and different neural datamodalities. Furthermore, we discover that alignment with human brainrepresentations improves the model's adversarial robustness. Our findingssuggest that ReAlnet sets a new precedent in the field, bridging the gapbetween artificial and human vision, and paving the way for more brain-likeartificial intelligence systems.</description><author>Zitong Lu, Yile Wang, Julie D. Golomb</author><pubDate>Tue, 30 Jan 2024 18:18:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17231v1</guid></item><item><title>ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models</title><link>http://arxiv.org/abs/2401.17230v1</link><description>This paper introduces ESPnet-SPK, a toolkit designed with several objectivesfor training speaker embedding extractors. First, we provide an open-sourceplatform for researchers in the speaker recognition community to effortlesslybuild models. We provide several models, ranging from x-vector to recentSKA-TDNN. Through the modularized architecture design, variants can bedeveloped easily. We also aspire to bridge developed models with other domains,facilitating the broad research community to effortlessly incorporatestate-of-the-art embedding extractors. Pre-trained embedding extractors can beaccessed in an off-the-shelf manner and we demonstrate the toolkit'sversatility by showcasing its integration with two tasks. Another goal is tointegrate with diverse self-supervised learning features. We release areproducible recipe that achieves an equal error rate of 0.39% on the Vox1-Oevaluation protocol using WavLM-Large with ECAPA-TDNN.</description><author>Jee-weon Jung, Wangyou Zhang, Jiatong Shi, Zakaria Aldeneh, Takuya Higuchi, Barry-John Theobald, Ahmed Hussen Abdelaziz, Shinji Watanabe</author><pubDate>Tue, 30 Jan 2024 18:18:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17230v1</guid></item><item><title>Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?</title><link>http://arxiv.org/abs/2306.09267v3</link><description>The rise of Generative Artificial Intelligence systems ("AI systems") hascreated unprecedented social engagement. AI code generation systems provideresponses (output) to questions or requests by accessing the vast library ofopen-source code created by developers over the past few decades. However, theydo so by allegedly stealing the open-source code stored in virtual libraries,known as repositories. This Article focuses on how this happens and whetherthere is a solution that protects innovation and avoids years of litigation. Wealso touch upon the array of issues raised by the relationship between AI andcopyright. Looking ahead, we propose the following: (a) immediate changes tothe licenses for open-source code created by developers that will limit accessand/or use of any open-source code to humans only; (b) we suggest revisions tothe Massachusetts Institute of Technology ("MIT") license so that AI systemsare required to procure appropriate licenses from open-source code developers,which we believe will harmonize standards and build social consensus for thebenefit of all of humanity, rather than promote profit-driven centers ofinnovation; (c) we call for urgent legislative action to protect the future ofAI systems while also promoting innovation; and (d) we propose a shift in theburden of proof to AI systems in obfuscation cases.</description><author>Dimitrios Ioannidis, Jeremy Kepner, Andrew Bowne, Harriet S. Bryant</author><pubDate>Tue, 30 Jan 2024 18:15:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09267v3</guid></item><item><title>Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space using Contrastive Learning</title><link>http://arxiv.org/abs/2401.17228v1</link><description>Recent advances in NLP show that language models retain a discernible levelof knowledge in deontological ethics and moral norms. However, existing worksoften treat morality as binary, ranging from right to wrong. This simplisticview does not capture the nuances of moral judgment. Pluralist moralphilosophers argue that human morality can be deconstructed into a finitenumber of elements, respecting individual differences in moral judgment. Inline with this view, we build a pluralist moral sentence embedding space via astate-of-the-art contrastive learning approach. We systematically investigatethe embedding space by studying the emergence of relationships among moralelements, both quantitatively and qualitatively. Our results show that apluralist approach to morality can be captured in an embedding space. However,moral pluralism is challenging to deduce via self-supervision alone andrequires a supervised approach with human labels.</description><author>Jeongwoo Park, Enrico Liscio, Pradeep K. Murukannaiah</author><pubDate>Tue, 30 Jan 2024 18:15:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17228v1</guid></item><item><title>Evolvable Agents, a Fine Grained Approach for Distributed Evolutionary Computing: Walking towards the Peer-to-Peer Computing Frontiers</title><link>http://arxiv.org/abs/2401.17224v1</link><description>In this work we propose a fine grained approach with self-adaptive migrationrate for distributed evolutionary computation. Our target is to gain someinsights on the effects caused by communication when the algorithm scales. Tothis end, we consider a set of basic topologies in order to avoid theoverlapping of algorithmic effects between communication and topologicalstructures. We analyse the approach viability by comparing how solution qualityand algorithm speed change when the number of processors increases and compareit with an Island model based implementation. A finer-grained approach impliesa better chance of achieving a larger scalable system; such a feature iscrucial concerning large-scale parallel architectures such as Peer-to-Peersystems. In order to check scalability, we perform a threefold experimentalevaluation of this model: First, we concentrate on the algorithmic results whenthe problem scales up to eight nodes in comparison with how it does followingthe Island model. Second, we analyse the computing time speedup of the approachwhile scaling. Finally, we analyse the network performance with the proposedself-adaptive migration rate policy that depends on the link latency andbandwidth. With this experimental setup, our approach shows better scalabilitythan the Island model and a equivalent robustness on the average of the threetest functions under study.</description><author>Juan Luis Jim√©nez Laredo, Pedro A. Castillo, Antonio M. Mora, Juan Juli√°n Merelo</author><pubDate>Tue, 30 Jan 2024 18:11:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17224v1</guid></item><item><title>MouSi: Poly-Visual-Expert Vision-Language Models</title><link>http://arxiv.org/abs/2401.17221v1</link><description>Current large vision-language models (VLMs) often encounter challenges suchas insufficient capabilities of a single visual component and excessively longvisual tokens. These issues can limit the model's effectiveness in accuratelyinterpreting complex visual information and over-lengthy contextualinformation. Addressing these challenges is crucial for enhancing theperformance and applicability of VLMs. This paper proposes the use of ensembleexperts technique to synergizes the capabilities of individual visual encoders,including those skilled in image-text matching, OCR, image segmentation, etc.This technique introduces a fusion network to unify the processing of outputsfrom different visual experts, while bridging the gap between image encodersand pre-trained LLMs. In addition, we explore different positional encodingschemes to alleviate the waste of positional encoding caused by lengthy imagefeature sequences, effectively addressing the issue of position overflow andlength limitations. For instance, in our implementation, this techniquesignificantly reduces the positional occupancy in models like SAM, from asubstantial 4096 to a more efficient and manageable 64 or even down to 1.Experimental results demonstrate that VLMs with multiple experts exhibitconsistently superior performance over isolated visual encoders and mark asignificant performance boost as more experts are integrated. We haveopen-sourced the training code used in this report. All of these resources canbe found on our project website.</description><author>Xiaoran Fan, Tao Ji, Changhao Jiang, Shuo Li, Senjie Jin, Sirui Song, Junke Wang, Boyang Hong, Lu Chen, Guodong Zheng, Ming Zhang, Caishuang Huang, Rui Zheng, Zhiheng Xi, Yuhao Zhou, Shihan Dou, Junjie Ye, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Tue, 30 Jan 2024 18:09:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17221v1</guid></item><item><title>LFM-3D: Learnable Feature Matching Across Wide Baselines Using 3D Signals</title><link>http://arxiv.org/abs/2303.12779v3</link><description>Finding localized correspondences across different images of the same objectis crucial to understand its geometry. In recent years, this problem has seenremarkable progress with the advent of deep learning-based local image featuresand learnable matchers. Still, learnable matchers often underperform when thereexists only small regions of co-visibility between image pairs (i.e. widecamera baselines). To address this problem, we leverage recent progress incoarse single-view geometry estimation methods. We propose LFM-3D, a LearnableFeature Matching framework that uses models based on graph neural networks andenhances their capabilities by integrating noisy, estimated 3D signals to boostcorrespondence estimation. When integrating 3D signals into the matcher model,we show that a suitable positional encoding is critical to effectively make useof the low-dimensional 3D information. We experiment with two different 3Dsignals - normalized object coordinates and monocular depth estimates - andevaluate our method on large-scale (synthetic and real) datasets containingobject-centric image pairs across wide baselines. We observe strong featurematching improvements compared to 2D-only methods, with up to +6% total recalland +28% precision at fixed recall. Additionally, we demonstrate that theresulting improved correspondences lead to much higher relative posing accuracyfor in-the-wild image pairs - up to 8.6% compared to the 2D-only approach.</description><author>Arjun Karpur, Guilherme Perrotta, Ricardo Martin-Brualla, Howard Zhou, Andr√© Araujo</author><pubDate>Tue, 30 Jan 2024 18:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12779v3</guid></item><item><title>GazeGPT: Augmenting Human Capabilities using Gaze-contingent Contextual AI for Smart Eyewear</title><link>http://arxiv.org/abs/2401.17217v1</link><description>Multimodal large language models (LMMs) excel in world knowledge andproblem-solving abilities. Through the use of a world-facing camera andcontextual AI, emerging smart accessories aim to provide a seamless interfacebetween humans and LMMs. Yet, these wearable computing systems lack anunderstanding of the user's attention. We introduce GazeGPT as a new userinteraction paradigm for contextual AI. GazeGPT uses eye tracking to help theLMM understand which object in the world-facing camera view a user is payingattention to. Using extensive user evaluations, we show that thisgaze-contingent mechanism is a faster and more accurate pointing mechanism thanalternatives; that it augments human capabilities by significantly improvingtheir accuracy in a dog-breed classification task; and that it is consistentlyranked as more natural than head- or body-driven selection mechanisms forcontextual AI. Moreover, we prototype a variety of application scenarios thatsuggest GazeGPT could be of significant value to users as part of futureAI-driven personal assistants.</description><author>Robert Konrad, Nitish Padmanaban, J. Gabriel Buckmaster, Kevin C. Boyle, Gordon Wetzstein</author><pubDate>Tue, 30 Jan 2024 18:02:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17217v1</guid></item><item><title>On CNF formulas irredundant with respect to unit clause propagation</title><link>http://arxiv.org/abs/2309.01750v3</link><description>Two CNF formulas are called ucp-equivalent, if they behave in the same waywith respect to the unit clause propagation (UCP). A formula is calleducp-irredundant, if removing any clause leads to a formula which is notucp-equivalent to the original one. As a consequence of known results, theratio of the size of a ucp-irredundant formula and the size of a smallestucp-equivalent formula is at most $n^2$, where $n$ is the number of thevariables. We demonstrate an example of a ucp-irredundant formula for asymmetric definite Horn function which is larger than a smallest ucp-equivalentformula by a factor $\Omega(n/\ln n)$ and, hence, a general upper bound on theabove ratio cannot be smaller than this.</description><author>Petr Savick√Ω</author><pubDate>Tue, 30 Jan 2024 17:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01750v3</guid></item><item><title>ContactGen: Contact-Guided Interactive 3D Human Generation for Partners</title><link>http://arxiv.org/abs/2401.17212v1</link><description>Among various interactions between humans, such as eye contact and gestures,physical interactions by contact can act as an essential moment inunderstanding human behaviors. Inspired by this fact, given a 3D partner humanwith the desired interaction label, we introduce a new task of 3D humangeneration in terms of physical contact. Unlike previous works of interactingwith static objects or scenes, a given partner human can have diverse poses anddifferent contact regions according to the type of interaction. To handle thischallenge, we propose a novel method of generating interactive 3D humans for agiven partner human based on a guided diffusion framework. Specifically, wenewly present a contact prediction module that adaptively estimates potentialcontact regions between two input humans according to the interaction label.Using the estimated potential contact regions as complementary guidances, wedynamically enforce ContactGen to generate interactive 3D humans for a givenpartner human within a guided diffusion model. We demonstrate ContactGen on theCHI3D dataset, where our method generates physically plausible and diverseposes compared to comparison methods.</description><author>Dongjun Gu, Jaehyeok Shim, Jaehoon Jang, Changwoo Kang, Kyungdon Joo</author><pubDate>Tue, 30 Jan 2024 17:57:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17212v1</guid></item><item><title>FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking</title><link>http://arxiv.org/abs/2401.15139v2</link><description>In high-dimensional data analysis, such as financial index tracking orbiomedical applications, it is crucial to select the few relevant variableswhile maintaining control over the false discovery rate (FDR). In theseapplications, strong dependencies often exist among the variables (e.g., stockreturns), which can undermine the FDR control property of existing methods likethe model-X knockoff method or the T-Rex selector. To address this issue, wehave expanded the T-Rex framework to accommodate overlapping groups of highlycorrelated variables. This is achieved by integrating a nearest neighborspenalization mechanism into the framework, which provably controls the FDR atthe user-defined target level. A real-world example of sparse index trackingdemonstrates the proposed method's ability to accurately track the S&amp;P 500index over the past 20 years based on a small number of stocks. An open-sourceimplementation is provided within the R package TRexSelector on CRAN.</description><author>Jasin Machkour, Daniel P. Palomar, Michael Muma</author><pubDate>Tue, 30 Jan 2024 17:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15139v2</guid></item><item><title>High-Dimensional False Discovery Rate Control for Dependent Variables</title><link>http://arxiv.org/abs/2401.15796v2</link><description>Algorithms that ensure reproducible findings from large-scale,high-dimensional data are pivotal in numerous signal processing applications.In recent years, multivariate false discovery rate (FDR) controlling methodshave emerged, providing guarantees even in high-dimensional settings where thenumber of variables surpasses the number of samples. However, these methodsoften fail to reliably control the FDR in the presence of highly dependentvariable groups, a common characteristic in fields such as genomics andfinance. To tackle this critical issue, we introduce a novel framework thataccounts for general dependency structures. Our proposed dependency-aware T-Rexselector integrates hierarchical graphical models within the T-Rex framework toeffectively harness the dependency structure among variables. Leveragingmartingale theory, we prove that our variable penalization mechanism ensuresFDR control. We further generalize the FDR-controlling framework by stating andproving a clear condition necessary for designing both graphical andnon-graphical models that capture dependencies. Additionally, we formulate afully integrated optimal calibration algorithm that concurrently determines theparameters of the graphical model and the T-Rex framework, such that the FDR iscontrolled while maximizing the number of selected variables. Numericalexperiments and a breast cancer survival analysis use-case demonstrate that theproposed method is the only one among the state-of-the-art benchmark methodsthat controls the FDR and reliably detects genes that have been previouslyidentified to be related to breast cancer. An open-source implementation isavailable within the R package TRexSelector on CRAN.</description><author>Jasin Machkour, Michael Muma, Daniel P. Palomar</author><pubDate>Tue, 30 Jan 2024 17:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15796v2</guid></item><item><title>Weighted least-squares approximation with determinantal point processes and generalized volume sampling</title><link>http://arxiv.org/abs/2312.14057v2</link><description>We consider the problem of approximating a function from $L^2$ by an elementof a given $m$-dimensional space $V_m$, associated with some feature map$\varphi$, using evaluations of the function at random points $x_1,\dots,x_n$.After recalling some results on optimal weighted least-squares usingindependent and identically distributed points, we consider weightedleast-squares using projection determinantal point processes (DPP) or volumesampling. These distributions introduce dependence between the points thatpromotes diversity in the selected features $\varphi(x_i)$. We first provide ageneralized version of volume-rescaled sampling yielding quasi-optimalityresults in expectation with a number of samples $n = O(m\log(m))$, that meansthat the expected $L^2$ error is bounded by a constant times the bestapproximation error in $L^2$. Also, further assuming that the function is insome normed vector space $H$ continuously embedded in $L^2$, we further provethat the approximation is almost surely bounded by the best approximation errormeasured in the $H$-norm. This includes the cases of functions from $L^\infty$or reproducing kernel Hilbert spaces. Finally, we present an alternativestrategy consisting in using independent repetitions of projection DPP (orvolume sampling), yielding similar error bounds as with i.i.d. or volumesampling, but in practice with a much lower number of samples. Numericalexperiments illustrate the performance of the different strategies.</description><author>Anthony Nouy, Bertrand Michel</author><pubDate>Tue, 30 Jan 2024 17:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14057v2</guid></item><item><title>Self-Supervised Representation Learning for Nerve Fiber Distribution Patterns in 3D-PLI</title><link>http://arxiv.org/abs/2401.17207v1</link><description>A comprehensive understanding of the organizational principles in the humanbrain requires, among other factors, well-quantifiable descriptors of nervefiber architecture. Three-dimensional polarized light imaging (3D-PLI) is amicroscopic imaging technique that enables insights into the fine-grainedorganization of myelinated nerve fibers with high resolution. Descriptorscharacterizing the fiber architecture observed in 3D-PLI would enabledownstream analysis tasks such as multimodal correlation studies, clustering,and mapping. However, best practices for observer-independent characterizationof fiber architecture in 3D-PLI are not yet available. To this end, we proposethe application of a fully data-driven approach to characterize nerve fiberarchitecture in 3D-PLI images using self-supervised representation learning. Weintroduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes thespatial neighborhood of texture examples across histological brain sections ofa 3D reconstructed volume to sample positive pairs for contrastive learning. Wecombine this sampling strategy with specifically designed image augmentationsto gain robustness to typical variations in 3D-PLI parameter maps. The approachis demonstrated for the 3D reconstructed occipital lobe of a vervet monkeybrain. We show that extracted features are highly sensitive to differentconfigurations of nerve fibers, yet robust to variations between consecutivebrain sections arising from histological processing. We demonstrate theirpractical applicability for retrieving clusters of homogeneous fiberarchitecture and performing data mining for interactively selected templates ofspecific components of fiber architecture such as U-fibers.</description><author>Alexander Oberstrass, Sascha E. A. Muenzing, Meiqi Niu, Nicola Palomero-Gallagher, Christian Schiffer, Markus Axer, Katrin Amunts, Timo Dickscheid</author><pubDate>Tue, 30 Jan 2024 17:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17207v1</guid></item><item><title>Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT Semantic Embeddings K-Means-Infused CRF Model</title><link>http://arxiv.org/abs/2401.17206v1</link><description>Named Entity Recognition (NER) is a sub-task of Natural Language Processing(NLP) that distinguishes entities from unorganized text into predefinedcategorization. In recent years, a lot of Bangla NLP subtasks have receivedquite a lot of attention; but Named Entity Recognition in Bangla still lagsbehind. In this research, we explored the existing state of research in BanglaNamed Entity Recognition. We tried to figure out the limitations that currenttechniques and datasets face, and we would like to address these limitations inour research. Additionally, We developed a Gazetteer that has the ability tosignificantly boost the performance of NER. We also proposed a new NER solutionby taking advantage of state-of-the-art NLP tools that outperform conventionaltechniques.</description><author>Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque</author><pubDate>Tue, 30 Jan 2024 17:47:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17206v1</guid></item><item><title>Adaptive Experiment Design with Synthetic Controls</title><link>http://arxiv.org/abs/2401.17205v1</link><description>Clinical trials are typically run in order to understand the effects of a newtreatment on a given population of patients. However, patients in largepopulations rarely respond the same way to the same treatment. Thisheterogeneity in patient responses necessitates trials that investigate effectson multiple subpopulations - especially when a treatment has marginal or nobenefit for the overall population but might have significant benefit for aparticular subpopulation. Motivated by this need, we propose Syntax, anexploratory trial design that identifies subpopulations with positive treatmenteffect among many subpopulations. Syntax is sample efficient as it (i) recruitsand allocates patients adaptively and (ii) estimates treatment effects byforming synthetic controls for each subpopulation that combines control samplesfrom other subpopulations. We validate the performance of Syntax and provideinsights into when it might have an advantage over conventional trial designsthrough experiments.</description><author>Alihan H√ºy√ºk, Zhaozhi Qian, Mihaela van der Schaar</author><pubDate>Tue, 30 Jan 2024 17:45:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17205v1</guid></item><item><title>CPR++: Object Localization via Single Coarse Point Supervision</title><link>http://arxiv.org/abs/2401.17203v1</link><description>Point-based object localization (POL), which pursues high-performance objectsensing under low-cost data annotation, has attracted increased attention.However, the point annotation mode inevitably introduces semantic variance dueto the inconsistency of annotated points. Existing POL heavily rely on strictannotation rules, which are difficult to define and apply, to handle theproblem. In this study, we propose coarse point refinement (CPR), which to ourbest knowledge is the first attempt to alleviate semantic variance from analgorithmic perspective. CPR reduces the semantic variance by selecting asemantic centre point in a neighbourhood region to replace the initialannotated point. Furthermore, We design a sampling region estimation module todynamically compute a sampling region for each object and use a cascadedstructure to achieve end-to-end optimization. We further integrate a varianceregularization into the structure to concentrate the predicted scores, yieldingCPR++. We observe that CPR++ can obtain scale information and further reducethe semantic variance in a global region, thus guaranteeing high-performanceobject localization. Extensive experiments on four challenging datasetsvalidate the effectiveness of both CPR and CPR++. We hope our work can inspiremore research on designing algorithms rather than annotation rules to addressthe semantic variance problem in POL. The dataset and code will be public atgithub.com/ucas-vg/PointTinyBenchmark.</description><author>Xuehui Yu, Pengfei Chen, Kuiran Wang, Xumeng Han, Guorong Li, Zhenjun Han, Qixiang Ye, Jianbin Jiao</author><pubDate>Tue, 30 Jan 2024 17:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17203v1</guid></item><item><title>NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble Techniques</title><link>http://arxiv.org/abs/2401.17200v1</link><description>This paper presents a comprehensive comparative analysis of explainableartificial intelligence (XAI) ensembling methods. Our research brings threesignificant contributions. Firstly, we introduce a novel ensembling method,NormEnsembleXAI, that leverages minimum, maximum, and average functions inconjunction with normalization techniques to enhance interpretability.Secondly, we offer insights into the strengths and weaknesses of XAI ensemblemethods. Lastly, we provide a library, facilitating the practicalimplementation of XAI ensembling, thus promoting the adoption of transparentand interpretable deep learning models.</description><author>Weronika Hryniewska-Guzik, Bartosz Sawicki, Przemys≈Çaw Biecek</author><pubDate>Tue, 30 Jan 2024 17:33:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17200v1</guid></item><item><title>Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers</title><link>http://arxiv.org/abs/2401.17196v1</link><description>In text classification, creating an adversarial example means subtlyperturbing a few words in a sentence without changing its meaning, causing itto be misclassified by a classifier. A concerning observation is that asignificant portion of adversarial examples generated by existing methodschange only one word. This single-word perturbation vulnerability represents asignificant weakness in classifiers, which malicious users can exploit toefficiently create a multitude of adversarial examples. This paper studies thisproblem and makes the following key contributions: (1) We introduce a novelmetric \r{ho} to quantitatively assess a classifier's robustness againstsingle-word perturbation. (2) We present the SP-Attack, designed to exploit thesingle-word perturbation vulnerability, achieving a higher attack success rate,better preserving sentence meaning, while reducing computation costs comparedto state-of-the-art adversarial methods. (3) We propose SP-Defense, which aimsto improve \r{ho} by applying data augmentation in learning. Experimentalresults on 4 datasets and BERT and distilBERT classifiers show that SP-Defenseimproves \r{ho} by 14.6% and 13.9% and decreases the attack success rate ofSP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases theattack success rate of existing attack methods that involve multiple-wordperturbations.</description><author>Lei Xu, Sarah Alnegheimish, Laure Berti-Equille, Alfredo Cuesta-Infante, Kalyan Veeramachaneni</author><pubDate>Tue, 30 Jan 2024 17:30:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17196v1</guid></item><item><title>Nested Construction of Polar Codes via Transformers</title><link>http://arxiv.org/abs/2401.17188v1</link><description>Tailoring polar code construction for decoding algorithms beyond successivecancellation has remained a topic of significant interest in the field.However, despite the inherent nested structure of polar codes, the use ofsequence models in polar code construction is understudied. In this work, wepropose using a sequence modeling framework to iteratively construct a polarcode for any given length and rate under various channel conditions.Simulations show that polar codes designed via sequential modeling usingtransformers outperform both 5G-NR sequence and Density Evolution basedapproaches for both AWGN and Rayleigh fading channels.</description><author>Sravan Kumar Ankireddy, S Ashwin Hebbar, Heping Wan, Joonyoung Cho, Charlie Zhang</author><pubDate>Tue, 30 Jan 2024 17:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17188v1</guid></item><item><title>Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning</title><link>http://arxiv.org/abs/2401.17186v1</link><description>While vision-language pre-trained models (VL-PTMs) have advanced multimodalresearch in recent years, their mastery in a few languages like Englishrestricts their applicability in broader communities. To this end, there is anincreasing interest in developing multilingual VL models via a joint-learningsetup, which, however, could be unrealistic due to expensive costs and dataavailability. In this work, we propose to extend VL-PTMs' language capacity bycontinual language learning (CLL), where a model needs to update its linguisticknowledge incrementally without suffering from catastrophic forgetting (CF). Webegin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP,a prevailing VL-PTM that has acquired image-English text alignment.Specifically, CLL-CLIP contains an expandable token embedding layer to handlelinguistic differences. It solely trains token embeddings to improve memorystability and is optimized under cross-modal and cross-lingual objectives tolearn the alignment between images and multilingual texts. To alleviate CFraised by covariate shift and lexical overlap, we further propose a novelapproach that ensures the identical distribution of all token embeddings duringinitialization and regularizes token embedding learning during training. Weconstruct a CLL benchmark covering 36 languages based on MSCOCO and XM3600datasets and then evaluate multilingual image-text retrieval performance.Extensive experiments verify the effectiveness of CLL-CLIP and show that ourapproach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 onXM3600, and improve various state-of-the-art methods consistently. Our code anddata are available at \url{https://github.com/yangbang18/CLFM}.</description><author>Bang Yang, Yong Dai, Xuxin Cheng, Yaowei Li, Asif Raza, Yuexian Zou</author><pubDate>Tue, 30 Jan 2024 17:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17186v1</guid></item><item><title>Multi-Camera Asynchronous Ball Localization and Trajectory Prediction with Factor Graphs and Human Poses</title><link>http://arxiv.org/abs/2401.17185v1</link><description>The rapid and precise localization and prediction of a ball are critical fordeveloping agile robots in ball sports, particularly in sports like tennischaracterized by high-speed ball movements and powerful spins. The Magnuseffect induced by spin adds complexity to trajectory prediction during flightand bounce dynamics upon contact with the ground. In this study, we introducean innovative approach that combines a multi-camera system with factor graphsfor real-time and asynchronous 3D tennis ball localization. Additionally, weestimate hidden states like velocity and spin for trajectory prediction.Furthermore, to enhance spin inference early in the ball's flight, wherelimited observations are available, we integrate human pose data using atemporal convolutional network (TCN) to compute spin priors within the factorgraph. This refinement provides more accurate spin priors at the beginning ofthe factor graph, leading to improved early-stage hidden state inference forprediction. Our result shows the trained TCN can predict the spin priors withRMSE of 5.27 Hz. Integrating TCN into the factor graph reduces the predictionerror of landing positions by over 63.6% compared to a baseline method thatutilized an adaptive extended Kalman filter.</description><author>Qingyu Xiao, Zulfiqar Zaidi, Matthew Gombolay</author><pubDate>Tue, 30 Jan 2024 17:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17185v1</guid></item><item><title>Transfer Learning for Text Diffusion Models</title><link>http://arxiv.org/abs/2401.17181v1</link><description>In this report, we explore the potential for text diffusion to replaceautoregressive (AR) decoding for the training and deployment of large languagemodels (LLMs). We are particularly interested to see whether pretrained ARmodels can be transformed into text diffusion models through a lightweightadaptation procedure we call ``AR2Diff''. We begin by establishing a strongbaseline setup for training text diffusion models. Comparing across multiplearchitectures and pretraining objectives, we find that training a decoder-onlymodel with a prefix LM objective is best or near-best across several tasks.Building on this finding, we test various transfer learning setups for textdiffusion models. On machine translation, we find that text diffusionunderperforms the standard AR approach. However, on code synthesis andextractive QA, we find diffusion models trained from scratch outperform ARmodels in many cases. We also observe quality gains from AR2Diff -- adapting ARmodels to use diffusion decoding. These results are promising given that textdiffusion is relatively underexplored and can be significantly faster than ARdecoding for long text generation.</description><author>Kehang Han, Kathleen Kenealy, Aditya Barua, Noah Fiedel, Noah Constant</author><pubDate>Tue, 30 Jan 2024 17:11:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17181v1</guid></item><item><title>GraphViz2Vec: A Structure-aware Feature Generation Model to Improve Classification in GNNs</title><link>http://arxiv.org/abs/2401.17178v1</link><description>GNNs are widely used to solve various tasks including node classification andlink prediction. Most of the GNN architectures assume the initial embedding tobe random or generated from popular distributions. These initial embeddingsrequire multiple layers of transformation to converge into a meaningful latentrepresentation. While number of layers allow accumulation of largerneighbourhood of a node it also introduce the problem of over-smoothing. Inaddition, GNNs are inept at representing structural information. For example,the output embedding of a node does not capture its triangles participation. Inthis paper, we presented a novel feature extraction methodology GraphViz2Vecthat can capture the structural information of a node's local neighbourhood tocreate meaningful initial embeddings for a GNN model. These initial embeddingshelps existing models achieve state-of-the-art results in variousclassification tasks. Further, these initial embeddings help the model toproduce desired results with only two layers which in turn reduce the problemof over-smoothing. The initial encoding of a node is obtained from an imageclassification model trained on multiple energy diagrams of its localneighbourhood. These energy diagrams are generated with the induced sub-graphof the nodes traversed by multiple random walks. The generated encodingsincrease the performance of existing models on classification tasks (with amean increase of $4.65\%$ and $2.58\%$ for the node and link classificationtasks, respectively), with some models achieving state-of-the-art results.</description><author>Shraban Kumar Chatterjee, Suman Kundu</author><pubDate>Tue, 30 Jan 2024 17:11:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17178v1</guid></item><item><title>Data-Driven Discovery of PDEs via the Adjoint Method</title><link>http://arxiv.org/abs/2401.17177v1</link><description>In this work, we present an adjoint-based method for discovering theunderlying governing partial differential equations (PDEs) given data. The ideais to consider a parameterized PDE in a general form, and formulate theoptimization problem that minimizes the error of PDE solution from data. Usingvariational calculus, we obtain an evolution equation for the Lagrangemultipliers (adjoint equations) allowing us to compute the gradient of theobjective function with respect to the parameters of PDEs given data in astraightforward manner. In particular, for a family of parameterized andnonlinear PDEs, we show how the corresponding adjoint equations can be derived.Here, we show that given smooth data set, the proposed adjoint method canrecover the true PDE up to machine accuracy. However, in the presence of noise,the accuracy of the adjoint method becomes comparable to the famous PDEFunctional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudyet al., 2017). Even though the presented adjoint method relies onforward/backward solvers, it outperforms PDE-FIND for large data sets thanks tothe analytic expressions for gradients of the cost function with respect toeach PDE parameter.</description><author>Mohsen Sadr, Tony Tohme, Kamal Youcef-Toumi</author><pubDate>Tue, 30 Jan 2024 17:10:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17177v1</guid></item><item><title>A large dataset curation and benchmark for drug target interaction</title><link>http://arxiv.org/abs/2401.17174v1</link><description>Bioactivity data plays a key role in drug discovery and repurposing. Theresource-demanding nature of \textit{in vitro} and \textit{in vivo}experiments, as well as the recent advances in data-driven computationalbiochemistry research, highlight the importance of \textit{in silico} drugtarget interaction (DTI) prediction approaches. While numerous large publicbioactivity data sources exist, research in the field could benefit from betterstandardization of existing data resources. At present, different researchworks that share similar goals are often difficult to compare properly becauseof different choices of data sources and train/validation/test splitstrategies. Additionally, many works are based on small data subsets, leadingto results and insights of possible limited validity. In this paper we proposea way to standardize and represent efficiently a very large dataset curatedfrom multiple public sources, split the data into train, validation and testsets based on different meaningful strategies, and provide a concreteevaluation protocol to accomplish a benchmark. We analyze the proposed datacuration, prove its usefulness and validate the proposed benchmark throughexperimental studies based on an existing neural network model.</description><author>Alex Golts, Vadim Ratner, Yoel Shoshan, Moshe Raboh, Sagi Polaczek, Michal Ozery-Flato, Daniel Shats, Liam Hazan, Sivan Ravid, Efrat Hexter</author><pubDate>Tue, 30 Jan 2024 17:06:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17174v1</guid></item><item><title>Zero-Shot Reinforcement Learning via Function Encoders</title><link>http://arxiv.org/abs/2401.17173v1</link><description>Although reinforcement learning (RL) can solve many challenging sequentialdecision making problems, achieving zero-shot transfer across related tasksremains a challenge. The difficulty lies in finding a good representation forthe current task so that the agent understands how it relates to previouslyseen tasks. To achieve zero-shot transfer, we introduce the function encoder, arepresentation learning algorithm which represents a function as a weightedcombination of learned, non-linear basis functions. By using a function encoderto represent the reward function or the transition function, the agent hasinformation on how the current task relates to previously seen tasks via acoherent vector representation. Thus, the agent is able to achieve transferbetween related tasks at run time with no additional training. We demonstratestate-of-the-art data efficiency, asymptotic performance, and trainingstability in three RL fields by augmenting basic RL algorithms with a functionencoder task representation.</description><author>Tyler Ingebrand, Amy Zhang, Ufuk Topcu</author><pubDate>Tue, 30 Jan 2024 17:04:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17173v1</guid></item><item><title>Auto311: A Confidence-guided Automated System for Non-emergency Calls</title><link>http://arxiv.org/abs/2312.14185v2</link><description>Emergency and non-emergency response systems are essential services providedby local governments and critical to protecting lives, the environment, andproperty. The effective handling of (non-)emergency calls is critical forpublic safety and well-being. By reducing the burden through non-emergencycallers, residents in critical need of assistance through 911 will receive afast and effective response. Collaborating with the Department of EmergencyCommunications (DEC) in Nashville, we analyzed 11,796 non-emergency callrecordings and developed Auto311, the first automated system to handle 311non-emergency calls, which (1) effectively and dynamically predicts ongoingnon-emergency incident types to generate tailored case reports during the call;(2) itemizes essential information from dialogue contexts to complete thegenerated reports; and (3) strategically structures system-caller dialogueswith optimized confidence. We used real-world data to evaluate the system'seffectiveness and deployability. The experimental results indicate that thesystem effectively predicts incident type with an average F-1 score of 92.54%.Moreover, the system successfully itemizes critical information from relevantcontexts to complete reports, evincing a 0.93 average consistency scorecompared to the ground truth. Additionally, emulations demonstrate that thesystem effectively decreases conversation turns as the utterance size gets moreextensive and categorizes the ongoing call with 94.49% mean accuracy.</description><author>Zirong Chen, Xutong Sun, Yuanhe Li, Meiyi Ma</author><pubDate>Tue, 30 Jan 2024 17:02:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14185v2</guid></item><item><title>Learning Domain-Independent Green's Function For Elliptic Partial Differential Equations</title><link>http://arxiv.org/abs/2401.17172v1</link><description>Green's function characterizes a partial differential equation (PDE) and mapsits solution in the entire domain as integrals. Finding the analytical form ofGreen's function is a non-trivial exercise, especially for a PDE defined on acomplex domain or a PDE with variable coefficients. In this paper, we propose anovel boundary integral network to learn the domain-independent Green'sfunction, referred to as BIN-G. We evaluate the Green's function in the BIN-Gusing a radial basis function (RBF) kernel-based neural network. We train theBIN-G by minimizing the residual of the PDE and the mean squared errors of thesolutions to the boundary integral equations for prescribed test functions. Byleveraging the symmetry of the Green's function and controlling refinements ofthe RBF kernel near the singularity of the Green function, we demonstrate thatour numerical scheme enables fast training and accurate evaluation of theGreen's function for PDEs with variable coefficients. The learned Green'sfunction is independent of the domain geometries, forcing terms, and boundaryconditions in the boundary integral formulation. Numerical experiments verifythe desired properties of the method and the expected accuracy for thetwo-dimensional Poisson and Helmholtz equations with variable coefficients.</description><author>Pawan Negi, Maggie Cheng, Mahesh Krishnamurthy, Wenjun Ying, Shuwang Li</author><pubDate>Tue, 30 Jan 2024 17:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17172v1</guid></item><item><title>Conditional and Modal Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2401.17169v1</link><description>The reasoning abilities of large language models (LLMs) are the topic of agrowing body of research in artificial intelligence and cognitive science. Inthis paper, we probe the extent to which a dozen LLMs are able to distinguishlogically correct inferences from logically fallacious ones. We focus oninference patterns involving conditionals (e.g., 'If Ann has a queen, then Bobhas a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob musthave a king'). These inference patterns have been of special interest tologicians, philosophers, and linguists, since they plausibly play a centralrole in human reasoning. Assessing LLMs on these inference patterns is thushighly relevant to the question of how much the reasoning abilities of LLMsmatch those of humans. Among the LLMs we tested, all but GPT-4 often make basicmistakes with conditionals. Moreover, even GPT-4 displays logicallyinconsistent judgments across inference patterns involving epistemic modals.</description><author>Wesley H. Holliday, Matthew Mandelkern</author><pubDate>Tue, 30 Jan 2024 16:56:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17169v1</guid></item><item><title>Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios</title><link>http://arxiv.org/abs/2401.17167v1</link><description>The recent trend of using Large Language Models (LLMs) as intelligent agentsin real-world applications underscores the necessity for comprehensiveevaluations of their capabilities, particularly in complex scenarios involvingplanning, creating, and using tools. However, existing benchmarks typicallyfocus on simple synthesized queries that do not reflect real-world complexity,thereby offering limited perspectives in evaluating tool utilization. Toaddress this issue, we present UltraTool, a novel benchmark designed to improveand evaluate LLMs' ability in tool utilization within real-world scenarios.UltraTool focuses on the entire process of using tools - from planning andcreating to applying them in complex tasks. It emphasizes real-worldcomplexities, demanding accurate, multi-step planning for effectiveproblem-solving. A key feature of UltraTool is its independent evaluation ofplanning with natural language, which happens before tool usage and simplifiesthe task solving by mapping out the intermediate steps. Thus, unlike previouswork, it eliminates the restriction of pre-defined toolset during planning.Through extensive experiments on various LLMs, we offer novel insights into theevaluation of capabilities of LLMs in tool utilization, thereby contributing afresh perspective to this rapidly evolving field. The benchmark is publiclyavailable at https://github.com/JoeYing1019/UltraTool.</description><author>Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, Qun Liu</author><pubDate>Tue, 30 Jan 2024 16:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17167v1</guid></item><item><title>Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis</title><link>http://arxiv.org/abs/2401.17159v1</link><description>Modern SMT solvers, such as Z3, offer user-controllable strategies, enablingusers to tailor them for their unique set of instances, thus dramaticallyenhancing solver performance for their use case. However, this approach ofstrategy customization presents a significant challenge: handcrafting anoptimized strategy for a class of SMT instances remains a complex and demandingtask for both solver developers and users alike. In this paper, we address this problem of automatic SMT strategy synthesisvia a novel Monte Carlo Tree Search (MCTS) based method. Our method treatsstrategy synthesis as a sequential decision-making process, whose search treecorresponds to the strategy space, and employs MCTS to navigate this vastsearch space. The key innovations that enable our method to identify effectivestrategies, while keeping costs low, are the ideas of layered and staged MCTSsearch. These novel approaches allow for a deeper and more efficientexploration of the strategy space, enabling us to synthesize more effectivestrategies than the default ones in state-of-the-art (SOTA) SMT solvers. Weimplement our method, dubbed Z3alpha, as part of the Z3 SMT solver. Throughextensive evaluations across 6 important SMT logics, Z3alpha demonstratessuperior performance compared to the SOTA synthesis tool FastSMT, the defaultZ3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challengingQF_BV benchmark set, Z3alpha solves 42.7% more instances than the defaultstrategy in the Z3 SMT solver.</description><author>Zhengyang Lu, Stefan Siemer, Piyush Jha, Joel Day, Florin Manea, Vijay Ganesh</author><pubDate>Tue, 30 Jan 2024 16:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17159v1</guid></item><item><title>FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling</title><link>http://arxiv.org/abs/2310.15169v3</link><description>With the availability of large-scale video datasets and the advances ofdiffusion models, text-driven video generation has achieved substantialprogress. However, existing video generation models are typically trained on alimited number of frames, resulting in the inability to generate high-fidelitylong videos during inference. Furthermore, these models only supportsingle-text conditions, whereas real-life scenarios often require multi-textconditions as the video content changes over time. To tackle these challenges,this study explores the potential of extending the text-driven capability togenerate longer videos conditioned on multiple texts. 1) We first analyze theimpact of initial noise in video diffusion models. Then building upon theobservation of noise, we propose FreeNoise, a tuning-free and time-efficientparadigm to enhance the generative capabilities of pretrained video diffusionmodels while preserving content consistency. Specifically, instead ofinitializing noises for all frames, we reschedule a sequence of noises forlong-range correlation and perform temporal attention over them by window-basedfunction. 2) Additionally, we design a novel motion injection method to supportthe generation of videos conditioned on multiple text prompts. Extensiveexperiments validate the superiority of our paradigm in extending thegenerative capabilities of video diffusion models. It is noteworthy thatcompared with the previous best-performing method which brought about 255%extra time cost, our method incurs only negligible time cost of approximately17%. Generated video samples are available at our website:http://haonanqiu.com/projects/FreeNoise.html.</description><author>Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, Ziwei Liu</author><pubDate>Tue, 30 Jan 2024 16:44:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15169v3</guid></item><item><title>SLANG: New Concept Comprehension of Large Language Models</title><link>http://arxiv.org/abs/2401.12585v2</link><description>The dynamic nature of language, particularly evident in the realm of slangand memes on the Internet, poses serious challenges to the adaptability oflarge language models (LLMs). Traditionally anchored to static datasets, thesemodels often struggle to keep up with the rapid linguistic evolutioncharacteristic of online communities. This research addresses the critical needto bridge this gap, aiming to enhance LLMs' comprehension of the evolving newconcepts on the internet, without the high cost of continual retraining. Toaddress this issue, we propose a new benchmark $\textbf{SLANG}$, which canautonomously integrates novel data to stay dataset up-to-date, to assess LLMs'capability in comprehending emerging concepts and an approach $\textbf{FOCUS}$,which uses causal inference to enhance LLMs to understand new phrases and theircolloquial context. This benchmark and approach involves digesting real-worldinstances of linguistic shifts, serving as contextual beacons, to form moreprecise and contextually relevant connections between newly emergingexpressions and their intended meanings. The empirical analysis shows that ourcausal inference-based approach outperforms the traditional models in terms ofprecision and relevance in the interpretation of internet slang and memes.</description><author>Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Xueqi Chen</author><pubDate>Tue, 30 Jan 2024 16:37:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12585v2</guid></item><item><title>FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo Federated Recommendation</title><link>http://arxiv.org/abs/2305.06272v2</link><description>Cross-platform recommendation aims to improve recommendation accuracy bygathering heterogeneous features from different platforms. However, suchcross-silo collaborations between platforms are restricted by increasinglystringent privacy protection regulations, thus data cannot be aggregated fortraining. Federated learning (FL) is a practical solution to deal with the datasilo problem in recommendation scenarios. Existing cross-silo FL methodstransmit model information to collaboratively build a global model byleveraging the data of overlapped users. However, in reality, the number ofoverlapped users is often very small, thus largely limiting the performance ofsuch approaches. Moreover, transmitting model information during trainingrequires high communication costs and may cause serious privacy leakage. Inthis paper, we propose a novel privacy-preserving double distillation frameworknamed FedPDD for cross-silo federated recommendation, which efficientlytransfers knowledge when overlapped users are limited. Specifically, our doubledistillation strategy enables local models to learn not only explicit knowledgefrom the other party but also implicit knowledge from its past predictions.Moreover, to ensure privacy and high efficiency, we employ an offline trainingscheme to reduce communication needs and privacy leakage risk. In addition, weadopt differential privacy to further protect the transmitted information. Theexperiments on two real-world recommendation datasets, HetRec-MovieLens andCriteo, demonstrate the effectiveness of FedPDD compared to thestate-of-the-art approaches.</description><author>Sheng Wan, Dashan Gao, Hanlin Gu, Daning Hu</author><pubDate>Tue, 30 Jan 2024 16:32:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06272v2</guid></item><item><title>An Open Software Suite for Event-Based Video</title><link>http://arxiv.org/abs/2401.17151v1</link><description>While traditional video representations are organized around discrete imageframes, event-based video is a new paradigm that forgoes image framesaltogether. Rather, pixel samples are temporally asynchronous and independentof one another. Until now, researchers have lacked a cohesive softwareframework for exploring the representation, compression, and applications ofevent-based video. I present the AD$\Delta$ER software suite to fill this gap.This framework includes utilities for transcoding framed and multimodalevent-based video sources to a common representation, rate control mechanisms,lossy compression, application support, and an interactive GUI for transcodingand playback. In this paper, I describe these various software components andtheir usage.</description><author>Andrew C. Freeman</author><pubDate>Tue, 30 Jan 2024 16:32:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17151v1</guid></item><item><title>A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding</title><link>http://arxiv.org/abs/2401.10746v2</link><description>Electroencephalography (EEG) signals are frequently used for variousBrain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques haveshown promising results, they are hindered by the substantial datarequirements. By leveraging data from multiple subjects, transfer learningenables more effective training of DL models. A technique that is gainingpopularity is Euclidean Alignment (EA) due to its ease of use, lowcomputational complexity, and compatibility with Deep Learning models. However,few studies evaluate its impact on the training performance of shared andindividual DL models. In this work, we systematically evaluate the effect of EAcombined with DL for decoding BCI signals. We used EA to train shared modelswith data from multiple subjects and evaluated its transferability to newsubjects. Our experimental results show that it improves decoding in the targetsubject by 4.33% and decreases convergence time by more than 70%. We alsotrained individual models for each subject to use as a majority-voting ensembleclassifier. In this scenario, using EA improved the 3-model ensemble accuracyby 3.7%. However, when compared to the shared model with EA, the ensembleaccuracy was 3.62% lower.</description><author>Bruna Junqueira, Bruno Aristimunha, Sylvain Chevallier, Raphael Y. de Camargo</author><pubDate>Tue, 30 Jan 2024 16:32:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10746v2</guid></item><item><title>Interpretable Imitation Learning with Dynamic Causal Relations</title><link>http://arxiv.org/abs/2310.00489v4</link><description>Imitation learning, which learns agent policy by mimicking expertdemonstration, has shown promising results in many applications such as medicaltreatment regimes and self-driving vehicles. However, it remains a difficulttask to interpret control policies learned by the agent. Difficulties mainlycome from two aspects: 1) agents in imitation learning are usually implementedas deep neural networks, which are black-box models and lack interpretability;2) the latent causal mechanism behind agents' decisions may vary along thetrajectory, rather than staying static throughout time steps. To increasetransparency and offer better interpretability of the neural agent, we proposeto expose its captured knowledge in the form of a directed acyclic causalgraph, with nodes being action and state variables and edges denoting thecausal relations behind predictions. Furthermore, we design this causaldiscovery process to be state-dependent, enabling it to model the dynamics inlatent causal graphs. Concretely, we conduct causal discovery from theperspective of Granger causality and propose a self-explainable imitationlearning framework, {\method}. The proposed framework is composed of threeparts: a dynamic causal discovery module, a causality encoding module, and aprediction module, and is trained in an end-to-end manner. After the model islearned, we can obtain causal relations among states and action variablesbehind its decisions, exposing policies learned by it. Experimental results onboth synthetic and real-world datasets demonstrate the effectiveness of theproposed {\method} in learning the dynamic causal graphs for understanding thedecision-making of imitation learning meanwhile maintaining high predictionaccuracy.</description><author>Tianxiang Zhao, Wenchao Yu, Suhang Wang, Lu Wang, Xiang Zhang, Yuncong Chen, Yanchi Liu, Wei Cheng, Haifeng Chen</author><pubDate>Tue, 30 Jan 2024 16:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00489v4</guid></item><item><title>SSLRec: A Self-Supervised Learning Framework for Recommendation</title><link>http://arxiv.org/abs/2308.05697v3</link><description>Self-supervised learning (SSL) has gained significant interest in recentyears as a solution to address the challenges posed by sparse and noisy data inrecommender systems. Despite the growing number of SSL algorithms designed toprovide state-of-the-art performance in various recommendation scenarios (e.g.,graph collaborative filtering, sequential recommendation, socialrecommendation, KG-enhanced recommendation), there is still a lack of unifiedframeworks that integrate recommendation algorithms across different domains.Such a framework could serve as the cornerstone for self-supervisedrecommendation algorithms, unifying the validation of existing methods anddriving the design of new ones. To address this gap, we introduce SSLRec, anovel benchmark platform that provides a standardized, flexible, andcomprehensive framework for evaluating various SSL-enhanced recommenders. TheSSLRec framework features a modular architecture that allows users to easilyevaluate state-of-the-art models and a complete set of data augmentation andself-supervised toolkits to help create SSL recommendation models with specificneeds. Furthermore, SSLRec simplifies the process of training and evaluatingdifferent recommendation models with consistent and fair settings. Our SSLRecplatform covers a comprehensive set of state-of-the-art SSL-enhancedrecommendation models across different scenarios, enabling researchers toevaluate these cutting-edge models and drive further innovation in the field.Our implemented SSLRec framework is available at the source code repositoryhttps://github.com/HKUDS/SSLRec.</description><author>Xubin Ren, Lianghao Xia, Yuhao Yang, Wei Wei, Tianle Wang, Xuheng Cai, Chao Huang</author><pubDate>Tue, 30 Jan 2024 16:30:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05697v3</guid></item><item><title>Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses</title><link>http://arxiv.org/abs/2307.11714v2</link><description>Optimal Transport has sparked vivid interest in recent years, in particularthanks to the Wasserstein distance, which provides a geometrically sensible andintuitive way of comparing probability measures. For computational reasons, theSliced Wasserstein (SW) distance was introduced as an alternative to theWasserstein distance, and has seen uses for training generative Neural Networks(NNs). While convergence of Stochastic Gradient Descent (SGD) has been observedpractically in such a setting, there is to our knowledge no theoreticalguarantee for this observation. Leveraging recent works on convergence of SGDon non-smooth and non-convex functions by Bianchi et al. (2022), we aim tobridge that knowledge gap, and provide a realistic context under whichfixed-step SGD trajectories for the SW loss on NN parameters converge. Moreprecisely, we show that the trajectories approach the set of (sub)-gradientflow equations as the step decreases. Under stricter assumptions, we show amuch stronger convergence result for noised and projected SGD schemes, namelythat the long-run limits of the trajectories approach a set of generalisedcritical points of the loss function.</description><author>Eloi Tanguy</author><pubDate>Tue, 30 Jan 2024 16:24:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11714v2</guid></item><item><title>Adversarial Machine Learning in Latent Representations of Neural Networks</title><link>http://arxiv.org/abs/2309.17401v3</link><description>Distributed deep neural networks (DNNs) have been shown to reduce thecomputational burden of mobile devices and decrease the end-to-end inferencelatency in edge computing scenarios. While distributed DNNs have been studied,to the best of our knowledge the resilience of distributed DNNs to adversarialaction still remains an open problem. In this paper, we fill the existingresearch gap by rigorously analyzing the robustness of distributed DNNs againstadversarial action. We cast this problem in the context of information theoryand introduce two new measurements for distortion and robustness. Ourtheoretical findings indicate that (i) assuming the same level of informationdistortion, latent features are always more robust than input representations;(ii) the adversarial robustness is jointly determined by the feature dimensionand the generalization capability of the DNN. To test our theoretical findings,we perform extensive experimental analysis by considering 6 different DNNarchitectures, 6 different approaches for distributed DNN and 10 differentadversarial attacks to the ImageNet-1K dataset. Our experimental resultssupport our theoretical findings by showing that the compressed latentrepresentations can reduce the success rate of adversarial attacks by 88% inthe best case and by 57% on the average compared to attacks to the input space.</description><author>Milin Zhang, Mohammad Abdi, Francesco Restuccia</author><pubDate>Tue, 30 Jan 2024 16:24:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17401v3</guid></item><item><title>Large Language Model Evaluation via Matrix Entropy</title><link>http://arxiv.org/abs/2401.17139v1</link><description>Large language models (LLMs) have revolutionized the field of naturallanguage processing, extending their strong capabilities into multi-modaldomains. Thus, it is vital to define proper and diversified metrics for theevaluation of LLMs. In this paper, we introduce matrix entropy, a novel metric rooted ininformation theory and geometry principles to quantify the data compressionproficiency in LLMs. It reflects the model's ability to extract relevantinformation and eliminate unnecessary elements, thereby providing insight intothe language model's intrinsic capability. Specifically, we demonstrate itsapplicability in both single-modal (language) and multi-modal settings. Forlanguage models, our findings reveal that the matrix entropy of representationsfollows a scaling law type reduction when the model scales up, serving as acomplement to the traditional loss scaling law. For the multi-modal setting, wealso propose an evaluation method based on matrix entropy for assessingalignment quality and we find that modern large multi-modal models exhibitgreat alignment performance.</description><author>Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, Weiran Huang</author><pubDate>Tue, 30 Jan 2024 16:19:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17139v1</guid></item><item><title>Self-Infilling Code Generation</title><link>http://arxiv.org/abs/2311.17972v2</link><description>This work introduces self-infilling code generation, a general framework thatincorporates infilling operations into auto-regressive decoding. Our approachcapitalizes on the observation that recent infilling-capable code languagemodels can self-infill: whereas infilling operations aim to fill in the middlebased on a predefined prefix and suffix, self-infilling sequentially generatesboth such surrounding context and the infilled content. We utilize thiscapability to introduce novel interruption and looping mechanisms inconventional decoding, evolving it into a non-monotonic process. Interruptionsallow for postponing the generation of specific code until a definitive suffixis established, enhancing control over the output. Meanwhile, the loopingmechanism, which leverages the complementary nature of self-infilling andleft-to-right decoding, can iteratively update and synchronize each piece ofgeneration cyclically. Extensive experiments are conducted to demonstrate thatour proposed decoding process is effective in enhancing both regularity andquality across several code generation benchmarks.</description><author>Lin Zheng, Jianbo Yuan, Zhi Zhang, Hongxia Yang, Lingpeng Kong</author><pubDate>Tue, 30 Jan 2024 16:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17972v2</guid></item><item><title>Systematically Assessing the Security Risks of AI/ML-enabled Connected Healthcare Systems</title><link>http://arxiv.org/abs/2401.17136v1</link><description>The adoption of machine-learning-enabled systems in the healthcare domain ison the rise. While the use of ML in healthcare has several benefits, it alsoexpands the threat surface of medical systems. We show that the use of ML inmedical systems, particularly connected systems that involve interfacing the MLengine with multiple peripheral devices, has security risks that might causelife-threatening damage to a patient's health in case of adversarialinterventions. These new risks arise due to security vulnerabilities in theperipheral devices and communication channels. We present a case study where wedemonstrate an attack on an ML-enabled blood glucose monitoring system byintroducing adversarial data points during inference. We show that an adversarycan achieve this by exploiting a known vulnerability in the Bluetoothcommunication channel connecting the glucose meter with the ML-enabled app. Wefurther show that state-of-the-art risk assessment techniques are not adequatefor identifying and assessing these new risks. Our study highlights the needfor novel risk analysis methods for analyzing the security of AI-enabledconnected health devices.</description><author>Mohammed Elnawawy, Mohammadreza Hallajiyan, Gargi Mitra, Shahrear Iqbal, Karthik Pattabiraman</author><pubDate>Tue, 30 Jan 2024 16:15:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17136v1</guid></item><item><title>A Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion</title><link>http://arxiv.org/abs/2401.17133v1</link><description>Singing voice conversion (SVC) automates song covers by converting onesinger's singing voice into another target singer's singing voice with theoriginal lyrics and melody. However, it raises serious concerns about copyrightand civil right infringements to multiple entities. This work proposesSongBsAb, the first proactive approach to mitigate unauthorized SVC-basedillegal song covers. SongBsAb introduces human-imperceptible perturbations tosinging voices before releasing them, so that when they are used, thegeneration process of SVC will be interfered, resulting in unexpected singingvoices. SongBsAb features a dual prevention effect by causing both (singer)identity disruption and lyric disruption, namely, the SVC-covered singing voiceneither imitates the target singer nor preserves the original lyrics. Toimprove the imperceptibility of perturbations, we refine a psychoacousticmodel-based loss with the backing track as an additional masker, a uniqueaccompanying element for singing voices compared to ordinary speech voices. Toenhance the transferability, we propose to utilize a frame-level interactionreduction-based loss. We demonstrate the prevention effectiveness, utility, androbustness of SongBsAb on three SVC models and two datasets using bothobjective and human study-based subjective metrics. Our work fosters anemerging research direction for mitigating illegal automated song covers.</description><author>Guangke Chen, Yedi Zhang, Fu Song, Ting Wang, Xiaoning Du, Yang Liu</author><pubDate>Tue, 30 Jan 2024 16:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17133v1</guid></item><item><title>Dual Relation Alignment for Composed Image Retrieval</title><link>http://arxiv.org/abs/2309.02169v2</link><description>Composed image retrieval, a task involving the search for a target imageusing a reference image and a complementary text as the query, has witnessedsignificant advancements owing to the progress made in cross-modal modeling.Unlike the general image-text retrieval problem with only one alignmentrelation, i.e., image-text, we argue for the existence of two types ofrelations in composed image retrieval. The explicit relation pertains to thereference image &amp; complementary text-target image, which is commonly exploitedby existing methods. Besides this intuitive relation, the observations duringour practice have uncovered another implicit yet crucial relation, i.e.,reference image &amp; target image-complementary text, since we found that thecomplementary text can be inferred by studying the relation between the targetimage and the reference image. Regrettably, existing methods largely focus onleveraging the explicit relation to learn their networks, while overlooking theimplicit relation. In response to this weakness, We propose a new framework forcomposed image retrieval, termed dual relation alignment, which integrates bothexplicit and implicit relations to fully exploit the correlations among thetriplets. Specifically, we design a vision compositor to fuse reference imageand target image at first, then the resulted representation will serve tworoles: (1) counterpart for semantic alignment with the complementary text and(2) compensation for the complementary text to boost the explicit relationmodeling, thereby implant the implicit relation into the alignment learning.Our method is evaluated on two popular datasets, CIRR and FashionIQ, throughextensive experiments. The results confirm the effectiveness of ourdual-relation learning in substantially enhancing composed image retrievalperformance.</description><author>Xintong Jiang, Yaxiong Wang, Yujiao Wu, Meng Wang, Xueming Qian</author><pubDate>Tue, 30 Jan 2024 16:02:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02169v2</guid></item><item><title>Personalized Differential Privacy for Ridge Regression</title><link>http://arxiv.org/abs/2401.17127v1</link><description>The increased application of machine learning (ML) in sensitive domainsrequires protecting the training data through privacy frameworks, such asdifferential privacy (DP). DP requires to specify a uniform privacy level$\varepsilon$ that expresses the maximum privacy loss that each data point inthe entire dataset is willing to tolerate. Yet, in practice, different datapoints often have different privacy requirements. Having to set one uniformprivacy level is usually too restrictive, often forcing a learner to guaranteethe stringent privacy requirement, at a large cost to accuracy. To overcomethis limitation, we introduce our novel Personalized-DP Output Perturbationmethod (PDP-OP) that enables to train Ridge regression models with individualper data point privacy levels. We provide rigorous privacy proofs for ourPDP-OP as well as accuracy guarantees for the resulting model. This work is thefirst to provide such theoretical accuracy guarantees when it comes topersonalized DP in machine learning, whereas previous work only providedempirical evaluations. We empirically evaluate PDP-OP on synthetic and realdatasets and with diverse privacy distributions. We show that by enabling eachdata point to specify their own privacy requirement, we can significantlyimprove the privacy-accuracy trade-offs in DP. We also show that PDP-OPoutperforms the personalized privacy techniques of Jorgensen et al. (2015).</description><author>Krishna Acharya, Franziska Boenisch, Rakshit Naidu, Juba Ziani</author><pubDate>Tue, 30 Jan 2024 16:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17127v1</guid></item><item><title>Graph Neural Networks with polynomial activations have limited expressivity</title><link>http://arxiv.org/abs/2310.13139v4</link><description>The expressivity of Graph Neural Networks (GNNs) can be entirelycharacterized by appropriate fragments of the first order logic. Namely, anyquery of the two variable fragment of graded modal logic (GC2) interpreted overlabeled graphs can be expressed using a GNN whose size depends only on thedepth of the query. As pointed out by [Barcelo &amp; Al., 2020, Grohe, 2021], thisdescription holds for a family of activation functions, leaving thepossibibility for a hierarchy of logics expressible by GNNs depending on thechosen activation function. In this article, we show that such hierarchy indeedexists by proving that GC2 queries cannot be expressed by GNNs with polynomialactivation functions. This implies a separation between polynomial and popularnon polynomial activations (such as Rectified Linear Units) and answers an openquestion formulated by [Grohe, 21].</description><author>Sammy Khalife</author><pubDate>Tue, 30 Jan 2024 15:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13139v4</guid></item><item><title>GarchingSim: An Autonomous Driving Simulator with Photorealistic Scenes and Minimalist Workflow</title><link>http://arxiv.org/abs/2401.15803v2</link><description>Conducting real road testing for autonomous driving algorithms can beexpensive and sometimes impractical, particularly for small startups andresearch institutes. Thus, simulation becomes an important method forevaluating these algorithms. However, the availability of free and open-sourcesimulators is limited, and the installation and configuration process can bedaunting for beginners and interdisciplinary researchers. We introduce anautonomous driving simulator with photorealistic scenes, meanwhile keeping auser-friendly workflow. The simulator is able to communicate with externalalgorithms through ROS2 or Socket.IO, making it compatible with existingsoftware stacks. Furthermore, we implement a highly accurate vehicle dynamicsmodel within the simulator to enhance the realism of the vehicle's physicaleffects. The simulator is able to serve various functions, including generatingsynthetic data and driving with machine learning-based algorithms. Moreover, weprioritize simplicity in the deployment process, ensuring that beginners findit approachable and user-friendly.</description><author>Liguo Zhou, Yinglei Song, Yichao Gao, Zhou Yu, Michael Sodamin, Hongshen Liu, Liang Ma, Lian Liu, Hao Liu, Yang Liu, Haichuan Li, Guang Chen, Alois Knoll</author><pubDate>Tue, 30 Jan 2024 15:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15803v2</guid></item><item><title>Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers</title><link>http://arxiv.org/abs/2309.12570v3</link><description>The development of large language models (LLMs) capable of followinginstructions and engaging in conversational interactions sparked increasedinterest in their utilization across various support tools. We investigate theutility of modern LLMs in assisting professional writers via an empirical userstudy (n=30). The design of our collaborative writing interface is grounded inthe cognitive process model of writing that views writing as a goal-orientedthinking process encompassing non-linear cognitive activities: planning,translating, and reviewing. Participants are asked to submit a post-completionsurvey to provide feedback on the potential and pitfalls of LLMs as writingcollaborators. Upon analyzing the writer-LLM interactions, we find that whilewriters seek LLM's help across all three types of cognitive activities, theyfind LLMs more helpful in translation and reviewing. Our findings fromanalyzing both the interactions and the survey responses highlight futureresearch directions in creative writing assistance using LLMs.</description><author>Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman, Smaranda Muresan</author><pubDate>Tue, 30 Jan 2024 15:56:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12570v3</guid></item><item><title>Explainable data-driven modeling via mixture of experts: towards effective blending of grey and black-box models</title><link>http://arxiv.org/abs/2401.17118v1</link><description>Traditional models grounded in first principles often struggle with accuracyas the system's complexity increases. Conversely, machine learning approaches,while powerful, face challenges in interpretability and in handling physicalconstraints. Efforts to combine these models often often stumble upondifficulties in finding a balance between accuracy and complexity. To addressthese issues, we propose a comprehensive framework based on a "mixture ofexperts" rationale. This approach enables the data-based fusion of diverselocal models, leveraging the full potential of first-principle-based priors.Our solution allows independent training of experts, drawing on techniques fromboth machine learning and system identification, and it supports bothcollaborative and competitive learning paradigms. To enhance interpretability,we penalize abrupt variations in the expert's combination. Experimental resultsvalidate the effectiveness of our approach in producing an interpretablecombination of models closely resembling the target phenomena.</description><author>Jessica Leoni, Valentina Breschi, Simone Formentin, Mara Tanelli</author><pubDate>Tue, 30 Jan 2024 15:53:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17118v1</guid></item><item><title>Causal Forecasting for Pricing</title><link>http://arxiv.org/abs/2312.15282v3</link><description>This paper proposes a novel method for demand forecasting in a pricingcontext. Here, modeling the causal relationship between price as an inputvariable to demand is crucial because retailers aim to set prices in a (profit)optimal manner in a downstream decision making problem. Our methods bringtogether the Double Machine Learning methodology for causal inference andstate-of-the-art transformer-based forecasting models. In extensive empiricalexperiments, we show on the one hand that our method estimates the causaleffect better in a fully controlled setting via synthetic, yet realistic data.On the other hand, we demonstrate on real-world data that our methodoutperforms forecasting methods in off-policy settings (i.e., when there's achange in the pricing policy) while only slightly trailing in the on-policysetting.</description><author>Douglas Schultz, Johannes Stephan, Julian Sieber, Trudie Yeh, Manuel Kunz, Patrick Doupe, Tim Januschowski</author><pubDate>Tue, 30 Jan 2024 15:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15282v3</guid></item><item><title>Quantum error mitigation and correction mediated by Yang-Baxter equation and artificial neural network</title><link>http://arxiv.org/abs/2401.17116v1</link><description>Quantum computing shows great potential, but errors pose a significantchallenge. This study explores new strategies for mitigating quantum errorsusing artificial neural networks (ANN) and the Yang-Baxter equation (YBE).Unlike traditional error correction methods, which are computationallyintensive, we investigate artificial error mitigation. The manuscriptintroduces the basics of quantum error sources and explores the potential ofusing classical computation for error mitigation. The Yang-Baxter equationplays a crucial role, allowing us to compress time dynamics simulations intoconstant-depth circuits. By introducing controlled noise through the YBE, weenhance the dataset for error mitigation. We train an ANN model on partial datafrom quantum simulations, demonstrating its effectiveness in correcting errorsin time-evolving quantum states.</description><author>Sahil Gulania, Yuri Alexeev, Stephen K. Gray, Bo Peng, Niranjan Govind</author><pubDate>Tue, 30 Jan 2024 15:50:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17116v1</guid></item><item><title>Evaluation in Neural Style Transfer: A Review</title><link>http://arxiv.org/abs/2401.17109v1</link><description>The field of Neural Style Transfer (NST) has witnessed remarkable progress inthe past few years, with approaches being able to synthesize artistic andphotorealistic images and videos of exceptional quality. To evaluate suchresults, a diverse landscape of evaluation methods and metrics is used,including authors' opinions based on side-by-side comparisons, human evaluationstudies that quantify the subjective judgements of participants, and amultitude of quantitative computational metrics which objectively assess thedifferent aspects of an algorithm's performance. However, there is no consensusregarding the most suitable and effective evaluation procedure that canguarantee the reliability of the results. In this review, we provide anin-depth analysis of existing evaluation techniques, identify theinconsistencies and limitations of current evaluation methods, and giverecommendations for standardized evaluation practices. We believe that thedevelopment of a robust evaluation framework will not only enable moremeaningful and fairer comparisons among NST methods but will also enhance thecomprehension and interpretation of research findings in the field.</description><author>Eleftherios Ioannou, Steve Maddock</author><pubDate>Tue, 30 Jan 2024 15:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17109v1</guid></item><item><title>LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs</title><link>http://arxiv.org/abs/2401.16160v2</link><description>Instruction finetuning on a variety of image-text instruction data is the keyto obtaining a versatile Multimodal Large Language Model (MLLM), and differentconfigurations of the instruction data can lead to finetuned models withdifferent capabilities. However, we have discovered that data conflicts areinevitable when mixing instruction data from distinct domains, which can resultin performance drops for tasks of a specific domain. To address this issue, wepropose to apply an efficient Mixture of Experts (MoE) design, which is asparse Mixture of LoRA Experts (MoLE) for instruction finetuning MLLMs. Withinthe Transformer layers, we extend the popular Low-Rank Adaption (LoRA) methodby creating a set of LoRA experts specifically for the MLP layer, and routeeach token to the top-1 expert based on a routing function, allowing adaptivechoices for tokens from different domains. Since the LoRA experts are sparselyactivated, the training and inference cost are kept roughly constant comparedto the original LoRA method. By replacing the plain-LoRA of LLaVA-1.5 with ourMoE design, our final model is named LLaVA-MoLE. Extensive experiments provedthat LLaVA-MoLE effectively mitigates the data conflict issue when mixingmultiple distinct instruction datasets with various configurations, andachieves consistent performance gains over the strong plain-LoRA baselines.Most importantly, on the mixed datasets, LLaVA-MoLE can even outperform theplain-LoRA baseline trained with twice the samples.</description><author>Shaoxiang Chen, Zequn Jie, Lin Ma</author><pubDate>Tue, 30 Jan 2024 15:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16160v2</guid></item><item><title>Blind Audio Bandwidth Extension: A Diffusion-Based Zero-Shot Approach</title><link>http://arxiv.org/abs/2306.01433v2</link><description>Audio bandwidth extension involves the realistic reconstruction ofhigh-frequency spectra from bandlimited observations. In cases where thelowpass degradation is unknown, such as in restoring historical audiorecordings, this becomes a blind problem. This paper introduces a novel methodcalled BABE (Blind Audio Bandwidth Extension) that addresses the blind problemin a zero-shot setting, leveraging the generative priors of a pre-trainedunconditional diffusion model. During the inference process, BABE utilizes ageneralized version of diffusion posterior sampling, where the degradationoperator is unknown but parametrized and inferred iteratively. The performanceof the proposed method is evaluated using objective and subjective metrics, andthe results show that BABE surpasses state-of-the-art blind bandwidth extensionbaselines and achieves competitive performance compared to informed methodswhen tested with synthetic data. Moreover, BABE exhibits robust generalizationcapabilities when enhancing real historical recordings, effectivelyreconstructing the missing high-frequency content while maintaining coherencewith the original recording. Subjective preference tests confirm that BABEsignificantly improves the audio quality of historical music recordings.Examples of historical recordings restored with the proposed method areavailable on the companion webpage:(http://research.spa.aalto.fi/publications/papers/ieee-taslp-babe/)</description><author>Eloi Moliner, Filip Elvander, Vesa V√§lim√§ki</author><pubDate>Tue, 30 Jan 2024 15:40:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01433v2</guid></item><item><title>H-SynEx: Using synthetic images and ultra-high resolution ex vivo MRI for hypothalamus subregion segmentation</title><link>http://arxiv.org/abs/2401.17104v1</link><description>Purpose: To develop a method for automated segmentation of hypothalamussubregions informed by ultra-high resolution ex vivo magnetic resonance images(MRI), which generalizes across MRI sequences and resolutions withoutretraining. Materials and Methods: We trained our deep learning method, H-synEx, withsynthetic images derived from label maps built from ultra-high resolution exvivo MRI scans, which enables finer-grained manual segmentation when comparedwith 1mm isometric in vivo images. We validated this retrospective study using1535 in vivo images from six datasets and six MRI sequences. The quantitativeevaluation used the Dice Coefficient (DC) and Average Hausdorff distance (AVD).Statistical analysis compared hypothalamic subregion volumes in controls,Alzheimer's disease (AD), and behavioral variant frontotemporal dementia(bvFTD) subjects using the area under the curve (AUC) and Wilcoxon rank sumtest. Results: H-SynEx can segment the hypothalamus across various MRI sequences,encompassing FLAIR sequences with significant slice spacing (5mm). Usinghypothalamic volumes on T1w images to distinguish control from AD and bvFTDpatients, we observed AUC values of 0.74 and 0.79 respectively. Additionally,AUC=0.66 was found for volume variation on FLAIR scans when comparing controland non-patients. Conclusion: Our results show that H-SynEx successfully leverages informationfrom ultra-high resolution scans to segment in vivo from different MRIsequences such as T1w, T2w, PD, qT1, FA, and FLAIR. We also found that ourautomated segmentation was able to discriminate controls versus patients onFLAIR images with 5mm spacing. H-SynEx is openly available athttps://github.com/liviamarodrigues/hsynex.</description><author>Livia Rodrigues, Martina Bocchetta, Oula Puonti, Douglas Greve, Ana Carolina Londe, Marcondes Fran√ßa, Simone Appenzeller, Juan Eugenio Iglesias, Leticia Rittner</author><pubDate>Tue, 30 Jan 2024 15:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17104v1</guid></item><item><title>News and Load: A Quantitative Exploration of Natural Language Processing Applications for Forecasting Day-ahead Electricity System Demand</title><link>http://arxiv.org/abs/2301.07535v2</link><description>The relationship between electricity demand and weather is well establishedin power systems, along with the importance of behavioral and social aspectssuch as holidays and significant events. This study explores the link betweenelectricity demand and more nuanced information about social events. This isdone using mature Natural Language Processing (NLP) and demand forecastingtechniques. The results indicate that day-ahead forecasts are improved bytextual features such as word frequencies, public sentiments, topicdistributions, and word embeddings. The social events contained in thesefeatures include global pandemics, politics, international conflicts,transportation, etc. Causality effects and correlations are discussed topropose explanations for the mechanisms behind the links highlighted. Thisstudy is believed to bring a new perspective to traditional electricity demandanalysis. It confirms the feasibility of improving forecasts from unstructuredtext, with potential consequences for sociology and economics.</description><author>Yun Bai, Simon Camal, Andrea Michiorri</author><pubDate>Tue, 30 Jan 2024 15:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07535v2</guid></item><item><title>MT-Ranker: Reference-free machine translation evaluation by inter-system ranking</title><link>http://arxiv.org/abs/2401.17099v1</link><description>Traditionally, Machine Translation (MT) Evaluation has been treated as aregression problem -- producing an absolute translation-quality score. Thisapproach has two limitations: i) the scores lack interpretability, and humanannotators struggle with giving consistent scores; ii) most scoring methods arebased on (reference, translation) pairs, limiting their applicability inreal-world scenarios where references are absent. In practice, we often careabout whether a new MT system is better or worse than some competitors. Inaddition, reference-free MT evaluation is increasingly practical and necessary.Unfortunately, these two practical considerations have yet to be jointlyexplored. In this work, we formulate the reference-free MT evaluation into apairwise ranking problem. Given the source sentence and a pair of translations,our system predicts which translation is better. In addition to proposing thisnew formulation, we further show that this new paradigm can demonstratesuperior correlation with human judgments by merely using indirect supervisionfrom natural language inference and weak supervision from our synthetic data.In the context of reference-free evaluation, MT-Ranker, trained without anyhuman annotations, achieves state-of-the-art results on the WMT Shared MetricsTask benchmarks DARR20, MQM20, and MQM21. On a more challenging benchmark,ACES, which contains fine-grained evaluation criteria such as addition,omission, and mistranslation errors, MT-Ranker marks state-of-the-art againstreference-free as well as reference-based baselines.</description><author>Ibraheem Muhammad Moosa, Rui Zhang, Wenpeng Yin</author><pubDate>Tue, 30 Jan 2024 15:30:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17099v1</guid></item><item><title>CharNet: Generalized Approach for High-Complexity Character Classification</title><link>http://arxiv.org/abs/2401.17098v1</link><description>Handwritten character recognition (HCR) is a challenging problem for machinelearning researchers. Unlike printed text data, handwritten character datasetshave more variation due to human-introduced bias. With numerous uniquecharacter classes present, some data, such as Logographic Scripts orSino-Korean character sequences, bring new complications to the HCR problem.The classification task on such datasets requires the model to learnhigh-complexity details of the images that share similar features. With recentadvances in computational resource availability and further computer visiontheory development, some research teams have effectively addressed the arisingchallenges. Although known for achieving high efficiency, many commonapproaches are still not generalizable and use dataset-specific solutions toachieve better results. Due to complex structure and high computing demands,existing methods frequently prevent the solutions from gaining popularity. Thispaper proposes a straightforward, generalizable, and highly effective approach(CharNet) for detailed character image classification and compares itsperformance to that of existing approaches.</description><author>Boris Kriuk</author><pubDate>Tue, 30 Jan 2024 15:29:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17098v1</guid></item><item><title>Augmenting Math Word Problems via Iterative Question Composing</title><link>http://arxiv.org/abs/2401.09003v3</link><description>Despite the advancements in large language models (LLMs) for mathematicalreasoning, solving competition-level math problems remains a significantchallenge, especially for open-source LLMs without external tools. We introducethe MMIQC dataset, comprising a mixture of processed web data and syntheticquestion-response pairs, aimed at enhancing the mathematical reasoningcapabilities of base language models. Models fine-tuned on MMIQC consistentlysurpass their counterparts in performance on the MATH benchmark across variousmodel sizes. Notably, Qwen-72B-MMIQC achieves a 45.0% accuracy, exceeding theprevious open-source state-of-the-art by 8.2% and outperforming the initialversion GPT-4 released in 2023. Extensive evaluation results on Hungarian highschool finals suggest that such improvement can generalize to unseen data. Ourablation study on MMIQC reveals that a large part of the improvement can beattributed to our novel augmentation method, Iterative Question Composing(IQC), which involves iteratively composing new questions from seed problemsusing an LLM and applying rejection sampling through another LLM. The MMIQCdataset is available on the HuggingFace hub athttps://huggingface.co/datasets/Vivacem/MMIQC. Our code is available athttps://github.com/iiis-ai/IterativeQuestionComposing.</description><author>Haoxiong Liu, Yifan Zhang, Yifan Luo, Andrew Chi-Chih Yao</author><pubDate>Tue, 30 Jan 2024 15:29:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09003v3</guid></item><item><title>Traffic estimation in unobserved network locations using data-driven macroscopic models</title><link>http://arxiv.org/abs/2401.17095v1</link><description>This paper leverages macroscopic models and multi-source spatiotemporal datacollected from automatic traffic counters and probe vehicles to accuratelyestimate traffic flow and travel time in links where these measurements areunavailable. This problem is critical in transportation planning applicationswhere the sensor coverage is low and the planned interventions havenetwork-wide impacts. The proposed model, named the Macroscopic TrafficEstimator (MaTE), can perform network-wide estimations of traffic flow andtravel time only using the set of observed measurements of these quantities.Because MaTE is grounded in macroscopic flow theory, all parameters andvariables are interpretable. The estimated traffic flow satisfies fundamentalflow conservation constraints and exhibits an increasing monotonic relationshipwith the estimated travel time. Using logit-based stochastic traffic assignmentas the principle for routing flow behavior makes the model fully differentiablewith respect to the model parameters. This property facilitates the applicationof computational graphs to learn parameters from vast amounts of spatiotemporaldata. We also integrate neural networks and polynomial kernel functions tocapture link flow interactions and enrich the mapping of traffic flows intotravel times. MaTE also adds a destination choice model and a trip generationmodel that uses historical data on the number of trips generated by location.Experiments on synthetic data show that the model can accurately estimatetravel time and traffic flow in out-of-sample links. Results obtained usingreal-world multi-source data from a large-scale transportation network suggestthat MaTE outperforms data-driven benchmarks, especially in travel timeestimation. The estimated parameters of MaTE are also informative about thehourly change in travel demand and supply characteristics of the transportationnetwork.</description><author>Pablo Guarda, Sean Qian</author><pubDate>Tue, 30 Jan 2024 15:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17095v1</guid></item><item><title>StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis</title><link>http://arxiv.org/abs/2401.17093v1</link><description>To leverage LLMs for visual synthesis, traditional methods convert rasterimage information into discrete grid tokens through specialized visual modules,while disrupting the model's ability to capture the true semanticrepresentation of visual scenes. This paper posits that an alternativerepresentation of images, vector graphics, can effectively surmount thislimitation by enabling a more natural and semantically coherent segmentation ofthe image information. Thus, we introduce StrokeNUWA, a pioneering workexploring a better visual representation ''stroke tokens'' on vector graphics,which is inherently visual semantics rich, naturally compatible with LLMs, andhighly compressed. Equipped with stroke tokens, StrokeNUWA can significantlysurpass traditional LLM-based and optimization-based methods across variousmetrics in the vector graphic generation task. Besides, StrokeNUWA achieves upto a 94x speedup in inference over the speed of prior methods with anexceptional SVG code compression ratio of 6.9%.</description><author>Zecheng Tang, Chenfei Wu, Zekai Zhang, Mingheng Ni, Shengming Yin, Yu Liu, Zhengyuan Yang, Lijuan Wang, Zicheng Liu, Juntao Li, Nan Duan</author><pubDate>Tue, 30 Jan 2024 15:20:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17093v1</guid></item><item><title>NNOSE: Nearest Neighbor Occupational Skill Extraction</title><link>http://arxiv.org/abs/2401.17092v1</link><description>The labor market is changing rapidly, prompting increased interest in theautomatic extraction of occupational skills from text. With the advent ofEnglish benchmark job description datasets, there is a need for systems thathandle their diversity well. We tackle the complexity in occupational skilldatasets tasks -- combining and leveraging multiple datasets for skillextraction, to identify rarely observed skills within a dataset, and overcomingthe scarcity of skills across datasets. In particular, we investigate theretrieval-augmentation of language models, employing an external datastore forretrieving similar skills in a dataset-unifying manner. Our proposed method,\textbf{N}earest \textbf{N}eighbor \textbf{O}ccupational \textbf{S}kill\textbf{E}xtraction (NNOSE) effectively leverages multiple datasets byretrieving neighboring skills from other datasets in the datastore. Thisimproves skill extraction \emph{without} additional fine-tuning. Crucially, weobserve a performance gain in predicting infrequent patterns, with substantialgains of up to 30\% span-F1 in cross-dataset settings.</description><author>Mike Zhang, Rob van der Goot, Min-Yen Kan, Barbara Plank</author><pubDate>Tue, 30 Jan 2024 15:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17092v1</guid></item><item><title>EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling</title><link>http://arxiv.org/abs/2310.04691v6</link><description>Neural language models are probabilistic models of human text. They arepredominantly trained using maximum likelihood estimation (MLE), which isequivalent to minimizing the forward cross-entropy between the empirical datadistribution and the model distribution. However, various degenerationphenomena are still widely observed when decoding from the distributionslearned by such models. We establish that the forward cross-entropy issuboptimal as a distance metric for aligning human and model distribution dueto its (1) recall-prioritization (2) negative diversity ignorance and (3)train-test mismatch. In this paper, we propose Earth Mover DistanceOptimization (EMO) for auto-regressive language modeling. EMO capitalizes onthe inherent properties of earth mover distance to address the aforementionedchallenges. Due to the high complexity of direct computation, we furtherintroduce a feasible upper bound for EMO to ease end-to-end training. Uponextensive evaluation of language models trained using EMO and MLE. We find thatEMO demonstrates a consistently better language modeling performance than MLEacross domains. Moreover, EMO demonstrates noteworthy enhancements indownstream performance with minimal fine-tuning on merely 25,000 sentences.This highlights the tremendous potential of EMO as a lightweight calibrationmethod for enhancing large-scale pre-trained language models.</description><author>Siyu Ren, Zhiyong Wu, Kenny Q. Zhu</author><pubDate>Tue, 30 Jan 2024 15:11:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04691v6</guid></item><item><title>Active Generation Network of Human Skeleton for Action Recognition</title><link>http://arxiv.org/abs/2401.17086v1</link><description>Data generation is a data augmentation technique for enhancing thegeneralization ability for skeleton-based human action recognition. Mostexisting data generation methods face challenges to ensure the temporalconsistency of the dynamic information for action. In addition, the datagenerated by these methods lack diversity when only a few training samples areavailable. To solve those problems, We propose a novel active generativenetwork (AGN), which can adaptively learn various action categories by motionstyle transfer to generate new actions when the data for a particular action isonly a single sample or few samples. The AGN consists of an action generationnetwork and an uncertainty metric network. The former, with ST-GCN as theBackbone, can implicitly learn the morphological features of the target actionwhile preserving the category features of the source action. The latter guidesgenerating actions. Specifically, an action recognition model generatesprediction vectors for each action, which is then scored using an uncertaintymetric. Finally, UMN provides the uncertainty sampling basis for the generatedactions.</description><author>Long Liu, Xin Wang, Fangming Li, Jiayu Chen</author><pubDate>Tue, 30 Jan 2024 15:09:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17086v1</guid></item><item><title>Policy Learning with Distributional Welfare</title><link>http://arxiv.org/abs/2311.15878v2</link><description>In this paper, we explore optimal treatment allocation policies that targetdistributional welfare. Most literature on treatment choice has consideredutilitarian welfare based on the conditional average treatment effect (ATE).While average welfare is intuitive, it may yield undesirable allocationsespecially when individuals are heterogeneous (e.g., with outliers) - the veryreason individualized treatments were introduced in the first place. Thisobservation motivates us to propose an optimal policy that allocates thetreatment based on the conditional quantile of individual treatment effects(QoTE). Depending on the choice of the quantile probability, this criterion canaccommodate a policymaker who is either prudent or negligent. The challenge ofidentifying the QoTE lies in its requirement for knowledge of the jointdistribution of the counterfactual outcomes, which is generally hard to recovereven with experimental data. Therefore, we introduce minimax policies that arerobust to model uncertainty. A range of identifying assumptions can be used toyield more informative policies. For both stochastic and deterministicpolicies, we establish the asymptotic bound on the regret of implementing theproposed policies. In simulations and two empirical applications, we compareoptimal decisions based on the QoTE with decisions based on other criteria. Theframework can be generalized to any setting where welfare is defined as afunctional of the joint distribution of the potential outcomes.</description><author>Yifan Cui, Sukjin Han</author><pubDate>Tue, 30 Jan 2024 15:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15878v2</guid></item><item><title>Dynamical Survival Analysis with Controlled Latent States</title><link>http://arxiv.org/abs/2401.17077v1</link><description>We consider the task of learning individual-specific intensities of countingprocesses from a set of static variables and irregularly sampled time series.We introduce a novel modelization approach in which the intensity is thesolution to a controlled differential equation. We first design a neuralestimator by building on neural controlled differential equations. In a secondtime, we show that our model can be linearized in the signature space undersufficient regularity conditions, yielding a signature-based estimator which wecall CoxSig. We provide theoretical learning guarantees for both estimators,before showcasing the performance of our models on a vast array of simulatedand real-world datasets from finance, predictive maintenance and food supplychain management.</description><author>Linus Bleistein, Van-Tuan Nguyen, Adeline Fermanian, Agathe Guilloux</author><pubDate>Tue, 30 Jan 2024 14:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17077v1</guid></item><item><title>Non-central panorama indoor dataset</title><link>http://arxiv.org/abs/2401.17075v1</link><description>Omnidirectional images are one of the main sources of information forlearning based scene understanding algorithms. However, annotated datasets ofomnidirectional images cannot keep the pace of these learning based algorithmsdevelopment. Among the different panoramas and in contrast to standard centralones, non-central panoramas provide geometrical information in the distortionof the image from which we can retrieve 3D information of the environment [2].However, due to the lack of commercial non-central devices, up until now therewas no dataset of these kinds of panoramas. In this data paper, we present thefirst dataset of non-central panoramas for indoor scene understanding. Thedataset is composed by {\bf 2574} RGB non-central panoramas taken in around 650different rooms. Each panorama has associated a depth map and annotations toobtain the layout of the room from the image as a structural edge map, list ofcorners in the image, the 3D corners of the room and the camera pose. Theimages are taken from photorealistic virtual environments and pixel-wiseautomatically annotated.</description><author>Bruno Berenguel-Baeta, Jesus Bermudez-Cameo, Jose J. Guerrero</author><pubDate>Tue, 30 Jan 2024 14:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17075v1</guid></item><item><title>SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity</title><link>http://arxiv.org/abs/2401.17072v1</link><description>Instruction-tuned Large Language Models (LLMs) have recently showcasedremarkable advancements in their ability to generate fitting responses tonatural language instructions. However, many current works rely on manualevaluation to judge the quality of generated responses. Since such manualevaluation is time-consuming, it does not easily scale to the evaluation ofmultiple models and model variants. In this short paper, we propose astraightforward but remarkably effective evaluation metric called SemScore, inwhich we directly compare model outputs to gold target responses using semantictextual similarity (STS). We conduct a comparative evaluation of the modeloutputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluationmetrics for text generation. We find that our proposed SemScore metricoutperforms all other, in many cases more complex, evaluation metrics in termsof correlation to human evaluation. These findings indicate the utility of ourproposed metric for the evaluation of instruction-tuned LLMs.</description><author>Ansar Aynetdinov, Alan Akbik</author><pubDate>Tue, 30 Jan 2024 14:52:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17072v1</guid></item><item><title>Discrete Graph Auto-Encoder</title><link>http://arxiv.org/abs/2306.07735v2</link><description>Despite advances in generative methods, accurately modeling the distributionof graphs remains a challenging task primarily because of the absence ofpredefined or inherent unique graph representation. Two main strategies haveemerged to tackle this issue: 1) restricting the number of possiblerepresentations by sorting the nodes, or 2) usingpermutation-invariant/equivariant functions, specifically Graph Neural Networks(GNNs). In this paper, we introduce a new framework named Discrete Graph Auto-Encoder(DGAE), which leverages the strengths of both strategies and mitigate theirrespective limitations. In essence, we propose a strategy in 2 steps. We firstuse a permutation-equivariant auto-encoder to convert graphs into sets ofdiscrete latent node representations, each node being represented by a sequenceof quantized vectors. In the second step, we sort the sets of discrete latentrepresentations and learn their distribution with a specifically designedauto-regressive model based on the Transformer architecture. Through multiple experimental evaluations, we demonstrate the competitiveperformances of our model in comparison to the existing state-of-the-art acrossvarious datasets. Various ablation studies support the interest of our method.</description><author>Yoann Boget, Magda Gregorova, Alexandros Kalousis</author><pubDate>Tue, 30 Jan 2024 14:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07735v2</guid></item><item><title>Efficient Gesture Recognition on Spiking Convolutional Networks Through Sensor Fusion of Event-Based and Depth Data</title><link>http://arxiv.org/abs/2401.17064v1</link><description>As intelligent systems become increasingly important in our daily lives, newways of interaction are needed. Classical user interfaces pose issues for thephysically impaired and are partially not practical or convenient. Gesturerecognition is an alternative, but often not reactive enough when conventionalcameras are used. This work proposes a Spiking Convolutional Neural Network,processing event- and depth data for gesture recognition. The network issimulated using the open-source neuromorphic computing framework LAVA foroffline training and evaluation on an embedded system. For the evaluation threeopen source data sets are used. Since these do not represent the appliedbi-modality, a new data set with synchronized event- and depth data wasrecorded. The results show the viability of temporal encoding on depthinformation and modality fusion, even on differently encoded data, to bebeneficial to network performance and generalization capabilities.</description><author>Lea Steffen, Thomas Trapp, Arne Roennau, R√ºdiger Dillmann</author><pubDate>Tue, 30 Jan 2024 14:42:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17064v1</guid></item><item><title>Outline of an Independent Systematic Blackbox Test for ML-based Systems</title><link>http://arxiv.org/abs/2401.17062v1</link><description>This article proposes a test procedure that can be used to test ML models andML-based systems independently of the actual training process. In this way, thetypical quality statements such as accuracy and precision of these models andsystem can be verified independently, taking into account their black boxcharacter and the immanent stochastic properties of ML models and theirtraining data. The article presents first results from a set of testexperiments and suggest extensions to existing test methods reflecting thestochastic nature of ML models and ML-based systems.</description><author>Hans-Werner Wiesbrock, J√ºrgen Gro√ümann</author><pubDate>Tue, 30 Jan 2024 14:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17062v1</guid></item><item><title>OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision</title><link>http://arxiv.org/abs/2401.17061v1</link><description>Omnidirectional and 360{\deg} images are becoming widespread in industry andin consumer society, causing omnidirectional computer vision to gain attention.Their wide field of view allows the gathering of a great amount of informationabout the environment from only an image. However, the distortion of theseimages requires the development of specific algorithms for their treatment andinterpretation. Moreover, a high number of images is essential for the correcttraining of computer vision algorithms based on learning. In this paper, wepresent a tool for generating datasets of omnidirectional images with semanticand depth information. These images are synthesized from a set of captures thatare acquired in a realistic virtual environment for Unreal Engine 4 through aninterface plugin. We gather a variety of well-known projection models such asequirectangular and cylindrical panoramas, different fish-eye lenses,catadioptric systems, and empiric models. Furthermore, we include in our toolphotorealistic non-central-projection systems as non-central panoramas andnon-central catadioptric systems. As far as we know, this is the first reportedtool for generating photorealistic non-central images in the literature.Moreover, since the omnidirectional images are made virtually, we providepixel-wise information about semantics and depth as well as perfect knowledgeof the calibration parameters of the cameras. This allows the creation ofground-truth information with pixel precision for training learning algorithmsand testing 3D vision approaches. To validate the proposed tool, differentcomputer vision algorithms are tested as line extractions from dioptric andcatadioptric central images, 3D Layout recovery and SLAM using equirectangularpanoramas, and 3D reconstruction from non-central panoramas.</description><author>Bruno Berenguel-Baeta, Jesus Bermudez-Cameo, Jose J. Guerrero</author><pubDate>Tue, 30 Jan 2024 14:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17061v1</guid></item><item><title>Atlanta Scaled layouts from non-central panoramas</title><link>http://arxiv.org/abs/2401.17058v1</link><description>In this work we present a novel approach for 3D layout recovery of indoorenvironments using a non-central acquisition system. From a non-centralpanorama, full and scaled 3D lines can be independently recovered by geometryreasoning without geometric nor scale assumptions. However, their sensitivityto noise and complex geometric modeling has led these panoramas being littleinvestigated. Our new pipeline aims to extract the boundaries of the structurallines of an indoor environment with a neural network and exploit the propertiesof non-central projection systems in a new geometrical processing to recover anscaled 3D layout. The results of our experiments show that we improvestate-of-the-art methods for layout reconstruction and line extraction innon-central projection systems. We completely solve the problem in Manhattanand Atlanta environments, handling occlusions and retrieving the metric scaleof the room without extra measurements. As far as the authors knowledge goes,our approach is the first work using deep learning on non-central panoramas andrecovering scaled layouts from single panoramas.</description><author>Bruno Berenguel-Baeta, Jesus Bermudez-Cameo, Jose J. Guerrero</author><pubDate>Tue, 30 Jan 2024 14:39:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17058v1</guid></item><item><title>Floor extraction and door detection for visually impaired guidance</title><link>http://arxiv.org/abs/2401.17056v1</link><description>Finding obstacle-free paths in unknown environments is a big navigation issuefor visually impaired people and autonomous robots. Previous works focus onobstacle avoidance, however they do not have a general view of the environmentthey are moving in. New devices based on computer vision systems can helpimpaired people to overcome the difficulties of navigating in unknownenvironments in safe conditions. In this work it is proposed a combination ofsensors and algorithms that can lead to the building of a navigation system forvisually impaired people. Based on traditional systems that use RGB-D camerasfor obstacle avoidance, it is included and combined the information of afish-eye camera, which will give a better understanding of the user'ssurroundings. The combination gives robustness and reliability to the system aswell as a wide field of view that allows to obtain many information from theenvironment. This combination of sensors is inspired by human vision where thecenter of the retina (fovea) provides more accurate information than theperiphery, where humans have a wider field of view. The proposed system ismounted on a wearable device that provides the obstacle-free zones of thescene, allowing the planning of trajectories for people guidance.</description><author>Bruno Berenguel-Baeta, Manuel Guerrero-Viu, Alejandro de Nova, Jesus Bermudez-Cameo, Alejandro Perez-Yus, Jose J. Guerrero</author><pubDate>Tue, 30 Jan 2024 14:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17056v1</guid></item><item><title>BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation</title><link>http://arxiv.org/abs/2401.17053v1</link><description>We present BlockFusion, a diffusion-based model that generates 3D scenes asunit blocks and seamlessly incorporates new blocks to extend the scene.BlockFusion is trained using datasets of 3D blocks that are randomly croppedfrom complete 3D scene meshes. Through per-block fitting, all training blocksare converted into the hybrid neural fields: with a tri-plane containing thegeometry features, followed by a Multi-layer Perceptron (MLP) for decoding thesigned distance values. A variational auto-encoder is employed to compress thetri-planes into the latent tri-plane space, on which the denoising diffusionprocess is performed. Diffusion applied to the latent representations allowsfor high-quality and diverse 3D scene generation. To expand a scene duringgeneration, one needs only to append empty blocks to overlap with the currentscene and extrapolate existing latent tri-planes to populate new blocks. Theextrapolation is done by conditioning the generation process with the featuresamples from the overlapping tri-planes during the denoising iterations. Latenttri-plane extrapolation produces semantically and geometrically meaningfultransitions that harmoniously blend with the existing scene. A 2D layoutconditioning mechanism is used to control the placement and arrangement ofscene elements. Experimental results indicate that BlockFusion is capable ofgenerating diverse, geometrically consistent and unbounded large 3D scenes withunprecedented high-quality shapes in both indoor and outdoor scenarios.</description><author>Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, Pan Ji</author><pubDate>Tue, 30 Jan 2024 14:34:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17053v1</guid></item><item><title>Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again</title><link>http://arxiv.org/abs/2401.17052v1</link><description>Deep learning for tabular data has garnered increasing attention in recentyears, yet employing deep models for structured data remains challenging. Whilethese models excel with unstructured data, their efficacy with structured datahas been limited. Recent research has introduced retrieval-augmented models toaddress this gap, demonstrating promising results in supervised tasks such asclassification and regression. In this work, we investigate usingretrieval-augmented models for anomaly detection on tabular data. We propose areconstruction-based approach in which a transformer model learns toreconstruct masked features of \textit{normal} samples. We test theeffectiveness of KNN-based and attention-based modules to select relevantsamples to help in the reconstruction process of the target sample. Ourexperiments on a benchmark of 31 tabular datasets reveal that augmenting thisreconstruction-based anomaly detection (AD) method with non-parametricrelationships via retrieval modules may significantly boost performance.</description><author>Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Li√™n Doan</author><pubDate>Tue, 30 Jan 2024 14:33:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17052v1</guid></item><item><title>ViTree: Single-path Neural Tree for Step-wise Interpretable Fine-grained Visual Categorization</title><link>http://arxiv.org/abs/2401.17050v1</link><description>As computer vision continues to advance and finds widespread applicationsacross various domains, the need for interpretability in deep learning modelsbecomes paramount. Existing methods often resort to post-hoc techniques orprototypes to explain the decision-making process, which can be indirect andlack intrinsic illustration. In this research, we introduce ViTree, a novelapproach for fine-grained visual categorization that combines the popularvision transformer as a feature extraction backbone with neural decision trees.By traversing the tree paths, ViTree effectively selects patches fromtransformer-processed features to highlight informative local regions, therebyrefining representations in a step-wise manner. Unlike previous tree-basedmodels that rely on soft distributions or ensembles of paths, ViTree selects asingle tree path, offering a clearer and simpler decision-making process. Thispatch and path selectivity enhances model interpretability of ViTree, enablingbetter insights into the model's inner workings. Remarkably, extensiveexperimentation validates that this streamlined approach surpasses variousstrong competitors and achieves state-of-the-art performance while maintainingexceptional interpretability which is proved by multi-perspective methods. Codecan be found at https://github.com/SJTU-DeepVisionLab/ViTree.</description><author>Danning Lao, Qi Liu, Jiazi Bu, Junchi Yan, Wei Shen</author><pubDate>Tue, 30 Jan 2024 14:32:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17050v1</guid></item><item><title>Data-dependent Generalization Bounds via Variable-Size Compressibility</title><link>http://arxiv.org/abs/2303.05369v2</link><description>In this paper, we establish novel data-dependent upper bounds on thegeneralization error through the lens of a "variable-size compressibility"framework that we introduce newly here. In this framework, the generalizationerror of an algorithm is linked to a variable-size 'compression rate' of itsinput data. This is shown to yield bounds that depend on the empirical measureof the given input data at hand, rather than its unknown distribution. Our newgeneralization bounds that we establish are tail bounds, tail bounds on theexpectation, and in-expectations bounds. Moreover, it is shown that ourframework also allows to derive general bounds on any function of the inputdata and output hypothesis random variables. In particular, these generalbounds are shown to subsume and possibly improve over several existingPAC-Bayes and data-dependent intrinsic dimension-based bounds that arerecovered as special cases, thus unveiling a unifying character of ourapproach. For instance, a new data-dependent intrinsic dimension-based bound isestablished, which connects the generalization error to the optimizationtrajectories and reveals various interesting connections with therate-distortion dimension of a process, the R\'enyi information dimension of aprocess, and the metric mean dimension.</description><author>Milad Sefidgaran, Abdellatif Zaidi</author><pubDate>Tue, 30 Jan 2024 14:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05369v2</guid></item><item><title>Explaining Explanations in Probabilistic Logic Programming</title><link>http://arxiv.org/abs/2401.17045v1</link><description>The emergence of tools based on artificial intelligence has also led to theneed of producing explanations which are understandable by a human being. Insome approaches, the system is not transparent (often referred to as a "blackbox"), making it difficult to generate appropriate explanations. In this work,though, we consider probabilistic logic programming, a combination of logicprogramming (for knowledge representation) and probability (to modeluncertainty). In this setting, one can say that models are interpretable, whicheases its understanding. However, given a particular query, the usual notion of"explanation" is associated with a set of choices, one for each random variableof the model. Unfortunately, this set does not have a causal structure and, infact, some of the choices are actually irrelevant to the considered query. Inorder to overcome these shortcomings, we present an approach to explainingexplanations which is based on the definition of a query-driven inferencemechanism for probabilistic logic programs.</description><author>Germ√°n Vidal</author><pubDate>Tue, 30 Jan 2024 14:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17045v1</guid></item><item><title>Scalable Mechanism Design for Multi-Agent Path Finding</title><link>http://arxiv.org/abs/2401.17044v1</link><description>Multi-Agent Path Finding (MAPF) involves determining paths for multipleagents to travel simultaneously through a shared area toward particular goallocations. This problem is computationally complex, especially when dealingwith large numbers of agents, as is common in realistic applications likeautonomous vehicle coordination. Finding an optimal solution is oftencomputationally infeasible, making the use of approximate algorithms essential.Adding to the complexity, agents might act in a self-interested and strategicway, possibly misrepresenting their goals to the MAPF algorithm if it benefitsthem. Although the field of mechanism design offers tools to align incentives,using these tools without careful consideration can fail when only havingaccess to approximately optimal outcomes. Since approximations are crucial forscalable MAPF algorithms, this poses a significant challenge. In this work, weintroduce the problem of scalable mechanism design for MAPF and propose threestrategyproof mechanisms, two of which even use approximate MAPF algorithms. Wetest our mechanisms on realistic MAPF domains with problem sizes ranging fromdozens to hundreds of agents. Our findings indicate that they improve welfarebeyond a simple baseline.</description><author>Paul Friedrich, Yulun Zhang, Michael Curry, Ludwig Dierks, Stephen McAleer, Jiaoyang Li, Tuomas Sandholm, Sven Seuken</author><pubDate>Tue, 30 Jan 2024 14:26:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17044v1</guid></item><item><title>CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models</title><link>http://arxiv.org/abs/2401.17043v1</link><description>Retrieval-Augmented Generation (RAG) is a technique that enhances thecapabilities of large language models (LLMs) by incorporating externalknowledge sources. This method addresses common LLM limitations, includingoutdated information and the tendency to produce inaccurate "hallucinated"content. However, the evaluation of RAG systems is challenging, as existingbenchmarks are limited in scope and diversity. Most of the current benchmarkspredominantly assess question-answering applications, overlooking the broaderspectrum of situations where RAG could prove advantageous. Moreover, they onlyevaluate the performance of the LLM component of the RAG pipeline in theexperiments, and neglect the influence of the retrieval component and theexternal knowledge database. To address these issues, this paper constructs alarge-scale and more comprehensive benchmark, and evaluates all the componentsof RAG systems in various RAG application scenarios. Specifically, we havecategorized the range of RAG applications into four distinct types-Create,Read, Update, and Delete (CRUD), each representing a unique use case. "Create"refers to scenarios requiring the generation of original, varied content."Read" involves responding to intricate questions in knowledge-intensivesituations. "Update" focuses on revising and rectifying inaccuracies orinconsistencies in pre-existing texts. "Delete" pertains to the task ofsummarizing extensive texts into more concise forms. For each of these CRUDcategories, we have developed comprehensive datasets to evaluate theperformance of RAG systems. We also analyze the effects of various componentsof the RAG system, such as the retriever, the context length, the knowledgebase construction, and the LLM. Finally, we provide useful insights foroptimizing the RAG technology for different scenarios.</description><author>Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen</author><pubDate>Tue, 30 Jan 2024 14:25:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17043v1</guid></item></channel></rss>