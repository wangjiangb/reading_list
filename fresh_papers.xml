<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 06 Aug 2024 13:00:30 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics</title><link>http://arxiv.org/abs/2408.02672v1</link><description>Implicit Neural Networks (INRs) have emerged as powerful representations toencode all forms of data, including images, videos, audios, and scenes. Withvideo, many INRs for video have been proposed for the compression task, andrecent methods feature significant improvements with respect to encoding time,storage, and reconstruction quality. However, these encoded representationslack semantic meaning, so they cannot be used for any downstream tasks thatrequire such properties, such as retrieval. This can act as a barrier foradoption of video INRs over traditional codecs as they do not offer anysignificant edge apart from compression. To alleviate this, we propose aflexible framework that decouples the spatial and temporal aspects of the videoINR. We accomplish this with a dictionary of per-frame latents that are learnedjointly with a set of video specific hypernetworks, such that given a latent,these hypernetworks can predict the INR weights to reconstruct the given frame.This framework not only retains the compression efficiency, but the learnedlatents can be aligned with features from large vision models, which grantsthem discriminative properties. We align these latents with CLIP and show goodperformance for both compression and video retrieval tasks. By aligning withVideoLlama, we are able to perform open-ended chat with our learned latents asthe visual inputs. Additionally, the learned latents serve as a proxy for theunderlying weights, allowing us perform tasks like video interpolation. Thesesemantic properties and applications, existing simultaneously with ability toperform compression, interpolation, and superresolution properties, are a firstin this field of work.</description><author>Shishira R Maiya, Anubhav Gupta, Matthew Gwilliam, Max Ehrlich, Abhinav Shrivastava</author><pubDate>Mon, 05 Aug 2024 17:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02672v1</guid></item><item><title>Self-Taught Evaluators</title><link>http://arxiv.org/abs/2408.02666v1</link><description>Model-based evaluation is at the heart of successful model development -- asa reward model for training, and as a replacement for human evaluation. Totrain such evaluators, the standard approach is to collect a large amount ofhuman preference judgments over model responses, which is costly and the databecomes stale as models improve. In this work, we present an approach that aimsto im-prove evaluators without human annotations, using synthetic training dataonly. Starting from unlabeled instructions, our iterative self-improvementscheme generates contrasting model outputs and trains an LLM-as-a-Judge toproduce reasoning traces and final judgments, repeating this training at eachnew iteration using the improved predictions. Without any labeled preferencedata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperformscommonly used LLM judges such as GPT-4 and matches the performance of thetop-performing reward models trained with labeled examples.</description><author>Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li</author><pubDate>Mon, 05 Aug 2024 17:57:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02666v1</guid></item><item><title>FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic Segmentation of Diverse Landscapes</title><link>http://arxiv.org/abs/2405.04634v3</link><description>Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as anew tool to monitor territory and support public policies. Processing ALS dataat scale requires efficient point classification methods that perform well overhighly diverse territories. To evaluate them, researchers need large annotatedLidar datasets, however, current Lidar benchmark datasets have restricted scopeand often cover a single urban area. To bridge this data gap, we present theFRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: anultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds withhigh-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL isbuilt upon France's nationwide open Lidar data. It achieves spatial andsemantic diversity via a sampling scheme that explicitly concentrates rareclasses and challenging landscapes from five French regions. It should supportthe development of 3D deep learning approaches for large-scale land monitoring.We describe the nature of the source data, the sampling workflow, the contentof the resulting dataset, and provide an initial evaluation of segmentationperformance using a performant 3D neural architecture.</description><author>Charles Gaydon, Michel Daab, Floryne Roche</author><pubDate>Mon, 05 Aug 2024 17:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04634v3</guid></item><item><title>Quantised Global Autoencoder: A Holistic Approach to Representing Visual Data</title><link>http://arxiv.org/abs/2407.11913v2</link><description>In quantised autoencoders, images are usually split into local patches, eachencoded by one token. This representation is redundant in the sense that thesame number of tokens is spend per region, regardless of the visual informationcontent in that region. Adaptive discretisation schemes like quadtrees areapplied to allocate tokens for patches with varying sizes, but this just variesthe region of influence for a token which nevertheless remains a localdescriptor. Modern architectures add an attention mechanism to the autoencoderwhich infuses some degree of global information into the local tokens. Despitethe global context, tokens are still associated with a local image region. Incontrast, our method is inspired by spectral decompositions which transform aninput signal into a superposition of global frequencies. Taking the data-drivenperspective, we learn custom basis functions corresponding to the codebookentries in our VQ-VAE setup. Furthermore, a decoder combines these basisfunctions in a non-linear fashion, going beyond the simple linear superpositionof spectral decompositions. We can achieve this global description with anefficient transpose operation between features and channels and demonstrate ourperformance on compression.</description><author>Tim Elsner, Paula Usinger, Victor Czech, Gregor Kobsik, Yanjiang He, Isaak Lim, Leif Kobbelt</author><pubDate>Mon, 05 Aug 2024 17:50:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11913v2</guid></item><item><title>Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining</title><link>http://arxiv.org/abs/2408.02657v1</link><description>We present Lumina-mGPT, a family of multimodal autoregressive models capableof various vision and language tasks, particularly excelling in generatingflexible photorealistic images from text descriptions. Unlike existingautoregressive image generation approaches, Lumina-mGPT employs a pretraineddecoder-only transformer as a unified framework for modeling multimodal tokensequences. Our key insight is that a simple decoder-only transformer withmultimodal Generative PreTraining (mGPT), utilizing the next-token predictionobjective on massive interleaved text-image sequences, can learn broad andgeneral multimodal capabilities, thereby illuminating photorealistictext-to-image generation. Building on these pretrained models, we proposeFlexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-textpairs to fully unlock their potential for high-aesthetic image synthesis at anyresolution while maintaining their general multimodal capabilities.Furthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),transforming Lumina-mGPT into a foundation model that seamlessly achievesomnipotent task unification. The resulting model demonstrates versatilemultimodal capabilities, including visual generation tasks like flexibletext-to-image generation and controllable generation, visual recognition taskslike segmentation and depth estimation, and vision-language tasks likemultiturn visual question answering. Additionally, we analyze the differencesand similarities between diffusion-based and autoregressive methods in a directcomparison.</description><author>Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, Peng Gao</author><pubDate>Mon, 05 Aug 2024 17:46:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02657v1</guid></item><item><title>BSRBF-KAN: A combination of B-splines and Radial Basic Functions in Kolmogorov-Arnold Networks</title><link>http://arxiv.org/abs/2406.11173v3</link><description>In this paper, we introduce BSRBF-KAN, a Kolmogorov Arnold Network (KAN) thatcombines Bsplines and radial basis functions (RBFs) to fit input vectors indata training. We perform experiments with BSRBF-KAN, MLP, and other popularKANs, including EfficientKAN, FastKAN, FasterKAN, and GottliebKAN over theMNIST and Fashion-MNIST datasets. BSRBF-KAN shows stability in 5 trainingsessions with a competitive average accuracy of 97.55% on MNIST and 89.33% onFashionMNIST and obtains convergence better than other networks. We expectBSRBF-KAN to open many combinations of mathematical functions to design KANs.Our repo is publicly available at: https://github.com/hoangthangta/BSRBF-KAN.</description><author>Hoang-Thang Ta</author><pubDate>Mon, 05 Aug 2024 17:39:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11173v3</guid></item><item><title>On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization</title><link>http://arxiv.org/abs/2408.02654v1</link><description>The effectiveness of training neural networks directly impacts computationalcosts, resource allocation, and model development timelines in machine learningapplications. An optimizer's ability to train the model adequately (in terms oftrained model performance) depends on the model's initial weights. Model weightinitialization schemes use pseudorandom number generators (PRNGs) as a sourceof randomness. We investigate whether substituting PRNGs for low-discrepancy quasirandomnumber generators (QRNGs) -- namely Sobol' sequences -- as a source ofrandomness for initializers can improve model performance. We examineMulti-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), LongShort-Term Memory (LSTM), and Transformer architectures trained on MNIST,CIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis usesten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);Orthogonal, Random Normal, Truncated Normal, and Random Uniform. Models withweights set using PRNG- and QRNG-based initializers are compared pairwise foreach combination of dataset, architecture, optimizer, and initializationscheme. Our findings indicate that QRNG-based neural network initializers eitherreach a higher accuracy or achieve the same accuracy more quickly thanPRNG-based initializers in 60% of the 120 experiments conducted. Thus, usingQRNG-based initializers instead of PRNG-based initializers can speed up andimprove model training.</description><author>Andriy Miranskyy, Adam Sorrenti, Viral Thakar</author><pubDate>Mon, 05 Aug 2024 17:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02654v1</guid></item><item><title>Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?</title><link>http://arxiv.org/abs/2408.02651v1</link><description>Large Language Models (LLMs) have demonstrated impressive capabilities innatural language tasks, but their safety and morality remain contentious due totheir training on internet text corpora. To address these concerns, alignmenttechniques have been developed to improve the public usability and safety ofLLMs. Yet, the potential for generating harmful content through these modelsseems to persist. This paper explores the concept of jailbreakingLLMs-reversing their alignment through adversarial triggers. Previous methods,such as soft embedding prompts, manually crafted prompts, and gradient-basedautomatic prompts, have had limited success on black-box models due to theirrequirements for model access and for producing a low variety of manuallycrafted prompts, making them susceptible to being blocked. This paperintroduces a novel approach using reinforcement learning to optimizeadversarial triggers, requiring only inference API access to the target modeland a small surrogate model. Our method, which leverages a BERTScore-basedreward function, enhances the transferability and effectiveness of adversarialtriggers on new black-box models. We demonstrate that this approach improvesthe performance of adversarial triggers on a previously untested languagemodel.</description><author>Mohammad Bahrami Karkevandi, Nishant Vishwamitra, Peyman Najafirad</author><pubDate>Mon, 05 Aug 2024 17:27:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02651v1</guid></item><item><title>Detection of Compromised Functions in a Serverless Cloud Environment</title><link>http://arxiv.org/abs/2408.02641v1</link><description>Serverless computing is an emerging cloud paradigm with serverless functionsat its core. While serverless environments enable software developers to focuson developing applications without the need to actively manage the underlyingruntime infrastructure, they open the door to a wide variety of securitythreats that can be challenging to mitigate with existing methods. Existingsecurity solutions do not apply to all serverless architectures, since theyrequire significant modifications to the serverless infrastructure or rely onthird-party services for the collection of more detailed data. In this paper,we present an extendable serverless security threat detection model thatleverages cloud providers' native monitoring tools to detect anomalous behaviorin serverless applications. Our model aims to detect compromised serverlessfunctions by identifying post-exploitation abnormal behavior related todifferent types of attacks on serverless functions, and therefore, it is a lastline of defense. Our approach is not tied to any specific serverlessapplication, is agnostic to the type of threats, and is adaptable through modeladjustments. To evaluate our model's performance, we developed a serverlesscybersecurity testbed in an AWS cloud environment, which includes two differentserverless applications and simulates a variety of attack scenarios that coverthe main security threats faced by serverless functions. Our evaluationdemonstrates our model's ability to detect all implemented attacks whilemaintaining a negligible false alarm rate.</description><author>Danielle Lavi, Oleg Brodt, Dudu Mimran, Yuval Elovici, Asaf Shabtai</author><pubDate>Mon, 05 Aug 2024 17:14:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02641v1</guid></item><item><title>Command-line Obfuscation Detection using Small Language Models</title><link>http://arxiv.org/abs/2408.02637v1</link><description>To avoid detection, adversaries often use command-line obfuscation. There arenumerous techniques of the command-line obfuscation, all designed to alter thecommand-line syntax without affecting its original functionality. Thisvariability forces most security solutions to create an exhaustive enumerationof signatures for even a single pattern. In contrast to using signatures, wehave implemented a scalable NLP-based detection method that leverages acustom-trained, small transformer language model that can be applied to anysource of execution logs. The evaluation on top of real-world telemetrydemonstrates that our approach yields high-precision detections even onhigh-volume telemetry from a diverse set of environments spanning fromuniversities and businesses to healthcare or finance. The practical value isdemonstrated in a case study of real-world samples detected by our model. Weshow the model's superiority to signatures on established malware known toemploy obfuscation and showcase previously unseen obfuscated samples detectedby our model.</description><author>Vojtech Outrata, Michael Adam Polak, Martin Kopp</author><pubDate>Mon, 05 Aug 2024 17:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02637v1</guid></item><item><title>Partial End-to-end Reinforcement Learning for Robustness Against Modelling Error in Autonomous Racing</title><link>http://arxiv.org/abs/2312.06406v2</link><description>In this paper, we address the issue of increasing the performance ofreinforcement learning (RL) solutions for autonomous racing cars whennavigating under conditions where practical vehicle modelling errors (commonlyknown as \emph{model mismatches}) are present. To address this challenge, wepropose a partial end-to-end algorithm that decouples the planning and controltasks. Within this framework, an RL agent generates a trajectory comprising apath and velocity, which is subsequently tracked using a pure pursuit steeringcontroller and a proportional velocity controller, respectively. In contrast,many current learning-based (i.e., reinforcement and imitation learning)algorithms utilise an end-to-end approach whereby a deep neural networkdirectly maps from sensor data to control commands. By leveraging therobustness of a classical controller, our partial end-to-end driving algorithmexhibits better robustness towards model mismatches than standard end-to-endalgorithms.</description><author>Andrew Murdoch, Johannes Cornelius Schoeman, Hendrik Willem Jordaan</author><pubDate>Mon, 05 Aug 2024 17:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06406v2</guid></item><item><title>Interactive 3D Medical Image Segmentation with SAM 2</title><link>http://arxiv.org/abs/2408.02635v1</link><description>Interactive medical image segmentation (IMIS) has shown significant potentialin enhancing segmentation accuracy by integrating iterative feedback frommedical professionals. However, the limited availability of enough 3D medicaldata restricts the generalization and robustness of most IMIS methods. TheSegment Anything Model (SAM), though effective for 2D images, requiresexpensive semi-auto slice-by-slice annotations for 3D medical images. In thispaper, we explore the zero-shot capabilities of SAM 2, the next-generation MetaSAM model trained on videos, for 3D medical image segmentation. By treatingsequential 2D slices of 3D images as video frames, SAM 2 can fullyautomatically propagate annotations from a single frame to the entire 3Dvolume. We propose a practical pipeline for using SAM 2 in 3D medical imagesegmentation and present key findings highlighting its efficiency and potentialfor further optimization. Concretely, numerical experiments on the BraTS2020and the medical segmentation decathlon datasets demonstrate that SAM 2 stillhas a gap with supervised methods but can narrow the gap in specific settingsand organ types, significantly reducing the annotation burden on medicalprofessionals. Our code will be open-sourced and available athttps://github.com/Chuyun-Shen/SAM_2_Medical_3D.</description><author>Chuyun Shen, Wenhao Li, Yuhang Shi, Xiangfeng Wang</author><pubDate>Mon, 05 Aug 2024 16:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02635v1</guid></item><item><title>SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models</title><link>http://arxiv.org/abs/2408.02632v1</link><description>As large language models (LLMs) continue to advance in capability andinfluence, ensuring their security and preventing harmful outputs has becomecrucial. A promising approach to address these concerns involves trainingmodels to automatically generate adversarial prompts for red teaming. However,the evolving subtlety of vulnerabilities in LLMs challenges the effectivenessof current adversarial methods, which struggle to specifically target andexplore the weaknesses of these models. To tackle these challenges, weintroduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving}\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$optimization framework, which enhances security by leveraging data generated bythe model itself. SEAS operates through three iterative stages: Initialization,Attack, and Adversarial Optimization, refining both the Red Team and Targetmodels to improve robustness and safety. This framework reduces reliance onmanual testing and significantly enhances the security capabilities of LLMs.Our contributions include a novel adversarial framework, a comprehensive safetydataset, and after three iterations, the Target model achieves a security levelcomparable to GPT-4, while the Red Team model shows a marked increase in attacksuccess rate (ASR) against advanced models.</description><author>Muxi Diao, Rumei Li, Shiyang Liu, Guogang Liao, Jingang Wang, Xunliang Cai, Weiran Xu</author><pubDate>Mon, 05 Aug 2024 16:55:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02632v1</guid></item><item><title>VidGen-1M: A Large-Scale Dataset for Text-to-video Generation</title><link>http://arxiv.org/abs/2408.02629v1</link><description>The quality of video-text pairs fundamentally determines the upper bound oftext-to-video models. Currently, the datasets used for training these modelssuffer from significant shortcomings, including low temporal consistency,poor-quality captions, substandard video quality, and imbalanced datadistribution. The prevailing video curation process, which depends on imagemodels for tagging and manual rule-based curation, leads to a highcomputational load and leaves behind unclean data. As a result, there is a lackof appropriate training datasets for text-to-video models. To address thisproblem, we present VidGen-1M, a superior training dataset for text-to-videomodels. Produced through a coarse-to-fine curation strategy, this datasetguarantees high-quality videos and detailed captions with excellent temporalconsistency. When used to train the video generation model, this dataset hasled to experimental results that surpass those obtained with other models.</description><author>Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, Hao Li</author><pubDate>Mon, 05 Aug 2024 16:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02629v1</guid></item><item><title>Unsupervised Change Detection for Space Habitats Using 3D Point Clouds</title><link>http://arxiv.org/abs/2312.02396v3</link><description>This work presents an algorithm for scene change detection from point cloudsto enable autonomous robotic caretaking in future space habitats. Autonomousrobotic systems will help maintain future deep-space habitats, such as theGateway space station, which will be uncrewed for extended periods. Existingscene analysis software used on the International Space Station (ISS) relies onmanually-labeled images for detecting changes. In contrast, the algorithmpresented in this work uses raw, unlabeled point clouds as inputs. Thealgorithm first applies modified Expectation-Maximization Gaussian MixtureModel (GMM) clustering to two input point clouds. It then performs changedetection by comparing the GMMs using the Earth Mover's Distance. The algorithmis validated quantitatively and qualitatively using a test dataset collected byan Astrobee robot in the NASA Ames Granite Lab comprising single frame depthimages taken directly by Astrobee and full-scene reconstructed maps built withRGB-D and pose data from Astrobee. The runtimes of the approach are alsoanalyzed in depth. The source code is publicly released to promote furtherdevelopment.</description><author>Jamie Santos, Holly Dinkel, Julia Di, Paulo V. K. Borges, Marina Moreira, Oleg Alexandrov, Brian Coltin, Trey Smith</author><pubDate>Mon, 05 Aug 2024 16:49:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02396v3</guid></item><item><title>YOWOv3: An Efficient and Generalized Framework for Human Action Detection and Recognition</title><link>http://arxiv.org/abs/2408.02623v1</link><description>In this paper, we propose a new framework called YOWOv3, which is an improvedversion of YOWOv2, designed specifically for the task of Human Action Detectionand Recognition. This framework is designed to facilitate extensiveexperimentation with different configurations and supports easy customizationof various components within the model, reducing efforts required forunderstanding and modifying the code. YOWOv3 demonstrates its superiorperformance compared to YOWOv2 on two widely used datasets for Human ActionDetection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessormodel YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2,respectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model -YOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33%and 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate thatYOWOv3 significantly reduces the number of parameters and GFLOPs while stillachieving comparable performance.</description><author>Duc Manh Nguyen Dang, Viet Hang Duong, Jia Ching Wang, Nhan Bui Duc</author><pubDate>Mon, 05 Aug 2024 16:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02623v1</guid></item><item><title>Language Model Can Listen While Speaking</title><link>http://arxiv.org/abs/2408.02622v1</link><description>Dialogue serves as the most natural manner of human-computer interaction(HCI). Recent advancements in speech language models (SLM) have significantlyenhanced speech-based conversational AI. However, these models are limited toturn-based conversation, lacking the ability to interact with humans inreal-time spoken scenarios, for example, being interrupted when the generatedcontent is not satisfactory. To address these limitations, we explore fullduplex modeling (FDM) in interactive speech language models (iSLM), focusing onenhancing real-time interaction and, more explicitly, exploring thequintessential ability of interruption. We introduce a novel model design,namely listening-while-speaking language model (LSLM), an end-to-end systemequipped with both listening and speaking channels. Our LSLM employs atoken-based decoder-only TTS for speech generation and a streamingself-supervised learning (SSL) encoder for real-time audio input. LSLM fusesboth channels for autoregressive generation and detects turn-taking in realtime. Three fusion strategies -- early fusion, middle fusion, and late fusion-- are explored, with middle fusion achieving an optimal balance between speechgeneration and real-time interaction. Two experimental settings, command-basedFDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivityto diverse instructions. Our results highlight LSLM's capability to achieveduplex communication with minimal impact on existing systems. This study aimsto advance the development of interactive speech dialogue systems, enhancingtheir applicability in real-world contexts.</description><author>Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen</author><pubDate>Mon, 05 Aug 2024 16:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02622v1</guid></item><item><title>LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba</title><link>http://arxiv.org/abs/2408.02615v1</link><description>Recent Transformer-based diffusion models have shown remarkable performance,largely attributed to the ability of the self-attention mechanism to accuratelycapture both global and local contexts by computing all-pair interactions amonginput tokens. However, their quadratic complexity poses significantcomputational challenges for long-sequence inputs. Conversely, a recent statespace model called Mamba offers linear complexity by compressing a filteredglobal context into a hidden state. Despite its efficiency, compressioninevitably leads to information loss of fine-grained local dependencies amongtokens, which are crucial for effective visual generative modeling. Motivatedby these observations, we introduce Local Attentional Mamba (LaMamba) blocksthat combine the strengths of self-attention and Mamba, capturing both globalcontexts and local details with linear complexity. Leveraging the efficientU-Net architecture, our model exhibits exceptional scalability and surpassesthe performance of DiT across various model scales on ImageNet at 256x256resolution, all while utilizing substantially fewer GFLOPs and a comparablenumber of parameters. Compared to state-of-the-art diffusion models on ImageNet256x256 and 512x512, our largest model presents notable advantages, such as areduction of up to 62\% GFLOPs compared to DiT-XL/2, while achieving superiorperformance with comparable or fewer parameters.</description><author>Yunxiang Fu, Chaoqi Chen, Yizhou Yu</author><pubDate>Mon, 05 Aug 2024 16:39:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02615v1</guid></item><item><title>SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications</title><link>http://arxiv.org/abs/2403.11515v2</link><description>Monocular depth estimation (MDE) has advanced significantly, primarilythrough the integration of convolutional neural networks (CNNs) and morerecently, Transformers. However, concerns about their susceptibility toadversarial attacks have emerged, especially in safety-critical domains likeautonomous driving and robotic navigation. Existing approaches for assessingCNN-based depth prediction methods have fallen short in inducing comprehensivedisruptions to the vision system, often limited to specific local areas. Inthis paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novelapproach designed to comprehensively disrupt monocular depth estimation (MDE)in autonomous navigation applications. Our patch is crafted to selectivelyundermine MDE in two distinct ways: by distorting estimated distances or bycreating the illusion of an object disappearing from the system's perspective.Notably, our patch is shape-sensitive, meaning it considers the specific shapeand scale of the target object, thereby extending its influence beyondimmediate proximity. Furthermore, our patch is trained to effectively addressdifferent scales and distances from the camera. Experimental resultsdemonstrate that our approach induces a mean depth estimation error surpassing0.5, impacting up to 99% of the targeted region for CNN-based MDE models.Additionally, we investigate the vulnerability of Transformer-based MDE modelsto patch-based attacks, revealing that SSAP yields a significant error of 0.59and exerts substantial influence over 99% of the target region on these models.</description><author>Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Bassem Ouni, Muhammad Shafique</author><pubDate>Mon, 05 Aug 2024 16:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11515v2</guid></item><item><title>APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation</title><link>http://arxiv.org/abs/2303.01351v3</link><description>In recent times, monocular depth estimation (MDE) has experienced significantadvancements in performance, largely attributed to the integration ofinnovative architectures, i.e., convolutional neural networks (CNNs) andTransformers. Nevertheless, the susceptibility of these models to adversarialattacks has emerged as a noteworthy concern, especially in domains where safetyand security are paramount. This concern holds particular weight for MDE due toits critical role in applications like autonomous driving and roboticnavigation, where accurate scene understanding is pivotal. To assess thevulnerability of CNN-based depth prediction methods, recent work tries todesign adversarial patches against MDE. However, the existing approaches fallshort of inducing a comprehensive and substantially disruptive impact on thevision system. Instead, their influence is partial and confined to specificlocal areas. These methods lead to erroneous depth predictions only within theoverlapping region with the input image, without considering thecharacteristics of the target object, such as its size, shape, and position. Inthis paper, we introduce a novel adversarial patch named APARATE. This patchpossesses the ability to selectively undermine MDE in two distinct ways: bydistorting the estimated distances or by creating the illusion of an objectdisappearing from the perspective of the autonomous system. Notably, APARATE isdesigned to be sensitive to the shape and scale of the target object, and itsinfluence extends beyond immediate proximity. APARATE, results in a mean depthestimation error surpassing $0.5$, significantly impacting as much as $99\%$ ofthe targeted region when applied to CNN-based MDE models. Furthermore, ityields a significant error of $0.34$ and exerts substantial influence over$94\%$ of the target region in the context of Transformer-based MDE.</description><author>Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Muhammad Shafique</author><pubDate>Mon, 05 Aug 2024 16:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.01351v3</guid></item><item><title>Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need</title><link>http://arxiv.org/abs/2303.07338v2</link><description>Class-incremental learning (CIL) aims to adapt to emerging new classeswithout forgetting old ones. Traditional CIL models are trained from scratch tocontinually acquire knowledge as data evolves. Recently, pre-training hasachieved substantial progress, making vast pre-trained models (PTMs) accessiblefor CIL. Contrary to traditional methods, PTMs possess generalizableembeddings, which can be easily transferred for CIL. In this work, we revisitCIL with PTMs and argue that the core factors in CIL are adaptivity for modelupdating and generalizability for knowledge transferring. 1) We first revealthat frozen PTM can already provide generalizable embeddings for CIL.Surprisingly, a simple baseline (SimpleCIL) which continually sets theclassifiers of PTM to prototype features can beat state-of-the-art even withouttraining on the downstream task. 2) Due to the distribution gap betweenpre-trained and downstream datasets, PTM can be further cultivated withadaptivity via model adaptation. We propose AdaPt and mERge (APER), whichaggregates the embeddings of PTM and adapted models for classifierconstruction. APER is a general framework that can be orthogonally combinedwith any parameter-efficient tuning method, which holds the advantages of PTM'sgeneralizability and adapted model's adaptivity. 3) Additionally, consideringprevious ImageNet-based benchmarks are unsuitable in the era of PTM due to dataoverlapping, we propose four new benchmarks for assessment, namely ImageNet-A,ObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate theeffectiveness of APER with a unified and concise framework. Code is availableat https://github.com/zhoudw-zdw/RevisitingCIL</description><author>Da-Wei Zhou, Zi-Wen Cai, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu</author><pubDate>Mon, 05 Aug 2024 16:34:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07338v2</guid></item><item><title>Backward explanations via redefinition of predicates</title><link>http://arxiv.org/abs/2408.02606v1</link><description>History eXplanation based on Predicates (HXP), studies the behavior of aReinforcement Learning (RL) agent in a sequence of agent's interactions withthe environment (a history), through the prism of an arbitrary predicate. Tothis end, an action importance score is computed for each action in thehistory. The explanation consists in displaying the most important actions tothe user. As the calculation of an action's importance is #W[1]-hard, it isnecessary for long histories to approximate the scores, at the expense of theirquality. We therefore propose a new HXP method, called Backward-HXP, to provideexplanations for these histories without having to approximate scores.Experiments show the ability of B-HXP to summarise long histories.</description><author>Léo Saulières, Martin C. Cooper, Florence Dupin de Saint Cyr</author><pubDate>Mon, 05 Aug 2024 16:31:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02606v1</guid></item><item><title>Learning rheological parameters of non-Newtonian fluids from velocimetry data</title><link>http://arxiv.org/abs/2408.02604v1</link><description>We solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilatesvelocimetry data in order to jointly reconstruct the flow field and learn theunknown N-S parameters. By incorporating a Carreau shear-thinning viscositymodel into the N-S problem, we devise an algorithm that learns the most likelyCarreau parameters of a shear-thinning fluid, and estimates theiruncertainties, from velocimetry data alone. We then conduct a flow-MRIexperiment to obtain velocimetry data of an axisymmetric laminar jet through anidealised medical device (FDA nozzle) for a blood analogue fluid. We show thatthe algorithm can successfully reconstruct the flow field by learning the mostlikely Carreau parameters, and that the learned parameters are in very goodagreement with rheometry measurements. The algorithm accepts any algebraiceffective viscosity model, as long as the model is differentiable, and it canbe extended to more complicated non-Newtonian fluids (e.g. Oldroyd-B fluid) ifa viscoelastic model is incorporated into the N-S problem.</description><author>Alexandros Kontogiannis, Richard Hodgkinson, Emily L. Manchester</author><pubDate>Mon, 05 Aug 2024 16:27:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02604v1</guid></item><item><title>BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba</title><link>http://arxiv.org/abs/2408.02600v1</link><description>The advancement of natural language processing (NLP) in biology hinges onmodels' ability to interpret intricate biomedical literature. Traditionalmodels often struggle with the complex and domain-specific language in thisfield. In this paper, we present BioMamba, a pre-trained model specificallydesigned for biomedical text mining. BioMamba builds upon the Mambaarchitecture and is pre-trained on an extensive corpus of biomedicalliterature. Our empirical studies demonstrate that BioMamba significantlyoutperforms models like BioBERT and general-domain Mamba across variousbiomedical tasks. For instance, BioMamba achieves a 100 times reduction inperplexity and a 4 times reduction in cross-entropy loss on the BioASQ testset. We provide an overview of the model architecture, pre-training process,and fine-tuning techniques. Additionally, we release the code and trained modelto facilitate further research.</description><author>Ling Yue, Sixue Xing, Yingzhou Lu, Tianfan Fu</author><pubDate>Mon, 05 Aug 2024 16:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02600v1</guid></item><item><title>Progressively Selective Label Enhancement for Language Model Alignment</title><link>http://arxiv.org/abs/2408.02599v1</link><description>Large Language Models have demonstrated impressive capabilities in variouslanguage tasks but may produce content that misaligns with human expectations,raising ethical and legal concerns. Therefore, it is important to explore thelimitations and implement restrictions on the models to ensure safety andcompliance, with Reinforcement Learning from Human Feedback (RLHF) being theprimary method. Due to challenges in stability and scalability with the RLHFstages, researchers are exploring alternative methods to achieve effectscomparable to those of RLHF. However, these methods often depend on largehigh-quality datasets and inefficiently utilize generated data. To deal withthis problem, we propose PSLE, i.e., Progressively Selective Label Enhancementfor Language Model Alignment, a framework that fully utilizes all generateddata by guiding the model with principles to align outputs with humanexpectations. Using a dynamically updated threshold, our approach ensuresefficient data utilization by incorporating all generated responses andweighting them based on their corresponding reward scores. Experimental resultson multiple datasets demonstrate the effectiveness of PSLE compared to existinglanguage model alignment methods.</description><author>Biao Liu, Ning Xu, Xin Geng</author><pubDate>Mon, 05 Aug 2024 16:21:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02599v1</guid></item><item><title>Predicting and Understanding Human Action Decisions: Insights from Large Language Models and Cognitive Instance-Based Learning</title><link>http://arxiv.org/abs/2407.09281v2</link><description>Large Language Models (LLMs) have demonstrated their capabilities acrossvarious tasks, from language translation to complex reasoning. Understandingand predicting human behavior and biases are crucial for artificialintelligence (AI) assisted systems to provide useful assistance, yet it remainsan open question whether these models can achieve this. This paper addressesthis gap by leveraging the reasoning and generative capabilities of the LLMs topredict human behavior in two sequential decision-making tasks. These tasksinvolve balancing between exploitative and exploratory actions and handlingdelayed feedback, both essential for simulating real-life decision processes.We compare the performance of LLMs with a cognitive instance-based learning(IBL) model, which imitates human experiential decision-making. Our findingsindicate that LLMs excel at rapidly incorporating feedback to enhanceprediction accuracy. In contrast, the cognitive IBL model better accounts forhuman exploratory behaviors and effectively captures loss aversion bias, i.e.,the tendency to choose a sub-optimal goal with fewer step-cost penalties ratherthan exploring to find the optimal choice, even with limited experience. Theresults highlight the benefits of integrating LLMs with cognitivearchitectures, suggesting that this synergy could enhance the modeling andunderstanding of complex human decision-making patterns.</description><author>Thuy Ngoc Nguyen, Kasturi Jamale, Cleotilde Gonzalez</author><pubDate>Mon, 05 Aug 2024 16:16:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09281v2</guid></item><item><title>AI-Driven Strategies for Reducing Student Withdrawal -- A Study of EMU Student Stopout</title><link>http://arxiv.org/abs/2408.02598v1</link><description>Not everyone who enrolls in college will leave with a certificate or degree,but the number of people who drop out or take a break is much higher thanexperts previously believed. In December 2013, there were 29 million peoplewith some college education but no degree. That number jumped to 36 million byDecember of 2018, according to a new report from the National StudentClearinghouse Research Center[1]. It is imperative to understand the underlyingfactors contributing to student withdrawal and to assist decision-makers toidentify effective strategies to prevent it. By analyzing the characteristicsand educational pathways of the stopout student population, our aim is toprovide actionable insights that can benefit institutions facing similarchallenges. Eastern Michigan University (EMU) faces significant challenges instudent retention, with approximately 55% of its undergraduate students notcompleting their degrees within six years. As an institution committed tostudent success, EMU conducted a comprehensive study of student withdrawals tounderstand the influencing factors. And the paper revealed a high correlationbetween certain factors and withdrawals, even in the early stages of universityattendance. Based on these findings, we developed a predictive model thatemploys artificial intelligence techniques to assess the potential risk thatstudents abandon their studies. These models enable universities to implementearly intervention strategies, support at-risk students, and improve overallhigher education success.</description><author>Yan Zhao, Amy Otteson</author><pubDate>Mon, 05 Aug 2024 16:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02598v1</guid></item><item><title>Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection</title><link>http://arxiv.org/abs/2408.02595v1</link><description>Sarcasm is a type of irony, characterized by an inherent mismatch between theliteral interpretation and the intended connotation. Though sarcasm detectionin text has been extensively studied, there are situations in which textualinput alone might be insufficient to perceive sarcasm. The inclusion ofadditional contextual cues, such as images, is essential to recognize sarcasmin social media data effectively. This study presents a novel framework formultimodal sarcasm detection that can process input triplets. Two components ofthese triplets comprise the input text and its associated image, as provided inthe datasets. Additionally, a supplementary modality is introduced in the formof descriptive image captions. The motivation behind incorporating this visualsemantic representation is to more accurately capture the discrepancies betweenthe textual and visual content, which are fundamental to the sarcasm detectiontask. The primary contributions of this study are: (1) a robust textual featureextraction branch that utilizes a cross-lingual language model; (2) a visualfeature extraction branch that incorporates a self-regulated residual ConvNetintegrated with a lightweight spatially aware attention module; (3) anadditional modality in the form of image captions generated using anencoder-decoder architecture capable of reading text embedded in images; (4)distinct attention modules to effectively identify the incongruities betweenthe text and two levels of image representations; (5) multi-level cross-domainsemantic incongruity representation achieved through feature fusion. Comparedwith cutting-edge baselines, the proposed model achieves the best accuracy of92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm andMultiBully datasets.</description><author>Sajal Aggarwal, Ananya Pandey, Dinesh Kumar Vishwakarma</author><pubDate>Mon, 05 Aug 2024 16:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02595v1</guid></item><item><title>Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning</title><link>http://arxiv.org/abs/2402.16517v2</link><description>Finite element-based high-order solvers of conservation laws offer largeaccuracy but face challenges near discontinuities due to the Gibbs phenomenon.Artificial viscosity is a popular and effective solution to this problem basedon physical insight. In this work, we present a physics-informed machinelearning algorithm to automate the discovery of artificial viscosity models ina non-supervised paradigm. The algorithm is inspired by reinforcement learningand trains a neural network acting cell-by-cell (the viscosity model) byminimizing a loss defined as the difference with respect to a referencesolution thanks to automatic differentiation. This enables a dataset-freetraining procedure. We prove that the algorithm is effective by integrating itinto a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcaseseveral numerical tests on scalar and vectorial problems, such as Burgers' andEuler's equations in one and two dimensions. Results demonstrate that theproposed approach trains a model that is able to outperform classical viscositymodels. Moreover, we show that the learnt artificial viscosity model is able togeneralize across different problems and parameters.</description><author>Matteo Caldana, Paola F. Antonietti, Luca Dede'</author><pubDate>Mon, 05 Aug 2024 16:02:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16517v2</guid></item><item><title>Intent Detection and Entity Extraction from BioMedical Literature</title><link>http://arxiv.org/abs/2404.03598v2</link><description>Biomedical queries have become increasingly prevalent in web searches,reflecting the growing interest in accessing biomedical literature. Despiterecent research on large-language models (LLMs) motivated by endeavours toattain generalized intelligence, their efficacy in replacing task anddomain-specific natural language understanding approaches remains questionable.In this paper, we address this question by conducting a comprehensive empiricalevaluation of intent detection and named entity recognition (NER) tasks frombiomedical text. We show that Supervised Fine Tuned approaches are stillrelevant and more effective than general-purpose LLMs. Biomedical transformermodels such as PubMedBERT can surpass ChatGPT on NER task with only 5supervised examples.</description><author>Ankan Mullick, Mukur Gupta, Pawan Goyal</author><pubDate>Mon, 05 Aug 2024 16:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03598v2</guid></item><item><title>Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization</title><link>http://arxiv.org/abs/2408.02584v1</link><description>The ever-increasing volume of digital information necessitates efficientmethods for users to extract key insights from lengthy documents. Aspect-basedsummarization offers a targeted approach, generating summaries focused onspecific aspects within a document. Despite advancements in aspect-basedsummarization research, there is a continuous quest for improved modelperformance. Given that large language models (LLMs) have demonstrated thepotential to revolutionize diverse tasks within natural language processing,particularly in the problem of summarization, this paper explores the potentialof fine-tuning LLMs for the aspect-based summarization task. We evaluate theimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,Gemma and Aya, on a publicly available domain-specific aspect based summarydataset. We hypothesize that this approach will enable these models toeffectively identify and extract aspect-related information, leading tosuperior quality aspect-based summaries compared to the state-of-the-art. Weestablish a comprehensive evaluation framework to compare the performance offine-tuned LLMs against competing aspect-based summarization methods andvanilla counterparts of the fine-tuned LLMs. Our work contributes to the fieldof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMsfor generating high-quality aspect-based summaries. Furthermore, it opens doorsfor further exploration of using LLMs for targeted information extraction tasksacross various NLP domains.</description><author>Ankan Mullick, Sombit Bose, Rounak Saha, Ayan Kumar Bhowmick, Aditya Vempaty, Pawan Goyal, Niloy Ganguly, Prasenjit Dey, Ravi Kokku</author><pubDate>Mon, 05 Aug 2024 16:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02584v1</guid></item><item><title>Clustering and Mining Accented Speech for Inclusive and Fair Speech Recognition</title><link>http://arxiv.org/abs/2408.02582v1</link><description>Modern automatic speech recognition (ASR) systems are typically trained onmore than tens of thousands hours of speech data, which is one of the mainfactors for their great success. However, the distribution of such data istypically biased towards common accents or typical speech patterns. As aresult, those systems often poorly perform on atypical accented speech. In thispaper, we present accent clustering and mining schemes for fair speechrecognition systems which can perform equally well on under-representedaccented speech. For accent recognition, we applied three schemes to overcomelimited size of supervised accent data: supervised or unsupervisedpre-training, distributionally robust optimization (DRO) and unsupervisedclustering. Three schemes can significantly improve the accent recognitionmodel especially for unbalanced and small accented speech. Fine-tuning ASR onthe mined Indian accent speech using the proposed supervised or unsupervisedclustering schemes showed 10.0% and 5.3% relative improvements compared tofine-tuning on the randomly sampled speech, respectively.</description><author>Jaeyoung Kim, Han Lu, Soheil Khorram, Anshuman Tripathi, Qian Zhang, Hasim Sak</author><pubDate>Mon, 05 Aug 2024 16:00:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02582v1</guid></item><item><title>Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles</title><link>http://arxiv.org/abs/2407.18932v2</link><description>Human mobility is inextricably linked to social issues such as trafficcongestion, energy consumption, and public health; however, privacy concernsrestrict access to mobility data. Recently, research have utilized LargeLanguage Models (LLMs) for human mobility generation, in which the challenge ishow LLMs can understand individuals' mobility behavioral differences togenerate realistic trajectories conforming to real world contexts. This studyhandles this problem by presenting an LLM agent-based framework (MobAgent)composing two phases: understanding-based mobility pattern extraction andreasoning-based trajectory generation, which enables generate more real traveldiaries at urban scale, considering different individual profiles. MobAgentextracts reasons behind specific mobility trendiness and attribute influencesto provide reliable patterns; infers the relationships between contextualfactors and underlying motivations of mobility; and based on the patterns andthe recursive reasoning process, MobAgent finally generates more authentic andpersonalized mobilities that reflect both individual differences and real-worldconstraints. We validate our framework with 0.2 million travel survey data,demonstrating its effectiveness in producing personalized and accurate traveldiaries. This study highlights the capacity of LLMs to provide detailed andsophisticated understanding of human mobility through the real-world mobilitydata.</description><author>Xuchuan Li, Fei Huang, Jianrong Lv, Zhixiong Xiao, Guolong Li, Yang Yue</author><pubDate>Mon, 05 Aug 2024 15:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18932v2</guid></item><item><title>Operational range bounding of spectroscopy models with anomaly detection</title><link>http://arxiv.org/abs/2408.02581v1</link><description>Safe operation of machine learning models requires architectures thatexplicitly delimit their operational ranges. We evaluate the ability of anomalydetection algorithms to provide indicators correlated with degraded modelperformance. By placing acceptance thresholds over such indicators, hardboundaries are formed that define the model's coverage. As a use case, weconsider the extraction of exoplanetary spectra from transit light curves,specifically within the context of ESA's upcoming Ariel mission. IsolationForests are shown to effectively identify contexts where prediction models arelikely to fail. Coverage/error trade-offs are evaluated under conditions ofdata and concept drift. The best performance is seen when Isolation Forestsmodel projections of the prediction model's explainability SHAP values.</description><author>Luís F. Simões, Pierluigi Casale, Marília Felismino, Kai Hou Yip, Ingo P. Waldmann, Giovanna Tinetti, Theresa Lueftinger</author><pubDate>Mon, 05 Aug 2024 15:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02581v1</guid></item><item><title>Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human Annotation: A Case Study Using Schedule-of-Event Table Detection</title><link>http://arxiv.org/abs/2405.06093v2</link><description>Large Language Models (LLMs) have demonstrated their efficacy across a broadspectrum of tasks in healthcare applications. However, often LLMs need to befine-tuned on task-specific expert annotated data to achieve optimalperformance, which can be expensive and time consuming. In this study, wefine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labelsobtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)tables, which specify care plan in clinical trial protocols. We introduce afiltering mechanism to select high-confidence labels for this tableclassification task, thereby reducing the noise in the auto-generated labels.We show that fine-tuned PaLM-2 with those labels achieves performance thatexceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance isclose to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Ourresults show that leveraging LLM-generated labels through powerful models likegemini-pro can potentially serve as a viable strategy for improving LLMperformance through fine-tuning in specialized tasks, particularly in domainswhere expert annotations are scarce, expensive, or time-consuming to obtain.</description><author>Bhawesh Kumar, Jonathan Amar, Eric Yang, Nan Li, Yugang Jia</author><pubDate>Mon, 05 Aug 2024 15:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06093v2</guid></item><item><title>Bridging Smoothness and Approximation: Theoretical Insights into Over-Smoothing in Graph Neural Networks</title><link>http://arxiv.org/abs/2407.01281v2</link><description>In this paper, we explore the approximation theory of functions defined ongraphs. Our study builds upon the approximation results derived from the$K$-functional. We establish a theoretical framework to assess the lower boundsof approximation for target functions using Graph Convolutional Networks (GCNs)and examine the over-smoothing phenomenon commonly observed in these networks.Initially, we introduce the concept of a $K$-functional on graphs, establishingits equivalence to the modulus of smoothness. We then analyze a typical type ofGCN to demonstrate how the high-frequency energy of the output decays, anindicator of over-smoothing. This analysis provides theoretical insights intothe nature of over-smoothing within GCNs. Furthermore, we establish a lowerbound for the approximation of target functions by GCNs, which is governed bythe modulus of smoothness of these functions. This finding offers a newperspective on the approximation capabilities of GCNs. In our numericalexperiments, we analyze several widely applied GCNs and observe the phenomenonof energy decay. These observations corroborate our theoretical results onexponential decay order.</description><author>Guangrui Yang, Jianfei Li, Ming Li, Han Feng, Ding-Xuan Zhou</author><pubDate>Mon, 05 Aug 2024 15:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01281v2</guid></item><item><title>Artificial Intelligence for Public Health Surveillance in Africa: Applications and Opportunities</title><link>http://arxiv.org/abs/2408.02575v1</link><description>Artificial Intelligence (AI) is revolutionizing various fields, includingpublic health surveillance. In Africa, where health systems frequentlyencounter challenges such as limited resources, inadequate infrastructure,failed health information systems and a shortage of skilled healthprofessionals, AI offers a transformative opportunity. This paper investigatesthe applications of AI in public health surveillance across the continent,presenting successful case studies and examining the benefits, opportunities,and challenges of implementing AI technologies in African healthcare settings.Our paper highlights AI's potential to enhance disease monitoring and healthoutcomes, and support effective public health interventions. The findingspresented in the paper demonstrate that AI can significantly improve theaccuracy and timeliness of disease detection and prediction, optimize resourceallocation, and facilitate targeted public health strategies. Additionally, ourpaper identified key barriers to the widespread adoption of AI in Africanpublic health systems and proposed actionable recommendations to overcome thesechallenges.</description><author>Jean Marie Tshimula, Mitterrand Kalengayi, Dieumerci Makenga, Dorcas Lilonge, Marius Asumani, Déborah Madiya, Élie Nkuba Kalonji, Hugues Kanda, René Manassé Galekwa, Josias Kumbu, Hardy Mikese, Grace Tshimula, Jean Tshibangu Muabila, Christian N. Mayemba, D'Jeff K. Nkashama, Kalonji Kalala, Steve Ataky, Tighana Wenge Basele, Mbuyi Mukendi Didier, Selain K. Kasereka, Maximilien V. Dialufuma, Godwill Ilunga Wa Kumwita, Lionel Muyuku, Jean-Paul Kimpesa, Dominique Muteba, Aaron Aruna Abedi, Lambert Mukendi Ntobo, Gloria M. Bundutidi, Désiré Kulimba Mashinda, Emmanuel Kabengele Mpinga, Nathanaël M. Kasoro</author><pubDate>Mon, 05 Aug 2024 15:48:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02575v1</guid></item><item><title>Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs</title><link>http://arxiv.org/abs/2408.02571v1</link><description>The emoticons are symbolic representations that generally accompany thetextual content to visually enhance or summarize the true intention of awritten message. Although widely utilized in the realm of social media, thecore semantics of these emoticons have not been extensively explored based onmultiple modalities. Incorporating textual and visual information within asingle message develops an advanced way of conveying information. Hence, thisresearch aims to analyze the relationship among sentences, visuals, andemoticons. For an orderly exposition, this paper initially provides a detailedexamination of the various techniques for extracting multimodal features,emphasizing the pros and cons of each method. Through conducting acomprehensive examination of several multimodal algorithms, with specificemphasis on the fusion approaches, we have proposed a novel contrastivelearning based multimodal architecture. The proposed model employs the jointtraining of dual-branch encoder along with the contrastive learning toaccurately map text and images into a common latent space. Our key finding isthat by integrating the principle of contrastive learning with that of theother two branches yields superior results. The experimental resultsdemonstrate that our suggested methodology surpasses existing multimodalapproaches in terms of accuracy and robustness. The proposed model attained anaccuracy of 91% and an MCC-score of 90% while assessing emoticons using theMultimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidencethat deep features acquired by contrastive learning are more efficient,suggesting that the proposed fusion technique also possesses stronggeneralisation capabilities for recognising emoticons across several modes.</description><author>Ananya Pandey, Dinesh Kumar Vishwakarma</author><pubDate>Mon, 05 Aug 2024 15:45:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02571v1</guid></item><item><title>Cross-Modality Clustering-based Self-Labeling for Multimodal Data Classification</title><link>http://arxiv.org/abs/2408.02568v1</link><description>Technological advances facilitate the ability to acquire multimodal data,posing a challenge for recognition systems while also providing an opportunityto use the heterogeneous nature of the information to increase thegeneralization capability of models. An often overlooked issue is the cost ofthe labeling process, which is typically high due to the need for a significantinvestment in time and money associated with human experts. Existingsemi-supervised learning methods often focus on operating in the feature spacecreated by the fusion of available modalities, neglecting the potential forcross-utilizing complementary information available in each modality. Toaddress this problem, we propose Cross-Modality Clustering-based Self-Labeling(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instancesbelonging to each modality in the deep feature space and then propagates knownlabels within the resulting clusters. Next, information about the instances'class membership in each modality is exchanged based on the Euclidean distanceto ensure more accurate labeling. Experimental evaluation conducted on 20datasets derived from the MM-IMDb dataset indicates that cross-propagation oflabels between modalities -- especially when the number of pre-labeledinstances is small -- can allow for more reliable labeling and thus increasethe classification performance in each modality.</description><author>Paweł Zyblewski, Leandro L. Minku</author><pubDate>Mon, 05 Aug 2024 15:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02568v1</guid></item><item><title>Vision Learners Meet Web Image-Text Pairs</title><link>http://arxiv.org/abs/2301.07088v3</link><description>Many self-supervised learning methods are pre-trained on the well-curatedImageNet-1K dataset. In this work, given the excellent scalability of web data,we consider self-supervised pre-training on noisy web sourced image-text paireddata. First, we conduct a benchmark study of representative self-supervisedpre-training methods on large-scale web data in a like-for-like setting. Wecompare a range of methods, including single-modal ones that use maskedtraining objectives and multi-modal ones that use image-text constrastivetraining. We observe that existing multi-modal methods do not outperform theirsingle-modal counterparts on vision transfer learning tasks. We derive aninformation-theoretical view to explain these benchmark results, which providesinsight into how to design a novel vision learner. Inspired by this insight, wepresent a new visual representation pre-training method, MUlti-modalGenerator~(MUG), that learns from scalable web sourced image-text data. MUGachieves state-of-the-art transfer performance on a variety of tasks anddemonstrates promising scaling properties. Pre-trained models and code will bemade public upon acceptance.</description><author>Bingchen Zhao, Quan Cui, Hao Wu, Osamu Yoshie, Cheng Yang, Oisin Mac Aodha</author><pubDate>Mon, 05 Aug 2024 15:38:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07088v3</guid></item><item><title>HQOD: Harmonious Quantization for Object Detection</title><link>http://arxiv.org/abs/2408.02561v1</link><description>Task inharmony problem commonly occurs in modern object detectors, leading toinconsistent qualities between classification and regression tasks. Thepredicted boxes with high classification scores but poor localization positionsor low classification scores but accurate localization positions will worsenthe performance of detectors after Non-Maximum Suppression. Furthermore, whenobject detectors collaborate with Quantization-Aware Training (QAT), we observethat the task inharmony problem will be further exacerbated, which isconsidered one of the main causes of the performance degradation of quantizeddetectors. To tackle this issue, we propose the Harmonious Quantization forObject Detection (HQOD) framework, which consists of two components. Firstly,we propose a task-correlated loss to encourage detectors to focus on improvingsamples with lower task harmony quality during QAT. Secondly, a harmoniousIntersection over Union (IoU) loss is incorporated to balance the optimizationof the regression branch across different IoU levels. The proposed HQOD can beeasily integrated into different QAT algorithms and detectors. Remarkably, onthe MS COCO dataset, our 4-bit ATSS with ResNet-50 backbone achieves astate-of-the-art mAP of 39.6%, even surpassing the full-precision one.</description><author>Long Huang, Zhiwei Dong, Song-Lu Chen, Ruiyao Zhang, Shutong Ti, Feng Chen, Xu-Cheng Yin</author><pubDate>Mon, 05 Aug 2024 15:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02561v1</guid></item><item><title>Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information</title><link>http://arxiv.org/abs/2408.02559v1</link><description>Large language models (LLMs) have shown success in handling simple games withimperfect information and enabling multi-agent coordination, but their abilityto facilitate practical collaboration against other agents in complex,imperfect information environments, especially in a non-English environment,still needs to be explored. This study investigates the applicability ofknowledge acquired by open-source and API-based LLMs to sophisticatedtext-based games requiring agent collaboration under imperfect information,comparing their performance to established baselines using other types ofagents. We propose a Theory of Mind (ToM) planning technique that allows LLMagents to adapt their strategy against various adversaries using only gamerules, current state, and historical context as input. An external tool wasincorporated to mitigate the challenge of dynamic and extensive action spacesin this card game. Our results show that although a performance gap existsbetween current LLMs and state-of-the-art reinforcement learning (RL) models,LLMs demonstrate ToM capabilities in this game setting. It consistentlyimproves their performance against opposing agents, suggesting their ability tounderstand the actions of allies and adversaries and establish collaborationwith allies. To encourage further research and understanding, we have made ourcodebase openly accessible.</description><author>Yauwai Yim, Chunkit Chan, Tianyu Shi, Zheye Deng, Wei Fan, Tianshi Zheng, Yangqiu Song</author><pubDate>Mon, 05 Aug 2024 15:36:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02559v1</guid></item><item><title>Peer-induced Fairness: A Causal Approach to Reveal Algorithmic Unfairness in Credit Approval</title><link>http://arxiv.org/abs/2408.02558v1</link><description>This paper introduces a novel framework, "peer-induced fairness", toscientifically audit algorithmic fairness. It addresses a critical but oftenoverlooked issue: distinguishing between adverse outcomes due to algorithmicdiscrimination and those resulting from individuals' insufficient capabilities.By utilizing counterfactual fairness and advanced causal inference techniques,such as the Single World Intervention Graph, this model-agnostic approachevaluates fairness at the individual level through peer comparisons andhypothesis testing. It also tackles challenges like data scarcity andimbalance, offering a flexible, plug-and-play self-audit tool for stakeholdersand an external audit tool for regulators, while providing explainable feedbackfor those affected by unfavorable decisions.</description><author>Shiqi Fang, Zexun Chen, Jake Ansell</author><pubDate>Mon, 05 Aug 2024 15:35:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02558v1</guid></item><item><title>MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization</title><link>http://arxiv.org/abs/2408.02555v1</link><description>We introduce MeshAnything V2, an autoregressive transformer that generatesArtist-Created Meshes (AM) aligned to given shapes. It can be integrated withvarious 3D asset production pipelines to achieve high-quality, highlycontrollable AM generation. MeshAnything V2 surpasses previous methods in bothefficiency and performance using models of the same size. These improvementsare due to our newly proposed mesh tokenization method: Adjacent MeshTokenization (AMT). Different from previous methods that represent each facewith three vertices, AMT uses a single vertex whenever possible. Compared toprevious methods, AMT requires about half the token sequence length torepresent the same mesh in average. Furthermore, the token sequences from AMTare more compact and well-structured, fundamentally benefiting AM generation.Our extensive experiments show that AMT significantly improves the efficiencyand performance of AM generation. Project Page:https://buaacyw.github.io/meshanything-v2/</description><author>Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, Guosheng Lin</author><pubDate>Mon, 05 Aug 2024 15:33:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02555v1</guid></item><item><title>Process-constrained batch Bayesian approaches for yield optimization in multi-reactor systems</title><link>http://arxiv.org/abs/2408.02551v1</link><description>The optimization of yields in multi-reactor systems, which are advanced toolsin heterogeneous catalysis research, presents a significant challenge due tohierarchical technical constraints. To this respect, this work introduces anovel approach called process-constrained batch Bayesian optimization viaThompson sampling (pc-BO-TS) and its generalized hierarchical extension(hpc-BO-TS). This method, tailored for the efficiency demands in multi-reactorsystems, integrates experimental constraints and balances between explorationand exploitation in a sequential batch optimization strategy. It offers animprovement over other Bayesian optimization methods. The performance ofpc-BO-TS and hpc-BO-TS is validated in synthetic cases as well as in arealistic scenario based on data obtained from high-throughput experiments doneon a multi-reactor system available in the REALCAT platform. The proposedmethods often outperform other sequential Bayesian optimizations and existingprocess-constrained batch Bayesian optimization methods. This work proposes anovel approach to optimize the yield of a reaction in a multi-reactor system,marking a significant step forward in digital catalysis and generally inoptimization methods for chemical engineering.</description><author>Markus Grimm, Sébastien Paul, Pierre Chainais</author><pubDate>Mon, 05 Aug 2024 15:26:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02551v1</guid></item><item><title>The Role of Functional Muscle Networks in Improving Hand Gesture Perception for Human-Machine Interfaces</title><link>http://arxiv.org/abs/2408.02547v1</link><description>Developing accurate hand gesture perception models is critical for variousrobotic applications, enabling effective communication between humans andmachines and directly impacting neurorobotics and interactive robots. Recently,surface electromyography (sEMG) has been explored for its rich informationalcontext and accessibility when combined with advanced machine learningapproaches and wearable systems. The literature presents numerous approaches toboost performance while ensuring robustness for neurorobots using sEMG, oftenresulting in models requiring high processing power, large datasets, and lessscalable solutions. This paper addresses this challenge by proposing thedecoding of muscle synchronization rather than individual muscle activation. Westudy coherence-based functional muscle networks as the core of our perceptionmodel, proposing that functional synchronization between muscles and thegraph-based network of muscle connectivity encode contextual information aboutintended hand gestures. This can be decoded using shallow machine learningapproaches without the need for deep temporal networks. Our technique couldimpact myoelectric control of neurorobots by reducing computational burdens andenhancing efficiency. The approach is benchmarked on the Ninapro database,which contains 12 EMG signals from 40 subjects performing 17 hand gestures. Itachieves an accuracy of 85.1%, demonstrating improved performance compared toexisting methods while requiring much less computational power. The resultssupport the hypothesis that a coherence-based functional muscle network encodescritical information related to gesture execution, significantly enhancing handgesture perception with potential applications for neurorobotic systems andinteractive machines.</description><author>Costanza Armanini, Tuka Alhanai, Farah E. Shamout, S. Farokh Atashzar</author><pubDate>Mon, 05 Aug 2024 15:17:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02547v1</guid></item><item><title>RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2408.02545v1</link><description>Implementing Retrieval-Augmented Generation (RAG) systems is inherentlycomplex, requiring deep understanding of data, use cases, and intricate designdecisions. Additionally, evaluating these systems presents significantchallenges, necessitating assessment of both retrieval accuracy and generativequality through a multi-faceted approach. We introduce RAG Foundry, anopen-source framework for augmenting large language models for RAG use cases.RAG Foundry integrates data creation, training, inference and evaluation into asingle workflow, facilitating the creation of data-augmented datasets fortraining and evaluating large language models in RAG settings. This integrationenables rapid prototyping and experimentation with various RAG techniques,allowing users to easily generate datasets and train RAG models using internalor specialized knowledge sources. We demonstrate the framework effectiveness byaugmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAGconfigurations, showcasing consistent improvements across threeknowledge-intensive datasets. Code is released as open-source inhttps://github.com/IntelLabs/RAGFoundry.</description><author>Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat, Peter Izsak</author><pubDate>Mon, 05 Aug 2024 15:16:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02545v1</guid></item><item><title>Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions</title><link>http://arxiv.org/abs/2408.02544v1</link><description>This paper investigates the faithfulness of multimodal large language model(MLLM) agents in the graphical user interface (GUI) environment, aiming toaddress the research question of whether multimodal GUI agents can bedistracted by environmental context. A general setting is proposed where boththe user and the agent are benign, and the environment, while not malicious,contains unrelated content. A wide range of MLLMs are evaluated as GUI agentsusing our simulated dataset, following three working patterns with differentlevels of perception. Experimental results reveal that even the most powerfulmodels, whether generalist agents or specialist GUI agents, are susceptible todistractions. While recent studies predominantly focus on the helpfulness(i.e., action accuracy) of multimodal agents, our findings indicate that theseagents are prone to environmental distractions, resulting in unfaithfulbehaviors. Furthermore, we switch to the adversarial perspective and implementenvironment injection, demonstrating that such unfaithfulness can be exploited,leading to unexpected risks.</description><author>Xinbei Ma, Yiting Wang, Yao Yao, Tongxin Yuan, Aston Zhang, Zhuosheng Zhang, Hai Zhao</author><pubDate>Mon, 05 Aug 2024 15:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02544v1</guid></item><item><title>Transformer Layers as Painters</title><link>http://arxiv.org/abs/2407.09298v2</link><description>Despite their nearly universal adoption for large language models, theinternal workings of transformers are not well understood. We aim to betterunderstand the impact of removing or reorganizing information throughout thelayers of a pretrained transformer. Such an understanding could both yieldbetter usage of existing models as well as to make architectural improvementsto produce new variants. We present a series of empirical studies on frozenmodels that show that the lower and final layers of pretrained transformersdiffer from middle layers, but that middle layers have a surprising amount ofuniformity. We further show that some classes of problems have robustness toskipping layers, running the layers in an order different from how they weretrained, or running the layers in parallel. Our observations suggest that evenfrozen pretrained models may gracefully trade accuracy for latency by skippinglayers or running layers in parallel.</description><author>Qi Sun, Marc Pickett, Aakash Kumar Nain, Llion Jones</author><pubDate>Mon, 05 Aug 2024 15:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09298v2</guid></item><item><title>What Do Language Models Learn in Context? The Structured Task Hypothesis</title><link>http://arxiv.org/abs/2406.04216v3</link><description>Large language models (LLMs) exhibit an intriguing ability to learn a noveltask from in-context examples presented in a demonstration, termed in-contextlearning (ICL). Understandably, a swath of research has been dedicated touncovering the theories underpinning ICL. One popular hypothesis explains ICLby task selection. LLMs identify the task based on the demonstration andgeneralize it to the prompt. Another popular hypothesis is that ICL is a formof meta-learning, i.e., the models learn a learning algorithm at pre-trainingtime and apply it to the demonstration. Finally, a third hypothesis argues thatLLMs use the demonstration to select a composition of tasks learned duringpre-training to perform ICL. In this paper, we empirically explore these threehypotheses that explain LLMs' ability to learn in context with a suite ofexperiments derived from common text classification tasks. We invalidate thefirst two hypotheses with counterexamples and provide evidence in support ofthe last hypothesis. Our results suggest an LLM could learn a novel task incontext via composing tasks learned during pre-training.</description><author>Jiaoda Li, Yifan Hou, Mrinmaya Sachan, Ryan Cotterell</author><pubDate>Mon, 05 Aug 2024 15:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04216v3</guid></item><item><title>Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms</title><link>http://arxiv.org/abs/2310.02812v2</link><description>Manufacturing is gathering extensive amounts of diverse data, thanks to thegrowing number of sensors and rapid advances in sensing technologies. Among thevarious data types available in SMS settings, time-series data plays a pivotalrole. Hence, TSC emerges is crucial in this domain. The objective of this studyis to fill this gap by providing a rigorous experimental evaluation of the SoTAML and DL algorithms for TSC tasks in manufacturing and industrial settings. Wefirst explored and compiled a comprehensive list of more than 92 SoTAalgorithms from both TSC and manufacturing literature. Following, we selectedthe 36 most representative algorithms from this list. To evaluate theirperformance across various manufacturing classification tasks, we curated a setof 22 manufacturing datasets, representative of different characteristics thatcover diverse manufacturing problems. Subsequently, we implemented andevaluated the algorithms on the manufacturing benchmark datasets, and analyzedthe results for each dataset. Based on the results, ResNet, DrCIF,InceptionTime, and ARSENAL are the top-performing algorithms, boasting anaverage accuracy of over 96.6% across all 22 manufacturing TSC datasets. Thesefindings underscore the robustness, efficiency, scalability, and effectivenessof convolutional kernels in capturing temporal features in time-series data, asthree out of the top four performing algorithms leverage these kernels forfeature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserverecognition for their effectiveness in capturing features within time-seriesdata using RNN-based structures.</description><author>Mojtaba A. Farahani, M. R. McCormick, Ramy Harik, Thorsten Wuest</author><pubDate>Mon, 05 Aug 2024 15:06:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02812v2</guid></item><item><title>LMEMs for post-hoc analysis of HPO Benchmarking</title><link>http://arxiv.org/abs/2408.02533v1</link><description>The importance of tuning hyperparameters in Machine Learning (ML) and DeepLearning (DL) is established through empirical research and applications,evident from the increase in new hyperparameter optimization (HPO) algorithmsand benchmarks steadily added by the community. However, current benchmarkingpractices using averaged performance across many datasets may obscure keydifferences between HPO methods, especially for pairwise comparisons. In thiswork, we apply Linear Mixed-Effect Models-based (LMEMs) significance testingfor post-hoc analysis of HPO benchmarking runs. LMEMs allow flexible andexpressive modeling on the entire experiment data, including information suchas benchmark meta-features, offering deeper insights than current analysispractices. We demonstrate this through a case study on the PriorBand paper'sexperiment data to find insights not reported in the original work.</description><author>Anton Geburek, Neeratyoy Mallik, Danny Stoll, Xavier Bouthillier, Frank Hutter</author><pubDate>Mon, 05 Aug 2024 15:03:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02533v1</guid></item><item><title>High-arity PAC learning via exchangeability</title><link>http://arxiv.org/abs/2402.14294v2</link><description>We develop a theory of high-arity PAC learning, which is statistical learningin the presence of "structured correlation". In this theory, hypotheses areeither graphs, hypergraphs or, more generally, structures in finite relationallanguages, and i.i.d. sampling is replaced by sampling an induced substructure,producing an exchangeable distribution. Our main theorems establish ahigh-arity (agnostic) version of the fundamental theorem of statisticallearning.</description><author>Leonardo N. Coregliano, Maryanthe Malliaris</author><pubDate>Mon, 05 Aug 2024 14:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14294v2</guid></item><item><title>Counterfactual Shapley Values for Explaining Reinforcement Learning</title><link>http://arxiv.org/abs/2408.02529v1</link><description>This paper introduces a novel approach Counterfactual Shapley Values (CSV),which enhances explainability in reinforcement learning (RL) by integratingcounterfactual analysis with Shapley Values. The approach aims to quantify andcompare the contributions of different state dimensions to various actionchoices. To more accurately analyze these impacts, we introduce newcharacteristic value functions, the ``Counterfactual Difference CharacteristicValue" and the ``Average Counterfactual Difference Characteristic Value." Thesefunctions help calculate the Shapley values to evaluate the differences incontributions between optimal and non-optimal actions. Experiments acrossseveral RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate theeffectiveness of the CSV method. The results show that this method not onlyimproves transparency in complex RL systems but also quantifies the differencesacross various decisions.</description><author>Yiwei Shi, Qi Zhang, Kevin McAreavey, Weiru Liu</author><pubDate>Mon, 05 Aug 2024 14:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02529v1</guid></item><item><title>Doubly-Dynamic ISAC Precoding for Vehicular Networks: A Constrained Deep Reinforcement Learning (CDRL) Approach</title><link>http://arxiv.org/abs/2405.14347v2</link><description>Integrated sensing and communication (ISAC) technology is essential forenabling the vehicular networks. However, the communication channel in thisscenario exhibits time-varying characteristics, and the potential targets maymove rapidly, creating a doubly-dynamic phenomenon. This nature poses achallenge for real-time precoder design. While optimization-based solutions arewidely researched, they are complex and heavily rely on perfect priorinformation, which is impractical in double dynamics. To address thischallenge, we propose using constrained deep reinforcement learning (CDRL) tofacilitate dynamic updates to the ISAC precoder design. Additionally, theprimal dual-deep deterministic policy gradient (PD-DDPG) and Wolpertingerarchitecture are tailored to efficiently train the algorithm under complexconstraints and variable numbers of users. The proposed scheme not only adaptsto the dynamics based on observations but also leverages environmentalinformation to enhance performance and reduce complexity. Its superiority overexisting candidates has been validated through experiments.</description><author>Zonghui Yang, Shijian Gao, Xiang Cheng</author><pubDate>Mon, 05 Aug 2024 14:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14347v2</guid></item><item><title>Single-tap Latency Reduction with Single- or Double- tap Prediction</title><link>http://arxiv.org/abs/2408.02525v1</link><description>Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops(touchpad), and single and double taps are the most basic and common operationson them. The detection of single or double taps causes the single-tap latencyproblem, which creates a bottleneck in terms of the sensitivity of touchinputs. To reduce the single-tap latency, we propose a novelmachine-learning-based tap prediction method called PredicTaps. Our methodpredicts whether a detected tap is a single tap or the first contact of adouble tap without having to wait for the hundreds of millisecondsconventionally required. We present three evaluations and one user evaluationthat demonstrate its broad applicability and usability for various tapsituations on two form factors (touchpad and smartphone). The results showedPredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptopsand to 17.6 ms on smartphones without reducing usability.</description><author>Naoto Nishida, Kaori Ikematsu, Junichi Sato, Shota Yamanaka, Kota Tsubouchi</author><pubDate>Mon, 05 Aug 2024 14:46:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02525v1</guid></item><item><title>OneLove beyond the field -- A few-shot pipeline for topic and sentiment analysis during the FIFA World Cup in Qatar</title><link>http://arxiv.org/abs/2408.02520v1</link><description>The FIFA World Cup in Qatar was discussed extensively in the news and onsocial media. Due to news reports with allegations of human rights violations,there were calls to boycott it. Wearing a OneLove armband was part of a plannedprotest activity. Controversy around the armband arose when FIFA threatened tosanction captains who wear it. To understand what topics Twitter users Tweetedabout and what the opinion of German Twitter users was towards the OneLovearmband, we performed an analysis of German Tweets published during the WorldCup using in-context learning with LLMs. We validated the labels on humanannotations. We found that Twitter users initially discussed the armband'simpact, LGBT rights, and politics; after the ban, the conversation shiftedtowards politics in sports in general, accompanied by a subtle shift insentiment towards neutrality. Our evaluation serves as a framework for futureresearch to explore the impact of sports activism and evolving publicsentiment. This is especially useful in settings where labeling datasets forspecific opinions is unfeasible, such as when events are unfolding.</description><author>Christoph Rauchegger, Sonja Mei Wang, Pieter Delobelle</author><pubDate>Mon, 05 Aug 2024 14:40:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02520v1</guid></item><item><title>Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem Compatibility Estimation</title><link>http://arxiv.org/abs/2408.02514v1</link><description>This paper explores the automated process of determining stem compatibilityby identifying audio recordings of single instruments that blend well with agiven musical context. To tackle this challenge, we present Stem-JEPA, a novelJoint-Embedding Predictive Architecture (JEPA) trained on a multi-track datasetusing a self-supervised learning approach. Our model comprises two networks: an encoder and a predictor, which arejointly trained to predict the embeddings of compatible stems from theembeddings of a given context, typically a mix of several instruments. Traininga model in this manner allows its use in estimating stem compatibility -retrieving, aligning, or generating a stem to match a given mix - or fordownstream tasks such as genre or key estimation, as the training paradigmrequires the model to learn information related to timbre, harmony, and rhythm. We evaluate our model's performance on a retrieval task on the MUSDB18dataset, testing its ability to find the missing stem from a mix and through asubjective user study. We also show that the learned embeddings capturetemporal alignment information and, finally, evaluate the representationslearned by our model on several downstream tasks, highlighting that theyeffectively capture meaningful musical features.</description><author>Alain Riou, Stefan Lattner, Gaëtan Hadjeres, Michael Anslow, Geoffroy Peeters</author><pubDate>Mon, 05 Aug 2024 14:34:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02514v1</guid></item><item><title>Practical Attacks against Black-box Code Completion Engines</title><link>http://arxiv.org/abs/2408.02509v1</link><description>Modern code completion engines, powered by large language models, havedemonstrated impressive capabilities to generate functionally correct codebased on surrounding context. As these tools are extensively used by millionsof developers, it is crucial to investigate their security implications. Inthis work, we present INSEC, a novel attack that directs code completionengines towards generating vulnerable code. In line with most commercialcompletion engines, such as GitHub Copilot, INSEC assumes only black-box queryaccess to the targeted engine, without requiring any knowledge of the engine'sinternals. Our attack works by inserting a malicious attack string as a shortcomment in the completion input. To derive the attack string, we design aseries of specialized initialization schemes and an optimization procedure forfurther refinement. We demonstrate the strength of INSEC not only onstate-of-the-art open-source models but also on black-box commercial servicessuch as the OpenAI API and GitHub Copilot. On a comprehensive set ofsecurity-critical test cases covering 16 CWEs across 5 programming languages,INSEC significantly increases the likelihood of the considered completionengines in generating unsafe code by &gt;50% in absolute, while maintaining theability in producing functionally correct code. At the same time, our attackhas low resource requirements, and can be developed for a cost of well underten USD on commodity hardware.</description><author>Slobodan Jenko, Jingxuan He, Niels Mündler, Mark Vero, Martin Vechev</author><pubDate>Mon, 05 Aug 2024 14:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02509v1</guid></item><item><title>Estimating Pore Location of PBF-LB/M Processes with Segmentation Models</title><link>http://arxiv.org/abs/2408.02507v1</link><description>Reliably manufacturing defect free products is still an open challenge forLaser Powder Bed Fusion processes. Particularly, pores that occur frequentlyhave a negative impact on mechanical properties like fatigue performance.Therefore, an accurate localisation of pores is mandatory for qualityassurance, but requires time-consuming post-processing steps like computertomography scans. Although existing solutions using in-situ monitoring data candetect pore occurrence within a layer, they are limited in their localisationprecision. Therefore, we propose a pore localisation approach that estimatestheir position within a single layer using a Gaussian kernel densityestimation. This allows segmentation models to learn the correlation betweenin-situ monitoring data and the derived probability distribution of poreoccurrence. Within our experiments, we compare the prediction performance ofdifferent segmentation models depending on machine parameter configuration andgeometry features. From our results, we conclude that our approach allows aprecise localisation of pores that requires minimal data preprocessing. Ourresearch extends the literature by providing a foundation for more precise poredetection systems.</description><author>Hans Aoyang Zhou, Jan Theunissen, Marco Kemmerling, Anas Abdelrazeq, Johannes Henrich Schleifenbaum, Robert H. Schmitt</author><pubDate>Mon, 05 Aug 2024 14:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02507v1</guid></item><item><title>UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model</title><link>http://arxiv.org/abs/2408.02503v1</link><description>Significant advancements has recently been achieved in the field ofmulti-modal large language models (MLLMs), demonstrating their remarkablecapabilities in understanding and reasoning across diverse tasks. However,these models are often trained for specific tasks and rely on task-specificinput-output formats, limiting their applicability to a broader range of tasks.This raises a fundamental question: Can we develop a unified approach torepresent and handle different multi-modal tasks to maximize thegeneralizability of MLLMs? In this paper, we propose UnifiedMLLM, acomprehensive model designed to represent various tasks using a unifiedrepresentation. Our model exhibits strong capabilities in comprehending theimplicit intent of user instructions and preforming reasoning. In addition togenerating textual responses, our model also outputs task tokens and groundingtokens, serving as indicators of task types and task granularity. These outputsare subsequently routed through the task router and directed to specific expertmodels for task completion. To train our model, we construct a task-specificdataset and an 100k multi-task dataset encompassing complex scenarios.Employing a three-stage training strategy, we equip our model with robustreasoning and task processing capabilities while preserving its generalizationcapacity and knowledge reservoir. Extensive experiments showcase the impressiveperformance of our unified representation approach across various tasks,surpassing existing methodologies. Furthermore, our approach exhibitsexceptional scalability and generality. Our code, model, and dataset will beavailable at \url{https://github.com/lzw-lzw/UnifiedMLLM}.</description><author>Zhaowei Li, Wei Wang, YiQing Cai, Xu Qi, Pengyu Wang, Dong Zhang, Hang Song, Botian Jiang, Zhida Huang, Tao Wang</author><pubDate>Mon, 05 Aug 2024 14:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02503v1</guid></item><item><title>Automatic rating of incomplete hippocampal inversions evaluated across multiple cohorts</title><link>http://arxiv.org/abs/2408.02496v1</link><description>Incomplete Hippocampal Inversion (IHI), sometimes called hippocampalmalrotation, is an atypical anatomical pattern of the hippocampus found inabout 20% of the general population. IHI can be visually assessed on coronalslices of T1 weighted MR images, using a composite score that combines fouranatomical criteria. IHI has been associated with several brain disorders(epilepsy, schizophrenia). However, these studies were based on small samples.Furthermore, the factors (genetic or environmental) that contribute to thegenesis of IHI are largely unknown. Large-scale studies are thus needed tofurther understand IHI and their potential relationships to neurological andpsychiatric disorders. However, visual evaluation is long and tedious,justifying the need for an automatic method. In this paper, we propose, for thefirst time, to automatically rate IHI. We proceed by predicting four anatomicalcriteria, which are then summed up to form the IHI score, providing theadvantage of an interpretable score. We provided an extensive experimentalinvestigation of different machine learning methods and training strategies. Weperformed automatic rating using a variety of deep learning models (conv5-FC3,ResNet and SECNN) as well as a ridge regression. We studied the generalizationof our models using different cohorts and performed multi-cohort learning. Werelied on a large population of 2,008 participants from the IMAGEN study, 993and 403 participants from the QTIM/QTAB studies as well as 985 subjects fromthe UKBiobank. We showed that deep learning models outperformed a ridgeregression. We demonstrated that the performances of the conv5-FC3 network wereat least as good as more complex networks while maintaining a low complexityand computation time. We showed that training on a single cohort may lack invariability while training on several cohorts improves generalization.</description><author>Lisa Hemforth, Baptiste Couvy-Duchesne, Kevin De Matos, Camille Brianceau, Matthieu Joulot, Tobias Banaschewski, Arun L. W. Bokde, Sylvane Desrivières, Herta Flor, Antoine Grigis, Hugh Garavan, Penny Gowland, Andreas Heinz, Rüdiger Brühl, Jean-Luc Martinot, Marie-Laure Paillère Martinot, Eric Artiges, Dimitri Papadopoulos, Herve Lemaitre, Tomas Paus, Luise Poustka, Sarah Hohmann, Nathalie Holz, Juliane H. Fröhner, Michael N. Smolka, Nilakshi Vaidya, Henrik Walter, Robert Whelan, Gunter Schumann, Christian Büchel, JB Poline, Bernd Itterman, Vincent Frouin, Alexandre Martin, IMAGEN study group, Claire Cury, Olivier Colliot</author><pubDate>Mon, 05 Aug 2024 14:19:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02496v1</guid></item><item><title>HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions</title><link>http://arxiv.org/abs/2408.02494v1</link><description>Traditional deep learning models rely on methods such as softmaxcross-entropy and ArcFace loss for tasks like classification and facerecognition. These methods mainly explore angular features in a hypersphericalspace, often resulting in entangled inter-class features due to dense angulardata across many classes. In this paper, a new field of feature exploration isproposed known as HyperSpaceX which enhances class discrimination by exploringboth angular and radial dimensions in multi-hyperspherical spaces, facilitatedby a novel DistArc loss. The proposed DistArc loss encompasses three featurearrangement components: two angular and one radial, enforcing intra-classbinding and inter-class separation in multi-radial arrangement, improvingfeature discriminability. Evaluation of HyperSpaceX framework for the novelrepresentation utilizes a proposed predictive measure that accounts for bothangular and radial elements, providing a more comprehensive assessment of modelaccuracy beyond standard metrics. Experiments across seven objectclassification and six face recognition datasets demonstrate state-of-the-art(SoTA) results obtained from HyperSpaceX, achieving up to a 20% performanceimprovement on large-scale object datasets in lower dimensions and up to 6%gain in higher dimensions.</description><author>Chiranjeev Chiranjeev, Muskan Dosi, Kartik Thakral, Mayank Vatsa, Richa Singh</author><pubDate>Mon, 05 Aug 2024 14:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02494v1</guid></item><item><title>Full error analysis of policy gradient learning algorithms for exploratory linear quadratic mean-field control problem in continuous time with common noise</title><link>http://arxiv.org/abs/2408.02489v1</link><description>We consider reinforcement learning (RL) methods for finding optimal policiesin linear quadratic (LQ) mean field control (MFC) problems over an infinitehorizon in continuous time, with common noise and entropy regularization. Westudy policy gradient (PG) learning and first demonstrate convergence in amodel-based setting by establishing a suitable gradient dominationcondition.Next, our main contribution is a comprehensive error analysis, wherewe prove the global linear convergence and sample complexity of the PGalgorithm with two-point gradient estimates in a model-free setting withunknown parameters. In this setting, the parameterized optimal policies arelearned from samples of the states and population distribution.Finally, weprovide numerical evidence supporting the convergence of our implementedalgorithms.</description><author>Noufel Frikha, Huyên Pham, Xuanye Song</author><pubDate>Mon, 05 Aug 2024 14:11:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02489v1</guid></item><item><title>A First Look at License Compliance Capability of LLMs in Code Generation</title><link>http://arxiv.org/abs/2408.02487v1</link><description>Recent advances in Large Language Models (LLMs) have revolutionized codegeneration, leading to widespread adoption of AI coding tools by developers.However, LLMs can generate license-protected code without providing thenecessary license information, leading to potential intellectual propertyviolations during software production. This paper addresses the critical, yetunderexplored, issue of license compliance in LLM-generated code byestablishing a benchmark to evaluate the ability of LLMs to provide accuratelicense information for their generated code. To establish this benchmark, weconduct an empirical study to identify a reasonable standard for "strikingsimilarity" that excludes the possibility of independent creation, indicating acopy relationship between the LLM output and certain open-source code. Based onthis standard, we propose an evaluation benchmark LiCoEval, to evaluate thelicense compliance capabilities of LLMs. Using LiCoEval, we evaluate 14 popularLLMs, finding that even top-performing LLMs produce a non-negligible proportion(0.88% to 2.01%) of code strikingly similar to existing open-sourceimplementations. Notably, most LLMs fail to provide accurate licenseinformation, particularly for code under copyleft licenses. These findingsunderscore the urgent need to enhance LLM compliance capabilities in codegeneration tasks. Our study provides a foundation for future research anddevelopment to improve license compliance in AI-assisted software development,contributing to both the protection of open-source software copyrights and themitigation of legal risks for LLM users.</description><author>Weiwei Xu, Kai Gao, Hao He, Minghui Zhou</author><pubDate>Mon, 05 Aug 2024 14:09:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02487v1</guid></item><item><title>Annotation Cost-Efficient Active Learning for Deep Metric Learning Driven Remote Sensing Image Retrieval</title><link>http://arxiv.org/abs/2406.10107v2</link><description>Deep metric learning (DML) has shown to be effective for content-based imageretrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR rely on ahigh number of annotated images to accurately learn model parameters of deepneural networks (DNNs). However, gathering such data is time-consuming andcostly. To address this, we propose an annotation cost-efficient activelearning (ANNEAL) method tailored to DML-driven CBIR in RS. ANNEAL aims tocreate a small but informative training set made up of similar and dissimilarimage pairs to be utilized for accurately learning a metric space. Theinformativeness of image pairs is evaluated by combining uncertainty anddiversity criteria. To assess the uncertainty of image pairs, we introduce twoalgorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binaryclassifier guided uncertainty estimation (BCGUE). MGUE algorithm automaticallyestimates a threshold value that acts as a boundary between similar anddissimilar image pairs based on the distances in the metric space. The closerthe similarity between image pairs is to the estimated threshold value thehigher their uncertainty. BCGUE algorithm estimates the uncertainty of theimage pairs based on the confidence of the classifier in assigning correctsimilarity labels. The diversity criterion is assessed through aclustering-based strategy. ANNEAL combines either MGUE or BCGUE algorithm withthe clustering-based strategy to select the most informative image pairs, whichare then labelled by expert annotators as similar or dissimilar. This way ofannotating images significantly reduces the annotation cost compared toannotating images with land-use land-cover class labels. Experimental resultson two RS benchmark datasets demonstrate the effectiveness of our method. Thecode of this work is publicly available at\url{https://git.tu-berlin.de/rsim/anneal_tgrs}.</description><author>Genc Hoxha, Gencer Sumbul, Julia Henkel, Lars Möllenbrok, Begüm Demir</author><pubDate>Mon, 05 Aug 2024 14:06:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10107v2</guid></item><item><title>Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection</title><link>http://arxiv.org/abs/2408.02484v1</link><description>Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontiertopic due to its capability to detect HOIs beyond a predefined set ofcategories. This task entails not only identifying the interactiveness ofhuman-object pairs and localizing them but also recognizing both seen andunseen interaction categories. In this paper, we introduce a novel frameworkfor zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP.This approach enhances the generalization of large foundation models, such asCLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learningmethods, we propose learning decoupled vision and language prompts forinteractiveness-aware visual feature extraction and generalizable interactionclassification, respectively. Specifically, we integrate prior knowledge ofdifferent granularity into conditional vision prompts, including aninput-conditioned instance prior and a global spatial pattern prior. The formerencourages the image encoder to treat instances belonging to seen orpotentially unseen HOI concepts equally while the latter providesrepresentative plausible spatial configuration of the human and object underinteraction. Besides, we employ language-aware prompt learning with aconsistency constraint to preserve the knowledge of the large foundation modelto enable better generalization in the text branch. Extensive experimentsdemonstrate the efficacy of our detector with conditional multi-modal prompts,outperforming previous state-of-the-art on unseen classes of various zero-shotsettings. The code and models are available at\url{https://github.com/ltttpku/CMMP}.</description><author>Ting Lei, Shaofeng Yin, Yuxin Peng, Yang Liu</author><pubDate>Mon, 05 Aug 2024 14:05:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02484v1</guid></item><item><title>On the influence of dependent features in classification problems: a game-theoretic perspective</title><link>http://arxiv.org/abs/2408.02481v1</link><description>This paper deals with a new measure of the influence of each feature on theresponse variable in classification problems, accounting for potentialdependencies among certain feature subsets. Within this framework, we considera sample of individuals characterized by specific features, each featureencompassing a finite range of values, and classified based on a binaryresponse variable. This measure turns out to be an influence measure exploredin existing literature and related to cooperative game theory. We provide anaxiomatic characterization of our proposed influence measure by tailoringproperties from the cooperative game theory to our specific context.Furthermore, we demonstrate that our influence measure becomes a generalcharacterization of the well-known Banzhaf-Owen value for games with a prioriunions, from the perspective of classification problems. The definitions andresults presented herein are illustrated through numerical examples and variousapplications, offering practical insights into our methodologies.</description><author>Laura Davila-Pena, Alejandro Saavedra-Nieves, Balbina Casas-Méndez</author><pubDate>Mon, 05 Aug 2024 14:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02481v1</guid></item><item><title>Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples</title><link>http://arxiv.org/abs/2403.02875v2</link><description>Current multimodal models leveraging contrastive learning often facelimitations in developing fine-grained conceptual understanding. This is due torandom negative samples during pretraining, causing almost exclusively verydissimilar concepts to be compared in the loss function. Consequently, themodels struggle with fine-grained semantic differences. To address thisproblem, we introduce a novel pretraining method incorporating synthetic hardnegative text examples. The hard negatives permute terms corresponding tovisual concepts, leading to a more fine-grained visual and textual conceptalignment. Further, we introduce InpaintCOCO, a new challenging dataset forassessing the fine-grained alignment of colors, objects, and sizes invision-language models. We created the dataset using generative inpainting fromCOCO images by changing the visual concepts so that the images no longer matchtheir original captions. Our results show significant improvements infine-grained concept understanding across a wide range of vision-languagedatasets, including our InpaintCOCO dataset.</description><author>Philipp J. Rösch, Norbert Oswald, Michaela Geierhos, Jindřich Libovický</author><pubDate>Mon, 05 Aug 2024 14:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02875v2</guid></item><item><title>From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future</title><link>http://arxiv.org/abs/2408.02479v1</link><description>With the rise of large language models (LLMs), researchers are increasinglyexploring their applications in var ious vertical domains, such as softwareengineering. LLMs have achieved remarkable success in areas including codegeneration and vulnerability detection. However, they also exhibit numerouslimitations and shortcomings. LLM-based agents, a novel tech nology with thepotential for Artificial General Intelligence (AGI), combine LLMs as the corefor decision-making and action-taking, addressing some of the inherentlimitations of LLMs such as lack of autonomy and self-improvement. Despitenumerous studies and surveys exploring the possibility of using LLMs insoftware engineering, it lacks a clear distinction between LLMs and LLM basedagents. It is still in its early stage for a unified standard and benchmarkingto qualify an LLM solution as an LLM-based agent in its domain. In this survey,we broadly investigate the current practice and solutions for LLMs andLLM-based agents for software engineering. In particular we summarise six keytopics: requirement engineering, code generation, autonomous decision-making,software design, test generation, and software maintenance. We review anddifferentiate the work of LLMs and LLM-based agents from these six topics,examining their differences and similarities in tasks, benchmarks, andevaluation metrics. Finally, we discuss the models and benchmarks used,providing a comprehensive analysis of their applications and effectiveness insoftware engineering. We anticipate this work will shed some lights on pushingthe boundaries of LLM-based agents in software engineering for future research.</description><author>Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen</author><pubDate>Mon, 05 Aug 2024 14:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02479v1</guid></item><item><title>UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with Ultrasound Reflection Direction Parameterization</title><link>http://arxiv.org/abs/2408.00860v2</link><description>Three-dimensional ultrasound imaging is a critical technology widely used inmedical diagnostics. However, traditional 3D ultrasound imaging methods havelimitations such as fixed resolution, low storage efficiency, and insufficientcontextual connectivity, leading to poor performance in handling complexartifacts and reflection characteristics. Recently, techniques based on NeRF(Neural Radiance Fields) have made significant progress in view synthesis and3D reconstruction, but there remains a research gap in high-quality ultrasoundimaging. To address these issues, we propose a new model, UlRe-NeRF, whichcombines implicit neural networks and explicit ultrasound volume rendering intoan ultrasound neural rendering architecture. This model incorporates reflectiondirection parameterization and harmonic encoding, using a directional MLPmodule to generate view-dependent high-frequency reflection intensityestimates, and a spatial MLP module to produce the medium's physical propertyparameters. These parameters are used in the volume rendering process toaccurately reproduce the propagation and reflection behavior of ultrasoundwaves in the medium. Experimental results demonstrate that the UlRe-NeRF modelsignificantly enhances the realism and accuracy of high-fidelity ultrasoundimage reconstruction, especially in handling complex medium structures.</description><author>Ziwen Guo, Zi Fang, Zhuang Fu</author><pubDate>Mon, 05 Aug 2024 14:00:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00860v2</guid></item><item><title>Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture and Automated Deployment Flow</title><link>http://arxiv.org/abs/2408.02473v1</link><description>One of the challenges for Tiny Machine Learning (tinyML) is keeping up withthe evolution of Machine Learning models from Convolutional Neural Networks toTransformers. We address this by leveraging a heterogeneous architecturaltemplate coupling RISC-V processors with hardwired accelerators supported by anautomated deployment flow. We demonstrate an Attention-based model in a tinyMLpower envelope with an octa-core cluster coupled with an accelerator forquantized Attention. Our deployment flow enables an end-to-end 8-bitMobileBERT, achieving leading-edge energy efficiency and throughput of 2960GOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOItechnology).</description><author>Philip Wiese, Gamze İslamoğlu, Moritz Scherer, Luka Macan, Victor J. B. Jung, Alessio Burrello, Francesco Conti, Luca Benini</author><pubDate>Mon, 05 Aug 2024 13:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02473v1</guid></item><item><title>Fairness and Bias Mitigation in Computer Vision: A Survey</title><link>http://arxiv.org/abs/2408.02464v1</link><description>Computer vision systems have witnessed rapid progress over the past twodecades due to multiple advances in the field. As these systems areincreasingly being deployed in high-stakes real-world applications, there is adire need to ensure that they do not propagate or amplify any discriminatorytendencies in historical or human-curated data or inadvertently learn biasesfrom spurious correlations. This paper presents a comprehensive survey onfairness that summarizes and sheds light on ongoing trends and successes in thecontext of computer vision. The topics we discuss include 1) The origin andtechnical definitions of fairness drawn from the wider fair machine learningliterature and adjacent disciplines. 2) Work that sought to discover andanalyze biases in computer vision systems. 3) A summary of methods proposed tomitigate bias in computer vision systems in recent years. 4) A comprehensivesummary of resources and datasets produced by researchers to measure, analyze,and mitigate bias and enhance fairness. 5) Discussion of the field's success,continuing trends in the context of multimodal foundation and generativemodels, and gaps that still need to be addressed. The presentedcharacterization should help researchers understand the importance ofidentifying and mitigating bias in computer vision and the state of the fieldand identify potential directions for future research.</description><author>Sepehr Dehdashtian, Ruozhen He, Yi Li, Guha Balakrishnan, Nuno Vasconcelos, Vicente Ordonez, Vishnu Naresh Boddeti</author><pubDate>Mon, 05 Aug 2024 13:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02464v1</guid></item><item><title>An investigation into the causes of race bias in AI-based cine CMR segmentation</title><link>http://arxiv.org/abs/2408.02462v1</link><description>Artificial intelligence (AI) methods are being used increasingly for theautomated segmentation of cine cardiac magnetic resonance (CMR) imaging.However, these methods have been shown to be subject to race bias, i.e. theyexhibit different levels of performance for different races depending on the(im)balance of the data used to train the AI model. In this paper weinvestigate the source of this bias, seeking to understand its root cause(s) sothat it can be effectively mitigated. We perform a series of classification andsegmentation experiments on short-axis cine CMR images acquired from Black andWhite subjects from the UK Biobank and apply AI interpretability methods tounderstand the results. In the classification experiments, we found that racecan be predicted with high accuracy from the images alone, but less accuratelyfrom ground truth segmentations, suggesting that the distributional shiftbetween races, which is often the cause of AI bias, is mostly image-basedrather than segmentation-based. The interpretability methods showed that mostattention in the classification models was focused on non-heart regions, suchas subcutaneous fat. Cropping the images tightly around the heart reducedclassification accuracy to around chance level. Similarly, race can bepredicted from the latent representations of a biased segmentation model,suggesting that race information is encoded in the model. Cropping imagestightly around the heart reduced but did not eliminate segmentation bias. Wealso investigate the influence of possible confounders on the bias observed.</description><author>Tiarna Lee, Esther Puyol-Anton, Bram Ruijsink, Sebastien Roujol, Theodore Barfoot, Shaheim Ogbomo-Harmitt, Miaojing Shi, Andrew P. King</author><pubDate>Mon, 05 Aug 2024 13:40:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02462v1</guid></item><item><title>SocialBench: Sociality Evaluation of Role-Playing Conversational Agents</title><link>http://arxiv.org/abs/2403.13679v4</link><description>Large language models (LLMs) have advanced the development of various AIconversational agents, including role-playing conversational agents that mimicdiverse characters and human behaviors. While prior research has predominantlyfocused on enhancing the conversational capability, role-specific knowledge,and stylistic attributes of these agents, there has been a noticeable gap inassessing their social intelligence. In this paper, we introduce SocialBench,the first benchmark designed to systematically evaluate the sociality ofrole-playing conversational agents at both individual and group levels ofsocial interactions. The benchmark is constructed from a variety of sources andcovers a wide range of 500 characters and over 6,000 question prompts and30,800 multi-turn role-playing utterances. We conduct comprehensive evaluationson this benchmark using mainstream open-source and closed-source LLMs. We findthat agents excelling in individual level does not imply their proficiency ingroup level. Moreover, the behavior of individuals may drift as a result of theinfluence exerted by other agents within the group. Experimental results onSocialBench confirm its significance as a testbed for assessing the socialinteraction of role-playing conversational agents. The benchmark is publiclyaccessible at https://github.com/X-PLUG/SocialBench.</description><author>Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, Jingren Zhou</author><pubDate>Mon, 05 Aug 2024 13:32:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13679v4</guid></item><item><title>Enhancing Heterogeneous Knowledge Graph Completion with a Novel GAT-based Approach</title><link>http://arxiv.org/abs/2408.02456v1</link><description>Knowledge graphs (KGs) play a vital role in enhancing search results andrecommendation systems. With the rapid increase in the size of the KGs, theyare becoming inaccuracy and incomplete. This problem can be solved by theknowledge graph completion methods, of which graph attention network(GAT)-based methods stand out since their superior performance. However,existing GAT-based knowledge graph completion methods often suffer fromoverfitting issues when dealing with heterogeneous knowledge graphs, primarilydue to the unbalanced number of samples. Additionally, these methodsdemonstrate poor performance in predicting the tail (head) entity that sharesthe same relation and head (tail) entity with others. To solve these problems,we propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATHincorporates two separate attention network modules that work synergisticallyto predict the missing entities. We also introduce novel encoding and featuretransformation approaches, enabling the robust performance of GATH in scenarioswith imbalanced samples. Comprehensive experiments are conducted to evaluatethe GATH's performance. Compared with the existing SOTA GAT-based model onHits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on theFB15K-237 dataset, and by 4.5% and 14.6% on the WN18RR dataset, respectively.</description><author>Wanxu Wei, Yitong Song, Bin Yao</author><pubDate>Mon, 05 Aug 2024 13:28:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02456v1</guid></item><item><title>You Only Acquire Sparse-channel (YOAS): A Unified Framework for Dense-channel EEG Generation</title><link>http://arxiv.org/abs/2406.15269v2</link><description>High-precision acquisition of dense-channel electroencephalogram (EEG)signals is often impeded by the costliness and lack of portability ofequipment. In contrast, generating dense-channel EEG signals effectively fromsparse channels shows promise and economic viability. However, sparse-channelEEG poses challenges such as reduced spatial resolution, information loss,signal mixing, and heightened susceptibility to noise and interference. Toaddress these challenges, we first theoretically formulate the dense-channelEEG generation problem as by optimizing a set of cross-channel EEG signalgeneration problems. Then, we propose the YOAS framework for generatingdense-channel data from sparse-channel EEG signals. The YOAS totally consistsof four sequential stages: Data Preparation, Data Preprocessing, Biased-EEGGeneration, and Synthetic EEG Generation. Data Preparation and Preprocessingcarefully consider the distribution of EEG electrodes and low signal-to-noiseratio problem of EEG signals. Biased-EEG Generation includes sub-modules ofBiasEEGGanFormer and BiasEEGDiffFormer, which facilitate long-term featureextraction with attention and generate signals by combining electrode positionalignment with diffusion model, respectively. Synthetic EEG Generationsynthesizes the final signals, employing a deduction paradigm for multi-channelEEG generation. Extensive experiments confirmed YOAS's feasibility, efficiency,and theoretical validity, even remarkably enhancing data discernibility. Thisbreakthrough in dense-channel EEG signal generation from sparse-channel dataopens new avenues for exploration in EEG signal processing and application.</description><author>Hongyu Chen, Weiming Zeng, Luhui Cai, Lei Wang, Jia Lu, Yueyang Li, Hongjie Yan, Wai Ting Siok, Nizhuan Wang</author><pubDate>Mon, 05 Aug 2024 13:23:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.15269v2</guid></item><item><title>An investigation on the use of Large Language Models for hyperparameter tuning in Evolutionary Algorithms</title><link>http://arxiv.org/abs/2408.02451v1</link><description>Hyperparameter optimization is a crucial problem in Evolutionary Computation.In fact, the values of the hyperparameters directly impact the trajectory takenby the optimization process, and their choice requires extensive reasoning byhuman operators. Although a variety of self-adaptive Evolutionary Algorithmshave been proposed in the literature, no definitive solution has been found. Inthis work, we perform a preliminary investigation to automate the reasoningprocess that leads to the choice of hyperparameter values. We employ twoopen-source Large Language Models (LLMs), namely Llama2-70b and Mixtral, toanalyze the optimization logs online and provide novel real-time hyperparameterrecommendations. We study our approach in the context of step-size adaptationfor (1+1)-ES. The results suggest that LLMs can be an effective method foroptimizing hyperparameters in Evolution Strategies, encouraging furtherresearch in this direction.</description><author>Leonardo Lucio Custode, Fabio Caraffini, Anil Yaman, Giovanni Iacca</author><pubDate>Mon, 05 Aug 2024 13:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02451v1</guid></item><item><title>Using Letter Positional Probabilities to Assess Word Complexity</title><link>http://arxiv.org/abs/2404.07768v4</link><description>Word complexity is defined in a number of different ways. Psycholinguistic,morphological and lexical proxies are often used. Human ratings are also used.The problem here is that these proxies do not measure complexity directly, andhuman ratings are susceptible to subjective bias. In this study we contend thatsome form of 'latent complexity' can be approximated by using samples of simpleand complex words. We use a sample of 'simple' words from primary schoolpicture books and a sample of 'complex' words from high school and academicsettings. In order to analyse the differences between these classes, we look atthe letter positional probabilities (LPPs). We find strong statisticalassociations between several LPPs and complexity. For example, simple words aresignificantly (p&lt;.001) more likely to start with w, b, s, h, g, k, j, t, y orf, while complex words are significantly (p&lt;.001) more likely to start with i,a, e, r, v, u or d. We find similar strong associations for subsequent letterpositions, with 84 letter-position variables in the first 6 positions beingsignificant at the p&lt;.001 level. We then use LPPs as variables in creating aclassifier which can classify the two classes with an 83% accuracy. We testthese findings using a second data set, with 66 LPPs significant (p&lt;.001) inthe first 6 positions common to both datasets. We use these 66 variables tocreate a classifier that is able to classify a third dataset with an accuracyof 70%. Finally, we create a fourth sample by combining the extreme high andlow scoring words generated by three classifiers built on the first threeseparate datasets and use this sample to build a classifier which has anaccuracy of 97%. We use this to score the four levels of English word groupsfrom an ESL program.</description><author>Michael Dalvean</author><pubDate>Mon, 05 Aug 2024 13:12:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07768v4</guid></item><item><title>De-fine: Decomposing and Refining Visual Programs with Auto-Feedback</title><link>http://arxiv.org/abs/2311.12890v3</link><description>Visual programming, a modular and generalizable paradigm, integratesdifferent modules and Python operators to solve various vision-language tasks.Unlike end-to-end models that need task-specific data, it advances inperforming visual processing and reasoning in an unsupervised manner. Currentvisual programming methods generate programs in a single pass for each taskwhere the ability to evaluate and optimize based on feedback, unfortunately, islacking, which consequentially limits their effectiveness for complex,multi-step problems. Drawing inspiration from benders decomposition, weintroduce De-fine, a training-free framework that automatically decomposescomplex tasks into simpler subtasks and refines programs through auto-feedback.This model-agnostic approach can improve logical reasoning performance byintegrating the strengths of multiple models. Our experiments across variousvisual tasks show that De-fine creates more robust programs. Moreover, viewingeach feedback module as an independent agent will yield fresh prospects for thefield of agent research.</description><author>Minghe Gao, Juncheng Li, Hao Fei, Liang Pang, Wei Ji, Guoming Wang, Zheqi Lv, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</author><pubDate>Mon, 05 Aug 2024 13:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12890v3</guid></item><item><title>Automata-based constraints for language model decoding</title><link>http://arxiv.org/abs/2407.08103v3</link><description>Language models (LMs) are often expected to generate strings in some formallanguage; for example, structured data, API calls, or code snippets. AlthoughLMs can be tuned to improve their adherence to formal syntax, this does notguarantee conformance, especially with smaller LMs suitable for large-scaledeployment. In addition, tuning requires significant resources, making itimpractical for uncommon or task-specific formats. To prevent downstreamparsing errors we would ideally constrain the LM to only produce valid output,but this is severely complicated by tokenization, which is typically bothambiguous and misaligned with the formal grammar. We solve these issues throughthe application of automata theory, deriving an efficient closed-form solutionfor the regular languages, a broad class of formal languages with manypractical applications, including API calls or schema-guided JSON and YAML. Wealso discuss pragmatic extensions for coping with the issue of high branchingfactor, and extend our techniques to deterministic context-free languages,which similarly admit an efficient closed-form solution. Previous work on thistopic (Willard and Louf, 2023) layers bespoke solutions onto automata, leadingto problems with speed, correctness, and extensibility. Instead, we reformulatethe entire task in terms of automata so we can leverage well-studied andwell-optimized algorithms. Our system compiles constraints ~7,000x faster, isprovably correct, and can be extended in a modular fashion.</description><author>Terry Koo, Frederick Liu, Luheng He</author><pubDate>Mon, 05 Aug 2024 13:08:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08103v3</guid></item><item><title>Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models</title><link>http://arxiv.org/abs/2408.02442v1</link><description>Structured generation, the process of producing content in standardizedformats like JSON and XML, is widely utilized in real-world applications toextract key output information from large language models (LLMs). This studyinvestigates whether such constraints on generation space impact LLMs'abilities, including reasoning and domain knowledge comprehension.Specifically, we evaluate LLMs' performance when restricted to adhere tostructured formats versus generating free-form responses across various commontasks. Surprisingly, we observe a significant decline in LLMs' reasoningabilities under format restrictions. Furthermore, we find that stricter formatconstraints generally lead to greater performance degradation in reasoningtasks.</description><author>Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, Yun-Nung Chen</author><pubDate>Mon, 05 Aug 2024 13:08:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02442v1</guid></item><item><title>Conditioning of Banach Space Valued Gaussian Random Variables: An Approximation Approach Based on Martingales</title><link>http://arxiv.org/abs/2404.03453v2</link><description>In this paper we investigate the conditional distributions of two Banachspace valued, jointly Gaussian random variables. We show that these conditionaldistributions are again Gaussian and that their means and covariances aredetermined by a general finite dimensional approximation scheme based upon amartingale approach. In particular, it turns out that the covariance operatorsoccurring in this scheme converge with respect to the nuclear norm and that theconditional probabilities converge weakly. Moreover, we discuss in detail, howour approximation scheme can be implemented in several classes of importantBanach spaces such as RKHSs and $C(T)$. As an example, we then apply ourgeneral results to the case of Gaussian processes with continuous pathsconditioned to partial but infinite observations of their paths. Here we showthat conditioning on sufficiently rich, increasing sets of finitely manyobservations leads to consistent approximations, in the sense that both themean and covariance functions converge uniformly. Moreover, we discuss howthese results improve our understanding of the popular Gaussian processes formachine learning.</description><author>Ingo Steinwart</author><pubDate>Mon, 05 Aug 2024 13:01:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03453v2</guid></item><item><title>Long Input Benchmark for Russian Analysis</title><link>http://arxiv.org/abs/2408.02439v1</link><description>Recent advancements in Natural Language Processing (NLP) have fostered thedevelopment of Large Language Models (LLMs) that can solve an immense varietyof tasks. One of the key aspects of their application is their ability to workwith long text documents and to process long sequences of tokens. This hascreated a demand for proper evaluation of long-context understanding. Toaddress this need for the Russian language, we propose LIBRA (Long InputBenchmark for Russian Analysis), which comprises 21 adapted datasets to studythe LLM's abilities to understand long texts thoroughly. The tests are dividedinto four complexity groups and allow the evaluation of models across variouscontext lengths ranging from 4k up to 128k tokens. We provide the open-sourcedatasets, codebase, and public leaderboard for LIBRA to guide forthcomingresearch.</description><author>Igor Churin, Murat Apishev, Maria Tikhonova, Denis Shevelev, Aydar Bulatov, Yuri Kuratov, Sergej Averkiev, Alena Fenogenova</author><pubDate>Mon, 05 Aug 2024 12:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02439v1</guid></item><item><title>Tell me why: Training preferences-based RL with human preferences and step-level explanations</title><link>http://arxiv.org/abs/2405.14244v2</link><description>Human-in-the-loop reinforcement learning allows the training of agentsthrough various interfaces, even for non-expert humans. Recently,preference-based methods (PbRL), where the human has to give his preferenceover two trajectories, increased in popularity since they allow training indomains where more direct feedback is hard to formulate. However, the currentPBRL methods have limitations and do not provide humans with an expressiveinterface for giving feedback. With this work, we propose a newpreference-based learning method that provides humans with a more expressiveinterface to provide their preference over trajectories and a factualexplanation (or annotation of why they have this preference). Theseexplanations allow the human to explain what parts of the trajectory are mostrelevant for the preference. We allow the expression of the explanations overindividual trajectory steps. We evaluate our method in various simulationsusing a simulated human oracle (with realistic restrictions), and our resultsshow that our extended feedback can improve the speed of learning.</description><author>Jakob Karalus</author><pubDate>Mon, 05 Aug 2024 12:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14244v2</guid></item><item><title>Vertical Federated Learning: Challenges, Methodologies and Experiments</title><link>http://arxiv.org/abs/2202.04309v2</link><description>Recently, federated learning (FL) has emerged as a promising distributedmachine learning (ML) technology, owing to the advancing computational andsensing capacities of end-user devices, however with the increasing concerns onusers' privacy. As a special architecture in FL, vertical FL (VFL) is capableof constructing a hyper ML model by embracing sub-models from differentclients. These sub-models are trained locally by vertically partitioned datawith distinct attributes. Therefore, the design of VFL is fundamentallydifferent from that of conventional FL, raising new and unique research issues.In this paper, we aim to discuss key challenges in VFL with effectivesolutions, and conduct experiments on real-life datasets to shed light on theseissues. Specifically, we first propose a general framework on VFL, andhighlight the key differences between VFL and conventional FL. Then, we discussresearch challenges rooted in VFL systems under four aspects, i.e., securityand privacy risks, expensive computation and communication costs, possiblestructural damage caused by model splitting, and system heterogeneity.Afterwards, we develop solutions to addressing the aforementioned challenges,and conduct extensive experiments to showcase the effectiveness of our proposedsolutions.</description><author>Kang Wei, Jun Li, Chuan Ma, Ming Ding, Sha Wei, Fan Wu, Guihai Chen, Thilina Ranbaduge</author><pubDate>Mon, 05 Aug 2024 12:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.04309v2</guid></item><item><title>OpenBias: Open-set Bias Detection in Text-to-Image Generative Models</title><link>http://arxiv.org/abs/2404.07990v2</link><description>Text-to-image generative models are becoming increasingly popular andaccessible to the general public. As these models see large-scale deployments,it is necessary to deeply investigate their safety and fairness to notdisseminate and perpetuate any kind of biases. However, existing works focus ondetecting closed sets of biases defined a priori, limiting the studies towell-known concepts. In this paper, we tackle the challenge of open-set biasdetection in text-to-image generative models presenting OpenBias, a newpipeline that identifies and quantifies the severity of biases agnostically,without access to any precompiled set. OpenBias has three stages. In the firstphase, we leverage a Large Language Model (LLM) to propose biases given a setof captions. Secondly, the target generative model produces images using thesame set of captions. Lastly, a Vision Question Answering model recognizes thepresence and extent of the previously proposed biases. We study the behavior ofStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigatedbefore. Via quantitative experiments, we demonstrate that OpenBias agrees withcurrent closed-set bias detection methods and human judgement.</description><author>Moreno D'Incà, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, Nicu Sebe</author><pubDate>Mon, 05 Aug 2024 12:55:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07990v2</guid></item><item><title>On Probabilistic Embeddings in Optimal Dimension Reduction</title><link>http://arxiv.org/abs/2408.02433v1</link><description>Dimension reduction algorithms are a crucial part of many data sciencepipelines, including data exploration, feature creation and selection, anddenoising. Despite their wide utilization, many non-linear dimension reductionalgorithms are poorly understood from a theoretical perspective. In this workwe consider a generalized version of multidimensional scaling, which is posedas an optimization problem in which a mapping from a high-dimensional featurespace to a lower-dimensional embedding space seeks to preserve either innerproducts or norms of the distribution in feature space, and which encompassesmany commonly used dimension reduction algorithms. We analytically investigatethe variational properties of this problem, leading to the following insights:1) Solutions found using standard particle descent methods may lead tonon-deterministic embeddings, 2) A relaxed or probabilistic formulation of theproblem admits solutions with easily interpretable necessary conditions, 3) Theglobally optimal solutions to the relaxed problem actually must give adeterministic embedding. This progression of results mirrors the classicaldevelopment of optimal transportation, and in a case relating to theGromov-Wasserstein distance actually gives explicit insight into the structureof the optimal embeddings, which are parametrically determined anddiscontinuous. Finally, we illustrate that a standard computationalimplementation of this task does not learn deterministic embeddings, whichmeans that it learns sub-optimal mappings, and that the embeddings learned inthat context have highly misleading clustering structure, underscoring thedelicate nature of solving this problem computationally.</description><author>Ryan Murray, Adam Pickarski</author><pubDate>Mon, 05 Aug 2024 12:46:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02433v1</guid></item><item><title>Reinforcement Learning Friendly Vision-Language Model for Minecraft</title><link>http://arxiv.org/abs/2303.10571v2</link><description>One of the essential missions in the AI research community is to build anautonomous embodied agent that can achieve high-level performance across a widespectrum of tasks. However, acquiring or manually designing rewards for allopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modalcontrastive learning framework architecture, CLIP4MC, aiming to learn areinforcement learning (RL) friendly vision-language model (VLM) that serves asan intrinsic reward function for open-ended tasks. Simply utilizing thesimilarity between the video snippet and the language prompt is not RL-friendlysince standard VLMs may only capture the similarity at a coarse level. Toachieve RL-friendliness, we incorporate the task completion degree into the VLMtraining objective, as this information can assist agents in distinguishing theimportance between different states. Moreover, we provide neat YouTube datasetsbased on the large-scale YouTube database provided by MineDojo. Specifically,two rounds of filtering operations guarantee that the dataset covers enoughessential information and that the video-text pair is highly correlated.Empirically, we demonstrate that the proposed method achieves betterperformance on RL tasks compared with baselines. The code and datasets areavailable at https://github.com/PKU-RL/CLIP4MC.</description><author>Haobin Jiang, Junpeng Yue, Hao Luo, Ziluo Ding, Zongqing Lu</author><pubDate>Mon, 05 Aug 2024 12:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10571v2</guid></item><item><title>AI-Powered Energy Algorithmic Trading: Integrating Hidden Markov Models with Neural Networks</title><link>http://arxiv.org/abs/2407.19858v2</link><description>In quantitative finance, machine learning methods are essential for alphageneration. This study introduces a new approach that combines Hidden MarkovModels (HMM) and neural networks, integrated with Black-Litterman portfoliooptimization. During the COVID period (2019-2022), this dual-model approachachieved a 97% return with a Sharpe ratio of 0.992. It incorporates two riskmodels to enhance risk management, showing efficiency during volatile periods.The methodology was implemented on the QuantConnect platform, which was chosenfor its robust framework and experimental reproducibility. The system, whichpredicts future price movements, includes a three-year warm-up to ensure properalgorithm function. It targets highly liquid, large-cap energy stocks to ensurestable and predictable performance while also considering broker payments. Thedual-model alpha system utilizes log returns to select the optimal state basedon the historical performance. It combines state predictions with neuralnetwork outputs, which are based on historical data, to generate tradingsignals. This study examined the architecture of the trading system, datapre-processing, training, and performance. The full code and backtesting dataare available under the MIT license.</description><author>Tiago Monteiro</author><pubDate>Mon, 05 Aug 2024 12:42:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19858v2</guid></item><item><title>Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales</title><link>http://arxiv.org/abs/2404.11129v2</link><description>The remarkable performance of Multimodal Large Language Models (MLLMs) hasunequivocally demonstrated their proficient understanding capabilities inhandling a wide array of visual tasks. Nevertheless, the opaque nature of theirblack-box reasoning processes persists as an enigma, rendering themuninterpretable and struggling with hallucination. Their ability to executeintricate compositional reasoning tasks is also constrained, culminating in astagnation of learning progression for these models. In this work, we introduceFact, a novel paradigm designed to generate multimodal rationales that arefaithful, concise, and transferable for teaching MLLMs. This paradigm utilizesverifiable visual programming to generate executable code guaranteeingfaithfulness and precision. Subsequently, through a series of operationsincluding pruning, merging, and bridging, the rationale enhances itsconciseness. Furthermore, we filter rationales that can be transferred toend-to-end paradigms from programming paradigms to guarantee transferability.Empirical evidence from experiments demonstrates the superiority of our methodacross models of varying parameter sizes, significantly enhancing theircompositional reasoning and generalization ability. Our approach also reduceshallucinations owing to its high correlation between images and text.</description><author>Minghe Gao, Shuang Chen, Liang Pang, Yuan Yao, Jisheng Dang, Wenqiao Zhang, Juncheng Li, Siliang Tang, Yueting Zhuang, Tat-Seng Chua</author><pubDate>Mon, 05 Aug 2024 12:39:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11129v2</guid></item><item><title>Attenuation-adjusted deep learning of pore defects in 2D radiographs of additive manufacturing powders</title><link>http://arxiv.org/abs/2408.02427v1</link><description>The presence of gas pores in metal feedstock powder for additivemanufacturing greatly affects the final AM product. Since current porosityanalysis often involves lengthy X-ray computed tomography (XCT) scans with afull rotation around the sample, motivation exists to explore methods thatallow for high throughput -- possibly enabling in-line porosity analysis duringmanufacturing. Through labelling pore pixels on single 2D radiographs ofpowders, this work seeks to simulate such future efficient setups. Highsegmentation accuracy is achieved by combining a model of X-ray attenuationthrough particles with a variant of the widely applied UNet architecture;notably, F1-score increases by $11.4\%$ compared to the baseline UNet. Theproposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2)making tight particle cutouts, and 3) subtracting an ideal particle withoutpores generated from a distance map inspired by Lambert-Beers law. This paperexplores four image processing methods, where the fastest (yet stillunoptimized) segments a particle in mean $0.014s$ time with F1-score $0.78$,and the most accurate in $0.291s$ with F1-score $0.87$. Due to their scalablenature, these strategies can be involved in making high throughput porosityanalysis of metal feedstock powder for additive manufacturing.</description><author>Andreas Bjerregaard, David Schumacher, Jon Sporring</author><pubDate>Mon, 05 Aug 2024 12:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02427v1</guid></item><item><title>FPT+: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification</title><link>http://arxiv.org/abs/2408.02426v1</link><description>The success of large-scale pre-trained models has established fine-tuning asa standard method for achieving significant improvements in downstream tasks.However, fine-tuning the entire parameter set of a pre-trained model is costly.Parameter-efficient transfer learning (PETL) has recently emerged as acost-effective alternative for adapting pre-trained models to downstream tasks.Despite its advantages, the increasing model size and input resolution presentchallenges for PETL, as the training memory consumption is not reduced aseffectively as the parameter usage. In this paper, we introduce Fine-grainedPrompt Tuning plus (FPT+), a PETL method designed for high-resolution medicalimage classification, which significantly reduces memory consumption comparedto other PETL methods. FPT+ performs transfer learning by training alightweight side network and accessing pre-trained knowledge from a largepre-trained model (LPM) through fine-grained prompts and fusion modules.Specifically, we freeze the LPM and construct a learnable lightweight sidenetwork. The frozen LPM processes high-resolution images to extractfine-grained features, while the side network employs the correspondingdown-sampled low-resolution images to minimize the memory usage. To enable theside network to leverage pre-trained knowledge, we propose fine-grained promptsand fusion modules, which collaborate to summarize information through theLPM's intermediate activations. We evaluate FPT+ on eight medical imagedatasets of varying sizes, modalities, and complexities. Experimental resultsdemonstrate that FPT+ outperforms other PETL methods, using only 1.03% of thelearnable parameters and 3.18% of the memory required for fine-tuning an entireViT-B model. Our code is available at https://github.com/YijinHuang/FPT.</description><author>Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang</author><pubDate>Mon, 05 Aug 2024 12:33:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02426v1</guid></item><item><title>Revolutionizing Urban Safety Perception Assessments: Integrating Multimodal Large Language Models with Street View Images</title><link>http://arxiv.org/abs/2407.19719v2</link><description>Measuring urban safety perception is an important and complex task thattraditionally relies heavily on human resources. This process often involvesextensive field surveys, manual data collection, and subjective assessments,which can be time-consuming, costly, and sometimes inconsistent. Street ViewImages (SVIs), along with deep learning methods, provide a way to realizelarge-scale urban safety detection. However, achieving this goal often requiresextensive human annotation to train safety ranking models, and thearchitectural differences between cities hinder the transferability of thesemodels. Thus, a fully automated method for conducting safety evaluations isessential. Recent advances in multimodal large language models (MLLMs) havedemonstrated powerful reasoning and analytical capabilities. Cutting-edgemodels, e.g., GPT-4 have shown surprising performance in many tasks. Weemployed these models for urban safety ranking on a human-annotated anchor setand validated that the results from MLLMs align closely with human perceptions.Additionally, we proposed a method based on the pre-trained ContrastiveLanguage-Image Pre-training (CLIP) feature and K-Nearest Neighbors (K-NN)retrieval to quickly assess the safety index of the entire city. Experimentalresults show that our method outperforms existing training needed deep learningapproaches, achieving efficient and accurate urban safety evaluations. Theproposed automation for urban safety perception assessment is a valuable toolfor city planners, policymakers, and researchers aiming to improve urbanenvironments.</description><author>Jiaxin Zhang, Yunqin Li, Tomohiro Fukuda, Bowen Wang</author><pubDate>Mon, 05 Aug 2024 12:29:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19719v2</guid></item><item><title>FE-Adapter: Adapting Image-based Emotion Classifiers to Videos</title><link>http://arxiv.org/abs/2408.02421v1</link><description>Utilizing large pre-trained models for specific tasks has yielded impressiveresults. However, fully fine-tuning these increasingly large models is becomingprohibitively resource-intensive. This has led to a focus on moreparameter-efficient transfer learning, primarily within the same modality. Butthis approach has limitations, particularly in video understanding wheresuitable pre-trained models are less common. Addressing this, our studyintroduces a novel cross-modality transfer learning approach from images tovideos, which we call parameter-efficient image-to-video transfer learning. Wepresent the Facial-Emotion Adapter (FE-Adapter), designed for efficientfine-tuning in video tasks. This adapter allows pre-trained image models, whichtraditionally lack temporal processing capabilities, to analyze dynamic videocontent efficiently. Notably, it uses about 15 times fewer parameters thanprevious methods, while improving accuracy. Our experiments in video emotionrecognition demonstrate that the FE-Adapter can match or even surpass existingfine-tuning and video emotion models in both performance and efficiency. Thisbreakthrough highlights the potential for cross-modality approaches inenhancing the capabilities of AI models, particularly in fields like videoemotion analysis where the demand for efficiency and accuracy is constantlyrising.</description><author>Shreyank N Gowda, Boyan Gao, David A. Clifton</author><pubDate>Mon, 05 Aug 2024 12:27:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02421v1</guid></item><item><title>Infusing Emotions into Task-oriented Dialogue Systems: Understanding, Management, and Generation</title><link>http://arxiv.org/abs/2408.02417v1</link><description>Emotions are indispensable in human communication, but are often overlookedin task-oriented dialogue (ToD) modelling, where the task success is theprimary focus. While existing works have explored user emotions or similarconcepts in some ToD tasks, none has so far included emotion modelling into afully-fledged ToD system nor conducted interaction with human or simulatedusers. In this work, we incorporate emotion into the complete ToD processingloop, involving understanding, management, and generation. To this end, weextend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviourlabels. Through interactive experimentation involving both simulated and humanusers, we demonstrate that our proposed framework significantly enhances theuser's emotional experience as well as the task success.</description><author>Shutong Feng, Hsien-chin Lin, Christian Geishauser, Nurul Lubis, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Renato Vukovic, Milica Gašić</author><pubDate>Mon, 05 Aug 2024 12:21:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02417v1</guid></item><item><title>Zero shot VLMs for hate meme detection: Are we there yet?</title><link>http://arxiv.org/abs/2402.12198v2</link><description>Multimedia content on social media is rapidly evolving, with memes gainingprominence as a distinctive form. Unfortunately, some malicious users exploitmemes to target individuals or vulnerable communities, making it imperative toidentify and address such instances of hateful memes. Extensive research hasbeen conducted to address this issue by developing hate meme detection models.However, a notable limitation of traditional machine/deep learning models isthe requirement for labeled datasets for accurate classification. Recently, theresearch community has witnessed the emergence of several visual languagemodels that have exhibited outstanding performance across various tasks. Inthis study, we aim to investigate the efficacy of these visual language modelsin handling intricate tasks such as hate meme detection. We use various promptsettings to focus on zero-shot classification of hateful/harmful memes. Throughour analysis, we observe that large VLMs are still vulnerable for zero-shothate meme detection.</description><author>Naquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, Animesh Mukherjee</author><pubDate>Mon, 05 Aug 2024 12:20:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12198v2</guid></item><item><title>Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models</title><link>http://arxiv.org/abs/2408.02416v1</link><description>The drastic increase of large language models' (LLMs) parameters has led to anew research direction of fine-tuning-free downstream customization by prompts,i.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)play an important role in many businesses, there has emerged growing concernsabout the prompt leakage, which undermines the intellectual properties of theseservices and causes downstream attacks. In this paper, we analyze theunderlying mechanism of prompt leakage, which we refer to as promptmemorization, and develop corresponding defending strategies. By exploring thescaling laws in prompt extraction, we analyze key attributes that influenceprompt extraction, including model sizes, prompt lengths, as well as the typesof prompts. Then we propose two hypotheses that explain how LLMs expose theirprompts. The first is attributed to the perplexity, i.e. the familiarity ofLLMs to texts, whereas the second is based on the straightforward tokentranslation path in attention matrices. To defend against such threats, weinvestigate whether alignments can undermine the extraction of prompts. We findthat current LLMs, even those with safety alignments like GPT-4, are highlyvulnerable to prompt extraction attacks, even under the most straightforwarduser attacks. Therefore, we put forward several defense strategies with theinspiration of our findings, which achieve 83.8\% and 71.0\% drop in the promptextraction rate for Llama2-7B and GPT-3.5, respectively. Source code isavaliable at \url{https://github.com/liangzid/PromptExtractionEval}.</description><author>Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li</author><pubDate>Mon, 05 Aug 2024 12:20:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02416v1</guid></item><item><title>Affect Recognition in Conversations Using Large Language Models</title><link>http://arxiv.org/abs/2309.12881v2</link><description>Affect recognition, encompassing emotions, moods, and feelings, plays apivotal role in human communication. In the realm of conversational artificialintelligence, the ability to discern and respond to human affective cues is acritical factor for creating engaging and empathetic interactions. This studyinvestigates the capacity of large language models (LLMs) to recognise humanaffect in conversations, with a focus on both open-domain chit-chat dialoguesand task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP(Busso et al., 2008), EmoWOZ (Feng et al., 2022), and DAIC-WOZ (Gratch et al.,2014), covering a spectrum of dialogues from casual conversations to clinicalinterviews, we evaluate and compare LLMs' performance in affect recognition.Our investigation explores the zero-shot and few-shot capabilities of LLMsthrough in-context learning as well as their model capacities throughtask-specific fine-tuning. Additionally, this study takes into account thepotential impact of automatic speech recognition errors on LLM predictions.With this work, we aim to shed light on the extent to which LLMs can replicatehuman-like affect recognition capabilities in conversations.</description><author>Shutong Feng, Guangzhi Sun, Nurul Lubis, Wen Wu, Chao Zhang, Milica Gašić</author><pubDate>Mon, 05 Aug 2024 12:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12881v2</guid></item><item><title>Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model</title><link>http://arxiv.org/abs/2406.19905v2</link><description>The Mixture-of-Experts (MoE) has gained increasing attention in studyingLarge Vision-Language Models (LVLMs). It uses a sparse model to replace thedense model, achieving comparable performance while activating fewer parametersduring inference, thus significantly reducing the inference cost. Existing MoEmethods in LVLMs encourage different experts to handle different tokens, andthey usually employ a router to predict the routing of each token. However, thepredictions are based solely on sample features and do not truly reveal theoptimization directions of tokens. This may lead to severe optimizationinterference between different tokens assigned to an expert. To address thisproblem, this paper proposes a novel method based on token-level gradientanalysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we firstuse token-level gradients to identify conflicting tokens in experts. Afterthat, we add a specialized loss tailored to eliminate conflicts among tokenswithin each expert. Our method can serve as a plug-in for diverse LargeVision-Language Models, and extensive experimental results demonstrate itseffectiveness. The code will be publicly available athttps://github.com/longrongyang/STGC.</description><author>Longrong Yang, Dong Shen, Chaoxiang Cai, Fan Yang, Size Li, Di Zhang, Xi Li</author><pubDate>Mon, 05 Aug 2024 12:12:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19905v2</guid></item></channel></rss>