<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 16 Jan 2025 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DAViD: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video Diffusion Models</title><link>http://arxiv.org/abs/2501.08333v1</link><description>Understanding the ability of humans to use objects is crucial for AI toimprove daily life. Existing studies for learning such ability focus onhuman-object patterns (e.g., contact, spatial relation, orientation) in staticsituations, and learning Human-Object Interaction (HOI) patterns over time(i.e., movement of human and object) is relatively less explored. In thispaper, we introduce a novel type of affordance named Dynamic Affordance. For agiven input 3D object mesh, we learn dynamic affordance which models thedistribution of both (1) human motion and (2) human-guided object pose duringinteractions. As a core idea, we present a method to learn the 3D dynamicaffordance from synthetically generated 2D videos, leveraging a pre-trainedvideo diffusion model. Specifically, we propose a pipeline that first generates2D HOI videos from the 3D object and then lifts them into 3D to generate 4D HOIsamples. Once we generate diverse 4D HOI samples on various target objects, wetrain our DAViD, where we present a method based on the Low-Rank Adaptation(LoRA) module for pre-trained human motion diffusion model (MDM) and an objectpose diffusion model with human pose guidance. Our motion diffusion model isextended for multi-object interactions, demonstrating the advantage of ourpipeline with LoRA for combining the concepts of object usage. Throughextensive experiments, we demonstrate our DAViD outperforms the baselines ingenerating human motion with HOIs.</description><author>Hyeonwoo Kim, Sangwon Beak, Hanbyul Joo</author><pubDate>Tue, 14 Jan 2025 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08333v1</guid></item><item><title>MangaNinja: Line Art Colorization with Precise Reference Following</title><link>http://arxiv.org/abs/2501.08332v1</link><description>Derived from diffusion models, MangaNinjia specializes in the task ofreference-guided line art colorization. We incorporate two thoughtful designsto ensure precise character detail transcription, including a patch shufflingmodule to facilitate correspondence learning between the reference color imageand the target line art, and a point-driven control scheme to enablefine-grained color matching. Experiments on a self-collected benchmarkdemonstrate the superiority of our model over current solutions in terms ofprecise colorization. We further showcase the potential of the proposedinteractive point control in handling challenging cases, cross-charactercolorization, multi-reference harmonization, beyond the reach of existingalgorithms.</description><author>Zhiheng Liu, Ka Leong Cheng, Xi Chen, Jie Xiao, Hao Ouyang, Kai Zhu, Yu Liu, Yujun Shen, Qifeng Chen, Ping Luo</author><pubDate>Tue, 14 Jan 2025 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08332v1</guid></item><item><title>Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise</title><link>http://arxiv.org/abs/2501.08331v1</link><description>Generative modeling aims to transform random noise into structured outputs.In this work, we enhance video diffusion models by allowing motion control viastructured latent noise sampling. This is achieved by just a change in data: wepre-process training videos to yield structured noise. Consequently, our methodis agnostic to diffusion model design, requiring no changes to modelarchitectures or training pipelines. Specifically, we propose a novel noisewarping algorithm, fast enough to run in real time, that replaces randomtemporal Gaussianity with correlated warped noise derived from optical flowfields, while preserving the spatial Gaussianity. The efficiency of ouralgorithm enables us to fine-tune modern video diffusion base models usingwarped noise with minimal overhead, and provide a one-stop solution for a widerange of user-friendly motion control: local object motion control, globalcamera movement control, and motion transfer. The harmonization betweentemporal coherence and spatial Gaussianity in our warped noise leads toeffective motion control while maintaining per-frame pixel quality. Extensiveexperiments and user studies demonstrate the advantages of our method, makingit a robust and scalable approach for controlling motion in video diffusionmodels. Video results are available on our webpage:https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow/; sourcecode and model checkpoints are available on GitHub:https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.</description><author>Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu</author><pubDate>Tue, 14 Jan 2025 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08331v1</guid></item><item><title>Gradient Equilibrium in Online Learning: Theory and Applications</title><link>http://arxiv.org/abs/2501.08330v1</link><description>We present a new perspective on online learning that we refer to as gradientequilibrium: a sequence of iterates achieves gradient equilibrium if theaverage of gradients of losses along the sequence converges to zero. Ingeneral, this condition is not implied by nor implies sublinear regret. Itturns out that gradient equilibrium is achievable by standard online learningmethods such as gradient descent and mirror descent with constant step sizes(rather than decaying step sizes, as is usually required for no regret).Further, as we show through examples, gradient equilibrium translates into aninterpretable and meaningful property in online prediction problems spanningregression, classification, quantile estimation, and others. Notably, we showthat the gradient equilibrium framework can be used to develop a debiasingscheme for black-box predictions under arbitrary distribution shift, based onsimple post hoc online descent updates. We also show that post hoc gradientupdates can be used to calibrate predicted quantiles under distribution shift,and that the framework leads to unbiased Elo scores for pairwise preferenceprediction.</description><author>Anastasios N. Angelopoulos, Michael I. Jordan, Ryan J. Tibshirani</author><pubDate>Tue, 14 Jan 2025 18:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08330v1</guid></item><item><title>Predicting 4D Hand Trajectory from Monocular Videos</title><link>http://arxiv.org/abs/2501.08329v1</link><description>We present HaPTIC, an approach that infers coherent 4D hand trajectories frommonocular videos. Current video-based hand pose reconstruction methodsprimarily focus on improving frame-wise 3D pose using adjacent frames ratherthan studying consistent 4D hand trajectories in space. Despite the additionaltemporal cues, they generally underperform compared to image-based methods dueto the scarcity of annotated video data. To address these issues, we repurposea state-of-the-art image-based transformer to take in multiple frames anddirectly predict a coherent trajectory. We introduce two types of lightweightattention layers: cross-view self-attention to fuse temporal information, andglobal cross-attention to bring in larger spatial context. Our method infers 4Dhand trajectories similar to the ground truth while maintaining strong 2Dreprojection alignment. We apply the method to both egocentric and allocentricvideos. It significantly outperforms existing methods in global trajectoryaccuracy while being comparable to the state-of-the-art in single-image poseestimation. Project website: https://judyye.github.io/haptic-www</description><author>Yufei Ye, Yao Feng, Omid Taheri, Haiwen Feng, Shubham Tulsiani, Michael J. Black</author><pubDate>Tue, 14 Jan 2025 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08329v1</guid></item><item><title>PokerBench: Training Large Language Models to become Professional Poker Players</title><link>http://arxiv.org/abs/2501.08328v1</link><description>We introduce PokerBench - a benchmark for evaluating the poker-playingabilities of large language models (LLMs). As LLMs excel in traditional NLPtasks, their application to complex, strategic games like poker poses a newchallenge. Poker, an incomplete information game, demands a multitude of skillssuch as mathematics, reasoning, planning, strategy, and a deep understanding ofgame theory and human psychology. This makes Poker the ideal next frontier forlarge language models. PokerBench consists of a comprehensive compilation of11,000 most important scenarios, split between pre-flop and post-flop play,developed in collaboration with trained poker players. We evaluate prominentmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,finding that all state-of-the-art LLMs underperform in playing optimal poker.However, after fine-tuning, these models show marked improvements. We validatePokerBench by having models with different scores compete with each other,demonstrating that higher scores on PokerBench lead to higher win rates inactual poker games. Through gameplay between our fine-tuned model and GPT-4, wealso identify limitations of simple supervised fine-tuning for learning optimalplaying strategy, suggesting the need for more advanced methodologies foreffectively training language models to excel in games. PokerBench thuspresents a unique benchmark for a quick and reliable evaluation of thepoker-playing ability of LLMs as well as a comprehensive benchmark to study theprogress of LLMs in complex game-playing scenarios. The dataset and code willbe made available at: \url{https://github.com/pokerllm/pokerbench}.</description><author>Richard Zhuang, Akshat Gupta, Richard Yang, Aniket Rahane, Zhengyu Li, Gopala Anumanchipalli</author><pubDate>Tue, 14 Jan 2025 18:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08328v1</guid></item><item><title>Multigenre AI-powered Story Composition</title><link>http://arxiv.org/abs/2405.06685v2</link><description>This paper shows how to construct genre patterns, whose purpose is to guideinteractive story composition in a way that enforces thematic consistency. Tostart the discussion we argue, based on previous seminal works, for theexistence of five fundamental genres, namely comedy, romance - in the sense ofepic plots, flourishing since the twelfth century -, tragedy, satire, andmystery. To construct the patterns, a simple two-phase process is employed:first retrieving examples that match our genre characterizations, and thenapplying a form of most specific generalization to the groups of examples inorder to find their commonalities. In both phases, AI agents are instrumental,with our PatternTeller prototype being called to operate the story compositionprocess, offering the opportunity to generate stories from a given premise ofthe user, to be developed under the guidance of the chosen pattern and tryingto accommodate the user's suggestions along the composition stages.</description><author>Edirlei Soares de Lima, Margot M. E. Neggers, Antonio L. Furtado</author><pubDate>Tue, 14 Jan 2025 18:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06685v2</guid></item><item><title>Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks</title><link>http://arxiv.org/abs/2501.08326v1</link><description>We present Omni-RGPT, a multimodal large language model designed tofacilitate region-level comprehension for both images and videos. To achieveconsistent region representation across spatio-temporal dimensions, weintroduce Token Mark, a set of tokens highlighting the target regions withinthe visual feature space. These tokens are directly embedded into spatialregions using region prompts (e.g., boxes or masks) and simultaneouslyincorporated into the text prompt to specify the target, establishing a directconnection between visual and text tokens. To further support robust videounderstanding without requiring tracklets, we introduce an auxiliary task thatguides Token Mark by leveraging the consistency of the tokens, enabling stableregion interpretation across the video. Additionally, we introduce alarge-scale region-level video instruction dataset (RegVID-300k). Omni-RGPTachieves state-of-the-art results on image and video-based commonsensereasoning benchmarks while showing strong performance in captioning andreferring expression comprehension tasks.</description><author>Miran Heo, Min-Hung Chen, De-An Huang, Sifei Liu, Subhashree Radhakrishnan, Seon Joo Kim, Yu-Chiang Frank Wang, Ryo Hachiuma</author><pubDate>Tue, 14 Jan 2025 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08326v1</guid></item><item><title>GameFactory: Creating New Games with Generative Interactive Videos</title><link>http://arxiv.org/abs/2501.08325v1</link><description>Generative game engines have the potential to revolutionize game developmentby autonomously creating new content and reducing manual workload. However,existing video-based game generation methods fail to address the criticalchallenge of scene generalization, limiting their applicability to existinggames with fixed styles and scenes. In this paper, we present GameFactory, aframework focused on exploring scene generalization in game video generation.To enable the creation of entirely new and diverse games, we leveragepre-trained video diffusion models trained on open-domain video data. To bridgethe domain gap between open-domain priors and small-scale game dataset, wepropose a multi-phase training strategy that decouples game style learning fromaction control, preserving open-domain generalization while achieving actioncontrollability. Using Minecraft as our data source, we release GF-Minecraft, ahigh-quality and diversity action-annotated video dataset for research.Furthermore, we extend our framework to enable autoregressiveaction-controllable game video generation, allowing the production ofunlimited-length interactive game videos. Experimental results demonstrate thatGameFactory effectively generates open-domain, diverse, and action-controllablegame videos, representing a significant step forward in AI-driven gamegeneration. Our dataset and project page are publicly available at\url{https://vvictoryuki.github.io/gamefactory/}.</description><author>Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu</author><pubDate>Tue, 14 Jan 2025 18:57:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08325v1</guid></item><item><title>ADAM-1: AI and Bioinformatics for Alzheimer's Detection and Microbiome-Clinical Data Integrations</title><link>http://arxiv.org/abs/2501.08324v1</link><description>The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agentlarge language model (LLM) framework designed to integrate and analyzemulti-modal data, including microbiome profiles, clinical datasets, andexternal knowledge bases, to enhance the understanding and detection ofAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)techniques along with its multi-agent architecture, ADAM-1 synthesizes insightsfrom diverse data sources and contextualizes findings using literature-drivenevidence. Comparative evaluation against XGBoost revealed similar mean F1scores but significantly reduced variance for ADAM-1, highlighting itsrobustness and consistency, particularly in small laboratory datasets. Whilecurrently tailored for binary classification tasks, future iterations aim toincorporate additional data modalities, such as neuroimaging and biomarkers, tobroaden the scalability and applicability for Alzheimer's research anddiagnostics.</description><author>Ziyuan Huang, Vishaldeep Kaur Sekhon, Ouyang Guo, Mark Newman, Roozbeh Sadeghian, Maria L. Vaida, Cynthia Jo, Doyle Ward, Vanni Bucci, John P. Haran</author><pubDate>Tue, 14 Jan 2025 18:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08324v1</guid></item><item><title>Exploring Robustness of Multilingual LLMs on Real-World Noisy Data</title><link>http://arxiv.org/abs/2501.08322v1</link><description>Large Language Models (LLMs) are trained on Web data that might containspelling errors made by humans. But do they become robust to similar real-worldnoise? In this paper, we investigate the effect of real-world spelling mistakeson the performance of 9 language models, with parameters ranging from 0.2B to13B, in 3 different NLP tasks, namely Natural Language Inference (NLI), NameEntity Recognition (NER), and Intent Classification (IC). We perform ourexperiments on 6 different languages and build a dictionary of real-world noisefor them using the Wikipedia edit history. We show that the performance gap ofthe studied models on the clean and noisy test data averaged across all thedatasets and languages ranges from 2.3 to 4.3 absolute percentage points. Inaddition, mT5 models, in general, show more robustness compared to BLOOM,Falcon, and BERT-like models. In particular, mT5 (13B), was the most robust onaverage overall, across the 3 tasks, and in 4 of the 6 languages.</description><author>Amirhossein Aliakbarzadeh, Lucie Flek, Akbar Karimi</author><pubDate>Tue, 14 Jan 2025 18:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08322v1</guid></item><item><title>Enhancing Automated Interpretability with Output-Centric Feature Descriptions</title><link>http://arxiv.org/abs/2501.08319v1</link><description>Automated interpretability pipelines generate natural language descriptionsfor the concepts represented by features in large language models (LLMs), suchas plants or the first word in a sentence. These descriptions are derived usinginputs that activate the feature, which may be a dimension or a direction inthe model's representation space. However, identifying activating inputs iscostly, and the mechanistic role of a feature in model behavior is determinedboth by how inputs cause a feature to activate and by how feature activationaffects outputs. Using steering evaluations, we reveal that current pipelinesprovide descriptions that fail to capture the causal effect of the feature onoutputs. To fix this, we propose efficient, output-centric methods forautomatically generating feature descriptions. These methods use the tokensweighted higher after feature stimulation or the highest weight tokens afterapplying the vocabulary "unembedding" head directly to the feature. Ouroutput-centric descriptions better capture the causal effect of a feature onmodel outputs than input-centric descriptions, but combining the two leads tothe best performance on both input and output evaluations. Lastly, we show thatoutput-centric descriptions can be used to find inputs that activate featurespreviously thought to be "dead".</description><author>Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva</author><pubDate>Tue, 14 Jan 2025 18:53:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08319v1</guid></item><item><title>A Similarity Measure Between Functions with Applications to Statistical Learning and Optimization</title><link>http://arxiv.org/abs/2501.08317v1</link><description>In this note, we present a novel measure of similarity between two functions.It quantifies how the sub-optimality gaps of two functions convert to eachother, and unifies several existing notions of functional similarity. We showthat it has convenient operation rules, and illustrate its use in empiricalrisk minimization and non-stationary online optimization.</description><author>Chengpiao Huang, Kaizheng Wang</author><pubDate>Tue, 14 Jan 2025 18:52:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08317v1</guid></item><item><title>Diffusion Adversarial Post-Training for One-Step Video Generation</title><link>http://arxiv.org/abs/2501.08316v1</link><description>The diffusion models are widely used for image and video generation, buttheir iterative generation process is slow and expansive. While existingdistillation approaches have demonstrated the potential for one-step generationin the image domain, they still suffer from significant quality degradation. Inthis work, we propose Adversarial Post-Training (APT) against real datafollowing diffusion pre-training for one-step video generation. To improve thetraining stability and quality, we introduce several improvements to the modelarchitecture and training procedures, along with an approximated R1regularization objective. Empirically, our experiments show that ouradversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,24fps videos in real time using a single forward evaluation step. Additionally,our model is capable of generating 1024px images in a single step, achievingquality comparable to state-of-the-art methods.</description><author>Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, Lu Jiang</author><pubDate>Tue, 14 Jan 2025 18:51:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08316v1</guid></item><item><title>Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation</title><link>http://arxiv.org/abs/2412.07169v3</link><description>Accurate uncertainty estimation is crucial for deploying neural networks inrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is awidely used technique for approximating predictive uncertainty by performingstochastic forward passes with dropout during inference. However, using staticdropout rates across all layers and inputs can lead to suboptimal uncertaintyestimates, as it fails to adapt to the varying characteristics of individualinputs and network layers. Existing approaches optimize dropout rates duringtraining using labeled data, resulting in fixed inference-time parameters thatcannot adjust to new data distributions, compromising uncertainty estimates inMonte Carlo simulations. In this paper, we propose Rate-In, an algorithm that dynamically adjustsdropout rates during inference by quantifying the information loss induced bydropout in each layer's feature maps. By treating dropout as controlled noiseinjection and leveraging information-theoretic principles, Rate-In adaptsdropout rates per layer and per input instance without requiring ground truthlabels. By quantifying the functional information loss in feature maps, weadaptively tune dropout rates to maintain perceptual quality across diversemedical imaging tasks and architectural configurations. Our extensive empiricalstudy on synthetic data and real-world medical imaging tasks demonstrates thatRate-In improves calibration and sharpens uncertainty estimates compared tofixed or heuristic dropout rates without compromising predictive performance.Rate-In offers a practical, unsupervised, inference-time approach to optimizingdropout for more reliable predictive uncertainty estimation in criticalapplications.</description><author>Tal Zeevi, Ravid Shwartz-Ziv, Yann LeCun, Lawrence H. Staib, John A. Onofrey</author><pubDate>Tue, 14 Jan 2025 18:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07169v3</guid></item><item><title>MiniMax-01: Scaling Foundation Models with Lightning Attention</title><link>http://arxiv.org/abs/2501.08313v1</link><description>We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,which are comparable to top-tier models while offering superior capabilities inprocessing longer contexts. The core lies in lightning attention and itsefficient scaling. To maximize computational capacity, we integrate it withMixture of Experts (MoE), creating a model with 32 experts and 456 billiontotal parameters, of which 45.9 billion are activated for each token. Wedevelop an optimized parallel strategy and highly efficientcomputation-communication overlap techniques for MoE and lightning attention.This approach enables us to conduct efficient training and inference on modelswith hundreds of billions of parameters across contexts spanning millions oftokens. The context window of MiniMax-Text-01 can reach up to 1 million tokensduring training and extrapolate to 4 million tokens during inference at anaffordable cost. Our vision-language model, MiniMax-VL-01 is built throughcontinued training with 512 billion vision-language tokens. Experiments on bothstandard and in-house benchmarks show that our models match the performance ofstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32times longer context window. We publicly release MiniMax-01 athttps://github.com/MiniMax-AI.</description><author>MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong</author><pubDate>Tue, 14 Jan 2025 18:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08313v1</guid></item><item><title>Everybody Likes to Sleep: A Computer-Assisted Comparison of Object Naming Data from 30 Languages</title><link>http://arxiv.org/abs/2501.08312v1</link><description>Object naming - the act of identifying an object with a word or a phrase - isa fundamental skill in interpersonal communication, relevant to manydisciplines, such as psycholinguistics, cognitive linguistics, or language andvision research. Object naming datasets, which consist of concept lists withpicture pairings, are used to gain insights into how humans access and selectnames for objects in their surroundings and to study the cognitive processesinvolved in converting visual stimuli into semantic concepts. Unfortunately,object naming datasets often lack transparency and have a highly idiosyncraticstructure. Our study tries to make current object naming data transparent andcomparable by using a multilingual, computer-assisted approach that linksindividual items of object naming lists to unified concepts. Our current samplelinks 17 object naming datasets that cover 30 languages from 10 differentlanguage families. We illustrate how the comparative dataset can be explored bysearching for concepts that recur across the majority of datasets and comparingthe conceptual spaces of covered object naming datasets with classical basicvocabulary lists from historical linguistics and linguistic typology. Ourfindings can serve as a basis for enhancing cross-linguistic object namingresearch and as a guideline for future studies dealing with object namingtasks.</description><author>Alžběta Kučerová, Johann-Mattis List</author><pubDate>Tue, 14 Jan 2025 18:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08312v1</guid></item><item><title>Path Loss Prediction Using Machine Learning with Extended Features</title><link>http://arxiv.org/abs/2501.08306v1</link><description>Wireless communications rely on path loss modeling, which is most effectivewhen it includes the physical details of the propagation environment. Acquiringthis data has historically been challenging, but geographic information systemdata is becoming increasingly available with higher resolution and accuracy.Access to such details enables propagation models to more accurately predictcoverage and minimize interference in wireless deployments. Machinelearning-based modeling can significantly support this effort, withfeature-based approaches allowing for accurate, efficient, and scalablepropagation modeling. Building on previous work, we introduce an extended setof features that improves prediction accuracy while, most importantly,maintaining model generalization across a broad range of environments.</description><author>Jonathan Ethier, Mathieu Chateauvert, Ryan G. Dempsey, Alexis Bose</author><pubDate>Tue, 14 Jan 2025 18:44:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08306v1</guid></item><item><title>Benchmarking Graph Representations and Graph Neural Networks for Multivariate Time Series Classification</title><link>http://arxiv.org/abs/2501.08305v1</link><description>Multivariate Time Series Classification (MTSC) enables the analysis ifcomplex temporal data, and thus serves as a cornerstone in various real-worldapplications, ranging from healthcare to finance. Since the relationship amongvariables in MTS usually contain crucial cues, a large number of graph-basedMTSC approaches have been proposed, as the graph topology and edges canexplicitly represent relationships among variables (channels), where not onlyvarious MTS graph representation learning strategies but also different GraphNeural Networks (GNNs) have been explored. Despite such progresses, there is nocomprehensive study that fairly benchmarks and investigates the performances ofexisting widely-used graph representation learning strategies/GNN classifiersin the application of different MTSC tasks. In this paper, we present the firstbenchmark which systematically investigates the effectiveness of thewidely-used three node feature definition strategies, four edge featurelearning strategies and five GNN architecture, resulting in 60 differentvariants for graph-based MTSC. These variants are developed and evaluated witha standardized data pipeline and training/validation/testing strategy on 26widely-used suspensor MTSC datasets. Our experiments highlight that nodefeatures significantly influence MTSC performance, while the visualization ofedge features illustrates why adaptive edge learning outperforms other edgefeature learning methods. The code of the proposed benchmark is publiclyavailable at\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.</description><author>Wennuo Yang, Shiling Wu, Yuzhi Zhou, Weicheng Xie, Linlin Shen, Siyang Song</author><pubDate>Tue, 14 Jan 2025 18:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08305v1</guid></item><item><title>Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers</title><link>http://arxiv.org/abs/2501.08303v1</link><description>Semantic future prediction is important for autonomous systems navigatingdynamic environments. This paper introduces FUTURIST, a method for multimodalfuture semantic prediction that uses a unified and efficient visual sequencetransformer architecture. Our approach incorporates a multimodal masked visualmodeling objective and a novel masking mechanism designed for multimodaltraining. This allows the model to effectively integrate visible informationfrom various modalities, improving prediction accuracy. Additionally, wepropose a VAE-free hierarchical tokenization process, which reducescomputational complexity, streamlines the training pipeline, and enablesend-to-end training with high-resolution, multimodal inputs. We validateFUTURIST on the Cityscapes dataset, demonstrating state-of-the-art performancein future semantic segmentation for both short- and mid-term forecasting. Weprovide the implementation code at https://github.com/Sta8is/FUTURIST .</description><author>Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis</author><pubDate>Tue, 14 Jan 2025 18:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08303v1</guid></item><item><title>Polynomial Threshold Functions of Bounded Tree-Width: Some Explainability and Complexity Aspects</title><link>http://arxiv.org/abs/2501.08297v1</link><description>The tree-width of a multivariate polynomial is the tree-width of thehypergraph with hyperedges corresponding to its terms. Multivariate polynomialsof bounded tree-width have been studied by Makowsky and Meer as a new sparsitycondition that allows for polynomial solvability of problems which areintractable in general. We consider a variation on this theme for Booleanvariables. A representation of a Boolean function as the sign of a polynomialis called a polynomial threshold representation. We discuss Boolean functionsrepresentable as polynomial threshold functions of bounded tree-width andpresent two applications to Bayesian network classifiers, a probabilisticgraphical model. Both applications are in Explainable Artificial Intelligence(XAI), the research area dealing with the black-box nature of many recentmachine learning models. We also give a separation result between therepresentational power of positive and general polynomial threshold functions.</description><author>Karine Chubarian, Johnny Joyce, Gyorgy Turan</author><pubDate>Tue, 14 Jan 2025 18:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08297v1</guid></item><item><title>A Survey on Pedophile Attribution Techniques for Online Platforms</title><link>http://arxiv.org/abs/2501.08296v1</link><description>Reliance on anonymity in social media has increased its popularity on theseplatforms among all ages. The availability of public Wi-Fi networks hasfacilitated a vast variety of online content, including social mediaapplications. Although anonymity and ease of access can be a convenient meansof communication for their users, it is difficult to manage and protect itsvulnerable users against sexual predators. Using an automated identificationsystem that can attribute predators to their text would make the solution moreattainable. In this survey, we provide a review of the methods of pedophileattribution used in social media platforms. We examine the effect of the sizeof the suspect set and the length of the text on the task of attribution.Moreover, we review the most-used datasets, features, classification techniquesand performance measures for attributing sexual predators. We found that fewstudies have proposed tools to mitigate the risk of online sexual predators,but none of them can provide suspect attribution. Finally, we list several openresearch problems.</description><author>Hiba Fallatah, Ching Suen, Olga Ormandjieva</author><pubDate>Tue, 14 Jan 2025 18:25:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08296v1</guid></item><item><title>LayerAnimate: Layer-specific Control for Animation</title><link>http://arxiv.org/abs/2501.08295v1</link><description>Animated video separates foreground and background elements into layers, withdistinct processes for sketching, refining, coloring, and in-betweening.Existing video generation methods typically treat animation as a monolithicdata domain, lacking fine-grained control over individual layers. In thispaper, we introduce LayerAnimate, a novel architectural approach that enhancesfine-grained control over individual animation layers within a video diffusionmodel, allowing users to independently manipulate foreground and backgroundelements in distinct layers. To address the challenge of limited layer-specificdata, we propose a data curation pipeline that features automated elementsegmentation, motion-state hierarchical merging, and motion coherencerefinement. Through quantitative and qualitative comparisons, and user study,we demonstrate that LayerAnimate outperforms current methods in terms ofanimation quality, control precision, and usability, making it an ideal toolfor both professional animators and amateur enthusiasts. This framework opensup new possibilities for layer-specific animation applications and creativeflexibility. Our code is available at https://layeranimate.github.io.</description><author>Yuxue Yang, Lue Fan, Zuzen Lin, Feng Wang, Zhaoxiang Zhang</author><pubDate>Tue, 14 Jan 2025 18:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08295v1</guid></item><item><title>Gaussian Eigen Models for Human Heads</title><link>http://arxiv.org/abs/2407.04545v2</link><description>Current personalized neural head avatars face a trade-off: lightweight modelslack detail and realism, while high-quality, animatable avatars requiresignificant computational resources, making them unsuitable for commoditydevices. To address this gap, we introduce Gaussian Eigen Models (GEM), whichprovide high-quality, lightweight, and easily controllable head avatars. GEMutilizes 3D Gaussian primitives for representing the appearance combined withGaussian splatting for rendering. Building on the success of mesh-based 3Dmorphable face models (3DMM), we define GEM as an ensemble of linear eigenbasesfor representing the head appearance of a specific subject. In particular, weconstruct linear bases to represent the position, scale, rotation, and opacityof the 3D Gaussians. This allows us to efficiently generate Gaussian primitivesof a specific head shape by a linear combination of the basis vectors, onlyrequiring a low-dimensional parameter vector that contains the respectivecoefficients. We propose to construct these linear bases (GEM) by distillinghigh-quality compute-intense CNN-based Gaussian avatar models that can generateexpression-dependent appearance changes like wrinkles. These high-qualitymodels are trained on multi-view videos of a subject and are distilled using aseries of principal component analyses. Once we have obtained the bases thatrepresent the animatable appearance space of a specific human, we learn aregressor that takes a single RGB image as input and predicts thelow-dimensional parameter vector that corresponds to the shown facialexpression. In a series of experiments, we compare GEM's self-reenactment andcross-person reenactment results to state-of-the-art 3D avatar methods,demonstrating GEM's higher visual quality and better generalization to newexpressions.</description><author>Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies</author><pubDate>Tue, 14 Jan 2025 18:20:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.04545v2</guid></item><item><title>HALoGEN: Fantastic LLM Hallucinations and Where to Find Them</title><link>http://arxiv.org/abs/2501.08292v1</link><description>Despite their impressive ability to generate high-quality and fluent text,generative large language models (LLMs) also produce hallucinations: statementsthat are misaligned with established world knowledge or provided input context.However, measuring hallucination can be challenging, as having humans verifymodel generations on-the-fly is both expensive and time-consuming. In thiswork, we release HALoGEN, a comprehensive hallucination benchmark consistingof: (1) 10,923 prompts for generative models spanning nine domains includingprogramming, scientific attribution, and summarization, and (2) automatichigh-precision verifiers for each use case that decompose LLM generations intoatomic units, and verify each unit against a high-quality knowledge source. Weuse this framework to evaluate ~150,000 generations from 14 language models,finding that even the best-performing models are riddled with hallucinations(sometimes up to 86% of generated atomic facts depending on the domain). Wefurther define a novel error classification for LLM hallucinations based onwhether they likely stem from incorrect recollection of training data (Type Aerrors), or incorrect knowledge in training data (Type B errors), or arefabrication (Type C errors). We hope our framework provides a foundation toenable the principled study of why generative models hallucinate, and advancesthe development of trustworthy large language models.</description><author>Abhilasha Ravichander, Shrusti Ghela, David Wadden, Yejin Choi</author><pubDate>Tue, 14 Jan 2025 18:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08292v1</guid></item><item><title>Avoiding subtraction and division of stochastic signals using normalizing flows: NFdeconvolve</title><link>http://arxiv.org/abs/2501.08288v1</link><description>Across the scientific realm, we find ourselves subtracting or dividingstochastic signals. For instance, consider a stochastic realization, $x$,generated from the addition or multiplication of two stochastic signals $a$ and$b$, namely $x=a+b$ or $x = ab$. For the $x=a+b$ example, $a$ can befluorescence background and $b$ the signal of interest whose statistics are tobe learned from the measured $x$. Similarly, when writing $x=ab$, $a$ can bethought of as the illumination intensity and $b$ the density of fluorescentmolecules of interest. Yet dividing or subtracting stochastic signals amplifiesnoise, and we ask instead whether, using the statistics of $a$ and themeasurement of $x$ as input, we can recover the statistics of $b$. Here, weshow how normalizing flows can generate an approximation of the probabilitydistribution over $b$, thereby avoiding subtraction or division altogether.This method is implemented in our software package, NFdeconvolve, available onGitHub with a tutorial linked in the main text.</description><author>Pedro Pessoa, Max Schweiger, Lance W. Q. Xu, Tristan Manha, Ayush Saurabh, Julian Antolin Camarena, Steve Pressé</author><pubDate>Tue, 14 Jan 2025 18:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08288v1</guid></item><item><title>A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems using Disparity Maps</title><link>http://arxiv.org/abs/2410.24031v2</link><description>Face recognition technologies are increasingly used in various applications,yet they are vulnerable to face spoofing attacks. These spoofing attacks ofteninvolve unique 3D structures, such as printed papers or mobile device screens.Although stereo-depth cameras can detect such attacks effectively, theirhigh-cost limits their widespread adoption. Conversely, two-sensor systemswithout extrinsic calibration offer a cost-effective alternative but are unableto calculate depth using stereo techniques. In this work, we propose a methodto overcome this challenge by leveraging facial attributes to derive disparityinformation and estimate relative depth for anti-spoofing purposes, usingnon-calibrated systems. We introduce a multi-modal anti-spoofing model, coinedDisparity Model, that incorporates created disparity maps as a third modalityalongside the two original sensor modalities. We demonstrate the effectivenessof the Disparity Model in countering various spoof attacks using acomprehensive dataset collected from the Intel RealSense ID Solution F455. Ourmethod outperformed existing methods in the literature, achieving an EqualError Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a FalsePositive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than theerrors of the best comparison method, respectively. Additionally, we introducea model ensemble that addresses 3D spoof attacks as well, achieving an EER of2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides astate-of-the-art solution for the challenging task of anti-spoofing innon-calibrated systems that lack depth information.</description><author>Ariel Larey, Eyal Rond, Omer Achrack</author><pubDate>Tue, 14 Jan 2025 18:03:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24031v2</guid></item><item><title>VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes</title><link>http://arxiv.org/abs/2501.08286v1</link><description>VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM frameworkdesigned for large scenes. The framework comprises four main components: VIOFront End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIOFront End, RGB frames are processed through dense bundle adjustment anduncertainty estimation to extract scene geometry and poses. Based on thisoutput, the mapping module incrementally constructs and maintains a 2D Gaussianmap. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,Score Manager, and Pose Refinement, which collectively improve mapping speedand localization accuracy. This enables the SLAM system to handle large-scaleurban environments with up to 50 million Gaussian ellipsoids. To ensure globalconsistency in large-scale scenes, we design a Loop Closure module, whichinnovatively leverages the Novel View Synthesis (NVS) capabilities of GaussianSplatting for loop closure detection and correction of the Gaussian map.Additionally, we propose a Dynamic Eraser to address the inevitable presence ofdynamic objects in real-world outdoor scenes. Extensive evaluations in indoorand outdoor environments demonstrate that our approach achieves localizationperformance on par with Visual-Inertial Odometry while surpassing recentGS/NeRF SLAM methods. It also significantly outperforms all existing methods interms of mapping and rendering quality. Furthermore, we developed a mobile appand verified that our framework can generate high-quality Gaussian maps in realtime using only a smartphone camera and a low-frequency IMU sensor. To the bestof our knowledge, VINGS-Mono is the first monocular Gaussian SLAM methodcapable of operating in outdoor environments and supporting kilometer-scalelarge scenes.</description><author>Ke Wu, Zicheng Zhang, Muer Tie, Ziqing Ai, Zhongxue Gan, Wenchao Ding</author><pubDate>Tue, 14 Jan 2025 18:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08286v1</guid></item><item><title>Can Bayesian Neural Networks Explicitly Model Input Uncertainty?</title><link>http://arxiv.org/abs/2501.08285v1</link><description>Inputs to machine learning models can have associated noise or uncertainties,but they are often ignored and not modelled. It is unknown if Bayesian NeuralNetworks and their approximations are able to consider uncertainty in theirinputs. In this paper we build a two input Bayesian Neural Network (mean andstandard deviation) and evaluate its capabilities for input uncertaintyestimation across different methods like Ensembles, MC-Dropout, and Flipout.Our results indicate that only some uncertainty estimation methods forapproximate Bayesian NNs can model input uncertainty, in particular Ensemblesand Flipout.</description><author>Matias Valdenegro-Toro, Marco Zullich</author><pubDate>Tue, 14 Jan 2025 18:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08285v1</guid></item><item><title>AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages</title><link>http://arxiv.org/abs/2501.08284v1</link><description>Hate speech and abusive language are global phenomena that needsocio-cultural background knowledge to be understood, identified, andmoderated. However, in many regions of the Global South, there have beenseveral documented occurrences of (1) absence of moderation and (2) censorshipdue to the reliance on keyword spotting out of context. Further, high-profileindividuals have frequently been at the center of the moderation process, whilelarge and targeted hate speech campaigns against minorities have beenoverlooked. These limitations are mainly due to the lack of high-quality datain the local languages and the failure to include local communities in thecollection, annotation, and moderation processes. To address this issue, wepresent AfriHate: a multilingual collection of hate speech and abusive languagedatasets in 15 African languages. Each instance in AfriHate is annotated bynative speakers familiar with the local culture. We report the challengesrelated to the construction of the datasets and present various classificationbaseline results with and without using LLMs. The datasets, individualannotations, and hate speech and offensive language lexicons are available onhttps://github.com/AfriHate/AfriHate</description><author>Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Saminu Mohammad Aliyu, Nelson Odhiambo Onyango, Lilian D. A. Wanzare, Samuel Rutunda, Lukman Jibril Aliyu, Esubalew Alemneh, Oumaima Hourrane, Hagos Tesfahun Gebremichael, Elyas Abdi Ismail, Meriem Beloucif, Ebrahim Chekol Jibril, Andiswa Bukula, Rooweither Mabuya, Salomey Osei, Abigail Oppong, Tadesse Destaw Belay, Tadesse Kebede Guge, Tesfa Tegegne Asfaw, Chiamaka Ijeoma Chukwuneke, Paul Röttger, Seid Muhie Yimam, Nedjma Ousidhoum</author><pubDate>Tue, 14 Jan 2025 18:00:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08284v1</guid></item><item><title>LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding</title><link>http://arxiv.org/abs/2501.08282v1</link><description>Recent advancements in multimodal large language models (MLLMs) have shownpromising results, yet existing approaches struggle to effectively handle bothtemporal and spatial localization simultaneously. This challenge stems from twokey issues: first, incorporating spatial-temporal localization introduces avast number of coordinate combinations, complicating the alignment oflinguistic and visual coordinate representations; second, encoding fine-grainedtemporal and spatial information during video feature compression is inherentlydifficult. To address these issues, we propose LLaVA-ST, a MLLM forfine-grained spatial-temporal multimodal understanding. In LLaVA-ST, we proposeLanguage-Aligned Positional Embedding, which embeds the textual coordinatespecial token into the visual space, simplifying the alignment of fine-grainedspatial-temporal correspondences. Additionally, we design the Spatial-TemporalPacker, which decouples the feature compression of temporal and spatialresolutions into two distinct point-to-region attention processing streams.Furthermore, we propose ST-Align dataset with 4.3M training samples forfine-grained spatial-temporal multimodal understanding. With ST-align, wepresent a progressive training pipeline that aligns the visual and textualfeature through sequential coarse-to-fine stages.Additionally, we introduce anST-Align benchmark to evaluate spatial-temporal interleaved fine-grainedunderstanding tasks, which include Spatial-Temporal Video Grounding (STVG) ,Event Localization and Captioning (ELC) and Spatial Video Grounding (SVG).LLaVA-ST achieves outstanding performance on 11 benchmarks requiringfine-grained temporal, spatial, or spatial-temporal interleaving multimodalunderstanding. Our code, data and benchmark will be released at Our code, dataand benchmark will be released at https://github.com/appletea233/LLaVA-ST .</description><author>Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, Si Liu</author><pubDate>Tue, 14 Jan 2025 17:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08282v1</guid></item><item><title>Decoding Interpretable Logic Rules from Neural Networks</title><link>http://arxiv.org/abs/2501.08281v1</link><description>As deep neural networks continue to excel across various domains, theirblack-box nature has raised concerns about transparency and trust. Inparticular, interpretability has become increasingly essential for applicationsthat demand high safety and knowledge rigor, such as drug discovery, autonomousdriving, and genomics. However, progress in understanding even the simplestdeep neural networks - such as fully connected networks - has been limited,despite their role as foundational elements in state-of-the-art models likeResNet and Transformer. In this paper, we address this challenge by introducingNeuroLogic, a novel approach for decoding interpretable logic rules from neuralnetworks. NeuroLogic leverages neural activation patterns to capture themodel's critical decision-making processes, translating them into logical rulesrepresented by hidden predicates. Thanks to its flexible design in thegrounding phase, NeuroLogic can be adapted to a wide range of neural networks.For simple fully connected neural networks, hidden predicates can be groundedin certain split patterns of original input features to derivedecision-tree-like rules. For large, complex vision neural networks, NeuroLogicgrounds hidden predicates into high-level visual concepts that areunderstandable to humans. Our empirical study demonstrates that NeuroLogic canextract global and interpretable rules from state-of-the-art models such asResNet, a task at which existing work struggles. We believe NeuroLogic can helppave the way for understanding the black-box nature of neural networks.</description><author>Chuqin Geng, Xiaojie Xu, Zhaoyue Wang, Ziyu Zhao, Xujie Si</author><pubDate>Tue, 14 Jan 2025 17:57:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08281v1</guid></item><item><title>SmartEraser: Remove Anything from Images using Masked-Region Guidance</title><link>http://arxiv.org/abs/2501.08279v1</link><description>Object removal has so far been dominated by the mask-and-inpaint paradigm,where the masked region is excluded from the input, leaving models relying onunmasked areas to inpaint the missing region. However, this approach lackscontextual information for the masked area, often resulting in unstableperformance. In this work, we introduce SmartEraser, built with a new removingparadigm called Masked-Region Guidance. This paradigm retains the masked regionin the input, using it as guidance for the removal process. It offers severaldistinct advantages: (a) it guides the model to accurately identify the objectto be removed, preventing its regeneration in the output; (b) since the usermask often extends beyond the object itself, it aids in preserving thesurrounding context in the final result. Leveraging this new paradigm, wepresent Syn4Removal, a large-scale object removal dataset, where instancesegmentation data is used to copy and paste objects onto images as removaltargets, with the original images serving as ground truths. Experimentalresults demonstrate that SmartEraser significantly outperforms existingmethods, achieving superior performance in object removal, especially incomplex scenes with intricate compositions.</description><author>Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, Houqiang Li</author><pubDate>Tue, 14 Jan 2025 17:55:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08279v1</guid></item><item><title>Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax</title><link>http://arxiv.org/abs/2410.06993v2</link><description>Deep InfoMax (DIM) is a well-established method for self-supervisedrepresentation learning (SSRL) based on maximization of the mutual informationbetween the input and the output of a deep neural network encoder. Despite theDIM and contrastive SSRL in general being well-explored, the task of learningrepresentations conforming to a specific distribution (i.e., distributionmatching, DM) is still under-addressed. Motivated by the importance of DM toseveral downstream tasks (including generative modeling, disentanglement,outliers detection and other), we enhance DIM to enable automatic matching oflearned representations to a selected prior distribution. To achieve this, wepropose injecting an independent noise into the normalized outputs of theencoder, while keeping the same InfoMax training objective. We show that suchmodification allows for learning uniformly and normally distributedrepresentations, as well as representations of other absolutely continuousdistributions. Our approach is tested on various downstream tasks. The resultsindicate a moderate trade-off between the performance on the downstream tasksand quality of DM.</description><author>Ivan Butakov, Alexander Semenenko, Alexander Tolmachev, Andrey Gladkov, Marina Munkhoeva, Alexey Frolov</author><pubDate>Tue, 14 Jan 2025 17:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06993v2</guid></item><item><title>Exploring Robustness of LLMs to Sociodemographically-Conditioned Paraphrasing</title><link>http://arxiv.org/abs/2501.08276v1</link><description>Large Language Models (LLMs) have shown impressive performance in various NLPtasks. However, there are concerns about their reliability in different domainsof linguistic variations. Many works have proposed robustness evaluationmeasures for local adversarial attacks, but we need globally robust modelsunbiased to different language styles. We take a broader approach to explore awider range of variations across sociodemographic dimensions to performstructured reliability tests on the reasoning capacity of language models. Weextend the SocialIQA dataset to create diverse paraphrased sets conditioned onsociodemographic styles. The assessment aims to provide a deeper understandingof LLMs in (a) their capability of generating demographic paraphrases withengineered prompts and (b) their reasoning capabilities in real-world, complexlanguage scenarios. We also explore measures such as perplexity,explainability, and ATOMIC performance of paraphrases for fine-grainedreliability analysis of LLMs on these sets. We find that demographic-specificparaphrasing significantly impacts the performance of language models,indicating that the subtleties of language variations remain a significantchallenge. The code and dataset will be made available for reproducibility andfuture research.</description><author>Pulkit Arora, Akbar Karimi, Lucie Flek</author><pubDate>Tue, 14 Jan 2025 17:50:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08276v1</guid></item><item><title>RMem: Restricted Memory Banks Improve Video Object Segmentation</title><link>http://arxiv.org/abs/2406.08476v2</link><description>With recent video object segmentation (VOS) benchmarks evolving tochallenging scenarios, we revisit a simple but overlooked strategy: restrictingthe size of memory banks. This diverges from the prevalent practice ofexpanding memory banks to accommodate extensive historical information. Ourspecially designed "memory deciphering" study offers a pivotal insightunderpinning such a strategy: expanding memory banks, while seeminglybeneficial, actually increases the difficulty for VOS modules to decoderelevant features due to the confusion from redundant information. Byrestricting memory banks to a limited number of essential frames, we achieve anotable improvement in VOS accuracy. This process balances the importance andfreshness of frames to maintain an informative memory bank within a boundedcapacity. Additionally, restricted memory banks reduce the training-inferencediscrepancy in memory lengths compared with continuous expansion. This fostersnew opportunities in temporal reasoning and enables us to introduce thepreviously overlooked "temporal positional embedding." Finally, our insightsare embodied in "RMem" ("R" for restricted), a simple yet effective VOSmodification that excels at challenging VOS scenarios and establishes new stateof the art for object state changes (on the VOST dataset) and long videos (onthe Long Videos dataset). Our code and demo are available athttps://restricted-memory.github.io/.</description><author>Junbao Zhou, Ziqi Pang, Yu-Xiong Wang</author><pubDate>Tue, 14 Jan 2025 17:46:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08476v2</guid></item><item><title>Comparative Analysis of Efficient Adapter-Based Fine-Tuning of State-of-the-Art Transformer Models</title><link>http://arxiv.org/abs/2501.08271v1</link><description>In this work, we investigate the efficacy of various adapter architectures onsupervised binary classification tasks from the SuperGLUE benchmark as well asa supervised multi-class news category classification task from Kaggle.Specifically, we compare classification performance and time complexity ofthree transformer models, namely DistilBERT, ELECTRA, and BART, usingconventional fine-tuning as well as nine state-of-the-art (SoTA) adapterarchitectures. Our analysis reveals performance differences across adapterarchitectures, highlighting their ability to achieve comparable or betterperformance relative to fine-tuning at a fraction of the training time. Similarresults are observed on the new classification task, further supporting ourfindings and demonstrating adapters as efficient and flexible alternatives tofine-tuning. This study provides valuable insights and guidelines for selectingand implementing adapters in diverse natural language processing (NLP)applications.</description><author>Saad Mashkoor Siddiqui, Mohammad Ali Sheikh, Muhammad Aleem, Kajol R Singh</author><pubDate>Tue, 14 Jan 2025 17:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08271v1</guid></item><item><title>FaVoR: Features via Voxel Rendering for Camera Relocalization</title><link>http://arxiv.org/abs/2409.07571v3</link><description>Camera relocalization methods range from dense image alignment to directcamera pose regression from a query image. Among these, sparse feature matchingstands out as an efficient, versatile, and generally lightweight approach withnumerous applications. However, feature-based methods often struggle withsignificant viewpoint and appearance changes, leading to matching failures andinaccurate pose estimates. To overcome this limitation, we propose a novelapproach that leverages a globally sparse yet locally dense 3D representationof 2D features. By tracking and triangulating landmarks over a sequence offrames, we construct a sparse voxel map optimized to render image patchdescriptors observed during tracking. Given an initial pose estimate, we firstsynthesize descriptors from the voxels using volumetric rendering and thenperform feature matching to estimate the camera pose. This methodology enablesthe generation of descriptors for unseen views, enhancing robustness to viewchanges. We extensively evaluate our method on the 7-Scenes and CambridgeLandmarks datasets. Our results show that our method significantly outperformsexisting state-of-the-art feature representation techniques in indoorenvironments, achieving up to a 39% improvement in median translation error.Additionally, our approach yields comparable results to other methods foroutdoor scenarios while maintaining lower memory and computational costs.</description><author>Vincenzo Polizzi, Marco Cannici, Davide Scaramuzza, Jonathan Kelly</author><pubDate>Tue, 14 Jan 2025 17:33:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07571v3</guid></item><item><title>Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation</title><link>http://arxiv.org/abs/2501.06693v2</link><description>Sim-to-real gap has long posed a significant challenge for robot learning insimulation, preventing the deployment of learned models in the real world.Previous work has primarily focused on domain randomization and systemidentification to mitigate this gap. However, these methods are often limitedby the inherent constraints of the simulation and graphics engines. In thiswork, we propose Vid2Sim, a novel framework that effectively bridges thesim2real gap through a scalable and cost-efficient real2sim pipeline for neural3D scene reconstruction and simulation. Given a monocular video as input,Vid2Sim can generate photorealistic and physically interactable 3D simulationenvironments to enable the reinforcement learning of visual navigation agentsin complex urban environments. Extensive experiments demonstrate that Vid2Simsignificantly improves the performance of urban navigation in the digital twinsand real world by 31.2% and 68.3% in success rate compared with agents trainedwith prior simulation methods.</description><author>Ziyang Xie, Zhizheng Liu, Zhenghao Peng, Wayne Wu, Bolei Zhou</author><pubDate>Tue, 14 Jan 2025 17:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06693v2</guid></item><item><title>AI Driven Water Segmentation with deep learning models for Enhanced Flood Monitoring</title><link>http://arxiv.org/abs/2501.08266v1</link><description>Flooding is a major natural hazard causing significant fatalities andeconomic losses annually, with increasing frequency due to climate change.Rapid and accurate flood detection and monitoring are crucial for mitigatingthese impacts. This study compares the performance of three deep learningmodels UNet, ResNet, and DeepLabv3 for pixelwise water segmentation to aid inflood detection, utilizing images from drones, in field observations, andsocial media. This study involves creating a new dataset that augmentswellknown benchmark datasets with flood-specific images, enhancing therobustness of the models. The UNet, ResNet, and DeepLab v3 architectures aretested to determine their effectiveness in various environmental conditions andgeographical locations, and the strengths and limitations of each model arealso discussed here, providing insights into their applicability in differentscenarios by predicting image segmentation masks. This fully automated approachallows these models to isolate flooded areas in images, significantly reducingprocessing time compared to traditional semi-automated methods. The outcome ofthis study is to predict segmented masks for each image effected by a flooddisaster and the validation accuracy of these models. This methodologyfacilitates timely and continuous flood monitoring, providing vital data foremergency response teams to reduce loss of life and economic damages. It offersa significant reduction in the time required to generate flood maps, cuttingdown the manual processing time. Additionally, we present avenues for futureresearch, including the integration of multimodal data sources and thedevelopment of robust deep learning architectures tailored specifically forflood detection tasks. Overall, our work contributes to the advancement offlood management strategies through innovative use of deep learningtechnologies.</description><author>Sanjida Afrin Mou, Tasfia Noor Chowdhury, Adib Ibn Mannan, Sadia Nourin Mim, Lubana Tarannum, Tasrin Noman, Jamal Uddin Ahamed</author><pubDate>Tue, 14 Jan 2025 17:26:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08266v1</guid></item><item><title>Multiplayer Federated Learning: Reaching Equilibrium with Less Communication</title><link>http://arxiv.org/abs/2501.08263v1</link><description>Traditional Federated Learning (FL) approaches assume collaborative clientswith aligned objectives working towards a shared global model. However, in manyreal-world scenarios, clients act as rational players with individualobjectives and strategic behaviors, a concept that existing FL frameworks arenot equipped to adequately address. To bridge this gap, we introduceMultiplayer Federated Learning (MpFL), a novel framework that models theclients in the FL environment as players in a game-theoretic context, aiming toreach an equilibrium. In this scenario, each player tries to optimize their ownutility function, which may not align with the collective goal. Within MpFL, wepropose Per-Player Local Stochastic Gradient Descent (PEARL-SGD), an algorithmin which each player/client performs local updates independently andperiodically communicates with other players. We theoretically analyzePEARL-SGD and prove that it reaches a neighborhood of equilibrium with lesscommunication in the stochastic setup compared to its non-local counterpart.Finally, we verify our theoretical findings through numerical experiments.</description><author>TaeHo Yoon, Sayantan Choudhury, Nicolas Loizou</author><pubDate>Tue, 14 Jan 2025 17:23:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08263v1</guid></item><item><title>CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation</title><link>http://arxiv.org/abs/2410.02748v3</link><description>Existing automatic prompt engineering methods are typically designed fordiscriminative tasks, where new task prompts are iteratively refined withlimited feedback from a single metric reflecting a single aspect. However,these approaches are suboptimal for generative tasks, which require morenuanced guidance beyond a single numeric metric to improve the prompt andoptimize multiple aspects of the generated text. To address these challenges,we propose a novel multi-aspect Critique-Suggestion-guided automatic PromptOptimization (CriSPO) approach. CriSPO introduces a critique-suggestion moduleas its core component. This module spontaneously discovers aspects, andcompares generated and reference texts across these aspects, providing specificsuggestions for prompt modification. These clear critiques and actionablesuggestions guide a receptive optimizer module to make more substantialchanges, exploring a broader and more effective search space. To furtherimprove CriSPO with multi-metric optimization, we introduce an Automatic SuffixTuning (AST) extension to enhance the performance of task prompts acrossmultiple metrics. We evaluate CriSPO on 4 state-of-the-art LLMs across 4summarization and 5 QA datasets. Extensive experiments show 3-4% ROUGE scoreimprovement on summarization and substantial improvement of various metrics onQA. Code available at https://github.com/amazon-science/crispo</description><author>Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff</author><pubDate>Tue, 14 Jan 2025 17:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02748v3</guid></item><item><title>FDPP: Fine-tune Diffusion Policy with Human Preference</title><link>http://arxiv.org/abs/2501.08259v1</link><description>Imitation learning from human demonstrations enables robots to performcomplex manipulation tasks and has recently witnessed huge success. However,these techniques often struggle to adapt behavior to new preferences or changesin the environment. To address these limitations, we propose Fine-tuningDiffusion Policy with Human Preference (FDPP). FDPP learns a reward functionthrough preference-based learning. This reward is then used to fine-tune thepre-trained policy with reinforcement learning (RL), resulting in alignment ofpre-trained policy with new human preferences while still solving the originaltask. Our experiments across various robotic tasks and preferences demonstratethat FDPP effectively customizes policy behavior without compromisingperformance. Additionally, we show that incorporating Kullback-Leibler (KL)regularization during fine-tuning prevents over-fitting and helps maintain thecompetencies of the initial policy.</description><author>Yuxin Chen, Devesh K. Jha, Masayoshi Tomizuka, Diego Romeres</author><pubDate>Tue, 14 Jan 2025 17:15:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08259v1</guid></item><item><title>Towards an End-to-End (E2E) Adversarial Learning and Application in the Physical World</title><link>http://arxiv.org/abs/2501.08258v1</link><description>The traditional learning process of patch-based adversarial attacks,conducted in the digital domain and then applied in the physical domain (e.g.,via printed stickers), may suffer from reduced performance due to adversarialpatches' limited transferability from the digital domain to the physicaldomain. Given that previous studies have considered using projectors to applyadversarial attacks, we raise the following question: can adversarial learning(i.e., patch generation) be performed entirely in the physical domain with aprojector? In this work, we propose the Physical-domain Adversarial PatchLearning Augmentation (PAPLA) framework, a novel end-to-end (E2E) frameworkthat converts adversarial learning from the digital domain to the physicaldomain using a projector. We evaluate PAPLA across multiple scenarios,including controlled laboratory settings and realistic outdoor environments,demonstrating its ability to ensure attack success compared to conventionaldigital learning-physical application (DL-PA) methods. We also analyze theimpact of environmental factors, such as projection surface color, projectorstrength, ambient light, distance, and angle of the target object relative tothe camera, on the effectiveness of projected patches. Finally, we demonstratethe feasibility of the attack against a parked car and a stop sign in areal-world outdoor environment. Our results show that under specificconditions, E2E adversarial learning in the physical domain eliminates thetransferability issue and ensures evasion by object detectors. Finally, weprovide insights into the challenges and opportunities of applying adversariallearning in the physical domain and explain where such an approach is moreeffective than using a sticker.</description><author>Dudi Biton, Jacob Shams, Koda Satoru, Asaf Shabtai, Yuval Elovici, Ben Nassi</author><pubDate>Tue, 14 Jan 2025 17:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08258v1</guid></item><item><title>Automated Detection and Analysis of Minor Deformations in Flat Walls Due to Railway Vibrations Using LiDAR and Machine Learning</title><link>http://arxiv.org/abs/2501.06457v2</link><description>This study introduces an advanced methodology for automatically identifyingminor deformations in flat walls caused by vibrations from nearby railwaytracks. It leverages high-density Terrestrial Laser Scanner (TLS) LiDAR surveysand AI/ML techniques to collect and analyze data. The scan data is processedinto a detailed point cloud, which is segmented to distinguish ground points,trees, buildings, and other objects. The analysis focuses on identifyingsections along flat walls and estimating their deformations relative to theground orientation. Findings from the study, conducted at the RGIPT campus, reveal significantdeformations in walls close to the railway corridor, with the highestdeformations ranging from 7 to 8 cm and an average of 3 to 4 cm. In contrast,walls further from the corridor show negligible deformations. The developedautomated process for feature extraction and deformation monitoringdemonstrates potential for structural health monitoring. By integrating LiDARdata with machine learning, the methodology provides an efficient system foridentifying and analyzing structural deformations, highlighting the importanceof continuous monitoring for ensuring structural integrity and public safety inurban infrastructure. This approach represents a substantial advancement inautomated feature extraction and deformation analysis, contributing to moreeffective management of urban infrastructure.</description><author>Surjo Dey, Ankit Sharma, Hritu Raj, Susham Biswas</author><pubDate>Tue, 14 Jan 2025 16:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06457v2</guid></item><item><title>Language-Agnostic Modeling of Source Reliability on Wikipedia</title><link>http://arxiv.org/abs/2410.18803v2</link><description>Over the last few years, content verification through reliable sources hasbecome a fundamental need to combat disinformation. Here, we present alanguage-agnostic model designed to assess the reliability of sources acrossmultiple language editions of Wikipedia. Utilizing editorial activity data, themodel evaluates source reliability within different articles of varyingcontroversiality such as Climate Change, COVID-19, History, Media, and Biologytopics. Crafting features that express domain usage across articles, the modeleffectively predicts source reliability, achieving an F1 Macro score ofapproximately 0.80 for English and other high-resource languages. Formid-resource languages, we achieve 0.65 while the performance of low-resourcelanguages varies; in all cases, the time the domain remains present in thearticles (which we dub as permanence) is one of the most predictive features.We highlight the challenge of maintaining consistent model performance acrosslanguages of varying resource levels and demonstrate that adapting models fromhigher-resource languages can improve performance. This work contributes notonly to Wikipedia's efforts in ensuring content verifiability but in ensuringreliability across diverse user-generated content in various languagecommunities.</description><author>Jacopo D'Ignazi, Andreas Kaltenbrunner, Yelena Mejova, Michele Tizzani, Kyriaki Kalimeri, Mariano Beiró, Pablo Aragón</author><pubDate>Tue, 14 Jan 2025 16:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18803v2</guid></item><item><title>Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models</title><link>http://arxiv.org/abs/2410.10733v4</link><description>We present Deep Compression Autoencoder (DC-AE), a new family of autoencodermodels for accelerating high-resolution diffusion models. Existing autoencodermodels have demonstrated impressive results at a moderate spatial compressionratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy forhigh spatial compression ratios (e.g., 64x). We address this challenge byintroducing two key techniques: (1) Residual Autoencoding, where we design ourmodels to learn residuals based on the space-to-channel transformed features toalleviate the optimization difficulty of high spatial-compression autoencoders;(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phasestraining strategy for mitigating the generalization penalty of highspatial-compression autoencoders. With these designs, we improve theautoencoder's spatial compression ratio up to 128 while maintaining thereconstruction quality. Applying our DC-AE to latent diffusion models, weachieve significant speedup without accuracy drop. For example, on ImageNet512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedupon H100 GPU for UViT-H while achieving a better FID, compared with the widelyused SD-VAE-f8 autoencoder. Our code is available athttps://github.com/mit-han-lab/efficientvit.</description><author>Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han</author><pubDate>Tue, 14 Jan 2025 16:47:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10733v4</guid></item><item><title>Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination</title><link>http://arxiv.org/abs/2409.12746v2</link><description>In this article we present UNED-ACCESS 2024, a bilingual dataset thatconsists of 1003 multiple-choice questions of university entrance level examsin Spanish and English. Questions are originally formulated in Spanish andtranslated manually into English, and have not ever been publicly released. Aselection of current open-source and proprietary models are evaluated in auniform zero-shot experimental setting both on the UNED-ACCESS 2024 dataset andon an equivalent subset of MMLU questions. Results show that (i) reasoningquestions are challenging for models, (ii) smaller models perform worse thanlarger models and degrade faster in Spanish than in English and (iii) theperformance gap between languages is negligible for the best models and growsup to 37% for smaller models. Model ranking on UNED-ACCESS 2024 is almostidentical in English and Spanish, and has also a high correlation (0.98Pearson) with ranking on MMLU, suggesting that a small dataset is sufficientlydiverse and representative to measure performance by discipline.</description><author>Eva Sánchez Salido, Roser Morante, Julio Gonzalo, Guillermo Marco, Jorge Carrillo-de-Albornoz, Laura Plaza, Enrique Amigó, Andrés Fernández, Alejandro Benito-Santos, Adrián Ghajari Espinosa, Victor Fresno</author><pubDate>Tue, 14 Jan 2025 16:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12746v2</guid></item><item><title>Scaling White-Box Transformers for Vision</title><link>http://arxiv.org/abs/2405.20299v4</link><description>CRATE, a white-box transformer architecture designed to learn compressed andsparse representations, offers an intriguing alternative to standard visiontransformers (ViTs) due to its inherent mathematical interpretability. Despiteextensive investigations into the scaling behaviors of language and visiontransformers, the scalability of CRATE remains an open question which thispaper aims to address. Specifically, we propose CRATE-$\alpha$, featuringstrategic yet minimal modifications to the sparse coding block in the CRATEarchitecture design, and a light training recipe designed to improve thescalability of CRATE. Through extensive experiments, we demonstrate thatCRATE-$\alpha$ can effectively scale with larger model sizes and datasets. Forexample, our CRATE-$\alpha$-B substantially outperforms the prior best CRATE-Bmodel accuracy on ImageNet classification by 3.7%, achieving an accuracy of83.2%. Meanwhile, when scaling further, our CRATE-$\alpha$-L obtains anImageNet classification accuracy of 85.1%. More notably, these modelperformance improvements are achieved while preserving, and potentially evenenhancing the interpretability of learned CRATE models, as we demonstratethrough showing that the learned token representations of increasingly largertrained CRATE-$\alpha$ models yield increasingly higher-quality unsupervisedobject segmentation of images. The project page ishttps://rayjryang.github.io/CRATE-alpha/.</description><author>Jinrui Yang, Xianhang Li, Druv Pai, Yuyin Zhou, Yi Ma, Yaodong Yu, Cihang Xie</author><pubDate>Tue, 14 Jan 2025 16:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20299v4</guid></item><item><title>Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models</title><link>http://arxiv.org/abs/2501.08248v1</link><description>Recent advancements in long-context language models (LCLMs) promise totransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. Withtheir expanded context windows, LCLMs can process entire knowledge bases andperform retrieval and reasoning directly -- a capability we define asIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks likeLOFT often overestimate LCLM performance by providing overly simplifiedcontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMsin more realistic scenarios by including confounding passages retrieved withstrong retrievers. We then propose three methods to enhance LCLM performance:(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, whichuses attention heads to filter and de-noise long contexts during decoding, and(3) joint retrieval head training alongside the generation head. Our evaluationof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains withour best approach applied to Mistral-7B: +17 and +15 points by Exact Match onLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervisedfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasksdespite being a much smaller model.</description><author>Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay B. Cohen, Benjamin Han</author><pubDate>Tue, 14 Jan 2025 16:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08248v1</guid></item><item><title>Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful Behaviors with Proximity Constraints</title><link>http://arxiv.org/abs/2501.08246v1</link><description>Recent work has proposed automated red-teaming methods for testing thevulnerabilities of a given target large language model (LLM). These methods usered-teaming LLMs to uncover inputs that induce harmful behavior in a targetLLM. In this paper, we study red-teaming strategies that enable a targetedsecurity assessment. We propose an optimization framework for red-teaming withproximity constraints, where the discovered prompts must be similar toreference prompts from a given dataset. This dataset serves as a template forthe discovered prompts, anchoring the search for test-cases to specific topics,writing styles, or types of harmful behavior. We show that establishedauto-regressive model architectures do not perform well in this setting. Wetherefore introduce a black-box red-teaming method inspired by text-diffusionmodels: Diffusion for Auditing and Red-Teaming (DART). DART modifies thereference prompt by perturbing it in the embedding space, directly controllingthe amount of change introduced. We systematically evaluate our method bycomparing its effectiveness with established methods based on model fine-tuningand zero- and few-shot prompting. Our results show that DART is significantlymore effective at discovering harmful inputs in close proximity to thereference prompt.</description><author>Jonathan Nöther, Adish Singla, Goran Radanović</author><pubDate>Tue, 14 Jan 2025 16:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08246v1</guid></item><item><title>Continual Deep Active Learning for Medical Imaging: Replay-Base Architecture for Context Adaptation</title><link>http://arxiv.org/abs/2501.08245v1</link><description>Deep Learning for medical imaging faces challenges in adapting andgeneralizing to new contexts. Additionally, it often lacks sufficient labeleddata for specific tasks requiring significant annotation effort. ContinualLearning (CL) tackles adaptability and generalizability by enabling lifelonglearning from a data stream while mitigating forgetting of previously learnedknowledge. Active Learning (AL) reduces the number of required annotations foreffective training. This work explores both approaches (CAL) to develop a novelframework for robust medical image analysis. Based on the automatic recognitionof shifts in image characteristics, Replay-Base Architecture for ContextAdaptation (RBACA) employs a CL rehearsal method to continually learn fromdiverse contexts, and an AL component to select the most informative instancesfor annotation. A novel approach to evaluate CAL methods is established using adefined metric denominated IL-Score, which allows for the simultaneousassessment of transfer learning, forgetting, and final model performance. Weshow that RBACA works in domain and class-incremental learning scenarios, byassessing its IL-Score on the segmentation and diagnosis of cardiac images. Theresults show that RBACA outperforms a baseline framework without CAL, and astate-of-the-art CAL method across various memory sizes and annotation budgets.Our code is available in https://github.com/RuiDaniel/RBACA .</description><author>Rui Daniel, M. Rita Verdelho, Catarina Barata, Carlos Santiago</author><pubDate>Tue, 14 Jan 2025 16:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08245v1</guid></item><item><title>Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps</title><link>http://arxiv.org/abs/2501.08243v1</link><description>Cloud Operations (CloudOps) is a rapidly growing field focused on theautomated management and optimization of cloud infrastructure which isessential for organizations navigating increasingly complex cloud environments.MontyCloud Inc. is one of the major companies in the CloudOps domain thatleverages autonomous bots to manage cloud compliance, security, and continuousoperations. To make the platform more accessible and effective to thecustomers, we leveraged the use of GenAI. Developing a GenAI-based solution for autonomous CloudOps for the existingMontyCloud system presented us with various challenges such as i) diverse datasources; ii) orchestration of multiple processes; and iii) handling complexworkflows to automate routine tasks. To this end, we developed MOYA, amulti-agent framework that leverages GenAI and balances autonomy with thenecessary human control. This framework integrates various internal andexternal systems and is optimized for factors like task orchestration,security, and error mitigation while producing accurate, reliable, and relevantinsights by utilizing Retrieval Augmented Generation (RAG). Evaluations of ourmulti-agent system with the help of practitioners as well as using automatedchecks demonstrate enhanced accuracy, responsiveness, and effectiveness overnon-agentic approaches across complex workflows.</description><author>Kannan Parthasarathy, Karthik Vaidhyanathan, Rudra Dhar, Venkat Krishnamachari, Basil Muhammed, Adyansh Kakran, Sreemaee Akshathala, Shrikara Arun, Sumant Dubey, Mohan Veerubhotla, Amey Karan</author><pubDate>Tue, 14 Jan 2025 16:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08243v1</guid></item><item><title>A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images using Choquet Integral and Differential Evolution Optimization</title><link>http://arxiv.org/abs/2501.08241v1</link><description>The COVID-19 pandemic has profoundly impacted billions globally. Itchallenges public health and healthcare systems due to its rapid spread andsevere respiratory effects. An effective strategy to mitigate the COVID-19pandemic involves integrating testing to identify infected individuals. WhileRT-PCR is considered the gold standard for diagnosing COVID-19, it has somelimitations such as the risk of false negatives. To address this problem, thispaper introduces a novel Deep Learning Diagnosis System that integratespre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemblelearning framework to achieve precise identification of COVID-19 cases fromChest X-ray (CXR) images. We combine feature vectors from the final hiddenlayers of pre-trained DCNNs using the Choquet integral to capture interactionsbetween different DCNNs that a linear approach cannot. We employedSugeno-$\lambda$ measure theory to derive fuzzy measures for subsets ofnetworks to enable aggregation. We utilized Differential Evolution to estimatefuzzy densities. We developed a TensorFlow-based layer for Choquet operation tofacilitate efficient aggregation, due to the intricacies involved inaggregating feature vectors. Experimental results on the COVIDx dataset showthat our ensemble model achieved 98\% accuracy in three-class classificationand 99.50\% in binary classification, outperforming its components-DenseNet-201(97\% for three-class, 98.75\% for binary), Inception-v3 (96.25\% forthree-class, 98.50\% for binary), and Xception (94.50\% for three-class, 98\%for binary)-and surpassing many previous methods.</description><author>Amir Reza Takhsha, Maryam Rastgarpour, Mozhgan Naderi</author><pubDate>Tue, 14 Jan 2025 16:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08241v1</guid></item><item><title>Privacy-Preserving Model and Preprocessing Verification for Machine Learning</title><link>http://arxiv.org/abs/2501.08236v1</link><description>This paper presents a framework for privacy-preserving verification ofmachine learning models, focusing on models trained on sensitive data.Integrating Local Differential Privacy (LDP) with model explanations from LIMEand SHAP, our framework enables robust verification without compromisingindividual privacy. It addresses two key tasks: binary classification, toverify if a target model was trained correctly by applying the appropriatepreprocessing steps, and multi-class classification, to identify specificpreprocessing errors. Evaluations on three real-world datasets-Diabetes, Adult,and Student Record-demonstrate that while the ML-based approach is particularlyeffective in binary tasks, the threshold-based method performs comparably inmulti-class tasks. Results indicate that although verification accuracy variesacross datasets and noise levels, the framework provides effective detection ofpreprocessing errors, strong privacy guarantees, and practical applicabilityfor safeguarding sensitive data.</description><author>Wenbiao Li, Anisa Halimi, Xiaoqian Jiang, Jaideep Vaidya, Erman Ayday</author><pubDate>Tue, 14 Jan 2025 16:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08236v1</guid></item><item><title>Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2501.08234v1</link><description>This paper addresses a critical challenge in the high-speed passenger railwayindustry: designing effective dynamic pricing strategies in the context ofcompeting and cooperating operators. To address this, a multi-agentreinforcement learning (MARL) framework based on a non-zero-sum Markov game isproposed, incorporating random utility models to capture passenger decisionmaking. Unlike prior studies in areas such as energy, airlines, and mobilenetworks, dynamic pricing for railway systems using deep reinforcement learninghas received limited attention. A key contribution of this paper is aparametrisable and versatile reinforcement learning simulator designed to modela variety of railway network configurations and demand patterns while enablingrealistic, microscopic modelling of user behaviour, called RailPricing-RL. Thisenvironment supports the proposed MARL framework, which models heterogeneousagents competing to maximise individual profits while fostering cooperativebehaviour to synchronise connecting services. Experimental results validate theframework, demonstrating how user preferences affect MARL performance and howpricing policies influence passenger choices, utility, and overall systemdynamics. This study provides a foundation for advancing dynamic pricingstrategies in railway systems, aligning profitability with system-wideefficiency, and supporting future research on optimising pricing policies.</description><author>Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, David Muñoz-Valero, Giovanni Montana, Luis Jimenez-Linares</author><pubDate>Tue, 14 Jan 2025 16:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08234v1</guid></item><item><title>HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</title><link>http://arxiv.org/abs/2405.14831v3</link><description>In order to thrive in hostile and ever-changing natural environments,mammalian brains evolved to store large amounts of knowledge about the worldand continually integrate new information while avoiding catastrophicforgetting. Despite the impressive accomplishments, large language models(LLMs), even with retrieval-augmented generation (RAG), still struggle toefficiently and effectively integrate a large amount of new experiences afterpre-training. In this work, we introduce HippoRAG, a novel retrieval frameworkinspired by the hippocampal indexing theory of human long-term memory to enabledeeper and more efficient knowledge integration over new experiences. HippoRAGsynergistically orchestrates LLMs, knowledge graphs, and the PersonalizedPageRank algorithm to mimic the different roles of neocortex and hippocampus inhuman memory. We compare HippoRAG with existing RAG methods on multi-hopquestion answering and show that our method outperforms the state-of-the-artmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achievescomparable or better performance than iterative retrieval like IRCoT whilebeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG intoIRCoT brings further substantial gains. Finally, we show that our method cantackle new types of scenarios that are out of reach of existing methods. Codeand data are available at https://github.com/OSU-NLP-Group/HippoRAG.</description><author>Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su</author><pubDate>Tue, 14 Jan 2025 16:17:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14831v3</guid></item><item><title>A Comprehensive Survey of Foundation Models in Medicine</title><link>http://arxiv.org/abs/2406.10729v2</link><description>Foundation models (FMs) are large-scale deep learning models that aredeveloped using large datasets and self-supervised learning methods. Thesemodels serve as a base for different downstream tasks, including healthcare.FMs have been adopted with great success across various domains withinhealthcare. Existing healthcare-based surveys have not yet included all ofthese domains. Therefore, we provide a detailed survey of FMs in healthcare. Wefocus on the history, learning strategies, flagship models, applications, andchallenges of FMs. We explore how FMs such as the BERT and GPT families arereshaping various healthcare domains, including clinical large language models,medical image analysis, and omics. Furthermore, we provide a detailed taxonomyof healthcare applications facilitated by FMs, such as clinical NLP, medicalcomputer vision, graph learning, and other biology-related tasks. Despite thepromising opportunities FMs provide, they also have several associatedchallenges, which are explained in detail. We also outline open research issuesand potential lessons learned to provide researchers and practitioners withinsights into the capabilities of FMs in healthcare to advance their deploymentand mitigate associated risks.</description><author>Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang</author><pubDate>Tue, 14 Jan 2025 16:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10729v2</guid></item><item><title>Text-guided Image Restoration and Semantic Enhancement for Text-to-Image Person Retrieval</title><link>http://arxiv.org/abs/2307.09059v3</link><description>The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specificperson images according to the given textual descriptions. A primary challengein this task is bridging the substantial representational gap between visualand textual modalities. The prevailing methods map texts and images intounified embedding space for matching, while the intricate semanticcorrespondences between texts and images are still not effectively constructed.To address this issue, we propose a novel TIPR framework to build fine-grainedinteractions and alignment between person images and the corresponding texts.Specifically, via fine-tuning the Contrastive Language-Image Pre-training(CLIP) model, a visual-textual dual encoder is firstly constructed, topreliminarily align the image and text features. Secondly, a Text-guided ImageRestoration (TIR) auxiliary task is proposed to map abstract textual entitiesto specific image regions, improving the alignment between local textual andvisual embeddings. Additionally, a cross-modal triplet loss is presented tohandle hard samples, and further enhance the model's discriminability for minordifferences. Moreover, a pruning-based text data augmentation approach isproposed to enhance focus on essential elements in descriptions, therebyavoiding excessive model attention to less significant information. Theexperimental results show our proposed method outperforms state-of-the-artmethods on three popular benchmark datasets, and the code will be made publiclyavailable at https://github.com/Delong-liu-bupt/SEN.</description><author>Delong Liu, Haiwen Li, Zhicheng Zhao, Yuan Dong, Nikolaos V. Boulgouris</author><pubDate>Tue, 14 Jan 2025 16:11:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09059v3</guid></item><item><title>Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth Models</title><link>http://arxiv.org/abs/2501.08226v1</link><description>Glioblastoma, a highly aggressive brain tumor, poses major challenges due toits poor prognosis and high morbidity rates. Partial differentialequation-based models offer promising potential to enhance therapeutic outcomesby simulating patient-specific tumor behavior for improved radiotherapyplanning. However, model calibration remains a bottleneck due to the highcomputational demands of optimization methods like Monte Carlo sampling andevolutionary algorithms. To address this, we recently introduced an approachleveraging a neural forward solver with gradient-based optimization tosignificantly reduce calibration time. This approach requires a highly accurateand fully differentiable forward model. We investigate multiple architectures,including (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the bestoverall results, excelling in both tumor outline matching and voxel-levelprediction of tumor cell concentration. It halved the MSE relative to thebaseline model and achieved the highest Dice score across all tumor cellconcentration thresholds. Our study demonstrates significant enhancement inforward solver performance and outlines important future research directions.</description><author>Zeineb Haouari, Jonas Weidner, Ivan Ezhov, Aswathi Varma, Daniel Rueckert, Bjoern Menze, Benedikt Wiestler</author><pubDate>Tue, 14 Jan 2025 16:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08226v1</guid></item><item><title>FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors</title><link>http://arxiv.org/abs/2501.08225v1</link><description>Interactive image editing allows users to modify images through visualinteraction operations such as drawing, clicking, and dragging. Existingmethods construct such supervision signals from videos, as they capture howobjects change with various physical interactions. However, these models areusually built upon text-to-image diffusion models, so necessitate (i) massivetraining samples and (ii) an additional reference encoder to learn real-worlddynamics and visual consistency. In this paper, we reformulate this task as animage-to-video generation problem, so that inherit powerful video diffusionpriors to reduce training costs and ensure temporal consistency. Specifically,we introduce FramePainter as an efficient instantiation of this formulation.Initialized with Stable Video Diffusion, it only uses a lightweight sparsecontrol encoder to inject editing signals. Considering the limitations oftemporal attention in handling large motion between two frames, we furtherpropose matching attention to enlarge the receptive field while encouragingdense correspondence between edited and source image tokens. We highlight theeffectiveness and efficiency of FramePainter across various of editing signals:it domainantly outperforms previous state-of-the-art methods with far lesstraining data, achieving highly seamless and coherent editing of images, \eg,automatically adjust the reflection of the cup. Moreover, FramePainter alsoexhibits exceptional generalization in scenarios not present in real-worldvideos, \eg, transform the clownfish into shark-like shape. Our code will beavailable at https://github.com/YBYBZhang/FramePainter.</description><author>Yabo Zhang, Xinpeng Zhou, Yihan Zeng, Hang Xu, Hui Li, Wangmeng Zuo</author><pubDate>Tue, 14 Jan 2025 16:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08225v1</guid></item><item><title>Pareto Set Learning for Multi-Objective Reinforcement Learning</title><link>http://arxiv.org/abs/2501.06773v2</link><description>Multi-objective decision-making problems have emerged in numerous real-worldscenarios, such as video games, navigation and robotics. Considering the clearadvantages of Reinforcement Learning (RL) in optimizing decision-makingprocesses, researchers have delved into the development of Multi-Objective RL(MORL) methods for solving multi-objective decision problems. However, previousmethods either cannot obtain the entire Pareto front, or employ only a singlepolicy network for all the preferences over multiple objectives, which may notproduce personalized solutions for each preference. To address theselimitations, we propose a novel decomposition-based framework for MORL, ParetoSet Learning for MORL (PSL-MORL), that harnesses the generation capability ofhypernetwork to produce the parameters of the policy network for eachdecomposition weight, generating relatively distinct policies for variousscalarized subproblems with high efficiency. PSL-MORL is a general framework,which is compatible for any RL algorithm. The theoretical result guarantees thesuperiority of the model capacity of PSL-MORL and the optimality of theobtained policy network. Through extensive experiments on diverse benchmarks,we demonstrate the effectiveness of PSL-MORL in achieving dense coverage of thePareto front, significantly outperforming state-of-the-art MORL methods in thehypervolume and sparsity indicators.</description><author>Erlong Liu, Yu-Chang Wu, Xiaobin Huang, Chengrui Gao, Ren-Jian Wang, Ke Xue, Chao Qian</author><pubDate>Tue, 14 Jan 2025 16:08:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06773v2</guid></item><item><title>Big Batch Bayesian Active Learning by Considering Predictive Probabilities</title><link>http://arxiv.org/abs/2501.08223v1</link><description>We observe that BatchBALD, a popular acquisition function for batch Bayesianactive learning for classification, can conflate epistemic and aleatoricuncertainty, leading to suboptimal performance. Motivated by this observation,we propose to focus on the predictive probabilities, which only exhibitepistemic uncertainty. The result is an acquisition function that not onlyperforms better, but is also faster to evaluate, allowing for larger batchesthan before.</description><author>Sebastian W. Ober, Samuel Power, Tom Diethe, Henry B. Moss</author><pubDate>Tue, 14 Jan 2025 16:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08223v1</guid></item><item><title>Optimization of Link Configuration for Satellite Communication Using Reinforcement Learning</title><link>http://arxiv.org/abs/2501.08220v1</link><description>Satellite communication is a key technology in our modern connected world.With increasingly complex hardware, one challenge is to efficiently configurelinks (connections) on a satellite transponder. Planning an optimal linkconfiguration is extremely complex and depends on many parameters and metrics.The optimal use of the limited resources, bandwidth and power of thetransponder is crucial. Such an optimization problem can be approximated usingmetaheuristic methods such as simulated annealing, but recent research resultsalso show that reinforcement learning can achieve comparable or even betterperformance in optimization methods. However, there have not yet been anystudies on link configuration on satellite transponders. In order to close thisresearch gap, a transponder environment was developed as part of this work. Forthis environment, the performance of the reinforcement learning algorithm PPOwas compared with the metaheuristic simulated annealing in two experiments. Theresults show that Simulated Annealing delivers better results for this staticproblem than the PPO algorithm, however, the research in turn also underlinesthe potential of reinforcement learning for optimization problems.</description><author>Tobias Rohe, Michael Kölle, Jan Matheis, Rüdiger Höpfl, Leo Sünkel, Claudia Linnhoff-Popien</author><pubDate>Tue, 14 Jan 2025 16:04:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08220v1</guid></item><item><title>Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings</title><link>http://arxiv.org/abs/2501.08219v1</link><description>Large language models (LLMs) have shown significant improvements in manynatural language processing (NLP) tasks, accelerating their rapid adoptionacross many industries. These models are resource-intensive, requiringextensive computational resources both during training and inference, leadingto increased energy consumption and negative environmental impact. As theiradoption accelerates, the sustainability of LLMs has become a critical issue,necessitating strategies to optimize their runtime efficiency withoutcompromising performance. Hence, it is imperative to identify the parametersthat significantly influence the performance and energy efficiency of LLMs. Tothat end, in this work, we investigate the effect of important parameters onthe performance and energy efficiency of LLMs during inference and examinetheir trade-offs. First, we analyze how different types of models with varying numbers ofparameters and architectures perform on tasks like text generation, questionanswering, and summarization by benchmarking LLMs such as Falcon-7B,Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we studyinput and output sequence characteristics such as sequence length concerningenergy consumption, performance, and throughput. Finally, we explore the impactof hardware-based power-saving techniques, i.e., Dynamic Voltage FrequencyScaling (DVFS), on the models' latency and energy efficiency. Our extensivebenchmarking and statistical analysis reveal many interesting findings,uncovering how specific optimizations can reduce energy consumption whilemaintaining throughput and accuracy. This study provides actionable insightsfor researchers and practitioners to design energy-efficient LLM inferencesystems.</description><author>Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic</author><pubDate>Tue, 14 Jan 2025 16:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08219v1</guid></item><item><title>Logic Augmented Generation</title><link>http://arxiv.org/abs/2411.14012v2</link><description>Semantic Knowledge Graphs (SKG) face challenges with scalability,flexibility, contextual understanding, and handling unstructured or ambiguousinformation. However, they offer formal and structured knowledge enablinghighly interpretable and reliable results by means of reasoning and querying.Large Language Models (LLMs) overcome those limitations making them suitable inopen-ended tasks and unstructured environments. Nevertheless, LLMs are neitherinterpretable nor reliable. To solve the dichotomy between LLMs and SKGs weenvision Logic Augmented Generation (LAG) that combines the benefits of the twoworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generatepotentially infinite relations and tacit knowledge on-demand. SKGs are key forinjecting a discrete heuristic dimension with clear logical and factualboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,medical diagnostics and climate projections. Understanding the properties andlimitations of LAG, which are still mostly unknown, is of utmost importance forenabling a variety of tasks involving tacit knowledge in order to provideinterpretable and effective results.</description><author>Aldo Gangemi, Andrea Giovanni Nuzzolese</author><pubDate>Tue, 14 Jan 2025 15:58:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.14012v2</guid></item><item><title>ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems</title><link>http://arxiv.org/abs/2501.08208v1</link><description>Large Language Models (LLMs) have shown impressive potential in clinicalquestion answering (QA), with Retrieval Augmented Generation (RAG) emerging asa leading approach for ensuring the factual accuracy of model responses.However, current automated RAG metrics perform poorly in clinical andconversational use cases. Using clinical human evaluations of responses isexpensive, unscalable, and not conducive to the continuous iterativedevelopment of RAG systems. To address these challenges, we introduce ASTRID -an Automated and Scalable TRIaD for evaluating clinical QA systems leveragingRAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy(RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, isdesigned to better capture the faithfulness of a model's response to theknowledge base without penalising conversational elements. To validate ourtriad, we curate a dataset of over 200 real-world patient questions posed to anLLM-based QA agent during surgical follow-up for cataract surgery - the highestvolume operation in the world - augmented with clinician-selected questions foremergency, clinical, and non-clinical out-of-domain scenarios. We demonstratethat CF can predict human ratings of faithfulness better than existingdefinitions for conversational use cases. Furthermore, we show that evaluationusing our triad consisting of CF, RA, and CR exhibits alignment with clinicianassessment for inappropriate, harmful, or unhelpful responses. Finally, usingnine different LLMs, we demonstrate that the three metrics can closely agreewith human evaluations, highlighting the potential of these metrics for use inLLM-driven automated evaluation pipelines. We also publish the prompts anddatasets for these experiments, providing valuable resources for furtherresearch and development.</description><author>Mohita Chowdhury, Yajie Vera He, Aisling Higham, Ernest Lim</author><pubDate>Tue, 14 Jan 2025 15:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08208v1</guid></item><item><title>Modeling Feature Maps for Quantum Machine Learning</title><link>http://arxiv.org/abs/2501.08205v1</link><description>Quantum Machine Learning (QML) offers significant potential for complex taskslike genome sequence classification, but quantum noise on NoisyIntermediate-Scale Quantum (NISQ) devices poses practical challenges. Thisstudy systematically evaluates how various quantum noise models includingdephasing, amplitude damping, depolarizing, thermal noise, bit-flip, andphase-flip affect key QML algorithms (QSVC, Peg-QSVC, QNN, VQC) and featuremapping techniques (ZFeatureMap, ZZFeatureMap, and PauliFeatureMap). Resultsindicate that QSVC is notably robust under noise, whereas Peg-QSVC and QNN aremore sensitive, particularly to depolarizing and amplitude-damping noise. ThePauliFeatureMap is especially vulnerable, highlighting difficulties inmaintaining accurate classification under noisy conditions. These findingsunderscore the critical importance of feature map selection and noisemitigation strategies in optimizing QML for genomic classification, withpromising implications for personalized medicine.</description><author>Navneet Singh, Shiva Raj Pokhrel</author><pubDate>Tue, 14 Jan 2025 15:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08205v1</guid></item><item><title>Sharp Matrix Empirical Bernstein Inequalities</title><link>http://arxiv.org/abs/2411.09516v3</link><description>We present two sharp empirical Bernstein inequalities for symmetric randommatrices with bounded eigenvalues. By sharp, we mean that both inequalitiesadapt to the unknown variance in a tight manner: the deviation captured by thefirst-order $1/\sqrt{n}$ term asymptotically matches the matrix Bernsteininequality exactly, including constants, the latter requiring knowledge of thevariance. Our first inequality holds for the sample mean of independentmatrices, and our second inequality holds for a mean estimator under martingaledependence at stopping times.</description><author>Hongjian Wang, Aaditya Ramdas</author><pubDate>Tue, 14 Jan 2025 15:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09516v3</guid></item><item><title>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</title><link>http://arxiv.org/abs/2501.08203v1</link><description>While Large Language Models (LLMs) have shown impressive capabilities in mathproblem-solving tasks, their robustness to noisy inputs is not well-studied. Inthis work, we propose ArithmAttack to examine how robust the LLMs are when theyencounter noisy prompts that contain extra noise in the form of punctuationmarks. While being easy to implement, ArithmAttack does not cause anyinformation loss since words are not added or deleted from the context. Weevaluate the robustness of seven LLMs, including LLama3, Mistral, andMathstral, on noisy GSM8K and MultiArith datasets. Our experiments suggest thatall the studied models show vulnerability to such noise, with more noiseleading to poorer performances.</description><author>Zain Ul Abedin, Shahzeb Qamar, Lucie Flek, Akbar Karimi</author><pubDate>Tue, 14 Jan 2025 15:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08203v1</guid></item><item><title>Data-driven system identification using quadratic embeddings of nonlinear dynamics</title><link>http://arxiv.org/abs/2501.08202v1</link><description>We propose a novel data-driven method called QENDy (Quadratic Embedding ofNonlinear Dynamics) that not only allows us to learn quadratic representationsof highly nonlinear dynamical systems, but also to identify the governingequations. The approach is based on an embedding of the system into ahigher-dimensional feature space in which the dynamics become quadratic. Justlike SINDy (Sparse Identification of Nonlinear Dynamics), our method requirestrajectory data, time derivatives for the training data points, which can alsobe estimated using finite difference approximations, and a set of preselectedbasis functions, called dictionary. We illustrate the efficacy and accuracy ofQENDy with the aid of various benchmark problems and compare its performancewith SINDy and a deep learning method for identifying quadratic embeddings.Furthermore, we analyze the convergence of QENDy and SINDy in the infinite datalimit, highlight their similarities and main differences, and compare thequadratic embedding with linearization techniques based on the Koopmanoperator.</description><author>Stefan Klus, Joel-Pascal N'Konzi</author><pubDate>Tue, 14 Jan 2025 15:37:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08202v1</guid></item><item><title>Globally Convergent Variational Inference</title><link>http://arxiv.org/abs/2501.08201v1</link><description>In variational inference (VI), an approximation of the posterior distributionis selected from a family of distributions through numerical optimization. Withthe most common variational objective function, known as the evidence lowerbound (ELBO), only convergence to a local optimum can be guaranteed. In thiswork, we instead establish the global convergence of a particular VI method.This VI method, which may be considered an instance of neural posteriorestimation (NPE), minimizes an expectation of the inclusive (forward) KLdivergence to fit a variational distribution that is parameterized by a neuralnetwork. Our convergence result relies on the neural tangent kernel (NTK) tocharacterize the gradient dynamics that arise from considering the variationalobjective in function space. In the asymptotic regime of a fixed,positive-definite neural tangent kernel, we establish conditions under whichthe variational objective admits a unique solution in a reproducing kernelHilbert space (RKHS). Then, we show that the gradient descent dynamics infunction space converge to this unique function. In ablation studies andpractical problems, we demonstrate that our results explain the behavior of NPEin non-asymptotic finite-neuron settings, and show that NPE outperformsELBO-based optimization, which often converges to shallow local optima.</description><author>Declan McNamara, Jackson Loper, Jeffrey Regier</author><pubDate>Tue, 14 Jan 2025 15:36:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08201v1</guid></item><item><title>Relaxed Rotational Equivariance via $G$-Biases in Vision</title><link>http://arxiv.org/abs/2408.12454v3</link><description>Group Equivariant Convolution (GConv) can capture rotational equivariancefrom original data. It assumes uniform and strict rotational equivarianceacross all features as the transformations under the specific group. However,the presentation or distribution of real-world data rarely conforms to strictrotational equivariance, commonly referred to as Rotational Symmetry-Breaking(RSB) in the system or dataset, making GConv unable to adapt effectively tothis phenomenon. Motivated by this, we propose a simple but highly effectivemethod to address this problem, which utilizes a set of learnable biases called$G$-Biases under the group order to break strict group constraints and thenachieve a Relaxed Rotational Equivariant Convolution (RREConv). To validate theefficiency of RREConv, we conduct extensive ablation experiments on thediscrete rotational group $\mathcal{C}_n$. Experiments demonstrate that theproposed RREConv-based methods achieve excellent performance compared toexisting GConv-based methods in both classification and 2D object detectiontasks on the natural image datasets.</description><author>Zhiqiang Wu, Yingjie Liu, Licheng Sun, Jian Yang, Hanlin Dong, Shing-Ho J. Lin, Xuan Tang, Jinpeng Mi, Bo Jin, Xian Wei</author><pubDate>Tue, 14 Jan 2025 15:35:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12454v3</guid></item><item><title>Personalized LLM Response Generation with Parameterized Memory Injection</title><link>http://arxiv.org/abs/2404.03565v3</link><description>Large Language Models (LLMs) have exhibited remarkable proficiency incomprehending and generating natural language. On the other hand, personalizedLLM response generation holds the potential to offer substantial benefits forindividuals in critical areas such as medical. Existing research has exploredmemory-augmented methods to prompt the LLM with pre-stored user-specificknowledge for personalized response generation in terms of new queries. Wecontend that such paradigm is unable to perceive fine-granularity information.In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approachusing parameter-efficient fine-tuning (PEFT) and along with a BayesianOptimisation searching strategy to achieve \textbf{L}LM\textbf{P}ersonalization(\textbf{MiLP}).</description><author>Kai Zhang, Yejin Kim, Xiaozhong Liu</author><pubDate>Tue, 14 Jan 2025 15:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03565v3</guid></item><item><title>CWEval: Outcome-driven Evaluation on Functionality and Security of LLM Code Generation</title><link>http://arxiv.org/abs/2501.08200v1</link><description>Large Language Models (LLMs) have significantly aided developers bygenerating or assisting in code writing, enhancing productivity across varioustasks. While identifying incorrect code is often straightforward, detectingvulnerabilities in functionally correct code is more challenging, especiallyfor developers with limited security knowledge, which poses considerablesecurity risks of using LLM-generated code and underscores the need for robustevaluation benchmarks that assess both functional correctness and security.Current benchmarks like CyberSecEval and SecurityEval attempt to solve it butare hindered by unclear and impractical specifications, failing to assess bothfunctionality and security accurately. To tackle these deficiencies, weintroduce CWEval, a novel outcome-driven evaluation framework designed toenhance the evaluation of secure code generation by LLMs. This framework notonly assesses code functionality but also its security simultaneously withhigh-quality task specifications and outcome-driven test oracles which provideshigh accuracy. Coupled with CWEval-bench, a multilingual, security-criticalcoding benchmark, CWEval provides a rigorous empirical security evaluation onLLM-generated code, overcoming previous benchmarks' shortcomings. Through ourevaluations, CWEval reveals a notable portion of functional but insecure codeproduced by LLMs, and shows a serious inaccuracy of previous evaluations,ultimately contributing significantly to the field of secure code generation.We open-source our artifact at: https://github.com/Co1lin/CWEval .</description><author>Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, Baishakhi Ray</author><pubDate>Tue, 14 Jan 2025 15:27:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08200v1</guid></item><item><title>EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition</title><link>http://arxiv.org/abs/2501.08199v1</link><description>Facial expressions play a crucial role in human communication serving as apowerful and impactful means to express a wide range of emotions. Withadvancements in artificial intelligence and computer vision, deep neuralnetworks have emerged as effective tools for facial emotion recognition. Inthis paper, we propose EmoNeXt, a novel deep learning framework for facialexpression recognition based on an adapted ConvNeXt architecture network. Weintegrate a Spatial Transformer Network (STN) to focus on feature-rich regionsof the face and Squeeze-and-Excitation blocks to capture channel-wisedependencies. Moreover, we introduce a self-attention regularization term,encouraging the model to generate compact feature vectors. We demonstrate thesuperiority of our model over existing state-of-the-art deep learning models onthe FER2013 dataset regarding emotion classification accuracy.</description><author>Yassine El Boudouri, Amine Bohi</author><pubDate>Tue, 14 Jan 2025 15:23:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08199v1</guid></item><item><title>OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training</title><link>http://arxiv.org/abs/2501.08197v1</link><description>Large language models (LLMs) have demonstrated remarkable capabilities, buttheir success heavily relies on the quality of pretraining corpora. For ChineseLLMs, the scarcity of high-quality Chinese datasets presents a significantchallenge, often limiting their performance. To address this issue, we proposethe OpenCSG Chinese Corpus, a series of high-quality datasets specificallydesigned for LLM pretraining, post-training, and fine-tuning. This corpusincludes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, andSmoltalk-chinese, each with distinct characteristics: Fineweb-edu datasetsfocus on filtered, high-quality content derived from diverse Chinese websources; Cosmopedia-chinese provides synthetic, textbook-style data forknowledge-intensive training; and Smoltalk-chinese emphasizes stylistic anddiverse chat-format data. The OpenCSG Chinese Corpus is characterized by itshigh-quality text, diverse coverage across domains, and scalable, reproducibledata curation processes. Additionally, we conducted extensive experimentalanalyses, including evaluations on smaller parameter models, which demonstratedsignificant performance improvements in tasks such as C-Eval, showcasing theeffectiveness of the corpus for training Chinese LLMs.</description><author>Yijiong Yu, Ziyun Dai, Zekun Wang, Wei Wang, Ran Chen, Ji Pei</author><pubDate>Tue, 14 Jan 2025 15:22:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08197v1</guid></item><item><title>KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model</title><link>http://arxiv.org/abs/2501.01028v3</link><description>As retrieval-augmented generation prevails in large language models,embedding models are becoming increasingly crucial. Despite the growing numberof general embedding models, prior work often overlooks the critical role oftraining data quality. In this work, we introduce KaLM-Embedding, a generalmultilingual embedding model that leverages a large quantity of cleaner, morediverse, and domain-specific training data. Our model has been trained with keytechniques proven to enhance performance: (1) persona-based synthetic data tocreate diversified examples distilled from LLMs, (2) ranking consistencyfiltering to remove less informative samples, and (3) semi-homogeneous taskbatch sampling to improve training efficacy. Departing from traditionalBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,facilitating the adaptation of auto-regressive language models for generalembedding tasks. Extensive evaluations of the MTEB benchmark across multiplelanguages show that our model outperforms others of comparable size, setting anew standard for multilingual embedding models with &lt;1B parameters.</description><author>Xinshuo Hu, Zifei Shan, Xinping Zhao, Zetian Sun, Zhenyu Liu, Dongfang Li, Shaolin Ye, Xinyuan Wei, Qian Chen, Baotian Hu, Min Zhang</author><pubDate>Tue, 14 Jan 2025 15:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.01028v3</guid></item><item><title>Self-supervised Deep Hyperspectral Inpainting with the Plug and Play and Deep Image Prior Models</title><link>http://arxiv.org/abs/2501.08195v1</link><description>Hyperspectral images are typically composed of hundreds of narrow andcontiguous spectral bands, each containing information regarding the materialcomposition of the imaged scene. However, these images can be affected byvarious sources of noise, distortions, or data loss, which can significantlydegrade their quality and usefulness. This paper introduces a convergentguaranteed algorithm, LRS-PnP-DIP(1-Lip), which successfully addresses theinstability issue of DHP that has been reported before. The proposed algorithmextends the successful joint low-rank and sparse model to further exploit theunderlying data structures beyond the conventional and sometimes restrictiveunions of subspace models. A stability analysis guarantees the convergence ofthe proposed algorithm under mild assumptions , which is crucial for itsapplication in real-world scenarios. Extensive experiments demonstrate that theproposed solution consistently delivers visually and quantitatively superiorinpainting results, establishing state-of-the-art performance.</description><author>Shuo Li, Mehrdad Yaghoobi</author><pubDate>Tue, 14 Jan 2025 15:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08195v1</guid></item><item><title>Modeling Quantum Machine Learning for Genomic Data Analysis</title><link>http://arxiv.org/abs/2501.08193v1</link><description>Quantum Machine Learning (QML) continues to evolve, unlocking newopportunities for diverse applications. In this study, we investigate andevaluate the applicability of QML models for binary classification of genomesequence data by employing various feature mapping techniques. We present anopen-source, independent Qiskit-based implementation to conduct experiments ona benchmark genomic dataset. Our simulations reveal that the interplay betweenfeature mapping techniques and QML algorithms significantly influencesperformance. Notably, the Pegasos Quantum Support Vector Classifier(Pegasos-QSVC) exhibits high sensitivity, particularly excelling in recallmetrics, while Quantum Neural Networks (QNN) achieve the highest trainingaccuracy across all feature maps. However, the pronounced variability inclassifier performance, dependent on feature mapping, highlights the risk ofoverfitting to localized output distributions in certain scenarios. This workunderscores the transformative potential of QML for genomic data classificationwhile emphasizing the need for continued advancements to enhance the robustnessand accuracy of these methodologies.</description><author>Navneet Singh, Shiva Raj Pokhrel</author><pubDate>Tue, 14 Jan 2025 15:14:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08193v1</guid></item><item><title>PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving</title><link>http://arxiv.org/abs/2501.08192v1</link><description>Large language models (LLMs) are widely used across various applications, buttheir substantial computational requirements pose significant challenges,particularly in terms of HBM bandwidth bottlenecks and inter-devicecommunication overhead. In this paper, we present PRESERVE, a novel prefetchingframework designed to optimize LLM inference by overlapping memory reads formodel weights and KV-cache with collective communication operations. Throughextensive experiments conducted on commercial AI accelerators, we demonstrateup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.Additionally, we perform a design space exploration that identifies the optimalhardware configuration for the proposed method, showing a further 1.25ximprovement in performance per cost by selecting the optimal L2 cache size. Ourresults show that PRESERVE has the potential to mitigate the memory bottlenecksand communication overheads, offering a solution to improve the performance andscalability of the LLM inference systems.</description><author>Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli</author><pubDate>Tue, 14 Jan 2025 15:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08192v1</guid></item><item><title>A Critical Synthesis of Uncertainty Quantification and Foundation Models in Monocular Depth Estimation</title><link>http://arxiv.org/abs/2501.08188v1</link><description>While recent foundation models have enabled significant breakthroughs inmonocular depth estimation, a clear path towards safe and reliable deploymentin the real-world remains elusive. Metric depth estimation, which involvespredicting absolute distances, poses particular challenges, as even the mostadvanced foundation models remain prone to critical errors. Since quantifyingthe uncertainty has emerged as a promising endeavor to address theselimitations and enable trustworthy deployment, we fuse five differentuncertainty quantification methods with the current state-of-the-artDepthAnythingV2 foundation model. To cover a wide range of metric depthdomains, we evaluate their performance on four diverse datasets. Our findingsidentify fine-tuning with the Gaussian Negative Log-Likelihood Loss (GNLL) as aparticularly promising approach, offering reliable uncertainty estimates whilemaintaining predictive performance and computational efficiency on par with thebaseline, encompassing both training and inference time. By fusing uncertaintyquantification and foundation models within the context of monocular depthestimation, this paper lays a critical foundation for future research aimed atimproving not only model performance but also its explainability. Extendingthis critical synthesis of uncertainty quantification and foundation modelsinto other crucial tasks, such as semantic segmentation and pose estimation,presents exciting opportunities for safer and more reliable machine visionsystems.</description><author>Steven Landgraf, Rongjun Qin, Markus Ulrich</author><pubDate>Tue, 14 Jan 2025 15:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08188v1</guid></item><item><title>A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following</title><link>http://arxiv.org/abs/2501.08187v1</link><description>Large language models excel at interpreting complex natural languageinstructions, enabling them to perform a wide range of tasks. In the lifesciences, single-cell RNA sequencing (scRNA-seq) data serves as the "languageof cellular biology", capturing intricate gene expression patterns at thesingle-cell level. However, interacting with this "language" throughconventional tools is often inefficient and unintuitive, posing challenges forresearchers. To address these limitations, we present InstructCell, amulti-modal AI copilot that leverages natural language as a medium for moredirect and flexible single-cell analysis. We construct a comprehensivemulti-modal instruction dataset that pairs text-based instructions withscRNA-seq profiles from diverse tissues and species. Building on this, wedevelop a multi-modal cell language architecture capable of simultaneouslyinterpreting and processing both modalities. InstructCell empowers researchersto accomplish critical tasks-such as cell type annotation, conditionalpseudo-cell generation, and drug sensitivity prediction-using straightforwardnatural language commands. Extensive evaluations demonstrate that InstructCellconsistently meets or exceeds the performance of existing single-cellfoundation models, while adapting to diverse experimental conditions. Moreimportantly, InstructCell provides an accessible and intuitive tool forexploring complex single-cell data, lowering technical barriers and enablingdeeper biological insights.</description><author>Yin Fang, Xinle Deng, Kangwei Liu, Ningyu Zhang, Jingyang Qian, Penghui Yang, Xiaohui Fan, Huajun Chen</author><pubDate>Tue, 14 Jan 2025 15:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08187v1</guid></item><item><title>Assessing AI Adoption and Digitalization in SMEs: A Framework for Implementation</title><link>http://arxiv.org/abs/2501.08184v1</link><description>The primary objective of this research is to examine the current state ofdigitalization and the integration of artificial intelligence (AI) within smalland medium-sized enterprises (SMEs) in Italy. There is a significant gapbetween SMEs and large corporations in their use of AI, with SMEs facingnumerous barriers to adoption. This study identifies critical drivers andobstacles to achieving intelligent transformation, proposing a framework modelto address key challenges and provide actionable guidelines</description><author>Serena Proietti, Roberto Magnani</author><pubDate>Tue, 14 Jan 2025 15:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08184v1</guid></item><item><title>CG-MER: A Card Game-based Multimodal dataset for Emotion Recognition</title><link>http://arxiv.org/abs/2501.08182v1</link><description>The field of affective computing has seen significant advancements inexploring the relationship between emotions and emerging technologies. Thispaper presents a novel and valuable contribution to this field with theintroduction of a comprehensive French multimodal dataset designed specificallyfor emotion recognition. The dataset encompasses three primary modalities:facial expressions, speech, and gestures, providing a holistic perspective onemotions. Moreover, the dataset has the potential to incorporate additionalmodalities, such as Natural Language Processing (NLP) to expand the scope ofemotion recognition research. The dataset was curated through engagingparticipants in card game sessions, where they were prompted to express a rangeof emotions while responding to diverse questions. The study included 10sessions with 20 participants (9 females and 11 males). The dataset serves as avaluable resource for furthering research in emotion recognition and providesan avenue for exploring the intricate connections between human emotions anddigital technologies.</description><author>Nessrine Farhat, Amine Bohi, Leila Ben Letaifa, Rim Slama</author><pubDate>Tue, 14 Jan 2025 15:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08182v1</guid></item><item><title>WebWalker: Benchmarking LLMs in Web Traversal</title><link>http://arxiv.org/abs/2501.07572v2</link><description>Retrieval-augmented generation (RAG) demonstrates remarkable performanceacross tasks in open-domain question-answering. However, traditional searchengines may retrieve shallow content, limiting the ability of LLMs to handlecomplex, multi-layered information. To address it, we introduce WebWalkerQA, abenchmark designed to assess the ability of LLMs to perform web traversal. Itevaluates the capacity of LLMs to traverse a website's subpages to extracthigh-quality data systematically. We propose WebWalker, which is a multi-agentframework that mimics human-like web navigation through an explore-criticparadigm. Extensive experimental results show that WebWalkerQA is challengingand demonstrates the effectiveness of RAG combined with WebWalker, through thehorizontal and vertical integration in real-world scenarios.</description><author>Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, Fei Huang</author><pubDate>Tue, 14 Jan 2025 15:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.07572v2</guid></item><item><title>D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models</title><link>http://arxiv.org/abs/2501.08180v1</link><description>Diffusion models have achieved cutting-edge performance in image generation.However, their lengthy denoising process and computationally intensive scoreestimation network impede their scalability in low-latency andresource-constrained scenarios. Post-training quantization (PTQ) compresses andaccelerates diffusion models without retraining, but it inevitably introducesadditional quantization noise, resulting in mean and variance deviations. Inthis work, we propose D2-DPM, a dual denoising mechanism aimed at preciselymitigating the adverse effects of quantization noise on the noise estimationnetwork. Specifically, we first unravel the impact of quantization noise on thesampling equation into two components: the mean deviation and the variancedeviation. The mean deviation alters the drift coefficient of the samplingequation, influencing the trajectory trend, while the variance deviationmagnifies the diffusion coefficient, impacting the convergence of the samplingtrajectory. The proposed D2-DPM is thus devised to denoise the quantizationnoise at each time step, and then denoise the noisy sample through the inversediffusion iterations. Experimental results demonstrate that D2-DPM achievessuperior generation quality, yielding a 1.42 lower FID than the full-precisionmodel while achieving 3.99x compression and 11.67x bit-operation acceleration.</description><author>Qian Zeng, Jie Song, Han Zheng, Hao Jiang, Mingli Song</author><pubDate>Tue, 14 Jan 2025 15:03:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08180v1</guid></item><item><title>Object-Centric 2D Gaussian Splatting: Background Removal and Occlusion-Aware Pruning for Compact Object Models</title><link>http://arxiv.org/abs/2501.08174v1</link><description>Current Gaussian Splatting approaches are effective for reconstructing entirescenes but lack the option to target specific objects, making themcomputationally expensive and unsuitable for object-specific applications. Wepropose a novel approach that leverages object masks to enable targetedreconstruction, resulting in object-centric models. Additionally, we introducean occlusion-aware pruning strategy to minimize the number of Gaussians withoutcompromising quality. Our method reconstructs compact object models, yieldingobject-centric Gaussian and mesh representations that are up to 96\% smallerand up to 71\% faster to train compared to the baseline while retainingcompetitive quality. These representations are immediately usable fordownstream applications such as appearance editing and physics simulationwithout additional processing.</description><author>Marcel Rogge, Didier Stricker</author><pubDate>Tue, 14 Jan 2025 14:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08174v1</guid></item><item><title>Feedback-driven object detection and iterative model improvement</title><link>http://arxiv.org/abs/2411.19835v2</link><description>Automated object detection has become increasingly valuable across diverseapplications, yet efficient, high-quality annotation remains a persistentchallenge. In this paper, we present the development and evaluation of aplatform designed to interactively improve object detection models. Theplatform allows uploading and annotating images as well as fine-tuning objectdetection models. Users can then manually review and refine annotations,further creating improved snapshots that are used for automatic objectdetection on subsequent image uploads - a process we refer to as semi-automaticannotation resulting in a significant gain in annotation efficiency. Whereas iterative refinement of model results to speed up annotation hasbecome common practice, we are the first to quantitatively evaluate itsbenefits with respect to time, effort, and interaction savings. Ourexperimental results show clear evidence for a significant time reduction of upto 53% for semi-automatic compared to manual annotation. Importantly, theseefficiency gains did not compromise annotation quality, while matching oroccasionally even exceeding the accuracy of manual annotations. These findingsdemonstrate the potential of our lightweight annotation platform for creatinghigh-quality object detection datasets and provide best practices to guidefuture development of annotation platforms. The platform is open-source, with the frontend and backend repositoriesavailable on GitHub (https://github.com/ml-lab-htw/iterative-annotate). Tosupport the understanding of our labeling process, we have created anexplanatory video demonstrating the methodology using microscopy images of E.coli bacteria as an example. The video is available on YouTube(https://www.youtube.com/watch?v=CM9uhE8NN5E).</description><author>Sönke Tenckhoff, Mario Koddenbrock, Erik Rodner</author><pubDate>Tue, 14 Jan 2025 14:53:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.19835v2</guid></item><item><title>Benchmarking Multimodal Models for Fine-Grained Image Analysis: A Comparative Study Across Diverse Visual Features</title><link>http://arxiv.org/abs/2501.08170v1</link><description>This article introduces a benchmark designed to evaluate the capabilities ofmultimodal models in analyzing and interpreting images. The benchmark focuseson seven key visual aspects: main object, additional objects, background,detail, dominant colors, style, and viewpoint. A dataset of 14,580 images,generated from diverse text prompts, was used to assess the performance ofseven leading multimodal models. These models were evaluated on their abilityto accurately identify and describe each visual aspect, providing insights intotheir strengths and weaknesses for comprehensive image understanding. Thefindings of this benchmark have significant implications for the developmentand selection of multimodal models for various image analysis tasks.</description><author>Evgenii Evstafev</author><pubDate>Tue, 14 Jan 2025 14:50:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08170v1</guid></item><item><title>Revolutionizing Communication with Deep Learning and XAI for Enhanced Arabic Sign Language Recognition</title><link>http://arxiv.org/abs/2501.08169v1</link><description>This study introduces an integrated approach to recognizing Arabic SignLanguage (ArSL) using state-of-the-art deep learning models such asMobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhancedby explainable AI (XAI) techniques to boost interpretability. The ArSL2018 andRGB Arabic Alphabets Sign Language (AASL) datasets are employed, withEfficientNet-B2 achieving peak accuracies of 99.48\% and 98.99\%, respectively.Key innovations include sophisticated data augmentation methods to mitigateclass imbalance, implementation of stratified 5-fold cross-validation forbetter generalization, and the use of Grad-CAM for clear model decisiontransparency. The proposed system not only sets new benchmarks in recognitionaccuracy but also emphasizes interpretability, making it suitable forapplications in healthcare, education, and inclusive communicationtechnologies.</description><author>Mazen Balat, Rewaa Awaad, Ahmed B. Zaky, Salah A. Aly</author><pubDate>Tue, 14 Jan 2025 14:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08169v1</guid></item><item><title>LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and Dual-Process Thinking</title><link>http://arxiv.org/abs/2501.08168v1</link><description>While autonomous driving technology has made remarkable strides, data-drivenapproaches still struggle with complex scenarios due to their limited reasoningcapabilities. Meanwhile, knowledge-driven autonomous driving systems haveevolved considerably with the popularization of visual language models. In thispaper, we propose LeapVAD, a novel method based on cognitive perception anddual-process thinking. Our approach implements a human-attentional mechanism toidentify and focus on critical traffic elements that influence drivingdecisions. By characterizing these objects through comprehensive attributes -including appearance, motion patterns, and associated risks - LeapVAD achievesmore effective environmental representation and streamlines the decision-makingprocess. Furthermore, LeapVAD incorporates an innovative dual-processdecision-making module miming the human-driving learning process. The systemconsists of an Analytic Process (System-II) that accumulates driving experiencethrough logical reasoning and a Heuristic Process (System-I) that refines thisknowledge via fine-tuning and few-shot learning. LeapVAD also includesreflective mechanisms and a growing memory bank, enabling it to learn from pastmistakes and continuously improve its performance in a closed-loop environment.To enhance efficiency, we develop a scene encoder network that generatescompact scene representations for rapid retrieval of relevant drivingexperiences. Extensive evaluations conducted on two leading autonomous drivingsimulators, CARLA and DriveArena, demonstrate that LeapVAD achieves superiorperformance compared to camera-only approaches despite limited training data.Comprehensive ablation studies further emphasize its effectiveness incontinuous learning and domain adaptation. Project page:https://pjlab-adg.github.io/LeapVAD/.</description><author>Yukai Ma, Tiantian Wei, Naiting Zhong, Jianbiao Mei, Tao Hu, Licheng Wen, Xuemeng Yang, Botian Shi, Yong Liu</author><pubDate>Tue, 14 Jan 2025 14:49:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08168v1</guid></item><item><title>Potential and Perils of Large Language Models as Judges of Unstructured Textual Data</title><link>http://arxiv.org/abs/2501.08167v1</link><description>Rapid advancements in large language models have unlocked remarkablecapabilities when it comes to processing and summarizing unstructured textdata. This has implications for the analysis of rich, open-ended datasets, suchas survey responses, where LLMs hold the promise of efficiently distilling keythemes and sentiments. However, as organizations increasingly turn to thesepowerful AI systems to make sense of textual feedback, a critical questionarises, can we trust LLMs to accurately represent the perspectives containedwithin these text based datasets? While LLMs excel at generating human-likesummaries, there is a risk that their outputs may inadvertently diverge fromthe true substance of the original responses. Discrepancies between theLLM-generated outputs and the actual themes present in the data could lead toflawed decision-making, with far-reaching consequences for organizations. Thisresearch investigates the effectiveness of LLMs as judge models to evaluate thethematic alignment of summaries generated by other LLMs. We utilized anAnthropic Claude model to generate thematic summaries from open-ended surveyresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving asLLM judges. The LLM-as-judge approach was compared to human evaluations usingCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalablealternative to traditional human centric evaluation methods. Our findingsreveal that while LLMs as judges offer a scalable solution comparable to humanraters, humans may still excel at detecting subtle, context-specific nuances.This research contributes to the growing body of knowledge on AI assisted textanalysis. We discuss limitations and provide recommendations for futureresearch, emphasizing the need for careful consideration when generalizing LLMjudge models across various contexts and use cases.</description><author>Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar</author><pubDate>Tue, 14 Jan 2025 14:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08167v1</guid></item><item><title>ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark Detection</title><link>http://arxiv.org/abs/2412.13174v2</link><description>Although facial landmark detection (FLD) has gained significant progress,existing FLD methods still suffer from performance drops on partiallynon-visible faces, such as faces with occlusions or under extreme lightingconditions or poses. To address this issue, we introduce ORFormer, a noveltransformer-based method that can detect non-visible regions and recover theirmissing features from visible parts. Specifically, ORFormer associates eachimage patch token with one additional learnable token called the messengertoken. The messenger token aggregates features from all but its patch. Thisway, the consensus between a patch and other patches can be assessed byreferring to the similarity between its regular and messenger embeddings,enabling non-visible region identification. Our method then recovers occludedpatches with features aggregated by the messenger tokens. Leveraging therecovered features, ORFormer compiles high-quality heatmaps for the downstreamFLD task. Extensive experiments show that our method generates heatmapsresilient to partial occlusions. By integrating the resultant heatmaps intoexisting FLD methods, our method performs favorably against the state of thearts on challenging datasets such as WFLW and COFW.</description><author>Jui-Che Chiang, Hou-Ning Hu, Bo-Syuan Hou, Chia-Yu Tseng, Yu-Lun Liu, Min-Hung Chen, Yen-Yu Lin</author><pubDate>Tue, 14 Jan 2025 14:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.13174v2</guid></item><item><title>I Can Find You in Seconds! Leveraging Large Language Models for Code Authorship Attribution</title><link>http://arxiv.org/abs/2501.08165v1</link><description>Source code authorship attribution is important in software forensics,plagiarism detection, and protecting software patch integrity. Existingtechniques often rely on supervised machine learning, which struggles withgeneralization across different programming languages and coding styles due tothe need for large labeled datasets. Inspired by recent advances in naturallanguage authorship analysis using large language models (LLMs), which haveshown exceptional performance without task-specific tuning, this paper exploresthe use of LLMs for source code authorship attribution. We present a comprehensive study demonstrating that state-of-the-art LLMs cansuccessfully attribute source code authorship across different languages. LLMscan determine whether two code snippets are written by the same author withzero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of0.78, and can attribute code authorship from a small set of reference codesnippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs showsome adversarial robustness against misattribution attacks. Despite these capabilities, we found that naive prompting of LLMs does notscale well with a large number of authors due to input token limitations. Toaddress this, we propose a tournament-style approach for large-scaleattribution. Evaluating this approach on datasets of C++ (500 authors, 26,355samples) and Java (686 authors, 55,267 samples) code from GitHub, we achieveclassification accuracy of up to 65% for C++ and 68.7% for Java using only onereference per author. These results open new possibilities for applying LLMs tocode authorship attribution in cybersecurity and software engineering.</description><author>Soohyeon Choi, Yong Kiam Tan, Mark Huasong Meng, Mohamed Ragab, Soumik Mondal, David Mohaisen, Khin Mi Mi Aung</author><pubDate>Tue, 14 Jan 2025 14:46:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08165v1</guid></item><item><title>DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction</title><link>http://arxiv.org/abs/2501.08163v1</link><description>The accelerated MRI reconstruction poses a challenging ill-posed inverseproblem due to the significant undersampling in k-space. Deep neural networks,such as CNNs and ViT, have shown substantial performance improvements for thistask while encountering the dilemma between global receptive fields andefficient computation. To this end, this paper pioneers exploring Mamba, a newparadigm for long-range dependency modeling with linear complexity, forefficient and effective MRI reconstruction. However, directly applying Mamba toMRI reconstruction faces three significant issues: (1) Mamba's row-wise andcolumn-wise scanning disrupts k-space's unique spectrum, leaving its potentialin k-space learning unexplored. (2) Existing Mamba methods unfold feature mapswith multiple lengthy scanning paths, leading to long-range forgetting and highcomputational burden. (3) Mamba struggles with spatially-varying contents,resulting in limited diversity of local representations. To address these, wepropose a dual-domain multi-scale Mamba for MRI reconstruction from thefollowing perspectives: (1) We pioneer vision Mamba in k-space learning. Acircular scanning is customized for spectrum unfolding, benefiting the globalmodeling of k-space. (2) We propose a multi-scale Mamba with an efficientscanning strategy in both image and k-space domains. It mitigates long-rangeforgetting and achieves a better trade-off between efficiency and performance.(3) We develop a local diversity enhancement module to improve thespatially-varying representation of Mamba. Extensive experiments are conductedon three public datasets for MRI reconstruction under various undersamplingpatterns. Comprehensive results demonstrate that our method significantlyoutperforms state-of-the-art methods with lower computational cost.Implementation code will be available athttps://github.com/XiaoMengLiLiLi/DM-Mamba.</description><author>Yucong Meng, Zhiwei Yang, Zhijian Song, Yonghong Shi</author><pubDate>Tue, 14 Jan 2025 14:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08163v1</guid></item><item><title>Diversified Augmentation with Domain Adaptation for Debiased Video Temporal Grounding</title><link>http://arxiv.org/abs/2501.06746v2</link><description>Temporal sentence grounding in videos (TSGV) faces challenges due to publicTSGV datasets containing significant temporal biases, which are attributed tothe uneven temporal distributions of target moments. Existing methods generateaugmented videos, where target moments are forced to have varying temporallocations. However, since the video lengths of the given datasets have smallvariations, only changing the temporal locations results in poor generalizationability in videos with varying lengths. In this paper, we propose a noveltraining framework complemented by diversified data augmentation and a domaindiscriminator. The data augmentation generates videos with various lengths andtarget moment locations to diversify temporal distributions. However, augmentedvideos inevitably exhibit distinct feature distributions which may introducenoise. To address this, we design a domain adaptation auxiliary task todiminish feature discrepancies between original and augmented videos. We alsoencourage the model to produce distinct predictions for videos with the sametext queries but different moment locations to promote debiased training.Experiments on Charades-CD and ActivityNet-CD datasets demonstrate theeffectiveness and generalization abilities of our method in multiple groundingstructures, achieving state-of-the-art results.</description><author>Junlong Ren, Gangjian Zhang, Haifeng Sun, Hao Wang</author><pubDate>Tue, 14 Jan 2025 14:40:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06746v2</guid></item><item><title>MSCViT: A Small-size ViT architecture with Multi-Scale Self-Attention Mechanism for Tiny Datasets</title><link>http://arxiv.org/abs/2501.06040v2</link><description>Vision Transformer (ViT) has demonstrated significant potential in variousvision tasks due to its strong ability in modelling long-range dependencies.However, such success is largely fueled by training on massive samples. In realapplications, the large-scale datasets are not always available, and ViTperforms worse than Convolutional Neural Networks (CNNs) if it is only trainedon small scale dataset (called tiny dataset), since it requires large amount oftraining data to ensure its representational capacity. In this paper, asmall-size ViT architecture with multi-scale self-attention mechanism andconvolution blocks is presented (dubbed MSCViT) to model different scales ofattention at each layer. Firstly, we introduced wavelet convolution, whichselectively combines the high-frequency components obtained by frequencydivision with our convolution channel to extract local features. Then, alightweight multi-head attention module is developed to reduce the number oftokens and computational costs. Finally, the positional encoding (PE) in thebackbone is replaced by a local feature extraction module. Compared with theoriginal ViT, it is parameter-efficient and is particularly suitable for tinydatasets. Extensive experiments have been conducted on tiny datasets, in whichour model achieves an accuracy of 84.68% on CIFAR-100 with 14.0M parameters and2.5 GFLOPs, without pre-training on large datasets.</description><author>Bowei Zhang, Yi Zhang</author><pubDate>Tue, 14 Jan 2025 14:33:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.06040v2</guid></item><item><title>Inference-Time-Compute: More Faithful? A Research Note</title><link>http://arxiv.org/abs/2501.08156v1</link><description>Models trained specifically to generate long Chains of Thought (CoTs) haverecently achieved impressive results. We refer to these models asInference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithfulcompared to traditional non-ITC models? We evaluate two ITC models (based onQwen-2.5 and Gemini-2) on an existing test of faithful CoT To measurefaithfulness, we test if models articulate cues in their prompt that influencetheir answers to MMLU questions. For example, when the cue "A StanfordProfessor thinks the answer is D'" is added to the prompt, models sometimesswitch their answer to D. In such cases, the Gemini ITC model articulates thecue 54% of the time, compared to 14% for the non-ITC Gemini. We evaluate 7 types of cue, such as misleading few-shot examples andanchoring on past responses. ITC models articulate cues that influence themmuch more reliably than all the 6 non-ITC models tested, such asClaude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time. However, our study has important limitations. We evaluate only two ITC models-- we cannot evaluate OpenAI's SOTA o1 model. We also lack details about thetraining of these ITC models, making it hard to attribute our findings tospecific processes. We think faithfulness of CoT is an important property for AI Safety. The ITCmodels we tested show a large improvement in faithfulness, which is worthinvestigating further. To speed up this investigation, we release these earlyresults as a research note.</description><author>James Chua, Owain Evans</author><pubDate>Tue, 14 Jan 2025 14:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08156v1</guid></item><item><title>FairTTTS: A Tree Test Time Simulation Method for Fairness-Aware Classification</title><link>http://arxiv.org/abs/2501.08155v1</link><description>Algorithmic decision-making has become deeply ingrained in many domains, yetbiases in machine learning models can still produce discriminatory outcomes,often harming unprivileged groups. Achieving fair classification is inherentlychallenging, requiring a careful balance between predictive performance andethical considerations. We present FairTTTS, a novel post-processing biasmitigation method inspired by the Tree Test Time Simulation (TTTS) method.Originally developed to enhance accuracy and robustness against adversarialinputs through probabilistic decision-path adjustments, TTTS serves as thefoundation for FairTTTS. By building on this accuracy-enhancing technique,FairTTTS mitigates bias and improves predictive performance. FairTTTS uses adistance-based heuristic to adjust decisions at protected attribute nodes,ensuring fairness for unprivileged samples. This fairness-oriented adjustmentoccurs as a post-processing step, allowing FairTTTS to be applied topre-trained models, diverse datasets, and various fairness metrics withoutretraining. Extensive evaluation on seven benchmark datasets shows thatFairTTTS outperforms traditional methods in fairness improvement, achieving a20.96% average increase over the baseline compared to 18.78% for related work,and further enhances accuracy by 0.55%. In contrast, competing methodstypically reduce accuracy by 0.42%. These results confirm that FairTTTSeffectively promotes more equitable decision-making while simultaneouslyimproving predictive performance.</description><author>Nurit Cohen-Inger, Lior Rokach, Bracha Shapira, Seffi Cohen</author><pubDate>Tue, 14 Jan 2025 14:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08155v1</guid></item></channel></rss>