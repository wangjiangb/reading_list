<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 02 Jan 2025 13:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>PERSE: Personalized 3D Generative Avatars from A Single Portrait</title><link>http://arxiv.org/abs/2412.21206v1</link><description>We present PERSE, a method for building an animatable personalized generativeavatar from a reference portrait. Our avatar model enables facial attributeediting in a continuous and disentangled latent space to control each facialattribute, while preserving the individual's identity. To achieve this, ourmethod begins by synthesizing large-scale synthetic 2D video datasets, whereeach video contains consistent changes in the facial expression and viewpoint,combined with a variation in a specific facial attribute from the originalinput. We propose a novel pipeline to produce high-quality, photorealistic 2Dvideos with facial attribute editing. Leveraging this synthetic attributedataset, we present a personalized avatar creation method based on the 3DGaussian Splatting, learning a continuous and disentangled latent space forintuitive facial attribute manipulation. To enforce smooth transitions in thislatent space, we introduce a latent space regularization technique by usinginterpolated 2D faces as supervision. Compared to previous approaches, wedemonstrate that PERSE generates high-quality avatars with interpolatedattributes while preserving identity of reference person.</description><author>Hyunsoo Cha, Inhee Lee, Hanbyul Joo</author><pubDate>Mon, 30 Dec 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21206v1</guid></item><item><title>Action-Agnostic Point-Level Supervision for Temporal Action Detection</title><link>http://arxiv.org/abs/2412.21205v1</link><description>We propose action-agnostic point-level (AAPL) supervision for temporal actiondetection to achieve accurate action instance detection with a lightlyannotated dataset. In the proposed scheme, a small portion of video frames issampled in an unsupervised manner and presented to human annotators, who thenlabel the frames with action categories. Unlike point-level supervision, whichrequires annotators to search for every action instance in an untrimmed video,frames to annotate are selected without human intervention in AAPL supervision.We also propose a detection model and learning method to effectively utilizethe AAPL labels. Extensive experiments on the variety of datasets (THUMOS '14,FineAction, GTEA, BEOID, and ActivityNet 1.3) demonstrate that the proposedapproach is competitive with or outperforms prior methods for video-level andpoint-level supervision in terms of the trade-off between the annotation costand detection performance.</description><author>Shuhei M. Yoshida, Takashi Shibata, Makoto Terao, Takayuki Okatani, Masashi Sugiyama</author><pubDate>Mon, 30 Dec 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21205v1</guid></item><item><title>SoS Certificates for Sparse Singular Values and Their Applications: Robust Statistics, Subspace Distortion, and More</title><link>http://arxiv.org/abs/2412.21203v1</link><description>We study $\textit{sparse singular value certificates}$ for random rectangularmatrices. If $M$ is an $n \times d$ matrix with independent Gaussian entries,we give a new family of polynomial-time algorithms which can certify upperbounds on the maximum of $\|M u\|$, where $u$ is a unit vector with at most$\eta n$ nonzero entries for a given $\eta \in (0,1)$. This basic algorithmicprimitive lies at the heart of a wide range of problems across algorithmicstatistics and theoretical computer science. Our algorithms certify a bound which is asymptotically smaller than the naiveone, given by the maximum singular value of $M$, for nearly the widest-possiblerange of $n,d,$ and $\eta$. Efficiently certifying such a bound for a range of$n,d$ and $\eta$ which is larger by any polynomial factor than what is achievedby our algorithm would violate lower bounds in the SQ and low-degreepolynomials models. Our certification algorithm makes essential use of theSum-of-Squares hierarchy. To prove the correctness of our algorithm, we developa new combinatorial connection between the graph matrix approach to analyzerandom matrices with dependent entries, and the Efron-Stein decomposition offunctions of independent random variables. As applications of our certification algorithm, we obtain new efficientalgorithms for a wide range of well-studied algorithmic tasks. In algorithmicrobust statistics, we obtain new algorithms for robust mean and covarianceestimation with tradeoffs between breakdown point and sample complexity, whichare nearly matched by SQ and low-degree polynomial lower bounds (that weestablish). We also obtain new polynomial-time guarantees for certification of$\ell_1/\ell_2$ distortion of random subspaces of $\mathbb{R}^n$ (also withnearly matching lower bounds), sparse principal component analysis, andcertification of the $2\rightarrow p$ norm of a random matrix.</description><author>Ilias Diakonikolas, Samuel B. Hopkins, Ankit Pensia, Stefan Tiegel</author><pubDate>Mon, 30 Dec 2024 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21203v1</guid></item><item><title>Distributed Mixture-of-Agents for Edge Inference with Large Language Models</title><link>http://arxiv.org/abs/2412.21200v1</link><description>Mixture-of-Agents (MoA) has recently been proposed as a method to enhanceperformance of large language models (LLMs), enabling multiple individual LLMsto work together for collaborative inference. This collaborative approachresults in improved responses to user prompts compared to relying on a singleLLM. In this paper, we consider such an MoA architecture in a distributedsetting, where LLMs operate on individual edge devices, each uniquelyassociated with a user and equipped with its own distributed computing power.These devices exchange information using decentralized gossip algorithms,allowing different device nodes to talk without the supervision of acentralized server. In the considered setup, different users have their own LLMmodels to address user prompts. Additionally, the devices gossip either theirown user-specific prompts or augmented prompts to generate more refined answersto certain queries. User prompts are temporarily stored in the device queueswhen their corresponding LLMs are busy. Given the memory limitations of edgedevices, it is crucial to ensure that the average queue sizes in the systemremain bounded. In this paper, we address this by theoretically calculating thequeuing stability conditions for the device queues under reasonableassumptions, which we validate experimentally as well. Further, we demonstratethrough experiments, leveraging open-source LLMs for the implementation ofdistributed MoA, that certain MoA configurations produce higher-qualityresponses compared to others, as evaluated on AlpacaEval 2.0 benchmark. Theimplementation is available at:https://github.com/purbeshmitra/distributed_moa.</description><author>Purbesh Mitra, Priyanka Kaswan, Sennur Ulukus</author><pubDate>Mon, 30 Dec 2024 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21200v1</guid></item><item><title>HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation</title><link>http://arxiv.org/abs/2412.21199v1</link><description>We introduce self-invoking code generation, a new task designed to evaluatethe progressive reasoning and problem-solving capabilities of LLMs. In thistask, models are presented with a base problem and a related, more complexproblem. They must solve the base problem and then utilize its solution toaddress the more complex one. This work features three key contributions.First, we propose a general recipe for generating more challenging versions ofexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPPPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs onself-invoking code generation. Second, from the analysis of experimentalresults over twenty LLMs on our benchmarks, we have two important observations:(i) Most LLMs excel in traditional code generation benchmarks like HumanEvaland MBPP, but their performance declines on self-invoking tasks. For example,o1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.(ii) On self-invoking code generation task, the instruction-tuned modelsdemonstrate only marginal improvements compared to the base models. Third, wedisclose the types of failure modes that exist in our evaluation results. Allthese results underscore the need for further advancements in self-invokingcode generation tasks and provide a new direction for future research onenhancing LLMs' code reasoning capabilities.</description><author>Zhaojian Yu, Yilun Zhao, Arman Cohan, Xiao-Ping Zhang</author><pubDate>Mon, 30 Dec 2024 18:58:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21199v1</guid></item><item><title>A Large-Scale Study on Video Action Dataset Condensation</title><link>http://arxiv.org/abs/2412.21197v1</link><description>Dataset condensation has made significant progress in the image domain.Unlike images, videos possess an additional temporal dimension, which harborsconsiderable redundant information, making condensation even more crucial.However, video dataset condensation still remains an underexplored area. We aimto bridge this gap by providing a large-scale empirical study with systematicdesign and fair comparison. Specifically, our work delves into three keyaspects to provide valuable empirical insights: (1) temporal processing ofvideo data, (2) establishing a comprehensive evaluation protocol for videodataset condensation, and (3) adaptation of condensation methods to thespace-time domain and fair comparisons among them. From this study, we deriveseveral intriguing observations: (i) sample diversity appears to be morecrucial than temporal diversity for video dataset condensation, (ii) simpleslide-window sampling proves to be effective, and (iii) sample selectioncurrently outperforms dataset distillation in most cases. Furthermore, weconduct experiments on three prominent action recognition datasets (HMDB51,UCF101 and Kinetics-400) and achieve state-of-the-art results on all of them.Our code is available at https://github.com/MCG-NJU/Video-DC.</description><author>Yang Chen, Sheng Guo, Limin Wang</author><pubDate>Mon, 30 Dec 2024 18:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21197v1</guid></item><item><title>Sparse chaos in cortical circuits</title><link>http://arxiv.org/abs/2412.21188v1</link><description>Nerve impulses, the currency of information flow in the brain, are generatedby an instability of the neuronal membrane potential dynamics. Neuronalcircuits exhibit collective chaos that appears essential for learning, memory,sensory processing, and motor control. However, the factors controlling thenature and intensity of collective chaos in neuronal circuits are not wellunderstood. Here we use computational ergodic theory to demonstrate that basicfeatures of nerve impulse generation profoundly affect collective chaos inneuronal circuits. Numerically exact calculations of Lyapunov spectra,Kolmogorov-Sinai-entropy, and upper and lower bounds on attractor dimensionshow that changes in nerve impulse generation in individual neurons moderatelyimpact information encoding rates but qualitatively transform phase spacestructure. Specifically, we find a drastic reduction in the number of unstablemanifolds, Kolmogorov-Sinai entropy, and attractor dimension. Beyond a criticalpoint, marked by the simultaneous breakdown of the diffusion approximation, apeak in the largest Lyapunov exponent, and a localization transition of theleading covariant Lyapunov vector, networks exhibit sparse chaos: prolongedperiods of near stable dynamics interrupted by short bursts of intense chaos.Analysis of large, more realistically structured networks supports thegenerality of these findings. In cortical circuits, biophysical propertiesappear tuned to this regime of sparse chaos. Our results reveal a close linkbetween fundamental aspects of single-neuron biophysics and the collectivedynamics of cortical circuits, suggesting that nerve impulse generationmechanisms are adapted to enhance circuit controllability and information flow.</description><author>Rainer Engelken, Michael Monteforte, Fred Wolf</author><pubDate>Mon, 30 Dec 2024 18:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21188v1</guid></item><item><title>Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</title><link>http://arxiv.org/abs/2412.21187v1</link><description>The remarkable performance of models like the OpenAI o1 can be attributed totheir ability to emulate human-like long-time thinking during inference. Thesemodels employ extended chain-of-thought (CoT) processes, exploring multiplestrategies to enhance problem-solving capabilities. However, a criticalquestion remains: How to intelligently and efficiently scale computationalresources during testing. This paper presents the first comprehensive study onthe prevalent issue of overthinking in these models, where excessivecomputational resources are allocated for simple problems with minimal benefit.We introduce novel efficiency metrics from both outcome and processperspectives to evaluate the rational use of computational resources by o1-likemodels. Using a self-training paradigm, we propose strategies to mitigateoverthinking, streamlining reasoning processes without compromising accuracy.Experimental results show that our approach successfully reduces computationaloverhead while preserving model performance across a range of testsets withvarying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.</description><author>Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu</author><pubDate>Mon, 30 Dec 2024 18:55:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21187v1</guid></item><item><title>Two-component spatiotemporal template for activation-inhibition of speech in ECoG</title><link>http://arxiv.org/abs/2412.21178v1</link><description>I compute the average trial-by-trial power of band-limited speech activityacross epochs of multi-channel high-density electrocorticography (ECoG)recorded from multiple subjects during a consonant-vowel speaking task. I showthat previously seen anti-correlations of average beta frequency activity(12-35 Hz) to high-frequency gamma activity (70-140 Hz) during speech movementare observable between individual ECoG channels in the sensorimotor cortex(SMC). With this I fit a variance-based model using principal componentanalysis to the band-powers of individual channels of session-averaged ECoGdata in the SMC and project SMC channels onto their lower-dimensional principalcomponents. Spatiotemporal relationships between speech-related activity and principalcomponents are identified by correlating the principal components of bothfrequency bands to individual ECoG channels over time using windowedcorrelation. Correlations of principal component areas to sensorimotor areasreveal a distinct two-component activation-inhibition-like representation forspeech that resembles distinct local sensorimotor areas recently shown to havecomplex interplay in whole-body motor control, inhibition, and posture. Notablythe third principal component shows insignificant correlations across allsubjects, suggesting two components of ECoG are sufficient to represent SMCactivity during speech movement.</description><author>Eric Easthope</author><pubDate>Mon, 30 Dec 2024 18:50:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21178v1</guid></item><item><title>Adversarial Attack and Defense for LoRa Device Identification and Authentication via Deep Learning</title><link>http://arxiv.org/abs/2412.21164v1</link><description>LoRa provides long-range, energy-efficient communications in Internet ofThings (IoT) applications that rely on Low-Power Wide-Area Network (LPWAN)capabilities. Despite these merits, concerns persist regarding the security ofLoRa networks, especially in situations where device identification andauthentication are imperative to secure the reliable access to the LoRanetworks. This paper explores a deep learning (DL) approach to tackle theseconcerns, focusing on two critical tasks, namely (i) identifying LoRa devicesand (ii) classifying them to legitimate and rogue devices. Deep neural networks(DNNs), encompassing both convolutional and feedforward neural networks, aretrained for these tasks using actual LoRa signal data. In this setting, theadversaries may spoof rogue LoRa signals through the kernel density estimation(KDE) method based on legitimate device signals that are received by theadversaries. Two cases are considered, (i) training two separate classifiers,one for each of the two tasks, and (ii) training a multi-task classifier forboth tasks. The vulnerabilities of the resulting DNNs to manipulations in inputsamples are studied in form of untargeted and targeted adversarial attacksusing the Fast Gradient Sign Method (FGSM). Individual and common perturbationsare considered against single-task and multi-task classifiers for the LoRasignal analysis. To provide resilience against such attacks, a defense approachis presented by increasing the robustness of classifiers with adversarialtraining. Results quantify how vulnerable LoRa signal classification tasks areto adversarial attacks and emphasize the need to fortify IoT applicationsagainst these subtle yet effective threats.</description><author>Yalin E. Sagduyu, Tugba Erpek</author><pubDate>Mon, 30 Dec 2024 18:43:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21164v1</guid></item><item><title>Open RAN-Enabled Deep Learning-Assisted Mobility Management for Connected Vehicles</title><link>http://arxiv.org/abs/2412.21161v1</link><description>Connected Vehicles (CVs) can leverage the unique features of 5G and future6G/NextG networks to enhance Intelligent Transportation System (ITS) services.However, even with advancements in cellular network generations, CVapplications may experience communication interruptions in high-mobilityscenarios due to frequent changes of serving base station, also known ashandovers (HOs). This paper proposes the adoption of Open Radio Access Network(Open RAN/O-RAN) and deep learning models for decision-making to preventQuality of Service (QoS) degradation due to HOs and to ensure the timelyconnectivity needed for CV services. The solution utilizes the O-RAN SoftwareCommunity (OSC), an open-source O-RAN platform developed by the collaborationbetween the O-RAN Alliance and Linux Foundation, to develop xApps that areexecuted in the near-Real-Time RIC of OSC. To demonstrate the proposal'seffectiveness, an integrated framework combining the OMNeT++ simulator and OSCwas created. Evaluations used real-world datasets in urban applicationscenarios, such as video streaming transmission and over-the-air (OTA) updates.Results indicate that the proposal achieved superior performance and reducedlatency compared to the standard 3GPP HO procedure.</description><author>Maria Barbosa, Kelvin Dias</author><pubDate>Mon, 30 Dec 2024 18:41:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21161v1</guid></item><item><title>Unified dimensionality reduction techniques in chronic liver disease detection</title><link>http://arxiv.org/abs/2412.21156v1</link><description>Globally, chronic liver disease continues to be a major health concern thatrequires precise predictive models for prompt detection and treatment. Usingthe Indian Liver Patient Dataset (ILPD) from the University of California atIrvine's UCI Machine Learning Repository, a number of machine learningalgorithms are investigated in this study. The main focus of our research isthis dataset, which includes the medical records of 583 patients, 416 of whomhave been diagnosed with liver disease and 167 of whom have not. There areseveral aspects to this work, including feature extraction and dimensionalityreduction methods like Linear Discriminant Analysis (LDA), Factor Analysis(FA), t-distributed Stochastic Neighbour Embedding (t-SNE), and UniformManifold Approximation and Projection (UMAP). The purpose of the study is toinvestigate how well these approaches work for converting high-dimensionaldatasets and improving prediction accuracy. To assess the prediction ability ofthe improved models, a number of classification methods were used, such asMulti-layer Perceptron, Random Forest, K-nearest neighbours, and LogisticRegression. Remarkably, the improved models performed admirably, with RandomForest having the highest accuracy of 98.31\% in 10-fold cross-validation and95.79\% in train-test split evaluation. Findings offer important newperspectives on the choice and use of customized feature extraction anddimensionality reduction methods, which improve predictive models for patientswith chronic liver disease.</description><author>Anand Karna, Naina Khan, Rahul Rauniyar, Prashant Giridhar Shambharkar</author><pubDate>Mon, 30 Dec 2024 18:35:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21156v1</guid></item><item><title>Low coordinate degree algorithms II: Categorical signals and generalized stochastic block models</title><link>http://arxiv.org/abs/2412.21155v1</link><description>We study when low coordinate degree functions (LCDF) -- linear combinationsof functions depending on small subsets of entries of a vector -- can test forthe presence of categorical structure, including community structure andgeneralizations thereof, in high-dimensional data. This complements the firstpaper of this series, which studied the power of LCDF in testing for continuousstructure like real-valued signals perturbed by additive noise. We apply thetools developed there to a general form of stochastic block model (SBM), wherea population is assigned random labels and every $p$-tuple of the populationgenerates an observation according to an arbitrary probability measureassociated to the $p$ labels of its members. We show that the performance ofLCDF admits a unified analysis for this class of models. As applications, weprove tight lower bounds against LCDF (and therefore also against low degreepolynomials) for nearly arbitrary graph and regular hypergraph SBMs, alwaysmatching suitable generalizations of the Kesten-Stigum threshold. We also provetight lower bounds for group synchronization and abelian group sumset problemsunder the "truth-or-Haar" noise model, and use our technical results to give animproved analysis of Gaussian multi-frequency group synchronization. In most ofthese models, for some parameter settings our lower bounds give new evidencefor conjectural statistical-to-computational gaps. Finally, interpreting someof our findings, we propose a precise analogy between categorical andcontinuous signals: a general SBM as above behaves, in terms of the tradeoffbetween subexponential runtime cost of testing algorithms and the signalstrength needed for a testing algorithm to succeed, like a spiked $p_*$-tensormodel of a certain order $p_*$ that may be computed from the parameters of theSBM.</description><author>Dmitriy Kunisky</author><pubDate>Mon, 30 Dec 2024 18:34:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21155v1</guid></item><item><title>Aviary: training language agents on challenging scientific tasks</title><link>http://arxiv.org/abs/2412.21154v1</link><description>Solving complex real-world tasks requires cycles of actions and observations.This is particularly true in science, where tasks require many cycles ofanalysis, tool use, and experimentation. Language agents are promising forautomating intellectual tasks in science because they can interact with toolsvia natural language or code. Yet their flexibility creates conceptual andpractical challenges for software implementations, since agents may comprisenon-standard components such as internal reasoning, planning, tool usage, aswell as the inherent stochasticity of temperature-sampled language models.Here, we introduce Aviary, an extensible gymnasium for language agents. Weformalize agents as policies solving language-grounded partially observableMarkov decision processes, which we term language decision processes. We thenimplement five environments, including three challenging scientificenvironments: (1) manipulating DNA constructs for molecular cloning, (2)answering research questions by accessing scientific literature, and (3)engineering protein stability. These environments were selected for their focuson multi-step reasoning and their relevance to contemporary biology research.Finally, with online training and scaling inference-time compute, we show thatlanguage agents backed by open-source, non-frontier LLMs can match and exceedboth frontier LLM agents and human experts on multiple tasks at up to 100xlower inference cost.</description><author>Siddharth Narayanan, James D. Braza, Ryan-Rhys Griffiths, Manu Ponnapati, Albert Bou, Jon Laurent, Ori Kabeli, Geemi Wellawatte, Sam Cox, Samuel G. Rodriques, Andrew D. White</author><pubDate>Mon, 30 Dec 2024 18:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21154v1</guid></item><item><title>PyG-SSL: A Graph Self-Supervised Learning Toolkit</title><link>http://arxiv.org/abs/2412.21151v1</link><description>Graph Self-Supervised Learning (SSL) has emerged as a pivotal area ofresearch in recent years. By engaging in pretext tasks to learn the intricatetopological structures and properties of graphs using unlabeled data, thesegraph SSL models achieve enhanced performance, improved generalization, andheightened robustness. Despite the remarkable achievements of these graph SSLmethods, their current implementation poses significant challenges forbeginners and practitioners due to the complex nature of graph structures,inconsistent evaluation metrics, and concerns regarding reproducibility hinderfurther progress in this field. Recognizing the growing interest within theresearch community, there is an urgent need for a comprehensive,beginner-friendly, and accessible toolkit consisting of the most representativegraph SSL algorithms. To address these challenges, we present a Graph SSLtoolkit named PyG-SSL, which is built upon PyTorch and is compatible withvarious deep learning and scientific computing backends. Within the toolkit, weoffer a unified framework encompassing dataset loading, hyper-parameterconfiguration, model training, and comprehensive performance evaluation fordiverse downstream tasks. Moreover, we provide beginner-friendly tutorials andthe best hyper-parameters of each graph SSL algorithm on different graphdatasets, facilitating the reproduction of results. The GitHub repository ofthe library is https://github.com/iDEA-iSAIL-Lab-UIUC/pyg-ssl.</description><author>Lecheng Zheng, Baoyu Jing, Zihao Li, Zhichen Zeng, Tianxin Wei, Mengting Ai, Xinrui He, Lihui Liu, Dongqi Fu, Jiaxuan You, Hanghang Tong, Jingrui He</author><pubDate>Mon, 30 Dec 2024 18:32:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21151v1</guid></item><item><title>Functional Risk Minimization</title><link>http://arxiv.org/abs/2412.21149v1</link><description>The field of Machine Learning has changed significantly since the 1970s.However, its most basic principle, Empirical Risk Minimization (ERM), remainsunchanged. We propose Functional Risk Minimization~(FRM), a general frameworkwhere losses compare functions rather than outputs. This results in betterperformance in supervised, unsupervised, and RL experiments. In the FRMparadigm, for each data point $(x_i,y_i)$ there is function $f_{\theta_i}$ thatfits it: $y_i = f_{\theta_i}(x_i)$. This allows FRM to subsume ERM for manycommon loss functions and to capture more realistic noise processes. We alsoshow that FRM provides an avenue towards understanding generalization in themodern over-parameterized regime, as its objective can be framed as finding thesimplest model that fits the training data.</description><author>Ferran Alet, Clement Gehring, Tomás Lozano-Pérez, Kenji Kawaguchi, Joshua B. Tenenbaum, Leslie Pack Kaelbling</author><pubDate>Mon, 30 Dec 2024 18:29:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21149v1</guid></item><item><title>Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models</title><link>http://arxiv.org/abs/2401.10690v3</link><description>Dyadic regression models, which output real-valued predictions for pairs ofentities, are fundamental in many domains (e.g. obtaining user-product ratingsin Recommender Systems) and promising and under exploration in others (e.g.tuning patient-drug dosages in personalized pharmacology). In this work, weprove that non-uniform observed value distributions of individual entities leadto severe biases in state-of-the-art models, skewing predictions towards theaverage of observed past values for the entity and providing worse-than-randompredictive power in eccentric yet crucial cases; we name this phenomenoneccentricity bias. We show that global error metrics like Root Mean SquaredError (RMSE) are insufficient to capture this bias, and we introduceEccentricity-Area Under the Curve (EAUC) as a novel complementary metric thatcan quantify it in all studied domains and models. We prove the intuitiveinterpretation of EAUC by experimenting with naive post-training biascorrections, and theorize other options to use EAUC to guide the constructionof fair models. This work contributes a bias-aware evaluation of dyadicregression to prevent unfairness in critical real-world applications of suchsystems.</description><author>Jorge Paz-Ruza, Amparo Alonso-Betanzos, Bertha Guijarro-Berdiñas, Brais Cancela, Carlos Eiras-Franco</author><pubDate>Mon, 30 Dec 2024 18:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10690v3</guid></item><item><title>Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models</title><link>http://arxiv.org/abs/2408.05093v3</link><description>Large language models (LLMs) have generated significant attention since theirinception, finding applications across various academic and industrial domains.However, these models often suffer from the "hallucination problem", whereoutputs, though grammatically and logically coherent, lack factual accuracy orare entirely fabricated. A particularly troubling issue discovered and widelydiscussed recently is the numerical comparison error where multiple LLMsincorrectly infer that "9.11$&gt;$9.9". We discovered that the order in which LLMsgenerate answers and reasoning impacts their consistency. Specifically, resultsvary significantly when an LLM generates an answer first and then provides thereasoning versus generating the reasoning process first and then theconclusion. Inspired by this, we propose a new benchmark method for assessingLLM consistency: comparing responses generated through these two differentapproaches. This benchmark effectively identifies instances where LLMsfabricate answers and subsequently generate justifications. Furthermore, weintroduce a novel and straightforward prompt strategy designed to mitigate thisissue. Experimental results demonstrate that this strategy improves performanceacross various LLMs compared to direct questioning. This work not only shedslight on a critical flaw in LLMs but also offers a practical solution toenhance their reliability.</description><author>Zikai Xie</author><pubDate>Mon, 30 Dec 2024 18:21:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05093v3</guid></item><item><title>Facilitating large language model Russian adaptation with Learned Embedding Propagation</title><link>http://arxiv.org/abs/2412.21140v1</link><description>Rapid advancements of large language model (LLM) technologies led to theintroduction of powerful open-source instruction-tuned LLMs that have the sametext generation quality as the state-of-the-art counterparts such as GPT-4.While the emergence of such models accelerates the adoption of LLM technologiesin sensitive-information environments the authors of such models don notdisclose the training data necessary for replication of the results thus makingthe achievements model-exclusive. Since those open-source models are alsomultilingual this in turn reduces the benefits of training a language specificLLMs as improved inference computation efficiency becomes the only guaranteedadvantage of such costly procedure. More cost-efficient options such asvocabulary extension and subsequent continued pre-training are also inhibitedby the lack of access to high-quality instruction-tuning data since it is themajor factor behind the resulting LLM task-solving capabilities. To address thelimitations and cut the costs of the language adaptation pipeline we proposeLearned Embedding Propagation (LEP). Unlike existing approaches our method haslower training data size requirements due to minimal impact on existing LLMknowledge which we reinforce using novel ad-hoc embedding propagation procedurethat allows to skip the instruction-tuning step and instead implant the newlanguage knowledge directly into any existing instruct-tuned variant. Weevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,showing that LEP is competitive with traditional instruction-tuning methods,achieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, withfurther improvements via self-calibration and continued tuning enhancingtask-solving capabilities.</description><author>Mikhail Tikhomirov, Daniil Chernyshev</author><pubDate>Mon, 30 Dec 2024 18:15:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21140v1</guid></item><item><title>Training Software Engineering Agents and Verifiers with SWE-Gym</title><link>http://arxiv.org/abs/2412.21139v1</link><description>We present SWE-Gym, the first environment for training real-world softwareengineering (SWE) agents. SWE-Gym contains 2,438 real-world Python taskinstances, each comprising a codebase with an executable runtime environment,unit tests, and a task specified in natural language. We use SWE-Gym to trainlanguage model based SWE agents , achieving up to 19% absolute gains in resolverate on the popular SWE-Bench Verified and Lite test sets. We also experimentwith inference-time scaling through verifiers trained on agent trajectoriessampled from SWE-Gym. When combined with our fine-tuned SWE agents, we achieve32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, reflecting a newstate-of-the-art for open-weight SWE agents. To facilitate further research, wepublicly release SWE-Gym, models, and agent trajectories.</description><author>Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, Yizhe Zhang</author><pubDate>Mon, 30 Dec 2024 18:15:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21139v1</guid></item><item><title>DeepF-fNet: a physics-informed neural network for vibration isolation optimization</title><link>http://arxiv.org/abs/2412.21132v1</link><description>Structural optimization is essential for designing safe, efficient, anddurable components with minimal material usage. Traditional methods forvibration control often rely on active systems to mitigate unpredictablevibrations, which may lead to resonance and potential structural failure.However, these methods face significant challenges when addressing thenonlinear inverse eigenvalue problems required for optimizing structuressubjected to a wide range of frequencies. As a result, no existing approach haseffectively addressed the need for real-time vibration suppression within thiscontext, particularly in high-performance environments such as automotivenoise, vibration and harshness, where computational efficiency is crucial. This study introduces DeepF-fNet, a novel neural network framework designedto replace traditional active systems in vibration-based structuraloptimization. Leveraging DeepONets within the context of physics-informedneural networks, DeepF-fNet integrates both data and the governing physicallaws. This enables rapid identification of optimal parameters to suppresscritical vibrations at specific frequencies, offering a more efficient andreal-time alternative to conventional methods. The proposed framework is validated through a case study involving a locallyresonant metamaterial used to isolate structures from user-defined frequencyranges. The results demonstrate that DeepF-fNet outperforms traditional geneticalgorithms in terms of computational speed while achieving comparable results,making it a promising tool for vibration-sensitive applications. By replacingactive systems with machine learning techniques, DeepF-fNet paves the way formore efficient and cost-effective structural optimization in real-worldscenarios.</description><author>A. Tollardo, F. Cadini, M. Giglio, L. Lomazzi</author><pubDate>Mon, 30 Dec 2024 18:08:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21132v1</guid></item><item><title>High-Rank Irreducible Cartesian Tensor Decomposition and Bases of Equivariant Spaces</title><link>http://arxiv.org/abs/2412.18263v2</link><description>Irreducible Cartesian tensors (ICTs) play a crucial role in the design ofequivariant graph neural networks, as well as in theoretical chemistry andchemical physics. Meanwhile, the design space of available linear operations ontensors that preserve symmetry presents a significant challenge. The ICTdecomposition and a basis of this equivariant space are difficult to obtain forhigh-order tensors. After decades of research, we recently achieve an explicitICT decomposition for $n=5$ \citep{bonvicini2024irreducible} with factorialtime/space complexity. This work, for the first time, obtains decompositionmatrices for ICTs up to rank $n=9$ with reduced and affordable complexity, byconstructing what we call path matrices. The path matrices are obtained viaperforming chain-like contraction with Clebsch-Gordan matrices following theparentage scheme. We prove and leverage that the concatenation of path matricesis an orthonormal change-of-basis matrix between the Cartesian tensor productspace and the spherical direct sum spaces. Furthermore, we identify a completeorthogonal basis for the equivariant space, rather than a spanning set\citep{pearce2023brauer}, through this path matrices technique. We furtherextend our result to the arbitrary tensor product and direct sum spaces,enabling free design between different spaces while keeping symmetry. ThePython code is available inhttps://github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases wherethe $n=6,\dots,9$ ICT decomposition matrices are obtained in 1s, 3s, 11s, and4m32s, respectively.</description><author>Shihao Shao, Yikang Li, Zhouchen Lin, Qinghua Cui</author><pubDate>Mon, 30 Dec 2024 18:07:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18263v2</guid></item><item><title>What Makes for a Good Stereoscopic Image?</title><link>http://arxiv.org/abs/2412.21127v1</link><description>With rapid advancements in virtual reality (VR) headsets, effectivelymeasuring stereoscopic quality of experience (SQoE) has become essential fordelivering immersive and comfortable 3D experiences. However, most existingstereo metrics focus on isolated aspects of the viewing experience such asvisual discomfort or image quality, and have traditionally faced datalimitations. To address these gaps, we present SCOPE (Stereoscopic COntentPreference Evaluation), a new dataset comprised of real and syntheticstereoscopic images featuring a wide range of common perceptual distortions andartifacts. The dataset is labeled with preference annotations collected on a VRheadset, with our findings indicating a notable degree of consistency in userpreferences across different headsets. Additionally, we present iSQoE, a newmodel for stereo quality of experience assessment trained on our dataset. Weshow that iSQoE aligns better with human preferences than existing methods whencomparing mono-to-stereo conversion methods.</description><author>Netanel Y. Tamir, Shir Amir, Ranel Itzhaky, Noam Atia, Shobhita Sundaram, Stephanie Fu, Ron Sokolovsky, Phillip Isola, Tali Dekel, Richard Zhang, Miriam Farber</author><pubDate>Mon, 30 Dec 2024 17:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21127v1</guid></item><item><title>Adaptive Batch Size Schedules for Distributed Training of Language Models with Data and Model Parallelism</title><link>http://arxiv.org/abs/2412.21124v1</link><description>An appropriate choice of batch sizes in large-scale model training iscrucial, yet it involves an intrinsic yet inevitable dilemma: large-batchtraining improves training efficiency in terms of memory utilization, whilegeneralization performance often deteriorates due to small amounts of gradientnoise. Despite this dilemma, the common practice of choosing batch sizes inlanguage model training often prioritizes training efficiency -- employingeither constant large sizes with data parallelism or implementing batch sizewarmup schedules. However, such batch size schedule designs remain heuristicand often fail to adapt to training dynamics, presenting the challenge ofdesigning adaptive batch size schedules. Given the abundance of availabledatasets and the data-hungry nature of language models, data parallelism hasbecome an indispensable distributed training paradigm, enabling the use oflarger batch sizes for gradient computation. However, vanilla data parallelismrequires replicas of model parameters, gradients, and optimizer states at eachworker, which prohibits training larger models with billions of parameters. Tooptimize memory usage, more advanced parallelism strategies must be employed.In this work, we propose general-purpose and theoretically principled adaptivebatch size schedules compatible with data parallelism and model parallelism. Wedevelop a practical implementation with PyTorch Fully Sharded Data Parallel,facilitating the pretraining of language models of different sizes. Weempirically demonstrate that our proposed approaches outperform constant batchsizes and heuristic batch size warmup schedules in the pretraining of models inthe Llama family, with particular focus on smaller models with up to 3 billionparameters. We also establish theoretical convergence guarantees for suchadaptive batch size schedules with Adam for general smooth nonconvexobjectives.</description><author>Tim Tsz-Kit Lau, Weijian Li, Chenwei Xu, Han Liu, Mladen Kolar</author><pubDate>Mon, 30 Dec 2024 17:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21124v1</guid></item><item><title>Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation</title><link>http://arxiv.org/abs/2412.21117v1</link><description>In this work, we introduce Prometheus, a 3D-aware latent diffusion model fortext-to-3D generation at both object and scene levels in seconds. We formulate3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussiangeneration within the latent diffusion paradigm. To ensure generalizability, webuild our model upon pre-trained text-to-image generation model with onlyminimal adjustments, and further train it using a large number of images fromboth single-view and multi-view datasets. Furthermore, we introduce an RGB-Dlatent space into 3D Gaussian generation to disentangle appearance and geometryinformation, enabling efficient feed-forward generation of 3D Gaussians withbetter fidelity and geometry. Extensive experimental results demonstrate theeffectiveness of our method in both feed-forward 3D Gaussian reconstruction andtext-to-3D generation. Project page:https://freemty.github.io/project-prometheus/</description><author>Yuanbo Yang, Jiahao Shao, Xinyang Li, Yujun Shen, Andreas Geiger, Yiyi Liao</author><pubDate>Mon, 30 Dec 2024 17:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21117v1</guid></item><item><title>Non-asymptotic spectral bounds on the $\varepsilon$-entropy of kernel classes</title><link>http://arxiv.org/abs/2204.04512v2</link><description>Let $K: \boldsymbol{\Omega}\times \boldsymbol{\Omega}$ be a continuous Mercerkernel defined on a compact subset of ${\mathbb R}^n$ and $\mathcal{H}_K$ bethe reproducing kernel Hilbert space (RKHS) associated with $K$. Given a finitemeasure $\nu$ on $\boldsymbol{\Omega}$, we investigate upper and lower boundson the $\varepsilon$-entropy of the unit ball of $\mathcal{H}_K$ in the space$L_p(\nu)$. This topic is an important direction in the modern statisticaltheory of kernel-based methods. We prove sharp upper and lower bounds for $p\in [1,+\infty]$. For $p\in[1,2]$, the upper bounds are determined solely by the eigenvalue behaviour ofthe corresponding integral operator $\phi\to \int_{\boldsymbol{\Omega}}K(\cdot,{\mathbf y})\phi({\mathbf y})d\nu({\mathbf y})$. In constrast, for$p&gt;2$, the bounds additionally depend on the convergence rate of the truncatedMercer series to the kernel $K$ in the $L_p(\nu)$-norm. We discuss a number of consequences of our bounds and show that they aresubstantially tighter than previous bounds for general kernels. Furthermore,for specific cases, such as zonal kernels and the Gaussian kernel on a box, ourbounds are asymptotically tight as $\varepsilon\to +0$.</description><author>Rustem Takhanov</author><pubDate>Mon, 30 Dec 2024 17:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.04512v2</guid></item><item><title>On Parallel External-Memory Bidirectional Search</title><link>http://arxiv.org/abs/2412.21104v1</link><description>Parallelization and External Memory (PEM) techniques have significantlyenhanced the capabilities of search algorithms when solving large-scaleproblems. Previous research on PEM has primarily centered on unidirectionalalgorithms, with only one publication on bidirectional PEM that focuses on themeet-in-the-middle (MM) algorithm. Building upon this foundation, this paperpresents a framework that integrates both uni- and bi-directional best-firstsearch algorithms into this framework. We then develop a PEM variant of thestate-of-the-art bidirectional heuristic search (\BiHS) algorithm BAE*(PEM-BAE*). As previous work on \BiHS did not focus on scaling problem sizes,this work enables us to evaluate bidirectional algorithms on hard problems.Empirical evaluation shows that PEM-BAE* outperforms the PEM variants of A* andthe MM algorithm, as well as a parallel variant of IDA*. These findings mark asignificant milestone, revealing that bidirectional search algorithms clearlyoutperform unidirectional search algorithms across several domains, even whenequipped with state-of-the-art heuristics.</description><author>ior Siag, Shahaf S. Shperberg, Ariel Felner, Nathan R. Sturtevant</author><pubDate>Mon, 30 Dec 2024 17:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21104v1</guid></item><item><title>Applying Predictive Analytics to Occupational Health and Safety</title><link>http://arxiv.org/abs/2412.16038v2</link><description>Predictive analytics is revolutionizing occupational health and safety (OHS).It offers evidence-based insights. These insights enable proactive riskmanagement and informed, data-driven decision-making in organizationalsettings. This article explores the key components of predictive analytics inOHS, beginning with data collection, management, and preparation, and movingthrough to advanced predictive modelling techniques. We emphasize theimportance of data integrity through processes such as missing valueimputation, anomaly detection, and feature engineering to ensure accurate modelpredictions. Risk prioritization identifies and ranks hazards across variousfactors, including employee behaviours, organizational policies, environmentalconditions, and operational practices. We posit that insights derived frompredictive models must be effectively interpreted and implemented. Theseinsights guide organizations to focus on high-impact areas for accidentprevention and resource optimization. The integration of predictive analyticsin OHS brings notable benefits, including enhanced decision-making, greateroperational efficiency, cost savings, and improved compliance with safetystandards. We examine applications of predictive analytics in OHS in Indiansettings. We opine that, using predictive analytics, India can develop highsafety standards while traversing the complexities of its workforce settings.</description><author>Vyom Saxena</author><pubDate>Mon, 30 Dec 2024 17:28:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.16038v2</guid></item><item><title>Exploring and Controlling Diversity in LLM-Agent Conversation</title><link>http://arxiv.org/abs/2412.21102v1</link><description>Diversity is a critical aspect of multi-agent communication. In this paper,we focus on controlling and exploring diversity in the context of open-domainmulti-agent conversations, particularly for world simulation applications. Wepropose Adaptive Prompt Pruning (APP), a novel method that dynamically adjuststhe content of the utterance generation prompt to control diversity using asingle parameter, lambda. Through extensive experiments, we show that APPeffectively controls the output diversity across models and datasets, withpruning more information leading to more diverse output. We comprehensivelyanalyze the relationship between prompt content and conversational diversity.Our findings reveal that information from all components of the promptgenerally constrains the diversity of the output, with the Memory blockexerting the most significant influence. APP is compatible with establishedtechniques like temperature sampling and top-p sampling, providing a versatiletool for diversity management. To address the trade-offs of increaseddiversity, such as inconsistencies with omitted information, we incorporate apost-generation correction step, which effectively balances diversityenhancement with output consistency. Additionally, we examine how promptstructure, including component order and length, impacts diversity. This studyaddresses key questions surrounding diversity in multi-agent world simulation,offering insights into its control, influencing factors, and associatedtrade-offs. Our contributions lay the foundation for systematically engineeringdiversity in LLM-based multi-agent collaborations, advancing theireffectiveness in real-world applications.</description><author>KuanChao Chu, Yi-Pei Chen, Hideki Nakayama</author><pubDate>Mon, 30 Dec 2024 17:25:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21102v1</guid></item><item><title>Segment Discovery: Enhancing E-commerce Targeting</title><link>http://arxiv.org/abs/2409.13847v2</link><description>Modern e-commerce services frequently target customers with incentives orinterventions to engage them in their products such as games, shopping, videostreaming, etc. This customer engagement increases acquisition of morecustomers and retention of existing ones, leading to more business for thecompany while improving customer experience. Often, customers are eitherrandomly targeted or targeted based on the propensity of desirable behavior.However, such policies can be suboptimal as they do not target the set ofcustomers who would benefit the most from the intervention and they may alsonot take account of any constraints. In this paper, we propose a policyframework based on uplift modeling and constrained optimization that identifiescustomers to target for a use-case specific intervention so as to maximize thevalue to the business, while taking account of any given constraints. Wedemonstrate improvement over state-of-the-art targeting approaches using twolarge-scale experimental studies and a production implementation.</description><author>Qiqi Li, Roopali Singh, Charin Polpanumas, Tanner Fiez, Namita Kumar, Shreya Chakrabarti</author><pubDate>Mon, 30 Dec 2024 17:13:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13847v2</guid></item><item><title>On the Generalizability of Machine Learning-based Ransomware Detection in Block Storage</title><link>http://arxiv.org/abs/2412.21084v1</link><description>Ransomware represents a pervasive threat, traditionally countered at theoperating system, file-system, or network levels. However, these approachesoften introduce significant overhead and remain susceptible to circumvention byattackers. Recent research activity started looking into the detection ofransomware by observing block IO operations. However, this approach exhibitssignificant detection challenges. Recognizing these limitations, our researchpivots towards enabling robust ransomware detection in storage systems keepingin mind their limited computational resources available. To perform ourstudies, we propose a kernel-based framework capable of efficiently extractingand analyzing IO operations to identify ransomware activity. The framework canbe adopted to storage systems using computational storage devices to improvesecurity and fully hide detection overheads. Our method employs a refined setof computationally light features optimized for ML models to accurately discernmalicious from benign activities. Using this lightweight approach, we study a wide range of generalizabilityaspects and analyze the performance of these models across a large space ofsetups and configurations covering a wide range of realistic real-worldscenarios. We reveal various trade-offs and provide strong arguments for thegeneralizability of storage-based detection of ransomware and show that ourapproach outperforms currently available ML-based ransomware detection instorage. Empirical validation reveals that our decision tree-based modelsachieve remarkable effectiveness, evidenced by higher median F1 scores of up to12.8%, lower false negative rates of up to 10.9% and particularly decreasedfalse positive rates of up to 17.1% compared to existing storage-baseddetection approaches.</description><author>Nicolas Reategui, Roman Pletka, Dionysios Diamantopoulos</author><pubDate>Mon, 30 Dec 2024 17:02:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21084v1</guid></item><item><title>Quantum Diffusion Model for Quark and Gluon Jet Generation</title><link>http://arxiv.org/abs/2412.21082v1</link><description>Diffusion models have demonstrated remarkable success in image generation,but they are computationally intensive and time-consuming to train. In thispaper, we introduce a novel diffusion model that benefits from quantumcomputing techniques in order to mitigate computational challenges and enhancegenerative performance within high energy physics data. The fully quantumdiffusion model replaces Gaussian noise with random unitary matrices in theforward process and incorporates a variational quantum circuit within the U-Netin the denoising architecture. We run evaluations on the structurally complexquark and gluon jets dataset from the Large Hadron Collider. The resultsdemonstrate that the fully quantum and hybrid models are competitive with asimilar classical model for jet generation, highlighting the potential of usingquantum techniques for machine learning problems.</description><author>Mariia Baidachna, Rey Guadarrama, Gopal Ramesh Dahale, Tom Magorsch, Isabel Pedraza, Konstantin T. Matchev, Katia Matcheva, Kyoungchul Kong, Sergei Gleyzer</author><pubDate>Mon, 30 Dec 2024 17:00:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21082v1</guid></item><item><title>Vinci: A Real-time Embodied Smart Assistant based on Egocentric Vision-Language Model</title><link>http://arxiv.org/abs/2412.21080v1</link><description>We introduce Vinci, a real-time embodied smart assistant built upon anegocentric vision-language model. Designed for deployment on portable devicessuch as smartphones and wearable cameras, Vinci operates in an "always on"mode, continuously observing the environment to deliver seamless interactionand assistance. Users can wake up the system and engage in naturalconversations to ask questions or seek assistance, with responses deliveredthrough audio for hands-free convenience. With its ability to process longvideo streams in real-time, Vinci can answer user queries about currentobservations and historical context while also providing task planning based onpast interactions. To further enhance usability, Vinci integrates a videogeneration module that creates step-by-step visual demonstrations for tasksthat require detailed guidance. We hope that Vinci can establish a robustframework for portable, real-time egocentric AI systems, empowering users withcontextual and actionable insights. We release the complete implementation forthe development of the device in conjunction with a demo web platform to testuploaded videos at https://github.com/OpenGVLab/vinci.</description><author>Yifei Huang, Jilan Xu, Baoqi Pei, Yuping He, Guo Chen, Lijin Yang, Xinyuan Chen, Yaohui Wang, Zheng Nie, Jinyao Liu, Guoshun Fan, Dechen Lin, Fang Fang, Kunpeng Li, Chang Yuan, Yali Wang, Yu Qiao, Limin Wang</author><pubDate>Mon, 30 Dec 2024 16:57:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21080v1</guid></item><item><title>Edicho: Consistent Image Editing in the Wild</title><link>http://arxiv.org/abs/2412.21079v1</link><description>As a verified need, consistent editing across in-the-wild images remains atechnical challenge arising from various unmanageable factors, like objectposes, lighting conditions, and photography environments. Edicho steps in witha training-free solution based on diffusion models, featuring a fundamentaldesign principle of using explicit image correspondence to direct editing.Specifically, the key components include an attention manipulation module and acarefully refined classifier-free guidance (CFG) denoising strategy, both ofwhich take into account the pre-estimated correspondence. Such aninference-time algorithm enjoys a plug-and-play nature and is compatible tomost diffusion-based editing methods, such as ControlNet and BrushNet.Extensive results demonstrate the efficacy of Edicho in consistent cross-imageediting under diverse settings. We will release the code to facilitate futurestudies.</description><author>Qingyan Bai, Hao Ouyang, Yinghao Xu, Qiuyu Wang, Ceyuan Yang, Ka Leong Cheng, Yujun Shen, Qifeng Chen</author><pubDate>Mon, 30 Dec 2024 16:56:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21079v1</guid></item><item><title>ReXTrust: A Model for Fine-Grained Hallucination Detection in AI-Generated Radiology Reports</title><link>http://arxiv.org/abs/2412.15264v2</link><description>The increasing adoption of AI-generated radiology reports necessitates robustmethods for detecting hallucinations--false or unfounded statements that couldimpact patient care. We present ReXTrust, a novel framework for fine-grainedhallucination detection in AI-generated radiology reports. Our approachleverages sequences of hidden states from large vision-language models toproduce finding-level hallucination risk scores. We evaluate ReXTrust on asubset of the MIMIC-CXR dataset and demonstrate superior performance comparedto existing approaches, achieving an AUROC of 0.8751 across all findings and0.8963 on clinically significant findings. Our results show that white-boxapproaches leveraging model hidden states can provide reliable hallucinationdetection for medical AI systems, potentially improving the safety andreliability of automated radiology reporting.</description><author>Romain Hardy, Sung Eun Kim, Pranav Rajpurkar</author><pubDate>Mon, 30 Dec 2024 16:56:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15264v2</guid></item><item><title>Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data</title><link>http://arxiv.org/abs/2310.03146v5</link><description>Traditional deep learning (DL) models have two ubiquitous limitations. First,they assume training samples are independent and identically distributed(i.i.d), an assumption often violated in real-world datasets where samples haveadditional correlation due to repeat measurements (e.g., on the sameparticipants in a longitudinal study or cells from the same sequencer). Thisleads to performance degradation, limited generalization, and covariateconfounding, which induces Type I and Type II errors. Second, DL modelstypically prioritize overall accuracy, favoring accuracy on the majority whilesacrificing performance for underrepresented subpopulations, leading to unfair,biased models. This is critical to remediate, particularly in models whichinfluence decisions regarding loan approvals and healthcare. To address theseissues, we propose the Fair Mixed Effects Deep Learning (Fair MEDL) framework.This framework quantifies cluster-invariant fixed effects (FE) andcluster-specific random effects (RE) through: 1) a cluster adversary forlearning invariant FE, 2) a Bayesian neural network for RE, and 3) a mixingfunction combining FE and RE for final predictions. Fairness is enhancedthrough architectural and loss function changes introduced by an adversarialdebiasing network. We formally define and demonstrate improved fairness acrossthree metrics: equalized odds, demographic parity, and counterfactual fairness,for both classification and regression tasks. Our method also identifies andde-weights confounded covariates, mitigating Type I and II errors. Theframework is comprehensively evaluated across three datasets spanning twoindustries, including finance and healthcare. The Fair MEDL framework improvesfairness by 86.4% for Age, 64.9% for Race, 57.8% for Sex, and 36.2% for Maritalstatus, while maintaining robust predictive performance.</description><author>Son Nguyen, Adam Wang, Albert Montillo</author><pubDate>Mon, 30 Dec 2024 16:54:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03146v5</guid></item><item><title>LLM Distillation for Efficient Few-Shot Multiple Choice Question Answering</title><link>http://arxiv.org/abs/2412.09807v2</link><description>Multiple Choice Question Answering (MCQA) is an important problem withnumerous real-world applications, such as medicine, law, and education. Thehigh cost of building MCQA datasets makes few-shot learning pivotal in thisdomain. While Large Language Models (LLMs) can enable few-shot learning, theirdirect application in real-world scenarios is often hindered by their highcomputational cost. To address this challenge, we propose a simple yeteffective approach that uses LLMs for data generation and scoring. Our approachutilizes LLMs to create MCQA data which contains questions and choices, and toassign probability scores to the generated choices. We then use the generateddata and LLM-assigned scores to finetune a smaller and more efficientencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensiveexperiments on the Massive Multitask Language Understanding (MMLU) benchmarkdemonstrate that our method improves accuracy from 28.9% to 39.3%, representinga gain of over 10% compared to a baseline finetuned directly on 5-shotexamples. This shows the effectiveness of LLM-driven data generation andknowledge distillation for few-shot MCQA.</description><author>Patrick Sutanto, Joan Santoso, Esther Irawati Setiawan, Aji Prasetya Wibawa</author><pubDate>Mon, 30 Dec 2024 16:45:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.09807v2</guid></item><item><title>Enhanced coarsening of charge density waves induced by electron correlation: Machine-learning enabled large-scale dynamical simulations</title><link>http://arxiv.org/abs/2412.21072v1</link><description>The phase ordering kinetics of emergent orders in correlated electron systemsis a fundamental topic in non-equilibrium physics, yet it remains largelyunexplored. The intricate interplay between quasiparticles and emergentorder-parameter fields could lead to unusual coarsening dynamics that is beyondthe standard theories. However, accurate treatment of both quasiparticles andcollective degrees of freedom is a multi-scale challenge in dynamicalsimulations of correlated electrons. Here we leverage modern machine learning(ML) methods to achieve a linear-scaling algorithm for simulating thecoarsening of charge density waves (CDWs), one of the fundamental symmetrybreaking phases in functional electron materials. We demonstrate our approachon the square-lattice Hubbard-Holstein model and uncover an intriguingenhancement of CDW coarsening which is related to the screening of on-sitepotential by electron-electron interactions. Our study provides fresh insightsinto the role of electron correlations in non-equilibrium dynamics andunderscores the promise of ML force-field approaches for advancing multi-scaledynamical modeling of correlated electron systems.</description><author>Yang Yang, Chen Cheng, Yunhao Fan, Gia-Wei Chern</author><pubDate>Mon, 30 Dec 2024 16:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21072v1</guid></item><item><title>Investigating layer-selective transfer learning of QAOA parameters for Max-Cut problem</title><link>http://arxiv.org/abs/2412.21071v1</link><description>Quantum approximate optimization algorithm (QAOA) is a variational quantumalgorithm (VQA) ideal for noisy intermediate-scale quantum (NISQ) processors,and is highly successful for solving combinatorial optimization problems(COPs). It has been observed that the optimal variational parameters obtainedfrom one instance of a COP can be transferred to another instance, producingsufficiently satisfactory solutions for the latter. In this context, a suitablemethod for further improving the solution is to fine-tune a subset of thetransferred parameters. We numerically explore the role of optimizingindividual QAOA layers in improving the approximate solution of the Max-Cutproblem after parameter transfer. We also investigate the trade-off between agood approximation and the required optimization time when optimizingtransferred QAOA parameters. These studies show that optimizing a subset oflayers can be more effective at a lower time-cost compared to optimizing alllayers.</description><author>Francesco Aldo Venturelli, Sreetama Das, Filippo Caruso</author><pubDate>Mon, 30 Dec 2024 16:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21071v1</guid></item><item><title>PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion</title><link>http://arxiv.org/abs/2412.17780v2</link><description>Peptide therapeutics, a major class of medicines, have achieved remarkablesuccess across diseases such as diabetes and cancer, with landmark examplessuch as GLP-1 receptor agonists revolutionizing the treatment of type-2diabetes and obesity. Despite their success, designing peptides that satisfymultiple conflicting objectives, such as target binding affinity, solubility,and membrane permeability, remains a major challenge. Classical drugdevelopment and structure-based design are ineffective for such tasks, as theyfail to optimize global functional properties critical for therapeuticefficacy. Existing generative frameworks are largely limited to continuousspaces, unconditioned outputs, or single-objective guidance, making themunsuitable for discrete sequence optimization across multiple properties. Toaddress this, we present PepTune, a multi-objective discrete diffusion modelfor the simultaneous generation and optimization of therapeutic peptide SMILES.Built on the Masked Discrete Language Model (MDLM) framework, PepTune ensuresvalid peptide structures with state-dependent masking schedules andpenalty-based objectives. To guide the diffusion process, we propose a MonteCarlo Tree Search (MCTS)-based strategy that balances exploration andexploitation to iteratively refine Pareto-optimal sequences. MCTS integratesclassifier-based rewards with search-tree expansion, overcoming gradientestimation challenges and data sparsity inherent to discrete spaces. UsingPepTune, we generate diverse, chemically-modified peptides optimized formultiple therapeutic properties, including target binding affinity, membranepermeability, solubility, hemolysis, and non-fouling characteristics on variousdisease-relevant targets. In total, our results demonstrate that MCTS-guideddiscrete diffusion is a powerful and modular approach for multi-objectivesequence design in discrete state spaces.</description><author>Sophia Tang, Yinuo Zhang, Pranam Chatterjee</author><pubDate>Mon, 30 Dec 2024 16:41:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17780v2</guid></item><item><title>Privacy-Aware Multi-Device Cooperative Edge Inference with Distributed Resource Bidding</title><link>http://arxiv.org/abs/2412.21069v1</link><description>Mobile edge computing (MEC) has empowered mobile devices (MDs) in supportingartificial intelligence (AI) applications through collaborative efforts withproximal MEC servers. Unfortunately, despite the great promise of device-edgecooperative AI inference, data privacy becomes an increasing concern. In thispaper, we develop a privacy-aware multi-device cooperative edge inferencesystem for classification tasks, which integrates a distributed biddingmechanism for the MEC server's computational resources. Intermediate featurecompression is adopted as a principled approach to minimize data privacyleakage. To determine the bidding values and feature compression ratios in adistributed fashion, we formulate a decentralized partially observable Markovdecision process (DEC-POMDP) model, for which, a multi-agent deep deterministicpolicy gradient (MADDPG)-based algorithm is developed. Simulation resultsdemonstrate the effectiveness of the proposed algorithm in privacy-preservingcooperative edge inference. Specifically, given a sufficient level of dataprivacy protection, the proposed algorithm achieves 0.31-0.95% improvements inclassification accuracy compared to the approach being agnostic to the wirelesschannel conditions. The performance is further enhanced by 1.54-1.67% byconsidering the difficulties of inference data.</description><author>Wenhao Zhuang, Yuyi Mao</author><pubDate>Mon, 30 Dec 2024 16:37:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21069v1</guid></item><item><title>Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight Task-Specific Adapters for Automatic Scoring</title><link>http://arxiv.org/abs/2412.21065v1</link><description>The integration of Artificial Intelligence (AI) in education requiresscalable and efficient frameworks that balance performance, adaptability, andcost. This paper addresses these needs by proposing a shared backbone modelarchitecture enhanced with lightweight LoRA adapters for task-specificfine-tuning, targeting the automated scoring of student responses across 27mutually exclusive tasks. By achieving competitive performance (average QWK of0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memoryconsumption by 60% and inference latency by 40%, the framework demonstratessignificant efficiency gains. This approach aligns with the workshops' focus onimproving language models for educational tasks, creating responsibleinnovations for cost-sensitive deployment, and supporting educators bystreamlining assessment workflows. The findings underscore the potential ofscalable AI to enhance learning outcomes while maintaining fairness andtransparency in automated scoring systems.</description><author>Ehsan Latif, Xiaoming Zhai</author><pubDate>Mon, 30 Dec 2024 16:34:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21065v1</guid></item><item><title>Varformer: Adapting VAR's Generative Prior for Image Restoration</title><link>http://arxiv.org/abs/2412.21063v1</link><description>Generative models trained on extensive high-quality datasets effectivelycapture the structural and statistical properties of clean images, renderingthem powerful priors for transforming degraded features into clean ones inimage restoration. VAR, a novel image generative paradigm, surpasses diffusionmodels in generation quality by applying a next-scale prediction approach. Itprogressively captures both global structures and fine-grained details throughthe autoregressive process, consistent with the multi-scale restorationprinciple widely acknowledged in the restoration community. Furthermore, weobserve that during the image reconstruction process utilizing VAR, scalepredictions automatically modulate the input, facilitating the alignment ofrepresentations at subsequent scales with the distribution of clean images. Toharness VAR's adaptive distribution alignment capability in image restorationtasks, we formulate the multi-scale latent representations within VAR as therestoration prior, thus advancing our delicately designed VarFormer framework.The strategic application of these priors enables our VarFormer to achieveremarkable generalization on unseen tasks while also reducing trainingcomputational costs. Extensive experiments underscores that our VarFormeroutperforms existing multi-task image restoration methods across variousrestoration tasks.</description><author>Siyang Wang, Feng Zhao</author><pubDate>Mon, 30 Dec 2024 16:32:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21063v1</guid></item><item><title>BridgePure: Revealing the Fragility of Black-box Data Protection</title><link>http://arxiv.org/abs/2412.21061v1</link><description>Availability attacks, or unlearnable examples, are defensive techniques thatallow data owners to modify their datasets in ways that prevent unauthorizedmachine learning models from learning effectively while maintaining the data'sintended functionality. It has led to the release of popular black-box toolsfor users to upload personal data and receive protected counterparts. In thiswork, we show such black-box protections can be substantially bypassed if asmall set of unprotected in-distribution data is available. Specifically, anadversary can (1) easily acquire (unprotected, protected) pairs by querying theblack-box protections with the unprotected dataset; and (2) train a diffusionbridge model to build a mapping. This mapping, termed BridgePure, caneffectively remove the protection from any previously unseen data within thesame distribution. Under this threat model, our method demonstrates superiorpurification performance on classification and style mimicry tasks, exposingcritical vulnerabilities in black-box data protection.</description><author>Yihan Wang, Yiwei Lu, Xiao-Shan Gao, Gautam Kamath, Yaoliang Yu</author><pubDate>Mon, 30 Dec 2024 16:30:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21061v1</guid></item><item><title>DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought</title><link>http://arxiv.org/abs/2412.17498v2</link><description>Recently, O1-like models have emerged as representative examples,illustrating the effectiveness of long chain-of-thought (CoT) in reasoningtasks such as math and coding tasks. In this paper, we introduce DRT-o1, anattempt to bring the success of long CoT to neural machine translation (MT).Specifically, in view of the literature books that might involve similes andmetaphors, translating these texts to a target language is very difficult inpractice due to cultural differences. In such cases, literal translation oftenfails to convey the intended meaning effectively. Even for professional humantranslators, considerable thought must be given to preserving semanticsthroughout the translation process. To simulate LLMs' long thought ability inMT, we first mine sentences containing similes or metaphors from existingliterature books, and then develop a multi-agent framework to translate thesesentences via long thought. In the multi-agent framework, a translator is usedto iteratively translate the source sentence under the suggestions provided byan advisor. To ensure the effectiveness of the long thoughts, an evaluator isalso employed to quantify the translation in each round. In this way, wecollect tens of thousands of long-thought MT data, which is used to train ourDRT-o1. Using Qwen2.5 and LLama-3.1 as the backbones, DRT-o1 models can learnthe thought process during machine translation, and outperform vanilla LLMs aswell as existing O1-like LLMs, showing their effectiveness The project isavailable at https://github.com/krystalan/DRT-o1</description><author>Jiaan Wang, Fandong Meng, Yunlong Liang, Jie Zhou</author><pubDate>Mon, 30 Dec 2024 16:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17498v2</guid></item><item><title>VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation</title><link>http://arxiv.org/abs/2412.21059v1</link><description>We present a general strategy to aligning visual generation models -- bothimage and video generation -- with human preference. To start with, we buildVisionReward -- a fine-grained and multi-dimensional reward model. We decomposehuman preferences in images and videos into multiple dimensions, eachrepresented by a series of judgment questions, linearly weighted and summed toan interpretable and accurate score. To address the challenges of video qualityassessment, we systematically analyze various dynamic features of videos, whichhelps VisionReward surpass VideoScore by 17.2% and achieve top performance forvideo preference prediction. Based on VisionReward, we develop amulti-objective preference learning algorithm that effectively addresses theissue of confounding factors within preference data. Our approach significantlyoutperforms existing image and video scoring methods on both machine metricsand human evaluation. All code and datasets are provided athttps://github.com/THUDM/VisionReward.</description><author>Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, Yuxiao Dong</author><pubDate>Mon, 30 Dec 2024 16:24:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21059v1</guid></item><item><title>Why the Metric Backbone Preserves Community Structure</title><link>http://arxiv.org/abs/2406.03852v2</link><description>The metric backbone of a weighted graph is the union of all-pairs shortestpaths. It is obtained by removing all edges $(u,v)$ that are not the shortestpath between $u$ and $v$. In networks with well-separated communities, themetric backbone tends to preserve many inter-community edges, because theseedges serve as bridges connecting two communities, but tends to delete manyintra-community edges because the communities are dense. This suggests that themetric backbone would dilute or destroy the community structure of the network.However, this is not borne out by prior empirical work, which instead showedthat the metric backbone of real networks preserves the community structure ofthe original network well. In this work, we analyze the metric backbone of abroad class of weighted random graphs with communities, and we formally provethe robustness of the community structure with respect to the deletion of allthe edges that are not in the metric backbone. An empirical comparison ofseveral graph sparsification techniques confirms our theoretical finding andshows that the metric backbone is an efficient sparsifier in the presence ofcommunities.</description><author>Maximilien Dreveton, Charbel Chucri, Matthias Grossglauser, Patrick Thiran</author><pubDate>Mon, 30 Dec 2024 16:14:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03852v2</guid></item><item><title>Physically Guided Deep Unsupervised Inversion for 1D Magnetotelluric Models</title><link>http://arxiv.org/abs/2410.15274v2</link><description>The global demand for unconventional energy sources such as geothermal energyand white hydrogen requires new exploration techniques for precise subsurfacestructure characterization and potential reservoir identification. TheMagnetotelluric (MT) method is crucial for these tasks, providing criticalinformation on the distribution of subsurface electrical resistivity at depthsranging from hundreds to thousands of meters. However, traditional iterativealgorithm-based inversion methods require the adjustment of multipleparameters, demanding time-consuming and exhaustive tuning processes to achieveproper cost function minimization. Although recent advances have incorporateddeep learning algorithms for MT inversion, primarily based on supervisedlearning, \paul{and} needs large labeled datasets for training. This workutilizes TensorFlow operations to create a differentiable forward MT operator,leveraging its automatic differentiation capability. Moreover, instead ofsolving for the subsurface model directly, as classical algorithms perform,this paper presents a new deep unsupervised inversion algorithm guided byphysics to estimate 1D MT models. Instead of using datasets with the observeddata and their respective model as labels during training, our method employs adifferentiable modeling operator that physically guides the cost functionminimization, making the proposed method solely dependent on observed data.Therefore, the optimization \paul{algorithm} updates the network weights tominimize the data misfit. We test the proposed method with field and syntheticdata at different acquisition frequencies, demonstrating that the resistivitymodels obtained are more accurate than those calculated using other techniques.</description><author>Paul Goyes-Peñafiel, Umair bin Waheed, Henry Arguello</author><pubDate>Mon, 30 Dec 2024 16:12:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15274v2</guid></item><item><title>Towards Effective Discrimination Testing for Generative AI</title><link>http://arxiv.org/abs/2412.21052v1</link><description>Generative AI (GenAI) models present new challenges in regulating againstdiscriminatory behavior. In this paper, we argue that GenAI fairness researchstill has not met these challenges; instead, a significant gap remains betweenexisting bias assessment methods and regulatory goals. This leads toineffective regulation that can allow deployment of reportedly fair, yetactually discriminatory, GenAI systems. Towards remedying this problem, weconnect the legal and technical literature around GenAI bias evaluation andidentify areas of misalignment. Through four case studies, we demonstrate howthis misalignment between fairness testing techniques and regulatory goals canresult in discriminatory outcomes in real-world deployments, especially inadaptive or complex environments. We offer practical recommendations forimproving discrimination testing to better align with regulatory goals andenhance the reliability of fairness assessments in future deployments.</description><author>Thomas P. Zollo, Nikita Rajaneesh, Richard Zemel, Talia B. Gillis, Emily Black</author><pubDate>Mon, 30 Dec 2024 16:09:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21052v1</guid></item><item><title>Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense</title><link>http://arxiv.org/abs/2412.21051v1</link><description>The rapid evolution of cloud computing technologies and the increasing numberof cloud applications have provided a large number of benefits in daily lives.However, the diversity and complexity of different components pose asignificant challenge to cloud security, especially when dealing withsophisticated and advanced cyberattacks. Recent advancements in generativefoundation models (GFMs), particularly in the large language models (LLMs),offer promising solutions for security intelligence. By exploiting the powerfulabilities in language understanding, data analysis, task inference, actionplanning, and code generation, we present LLM-PD, a novel proactive defensearchitecture that defeats various threats in a proactive manner. LLM-PD canefficiently make a decision through comprehensive data analysis and sequentialreasoning, as well as dynamically creating and deploying actionable defensemechanisms on the target cloud. Furthermore, it can flexibly self-evolve basedon experience learned from previous interactions and adapt to new attackscenarios without additional training. The experimental results demonstrate itsremarkable ability in terms of defense effectiveness and efficiency,particularly highlighting an outstanding success rate when compared with otherexisting methods.</description><author>Yuyang Zhou, Guang Cheng, Kang Du, Zihan Chen</author><pubDate>Mon, 30 Dec 2024 16:09:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21051v1</guid></item><item><title>Improving Generalization for AI-Synthesized Voice Detection</title><link>http://arxiv.org/abs/2412.19279v2</link><description>AI-synthesized voice technology has the potential to create realistic humanvoices for beneficial applications, but it can also be misused for maliciouspurposes. While existing AI-synthesized voice detection models excel inintra-domain evaluation, they face challenges in generalizing across differentdomains, potentially becoming obsolete as new voice generators emerge. Currentsolutions use diverse data and advanced machine learning techniques (e.g.,domain-invariant representation, self-supervised learning), but are limited bypredefined vocoders and sensitivity to factors like background noise andspeaker identity. In this work, we introduce an innovative disentanglementframework aimed at extracting domain-agnostic artifact features related tovocoders. Utilizing these features, we enhance model learning in a flat losslandscape, enabling escape from suboptimal solutions and improvinggeneralization. Extensive experiments on benchmarks show our approachoutperforms state-of-the-art methods, achieving up to 5.12% improvement in theequal error rate metric in intra-domain and 7.59% in cross-domain evaluations.</description><author>Hainan Ren, Li Lin, Chun-Hao Liu, Xin Wang, Shu Hu</author><pubDate>Mon, 30 Dec 2024 16:08:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19279v2</guid></item><item><title>Learning Epidemiological Dynamics via the Finite Expression Method</title><link>http://arxiv.org/abs/2412.21049v1</link><description>Modeling and forecasting the spread of infectious diseases is essential foreffective public health decision-making. Traditional epidemiological modelsrely on expert-defined frameworks to describe complex dynamics, while neuralnetworks, despite their predictive power, often lack interpretability due totheir ``black-box" nature. This paper introduces the Finite Expression Method,a symbolic learning framework that leverages reinforcement learning to deriveexplicit mathematical expressions for epidemiological dynamics. Throughnumerical experiments on both synthetic and real-world datasets, FEXdemonstrates high accuracy in modeling and predicting disease spread, whileuncovering explicit relationships among epidemiological variables. Theseresults highlight FEX as a powerful tool for infectious disease modeling,combining interpretability with strong predictive performance to supportpractical applications in public health.</description><author>Jianda Du, Senwei Liang, Chunmei Wang</author><pubDate>Mon, 30 Dec 2024 16:08:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21049v1</guid></item><item><title>Mind the truncation gap: challenges of learning on dynamic graphs with recurrent architectures</title><link>http://arxiv.org/abs/2412.21046v1</link><description>Systems characterized by evolving interactions, prevalent in social,financial, and biological domains, are effectively modeled as continuous-timedynamic graphs (CTDGs). To manage the scale and complexity of these graphdatasets, machine learning (ML) approaches have become essential. However,CTDGs pose challenges for ML because traditional static graph methods do notnaturally account for event timings. Newer approaches, such as graph recurrentneural networks (GRNNs), are inherently time-aware and offer advantages overstatic methods for CTDGs. However, GRNNs face another issue: the shorttruncation of backpropagation-through-time (BPTT), whose impact has not beenproperly examined until now. In this work, we demonstrate that this truncationcan limit the learning of dependencies beyond a single hop, resulting inreduced performance. Through experiments on a novel synthetic task andreal-world datasets, we reveal a performance gap between fullbackpropagation-through-time (F-BPTT) and the truncatedbackpropagation-through-time (T-BPTT) commonly used to train GRNN models. Weterm this gap the "truncation gap" and argue that understanding and addressingit is essential as the importance of CTDGs grows, discussing potential futuredirections for research in this area.</description><author>João Bravo, Jacopo Bono, Pedro Saleiro, Hugo Ferreira, Pedro Bizarro</author><pubDate>Mon, 30 Dec 2024 16:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21046v1</guid></item><item><title>E2EDiff: Direct Mapping from Noise to Data for Enhanced Diffusion Models</title><link>http://arxiv.org/abs/2412.21044v1</link><description>Diffusion models have emerged as a powerful framework for generativemodeling, achieving state-of-the-art performance across various tasks. However,they face several inherent limitations, including a training-sampling gap,information leakage in the progressive noising process, and the inability toincorporate advanced loss functions like perceptual and adversarial lossesduring training. To address these challenges, we propose an innovativeend-to-end training framework that aligns the training and sampling processesby directly optimizing the final reconstruction output. Our method eliminatesthe training-sampling gap, mitigates information leakage by treating thetraining process as a direct mapping from pure noise to the target datadistribution, and enables the integration of perceptual and adversarial lossesinto the objective. Extensive experiments on benchmarks such as COCO30K andHW30K demonstrate that our approach consistently outperforms traditionaldiffusion models, achieving superior results in terms of FID and CLIP score,even with reduced sampling steps. These findings highlight the potential ofend-to-end training to advance diffusion-based generative models toward morerobust and efficient solutions.</description><author>Zhiyu Tan, WenXu Qian, Hesen Chen, Mengping Yang, Lei Chen, Hao Li</author><pubDate>Mon, 30 Dec 2024 16:06:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21044v1</guid></item><item><title>Visual Style Prompt Learning Using Diffusion Models for Blind Face Restoration</title><link>http://arxiv.org/abs/2412.21042v1</link><description>Blind face restoration aims to recover high-quality facial images fromvarious unidentified sources of degradation, posing significant challenges dueto the minimal information retrievable from the degraded images. Priorknowledge-based methods, leveraging geometric priors and facial features, haveled to advancements in face restoration but often fall short of capturing finedetails. To address this, we introduce a visual style prompt learning frameworkthat utilizes diffusion probabilistic models to explicitly generate visualprompts within the latent space of pre-trained generative models. These promptsare designed to guide the restoration process. To fully utilize the visualprompts and enhance the extraction of informative and rich patterns, weintroduce a style-modulated aggregation transformation layer. Extensiveexperiments and applications demonstrate the superiority of our method inachieving high-quality blind face restoration. The source code is available at\href{https://github.com/LonglongaaaGo/VSPBFR}{https://github.com/LonglongaaaGo/VSPBFR}.</description><author>Wanglong Lu, Jikai Wang, Tao Wang, Kaihao Zhang, Xianta Jiang, Hanli Zhao</author><pubDate>Mon, 30 Dec 2024 16:05:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21042v1</guid></item><item><title>Occam Gradient Descent</title><link>http://arxiv.org/abs/2405.20194v7</link><description>Deep learning neural network models must be large enough to adapt to theirproblem domain, while small enough to avoid overfitting training data duringgradient descent. To balance these competing demands, overprovisioned deeplearning models such as transformers are trained for a single epoch on largedata sets, and hence inefficient with both computing resources and trainingdata. In response to these inefficiencies, we exploit learning theory to deriveOccam Gradient Descent, an algorithm that interleaves adaptive reduction ofmodel size to minimize generalization error, with gradient descent on modelweights to minimize fitting error. In contrast, traditional gradient descentgreedily minimizes fitting error without regard to generalization error. Ouralgorithm simultaneously descends the space of weights and topological size ofany neural network without modification. With respect to loss, compute andmodel size, our experiments show (a) on image classification benchmarks, linearand convolutional neural networks trained with Occam Gradient Descentoutperform traditional gradient descent with or without post-train pruning; (b)on a range of tabular data classification tasks, neural networks trained withOccam Gradient Descent outperform traditional gradient descent, as well asRandom Forests; (c) on natural language transformers, Occam Gradient Descentoutperforms traditional gradient descent.</description><author>B. N. Kausik</author><pubDate>Mon, 30 Dec 2024 16:04:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20194v7</guid></item><item><title>TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization</title><link>http://arxiv.org/abs/2412.21037v1</link><description>We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative modelwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audioin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA modelslies in the difficulty of creating preference pairs, as TTA lacks structuredmechanisms like verifiable rewards or gold-standard answers available for LargeLanguage Models (LLMs). To address this, we propose CLAP-Ranked PreferenceOptimization (CRPO), a novel framework that iteratively generates and optimizespreference data to enhance TTA alignment. We demonstrate that the audiopreference dataset generated using CRPO outperforms existing alternatives. Withthis framework, TangoFlux achieves state-of-the-art performance across bothobjective and subjective benchmarks. We open source all code and models tosupport further research in TTA generation.</description><author>Chia-Yu Hung, Navonil Majumder, Zhifeng Kong, Ambuj Mehrish, Rafael Valle, Bryan Catanzaro, Soujanya Poria</author><pubDate>Mon, 30 Dec 2024 16:02:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21037v1</guid></item><item><title>GePBench: Evaluating Fundamental Geometric Perception for Multimodal Large Language Models</title><link>http://arxiv.org/abs/2412.21036v1</link><description>Multimodal large language models (MLLMs) have achieved significantadvancements in integrating visual and linguistic understanding. While existingbenchmarks evaluate these models in context-rich, real-life scenarios, theyoften overlook fundamental perceptual skills essential for environmentsdeviating from everyday realism. In particular, geometric perception, theability to interpret spatial relationships and abstract visual patterns,remains underexplored. To address this limitation, we introduce GePBench, anovel benchmark designed to assess the geometric perception capabilities ofMLLMs. Results from extensive evaluations reveal that current state-of-the-artMLLMs exhibit significant deficiencies in such tasks. Additionally, wedemonstrate that models trained with data sourced from GePBench show notableimprovements on a wide range of downstream tasks, underscoring the importanceof geometric perception as a foundation for advanced multimodal applications.Our code and datasets will be publicly available.</description><author>Shangyu Xing, Changhao Xiang, Yuteng Han, Yifan Yue, Zhen Wu, Xinyu Liu, Zhangtai Wu, Fei Zhao, Xinyu Dai</author><pubDate>Mon, 30 Dec 2024 16:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21036v1</guid></item><item><title>Machine Learning Optimal Ordering in Global Routing Problems in Semiconductors</title><link>http://arxiv.org/abs/2412.21035v1</link><description>In this work, we propose a new method for ordering nets during the process oflayer assignment in global routing problems. The global routing problems thatwe focus on in this work are based on routing problems that occur in the designof substrates in multilayered semiconductor packages. The proposed new methodis based on machine learning techniques and we show that the proposed methodsupersedes conventional net ordering techniques based on heuristic scorefunctions. We perform global routing experiments in multilayered semiconductorpackage environments in order to illustrate that the routing order based on ournew proposed technique outperforms previous methods based on heuristics. Ourapproach of using machine learning for global routing targets specifically thenet ordering step which we show in this work can be significantly improved bydeep learning.</description><author>Heejin Choi, Minji Lee, Chang Hyeong Lee, Jaeho Yang, Rak-Kyeong Seong</author><pubDate>Mon, 30 Dec 2024 15:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21035v1</guid></item><item><title>CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code</title><link>http://arxiv.org/abs/2404.15639v3</link><description>Large Language Models (LLMs) have achieved remarkable progress in codegeneration. It now becomes crucial to identify whether the code is AI-generatedand to determine the specific model used, particularly for purposes such asprotecting Intellectual Property (IP) in industry and preventing cheating inprogramming exercises. To this end, several attempts have been made to insertwatermarks into machine-generated code. However, existing approaches arelimited to inserting only a single bit of information. In this paper, weintroduce CodeIP, a novel multi-bit watermarking technique that insertsadditional information to preserve crucial provenance details, such as thevendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation.Furthermore, to ensure the syntactical correctness of the generated code, wepropose constraining the sampling process for predicting the next token bytraining a type predictor. Experiments conducted on a real-world dataset acrossfive programming languages demonstrate the effectiveness of CodeIP inwatermarking LLMs for code generation while maintaining the syntacticalcorrectness of code.</description><author>Batu Guan, Yao Wan, Zhangqian Bi, Zheng Wang, Hongyu Zhang, Pan Zhou, Lichao Sun</author><pubDate>Mon, 30 Dec 2024 15:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15639v3</guid></item><item><title>Plancraft: an evaluation dataset for planning with LLM agents</title><link>http://arxiv.org/abs/2412.21033v1</link><description>We present Plancraft, a multi-modal evaluation dataset for LLM agents.Plancraft has both a text-only and multi-modal interface, based on theMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use andRetrieval Augmented Generation (RAG), as well as an oracle planner and oracleRAG information extractor, to ablate the different components of a modern agentarchitecture. To evaluate decision-making, Plancraft also includes a subset ofexamples that are intentionally unsolvable, providing a realistic challengethat requires the agent not only to complete tasks but also to decide whetherthey are solvable at all. We benchmark both open-source and closed-source LLMsand strategies on our task and compare their performance to a handcraftedplanner. We find that LLMs and VLMs struggle with the planning problems thatPlancraft introduces, and we offer suggestions on how to improve theircapabilities.</description><author>Gautier Dagan, Frank Keller, Alex Lascarides</author><pubDate>Mon, 30 Dec 2024 15:58:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21033v1</guid></item><item><title>Improving Location-based Thermal Emission Side-Channel Analysis Using Iterative Transfer Learning</title><link>http://arxiv.org/abs/2412.21030v1</link><description>This paper proposes the use of iterative transfer learning applied to deeplearning models for side-channel attacks. Currently, most of the side-channelattack methods train a model for each individual byte, without considering thecorrelation between bytes. However, since the models' parameters for attackingdifferent bytes may be similar, we can leverage transfer learning, meaning thatwe first train the model for one of the key bytes, then use the trained modelas a pretrained model for the remaining bytes. This technique can be appliediteratively, a process known as iterative transfer learning. Experimentalresults show that when using thermal or power consumption map images as input,and multilayer perceptron or convolutional neural network as the model, ourmethod improves average performance, especially when the amount of data isinsufficient.</description><author>Tun-Chieh Lou, Chung-Che Wang, Jyh-Shing Roger Jang, Henian Li, Lang Lin, Norman Chang</author><pubDate>Mon, 30 Dec 2024 15:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21030v1</guid></item><item><title>EdgeRAG: Online-Indexed RAG for Edge Devices</title><link>http://arxiv.org/abs/2412.21023v1</link><description>Deploying Retrieval Augmented Generation (RAG) on resource-constrained edgedevices is challenging due to limited memory and processing power. In thiswork, we propose EdgeRAG which addresses the memory constraint by pruningembeddings within clusters and generating embeddings on-demand duringretrieval. To avoid the latency of generating embeddings for large tailclusters, EdgeRAG pre-computes and stores embeddings for these clusters, whileadaptively caching remaining embeddings to minimize redundant computations andfurther optimize latency. The result from BEIR suite shows that EdgeRAG offerssignificant latency reduction over the baseline IVF index, but with similargeneration quality while allowing all of our evaluated datasets to fit into thememory.</description><author>Korakit Seemakhupt, Sihang Liu, Samira Khan</author><pubDate>Mon, 30 Dec 2024 15:46:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21023v1</guid></item><item><title>Text Classification: Neural Networks VS Machine Learning Models VS Pre-trained Models</title><link>http://arxiv.org/abs/2412.21022v1</link><description>Text classification is a very common task nowadays and there are manyefficient methods and algorithms that we can employ to accomplish it.Transformers have revolutionized the field of deep learning, particularly inNatural Language Processing (NLP) and have rapidly expanded to other domainssuch as computer vision, time-series analysis and more. The transformer modelwas firstly introduced in the context of machine translation and itsarchitecture relies on self-attention mechanisms to capture complexrelationships within data sequences. It is able to handle long-rangedependencies more effectively than traditional neural networks (such asRecurrent Neural Networks and Multilayer Perceptrons). In this work, we presenta comparison between different techniques to perform text classification. Wetake into consideration seven pre-trained models, three standard neuralnetworks and three machine learning models. For standard neural networks andmachine learning models we also compare two embedding techniques: TF-IDF andGloVe, with the latter consistently outperforming the former. Finally, wedemonstrate the results from our experiments where pre-trained models such asBERT and DistilBERT always perform better than standard models/algorithms.</description><author>Christos Petridis</author><pubDate>Mon, 30 Dec 2024 15:44:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21022v1</guid></item><item><title>Automatic feature selection and weighting in molecular systems using Differentiable Information Imbalance</title><link>http://arxiv.org/abs/2411.00851v2</link><description>Feature selection is essential in the analysis of molecular systems and manyother fields, but several uncertainties remain: What is the optimal number offeatures for a simplified, interpretable model that retains essentialinformation? How should features with different units be aligned, and howshould their relative importance be weighted? Here, we introduce theDifferentiable Information Imbalance (DII), an automated method to rankinformation content between sets of features. Using distances in a ground truthfeature space, DII identifies a low-dimensional subset of features that bestpreserves these relationships. Each feature is scaled by a weight, which isoptimized by minimizing the DII through gradient descent. This allowssimultaneously performing unit alignment and relative importance scaling, whilepreserving interpretability. DII can also produce sparse solutions anddetermine the optimal size of the reduced feature space. We demonstrate theusefulness of this approach on two benchmark molecular problems: (1)identifying collective variables that describe conformations of a biomolecule,and (2) selecting features for training a machine-learning force field. Theseresults show the potential of DII in addressing feature selection challengesand optimizing dimensionality in various applications. The method is availablein the Python library DADApy.</description><author>Romina Wild, Felix Wodaczek, Vittorio Del Tatto, Bingqing Cheng, Alessandro Laio</author><pubDate>Mon, 30 Dec 2024 15:38:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00851v2</guid></item><item><title>MapQaTor: A System for Efficient Annotation of Map Query Datasets</title><link>http://arxiv.org/abs/2412.21015v1</link><description>Mapping and navigation services like Google Maps, Apple Maps, OpenstreetMaps, are essential for accessing various location-based data, yet they oftenstruggle to handle natural language geospatial queries. Recent advancements inLarge Language Models (LLMs) show promise in question answering (QA), butcreating reliable geospatial QA datasets from map services remains challenging.We introduce MapQaTor, a web application that streamlines the creation ofreproducible, traceable map-based QA datasets. With its plug-and-playarchitecture, MapQaTor enables seamless integration with any maps API, allowingusers to gather and visualize data from diverse sources with minimal setup. Bycaching API responses, the platform ensures consistent ground truth, enhancingthe reliability of the data even as real-world information evolves. MapQaTorcentralizes data retrieval, annotation, and visualization within a singleplatform, offering a unique opportunity to evaluate the current state ofLLM-based geospatial reasoning while advancing their capabilities for improvedgeospatial understanding. Evaluation metrics show that, MapQaTor speeds up theannotation process by at least 30 times compared to manual methods,underscoring its potential for developing geospatial resources, such as complexmap reasoning datasets. The website is live at: https://mapqator.github.io/ anda demo video is available at: https://youtu.be/7_aV9Wmhs6Q.</description><author>Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez</author><pubDate>Mon, 30 Dec 2024 15:33:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21015v1</guid></item><item><title>From Interests to Insights: An LLM Approach to Course Recommendations Using Natural Language Queries</title><link>http://arxiv.org/abs/2412.19312v2</link><description>Most universities in the United States encourage their students to exploreacademic areas before declaring a major and to acquire academic breadth bysatisfying a variety of requirements. Each term, students must choose amongmany thousands of offerings, spanning dozens of subject areas, a handful ofcourses to take. The curricular environment is also dynamic, and poorcommunication and search functions on campus can limit a student's ability todiscover new courses of interest. To support both students and their advisersin such a setting, we explore a novel Large Language Model (LLM) courserecommendation system that applies a Retrieval Augmented Generation (RAG)method to the corpus of course descriptions. The system first generates an'ideal' course description based on the user's query. This description isconverted into a search vector using embeddings, which is then used to findactual courses with similar content by comparing embedding similarities. Wedescribe the method and assess the quality and fairness of some exampleprompts. Steps to deploy a pilot system on campus are discussed.</description><author>Hugh Van Deventer, Mark Mills, August Evrard</author><pubDate>Mon, 30 Dec 2024 15:30:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19312v2</guid></item><item><title>Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline</title><link>http://arxiv.org/abs/2412.21009v1</link><description>Recent advancements in deep learning have significantly enhancedcontent-based retrieval methods, notably through models like CLIP that mapimages and texts into a shared embedding space. However, these methods oftenstruggle with domain-specific entities and long-tail concepts absent from theirtraining data, particularly in identifying specific individuals. In this paper,we explore the task of identity-aware cross-modal retrieval, which aims toretrieve images of persons in specific contexts based on natural languagequeries. This task is critical in various scenarios, such as for searching andbrowsing personalized video collections or large audio-visual archivesmaintained by national broadcasters. We introduce a novel dataset, COCO PersonFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enrichedwith deepfake-generated faces from VGGFace2. This dataset addresses the lack oflarge-scale datasets needed for training and evaluating models for this task.Our experiments assess the performance of different CLIP variations repurposedfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), whichachieves competitive retrieval performance through targeted fine-tuning. Ourcontributions lay the groundwork for more robust cross-modal retrieval systemscapable of recognizing long-tail identities and contextual nuances. Data andcode are available at https://github.com/mesnico/IdCLIP.</description><author>Nicola Messina, Lucia Vadicamo, Leo Maltese, Claudio Gennaro</author><pubDate>Mon, 30 Dec 2024 15:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21009v1</guid></item><item><title>Robust semi-parametric signal detection in particle physics with classifiers decorrelated via optimal transport</title><link>http://arxiv.org/abs/2409.06399v2</link><description>Searches of new signals in particle physics are usually done by training asupervised classifier to separate a signal model from the known Standard Modelphysics (also called the background model). However, even when the signal modelis correct, systematic errors in the background model can influence supervisedclassifiers and might adversely affect the signal detection procedure. Totackle this problem, one approach is to use the (possibly misspecified)classifier only to perform a preliminary signal-enrichment step and then tocarry out a bump hunt on the signal-rich sample using only the realexperimental data. For this procedure to work, we need a classifier constrainedto be decorrelated with one or more protected variables used for the signaldetection step. We do this by considering an optimal transport map of theclassifier output that makes it independent of the protected variable(s) forthe background. We then fit a semi-parametric mixture model to the distributionof the protected variable after making cuts on the transformed classifier todetect the presence of a signal. We compare and contrast this decorrelationmethod with previous approaches, show that the decorrelation procedure isrobust to moderate background misspecification, and analyse the power of thesignal detection test as a function of the cut on the classifier.</description><author>Purvasha Chakravarti, Lucas Kania, Olaf Behnke, Mikael Kuusela, Larry Wasserman</author><pubDate>Mon, 30 Dec 2024 15:16:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06399v2</guid></item><item><title>Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria</title><link>http://arxiv.org/abs/2412.21006v1</link><description>Large Language Models (LLMs) rely on generating extensive intermediatereasoning units (e.g., tokens, sentences) to enhance final answer qualityacross a wide range of complex tasks. While generating multiple reasoning pathsor iteratively refining rationales proves effective for improving performance,these approaches inevitably result in significantly higher inference costs. Inthis work, we propose a novel sentence-level rationale reduction trainingframework that leverages likelihood-based criteria, verbosity, to identify andremove redundant reasoning sentences. Unlike previous approaches that utilizetoken-level reduction, our sentence-level reduction framework maintains modelperformance while reducing generation length. This preserves the originalreasoning abilities of LLMs and achieves an average 17.15% reduction ingeneration costs across various models and tasks.</description><author>Joonwon Jang, Jaehee Kim, Wonbin Kweon, Hwanjo Yu</author><pubDate>Mon, 30 Dec 2024 15:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21006v1</guid></item><item><title>Weber-Fechner Law in Temporal Difference learning derived from Control as Inference</title><link>http://arxiv.org/abs/2412.21004v1</link><description>This paper investigates a novel nonlinear update rule based on temporaldifference (TD) errors in reinforcement learning (RL). The update rule in thestandard RL states that the TD error is linearly proportional to the degree ofupdates, treating all rewards equally without no bias. On the other hand, therecent biological studies revealed that there are nonlinearities in the TDerror and the degree of updates, biasing policies optimistic or pessimistic.Such biases in learning due to nonlinearities are expected to be useful andintentionally leftover features in biological learning. Therefore, thisresearch explores a theoretical framework that can leverage the nonlinearitybetween the degree of the update and TD errors. To this end, we focus on acontrol as inference framework, since it is known as a generalized formulationencompassing various RL and optimal control methods. In particular, weinvestigate the uncomputable nonlinear term needed to be approximately excludedin the derivation of the standard RL from control as inference. By analyzingit, Weber-Fechner law (WFL) is found, namely, perception (a.k.a. the degree ofupdates) in response to stimulus change (a.k.a. TD error) is attenuated byincrease in the stimulus intensity (a.k.a. the value function). To numericallyreveal the utilities of WFL on RL, we then propose a practical implementationusing a reward-punishment framework and modifying the definition of optimality.Analysis of this implementation reveals that two utilities can be expected i)to increase rewards to a certain level early, and ii) to sufficiently suppresspunishment. We finally investigate and discuss the expected utilities throughsimulations and robot experiments. As a result, the proposed RL algorithm withWFL shows the expected utilities that accelerate the reward-maximizing startupand continue to suppress punishments during learning.</description><author>Keiichiro Takahashi, Taisuke Kobayashi, Tomoya Yamanokuchi, Takamitsu Matsubara</author><pubDate>Mon, 30 Dec 2024 15:13:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21004v1</guid></item><item><title>LEASE: Offline Preference-based Reinforcement Learning with High Sample Efficiency</title><link>http://arxiv.org/abs/2412.21001v1</link><description>Offline preference-based reinforcement learning (PbRL) provides an effectiveway to overcome the challenges of designing reward and the high costs of onlineinteraction. However, since labeling preference needs real-time human feedback,acquiring sufficient preference labels is challenging. To solve this, thispaper proposes a offLine prEference-bAsed RL with high Sample Efficiency(LEASE) algorithm, where a learned transition model is leveraged to generateunlabeled preference data. Considering the pretrained reward model may generateincorrect labels for unlabeled data, we design an uncertainty-aware mechanismto ensure the performance of reward model, where only high confidence and lowvariance data are selected. Moreover, we provide the generalization bound ofreward model to analyze the factors influencing reward accuracy, anddemonstrate that the policy learned by LEASE has theoretical improvementguarantee. The developed theory is based on state-action pair, which can beeasily combined with other offline algorithms. The experimental results showthat LEASE can achieve comparable performance to baseline under fewerpreference data without online interaction.</description><author>Xiao-Yin Liu, Guotao Li, Xiao-Hu Zhou, Zeng-Guang Hou</author><pubDate>Mon, 30 Dec 2024 15:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.21001v1</guid></item><item><title>A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine</title><link>http://arxiv.org/abs/2405.08603v3</link><description>Since the release of ChatGPT and GPT-4, large language models (LLMs) andmultimodal large language models (MLLMs) have attracted widespread attentionfor their exceptional capabilities in understanding, reasoning, and generation,introducing transformative paradigms for integrating artificial intelligenceinto medicine. This survey provides a comprehensive overview of thedevelopment, principles, application scenarios, challenges, and futuredirections of LLMs and MLLMs in medicine. Specifically, it begins by examiningthe paradigm shift, tracing the transition from traditional models to LLMs andMLLMs, and highlighting the unique advantages of these LLMs and MLLMs inmedical applications. Next, the survey reviews existing medical LLMs and MLLMs,providing detailed guidance on their construction and evaluation in a clear andsystematic manner. Subsequently, to underscore the substantial value of LLMsand MLLMs in healthcare, the survey explores five promising applications in thefield. Finally, the survey addresses the challenges confronting medical LLMsand MLLMs and proposes practical strategies and future directions for theirintegration into medicine. In summary, this survey offers a comprehensiveanalysis of the technical methodologies and practical clinical applications ofmedical LLMs and MLLMs, with the goal of bridging the gap between theseadvanced technologies and clinical practice, thereby fostering the evolution ofthe next generation of intelligent healthcare systems.</description><author>Hanguang Xiao, Feizhong Zhou, Xingyue Liu, Tianqi Liu, Zhipeng Li, Xin Liu, Xiaoxuan Huang</author><pubDate>Mon, 30 Dec 2024 15:08:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08603v3</guid></item><item><title>Plug-and-Play Training Framework for Preference Optimization</title><link>http://arxiv.org/abs/2412.20996v1</link><description>Recently, preference optimization methods such as DPO have significantlyenhanced large language models (LLMs) in wide tasks including dialogue andquestion-answering. However, current methods fail to account for the varyingdifficulty levels of training samples during preference optimization, leadingto mediocre performance in tasks with high accuracy requirements, particularlyin mathematical reasoning. To address this limitation, we propose a noveltraining framework, which employs multiple sampling to analyze outputdistributions, assign different weights to samples, and incorporate theseweights into the preference optimization process. This plug-and-play approachenables LLMs to prioritize challenging examples during training, improvinglearning efficiency. Experimental results demonstrate that our frameworkintegrates seamlessly with various preference optimization methods and achievesconsistent improvements in mathematical reasoning tasks.</description><author>Jingyuan Ma, Rui Li, Zheng Li, Lei Sha, Zhifang Sui</author><pubDate>Mon, 30 Dec 2024 15:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20996v1</guid></item><item><title>EigenSR: Eigenimage-Bridged Pre-Trained RGB Learners for Single Hyperspectral Image Super-Resolution</title><link>http://arxiv.org/abs/2409.04050v2</link><description>Single hyperspectral image super-resolution (single-HSI-SR) aims to improvethe resolution of a single input low-resolution HSI. Due to the bottleneck ofdata scarcity, the development of single-HSI-SR lags far behind that of RGBnatural images. In recent years, research on RGB SR has shown that modelspre-trained on large-scale benchmark datasets can greatly improve performanceon unseen data, which may stand as a remedy for HSI. But how can we transferthe pre-trained RGB model to HSI, to overcome the data-scarcity bottleneck?Because of the significant difference in the channels between the pre-trainedRGB model and the HSI, the model cannot focus on the correlation along thespectral dimension, thus limiting its ability to utilize on HSI. Inspired bythe HSI spatial-spectral decoupling, we propose a new framework that firstfine-tunes the pre-trained model with the spatial components (known aseigenimages), and then infers on unseen HSI using an iterative spectralregularization (ISR) to maintain the spectral correlation. The advantages ofour method lie in: 1) we effectively inject the spatial texture processingcapabilities of the pre-trained RGB model into HSI while keeping spectralfidelity, 2) learning in the spectral-decorrelated domain can improve thegeneralizability to spectral-agnostic data, and 3) our inference in theeigenimage domain naturally exploits the spectral low-rank property of HSI,thereby reducing the complexity. This work bridges the gap between pre-trainedRGB models and HSI via eigenimages, addressing the issue of limited HSItraining data, hence the name EigenSR. Extensive experiments show that EigenSRoutperforms the state-of-the-art (SOTA) methods in both spatial and spectralmetrics.</description><author>Xi Su, Xiangfei Shen, Mingyang Wan, Jing Nie, Lihui Chen, Haijun Liu, Xichuan Zhou</author><pubDate>Mon, 30 Dec 2024 14:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.04050v2</guid></item><item><title>KARPA: A Training-free Method of Adapting Knowledge Graph as References for Large Language Model's Reasoning Path Aggregation</title><link>http://arxiv.org/abs/2412.20995v1</link><description>Large language models (LLMs) demonstrate exceptional performance across avariety of tasks, yet they are often affected by hallucinations and thetimeliness of knowledge. Leveraging knowledge graphs (KGs) as externalknowledge sources has emerged as a viable solution, but existing methods forLLM-based knowledge graph question answering (KGQA) are often limited bystep-by-step decision-making on KGs, restricting the global planning andreasoning capabilities of LLMs, or they require fine-tuning or pre-training onspecific KGs. To address these challenges, we propose Knowledge graph AssistedReasoning Path Aggregation (KARPA), a novel framework that harnesses the globalplanning abilities of LLMs for efficient and accurate KG reasoning. KARPAoperates in three steps: pre-planning relation paths using the LLM's globalplanning capabilities, matching semantically relevant paths via an embeddingmodel, and reasoning over these paths to generate answers. Unlike existing KGQAmethods, KARPA avoids stepwise traversal, requires no additional training, andis adaptable to various LLM architectures. Extensive experimental results showthat KARPA achieves state-of-the-art performance in KGQA tasks, delivering bothhigh efficiency and accuracy. Our code will be available on Github.</description><author>Siyuan Fang, Kaijing Ma, Tianyu Zheng, Xinrun Du, Ningxuan Lu, Ge Zhang, Qingkun Tang</author><pubDate>Mon, 30 Dec 2024 14:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20995v1</guid></item><item><title>Efficiently Serving LLM Reasoning Programs with Certaindex</title><link>http://arxiv.org/abs/2412.20993v1</link><description>The rapid evolution of large language models (LLMs) has unlocked theircapabilities in advanced reasoning tasks like mathematical problem-solving,code generation, and legal analysis. Central to this progress areinference-time reasoning algorithms, which refine outputs by exploring multiplesolution paths, at the cost of increasing compute demands and responselatencies. Existing serving systems fail to adapt to the scaling behaviors ofthese algorithms or the varying difficulty of queries, leading to inefficientresource use and unmet latency targets. We present Dynasor, a system that optimizes inference-time compute for LLMreasoning queries. Unlike traditional engines, Dynasor tracks and schedulesrequests within reasoning queries and uses Certaindex, a proxy that measuresstatistical reasoning progress based on model certainty, to guide computeallocation dynamically. Dynasor co-adapts scheduling with reasoning progress:it allocates more compute to hard queries, reduces compute for simpler ones,and terminates unpromising queries early, balancing accuracy, latency, andcost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50%in batch processing and sustaining 3.3x higher query rates or 4.7x tighterlatency SLOs in online serving.</description><author>Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Aurick Qiao, Hao Zhang</author><pubDate>Mon, 30 Dec 2024 14:57:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20993v1</guid></item><item><title>Verified Lifting of Deep learning Operators</title><link>http://arxiv.org/abs/2412.20992v1</link><description>Deep learning operators are fundamental components of modern deep learningframeworks. With the growing demand for customized operators, it has becomeincreasingly common for developers to create their own. However, designing andimplementing operators is complex and error-prone, due to hardware-specificoptimizations and the need for numerical stability. There is a pressing needfor tools that can summarize the functionality of both existing anduser-defined operators. To address this gap, this work introduces a novelframework for the verified lifting of deep learning operators, whichsynthesizes high-level mathematical formulas from low-level implementations.Our approach combines symbolic execution, syntax-guided synthesis, andSMT-based verification to produce readable and formally verified mathematicalformulas. In synthesis, we employ a combination of top-down and bottom-upstrategies to explore the vast search space efficiently; In verification, wedesign invariant synthesis patterns and leverage SMT solvers to validate thecorrectness of the derived summaries; In simplification, we use egraph-basedtechniques with custom rules to restore complex formulas to their natural,intuitive forms. Evaluated on a dataset of deep learning operators implementedin Triton from the real world, our method demonstrates the effectiveness ofsynthesis and verification compared to existing techniques. This frameworkbridges the gap between low-level implementations and high-level abstractions,improving understanding and reliability in deep learning operator development.</description><author>Qi Zhan, Xing Hu, Xin Xia, Shanping Li</author><pubDate>Mon, 30 Dec 2024 14:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20992v1</guid></item><item><title>Embodied Image Quality Assessment for Robotic Intelligence</title><link>http://arxiv.org/abs/2412.18774v2</link><description>Image quality assessment (IQA) of user-generated content (UGC) is a criticaltechnique for human quality of experience (QoE). However, for robot-generatedcontent (RGC), will its image quality be consistent with the Moravec paradoxand counter to human common sense? Human subjective scoring is more based onthe attractiveness of the image. Embodied agent are required to interact andperceive in the environment, and finally perform specific tasks. Visual imagesas inputs directly influence downstream tasks. In this paper, we first proposean embodied image quality assessment (EIQA) frameworks. We establish assessmentmetrics for input images based on the downstream tasks of robot. In addition,we construct an Embodied Preference Database (EPD) containing 5,000 referenceand distorted image annotations. The performance of mainstream IQA algorithmson EPD dataset is finally verified. The experiments demonstrate that qualityassessment of embodied images is different from that of humans. We sincerelyhope that the EPD can contribute to the development of embodied AI by focusingon image quality assessment. The benchmark is available athttps://github.com/Jianbo-maker/EPD_benchmark.</description><author>Jianbo Zhang, Chunyi Li, Liang Yuan, Guoquan Zheng, Jie Hao, Guangtao Zhai</author><pubDate>Mon, 30 Dec 2024 14:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18774v2</guid></item><item><title>RobustBlack: Challenging Black-Box Adversarial Attacks on State-of-the-Art Defenses</title><link>http://arxiv.org/abs/2412.20987v1</link><description>Although adversarial robustness has been extensively studied in white-boxsettings, recent advances in black-box attacks (including transfer- andquery-based approaches) are primarily benchmarked against weak defenses,leaving a significant gap in the evaluation of their effectiveness against morerecent and moderate robust models (e.g., those featured in the Robustbenchleaderboard). In this paper, we question this lack of attention from black-boxattacks to robust models. We establish a framework to evaluate theeffectiveness of recent black-box attacks against both top-performing andstandard defense mechanisms, on the ImageNet dataset. Our empirical evaluationreveals the following key findings: (1) the most advanced black-box attacksstruggle to succeed even against simple adversarially trained models; (2)robust models that are optimized to withstand strong white-box attacks, such asAutoAttack, also exhibits enhanced resilience against black-box attacks; and(3) robustness alignment between the surrogate models and the target modelplays a key factor in the success rate of transfer-based attacks</description><author>Mohamed Djilani, Salah Ghamizi, Maxime Cordy</author><pubDate>Mon, 30 Dec 2024 14:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20987v1</guid></item><item><title>SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</title><link>http://arxiv.org/abs/2412.12094v3</link><description>Large Language Models (LLMs) have exhibited exceptional performance across aspectrum of natural language processing tasks. However, their substantial sizespose considerable challenges, particularly in computational demands andinference speed, due to their quadratic complexity. In this work, we haveidentified a key pattern: certain seemingly meaningless special tokens (i.e.,separators) contribute disproportionately to attention scores compared tosemantically meaningful tokens. This observation suggests that information ofthe segments between these separator tokens can be effectively condensed intothe separator tokens themselves without significant information loss. Guided bythis insight, we introduce SepLLM, a plug-and-play framework that acceleratesinference by compressing these segments and eliminating redundant tokens.Additionally, we implement efficient kernels for training acceleration.Experimental results across training-free, training-from-scratch, andpost-training settings demonstrate SepLLM's effectiveness. Notably, using theLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on theGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, instreaming settings, SepLLM effectively processes sequences of up to 4 milliontokens or more while maintaining consistent language modeling capabilities.</description><author>Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, Chao Huang</author><pubDate>Mon, 30 Dec 2024 14:54:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12094v3</guid></item><item><title>AlignAb: Pareto-Optimal Energy Alignment for Designing Nature-Like Antibodies</title><link>http://arxiv.org/abs/2412.20984v1</link><description>We present a three-stage framework for training deep learning modelsspecializing in antibody sequence-structure co-design. We first pre-train alanguage model using millions of antibody sequence data. Then, we employ thelearned representations to guide the training of a diffusion model for jointoptimization over both sequence and structure of antibodies. During the finalalignment stage, we optimize the model to favor antibodies with low repulsionand high attraction to the antigen binding site, enhancing the rationality andfunctionality of the designs. To mitigate conflicting energy preferences, weextend AbDPO (Antibody Direct Preference Optimization) to guide the modeltowards Pareto optimality under multiple energy-based alignment objectives.Furthermore, we adopt an iterative learning paradigm with temperature scaling,enabling the model to benefit from diverse online datasets without requiringadditional data. In practice, our proposed methods achieve high stability andefficiency in producing a better Pareto front of antibody designs compared totop samples generated by baselines and previous alignment techniques. Throughextensive experiments, we showcase the superior performance of our methods ingenerating nature-like antibodies with high binding affinity consistently.</description><author>Yibo Wen, Chenwei Xu, Jerry Yao-Chieh Hu, Han Liu</author><pubDate>Mon, 30 Dec 2024 14:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20984v1</guid></item><item><title>CNNtention: Can CNNs do better with Attention?</title><link>http://arxiv.org/abs/2412.11657v3</link><description>Convolutional Neural Networks (CNNs) have been the standard for imageclassification tasks for a long time, but more recently attention-basedmechanisms have gained traction. This project aims to compare traditional CNNswith attention-augmented CNNs across an image classification task. Byevaluating and comparing their performance, accuracy and computationalefficiency, the project will highlight benefits and trade-off of the localizedfeature extraction of traditional CNNs and the global context capture inattention-augmented CNNs. By doing this, we can reveal further insights intotheir respective strengths and weaknesses, guide the selection of models basedon specific application needs and ultimately, enhance understanding of thesearchitectures in the deep learning community. This was our final project for CS7643 Deep Learning course at Georgia Tech.</description><author>Nikhil Kapila, Julian Glattki, Tejas Rathi</author><pubDate>Mon, 30 Dec 2024 14:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11657v3</guid></item><item><title>Efficient Parallel Genetic Algorithm for Perturbed Substructure Optimization in Complex Network</title><link>http://arxiv.org/abs/2412.20980v1</link><description>Evolutionary computing, particularly genetic algorithm (GA), is acombinatorial optimization method inspired by natural selection and thetransmission of genetic information, which is widely used to identify optimalsolutions to complex problems through simulated programming and iteration. Dueto its strong adaptability, flexibility, and robustness, GA has shownsignificant performance and potentiality on perturbed substructure optimization(PSSO), an important graph mining problem that achieves its goals by modifyingnetwork structures. However, the efficiency and practicality of GA-based PSSOface enormous challenges due to the complexity and diversity of applicationscenarios. While some research has explored acceleration frameworks inevolutionary computing, their performance on PSSO remains limited due to a lackof scenario generalizability. Based on these, this paper is the first topresent the GA-based PSSO Acceleration framework (GAPA), which simplifies theGA development process and supports distributed acceleration. Specifically, itreconstructs the genetic operation and designs a development framework forefficient parallel acceleration. Meanwhile, GAPA includes an extensible librarythat optimizes and accelerates 10 PSSO algorithms, covering 4 crucial tasks forgraph mining. Comprehensive experiments on 18 datasets across 4 tasks and 10algorithms effectively demonstrate the superiority of GAPA, achieving anaverage of 4x the acceleration of Evox. The repository is inhttps://github.com/NetAlsGroup/GAPA.</description><author>Shanqing Yu, Meng Zhou, Jintao Zhou, Minghao Zhao, Yidan Song, Yao Lu, Zeyu Wang, Qi Xuan</author><pubDate>Mon, 30 Dec 2024 14:32:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20980v1</guid></item><item><title>A Graph Neural Network deep-dive into successful counterattacks</title><link>http://arxiv.org/abs/2411.17450v2</link><description>A counterattack in soccer is a high speed, high intensity direct attack thatcan occur when a team transitions from a defensive state to an attacking stateafter regaining possession of the ball. The aim is to create a goal-scoringopportunity by convering a lot of ground with minimal passes before theopposing team can recover their defensive shape. The purpose of this researchis to build gender-specific Graph Neural Networks to model the likelihood of acounterattack being successful and uncover what factors make them successful inprofessional soccer. These models are trained on a total of 20863 frames ofsynchronized on-ball event and spatiotemporal (broadcast) tracking data. Thisdataset is derived from 632 games of MLS (2022), NWSL (2022) and internationalsoccer (2020-2022). With this data we demonstrate that gender-specific GraphNeural Networks outperform architecturally identical gender-ambiguous models inpredicting the successful outcome of counterattacks. We show, using PermutationFeature Importance, that byline to byline speed, angle to the goal, angle tothe ball and sideline to sideline speed are the node features with the highestimpact on model performance. Additionally, we offer some illustrative exampleson how to navigate the infinite solution search space to aid in identifyingimprovements for player decision making. This research is accompanied by an open-source repository containing all dataand code, and it is also accompanied by an open-source Python package whichsimplifies converting spatiotemporal data into graphs. This package alsofacilitates testing, validation, training and prediction with this data. Thisshould allow the reader to replicate and improve upon our research more easily.</description><author>Joris Bekkers, Amod Sahasrabudhe</author><pubDate>Mon, 30 Dec 2024 14:31:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.17450v2</guid></item><item><title>UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI</title><link>http://arxiv.org/abs/2412.20977v1</link><description>We introduce UnrealZoo, a rich collection of photo-realistic 3D virtualworlds built on Unreal Engine, designed to reflect the complexity andvariability of the open worlds. Additionally, we offer a variety of playableentities for embodied AI agents. Based on UnrealCV, we provide a suite ofeasy-to-use Python APIs and tools for various potential applications, such asdata collection, environment augmentation, distributed training, andbenchmarking. We optimize the rendering and communication efficiency ofUnrealCV to support advanced applications, such as multi-agent interaction. Ourexperiments benchmark agents in various complex scenes, focusing on visualnavigation and tracking, which are fundamental capabilities for embodied visualintelligence. The results yield valuable insights into the advantages ofdiverse training environments for reinforcement learning (RL) agents and thechallenges faced by current embodied vision agents, including those based on RLand large vision-language models (VLMs), in open worlds. These challengesinvolve latency in closed-loop control in dynamic scenes and reasoning about 3Dspatial structures in unstructured terrain.</description><author>Fangwei Zhong, Kui Wu, Churan Wang, Hao Chen, Hai Ci, Zhoujun Li, Yizhou Wang</author><pubDate>Mon, 30 Dec 2024 14:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20977v1</guid></item><item><title>FPGA-based Acceleration of Neural Network for Image Classification using Vitis AI</title><link>http://arxiv.org/abs/2412.20974v1</link><description>In recent years, Convolutional Neural Networks (CNNs) have been widelyadopted in computer vision. Complex CNN architecture running on CPU or GPU haseither insufficient throughput or prohibitive power consumption. Hence, thereis a need to have dedicated hardware to accelerate the computation workload tosolve these limitations. In this paper, we accelerate a CNN for imageclassification with the CIFAR-10 dataset using Vitis-AI on Xilinx ZynqUltraScale+ MPSoC ZCU104 FPGA evaluation board. The work achieves 3.33-5.82xhigher throughput and 3.39-6.30x higher energy efficiency than CPU and GPUbaselines. It shows the potential to extract 2D features for downstream tasks,such as depth estimation and 3D reconstruction.</description><author>Zhengdong Li, Frederick Ziyang Hong, C. Patrick Yue</author><pubDate>Mon, 30 Dec 2024 14:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20974v1</guid></item><item><title>On Reward Transferability in Adversarial Inverse Reinforcement Learning: Insights from Random Matrix Theory</title><link>http://arxiv.org/abs/2410.07643v2</link><description>In the context of inverse reinforcement learning (IRL) with a single expert,adversarial inverse reinforcement learning (AIRL) serves as a foundationalapproach to providing comprehensive and transferable task descriptions.However, AIRL faces practical performance challenges, primarily stemming fromthe framework's overly idealized decomposability condition, the unclear proofregarding the potential equilibrium in reward recovery, or questionablerobustness in high-dimensional environments. This paper revisits AIRL in\textbf{high-dimensional scenarios where the state space tends to infinity}.Specifically, we first establish a necessary and sufficient condition forreward transferability by examining the rank of the matrix derived fromsubtracting the identity matrix from the transition matrix. Furthermore,leveraging random matrix theory, we analyze the spectral distribution of thismatrix, demonstrating that our rank criterion holds with high probability evenwhen the transition matrices are unobservable. This suggests that thelimitations on transfer are not inherent to the AIRL framework itself, but areinstead related to the training variance of the reinforcement learningalgorithms employed within it. Based on this insight, we propose a hybridframework that integrates on-policy proximal policy optimization in the sourceenvironment with off-policy soft actor-critic in the target environment,leading to significant improvements in reward transfer effectiveness.</description><author>Yangchun Zhang, Wang Zhou, Yirui Zhou</author><pubDate>Mon, 30 Dec 2024 14:18:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07643v2</guid></item><item><title>Causal Flow-based Variational Auto-Encoder for Disentangled Causal Representation Learning</title><link>http://arxiv.org/abs/2304.09010v5</link><description>Disentangled representation learning aims to learn low-dimensionalrepresentations where each dimension corresponds to an underlying generativefactor. While the Variational Auto-Encoder (VAE) is widely used for thispurpose, most existing methods assume independence among factors, asimplification that does not hold in many real-world scenarios where factorsare often interdependent and exhibit causal relationships. To overcome thislimitation, we propose the Disentangled Causal Variational Auto-Encoder(DCVAE), a novel supervised VAE framework that integrates causal flows into therepresentation learning process, enabling the learning of more meaningful andinterpretable disentangled representations. We evaluate DCVAE on both syntheticand real-world datasets, demonstrating its superior ability in causaldisentanglement and intervention experiments. Furthermore, DCVAE outperformsstate-of-the-art methods in various downstream tasks, highlighting itspotential for learning true causal structures among factors.</description><author>Di Fan, Yannian Kou, Chuanhou Gao</author><pubDate>Mon, 30 Dec 2024 14:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09010v5</guid></item><item><title>Hierarchical Banzhaf Interaction for General Video-Language Representation Learning</title><link>http://arxiv.org/abs/2412.20964v1</link><description>Multimodal representation learning, with contrastive learning, plays animportant role in the artificial intelligence domain. As an important subfield,video-language representation learning focuses on learning representationsusing global semantic interactions between pre-defined video-text pairs.However, to enhance and refine such coarse-grained global interactions, moredetailed interactions are necessary for fine-grained multimodal learning. Inthis study, we introduce a new approach that models video-text as game playersusing multivariate cooperative game theory to handle uncertainty duringfine-grained semantic interactions with diverse granularity, flexiblecombination, and vague intensity. Specifically, we design the HierarchicalBanzhaf Interaction to simulate the fine-grained correspondence between videoclips and textual words from hierarchical perspectives. Furthermore, tomitigate the bias in calculations within Banzhaf Interaction, we proposereconstructing the representation through a fusion of single-modal andcross-modal components. This reconstructed representation ensures finegranularity comparable to that of the single-modal representation, while alsopreserving the adaptive encoding characteristics of cross-modal representation.Additionally, we extend our original structure into a flexible encoder-decoderframework, enabling the model to adapt to various downstream tasks. Extensiveexperiments on commonly used text-video retrieval, video-question answering,and video captioning benchmarks, with superior performance, validate theeffectiveness and generalization of our method.</description><author>Peng Jin, Hao Li, Li Yuan, Shuicheng Yan, Jie Chen</author><pubDate>Mon, 30 Dec 2024 14:09:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20964v1</guid></item><item><title>MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct</title><link>http://arxiv.org/abs/2409.05840v4</link><description>The development of Multimodal Large Language Models (MLLMs) has seensignificant advancements with increasing demands in various fields (e.g.,multimodal agents, embodied intelligence). While model-driven approachesattempt to enhance MLLMs capabilities through diverse architectures, the gainshave become increasingly marginal. Conversely, data-driven methods, which scaleup image-text instruction data, are more effective but face limited datadiversity and complexity challenges. The absence of high-quality dataconstitutes a significant development barrier for MLLMs. To address the dataquality bottleneck, we propose MMEvol, a novel multimodal instruction dataevolution framework. This framework iteratively improve data quality through arefined combination of fine-grained perception, cognitive reasoning, andinteraction evolution, generating a more complex and diverse image-textinstruction dataset that empowers MLLMs with enhanced capabilities. Beginningwith an initial set of instructions, SEED-163K, we utilize MMEvol tosystematically broaden the diversity of instruction types, extend visualreasoning steps to improve cognitive reasoning abilities, and thoroughlyexplore fine-grained information within images to enhance visual understandingand robustness. To comprehensively evaluate the effectiveness of our approach,we conduct extensive qualitative analysis and quantitative experiments across13 vision-language tasks. Compared to baseline models trained with the initialseed data, the results demonstrate that our method achieves an average accuracyimprovement of 3.1 percentage points. Furthermore, our approach reachesstate-of-the-art (SOTA) performance in nine tasks using significantly less datacompared to state-of-the-art models.</description><author>Run Luo, Haonan Zhang, Longze Chen, Ting-En Lin, Xiong Liu, Yuchuan Wu, Min Yang, Minzheng Wang, Pengpeng Zeng, Lianli Gao, Heng Tao Shen, Yunshui Li, Xiaobo Xia, Fei Huang, Jingkuan Song, Yongbin Li</author><pubDate>Mon, 30 Dec 2024 14:08:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05840v4</guid></item><item><title>Towards Instance-Wise Calibration: Local Amortized Diagnostics and Reshaping of Conditional Densities (LADaR)</title><link>http://arxiv.org/abs/2205.14568v6</link><description>There is a growing interest in conditional density estimation and generativemodeling of a target $y$ given complex inputs $\mathbf{x}$. However,off-the-shelf methods often lack instance-wise calibration -- that is, forindividual inputs $\mathbf{x}$, the individual estimated probabilities can bevery different from the true probabilities, even when the estimates arereasonable when averaged over the entire population. This paper introduces theLADaR (Local Amortized Diagnostics and Reshaping of Conditional Densities)framework and proposes an algorithm called $\texttt{Cal-PIT}$ that producesinterpretable local calibration diagnostics and includes a mechanism torecalibrate the initial model. Our $\texttt{Cal-PIT}$ algorithm learns a singlelocal probability-probability map from calibration data to assess and quantifywhere corrections are needed across the feature space. When necessary, itreshapes the initial distribution into an estimate with approximateinstance-wise calibration. We illustrate the LADaR framework by applying$\texttt{Cal-PIT}$ to synthetic examples, including probabilistic forecastingwith sequences of images as inputs, akin to predicting the wind speed oftropical cyclones from satellite imagery. Our main science application isconditional density estimation of galaxy distances given imaging data(so-called photometric redshift estimation). On a benchmark photometricredshift data challenge, $\texttt{Cal-PIT}$ achieves better conditional densityestimation (as measured by the conditional density estimation loss) than all 11other literature methods tested. This demonstrates its potential for meetingthe stringent photometric redshift requirements for next generation weakgravitational lensing analyses.</description><author>Biprateep Dey, David Zhao, Brett H. Andrews, Jeffrey A. Newman, Rafael Izbicki, Ann B. Lee</author><pubDate>Mon, 30 Dec 2024 14:00:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.14568v6</guid></item><item><title>Conservation-informed Graph Learning for Spatiotemporal Dynamics Prediction</title><link>http://arxiv.org/abs/2412.20962v1</link><description>Data-centric methods have shown great potential in understanding andpredicting spatiotemporal dynamics, enabling better design and control of theobject system. However, pure deep learning models often lack interpretability,fail to obey intrinsic physics, and struggle to cope with the various domains.While geometry-based methods, e.g., graph neural networks (GNNs), have beenproposed to further tackle these challenges, they still need to find theimplicit physical laws from large datasets and rely excessively on rich labeleddata. In this paper, we herein introduce the conservation-informed GNN (CiGNN),an end-to-end explainable learning framework, to learn spatiotemporal dynamicsbased on limited training data. The network is designed to conform to thegeneral conservation law via symmetry, where conservative and non-conservativeinformation passes over a multiscale space enhanced by a latent temporalmarching strategy. The efficacy of our model has been verified in variousspatiotemporal systems based on synthetic and real-world datasets, showingsuperiority over baseline models. Results demonstrate that CiGNN exhibitsremarkable accuracy and generalization ability, and is readily applicable tolearning for prediction of various spatiotemporal dynamics in a spatial domainwith complex geometry.</description><author>Yuan Mi, Pu Ren, Hongteng Xu, Hongsheng Liu, Zidong Wang, Yike Guo, Ji-Rong Wen, Hao Sun, Yang Liu</author><pubDate>Mon, 30 Dec 2024 13:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20962v1</guid></item><item><title>Rise of Generative Artificial Intelligence in Science</title><link>http://arxiv.org/abs/2412.20960v1</link><description>Generative Artificial Intelligence (GenAI, generative AI) has rapidly becomeavailable as a tool in scientific research. To explore the use of generative AIin science, we conduct an empirical analysis using OpenAlex. Analyzing GenAIpublications and other AI publications from 2017 to 2023, we profile growthpatterns, the diffusion of GenAI publications across fields of study, and thegeographical spread of scientific research on generative AI. We alsoinvestigate team size and international collaborations to explore whetherGenAI, as an emerging scientific research area, shows different collaborationpatterns compared to other AI technologies. The results indicate thatgenerative AI has experienced rapid growth and increasing presence inscientific publications. The use of GenAI now extends beyond computer scienceto other scientific research domains. Over the study period, U.S. researcherscontributed nearly two-fifths of global GenAI publications. The U.S. isfollowed by China, with several small and medium-sized advanced economiesdemonstrating relatively high levels of GenAI deployment in their researchpublications. Although scientific research overall is becoming increasinglyspecialized and collaborative, our results suggest that GenAI research groupstend to have slightly smaller team sizes than found in other AI fields.Furthermore, notwithstanding recent geopolitical tensions, GenAI researchcontinues to exhibit levels of international collaboration comparable to otherAI technologies.</description><author>Liangping Ding, Cornelia Lawson, Philip Shapira</author><pubDate>Mon, 30 Dec 2024 13:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20960v1</guid></item><item><title>Efficient Link Prediction via GNN Layers Induced by Negative Sampling</title><link>http://arxiv.org/abs/2310.09516v2</link><description>Graph neural networks (GNNs) for link prediction can loosely be divided intotwo broad categories. First, \emph{node-wise} architectures pre-computeindividual embeddings for each node that are later combined by a simple decoderto make predictions. While extremely efficient at inference time, modelexpressiveness is limited such that isomorphic nodes contributing to candidateedges may not be distinguishable, compromising accuracy. In contrast,\emph{edge-wise} methods rely on the formation of edge-specific subgraphembeddings to enrich the representation of pair-wise relationships,disambiguating isomorphic nodes to improve accuracy, but with increased modelcomplexity. To better navigate this trade-off, we propose a novel GNNarchitecture whereby the \emph{forward pass} explicitly depends on \emph{both}positive (as is typical) and negative (unique to our approach) edges to informmore flexible, yet still cheap node-wise embeddings. This is achieved byrecasting the embeddings themselves as minimizers of a forward-pass-specificenergy function that favors separation of positive and negative samples.Notably, this energy is distinct from the actual training loss shared by mostexisting link prediction models, where contrastive pairs only influence the\textit{backward pass}. As demonstrated by extensive empirical evaluations, theresulting architecture retains the inference speed of node-wise models, whileproducing competitive accuracy with edge-wise alternatives. We released ourcode at https://github.com/yxzwang/SubmissionverOfYinYanGNN.</description><author>Yuxin Wang, Xiannian Hu, Quan Gan, Xuanjing Huang, Xipeng Qiu, David Wipf</author><pubDate>Mon, 30 Dec 2024 13:53:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09516v2</guid></item><item><title>HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for Multi-View 3D Object Detection</title><link>http://arxiv.org/abs/2412.18884v2</link><description>The application of vision-based multi-view environmental perception systemhas been increasingly recognized in autonomous driving technology, especiallythe BEV-based models. Current state-of-the-art solutions primarily encode imagefeatures from each camera view into the BEV space through explicit or implicitdepth prediction. However, these methods often focus on improving the accuracyof projecting 2D features into corresponding depth regions, while overlookingthe highly structured information of real-world objects and the varying heightdistributions of objects across different scenes. In this work, we proposeHV-BEV, a novel approach that decouples feature sampling in the BEV gridqueries paradigm into horizontal feature aggregation and vertical adaptiveheight-aware reference point sampling, aiming to improve both the aggregationof objects' complete information and generalization to diverse roadenvironments. Specifically, we construct a learnable graph structure in thehorizontal plane aligned with the ground for 3D reference points, reinforcingthe association of the same instance across different BEV grids, especiallywhen the instance spans multiple image views around the vehicle. Additionally,instead of relying on uniform sampling within a fixed height range, weintroduce a height-aware module that incorporates historical information,enabling the reference points to adaptively focus on the varying heights atwhich objects appear in different scenes. Extensive experiments validate theeffectiveness of our proposed method, demonstrating its superior performanceover the baseline across the nuScenes dataset. Moreover, our best-performingmodel achieves a remarkable 50.5% mAP and 59.8% NDS on the nuScenes testingset.</description><author>Di Wu, Feng Yang, Benlian Xu, Pan Liao, Wenhui Zhao, Dingwen Zhang</author><pubDate>Mon, 30 Dec 2024 13:49:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18884v2</guid></item><item><title>GASLITEing the Retrieval: Exploring Vulnerabilities in Dense Embedding-based Search</title><link>http://arxiv.org/abs/2412.20953v1</link><description>Dense embedding-based text retrieval$\unicode{x2013}$retrieval of relevantpassages from corpora via deep learning encodings$\unicode{x2013}$has emergedas a powerful method attaining state-of-the-art search results and popularizingthe use of Retrieval Augmented Generation (RAG). Still, like other searchmethods, embedding-based retrieval may be susceptible to search-engineoptimization (SEO) attacks, where adversaries promote malicious content byintroducing adversarial passages to corpora. To faithfully assess and gaininsights into the susceptibility of such systems to SEO, this work proposes theGASLITE attack, a mathematically principled gradient-based search method forgenerating adversarial passages without relying on the corpus content ormodifying the model. Notably, GASLITE's passages (1) carry adversary-choseninformation while (2) achieving high retrieval ranking for a selected querydistribution when inserted to corpora. We use GASLITE to extensively evaluateretrievers' robustness, testing nine advanced models under varied threatmodels, while focusing on realistic adversaries targeting queries on a specificconcept (e.g., a public figure). We found GASLITE consistently outperformedbaselines by $\geq$140% success rate, in all settings. Particularly,adversaries using GASLITE require minimal effort to manipulate searchresults$\unicode{x2013}$by injecting a negligible amount of adversarialpassages ($\leq$0.0001% of the corpus), they could make them visible in thetop-10 results for 61-100% of unseen concept-specific queries against mostevaluated models. Inspecting variance in retrievers' robustness, we identifykey factors that may contribute to models' susceptibility to SEO, includingspecific properties in the embedding space's geometry.</description><author>Matan Ben-Tov, Mahmood Sharif</author><pubDate>Mon, 30 Dec 2024 13:49:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.20953v1</guid></item><item><title>Finding the Right Moment: Human-Assisted Trailer Creation via Task Composition</title><link>http://arxiv.org/abs/2111.08774v2</link><description>Movie trailers perform multiple functions: they introduce viewers to thestory, convey the mood and artistic style of the film, and encourage audiencesto see the movie. These diverse functions make trailer creation a challengingendeavor. In this work, we focus on finding trailer moments in a movie, i.e.,shots that could be potentially included in a trailer. We decompose this taskinto two subtasks: narrative structure identification and sentiment prediction.We model movies as graphs, where nodes are shots and edges denote semanticrelations between them. We learn these relations using joint contrastivetraining which distills rich textual information (e.g., characters, actions,situations) from screenplays. An unsupervised algorithm then traverses thegraph and selects trailer moments from the movie that human judges prefer toones selected by competitive supervised approaches. A main advantage of ouralgorithm is that it uses interpretable criteria, which allows us to deploy itin an interactive tool for trailer creation with a human in the loop. Our toolallows users to select trailer shots in under 30 minutes that are superior tofully automatic methods and comparable to (exclusive) manual selection byexperts.</description><author>Pinelopi Papalampidi, Frank Keller, Mirella Lapata</author><pubDate>Mon, 30 Dec 2024 13:43:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.08774v2</guid></item><item><title>Nash CoT: Multi-Path Inference with Preference Equilibrium</title><link>http://arxiv.org/abs/2407.07099v3</link><description>Chain of thought (CoT) is a reasoning framework that can enhance theperformance of Large Language Models (LLMs) on complex inference tasks. Inparticular, among various studies related to CoT, multi-path inference standsout as a simple yet effective improvement. However, there is no optimal settingfor the number of inference paths. Therefore, we have to increase the number ofinference paths to obtain better results, which in turn increases the inferencecost. To address this limitation, we can utilize question-related roletemplates to guide LLMs into relevant roles, thereby increasing the possibilityof correct inferences for each path and further reducing dependence on thenumber of inference paths while improving reasoning accuracy. However, placingLLMs into specific roles may reduce their reasoning diversity and performanceon a few tasks where role dependence is low. To alleviate the excessiveimmersion of the LLM into a specific role, we propose Nash CoT by constructinga game system on each path that balances the generation from role-specificLLMs' and the general LLMs' generation, thereby ensuring both effective roleadoption and diversity in LLM generation further maintaining the performance ofmulti-path inference while reducing the requirement of the number of inferencepaths. We evaluate Nash CoT across various inference tasks, including ArabicReasoning, Commonsense Question Answering, and Symbolic Inference, achievingresults that are comparable to or better than those of multi-path CoT with theequal number of inference paths.</description><author>Ziqi Zhang, Cunxiang Wang, Xiong Xiao, Yue Zhang, Donglin Wang</author><pubDate>Mon, 30 Dec 2024 13:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07099v3</guid></item><item><title>Online Joint Assortment-Inventory Optimization under MNL Choices</title><link>http://arxiv.org/abs/2304.02022v2</link><description>We study an online joint assortment-inventory optimization problem, in whichwe assume that the choice behavior of each customer follows the MultinomialLogit (MNL) choice model, and the attraction parameters are unknown a priori.The retailer makes periodic assortment and inventory decisions to dynamicallylearn from the customer choice observations about the attraction parameterswhile maximizing the expected total profit over time. In this paper, we proposea novel algorithm that can effectively balance exploration and exploitation inthe online decision-making of assortment and inventory. Our algorithm builds ona new estimator for the MNL attraction parameters, an innovative approach toincentivize exploration by adaptively tuning certain known and unknownparameters, and an optimization oracle to static single-cycleassortment-inventory planning problems with given parameters. We establish aregret upper bound for our algorithm and a lower bound for the online jointassortment-inventory optimization problem, suggesting that our algorithmachieves nearly optimal regret rate, provided that the static optimizationoracle is exact. Then we incorporate more practical approximate staticoptimization oracles into our algorithm, and bound from above the impact ofstatic optimization errors on the regret of our algorithm. We perform numericalstudies to demonstrate the effectiveness of our proposed algorithm.At last, weextend our study by incorporating inventory carryover and the learning ofcustomer arrival distribution.</description><author>Yong Liang, Xiaojie Mao, Shiyuan Wang</author><pubDate>Mon, 30 Dec 2024 13:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02022v2</guid></item></channel></rss>