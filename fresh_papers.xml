<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 05 Dec 2023 06:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness</title><link>http://arxiv.org/abs/2312.02158v1</link><description>We propose the task of Panoptic Scene Completion (PSC) which extends therecently popular Semantic Scene Completion (SSC) task with instance-levelinformation to produce a richer understanding of the 3D scene. Our PSC proposalutilizes a hybrid mask-based technique on the non-empty voxels from sparsemulti-scale completions. Whereas the SSC literature overlooks uncertainty whichis critical for robotics applications, we instead propose an efficientensembling to estimate both voxel-wise and instance-wise uncertainties alongPSC. This is achieved by building on a multi-input multi-output (MIMO)strategy, while improving performance and yielding better uncertainty forlittle additional compute. Additionally, we introduce a technique to aggregatepermutation-invariant mask predictions. Our experiments demonstrate that ourmethod surpasses all baselines in both Panoptic Scene Completion anduncertainty estimation on three large-scale autonomous driving datasets. Ourcode and data are available at https://astra-vision.github.io/PaSCo .</description><author>Anh-Quan Cao, Angela Dai, Raoul de Charette</author><pubDate>Mon, 04 Dec 2023 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02158v1</guid></item><item><title>Mesh-Guided Neural Implicit Field Editing</title><link>http://arxiv.org/abs/2312.02157v1</link><description>Neural implicit fields have emerged as a powerful 3D representation forreconstructing and rendering photo-realistic views, yet they possess limitededitability. Conversely, explicit 3D representations, such as polygonal meshes,offer ease of editing but may not be as suitable for rendering high-qualitynovel views. To harness the strengths of both representations, we propose a newapproach that employs a mesh as a guiding mechanism in editing the neuralradiance field. We first introduce a differentiable method using marchingtetrahedra for polygonal mesh extraction from the neural implicit field andthen design a differentiable color extractor to assign colors obtained from thevolume renderings to this extracted mesh. This differentiable colored meshallows gradient back-propagation from the explicit mesh to the implicit fields,empowering users to easily manipulate the geometry and color of neural implicitfields. To enhance user control from coarse-grained to fine-grained levels, weintroduce an octree-based structure into its optimization. This structureprioritizes the edited regions and the surface part, making our method achievefine-grained edits to the neural implicit field and accommodate various usermodifications, including object additions, component removals, specific areadeformations, and adjustments to local and global colors. Through extensiveexperiments involving diverse scenes and editing operations, we havedemonstrated the capabilities and effectiveness of our method. Our project pageis: \url{https://cassiepython.github.io/MNeuEdit/}</description><author>Can Wang, Mingming He, Menglei Chai, Dongdong Chen, Jing Liao</author><pubDate>Mon, 04 Dec 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02157v1</guid></item><item><title>Latent Feature-Guided Diffusion Models for Shadow Removal</title><link>http://arxiv.org/abs/2312.02156v1</link><description>Recovering textures under shadows has remained a challenging problem due tothe difficulty of inferring shadow-free scenes from shadow images. In thispaper, we propose the use of diffusion models as they offer a promisingapproach to gradually refine the details of shadow regions during the diffusionprocess. Our method improves this process by conditioning on a learned latentfeature space that inherits the characteristics of shadow-free images, thusavoiding the limitation of conventional methods that condition on degradedimages only. Additionally, we propose to alleviate potential local optimaduring training by fusing noise features with the diffusion network. Wedemonstrate the effectiveness of our approach which outperforms the previousbest method by 13% in terms of RMSE on the AISTD dataset. Further, we exploreinstance-level shadow removal, where our model outperforms the previous bestmethod by 82% in terms of RMSE on the DESOBA dataset.</description><author>Kangfu Mei, Luis Figueroa, Zhe Lin, Zhihong Ding, Scott Cohen, Vishal M. Patel</author><pubDate>Mon, 04 Dec 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02156v1</guid></item><item><title>GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis</title><link>http://arxiv.org/abs/2312.02155v1</link><description>We present a new approach, termed GPS-Gaussian, for synthesizing novel viewsof a character in a real-time manner. The proposed method enables 2K-resolutionrendering under a sparse-view camera setting. Unlike the original GaussianSplatting or neural implicit rendering methods that necessitate per-subjectoptimizations, we introduce Gaussian parameter maps defined on the source viewsand regress directly Gaussian Splatting properties for instant novel viewsynthesis without any fine-tuning or optimization. To this end, we train ourGaussian parameter regression module on a large amount of human scan data,jointly with a depth estimation module to lift 2D parameter maps to 3D space.The proposed framework is fully differentiable and experiments on severaldatasets demonstrate that our method outperforms state-of-the-art methods whileachieving an exceeding rendering speed.</description><author>Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu</author><pubDate>Mon, 04 Dec 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02155v1</guid></item><item><title>Aligning and Prompting Everything All at Once for Universal Visual Perception</title><link>http://arxiv.org/abs/2312.02153v1</link><description>Vision foundation models have been explored recently to build general-purposevision systems. However, predominant paradigms, driven by castinginstance-level tasks as an object-word alignment, bring heavy cross-modalityinteraction, which is not effective in prompting object detection and visualgrounding. Another line of work that focuses on pixel-level tasks oftenencounters a large annotation gap of things and stuff, and suffers from mutualinterference between foreground-object and background-class segmentation. Instark contrast to the prevailing methods, we present APE, a universal visualperception model for aligning and prompting everything all at once in an imageto perform diverse tasks, i.e., detection, segmentation, and grounding, as aninstance-level sentence-object matching paradigm. Specifically, APE advancesthe convergence of detection and grounding by reformulating language-guidedgrounding as open-vocabulary detection, which efficiently scales up modelprompting to thousands of category vocabularies and region descriptions whilemaintaining the effectiveness of cross-modality fusion. To bridge thegranularity gap of different pixel-level tasks, APE equalizes semantic andpanoptic segmentation to proxy instance learning by considering any isolatedregions as individual instances. APE aligns vision and language representationon broad data with natural and challenging characteristics all at once withouttask-specific fine-tuning. The extensive experiments on over 160 datasetsdemonstrate that, with only one-suit of weights, APE outperforms (or is on parwith) the state-of-the-art models, proving that an effective yet universalperception for anything aligning and prompting is indeed feasible. Codes andtrained models are released at https://github.com/shenyunhang/APE.</description><author>Yunhang Shen, Chaoyou Fu, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu, Shaohui Lin, Rongrong Ji</author><pubDate>Mon, 04 Dec 2023 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02153v1</guid></item><item><title>Steerers: A framework for rotation equivariant keypoint descriptors</title><link>http://arxiv.org/abs/2312.02152v1</link><description>Image keypoint descriptions that are discriminative and matchable over largechanges in viewpoint are vital for 3D reconstruction. However, descriptionsoutput by learned descriptors are typically not robust to camera rotation.While they can be made more robust by, e.g., data augmentation, this degradesperformance on upright images. Another approach is test-time augmentation,which incurs a significant increase in runtime. We instead learn a lineartransform in description space that encodes rotations of the input image. Wecall this linear transform a steerer since it allows us to transform thedescriptions as if the image was rotated. From representation theory we knowall possible steerers for the rotation group. Steerers can be optimized (A)given a fixed descriptor, (B) jointly with a descriptor or (C) we can optimizea descriptor given a fixed steerer. We perform experiments in all of thesethree settings and obtain state-of-the-art results on the rotation invariantimage matching benchmarks AIMS and Roto-360. We publish code and model weightsat github.com/georg-bn/rotation-steerers.</description><author>Georg BÃ¶kman, Johan Edstedt, Michael Felsberg, Fredrik Kahl</author><pubDate>Mon, 04 Dec 2023 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02152v1</guid></item><item><title>Guarding Barlow Twins Against Overfitting with Mixed Samples</title><link>http://arxiv.org/abs/2312.02151v1</link><description>Self-supervised Learning (SSL) aims to learn transferable featurerepresentations for downstream applications without relying on labeled data.The Barlow Twins algorithm, renowned for its widespread adoption andstraightforward implementation compared to its counterparts like contrastivelearning methods, minimizes feature redundancy while maximizing invariance tocommon corruptions. Optimizing for the above objective forces the network tolearn useful representations, while avoiding noisy or constant features,resulting in improved downstream task performance with limited adaptation.Despite Barlow Twins' proven effectiveness in pre-training, the underlying SSLobjective can inadvertently cause feature overfitting due to the lack of stronginteraction between the samples unlike the contrastive learning approaches.From our experiments, we observe that optimizing for the Barlow Twins objectivedoesn't necessarily guarantee sustained improvements in representation qualitybeyond a certain pre-training phase, and can potentially degrade downstreamperformance on some datasets. To address this challenge, we introduce MixedBarlow Twins, which aims to improve sample interaction during Barlow Twinstraining via linearly interpolated samples. This results in an additionalregularization term to the original Barlow Twins objective, assuming linearinterpolation in the input space translates to linearly interpolated featuresin the feature space. Pre-training with this regularization effectivelymitigates feature overfitting and further enhances the downstream performanceon CIFAR-10, CIFAR-100, TinyImageNet, STL-10, and ImageNet datasets. The codeand checkpoints are available at: https://github.com/wgcban/mix-bt.git</description><author>Wele Gedara Chaminda Bandara, Celso M. De Melo, Vishal M. Patel</author><pubDate>Mon, 04 Dec 2023 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02151v1</guid></item><item><title>Readout Guidance: Learning Control from Diffusion Features</title><link>http://arxiv.org/abs/2312.02150v1</link><description>We present Readout Guidance, a method for controlling text-to-image diffusionmodels with learned signals. Readout Guidance uses readout heads, lightweightnetworks trained to extract signals from the features of a pre-trained, frozendiffusion model at every timestep. These readouts can encode single-imageproperties, such as pose, depth, and edges; or higher-order properties thatrelate multiple images, such as correspondence and appearance similarity.Furthermore, by comparing the readout estimates to a user-defined target, andback-propagating the gradient through the readout head, these estimates can beused to guide the sampling process. Compared to prior methods for conditionalgeneration, Readout Guidance requires significantly fewer added parameters andtraining samples, and offers a convenient and simple recipe for reproducingdifferent forms of conditional control under a single framework, with a singlearchitecture and sampling procedure. We showcase these benefits in theapplications of drag-based manipulation, identity-consistent generation, andspatially aligned control. Project page: https://readout-guidance.github.io.</description><author>Grace Luo, Trevor Darrell, Oliver Wang, Dan B Goldman, Aleksander Holynski</author><pubDate>Mon, 04 Dec 2023 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02150v1</guid></item><item><title>Generative Powers of Ten</title><link>http://arxiv.org/abs/2312.02149v1</link><description>We present a method that uses a text-to-image model to generate consistentcontent across multiple image scales, enabling extreme semantic zooms into ascene, e.g., ranging from a wide-angle landscape view of a forest to a macroshot of an insect sitting on one of the tree branches. We achieve this througha joint multi-scale diffusion sampling approach that encourages consistencyacross different scales while preserving the integrity of each individualsampling process. Since each generated scale is guided by a different textprompt, our method enables deeper levels of zoom than traditionalsuper-resolution methods that may struggle to create new contextual structureat vastly different scales. We compare our method qualitatively withalternative techniques in image super-resolution and outpainting, and show thatour method is most effective at generating consistent multi-scale content.</description><author>Xiaojuan Wang, Janne Kontkanen, Brian Curless, Steve Seitz, Ira Kemelmacher, Ben Mildenhall, Pratul Srinivasan, Dor Verbin, Aleksander Holynski</author><pubDate>Mon, 04 Dec 2023 18:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02149v1</guid></item><item><title>Rejuvenating image-GPT as Strong Visual Representation Learners</title><link>http://arxiv.org/abs/2312.02147v1</link><description>This paper enhances image-GPT (iGPT), one of the pioneering works thatintroduce autoregressive pretraining to predict next pixels for visualrepresentation learning. Two simple yet essential changes are made. First, weshift the prediction target from raw pixels to semantic tokens, enabling ahigher-level understanding of visual content. Second, we supplement theautoregressive modeling by instructing the model to predict not only the nexttokens but also the visible tokens. This pipeline is particularly effectivewhen semantic tokens are encoded by discriminatively trained models, such asCLIP. We introduce this novel approach as D-iGPT. Extensive experimentsshowcase that D-iGPT excels as a strong learner of visual representations: Anotable achievement of D-iGPT is its compelling performance on the ImageNet-1Kdataset -- by training on publicly available datasets, D-iGPT achieves 89.5\%top-1 accuracy with a vanilla ViT-Large model. This model also shows stronggeneralization on the downstream task and robustness on out-of-distributionsamples. Code is avaiable at\href{https://github.com/OliverRensu/D-iGPT}{https://github.com/OliverRensu/D-iGPT}.</description><author>Sucheng Ren, Zeyu Wang, Hongru Zhu, Junfei Xiao, Alan Yuille, Cihang Xie</author><pubDate>Mon, 04 Dec 2023 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02147v1</guid></item><item><title>Learning Polynomial Problems with $SL(2,\mathbb{R})$ Equivariance</title><link>http://arxiv.org/abs/2312.02146v1</link><description>Optimizing and certifying the positivity of polynomials are fundamentalprimitives across mathematics and engineering applications, from dynamicalsystems to operations research. However, solving these problems in practicerequires large semidefinite programs, with poor scaling in dimension anddegree. In this work, we demonstrate for the first time that neural networkscan effectively solve such problems in a data-driven fashion, achieving tenfoldspeedups while retaining high accuracy. Moreover, we observe that thesepolynomial learning problems are equivariant to the non-compact group$SL(2,\mathbb{R})$, which consists of area-preserving linear transformations.We therefore adapt our learning pipelines to accommodate this structure,including data augmentation, a new $SL(2,\mathbb{R})$-equivariant architecture,and an architecture equivariant with respect to its maximal compact subgroup,$SO(2, \mathbb{R})$. Surprisingly, the most successful approaches in practicedo not enforce equivariance to the entire group, which we prove arises from anunusual lack of architecture universality for $SL(2,\mathbb{R})$ in particular.A consequence of this result, which is of independent interest, is that thereexists an equivariant function for which there is no sequence of equivariantpolynomials multiplied by arbitrary invariants that approximates the originalfunction. This is a rare example of a symmetric problem where data augmentationoutperforms a fully equivariant architecture, and provides interesting lessonsin both theory and practice for other problems with non-compact symmetries.</description><author>Hannah Lawrence, Mitchell Tong Harris</author><pubDate>Mon, 04 Dec 2023 18:59:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02146v1</guid></item><item><title>Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation</title><link>http://arxiv.org/abs/2312.02145v1</link><description>Monocular depth estimation is a fundamental computer vision task. Recovering3D depth from a single image is geometrically ill-posed and requires sceneunderstanding, so it is not surprising that the rise of deep learning has ledto a breakthrough. The impressive progress of monocular depth estimators hasmirrored the growth in model capacity, from relatively modest CNNs to largeTransformer architectures. Still, monocular depth estimators tend to strugglewhen presented with images with unfamiliar content and layout, since theirknowledge of the visual world is restricted by the data seen during training,and challenged by zero-shot generalization to new domains. This motivates us toexplore whether the extensive priors captured in recent generative diffusionmodels can enable better, more generalizable depth estimation. We introduceMarigold, a method for affine-invariant monocular depth estimation that isderived from Stable Diffusion and retains its rich prior knowledge. Theestimator can be fine-tuned in a couple of days on a single GPU using onlysynthetic training data. It delivers state-of-the-art performance across a widerange of datasets, including over 20% performance gains in specific cases.Project page: https://marigoldmonodepth.github.io.</description><author>Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, Konrad Schindler</author><pubDate>Mon, 04 Dec 2023 18:59:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02145v1</guid></item><item><title>Optimizing Camera Configurations for Multi-View Pedestrian Detection</title><link>http://arxiv.org/abs/2312.02144v1</link><description>Jointly considering multiple camera views (multi-view) is very effective forpedestrian detection under occlusion. For such multi-view systems, it iscritical to have well-designed camera configurations, including cameralocations, directions, and fields-of-view (FoVs). Usually, these configurationsare crafted based on human experience or heuristics. In this work, we present anovel solution that features a transformer-based camera configurationgenerator. Using reinforcement learning, this generator autonomously exploresvast combinations within the action space and searches for configurations thatgive the highest detection accuracy according to the training dataset. Thegenerator learns advanced techniques like maximizing coverage, minimizingocclusion, and promoting collaboration. Across multiple simulation scenarios,the configurations generated by our transformer-based model consistentlyoutperform random search, heuristic-based methods, and configurations designedby human experts, shedding light on future camera layout optimization.</description><author>Yunzhong Hou, Xingjian Leng, Tom Gedeon, Liang Zheng</author><pubDate>Mon, 04 Dec 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02144v1</guid></item><item><title>Competition-Level Problems Are Effective Evaluators of LLMs</title><link>http://arxiv.org/abs/2312.02143v1</link><description>Large language models (LLMs) have demonstrated impressive reasoningcapabilities, yet there is ongoing debate about these abilities and thepotential data contamination problem recently. This paper aims to evaluate thereasoning capacities of LLMs, specifically in solving recent competition-levelprogramming problems in Codeforces, which are expert-crafted and unique,requiring deep understanding and robust reasoning skills. We first provide acomprehensive evaluation of GPT-4's peiceived zero-shot performance on thistask, considering various aspects such as problems' release time, difficulties,and types of errors encountered. Surprisingly, the peiceived performance ofGPT-4 has experienced a cliff like decline in problems after September 2021consistently across all the difficulties and types of problems, which shows thepotential data contamination, as well as the challenges for any existing LLM tosolve unseen complex reasoning problems. We further explore various approachessuch as fine-tuning, Chain-of-Thought prompting and problem descriptionsimplification, unfortunately none of them is able to consistently mitigate thechallenges. Through our work, we emphasis the importance of this excellent datasource for assessing the genuine reasoning capabilities of LLMs, and foster thedevelopment of LLMs with stronger reasoning abilities and better generalizationin the future.</description><author>Yiming Huang, Zhenghao Lin, Xiao Liu, Yeyun Gong, Shuai Lu, Fangyu Lei, Yaobo Liang, Yelong Shen, Chen Lin, Nan Duan, Weizhu Chen</author><pubDate>Mon, 04 Dec 2023 18:58:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02143v1</guid></item><item><title>Object Recognition as Next Token Prediction</title><link>http://arxiv.org/abs/2312.02142v1</link><description>We present an approach to pose object recognition as next token prediction.The idea is to apply a language decoder that auto-regressively predicts thetext tokens from image embeddings to form labels. To ground this predictionprocess in auto-regression, we customize a non-causal attention mask for thedecoder, incorporating two key features: modeling tokens from different labelsto be independent, and treating image tokens as a prefix. This maskingmechanism inspires an efficient method - one-shot sampling - to simultaneouslysample tokens of multiple labels in parallel and rank generated labels by theirprobabilities during inference. To further enhance the efficiency, we propose asimple strategy to construct a compact decoder by simply discarding theintermediate blocks of a pretrained language model. This approach yields adecoder that matches the full model's performance while being notably moreefficient. The code is available at https://github.com/kaiyuyue/nxtp</description><author>Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein, Ser-Nam Lim</author><pubDate>Mon, 04 Dec 2023 18:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02142v1</guid></item><item><title>iMatching: Imperative Correspondence Learning</title><link>http://arxiv.org/abs/2312.02141v1</link><description>Learning feature correspondence is a foundational task in computer vision,holding immense importance for downstream applications such as visual odometryand 3D reconstruction. Despite recent progress in data-driven models, featurecorrespondence learning is still limited by the lack of accurate per-pixelcorrespondence labels. To overcome this difficulty, we introduce a newself-supervised scheme, imperative learning (IL), for training featurecorrespondence. It enables correspondence learning on arbitrary uninterruptedvideos without any camera pose or depth labels, heralding a new era forself-supervised correspondence learning. Specifically, we formulated theproblem of correspondence learning as a bilevel optimization, which takes thereprojection error from bundle adjustment as a supervisory signal for themodel. To avoid large memory and computation overhead, we leverage thestationary point to effectively back-propagate the implicit gradients throughbundle adjustment. Through extensive experiments, we demonstrate superiorperformance on tasks including feature matching and pose estimation, in whichwe obtained an average of 30% accuracy gain over the state-of-the-art matchingmodels.</description><author>Zitong Zhan, Dasong Gao, Yun-Jou Lin, Youjie Xia, Chen Wang</author><pubDate>Mon, 04 Dec 2023 18:58:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02141v1</guid></item><item><title>DiffiT: Diffusion Vision Transformers for Image Generation</title><link>http://arxiv.org/abs/2312.02139v1</link><description>Diffusion models with their powerful expressivity and high sample qualityhave enabled many new applications and use-cases in various domains. For samplegeneration, these models rely on a denoising neural network that generatesimages by iterative denoising. Yet, the role of denoising network architectureis not well-studied with most efforts relying on convolutional residual U-Nets.In this paper, we study the effectiveness of vision transformers indiffusion-based generative learning. Specifically, we propose a new model,denoted as Diffusion Vision Transformers (DiffiT), which consists of a hybridhierarchical architecture with a U-shaped encoder and decoder. We introduce anovel time-dependent self-attention module that allows attention layers toadapt their behavior at different stages of the denoising process in anefficient manner. We also introduce latent DiffiT which consists of transformermodel with the proposed self-attention layers, for high-resolution imagegeneration. Our results show that DiffiT is surprisingly effective ingenerating high-fidelity images, and it achieves state-of-the-art (SOTA)benchmarks on a variety of class-conditional and unconditional synthesis tasks.In the latent space, DiffiT achieves a new SOTA FID score of 1.73 onImageNet-256 dataset. Repository: https://github.com/NVlabs/DiffiT</description><author>Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, Arash Vahdat</author><pubDate>Mon, 04 Dec 2023 18:57:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02139v1</guid></item><item><title>MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians</title><link>http://arxiv.org/abs/2312.02137v1</link><description>Understanding how we grasp objects with our hands has important applicationsin areas like robotics and mixed reality. However, this challenging problemrequires accurate modeling of the contact between hands and objects. To capturegrasps, existing methods use skeletons, meshes, or parametric models that cancause misalignments resulting in inaccurate contacts. We present MANUS, amethod for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians.We build a novel articulated 3D Gaussians representation that extends 3DGaussian splatting for high-fidelity representation of articulating hands.Since our representation uses Gaussian primitives, it enables us to efficientlyand accurately estimate contacts between the hand and the object. For the mostaccurate results, our method requires tens of camera views that currentdatasets do not provide. We therefore build MANUS-Grasps, a new dataset thatcontains hand-object grasps viewed from 53 cameras across 30+ scenes, 3subjects, and comprising over 7M frames. In addition to extensive qualitativeresults, we also show that our method outperforms others on a quantitativecontact evaluation method that uses paint transfer from the object to the hand.</description><author>Chandradeep Pokhariya, Ishaan N Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar</author><pubDate>Mon, 04 Dec 2023 18:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02137v1</guid></item><item><title>BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation</title><link>http://arxiv.org/abs/2312.02136v1</link><description>Generating large-scale 3D scenes cannot simply apply existing 3D objectsynthesis technique since 3D scenes usually hold complex spatial configurationsand consist of a number of objects at varying scales. We thus propose apractical and efficient 3D representation that incorporates an equivariantradiance field with the guidance of a bird's-eye view (BEV) map. Concretely,objects of synthesized 3D scenes could be easily manipulated through steeringthe corresponding BEV maps. Moreover, by adequately incorporating positionalencoding and low-pass filters into the generator, the representation becomesequivariant to the given BEV map. Such equivariance allows us to producelarge-scale, even infinite-scale, 3D scenes via synthesizing local scenes andthen stitching them with smooth consistency. Extensive experiments on 3D scenedatasets demonstrate the effectiveness of our approach. Our project website isat https://zqh0253.github.io/BerfScene/.</description><author>Qihang Zhang, Yinghao Xu, Yujun Shen, Bo Dai, Bolei Zhou, Ceyuan Yang</author><pubDate>Mon, 04 Dec 2023 18:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02136v1</guid></item><item><title>Fast View Synthesis of Casual Videos</title><link>http://arxiv.org/abs/2312.02135v1</link><description>Novel view synthesis from an in-the-wild video is difficult due to challengeslike scene dynamics and lack of parallax. While existing methods have shownpromising results with implicit neural radiance fields, they are slow to trainand render. This paper revisits explicit video representations to synthesizehigh-quality novel views from a monocular video efficiently. We treat staticand dynamic video content separately. Specifically, we build a global staticscene model using an extended plane-based scene representation to synthesizetemporally coherent novel video. Our plane-based scene representation isaugmented with spherical harmonics and displacement maps to captureview-dependent effects and model non-planar complex surface geometry. We opt torepresent the dynamic content as per-frame point clouds for efficiency. Whilesuch representations are inconsistency-prone, minor temporal inconsistenciesare perceptually masked due to motion. We develop a method to quickly estimatesuch a hybrid video representation and render novel views in real time. Ourexperiments show that our method can render high-quality novel views from anin-the-wild video with comparable quality to state-of-the-art methods whilebeing 100x faster in training and enabling real-time rendering.</description><author>Yao-Chih Lee, Zhoutong Zhang, Kevin Blackburn-Matzen, Simon Niklaus, Jianming Zhang, Jia-Bin Huang, Feng Liu</author><pubDate>Mon, 04 Dec 2023 18:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02135v1</guid></item><item><title>GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</title><link>http://arxiv.org/abs/2312.02134v1</link><description>We present GaussianAvatar, an efficient approach to creating realistic humanavatars with dynamic 3D appearances from a single video. We start byintroducing animatable 3D Gaussians to explicitly represent humans in variousposes and clothing styles. Such an explicit and animatable representation canfuse 3D appearances more efficiently and consistently from 2D observations. Ourrepresentation is further augmented with dynamic properties to supportpose-dependent appearance modeling, where a dynamic appearance network alongwith an optimizable feature tensor is designed to learn themotion-to-appearance mapping. Moreover, by leveraging the differentiable motioncondition, our method enables a joint optimization of motions and appearancesduring avatar modeling, which helps to tackle the long-standing issue ofinaccurate motion estimation in monocular settings. The efficacy ofGaussianAvatar is validated on both the public dataset and our collecteddataset, demonstrating its superior performances in terms of appearance qualityand rendering efficiency.</description><author>Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, Liqiang Nie</author><pubDate>Mon, 04 Dec 2023 18:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02134v1</guid></item><item><title>Style Aligned Image Generation via Shared Attention</title><link>http://arxiv.org/abs/2312.02133v1</link><description>Large-scale Text-to-Image (T2I) models have rapidly gained prominence acrosscreative fields, generating visually compelling outputs from textual prompts.However, controlling these models to ensure consistent style remainschallenging, with existing methods necessitating fine-tuning and manualintervention to disentangle content and style. In this paper, we introduceStyleAligned, a novel technique designed to establish style alignment among aseries of generated images. By employing minimal `attention sharing' during thediffusion process, our method maintains style consistency across images withinT2I models. This approach allows for the creation of style-consistent imagesusing a reference style through a straightforward inversion operation. Ourmethod's evaluation across diverse styles and text prompts demonstrateshigh-quality synthesis and fidelity, underscoring its efficacy in achievingconsistent style across various inputs.</description><author>Amir Hertz, Andrey Voynov, Shlomi Fruchter, Daniel Cohen-Or</author><pubDate>Mon, 04 Dec 2023 18:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02133v1</guid></item><item><title>Hot PATE: Private Aggregation of Distributions for Diverse Task</title><link>http://arxiv.org/abs/2312.02132v1</link><description>The Private Aggregation of Teacher Ensembles (PATE)framework~\cite{PapernotAEGT:ICLR2017} is a versatile approach toprivacy-preserving machine learning. In PATE, teacher models are trained ondistinct portions of sensitive data, and their predictions are privatelyaggregated to label new training examples for a student model. Until now, PATE has primarily been explored with classification-like tasks,where each example possesses a ground-truth label, and knowledge is transferredto the student by labeling public examples. Generative AI models, however,excel in open ended \emph{diverse} tasks with multiple valid responses andscenarios that may not align with traditional labeled examples. Furthermore,the knowledge of models is often encapsulated in the response distributionitself and may be transferred from teachers to student in a more fluid way. Wepropose \emph{hot PATE}, tailored for the diverse setting. In hot PATE, eachteacher model produces a response distribution and the aggregation method mustpreserve both privacy and diversity of responses. We demonstrate, analyticallyand empirically, that hot PATE achieves privacy-utility tradeoffs that arecomparable to, and in diverse settings, significantly surpass, the baseline``cold'' PATE.</description><author>Edith Cohen, Xin Lyu, Jelani Nelson, Tamas Sarlos, Uri Stemmer</author><pubDate>Mon, 04 Dec 2023 18:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02132v1</guid></item><item><title>Can we truly transfer an actor's genuine happiness to avatars? An investigation into virtual, real, posed and spontaneous faces</title><link>http://arxiv.org/abs/2312.02128v1</link><description>A look is worth a thousand words is a popular phrase. And why is a simplelook enough to portray our feelings about something or someone? Behind thisquestion are the theoretical foundations of the field of psychology regardingsocial cognition and the studies of psychologist Paul Ekman. Facialexpressions, as a form of non-verbal communication, are the primary way totransmit emotions between human beings. The set of movements and expressions offacial muscles that convey some emotional state of the individual to theirobservers are targets of studies in many areas. Our research aims to evaluateEkman's action units in datasets of real human faces, posed and spontaneous,and virtual human faces resulting from transferring real faces into ComputerGraphics faces. In addition, we also conducted a case study with specific moviecharacters, such as SheHulk and Genius. We intend to find differences andsimilarities in facial expressions between real and CG datasets, posed andspontaneous faces, and also to consider the actors' genders in the videos. Thisinvestigation can help several areas of knowledge, whether using real orvirtual human beings, in education, health, entertainment, games, security, andeven legal matters. Our results indicate that AU intensities are greater forposed than spontaneous datasets, regardless of gender. Furthermore, there is asmoothing of intensity up to 80 percent for AU6 and 45 percent for AU12 when areal face is transformed into CG.</description><author>Vitor Miguel Xavier Peres, Greice Pinho Dal Molin, Soraia Raupp Musse</author><pubDate>Mon, 04 Dec 2023 18:53:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02128v1</guid></item><item><title>SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM</title><link>http://arxiv.org/abs/2312.02126v1</link><description>Dense simultaneous localization and mapping (SLAM) is pivotal for embodiedscene understanding. Recent work has shown that 3D Gaussians enablehigh-quality reconstruction and real-time rendering of scenes using multipleposed cameras. In this light, we show for the first time that representing ascene by 3D Gaussians can enable dense SLAM using a single unposed monocularRGB-D camera. Our method, SplaTAM, addresses the limitations of prior radiancefield-based representations, including fast rendering and optimization, theability to determine if areas have been previously mapped, and structured mapexpansion by adding more Gaussians. We employ an online tracking and mappingpipeline while tailoring it to specifically use an underlying Gaussianrepresentation and silhouette-guided optimization via differentiable rendering.Extensive experiments show that SplaTAM achieves up to 2X state-of-the-artperformance in camera pose estimation, map construction, and novel-viewsynthesis, demonstrating its superiority over existing approaches, whileallowing real-time rendering of a high-resolution dense 3D map.</description><author>Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, Jonathon Luiten</author><pubDate>Mon, 04 Dec 2023 18:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02126v1</guid></item><item><title>TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques</title><link>http://arxiv.org/abs/2312.02125v1</link><description>Recent advances in language models (LMs), have demonstrated significantefficacy in tasks related to the arts and humanities. While LMs have exhibitedexceptional performance across a wide range of natural language processingtasks, there are notable challenges associated with their utilization on smalldatasets and their ability to replicate more creative human capacities. In thisstudy, we aim to address these challenges by training a Persian classicalpoetry generation model using a transformer architecture on a specializeddataset with no pretraining. Additionally, we propose a novel decoding methodto enhance coherence and meaningfulness in the generated poetry, effectivelymanaging the tradeoff between diversity and quality. Furthermore, the resultsof our training approach and the proposed decoding method are evaluated throughcomprehensive set of automatic and human evaluations and showed its superiorcapability to generate coherent and meaningful poetry in compare to otherdecoding methods and an existing Persian large language model (LLM).</description><author>Amir Panahandeh, Hanie Asemi, Esmail Nourani</author><pubDate>Mon, 04 Dec 2023 18:52:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02125v1</guid></item><item><title>AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents</title><link>http://arxiv.org/abs/2310.09971v3</link><description>We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that usessequence models to tackle the challenges of generalization, long-term memory,and meta-learning. Recent works have shown that off-policy learning can makein-context RL with recurrent policies viable. Nonetheless, these approachesrequire extensive tuning and limit scalability by creating key bottlenecks inagents' memory capacity, planning horizon, and model size. AMAGO revisits andredesigns the off-policy in-context approach to successfully trainlong-sequence Transformers over entire rollouts in parallel with end-to-end RL.Our agent is uniquely scalable and applicable to a wide range of problems. Wedemonstrate its strong performance empirically in meta-RL and long-term memorydomains. AMAGO's focus on sparse rewards and off-policy data also allowsin-context learning to extend to goal-conditioned problems with challengingexploration. When combined with a novel hindsight relabeling scheme, AMAGO cansolve a previously difficult category of open-world domains, where agentscomplete many possible instructions in procedurally generated environments. Weevaluate our agent on three goal-conditioned domains and study how itsindividual improvements connect to create a generalist policy.</description><author>Jake Grigsby, Linxi Fan, Yuke Zhu</author><pubDate>Mon, 04 Dec 2023 18:51:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09971v3</guid></item><item><title>VerA: Versatile Anonymization Fit for Clinical Facial Images</title><link>http://arxiv.org/abs/2312.02124v1</link><description>The escalating legislative demand for data privacy in facial imagedissemination has underscored the significance of image anonymization. Recentadvancements in the field surpass traditional pixelation or blur methods, yetthey predominantly address regular single images. This leaves clinical imageanonymization -- a necessity for illustrating medical interventions -- largelyunaddressed. We present VerA, a versatile facial image anonymization that isfit for clinical facial images where: (1) certain semantic areas must bepreserved to show medical intervention results, and (2) anonymizing image pairsis crucial for showing before-and-after results. VerA outperforms or is on parwith state-of-the-art methods in de-identification and photorealism for regularimages. In addition, we validate our results on paired anonymization, and onthe anonymization of both single and paired clinical images with extensivequantitative and qualitative evaluation.</description><author>Majed El Helou, Doruk Cetin, Petar Stamenkovic, Fabio Zund</author><pubDate>Mon, 04 Dec 2023 18:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02124v1</guid></item><item><title>Mathematical Supplement for the $\texttt{gsplat}$ Library</title><link>http://arxiv.org/abs/2312.02121v1</link><description>This report provides the mathematical details of the gsplat library, amodular toolbox for efficient differentiable Gaussian splatting, as proposed byKerbl et al. It provides a self-contained reference for the computationsinvolved in the forward and backward passes of differentiable Gaussiansplatting. To facilitate practical usage and development, we provide a userfriendly Python API that exposes each component of the forward and backwardpasses in rasterization at github.com/nerfstudio-project/gsplat .</description><author>Vickie Ye, Angjoo Kanazawa</author><pubDate>Mon, 04 Dec 2023 18:50:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02121v1</guid></item><item><title>Magicoder: Source Code Is All You Need</title><link>http://arxiv.org/abs/2312.02120v1</link><description>We introduce Magicoder, a series of fully open-source (code, weights, anddata) Large Language Models (LLMs) for code that significantly closes the gapwith top code models while having no more than 7B parameters. Magicoder modelsare trained on 75K synthetic instruction data using OSS-Instruct, a novelapproach to enlightening LLMs with open-source code snippets to generatehigh-quality instruction data for code. Our main motivation is to mitigate theinherent bias of the synthetic data generated by LLMs by empowering them with awealth of open-source references for the production of more diverse, realistic,and controllable data. The orthogonality of OSS-Instruct and other datageneration methods like Evol-Instruct further enables us to build an enhancedMagicoderS. Both Magicoder and MagicoderS substantially outperformstate-of-the-art code models with similar or even larger sizes on a wide rangeof coding benchmarks, including Python text-to-code generation, multilingualcoding, and data-science program completion. Notably, MagicoderS-CL-7B based onCodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 inpass@1). Overall, OSS-Instruct opens a new direction for low-bias andhigh-quality instruction tuning using abundant open-source references.</description><author>Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang</author><pubDate>Mon, 04 Dec 2023 18:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02120v1</guid></item><item><title>Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</title><link>http://arxiv.org/abs/2312.02119v1</link><description>While Large Language Models (LLMs) display versatile functionality, theycontinue to generate harmful, biased, and toxic content, as demonstrated by theprevalence of human-designed jailbreaks. In this work, we present Tree ofAttacks with Pruning (TAP), an automated method for generating jailbreaks thatonly requires black-box access to the target LLM. TAP utilizes an LLM toiteratively refine candidate (attack) prompts using tree-of-thoughts reasoninguntil one of the generated prompts jailbreaks the target. Crucially, beforesending prompts to the target, TAP assesses them and prunes the ones unlikelyto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigatea large search space of prompts and pruning reduces the total number of queriessent to the target. In empirical evaluations, we observe that TAP generatesprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)for more than 80% of the prompts using only a small number of queries. Thissignificantly improves upon the previous state-of-the-art black-box method forgenerating jailbreaks.</description><author>Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, Amin Karbasi</author><pubDate>Mon, 04 Dec 2023 18:49:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02119v1</guid></item><item><title>When it Rains, it Pours: Modeling Media Storms and the News Ecosystem</title><link>http://arxiv.org/abs/2312.02118v1</link><description>Most events in the world receive at most brief coverage by the news media.Occasionally, however, an event will trigger a media storm, with voluminous andwidespread coverage lasting for weeks instead of days. In this work, we developand apply a pairwise article similarity model, allowing us to identify storyclusters in corpora covering local and national online news, and thereby createa comprehensive corpus of media storms over a nearly two year period. Usingthis corpus, we investigate media storms at a new level of granularity,allowing us to validate claims about storm evolution and topical distribution,and provide empirical support for previously hypothesized patterns of influenceof storms on media coverage and intermedia agenda setting.</description><author>Benjamin Litterer, David Jurgens, Dallas Card</author><pubDate>Mon, 04 Dec 2023 18:49:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02118v1</guid></item><item><title>GIVT: Generative Infinite-Vocabulary Transformers</title><link>http://arxiv.org/abs/2312.02116v1</link><description>We introduce generative infinite-vocabulary transformers (GIVT) whichgenerate vector sequences with real-valued entries, instead of discrete tokensfrom a finite vocabulary. To this end, we propose two surprisingly simplemodifications to decoder-only transformers: 1) at the input, we replace thefinite-vocabulary lookup table with a linear projection of the input vectors;and 2) at the output, we replace the logits prediction (usually mapped to acategorical distribution) with the parameters of a multivariate Gaussianmixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,where transformers are used to model the discrete latent sequences of a VQ-VAE,we use GIVT to model the unquantized real-valued latent sequences of a VAE.When applying GIVT to class-conditional image generation with iterative maskedmodeling, we show competitive results with MaskGIT, while our approachoutperforms both VQ-GAN and MaskGIT when using it for causal modeling. Finally,we obtain competitive results outside of image generation when applying ourapproach to panoptic segmentation and depth estimation with a VAE-based variantof the UViM framework.</description><author>Michael Tschannen, Cian Eastwood, Fabian Mentzer</author><pubDate>Mon, 04 Dec 2023 18:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02116v1</guid></item><item><title>TriDeNT: Triple Deep Network Training for Privileged Knowledge Distillation in Histopathology</title><link>http://arxiv.org/abs/2312.02111v1</link><description>Computational pathology models rarely utilise data that will not be availablefor inference. This means most models cannot learn from highly informative datasuch as additional immunohistochemical (IHC) stains and spatialtranscriptomics. We present TriDeNT, a novel self-supervised method forutilising privileged data that is not available during inference to improveperformance. We demonstrate the efficacy of this method for a range ofdifferent paired data including immunohistochemistry, spatial transcriptomicsand expert nuclei annotations. In all settings, TriDeNT outperforms otherstate-of-the-art methods in downstream tasks, with observed improvements of upto 101%. Furthermore, we provide qualitative and quantitative measurements ofthe features learned by these models and how they differ from baselines.TriDeNT offers a novel method to distil knowledge from scarce or costly dataduring training, to create significantly better models for routine inputs.</description><author>Lucas Farndale, Robert Insall, Ke Yuan</author><pubDate>Mon, 04 Dec 2023 18:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02111v1</guid></item><item><title>ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis</title><link>http://arxiv.org/abs/2306.04527v3</link><description>Domain generalization is critical for real-world applications of machinelearning to microscopy images, including histopathology and fluorescenceimaging. Artifacts in these modalities arise through a complex combination offactors relating to tissue collection and laboratory processing, as well asfactors intrinsic to patient samples. In fluorescence imaging, these artifactsstem from variations across experimental batches. The complexity and subtletyof these artifacts make the enumeration of data domains intractable. Therefore,augmentation-based methods of domain generalization that require domainidentifiers and manual fine-tuning are inadequate in this setting. To overcomethis challenge, we introduce ContriMix, a domain generalization technique thatlearns to generate synthetic images by disentangling and permuting thebiological content ("content") and technical variations ("attributes") inmicroscopy images. ContriMix does not rely on domain identifiers or handcraftedaugmentations and makes no assumptions about the input characteristics ofimages. We assess the performance of ContriMix on two pathology datasetsdealing with patch classification and Whole Slide Image label prediction tasksrespectively (Camelyon17-WILDS and RCC subtyping), and one fluorescencemicroscopy dataset (RxRx1-WILDS). Without any access to domain identifiers attrain or test time, ContriMix performs similar or better than currentstate-of-the-art methods in all these datasets, motivating its usage formicroscopy image analysis in real-world settings where domain information ishard to come by. The code for ContriMix can be found athttps://gitlab.com/huutan86/contrimix</description><author>Tan H. Nguyen, Dinkar Juyal, Jin Li, Aaditya Prakash, Shima Nofallah, Chintan Shah, Sai Chowdary Gullapally, Limin Yu, Michael Griffin, Anand Sampat, John Abel, Justin Lee, Amaro Taylor-Weiner</author><pubDate>Mon, 04 Dec 2023 18:42:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04527v3</guid></item><item><title>SCOPE-RL: A Python Library for Offline Reinforcement Learning and Off-Policy Evaluation</title><link>http://arxiv.org/abs/2311.18206v2</link><description>This paper introduces SCOPE-RL, a comprehensive open-source Python softwaredesigned for offline reinforcement learning (offline RL), off-policy evaluation(OPE), and selection (OPS). Unlike most existing libraries that focus solely oneither policy learning or evaluation, SCOPE-RL seamlessly integrates these twokey aspects, facilitating flexible and complete implementations of both offlineRL and OPE processes. SCOPE-RL put particular emphasis on its OPE modules,offering a range of OPE estimators and robust evaluation-of-OPE protocols. Thisapproach enables more in-depth and reliable OPE compared to other packages. Forinstance, SCOPE-RL enhances OPE by estimating the entire reward distributionunder a policy rather than its mere point-wise expected value. Additionally,SCOPE-RL provides a more thorough evaluation-of-OPE by presenting therisk-return tradeoff in OPE results, extending beyond mere accuracy evaluationsin existing OPE literature. SCOPE-RL is designed with user accessibility inmind. Its user-friendly APIs, comprehensive documentation, and a variety ofeasy-to-follow examples assist researchers and practitioners in efficientlyimplementing and experimenting with various offline RL methods and OPEestimators, tailored to their specific problem contexts. The documentation ofSCOPE-RL is available at https://scope-rl.readthedocs.io/en/latest/.</description><author>Haruka Kiyohara, Ren Kishimoto, Kosuke Kawakami, Ken Kobayashi, Kazuhide Nakata, Yuta Saito</author><pubDate>Mon, 04 Dec 2023 18:42:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18206v2</guid></item><item><title>A new sampling methodology for defining heterogeneous subsets of samples for training image segmentation algorithms</title><link>http://arxiv.org/abs/2301.04517v3</link><description>Creating a dataset for training supervised machine learning algorithms can bea demanding task. This is especially true for medical image segmentation sinceone or more specialists are usually required for image annotation, and creatingground truth labels for just a single image can take up to several hours. Inaddition, it is paramount that the annotated samples represent well thedifferent conditions that might affect the imaged tissues as well as possiblechanges in the image acquisition process. This can only be achieved byconsidering samples that are typical in the dataset as well as atypical, oreven outlier, samples. We introduce a new sampling methodology for selectingrelevant images from a large dataset in a way that evenly considers bothprototypical as well as atypical samples. The methodology involves thegeneration of a uniform grid from a feature space representing the samples,which is then used for randomly drawing relevant images. The selected imagesprovide a uniform covering of the original dataset, and thus define aheterogeneous set of images that can be annotated and used for trainingsupervised segmentation algorithms. We provide a case example by creating adataset containing a representative set of blood vessel microscopy imagesselected from a larger dataset containing thousands of images. The dataset,which we call VessMAP, is being made available online to aid the development ofnew blood vessel segmentation algorithms.</description><author>Matheus Viana da Silva, NatÃ¡lia de Carvalho Santos, Julie Ouellette, Baptiste Lacoste, Cesar Henrique Comin</author><pubDate>Mon, 04 Dec 2023 18:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.04517v3</guid></item><item><title>ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation</title><link>http://arxiv.org/abs/2312.02109v1</link><description>This work introduces ArtAdapter, a transformative text-to-image (T2I) styletransfer framework that transcends traditional limitations of color,brushstrokes, and object shape, capturing high-level style elements such ascomposition and distinctive artistic expression. The integration of amulti-level style encoder with our proposed explicit adaptation mechanismenables ArtAdapte to achieve unprecedented fidelity in style transfer, ensuringclose alignment with textual descriptions. Additionally, the incorporation ofan Auxiliary Content Adapter (ACA) effectively separates content from style,alleviating the borrowing of content from style references. Moreover, our novelfast finetuning approach could further enhance zero-shot style representationwhile mitigating the risk of overfitting. Comprehensive evaluations confirmthat ArtAdapter surpasses current state-of-the-art methods.</description><author>Dar-Yen Chen, Hamish Tennent, Ching-Wen Hsu</author><pubDate>Mon, 04 Dec 2023 18:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02109v1</guid></item><item><title>Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation</title><link>http://arxiv.org/abs/2311.18207v2</link><description>Off-Policy Evaluation (OPE) aims to assess the effectiveness ofcounterfactual policies using only offline logged data and is often used toidentify the top-k promising policies for deployment in online A/B tests.Existing evaluation metrics for OPE estimators primarily focus on the"accuracy" of OPE or that of downstream policy selection, neglectingrisk-return tradeoff in the subsequent online policy deployment. To addressthis issue, we draw inspiration from portfolio evaluation in finance anddevelop a new metric, called SharpeRatio@k, which measures the risk-returntradeoff of policy portfolios formed by an OPE estimator under varying onlineevaluation budgets (k). We validate our metric in two example scenarios,demonstrating its ability to effectively distinguish between low-risk andhigh-risk estimators and to accurately identify the most efficient estimator.This efficient estimator is characterized by its capability to form the mostadvantageous policy portfolios, maximizing returns while minimizing risksduring online deployment, a nuance that existing metrics typically overlook. Tofacilitate a quick, accurate, and consistent evaluation of OPE viaSharpeRatio@k, we have also integrated this metric into an open-sourcesoftware, SCOPE-RL. Employing SharpeRatio@k and SCOPE-RL, we conductcomprehensive benchmarking experiments on various estimators and RL tasks,focusing on their risk-return tradeoff. These experiments offer severalinteresting directions and suggestions for future OPE research.</description><author>Haruka Kiyohara, Ren Kishimoto, Kosuke Kawakami, Ken Kobayashi, Kazuhide Nakata, Yuta Saito</author><pubDate>Mon, 04 Dec 2023 18:37:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18207v2</guid></item><item><title>Authoring Worked Examples for Java Programming with Human-AI Collaboration</title><link>http://arxiv.org/abs/2312.02105v1</link><description>Worked examples (solutions to typical programming problems presented as asource code in a certain language and are used to explain the topics from aprogramming class) are among the most popular types of learning content inprogramming classes. Most approaches and tools for presenting these examples tostudents are based on line-by-line explanations of the example code. However,instructors rarely have time to provide line-by-line explanations for a largenumber of examples typically used in a programming class. In this paper, weexplore and assess a human-AI collaboration approach to authoring workedexamples for Java programming. We introduce an authoring system for creatingJava worked examples that generates a starting version of code explanations andpresents it to the instructor to edit if necessary. We also present a studythat assesses the quality of explanations created with this approach.</description><author>Mohammad Hassany, Peter Brusilovsky, Jiaze Ke, Kamil Akhuseyinoglu, Arun Balajiee Lekshmi Narayanan</author><pubDate>Mon, 04 Dec 2023 18:32:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02105v1</guid></item><item><title>Learning Pseudo-Labeler beyond Noun Concepts for Open-Vocabulary Object Detection</title><link>http://arxiv.org/abs/2312.02103v1</link><description>Open-vocabulary object detection (OVOD) has recently gained significantattention as a crucial step toward achieving human-like visual intelligence.Existing OVOD methods extend target vocabulary from pre-defined categories toopen-world by transferring knowledge of arbitrary concepts from vision-languagepre-training models to the detectors. While previous methods have shownremarkable successes, they suffer from indirect supervision or limitedtransferable concepts. In this paper, we propose a simple yet effective methodto directly learn region-text alignment for arbitrary concepts. Specifically,the proposed method aims to learn arbitrary image-to-text mapping forpseudo-labeling of arbitrary concepts, named Pseudo-Labeling for ArbitraryConcepts (PLAC). The proposed method shows competitive performance on thestandard OVOD benchmark for noun concepts and a large improvement on referringexpression comprehension benchmark for arbitrary concepts.</description><author>Sunghun Kang, Junbum Cha, Jonghwan Mun, Byungseok Roh, Chang D. Yoo</author><pubDate>Mon, 04 Dec 2023 18:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02103v1</guid></item><item><title>Mitigating Data Injection Attacks on Federated Learning</title><link>http://arxiv.org/abs/2312.02102v1</link><description>Federated learning is a technique that allows multiple entities tocollaboratively train models using their data without compromising dataprivacy. However, despite its advantages, federated learning can be susceptibleto false data injection attacks. In these scenarios, a malicious entity withcontrol over specific agents in the network can manipulate the learningprocess, leading to a suboptimal model. Consequently, addressing these datainjection attacks presents a significant research challenge in federatedlearning systems. In this paper, we propose a novel technique to detect andmitigate data injection attacks on federated learning systems. Our mitigationmethod is a local scheme, performed during a single instance of training by thecoordinating node, allowing the mitigation during the convergence of thealgorithm. Whenever an agent is suspected to be an attacker, its data will beignored for a certain period, this decision will often be re-evaluated. Weprove that with probability 1, after a finite time, all attackers will beignored while the probability of ignoring a trustful agent becomes 0, providedthat there is a majority of truthful agents. Simulations show that when thecoordinating node detects and isolates all the attackers, the model recoversand converges to the truthful model.</description><author>Or Shalom, Amir Leshem, Waheed U. Bajwa</author><pubDate>Mon, 04 Dec 2023 18:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02102v1</guid></item><item><title>Accuracy Improvement in Differentially Private Logistic Regression: A Pre-training Approach</title><link>http://arxiv.org/abs/2307.13771v2</link><description>Machine learning (ML) models can memorize training datasets. As a result,training ML models over private datasets can lead to the violation ofindividuals' privacy. Differential privacy (DP) is a rigorous privacy notion topreserve the privacy of underlying training datasets. Yet, training ML modelsin a DP framework usually degrades the accuracy of ML models. This paper aimsto boost the accuracy of a DP logistic regression (LR) via a pre-trainingmodule. In more detail, we initially pre-train our LR model on a publictraining dataset that there is no privacy concern about it. Then, we fine-tuneour DP-LR model with the private dataset. In the numerical results, we showthat adding a pre-training module significantly improves the accuracy of theDP-LR model.</description><author>Mohammad Hoseinpour, Milad Hoseinpour, Ali Aghagolzadeh</author><pubDate>Mon, 04 Dec 2023 18:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13771v2</guid></item><item><title>A Nonstochastic Control Approach to Optimization</title><link>http://arxiv.org/abs/2301.07902v3</link><description>Selecting the best hyperparameters for a particular optimization instance,such as the learning rate and momentum, is an important but nonconvex problem.As a result, iterative optimization methods such as hypergradient descent lackglobal optimality guarantees in general. We propose an online nonstochastic control methodology for mathematicaloptimization. First, we formalize the setting of meta-optimization, an onlinelearning formulation of learning the best optimization algorithm from a classof methods. The meta-optimization problem over gradient-based methods can beframed as a feedback control problem over the choice of hyperparameters,including the learning rate, momentum, and the preconditioner. Although the original optimal control problem is nonconvex, we show howrecent methods from online nonstochastic control using convex relaxations canbe used to overcome the challenge of nonconvexity, and obtain regret guaranteesagainst the best offline solution. This guarantees that in meta-optimization,given a sequence of optimization problems, we can learn a method that attainsconvergence comparable to that of the best optimization method in hindsightfrom a class of methods.</description><author>Xinyi Chen, Elad Hazan</author><pubDate>Mon, 04 Dec 2023 18:19:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07902v3</guid></item><item><title>Single-sample versus case-control sampling scheme for Positive Unlabeled data: the story of two scenarios</title><link>http://arxiv.org/abs/2312.02095v1</link><description>In the paper we argue that performance of the classifiers based on EmpiricalRisk Minimization (ERM) for positive unlabeled data, which are designed forcase-control sampling scheme may significantly deteriorate when applied to asingle-sample scenario. We reveal why their behavior depends, in all but veryspecific cases, on the scenario. Also, we introduce a single-sample caseanalogue of the popular non-negative risk classifier designed for case-controldata and compare its performance with the original proposal. We show that thesignificant differences occur between them, especiall when half or morepositive of observations are labeled. The opposite case when ERM minimizerdesigned for the case-control case is applied for single-sample data is alsoconsidered and similar conclusions are drawn. Taking into account difference ofscenarios requires a sole, but crucial, change in the definition of theEmpirical Risk.</description><author>Jan Mielniczuk, Adam WawrzeÅczyk</author><pubDate>Mon, 04 Dec 2023 18:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02095v1</guid></item><item><title>AutoRepo: A general framework for multi-modal LLM-based automated construction reporting</title><link>http://arxiv.org/abs/2310.07944v2</link><description>Ensuring the safety, quality, and timely completion of construction projectsis paramount, with construction inspections serving as a vital instrumenttowards these goals. Nevertheless, the predominantly manual approach ofpresent-day inspections frequently results in inefficiencies and inadequateinformation management. Such methods often fall short of providing holistic,exhaustive assessments, consequently engendering regulatory oversights andpotential safety hazards. To address this issue, this paper presents a novelframework named AutoRepo for automated generation of construction inspectionreports. The unmanned vehicles efficiently perform construction inspections andcollect scene information, while the multimodal large language models (LLMs)are leveraged to automatically generate the inspection reports. The frameworkwas applied and tested on a real-world construction site, demonstrating itspotential to expedite the inspection process, significantly reduce resourceallocation, and produce high-quality, regulatory standard-compliant inspectionreports. This research thus underscores the immense potential of multimodallarge language models in revolutionizing construction inspection practices,signaling a significant leap forward towards a more efficient and saferconstruction management paradigm.</description><author>Hongxu Pu, Xincong Yang, Jing Li, Runhao Guo, Heng Li</author><pubDate>Mon, 04 Dec 2023 18:13:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07944v2</guid></item><item><title>Physics simulation capabilities of LLMs</title><link>http://arxiv.org/abs/2312.02091v1</link><description>[Abridged abstract] Large Language Models (LLMs) can solve someundergraduate-level to graduate-level physics textbook problems and areproficient at coding. Combining these two capabilities could one day enable AIsystems to simulate and predict the physical world. We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level toresearch-level computational physics problems. We condition LLM generation onthe use of well-documented and widely-used packages to elicit codingcapabilities in the physics and astrophysics domains. We contribute $\sim 50$original and challenging problems in celestial mechanics (with REBOUND),stellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-lineardynamics (with SciPy). Since our problems do not admit unique solutions, weevaluate LLM performance on several soft metrics: counts of lines that containdifferent types of errors (coding, physics, necessity and sufficiency) as wellas a more "educational" Pass-Fail metric focused on capturing the salientphysical ingredients of the problem at hand. As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems,although about 40\% of the solutions could plausibly get a passing grade. About$70-90 \%$ of the code lines produced are necessary, sufficient and correct(coding \&amp; physics). Physics and coding errors are the most common, with someunnecessary or insufficient lines. We observe significant variations acrossproblem class and difficulty. We identify several failure modes of GPT4 in thecomputational physics domain. Our reconnaissance work provides a snapshot of current computationalcapabilities in classical physics and points to obvious improvement targets ifAI systems are ever to reach a basic level of autonomy in physics simulationcapabilities.</description><author>Mohamad Ali-Dib, Kristen Menou</author><pubDate>Mon, 04 Dec 2023 18:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02091v1</guid></item><item><title>VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence</title><link>http://arxiv.org/abs/2312.02087v1</link><description>Current diffusion-based video editing primarily focuses onstructure-preserved editing by utilizing various dense correspondences toensure temporal consistency and motion alignment. However, these approaches areoften ineffective when the target edit involves a shape change. To embark onvideo editing with shape change, we explore customized video subject swappingin this work, where we aim to replace the main subject in a source video with atarget subject having a distinct identity and potentially different shape. Incontrast to previous methods that rely on dense correspondences, we introducethe VideoSwap framework that exploits semantic point correspondences, inspiredby our observation that only a small number of semantic points are necessary toalign the subject's motion trajectory and modify its shape. We also introducevarious user-point interactions (\eg, removing points and dragging points) toaddress various semantic point correspondence. Extensive experimentsdemonstrate state-of-the-art video subject swapping results across a variety ofreal-world videos.</description><author>Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, Kevin Tang</author><pubDate>Mon, 04 Dec 2023 17:58:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02087v1</guid></item><item><title>Bengali Fake Reviews: A Benchmark Dataset and Detection System</title><link>http://arxiv.org/abs/2308.01987v2</link><description>The proliferation of fake reviews on various online platforms has created amajor concern for both consumers and businesses. Such reviews can deceivecustomers and cause damage to the reputation of products or services, making itcrucial to identify them. Although the detection of fake reviews has beenextensively studied in English language, detecting fake reviews in non-Englishlanguages such as Bengali is still a relatively unexplored research area. Thispaper introduces the Bengali Fake Review Detection (BFRD) dataset, the firstpublicly available dataset for identifying fake reviews in Bengali. The datasetconsists of 7710 non-fake and 1339 fake food-related reviews collected fromsocial media posts. To convert non-Bengali words in a review, a unique pipelinehas been proposed that translates English words to their corresponding Bengalimeaning and also back transliterates Romanized Bengali to Bengali. We haveconducted rigorous experimentation using multiple deep learning and pre-trainedtransformer language models to develop a reliable detection system. Finally, wepropose a weighted ensemble model that combines four pre-trained transformers:BanglaBERT, BanglaBERT Base, BanglaBERT Large, and BanglaBERT Generator .According to the experiment results, the proposed ensemble model obtained aweighted F1-score of 0.9843 on 13390 reviews, including 1339 actual fakereviews and 5356 augmented fake reviews generated with the nlpaug library. Theremaining 6695 reviews were randomly selected from the 7710 non-fake instances.The model achieved a 0.9558 weighted F1-score when the fake reviews wereaugmented using the bnaug library.</description><author>G. M. Shahariar, Md. Tanvir Rouf Shawon, Faisal Muhammad Shah, Mohammad Shafiul Alam, Md. Shahriar Mahbub</author><pubDate>Mon, 04 Dec 2023 17:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01987v2</guid></item><item><title>I-AI: A Controllable &amp; Interpretable AI System for Decoding Radiologists' Intense Focus for Accurate CXR Diagnoses</title><link>http://arxiv.org/abs/2309.13550v3</link><description>In the field of chest X-ray (CXR) diagnosis, existing works often focussolely on determining where a radiologist looks, typically through tasks suchas detection, segmentation, or classification. However, these approaches areoften designed as black-box models, lacking interpretability. In this paper, weintroduce Interpretable Artificial Intelligence (I-AI) a novel and unifiedcontrollable interpretable pipeline for decoding the intense focus ofradiologists in CXR diagnosis. Our I-AI addresses three key questions: where aradiologist looks, how long they focus on specific areas, and what findingsthey diagnose. By capturing the intensity of the radiologist's gaze, we providea unified solution that offers insights into the cognitive process underlyingradiological interpretation. Unlike current methods that rely on black-boxmachine learning models, which can be prone to extracting erroneous informationfrom the entire input image during the diagnosis process, we tackle this issueby effectively masking out irrelevant information. Our proposed I-AI leveragesa vision-language model, allowing for precise control over the interpretationprocess while ensuring the exclusion of irrelevant features. To train our I-AImodel, we utilize an eye gaze dataset to extract anatomical gaze informationand generate ground truth heatmaps. Through extensive experimentation, wedemonstrate the efficacy of our method. We showcase that the attentionheatmaps, designed to mimic radiologists' focus, encode sufficient and relevantinformation, enabling accurate classification tasks using only a portion ofCXR.</description><author>Trong Thang Pham, Jacob Brecheisen, Anh Nguyen, Hien Nguyen, Ngan Le</author><pubDate>Mon, 04 Dec 2023 17:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13550v3</guid></item><item><title>From Monte Carlo to neural networks approximations of boundary value problems</title><link>http://arxiv.org/abs/2209.01432v2</link><description>In this paper we study probabilistic and neural network approximations forsolutions to Poisson equation subject to H\" older data in general boundeddomains of $\mathbb{R}^d$. We aim at two fundamental goals. The first, and the most important, we show that the solution to Poissonequation can be numerically approximated in the sup-norm by Monte Carlomethods, { and that this can be done highly efficiently if we use a modifiedversion} of the walk on spheres algorithm { as an acceleration method. Thisprovides estimates which are efficient with respect to the prescribedapproximation error and with polynomial complexity in the dimension and thereciprocal of the error.} {A crucial feature is that} the overall number ofsamples does not not depend on the point at which the approximation isperformed. As a second goal, we show that the obtained Monte Carlo solver renders { in aconstructive way} ReLU deep neural network (DNN) solutions to Poisson problem,whose sizes depend at most polynomialy in the dimension $d$ and in the desirederror. In fact we show that the random DNN provides with high probability asmall approximation error and low polynomial complexity in the dimension.</description><author>Lucian Beznea, Iulian Cimpean, Oana Lupascu-Stamate, Ionel Popescu, Arghir Zarnescu</author><pubDate>Mon, 04 Dec 2023 17:51:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01432v2</guid></item><item><title>Deep Set Neural Networks for forecasting asynchronous bioprocess timeseries</title><link>http://arxiv.org/abs/2312.02079v1</link><description>Cultivation experiments often produce sparse and irregular time series.Classical approaches based on mechanistic models, like Maximum Likelihoodfitting or Monte-Carlo Markov chain sampling, can easily account for sparsityand time-grid irregularities, but most statistical and Machine Learning toolsare not designed for handling sparse data out-of-the-box. Among popularapproaches there are various schemes for filling missing values (imputation)and interpolation into a regular grid (alignment). However, such methodstransfer the biases of the interpolation or imputation models to the targetmodel. We show that Deep Set Neural Networks equipped with triplet encoding ofthe input data can successfully handle bio-process data without any need forimputation or alignment procedures. The method is agnostic to the particularnature of the time series and can be adapted for any task, for example, onlinemonitoring, predictive control, design of experiments, etc. In this work, wefocus on forecasting. We argue that such an approach is especially suitable fortypical cultivation processes, demonstrate the performance of the method onseveral forecasting tasks using data generated from macrokinetic growth modelsunder realistic conditions, and compare the method to a conventional fittingprocedure and methods based on imputation and alignment.</description><author>Maxim Borisyak, Stefan Born, Peter Neubauer, NicolÃ¡s Cruz-Bournazou</author><pubDate>Mon, 04 Dec 2023 17:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02079v1</guid></item><item><title>Integrating AI into CCTV Systems: A Comprehensive Evaluation of Smart Video Surveillance in Community Space</title><link>http://arxiv.org/abs/2312.02078v1</link><description>This article presents an AI-enabled Smart Video Surveillance (SVS) designedto enhance safety in community spaces such as educational and recreationalareas, and small businesses. The proposed system innovatively integrates withexisting CCTV and wired camera networks, simplifying its adoption acrossvarious community cases to leverage recent AI advancements. Our SVS system,focusing on privacy, uses metadata instead of pixel data for activityrecognition, aligning with ethical standards. It features cloud-basedinfrastructure and a mobile app for real-time, privacy-conscious alerts incommunities. This article notably pioneers a comprehensive real-world evaluation of theSVS system, covering AI-driven visual processing, statistical analysis,database management, cloud communication, and user notifications. It's also thefirst to assess an end-to-end anomaly detection system's performance, vital foridentifying potential public safety incidents. For our evaluation, we implemented the system in a community college, servingas an ideal model to exemplify the proposed system's capabilities. Our findingsin this setting demonstrate the system's robustness, with throughput, latency,and scalability effectively managing 16 CCTV cameras. The system maintained aconsistent 16.5 frames per second (FPS) over a 21-hour operation. The averageend-to-end latency for detecting behavioral anomalies and alerting users was26.76 seconds.</description><author>Shanle Yao, Babak Rahimi Ardabili, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Christopher Neff, Hamed Tabkhi</author><pubDate>Mon, 04 Dec 2023 17:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02078v1</guid></item><item><title>Direct Unsupervised Denoising</title><link>http://arxiv.org/abs/2310.18116v2</link><description>Traditional supervised denoisers are trained using pairs of noisy input andclean target images. They learn to predict a central tendency of the posteriordistribution over possible clean images. When, e.g., trained with the popularquadratic loss function, the network's output will correspond to the minimummean square error (MMSE) estimate. Unsupervised denoisers based on VariationalAutoEncoders (VAEs) have succeeded in achieving state-of-the-art results whilerequiring only unpaired noisy data as training input. In contrast to thetraditional supervised approach, unsupervised denoisers do not directly producea single prediction, such as the MMSE estimate, but allow us to draw samplesfrom the posterior distribution of clean solutions corresponding to the noisyinput. To approximate the MMSE estimate during inference, unsupervised methodshave to create and draw a large number of samples - a computationally expensiveprocess - rendering the approach inapplicable in many situations. Here, wepresent an alternative approach that trains a deterministic network alongsidethe VAE to directly predict a central tendency. Our method achieves resultsthat surpass the results achieved by the unsupervised method at a fraction ofthe computational cost.</description><author>Benjamin Salmon, Alexander Krull</author><pubDate>Mon, 04 Dec 2023 17:38:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18116v2</guid></item><item><title>Federated Learning is Better with Non-Homomorphic Encryption</title><link>http://arxiv.org/abs/2312.02074v1</link><description>Traditional AI methodologies necessitate centralized data collection, whichbecomes impractical when facing problems with network communication, dataprivacy, or storage capacity. Federated Learning (FL) offers a paradigm thatempowers distributed AI model training without collecting raw data. There aredifferent choices for providing privacy during FL training. One of the popularmethodologies is employing Homomorphic Encryption (HE) - a breakthrough inprivacy-preserving computation from Cryptography. However, these methods have aprice in the form of extra computation and memory footprint. To resolve theseissues, we propose an innovative framework that synergizes permutation-basedcompressors with Classical Cryptography, even though employing ClassicalCryptography was assumed to be impossible in the past in the context of FL. Ourframework offers a way to replace HE with cheaper Classical Cryptographyprimitives which provides security for the training process. It fostersasynchronous communication and provides flexible deployment options in variouscommunication topologies.</description><author>Konstantin Burlachenko, Abdulmajeed Alrowithi, Fahad Ali Albalawi, Peter Richtarik</author><pubDate>Mon, 04 Dec 2023 17:37:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02074v1</guid></item><item><title>A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia</title><link>http://arxiv.org/abs/2312.02073v1</link><description>Large language models (LLMs) have demonstrated impressive capabilities instoring and recalling factual knowledge, but also in adapting to novelin-context information. Yet, the mechanisms underlying their in-contextgrounding remain unknown, especially in situations where in-context informationcontradicts factual knowledge embedded in the parameters. This is critical forretrieval-augmented generation methods, which enrich the context withup-to-date information, hoping that grounding can rectify the outdatedparametric knowledge. In this study, we introduce Fakepedia, a counterfactualdataset designed to evaluate grounding abilities when the parametric knowledgeclashes with the in-context information. We benchmark various LLMs withFakepedia and discover that GPT-4-turbo has a strong preference for itsparametric knowledge. Mistral-7B, on the contrary, is the model that mostrobustly chooses the grounded answer. Then, we conduct causal mediationanalysis on LLM components when answering Fakepedia queries. We demonstratethat inspection of the computational graph alone can predict LLM grounding with92.8% accuracy, especially because few MLPs in the Transformer can predictnon-grounded behavior. Our results, together with existing findings aboutfactual recall mechanisms, provide a coherent narrative of how grounding andfactual recall mechanisms interact within LLMs.</description><author>Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre KÄ±cÄ±man, Hamid Palangi, Barun Patra, Robert West</author><pubDate>Mon, 04 Dec 2023 17:35:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02073v1</guid></item><item><title>Improving Intrinsic Exploration by Creating Stationary Objectives</title><link>http://arxiv.org/abs/2310.18144v3</link><description>Exploration bonuses in reinforcement learning guide long-horizon explorationby defining custom intrinsic objectives. Several exploration objectives likecount-based bonuses, pseudo-counts, and state-entropy maximization arenon-stationary and hence are difficult to optimize for the agent. While thisissue is generally known, it is usually omitted and solutions remainunder-explored. The key contribution of our work lies in transforming theoriginal non-stationary rewards into stationary rewards through an augmentedstate representation. For this purpose, we introduce the Stationary ObjectivesFor Exploration (SOFE) framework. SOFE requires identifying sufficientstatistics for different exploration bonuses and finding an efficient encodingof these statistics to use as input to a deep network. SOFE is based onproposing state augmentations that expand the state space but hold the promiseof simplifying the optimization of the agent's objective. We show that SOFEimproves the performance of several exploration objectives, includingcount-based bonuses, pseudo-counts, and state-entropy maximization. Moreover,SOFE outperforms prior methods that attempt to stabilize the optimization ofintrinsic objectives. We demonstrate the efficacy of SOFE in hard-explorationproblems, including sparse-reward tasks, pixel-based observations, 3Dnavigation, and procedurally generated environments.</description><author>Roger Creus Castanyer, Joshua Romoff, Glen Berseth</author><pubDate>Mon, 04 Dec 2023 17:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.18144v3</guid></item><item><title>GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</title><link>http://arxiv.org/abs/2312.02069v1</link><description>We introduce GaussianAvatars, a new method to create photorealistic headavatars that are fully controllable in terms of expression, pose, andviewpoint. The core idea is a dynamic 3D representation based on 3D Gaussiansplats that are rigged to a parametric morphable face model. This combinationfacilitates photorealistic rendering while allowing for precise animationcontrol via the underlying parametric model, e.g., through expression transferfrom a driving sequence or by manually changing the morphable model parameters.We parameterize each splat by a local coordinate frame of a triangle andoptimize for explicit displacement offset to obtain a more accurate geometricrepresentation. During avatar reconstruction, we jointly optimize for themorphable model parameters and Gaussian splat parameters in an end-to-endfashion. We demonstrate the animation capabilities of our photorealistic avatarin several challenging scenarios. For instance, we show reenactments from adriving video, where our method outperforms existing works by a significantmargin.</description><author>Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias NieÃner</author><pubDate>Mon, 04 Dec 2023 17:28:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02069v1</guid></item><item><title>Rethinking Label Smoothing on Multi-hop Question Answering</title><link>http://arxiv.org/abs/2212.09512v2</link><description>Multi-Hop Question Answering (MHQA) is a significant area in questionanswering, requiring multiple reasoning components, including documentretrieval, supporting sentence prediction, and answer span extraction. In thiswork, we analyze the primary factors limiting the performance of multi-hopreasoning and introduce label smoothing into the MHQA task. This is aimed atenhancing the generalization capabilities of MHQA systems and mitigatingoverfitting of answer spans and reasoning paths in training set. We propose anovel label smoothing technique, F1 Smoothing, which incorporates uncertaintyinto the learning process and is specifically tailored for Machine ReadingComprehension (MRC) tasks. Inspired by the principles of curriculum learning,we introduce the Linear Decay Label Smoothing Algorithm (LDLA), whichprogressively reduces uncertainty throughout the training process. Experimenton the HotpotQA dataset demonstrates the effectiveness of our methods inenhancing performance and generalizability in multi-hop reasoning, achievingnew state-of-the-art results on the leaderboard.</description><author>Zhangyue Yin, Yuxin Wang, Xiannian Hu, Yiguang Wu, Hang Yan, Xinyu Zhang, Zhao Cao, Xuanjing Huang, Xipeng Qiu</author><pubDate>Mon, 04 Dec 2023 17:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09512v2</guid></item><item><title>Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?</title><link>http://arxiv.org/abs/2312.02065v1</link><description>Large language models (LLMs) offer a range of new possibilities, includingadapting the text to different audiences and their reading needs. But how welldo they adapt? We evaluate the readability of answers generated by fourstate-of-the-art LLMs (commercial and open-source) to science questions whenprompted to target different age groups and education levels. To assess theadaptability of LLMs to diverse audiences, we compare the readability scores ofthe generated responses against the recommended comprehension level of each ageand education group. We find large variations in the readability of the answersby different LLMs. Our results suggest LLM answers need to be better adapted tothe intended audience demographics to be more comprehensible. They underlinethe importance of enhancing the adaptability of LLMs in education settings tocater to diverse age and education levels. Overall, current LLMs have setreadability ranges and do not adapt well to different audiences, even whenprompted. That limits their potential for educational purposes.</description><author>Donya Rooein, Amanda Cercas Curry, Dirk Hovy</author><pubDate>Mon, 04 Dec 2023 17:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02065v1</guid></item><item><title>The GPU Phase Folding and Deep Learning Method for Detecting Exoplanet Transits</title><link>http://arxiv.org/abs/2312.02063v1</link><description>This paper presents GPFC, a novel Graphics Processing Unit (GPU) PhaseFolding and Convolutional Neural Network (CNN) system to detect exoplanetsusing the transit method. We devise a fast folding algorithm parallelized on aGPU to amplify low signal-to-noise ratio transit signals, allowing a search athigh precision and speed. A CNN trained on two million synthetic light curvesreports a score indicating the likelihood of a planetary signal at each period.GPFC improves on speed by three orders of magnitude over the predominantBox-fitting Least Squares (BLS) method. Our simulation results show GPFCachieves 97% training accuracy, higher true positive rate at the same falsepositive rate of detection, and higher precision at the same recall rate whencompared to BLS. GPFC recovers 100% of known ultra-short-period planets inKepler light curves from a blind search. These results highlight the promise ofGPFC as an alternative approach to the traditional BLS algorithm for findingnew transiting exoplanets in data taken with Kepler and other space transitmissions such as K2, TESS and future PLATO and Earth 2.0.</description><author>Kaitlyn Wang, Kevin Wang, Jian Ge, Yinan Zhao, Kevin Willis</author><pubDate>Mon, 04 Dec 2023 17:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02063v1</guid></item><item><title>AI ensemble for signal detection of higher order gravitational wave modes of quasi-circular, spinning, non-precessing binary black hole mergers</title><link>http://arxiv.org/abs/2310.00052v2</link><description>We introduce spatiotemporal-graph models that concurrently process data fromthe twin advanced LIGO detectors and the advanced Virgo detector. We trainedthese AI classifiers with 2.4 million IMRPhenomXPHM waveforms that describequasi-circular, spinning, non-precessing binary black hole mergers withcomponent masses $m_{\{1,2\}}\in[3M_\odot, 50 M_\odot]$, and individual spins$s^z_{\{1,2\}}\in[-0.9, 0.9]$; and which include the $(\ell, |m|) = \{(2, 2),(2, 1), (3, 3), (3, 2), (4, 4)\}$ modes, and mode mixing effects in the $\ell =3, |m| = 2$ harmonics. We trained these AI classifiers within 22 hours usingdistributed training over 96 NVIDIA V100 GPUs in the Summit supercomputer. Wethen used transfer learning to create AI predictors that estimate the totalmass of potential binary black holes identified by all AI classifiers in theensemble. We used this ensemble, 3 classifiers for signal detection and 2 totalmass predictors, to process a year-long test set in which we injected 300,000signals. This year-long test set was processed within 5.19 minutes using 1024NVIDIA A100 GPUs in the Polaris supercomputer (for AI inference) and 128 CPUnodes in the ThetaKNL supercomputer (for post-processing of noise triggers),housed at the Argonne Leadership Computing Facility. These studies indicatethat our AI ensemble provides state-of-the-art signal detection accuracy, andreports 2 misclassifications for every year of searched data. This is the firstAI ensemble designed to search for and find higher order gravitational wavemode signals.</description><author>Minyang Tian, E. A. Huerta, Huihuo Zheng</author><pubDate>Mon, 04 Dec 2023 17:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00052v2</guid></item><item><title>DUCK: Distance-based Unlearning via Centroid Kinematics</title><link>http://arxiv.org/abs/2312.02052v1</link><description>Machine Unlearning is rising as a new field, driven by the pressing necessityof ensuring privacy in modern artificial intelligence models. This techniqueprimarily aims to eradicate any residual influence of a specific subset of datafrom the knowledge acquired by a neural model during its training. This workintroduces a novel unlearning algorithm, denoted as Distance-based Unlearningvia Centroid Kinematics (DUCK), which employs metric learning to guide theremoval of samples matching the nearest incorrect centroid in the embeddingspace. Evaluation of the algorithm's performance is conducted across variousbenchmark datasets in two distinct scenarios, class removal, and homogeneoussampling removal, obtaining state-of-the-art performance. We introduce a novelmetric, called Adaptive Unlearning Score (AUS), encompassing not only theefficacy of the unlearning process in forgetting target data but alsoquantifying the performance loss relative to the original model. Moreover, wepropose a novel membership inference attack to assess the algorithm's capacityto erase previously acquired knowledge, designed to be adaptable to futuremethodologies.</description><author>Marco Cotogni, Jacopo Bonato, Luigi Sabetta, Francesco Pelosin, Alessandro Nicolosi</author><pubDate>Mon, 04 Dec 2023 17:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02052v1</guid></item><item><title>TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</title><link>http://arxiv.org/abs/2312.02051v1</link><description>This work proposes TimeChat, a time-sensitive multimodal large language modelspecifically designed for long video understanding. Our model incorporates twokey architectural contributions: (1) a timestamp-aware frame encoder that bindsvisual content with the timestamp of each frame, and (2) a sliding videoQ-Former that produces a video token sequence of varying lengths to accommodatevideos of various durations. Additionally, we construct an instruction-tuningdataset, encompassing 6 tasks and a total of 125K instances, to further enhanceTimeChat's instruction-following performance. Experiment results across variousvideo understanding tasks, such as dense captioning, temporal grounding, andhighlight detection, demonstrate TimeChat's strong zero-shot temporallocalization and reasoning capabilities. For example, it achieves +9.2 F1 scoreand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)on Charades-STA, compared to state-of-the-art video large language models,holding the potential to serve as a versatile video assistant for long-formvideo comprehension tasks and satisfy realistic user requirements.</description><author>Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou</author><pubDate>Mon, 04 Dec 2023 17:09:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02051v1</guid></item><item><title>How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model</title><link>http://arxiv.org/abs/2307.02129v3</link><description>Deep learning algorithms demonstrate a surprising ability to learnhigh-dimensional tasks from limited examples. This is commonly attributed tothe depth of neural networks, enabling them to build a hierarchy of abstract,low-dimensional data representations. However, how many training examples arerequired to learn such representations remains unknown. To quantitatively studythis question, we introduce the Random Hierarchy Model: a family of synthetictasks inspired by the hierarchical structure of language and images. The modelis a classification task where each class corresponds to a group of high-levelfeatures, chosen among several equivalent groups associated with the sameclass. In turn, each feature corresponds to a group of sub-features chosenamong several equivalent ones and so on, following a hierarchy of compositionrules. We find that deep networks learn the task by developing internalrepresentations invariant to exchanging equivalent groups. Moreover, the numberof data required corresponds to the point where correlations between low-levelfeatures and classes become detectable. Overall, our results indicate how deepnetworks overcome the curse of dimensionality by building invariantrepresentations, and provide an estimate of the number of data required tolearn a hierarchical task.</description><author>Francesco Cagnetta, Leonardo Petrini, Umberto M. Tomasini, Alessandro Favero, Matthieu Wyart</author><pubDate>Mon, 04 Dec 2023 17:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02129v3</guid></item><item><title>Generating Realistic Counterfactuals for Retinal Fundus and OCT Images using Diffusion Models</title><link>http://arxiv.org/abs/2311.11629v2</link><description>Counterfactual reasoning is often used in clinical settings to explaindecisions or weigh alternatives. Therefore, for imaging based specialties suchas ophthalmology, it would be beneficial to be able to create counterfactualimages, illustrating answers to questions like "If the subject had had diabeticretinopathy, how would the fundus image have looked?". Here, we demonstratethat using a diffusion model in combination with an adversarially robustclassifier trained on retinal disease classification tasks enables thegeneration of highly realistic counterfactuals of retinal fundus images andoptical coherence tomography (OCT) B-scans. The key to the realism ofcounterfactuals is that these classifiers encode salient features indicativefor each disease class and can steer the diffusion model to depict diseasesigns or remove disease-related lesions in a realistic way. In a user study,domain experts also found the counterfactuals generated using our methodsignificantly more realistic than counterfactuals generated from a previousmethod, and even indistinguishable from real images.</description><author>Indu Ilanchezian, Valentyn Boreiko, Laura KÃ¼hlewein, Ziwei Huang, Murat SeÃ§kin Ayhan, Matthias Hein, Lisa Koch, Philipp Berens</author><pubDate>Mon, 04 Dec 2023 17:01:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11629v2</guid></item><item><title>Interpreting and Improving Diffusion Models Using the Euclidean Distance Function</title><link>http://arxiv.org/abs/2306.04848v2</link><description>Denoising is intuitively related to projection. Indeed, under the manifoldhypothesis, adding random noise is approximately equivalent to orthogonalperturbation. Hence, learning to denoise is approximately learning to project.In this paper, we use this observation to reinterpret denoising diffusionmodels as approximate gradient descent applied to the Euclidean distancefunction. We then provide straight-forward convergence analysis of the DDIMsampler under simple assumptions on the projection-error of the denoiser.Finally, we propose a new sampler based on two simple modifications to DDIMusing insights from our theoretical results. In as few as 5-10 functionevaluations, our sampler achieves state-of-the-art FID scores on pretrainedCIFAR-10 and CelebA models and can generate high quality samples on latentdiffusion models.</description><author>Frank Permenter, Chenyang Yuan</author><pubDate>Mon, 04 Dec 2023 17:00:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04848v2</guid></item><item><title>GFS: Graph-based Feature Synthesis for Prediction over Relational Databases</title><link>http://arxiv.org/abs/2312.02037v1</link><description>Relational databases are extensively utilized in a variety of moderninformation system applications, and they always carry valuable data patterns.There are a huge number of data mining or machine learning tasks conducted onrelational databases. However, it is worth noting that there are limitedmachine learning models specifically designed for relational databases, as mostmodels are primarily tailored for single table settings. Consequently, theprevalent approach for training machine learning models on data stored inrelational databases involves performing feature engineering to merge the datafrom multiple tables into a single table and subsequently applying single tablemodels. This approach not only requires significant effort in featureengineering but also destroys the inherent relational structure present in thedata. To address these challenges, we propose a novel framework calledGraph-based Feature Synthesis (GFS). GFS formulates the relational database asa heterogeneous graph, thereby preserving the relational structure within thedata. By leveraging the inductive bias from single table models, GFSeffectively captures the intricate relationships inherent in each table.Additionally, the whole framework eliminates the need for manual featureengineering. In the extensive experiment over four real-world multi-tablerelational databases, GFS outperforms previous methods designed for relationaldatabases, demonstrating its superior performance.</description><author>Han Zhang, Quan Gan, David Wipf, Weinan Zhang</author><pubDate>Mon, 04 Dec 2023 16:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02037v1</guid></item><item><title>Implicit Learning of Scene Geometry from Poses for Global Localization</title><link>http://arxiv.org/abs/2312.02029v1</link><description>Global visual localization estimates the absolute pose of a camera using asingle image, in a previously mapped area. Obtaining the pose from a singleimage enables many robotics and augmented/virtual reality applications.Inspired by latest advances in deep learning, many existing approaches directlylearn and regress 6 DoF pose from an input image. However, these methods do notfully utilize the underlying scene geometry for pose regression. The challengein monocular relocalization is the minimal availability of supervised trainingdata, which is just the corresponding 6 DoF poses of the images. In this paper,we propose to utilize these minimal available labels (.i.e, poses) to learn theunderlying 3D geometry of the scene and use the geometry to estimate the 6 DoFcamera pose. We present a learning method that uses these pose labels and rigidalignment to learn two 3D geometric representations (\textit{X, Y, Zcoordinates}) of the scene, one in camera coordinate frame and the other inglobal coordinate frame. Given a single image, it estimates these two 3D scenerepresentations, which are then aligned to estimate a pose that matches thepose label. This formulation allows for the active inclusion of additionallearning constraints to minimize 3D alignment errors between the two 3D scenerepresentations, and 2D re-projection errors between the 3D global scenerepresentation and 2D image pixels, resulting in improved localizationaccuracy. During inference, our model estimates the 3D scene geometry in cameraand global frames and aligns them rigidly to obtain pose in real-time. Weevaluate our work on three common visual localization datasets, conductablation studies, and show that our method exceeds state-of-the-art regressionmethods' pose accuracy on all datasets.</description><author>Mohammad Altillawi, Shile Li, Sai Manoj Prakhya, Ziyuan Liu, Joan Serrat</author><pubDate>Mon, 04 Dec 2023 16:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02029v1</guid></item><item><title>Stochastic Optimal Control Matching</title><link>http://arxiv.org/abs/2312.02027v1</link><description>Stochastic optimal control, which has the goal of driving the behavior ofnoisy systems, is broadly applicable in science, engineering and artificialintelligence. Our work introduces Stochastic Optimal Control Matching (SOCM), anovel Iterative Diffusion Optimization (IDO) technique for stochastic optimalcontrol that stems from the same philosophy as the conditional score matchingloss for diffusion models. That is, the control is learned via a least squaresproblem by trying to fit a matching vector field. The training loss, which isclosely connected to the cross-entropy loss, is optimized with respect to boththe control function and a family of reparameterization matrices which appearin the matching vector field. The optimization with respect to thereparameterization matrices aims at minimizing the variance of the matchingvector field. Experimentally, our algorithm achieves lower error than all theexisting IDO techniques for stochastic optimal control for four differentcontrol settings. The key idea underlying SOCM is the path-wisereparameterization trick, a novel technique that is of independent interest,e.g., for generative modeling.</description><author>Carles Domingo-Enrich, Jiequn Han, Brandon Amos, Joan Bruna, Ricky T. Q. Chen</author><pubDate>Mon, 04 Dec 2023 16:49:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02027v1</guid></item><item><title>AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents</title><link>http://arxiv.org/abs/2311.17465v3</link><description>In this study, our goal is to create interactive avatar agents that canautonomously plan and animate nuanced facial movements realistically, from bothvisual and behavioral perspectives. Given high-level inputs about theenvironment and agent profile, our framework harnesses LLMs to produce a seriesof detailed text descriptions of the avatar agents' facial motions. Thesedescriptions are then processed by our task-agnostic driving engine into motiontoken sequences, which are subsequently converted into continuous motionembeddings that are further consumed by our standalone neural-based renderer togenerate the final photorealistic avatar animations. These streamlinedprocesses allow our framework to adapt to a variety of non-verbal avatarinteractions, both monadic and dyadic. Our extensive study, which includesexperiments on both newly compiled and existing datasets featuring two types ofagents -- one capable of monadic interaction with the environment, and theother designed for dyadic conversation -- validates the effectiveness andversatility of our approach. To our knowledge, we advanced a leap step bycombining LLMs and neural rendering for generalized non-verbal prediction andphoto-realistic rendering of avatar agents.</description><author>Duomin Wang, Bin Dai, Yu Deng, Baoyuan Wang</author><pubDate>Mon, 04 Dec 2023 16:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17465v3</guid></item><item><title>VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations for Domain Generalized Semantic Segmentation</title><link>http://arxiv.org/abs/2312.02021v1</link><description>Domain generalization (DG) remains a significant challenge for perceptionbased on deep neural networks (DNN), where domain shifts occur due to lighting,weather, or geolocation changes. In this work, we propose VLTSeg to enhancedomain generalization in semantic segmentation, where the network is solelytrained on the source domain and evaluated on unseen target domains. Our methodleverages the inherent semantic robustness of vision-language models. First, bysubstituting traditional vision-only backbones with pre-trained encoders fromCLIP and EVA-CLIP as transfer learning setting we find that in the field of DG,vision-language pre-training significantly outperforms supervised andself-supervised vision pre-training. We thus propose a new vision-languageapproach for domain generalized segmentation, which improves the domaingeneralization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset.We further show the superior generalization capabilities of vision-languagesegmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDCbenchmark, outperforming the previous SOTA approach by 6.9% mIoU on the testset at the time of writing. Additionally, our approach shows strong in-domaingeneralization capabilities indicated by 86.1% mIoU on the Cityscapes test set,resulting in a shared first place with the previous SOTA on the currentleaderboard at the time of submission.</description><author>Christoph HÃ¼mmer, Manuel Schwonberg, Liangwei Zhong, Hu Cao, Alois Knoll, Hanno Gottschalk</author><pubDate>Mon, 04 Dec 2023 16:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02021v1</guid></item><item><title>Space-Time Attention with Shifted Non-Local Search</title><link>http://arxiv.org/abs/2309.16849v2</link><description>Efficiently computing attention maps for videos is challenging due to themotion of objects between frames. While a standard non-local search ishigh-quality for a window surrounding each query point, the window's small sizecannot accommodate motion. Methods for long-range motion use an auxiliarynetwork to predict the most similar key coordinates as offsets from each querylocation. However, accurately predicting this flow field of offsets remainschallenging, even for large-scale networks. Small spatial inaccuraciessignificantly impact the attention module's quality. This paper proposes asearch strategy that combines the quality of a non-local search with the rangeof predicted offsets. The method, named Shifted Non-Local Search, executes asmall grid search surrounding the predicted offsets to correct small spatialerrors. Our method's in-place computation consumes 10 times less memory and isover 3 times faster than previous work. Experimentally, correcting the smallspatial errors improves the video frame alignment quality by over 3 dB PSNR.Our search upgrades existing space-time attention modules, which improves videodenoising results by 0.30 dB PSNR for a 7.5% increase in overall runtime. Weintegrate our space-time attention module into a UNet-like architecture toachieve state-of-the-art results on video denoising.</description><author>Kent Gauen, Stanley Chan</author><pubDate>Mon, 04 Dec 2023 16:44:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16849v2</guid></item><item><title>Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models</title><link>http://arxiv.org/abs/2312.02019v1</link><description>Unlike most reinforcement learning agents which require an unrealistic amountof environment interactions to learn a new behaviour, humans excel at learningquickly by merely observing and imitating others. This ability highly dependson the fact that humans have a model of their own embodiment that allows themto infer the most likely actions that led to the observed behaviour. In thispaper, we propose Action Inference by Maximising Evidence (AIME) to replicatethis behaviour using world models. AIME consists of two distinct phases. In thefirst phase, the agent learns a world model from its past experience tounderstand its own body by maximising the ELBO. While in the second phase, theagent is given some observation-only demonstrations of an expert performing anovel task and tries to imitate the expert's behaviour. AIME achieves this bydefining a policy as an inference model and maximising the evidence of thedemonstration under the policy and world model. Our method is "zero-shot" inthe sense that it does not require further training for the world model oronline interactions with the environment after given the demonstration. Weempirically validate the zero-shot imitation performance of our method on theWalker and Cheetah embodiment of the DeepMind Control Suite and find itoutperforms the state-of-the-art baselines. Code is available at:https://github.com/argmax-ai/aime.</description><author>Xingyuan Zhang, Philip Becker-Ehmck, Patrick van der Smagt, Maximilian Karl</author><pubDate>Mon, 04 Dec 2023 16:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02019v1</guid></item><item><title>A multi-channel cycleGAN for CBCT to CT synthesis</title><link>http://arxiv.org/abs/2312.02017v1</link><description>Image synthesis is used to generate synthetic CTs (sCTs) from on-treatmentcone-beam CTs (CBCTs) with a view to improving image quality and enablingaccurate dose computation to facilitate a CBCT-based adaptive radiotherapyworkflow. As this area of research gains momentum, developments in sCTgeneration methods are difficult to compare due to the lack of large publicdatasets and sizeable variation in training procedures. To compare and assessthe latest advancements in sCT generation, the SynthRAD2023 challenge providesa public dataset and evaluation framework for both MR and CBCT to sCTsynthesis. Our contribution focuses on the second task, CBCT-to-sCT synthesis.By leveraging a multi-channel input to emphasize specific image features, ourapproach effectively addresses some of the challenges inherent in CBCT imaging,whilst restoring the contrast necessary for accurate visualisation of patients'anatomy. Additionally, we introduce an auxiliary fusion network to furtherenhance the fidelity of generated sCT images.</description><author>Chelsea A. H. Sargeant, Edward G. A. Henderson, DÃ³nal M. McSweeney, Aaron G. Rankin, Denis Page</author><pubDate>Mon, 04 Dec 2023 16:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02017v1</guid></item><item><title>Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images</title><link>http://arxiv.org/abs/2311.13398v2</link><description>In this paper, we present a method to optimize Gaussian splatting with alimited number of images while avoiding overfitting. Representing a 3D scene bycombining numerous Gaussian splats has yielded outstanding visual quality.However, it tends to overfit the training views when only a small number ofimages are available. To address this issue, we introduce a dense depth map asa geometry guide to mitigate overfitting. We obtained the depth map using apre-trained monocular depth estimation model and aligning the scale and offsetusing sparse COLMAP feature points. The adjusted depth aids in the color-basedoptimization of 3D Gaussian splatting, mitigating floating artifacts, andensuring adherence to geometric constraints. We verify the proposed method onthe NeRF-LLFF dataset with varying numbers of few images. Our approachdemonstrates robust geometry compared to the original method that relies solelyon images. Project page: robot0321.github.io/DepthRegGS</description><author>Jaeyoung Chung, Jeongtaek Oh, Kyoung Mu Lee</author><pubDate>Mon, 04 Dec 2023 16:40:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13398v2</guid></item><item><title>ColonNeRF: Neural Radiance Fields for High-Fidelity Long-Sequence Colonoscopy Reconstruction</title><link>http://arxiv.org/abs/2312.02015v1</link><description>Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.However, accurate long-sequence colonoscopy reconstruction faces three majorchallenges: (1) dissimilarity among segments of the colon due to its meanderingand convoluted shape; (2) co-existence of simple and intricately foldedgeometry structures; (3) sparse viewpoints due to constrained cameratrajectories. To tackle these challenges, we introduce a new reconstructionframework based on neural radiance field (NeRF), named ColonNeRF, whichleverages neural rendering for novel view synthesis of long-sequencecolonoscopy. Specifically, to reconstruct the entire colon in a piecewisemanner, our ColonNeRF introduces a region division and integration module,effectively reducing shape dissimilarity and ensuring geometric consistency ineach segment. To learn both the simple and complex geometry in a unifiedframework, our ColonNeRF incorporates a multi-level fusion module thatprogressively models the colon regions from easy to hard. Additionally, toovercome the challenges from sparse views, we devise a DensiNet module fordensifying camera poses under the guidance of semantic consistency. We conductextensive experiments on both synthetic and real-world datasets to evaluate ourColonNeRF. Quantitatively, our ColonNeRF outperforms existing methods on twobenchmarks over four evaluation metrics. Notably, our LPIPS-ALEX scores exhibita substantial increase of about 67%-85% on the SimCol-to-3D dataset.Qualitatively, our reconstruction visualizations show much clearer textures andmore accurate geometric details. These sufficiently demonstrate our superiorperformance over the state-of-the-art methods.</description><author>Yufei Shi, Beijia Lu, Jia-Wei Liu, Ming Li, Mike Zheng Shou</author><pubDate>Mon, 04 Dec 2023 16:38:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02015v1</guid></item><item><title>Optimal Data Generation in Multi-Dimensional Parameter Spaces, using Bayesian Optimization</title><link>http://arxiv.org/abs/2312.02012v1</link><description>Acquiring a substantial number of data points for training accurate machinelearning (ML) models is a big challenge in scientific fields where datacollection is resource-intensive. Here, we propose a novel approach forconstructing a minimal yet highly informative database for training ML modelsin complex multi-dimensional parameter spaces. To achieve this, we mimic theunderlying relation between the output and input parameters using Gaussianprocess regression (GPR). Using a set of known data, GPR provides predictivemeans and standard deviation for the unknown data. Given the predicted standarddeviation by GPR, we select data points using Bayesian optimization to obtainan efficient database for training ML models. We compare the performance of MLmodels trained on databases obtained through this method, with databasesobtained using traditional approaches. Our results demonstrate that the MLmodels trained on the database obtained using Bayesian optimization approachconsistently outperform the other two databases, achieving high accuracy with asignificantly smaller number of data points. Our work contributes to theresource-efficient collection of data in high-dimensional complex parameterspaces, to achieve high precision machine learning predictions.</description><author>M. R. Mahani, Igor A. Nechepurenko, Yasmin Rahimof, Andreas Wicht</author><pubDate>Mon, 04 Dec 2023 16:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02012v1</guid></item><item><title>Towards Learning a Generalist Model for Embodied Navigation</title><link>http://arxiv.org/abs/2312.02010v1</link><description>Building a generalist agent that can interact with the world is theintriguing target of AI systems, thus spurring the research for embodiednavigation, where an agent is required to navigate according to instructions orrespond to queries. Despite the major progress attained, previous worksprimarily focus on task-specific agents and lack generalizability to unseenscenarios. Recently, LLMs have presented remarkable capabilities across variousfields, and provided a promising opportunity for embodied navigation. Drawingon this, we propose the first generalist model for embodied navigation,NaviLLM. It adapts LLMs to embodied navigation by introducing schema-basedinstruction. The schema-based instruction flexibly casts various tasks intogeneration problems, thereby unifying a wide range of tasks. This approachallows us to integrate diverse data sources from various datasets into thetraining, equipping NaviLLM with a wide range of capabilities required byembodied navigation. We conduct extensive experiments to evaluate theperformance and generalizability of our model. The experimental resultsdemonstrate that our unified model achieves state-of-the-art performance onCVDN, SOON, and ScanQA. Specifically, it surpasses the previousstats-of-the-art method by a significant margin of 29% in goal progress onCVDN. Moreover, our model also demonstrates strong generalizability andpresents impressive results on unseen tasks, e.g., embodied question answeringand 3D captioning.</description><author>Duo Zheng, Shijia huang, Lin Zhao, Yiwu Zhong, Liwei Wang</author><pubDate>Mon, 04 Dec 2023 16:32:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02010v1</guid></item><item><title>A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift</title><link>http://arxiv.org/abs/2311.14743v4</link><description>Foundation models, specifically Large Language Models (LLM's), have latelygained wide-spread attention and adoption. Reinforcement Learning with HumanFeedback (RLHF) involves training a reward model to capture desired behaviors,which is then used to align LLM's. These reward models are additionally used atinference-time to estimate LLM responses' adherence to those desired behaviors.However, there is little work measuring how robust these reward models are todistribution shifts. In this work, we evaluate how reward model performance -measured via accuracy and calibration (i.e. alignment between accuracy andconfidence) - is affected by distribution shift. We show novel calibrationpatterns and accuracy drops due to OOD prompts and responses, and that thereward model is more sensitive to shifts in responses than prompts.Additionally, we adapt an OOD detection technique commonly used inclassification to the reward model setting to detect these distribution shiftsin prompts and responses.</description><author>Ben Pikus, Will LeVine, Tony Chen, Sean Hendryx</author><pubDate>Mon, 04 Dec 2023 16:31:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14743v4</guid></item><item><title>SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes</title><link>http://arxiv.org/abs/2305.13998v4</link><description>The Surrogate Modeling Toolbox (SMT) is an open-source Python package thatoffers a collection of surrogate modeling methods, sampling techniques, and aset of sample problems. This paper presents SMT 2.0, a major new release of SMTthat introduces significant upgrades and new features to the toolbox. Thisrelease adds the capability to handle mixed-variable surrogate models andhierarchical variables. These types of variables are becoming increasinglyimportant in several surrogate modeling applications. SMT 2.0 also improves SMTby extending sampling methods, adding new surrogate models, and computingvariance and kernel derivatives for Kriging. This release also includes newfunctions to handle noisy and use multifidelity data. To the best of ourknowledge, SMT 2.0 is the first open-source surrogate library to proposesurrogate models for hierarchical and mixed inputs. This open-source softwareis distributed under the New BSD license.</description><author>Paul Saves, Remi Lafage, Nathalie Bartoli, Youssef Diouane, Jasper Bussemaker, Thierry Lefebvre, John T. Hwang, Joseph Morlier, Joaquim R. R. A. Martins</author><pubDate>Mon, 04 Dec 2023 16:27:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13998v4</guid></item><item><title>FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models</title><link>http://arxiv.org/abs/2306.00783v2</link><description>The ability to create high-quality 3D faces from a single image has becomeincreasingly important with wide applications in video conferencing, AR/VR, andadvanced video editing in movie industries. In this paper, we propose FaceDiffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-qualityFace NeRFs from single images, complete with semantic editing and relightingcapabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertlytrained 2D latent-diffusion model, allowing users to manipulate and constructFace NeRFs in zero-shot learning without the need for explicit 3D data. Withcarefully designed illumination and identity preserving loss, as well asmulti-modal pre-training, FaceDNeRF offers users unparalleled control over theediting process enabling them to create and edit face NeRFs using justsingle-view images, text prompts, and explicit target lighting. The advancedfeatures of FaceDNeRF have been designed to produce more impressive resultsthan existing 2D editing approaches that rely on 2D segmentation maps foreditable attributes. Experiments show that our FaceDNeRF achieves exceptionallyrealistic results and unprecedented flexibility in editing compared withstate-of-the-art 3D face reconstruction and editing methods. Our code will beavailable at https://github.com/BillyXYB/FaceDNeRF.</description><author>Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing Tai, Chi-Keung Tang</author><pubDate>Mon, 04 Dec 2023 16:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00783v2</guid></item><item><title>A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly</title><link>http://arxiv.org/abs/2312.02003v1</link><description>Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionizednatural language understanding and generation. They possess deep languagecomprehension, human-like text generation capabilities, contextual awareness,and robust problem-solving skills, making them invaluable in various domains(e.g., search engines, customer support, translation). In the meantime, LLMshave also gained traction in the security community, revealing securityvulnerabilities and showcasing their potential in security-related tasks. Thispaper explores the intersection of LLMs with security and privacy.Specifically, we investigate how LLMs positively impact security and privacy,potential risks and threats associated with their use, and inherentvulnerabilities within LLMs. Through a comprehensive literature review, thepaper categorizes findings into "The Good" (beneficial LLM applications), "TheBad" (offensive applications), and "The Ugly" (vulnerabilities and theirdefenses). We have some interesting findings. For example, LLMs have proven toenhance code and data security, outperforming traditional methods. However,they can also be harnessed for various attacks (particularly user-levelattacks) due to their human-like reasoning abilities. We have identified areasthat require further research efforts. For example, research on model andparameter extraction attacks is limited and often theoretical, hindered by LLMparameter scale and confidentiality. Safe instruction tuning, a recentdevelopment, requires more exploration. We hope that our work can shed light onthe LLMs' potential to both bolster and jeopardize cybersecurity.</description><author>Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun, Yue Zhang</author><pubDate>Mon, 04 Dec 2023 16:25:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02003v1</guid></item><item><title>SRTransGAN: Image Super-Resolution using Transformer based Generative Adversarial Network</title><link>http://arxiv.org/abs/2312.01999v1</link><description>Image super-resolution aims to synthesize high-resolution image from alow-resolution image. It is an active area to overcome the resolutionlimitations in several applications like low-resolution object-recognition,medical image enhancement, etc. The generative adversarial network (GAN) basedmethods have been the state-of-the-art for image super-resolution by utilizingthe convolutional neural networks (CNNs) based generator and discriminatornetworks. However, the CNNs are not able to exploit the global information veryeffectively in contrast to the transformers, which are the recent breakthroughin deep learning by exploiting the self-attention mechanism. Motivated from thesuccess of transformers in language and vision applications, we propose aSRTransGAN for image super-resolution using transformer based GAN.Specifically, we propose a novel transformer-based encoder-decoder network as agenerator to generate 2x images and 4x images. We design the discriminatornetwork using vision transformer which uses the image as sequence of patchesand hence useful for binary classification between synthesized and realhigh-resolution images. The proposed SRTransGAN outperforms the existingmethods by 4.38 % on an average of PSNR and SSIM scores. We also analyze thesaliency map to understand the learning ability of the proposed method.</description><author>Neeraj Baghel, Shiv Ram Dubey, Satish Kumar Singh</author><pubDate>Mon, 04 Dec 2023 16:22:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01999v1</guid></item><item><title>Language-only Efficient Training of Zero-shot Composed Image Retrieval</title><link>http://arxiv.org/abs/2312.01998v1</link><description>Composed image retrieval (CIR) task takes a composed query of image and text,aiming to search relative images for both conditions. Conventional CIRapproaches need a training dataset composed of triplets of query image, querytext, and target image, which is very expensive to collect. Several recentworks have worked on the zero-shot (ZS) CIR paradigm to tackle the issuewithout using pre-collected triplets. However, the existing ZS-CIR methods showlimited backbone scalability and generalizability due to the lack of diversityof the input texts during training. We propose a novel CIR framework, onlyusing language for its training. Our LinCIR (Language-only training for CIR)can be trained only with text datasets by a novel self-supervision namedself-masking projection (SMP). We project the text latent embedding to thetoken embedding space and construct a new text by replacing the keyword tokensof the original text. Then, we let the new and original texts have the samelatent embedding vector. With this simple strategy, LinCIR is surprisinglyefficient and highly effective; LinCIR with CLIP ViT-G backbone is trained in48 minutes and shows the best ZS-CIR performances on four different CIRbenchmarks, CIRCO, GeneCIS, FashionIQ, and CIRR, even outperforming supervisedmethod on FashionIQ. Code is available at https://github.com/navervision/lincir</description><author>Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang, Sangdoo Yun</author><pubDate>Mon, 04 Dec 2023 16:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01998v1</guid></item><item><title>Heuristic Optimal Transport in Branching Networks</title><link>http://arxiv.org/abs/2311.06650v2</link><description>Optimal transport aims to learn a mapping of sources to targets by minimizingthe cost, which is typically defined as a function of distance. The solution tothis problem consists of straight line segments optimally connecting sources totargets, and it does not exhibit branching. These optimal solutions are instark contrast with both natural, and man-made transportation networks, wherebranching structures are prevalent. Here we discuss a fast heuristic branchingmethod for optimal transport in networks, and we provide several applications.</description><author>M. Andrecut</author><pubDate>Mon, 04 Dec 2023 16:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06650v2</guid></item><item><title>Improving the Robustness of Summarization Models by Detecting and Removing Input Noise</title><link>http://arxiv.org/abs/2212.09928v2</link><description>The evaluation of abstractive summarization models typically uses test datathat is identically distributed as training data. In real-world practice,documents to be summarized may contain input noise caused by text extractionartifacts or data pipeline bugs. The robustness of model performance underdistribution shift caused by such noise is relatively under-studied. We presenta large empirical study quantifying the sometimes severe loss in performance(up to 12 ROUGE-1 points) from different types of input noise for a range ofdatasets and model sizes. We then propose a light-weight method for detectingand removing such noise in the input during model inference without requiringany extra training, auxiliary models, or even prior knowledge of the type ofnoise. Our proposed approach effectively mitigates the loss in performance,recovering a large fraction of the performance drop, sometimes as large as 11ROUGE-1 points.</description><author>Kundan Krishna, Yao Zhao, Jie Ren, Balaji Lakshminarayanan, Jiaming Luo, Mohammad Saleh, Peter J. Liu</author><pubDate>Mon, 04 Dec 2023 16:18:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09928v2</guid></item><item><title>A Generative Self-Supervised Framework using Functional Connectivity in fMRI Data</title><link>http://arxiv.org/abs/2312.01994v1</link><description>Deep neural networks trained on Functional Connectivity (FC) networksextracted from functional Magnetic Resonance Imaging (fMRI) data have gainedpopularity due to the increasing availability of data and advances in modelarchitectures, including Graph Neural Network (GNN). Recent research on theapplication of GNN to FC suggests that exploiting the time-varying propertiesof the FC could significantly improve the accuracy and interpretability of themodel prediction. However, the high cost of acquiring high-quality fMRI dataand corresponding phenotypic labels poses a hurdle to their application inreal-world settings, such that a model na\"ively trained in a supervisedfashion can suffer from insufficient performance or a lack of generalization ona small number of data. In addition, most Self-Supervised Learning (SSL)approaches for GNNs to date adopt a contrastive strategy, which tends to loseappropriate semantic information when the graph structure is perturbed or doesnot leverage both spatial and temporal information simultaneously. In light ofthese challenges, we propose a generative SSL approach that is tailored toeffectively harness spatio-temporal information within dynamic FC. Ourempirical results, experimented with large-scale (&gt;50,000) fMRI datasets,demonstrate that our approach learns valuable representations and enables theconstruction of accurate and robust models when fine-tuned for downstreamtasks.</description><author>Jungwon Choi, Seongho Keum, EungGu Yun, Byung-Hoon Kim, Juho Lee</author><pubDate>Mon, 04 Dec 2023 16:14:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01994v1</guid></item><item><title>BioCLIP: A Vision Foundation Model for the Tree of Life</title><link>http://arxiv.org/abs/2311.18803v2</link><description>Images of the natural world, collected by a variety of cameras, from dronesto individual phones, are increasingly abundant sources of biologicalinformation. There is an explosion of computational methods and tools,particularly computer vision, for extracting biologically relevant informationfrom images for science and conservation. Yet most of these are bespokeapproaches designed for a specific task and are not easily adaptable orextendable to new questions, contexts, and datasets. A vision model for generalorganismal biology questions on images is of timely need. To approach this, wecurate and release TreeOfLife-10M, the largest and most diverse ML-readydataset of biology images. We then develop BioCLIP, a foundation model for thetree of life, leveraging the unique properties of biology captured byTreeOfLife-10M, namely the abundance and variety of images of plants, animals,and fungi, together with the availability of rich structured biologicalknowledge. We rigorously benchmark our approach on diverse fine-grained biologyclassification tasks, and find that BioCLIP consistently and substantiallyoutperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluationreveals that BioCLIP has learned a hierarchical representation conforming tothe tree of life, shedding light on its strong generalizability. Our code,models and data will be made available athttps://github.com/Imageomics/bioclip.</description><author>Samuel Stevens, Jiaman Wu, Matthew J Thompson, Elizabeth G Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila M Dahdul, Charles Stewart, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su</author><pubDate>Mon, 04 Dec 2023 16:13:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18803v2</guid></item><item><title>Information Modified K-Nearest Neighbor</title><link>http://arxiv.org/abs/2312.01991v1</link><description>In this research paper, we introduce a novel classification method aimed atimproving the performance of the K-Nearest Neighbors (KNN) algorithm. Ourapproach leverages Mutual Information (MI) to enhance the significance ofweights and draw inspiration from Shapley values, a concept originating fromcooperative game theory, to refine value allocation. The fundamental conceptunderlying KNN is the classification of samples based on the majority thoroughtheir k-nearest neighbors. While both the distances and labels of theseneighbors are crucial, traditional KNN assigns equal weight to all samples andprevance considering the varying importance of each neighbor based on theirdistances and labels. In the proposed method, known as Information-Modified KNN (IMKNN), we addressthis issue by introducing a straightforward algorithm. To evaluate theeffectiveness of our approach, it is compared with 7 contemporary variants ofKNN, as well as the traditional KNN. Each of these variants exhibits its uniqueadvantages and limitations. We conduct experiments on 12 widely-used datasets,assessing the methods' performance in terms of accuracy, precision and recall. Our study demonstrates that IMKNN consistently outperforms other methodsacross different datasets and criteria by highlighting its superior performancein various classification tasks. These findings underscore the potential ofIMKNN as a valuable tool for enhancing the capabilities of the KNN algorithm indiverse applications.</description><author>Mohammad Ali Vahedifar, Azim Akhtarshenas, Mariam Sabbaghian, Mohammad Rafatpanah</author><pubDate>Mon, 04 Dec 2023 16:10:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01991v1</guid></item><item><title>SARA-RT: Scaling up Robotics Transformers with Self-Adaptive Robust Attention</title><link>http://arxiv.org/abs/2312.01990v1</link><description>We present Self-Adaptive Robust Attention for Robotics Transformers(SARA-RT): a new paradigm for addressing the emerging challenge of scaling upRobotics Transformers (RT) for on-robot deployment. SARA-RT relies on the newmethod of fine-tuning proposed by us, called up-training. It convertspre-trained or already fine-tuned Transformer-based robotic policies ofquadratic time complexity (including massive billion-parametervision-language-action models or VLAs), into their efficient linear-attentioncounterparts maintaining high quality. We demonstrate the effectiveness ofSARA-RT by speeding up: (a) the class of recently introduced RT-2 models, thefirst VLA robotic policies pre-trained on internet-scale data, as well as (b)Point Cloud Transformer (PCT) robotic policies operating on large point clouds.We complement our results with the rigorous mathematical analysis providingdeeper insight into the phenomenon of SARA.</description><author>Isabel Leal, Krzysztof Choromanski, Deepali Jain, Avinava Dubey, Jake Varley, Michael Ryoo, Yao Lu, Frederick Liu, Vikas Sindhwani, Quan Vuong, Tamas Sarlos, Ken Oslund, Karol Hausman, Kanishka Rao</author><pubDate>Mon, 04 Dec 2023 16:08:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01990v1</guid></item><item><title>Bootstrapping SparseFormers from Vision Foundation Models</title><link>http://arxiv.org/abs/2312.01987v1</link><description>The recently proposed SparseFormer architecture provides an alternativeapproach to visual understanding by utilizing a significantly lower number ofvisual tokens via adjusting RoIs, greatly reducing computational costs whilestill achieving promising performance. However, training SparseFormers fromscratch is still expensive, and scaling up the number of parameters can bechallenging. In this paper, we propose to bootstrap SparseFormers fromViT-based vision foundation models in a simple and efficient way. Since themajority of SparseFormer blocks are the standard transformer ones, we caninherit weights from large-scale pre-trained vision transformers and freezethem as much as possible. Therefore, we only need to train theSparseFormer-specific lightweight focusing transformer to adjust token RoIs andfine-tune a few early pre-trained blocks to align the final tokenrepresentation. In such a way, we can bootstrap SparseFormer architectures fromvarious large-scale pre-trained models (e.g., IN-21K pre-trained AugRegs orCLIPs) using a rather smaller amount of training samples (e.g., IN-1K) andwithout labels or captions within just a few hours. As a result, thebootstrapped unimodal SparseFormer (from AugReg-ViT-L/16-384) can reach 84.9%accuracy on IN-1K with only 49 tokens, and the multimodal SparseFormer fromCLIPs also demonstrates notable zero-shot performance with highly reducedcomputational cost without seeing any caption during the bootstrappingprocedure. In addition, CLIP-bootstrapped SparseFormers, which align the outputspace with language without seeing a word, can serve as efficient visionencoders in multimodal large language models. Code will be publicly availableat https://github.com/showlab/sparseformer</description><author>Ziteng Gao, Zhan Tong, Kevin Qinghong Lin, Joya Chen, Mike Zheng Shou</author><pubDate>Mon, 04 Dec 2023 16:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01987v1</guid></item><item><title>Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors</title><link>http://arxiv.org/abs/2310.02980v2</link><description>Modeling long-range dependencies across sequences is a longstanding goal inmachine learning and has led to architectures, such as state space models, thatdramatically outperform Transformers on long sequences. However, theseimpressive empirical gains have been by and large demonstrated on benchmarks(e.g. Long Range Arena), where models are randomly initialized and trained topredict a target label from an input sequence. In this work, we show thatrandom initialization leads to gross overestimation of the differences betweenarchitectures and that pretraining with standard denoising objectives, using$\textit{only the downstream task data}$, leads to dramatic gains acrossmultiple architectures and to very small gaps between Transformers and statespace models (SSMs). In stark contrast to prior works, we find vanillaTransformers to match the performance of S4 on Long Range Arena when properlypretrained, and we improve the best reported results of SSMs on the PathX-256task by 20 absolute points. Subsequently, we analyze the utility ofpreviously-proposed structured parameterizations for SSMs and show they becomemostly redundant in the presence of data-driven initialization obtained throughpretraining. Our work shows that, when evaluating different architectures onsupervised tasks, incorporation of data-driven priors via pretraining isessential for reliable performance estimation, and can be done efficiently.</description><author>Ido Amos, Jonathan Berant, Ankit Gupta</author><pubDate>Mon, 04 Dec 2023 16:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02980v2</guid></item><item><title>UniGS: Unified Representation for Image Generation and Segmentation</title><link>http://arxiv.org/abs/2312.01985v1</link><description>This paper introduces a novel unified representation of diffusion models forimage generation and segmentation. Specifically, we use a colormap to represententity-level masks, addressing the challenge of varying entity numbers whilealigning the representation closely with the image RGB domain. Two novelmodules, including the location-aware color palette and progressive dichotomymodule, are proposed to support our mask representation. On the one hand, alocation-aware palette guarantees the colors' consistency to entities'locations. On the other hand, the progressive dichotomy module can efficientlydecode the synthesized colormap to high-quality entity-level masks in adepth-first binary search without knowing the cluster numbers. To tackle theissue of lacking large-scale segmentation training data, we employ aninpainting pipeline and then improve the flexibility of diffusion models acrossvarious tasks, including inpainting, image synthesis, referring segmentation,and entity segmentation. Comprehensive experiments validate the efficiency ofour approach, demonstrating comparable segmentation mask quality tostate-of-the-art and adaptability to multiple tasks. The code will be releasedat \href{https://github.com/qqlu/Entity}{https://github.com/qqlu/Entity}.</description><author>Lu Qi, Lehan Yang, Weidong Guo, Yu Xu, Bo Du, Varun Jampani, Ming-Hsuan Yang</author><pubDate>Mon, 04 Dec 2023 15:59:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01985v1</guid></item><item><title>Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle</title><link>http://arxiv.org/abs/2310.17378v2</link><description>Recent advances in deep learning have given us some very promising results onthe generalization ability of deep neural networks, however literature stilllacks a comprehensive theory explaining why heavily over-parametrized modelsare able to generalize well while fitting the training data. In this paper wepropose a PAC type bound on the generalization error of feedforward ReLUnetworks via estimating the Rademacher complexity of the set of networksavailable from an initial parameter vector via gradient descent. The key ideais to bound the sensitivity of the network's gradient to perturbation of theinput data along the optimization trajectory. The obtained bound does notexplicitly depend on the depth of the network. Our results are experimentallyverified on the MNIST and CIFAR-10 datasets.</description><author>DÃ¡niel RÃ¡cz, MihÃ¡ly Petreczky, AndrÃ¡s CsertÃ¡n, BÃ¡lint DarÃ³czy</author><pubDate>Mon, 04 Dec 2023 15:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17378v2</guid></item><item><title>USB: A Unified Summarization Benchmark Across Tasks and Domains</title><link>http://arxiv.org/abs/2305.14296v2</link><description>While the NLP community has produced numerous summarization benchmarks, noneprovide the rich annotations required to simultaneously address many importantproblems related to control and reliability. We introduce a Wikipedia-derivedbenchmark, complemented by a rich set of crowd-sourced annotations, thatsupports $8$ interrelated tasks: (i) extractive summarization; (ii) abstractivesummarization; (iii) topic-based summarization; (iv) compressing selectedsentences into a one-line summary; (v) surfacing evidence for a summarysentence; (vi) predicting the factual accuracy of a summary sentence; (vii)identifying unsubstantiated spans in a summary sentence; (viii) correctingfactual errors in summaries. We compare various methods on this benchmark anddiscover that on multiple tasks, moderately-sized fine-tuned modelsconsistently outperform much larger few-shot prompted language models. Forfactuality-related tasks, we also evaluate existing heuristics to createtraining data and find that training on them results in worse performance thantraining on $20\times$ less human-labeled data. Our articles draw from $6$domains, facilitating cross-domain analysis. On some tasks, the amount oftraining data matters more than the domain where it comes from, while for othertasks training specifically on data from the target domain, even if limited, ismore beneficial.</description><author>Kundan Krishna, Prakhar Gupta, Sanjana Ramprasad, Byron C. Wallace, Jeffrey P. Bigham, Zachary C. Lipton</author><pubDate>Mon, 04 Dec 2023 15:53:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14296v2</guid></item><item><title>Enabling Non-Linear Quantum Operations through Variational Quantum Splines</title><link>http://arxiv.org/abs/2303.04788v3</link><description>The postulates of quantum mechanics impose only unitary transformations onquantum states, which is a severe limitation for quantum machine learningalgorithms. Quantum Splines (QSplines) have recently been proposed toapproximate quantum activation functions to introduce non-linearity in quantumalgorithms. However, QSplines make use of the HHL as a subroutine and require afault-tolerant quantum computer to be correctly implemented. This work proposesthe Generalised Hybrid Quantum Splines (GHQSplines), a novel method forapproximating non-linear quantum activation functions using hybridquantum-classical computation. The GHQSplines overcome the highly demandingrequirements of the original QSplines in terms of quantum hardware and can beimplemented using near-term quantum computers. Furthermore, the proposed methodrelies on a flexible problem representation for non-linear approximation and itis suitable to be embedded in existing quantum neural network architectures. Inaddition, we provide a practical implementation of the GHQSplines usingPennylane and show that our model outperforms the original QSplines in terms ofquality of fitting.</description><author>Matteo Antonio Inajetovic, Filippo Orazi, Antonio Macaluso, Stefano Lodi, Claudio Sartori</author><pubDate>Mon, 04 Dec 2023 15:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04788v3</guid></item><item><title>Efficient Deep Speech Understanding at the Edge</title><link>http://arxiv.org/abs/2311.17065v2</link><description>In contemporary speech understanding (SU), a sophisticated pipeline isemployed, encompassing the ingestion of streaming voice input. The pipelineexecutes beam search iteratively, invoking a deep neural network to generatetentative outputs (referred to as hypotheses) in an autoregressive manner.Periodically, the pipeline assesses attention and Connectionist TemporalClassification (CTC) scores. This paper aims to enhance SU performance on edge devices with limitedresources. Adopting a hybrid strategy, our approach focuses on acceleratingon-device execution and offloading inputs surpassing the device's capacity.While this approach is established, we tackle SU's distinctive challengesthrough innovative techniques: (1) Late Contextualization: This involves theparallel execution of a model's attentive encoder during input ingestion. (2)Pilot Inference: Addressing temporal load imbalances in the SU pipeline, thistechnique aims to mitigate them effectively. (3) Autoregression Offramps:Decisions regarding offloading are made solely based on hypotheses, presentinga novel approach. These techniques are designed to seamlessly integrate with existing speechmodels, pipelines, and frameworks, offering flexibility for independent orcombined application. Collectively, they form a hybrid solution for edge SU.Our prototype, named XYZ, has undergone testing on Arm platforms featuring 6 to8 cores, demonstrating state-of-the-art accuracy. Notably, it achieves a 2xreduction in end-to-end latency and a corresponding 2x decrease in offloadingrequirements.</description><author>Rongxiang Wang, Felix Xiaozhu Lin</author><pubDate>Mon, 04 Dec 2023 15:37:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17065v2</guid></item><item><title>On skip connections and normalisation layers in deep optimisation</title><link>http://arxiv.org/abs/2210.05371v4</link><description>We introduce a general theoretical framework, designed for the study ofgradient optimisation of deep neural networks, that encompasses ubiquitousarchitecture choices including batch normalisation, weight normalisation andskip connections. Our framework determines the curvature and regularityproperties of multilayer loss landscapes in terms of their constituent layers,thereby elucidating the roles played by normalisation layers and skipconnections in globalising these properties. We then demonstrate the utility ofthis framework in two respects. First, we give the only proof of which we areaware that a class of deep neural networks can be trained using gradientdescent to global optima even when such optima only exist at infinity, as isthe case for the cross-entropy cost. Second, we identify a novel causalmechanism by which skip connections accelerate training, which we verifypredictively with ResNets on MNIST, CIFAR10, CIFAR100 and ImageNet.</description><author>Lachlan Ewen MacDonald, Jack Valmadre, Hemanth Saratchandran, Simon Lucey</author><pubDate>Mon, 04 Dec 2023 15:37:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05371v4</guid></item><item><title>Automatic Textual Normalization for Hate Speech Detection</title><link>http://arxiv.org/abs/2311.06851v3</link><description>Social media data is a valuable resource for research, yet it contains a widerange of non-standard words (NSW). These irregularities hinder the effectiveoperation of NLP tools. Current state-of-the-art methods for the Vietnameselanguage address this issue as a problem of lexical normalization, involvingthe creation of manual rules or the implementation of multi-staged deeplearning frameworks, which necessitate extensive efforts to craft intricaterules. In contrast, our approach is straightforward, employing solely asequence-to-sequence (Seq2Seq) model. In this research, we provide a datasetfor textual normalization, comprising 2,181 human-annotated comments with aninter-annotator agreement of 0.9014. By leveraging the Seq2Seq model fortextual normalization, our results reveal that the accuracy achieved fallsslightly short of 70%. Nevertheless, textual normalization enhances theaccuracy of the Hate Speech Detection (HSD) task by approximately 2%,demonstrating its potential to improve the performance of complex NLP tasks.Our dataset is accessible for research purposes.</description><author>Anh Thi-Hoang Nguyen, Dung Ha Nguyen, Nguyet Thi Nguyen, Khanh Thanh-Duy Ho, Kiet Van Nguyen</author><pubDate>Mon, 04 Dec 2023 15:34:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06851v3</guid></item></channel></rss>