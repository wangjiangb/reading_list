<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 30 May 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>X-VILA: Cross-Modality Alignment for Large Language Model</title><link>http://arxiv.org/abs/2405.19335v1</link><description>We introduce X-VILA, an omni-modality model designed to extend thecapabilities of large language models (LLMs) by incorporating image, video, andaudio modalities. By aligning modality-specific encoders with LLM inputs anddiffusion decoders with LLM outputs, X-VILA achieves cross-modalityunderstanding, reasoning, and generation. To facilitate this cross-modalityalignment, we curate an effective interleaved any-to-any modalityinstruction-following dataset. Furthermore, we identify a significant problemwith the current cross-modality alignment method, which results in visualinformation loss. To address the issue, we propose a visual alignment mechanismwith a visual embedding highway module. We then introduce a resource-efficientrecipe for training X-VILA, that exhibits proficiency in any-to-any modalityconversation, surpassing previous approaches by large margins. X-VILA alsoshowcases emergent properties across modalities even in the absence of similartraining data. The project will be made open-source.</description><author>Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, Hongxu Yin</author><pubDate>Wed, 29 May 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19335v1</guid></item><item><title>LLMs Meet Multimodal Generation and Editing: A Survey</title><link>http://arxiv.org/abs/2405.19334v1</link><description>With the recent advancement in large language models (LLMs), there is agrowing interest in combining LLMs with multimodal learning. Previous surveysof multimodal large language models (MLLMs) mainly focus on understanding. Thissurvey elaborates on multimodal generation across different domains, includingimage, video, 3D, and audio, where we highlight the notable advancements withmilestone works in these fields. Specifically, we exhaustively investigate thekey technical components behind methods and multimodal datasets utilized inthese studies. Moreover, we dig into tool-augmented multimodal agents that canuse existing generative models for human-computer interaction. Lastly, we alsocomprehensively discuss the advancement in AI safety and investigate emergingapplications as well as future prospects. Our work provides a systematic andinsightful overview of multimodal generation, which is expected to advance thedevelopment of Artificial Intelligence for Generative Content (AIGC) and worldmodels. A curated list of all related papers can be found athttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation</description><author>Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, Jifeng Dai, Yong Zhang, Wei Xue, Qifeng Liu, Yike Guo, Qifeng Chen</author><pubDate>Wed, 29 May 2024 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19334v1</guid></item><item><title>Multi-Modal Generative Embedding Model</title><link>http://arxiv.org/abs/2405.19333v1</link><description>Most multi-modal tasks can be formulated into problems of either generationor embedding. Existing models usually tackle these two types of problems bydecoupling language modules into a text decoder for generation, and a textencoder for embedding. To explore the minimalism of multi-modal paradigms, weattempt to achieve only one model per modality in this work. We propose aMulti-Modal Generative Embedding Model (MM-GEM), whereby the generative andembedding objectives are encapsulated in one Large Language Model. We alsopropose a PoolAggregator to boost efficiency and enable the ability offine-grained embedding and generation. A surprising finding is that these twoobjectives do not significantly conflict with each other. For example, MM-GEMinstantiated from ViT-Large and TinyLlama shows competitive performance onbenchmarks for multimodal embedding models such as cross-modal retrieval andzero-shot classification, while has good ability of image captioning.Additionally, MM-GEM can seamlessly execute region-level image captiongeneration and retrieval tasks. Besides, the advanced text model in MM-GEMbrings over 5% improvement in Recall@1 for long text and image retrieval.</description><author>Feipeng Ma, Hongwei Xue, Guangting Wang, Yizhou Zhou, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, Xiaoyan Sun</author><pubDate>Wed, 29 May 2024 18:59:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19333v1</guid></item><item><title>Self-Exploring Language Models: Active Preference Elicitation for Online Alignment</title><link>http://arxiv.org/abs/2405.19332v1</link><description>Preference optimization, particularly through Reinforcement Learning fromHuman Feedback (RLHF), has achieved significant success in aligning LargeLanguage Models (LLMs) to adhere to human intentions. Unlike offline alignmentwith a fixed dataset, online feedback collection from humans or AI on modelgenerations typically leads to more capable reward models and better-alignedLLMs through an iterative process. However, achieving a globally accuratereward model requires systematic exploration to generate diverse responses thatspan the vast space of natural language. Random sampling from standardreward-maximizing LLMs alone is insufficient to fulfill this requirement. Toaddress this issue, we propose a bilevel objective optimistically biasedtowards potentially high-reward responses to actively exploreout-of-distribution regions. By solving the inner-level problem with thereparameterized reward function, the resulting algorithm, named Self-ExploringLanguage Models (SELM), eliminates the need for a separate RM and iterativelyupdates the LLM with a straightforward objective. Compared to Direct PreferenceOptimization (DPO), the SELM objective reduces indiscriminate favor of unseenextrapolations and enhances exploration efficiency. Our experimental resultsdemonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instructmodels, SELM significantly boosts the performance on instruction-followingbenchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standardacademic benchmarks in different settings. Our code and models are available athttps://github.com/shenao-zhang/SELM.</description><author>Shenao Zhang, Donghan Yu, Hiteshi Sharma, Ziyi Yang, Shuohang Wang, Hany Hassan, Zhaoran Wang</author><pubDate>Wed, 29 May 2024 18:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19332v1</guid></item><item><title>NPGA: Neural Parametric Gaussian Avatars</title><link>http://arxiv.org/abs/2405.19331v1</link><description>The creation of high-fidelity, digital versions of human heads is animportant stepping stone in the process of further integrating virtualcomponents into our everyday lives. Constructing such avatars is a challengingresearch problem, due to a high demand for photo-realism and real-timerendering performance. In this work, we propose Neural Parametric GaussianAvatars (NPGA), a data-driven approach to create high-fidelity, controllableavatars from multi-view video recordings. We build our method around 3DGaussian Splatting for its highly efficient rendering and to inherit thetopological flexibility of point clouds. In contrast to previous work, wecondition our avatars' dynamics on the rich expression space of neuralparametric head models (NPHM), instead of mesh-based 3DMMs. To this end, wedistill the backward deformation field of our underlying NPHM into forwarddeformations which are compatible with rasterization-based rendering. Allremaining fine-scale, expression-dependent details are learned from themulti-view videos. To increase the representational capacity of our avatars, weaugment the canonical Gaussian point cloud using per-primitive latent featureswhich govern its dynamic behavior. To regularize this increased dynamicexpressivity, we propose Laplacian terms on the latent features and predicteddynamics. We evaluate our method on the public NeRSemble dataset, demonstratingthat NPGA significantly outperforms the previous state-of-the-art avatars onthe self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurateanimation capabilities from real-world monocular videos.</description><author>Simon Giebenhain, Tobias Kirschstein, Martin Rünz, Lourdes Agapito, Matthias Nießner</author><pubDate>Wed, 29 May 2024 18:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19331v1</guid></item><item><title>MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series</title><link>http://arxiv.org/abs/2405.19327v1</link><description>Large Language Models (LLMs) have made great strides in recent years toachieve unprecedented performance across different tasks. However, due tocommercial interest, the most competitive models like GPT, Gemini, and Claudehave been gated behind proprietary interfaces without disclosing the trainingdetails. Recently, many institutions have open-sourced several strong LLMs likeLLaMA-3, comparable to existing closed-source LLMs. However, only the model'sweights are provided with most details (e.g., intermediate checkpoints,pre-training corpus, and training code, etc.) being undisclosed. To improve thetransparency of LLMs, the research community has formed to open-source trulyopen LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-trainingcorpus and training code) are being provided. These models have greatlyadvanced the scientific study of these large models including their strengths,weaknesses, biases and risks. However, we observe that the existing truly openLLMs on reasoning, knowledge, and coding tasks are still inferior to existingstate-of-the-art LLMs with similar model sizes. To this end, we open-sourceMAP-Neo, a highly capable and transparent bilingual language model with 7Bparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is thefirst fully open-sourced bilingual LLM with comparable performance compared toexisting state-of-the-art LLMs. Moreover, we open-source all details toreproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaningpipeline, checkpoints, and well-optimized training/evaluation framework areprovided. Finally, we hope our MAP-Neo will enhance and strengthen the openresearch community and inspire more innovations and creativities to facilitatethe further improvements of LLMs.</description><author>Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen</author><pubDate>Wed, 29 May 2024 18:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19327v1</guid></item><item><title>Code Simulation Challenges for Large Language Models</title><link>http://arxiv.org/abs/2401.09074v3</link><description>Many reasoning, planning, and problem-solving tasks share an intrinsicalgorithmic nature: correctly simulating each step is a sufficient condition tosolve them correctly. This work studies to what extent Large Language Models(LLMs) can simulate coding and algorithmic tasks to provide insights intogeneral capabilities in such algorithmic reasoning tasks. We introducebenchmarks for straight-line programs, code that contains critical paths, andapproximate and redundant instructions. We further assess the simulationcapabilities of LLMs with sorting algorithms and nested loops and show that aroutine's computational complexity directly affects an LLM's ability tosimulate its execution. While the most powerful LLMs exhibit relatively strongsimulation capabilities, the process is fragile, seems to rely heavily onpattern recognition, and is affected by memorisation. We propose a noveloff-the-shelf prompting method, Chain of Simulation (CoSm), which instructsLLMs to simulate code execution line by line/follow the computation pattern ofcompilers. CoSm efficiently helps LLMs reduce memorisation and shallow patternrecognition while improving simulation performance. We consider the success ofCoSm in code simulation to be inspirational for other general routinesimulation reasoning tasks.</description><author>Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin, Samuele Marro, Anthony Cohn, Nigel Shadbolt, Michael Wooldridge</author><pubDate>Wed, 29 May 2024 18:56:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09074v3</guid></item><item><title>Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models</title><link>http://arxiv.org/abs/2405.19326v1</link><description>In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentationfor parts searching and localization for objects, which is a new paradigm to 3Dsegmentation that transcends limitations for previous category-specific 3Dsemantic segmentation, 3D instance segmentation, and open-vocabulary 3Dsegmentation. We design a simple baseline method, Reasoning3D, with thecapability to understand and execute complex commands for (fine-grained)segmenting specific parts for 3D meshes with contextual awareness and reasonedanswers for interactive segmentation. Specifically, Reasoning3D leverages anoff-the-shelf pre-trained 2D segmentation network, powered by Large LanguageModels (LLMs), to interpret user input queries in a zero-shot manner. Previousresearch have shown that extensive pre-training endows foundation models withprior world knowledge, enabling them to comprehend complex commands, acapability we can harness to "segment anything" in 3D with limited 3D datasets(source efficient). Experimentation reveals that our approach is generalizableand can effectively localize and highlight parts of 3D objects (in 3D mesh)based on implicit textual queries, including these articulated 3d objects andreal-world scanned data. Our method can also generate natural languageexplanations corresponding to these 3D models and the decomposition. Moreover,our training-free approach allows rapid deployment and serves as a viableuniversal baseline for future research of part-level 3d (semantic) objectunderstanding in various fields including robotics, object manipulation, partassembly, autonomous driving applications, augment reality and virtual reality(AR/VR), and medical applications. The code, the model weight, the deploymentguide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/</description><author>Tianrun Chen, Chunan Yu, Jing Li, Jianqi Zhang, Lanyun Zhu, Deyi Ji, Yong Zhang, Ying Zang, Zejian Li, Lingyun Sun</author><pubDate>Wed, 29 May 2024 18:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19326v1</guid></item><item><title>Nearest Neighbor Speculative Decoding for LLM Generation and Attribution</title><link>http://arxiv.org/abs/2405.19325v1</link><description>Large language models (LLMs) often hallucinate and lack the ability toprovide attribution for their generations. Semi-parametric LMs, such as kNN-LM,approach these limitations by refining the output of an LM for a given promptusing its nearest neighbor matches in a non-parametric data store. However,these models often exhibit slow inference speeds and produce non-fluent texts.In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), anovel semi-parametric language modeling approach that is capable ofincorporating real-world text spans of arbitrary length into the LM generationsand providing attribution to their sources. NEST performs token-level retrievalat each inference step to compute a semi-parametric mixture distribution andidentify promising span continuations in a corpus. It then uses an approximatespeculative decoding procedure that accepts a prefix of the retrieved span orgenerates a new token. NEST significantly enhances the generation quality andattribution rate of the base LM across a variety of knowledge-intensive tasks,surpassing the conventional kNN-LM method and performing competitively within-context retrieval augmentation. In addition, NEST substantially improves thegeneration speed, achieving a 1.8x speedup in inference time when applied toLlama-2-Chat 70B.</description><author>Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, Xi Victoria Lin</author><pubDate>Wed, 29 May 2024 18:55:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19325v1</guid></item><item><title>Center-Based Relaxed Learning Against Membership Inference Attacks</title><link>http://arxiv.org/abs/2404.17674v2</link><description>Membership inference attacks (MIAs) are currently considered one of the mainprivacy attack strategies, and their defense mechanisms have also beenextensively explored. However, there is still a gap between the existingdefense approaches and ideal models in performance and deployment costs. Inparticular, we observed that the privacy vulnerability of the model is closelycorrelated with the gap between the model's data-memorizing ability andgeneralization ability. To address this, we propose a new architecture-agnostictraining paradigm called center-based relaxed learning (CRL), which is adaptiveto any classification model and provides privacy preservation by sacrificing aminimal or no loss of model generalizability. We emphasize that CRL can bettermaintain the model's consistency between member and non-member data. Throughextensive experiments on standard classification datasets, we empirically showthat this approach exhibits comparable performance without requiring additionalmodel capacity or data costs.</description><author>Xingli Fang, Jung-Eun Kim</author><pubDate>Wed, 29 May 2024 18:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17674v2</guid></item><item><title>Are Large Language Models Chameleons?</title><link>http://arxiv.org/abs/2405.19323v1</link><description>Do large language models (LLMs) have their own worldviews and personalitytendencies? Simulations in which an LLM was asked to answer subjectivequestions were conducted more than 1 million times. Comparison of the responsesfrom different LLMs with real data from the European Social Survey (ESS)suggests that the effect of prompts on bias and variability is fundamental,highlighting major cultural, age, and gender biases. Methods for measuring thedifference between LLMs and survey data are discussed, such as calculatingweighted means and a new proposed measure inspired by Jaccard similarity. Weconclude that it is important to analyze the robustness and variability ofprompts before using LLMs to model individual decisions or collective behavior,as their imitation abilities are approximate at best.</description><author>Mingmeng Geng, Sihong He, Roberto Trotta</author><pubDate>Wed, 29 May 2024 18:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19323v1</guid></item><item><title>DGD: Dynamic 3D Gaussians Distillation</title><link>http://arxiv.org/abs/2405.19321v1</link><description>We tackle the task of learning dynamic 3D semantic radiance fields given asingle monocular video as input. Our learned semantic radiance field capturesper-point semantics as well as color and geometric properties for a dynamic 3Dscene, enabling the generation of novel views and their correspondingsemantics. This enables the segmentation and tracking of a diverse set of 3Dsemantic entities, specified using a simple and intuitive interface thatincludes a user click or a text prompt. To this end, we present DGD, a unified3D representation for both the appearance and semantics of a dynamic 3D scene,building upon the recently proposed dynamic 3D Gaussians representation. Ourrepresentation is optimized over time with both color and semantic information.Key to our method is the joint optimization of the appearance and semanticattributes, which jointly affect the geometric properties of the scene. Weevaluate our approach in its ability to enable dense semantic 3D objecttracking and demonstrate high-quality results that are fast to render, for adiverse set of scenes. Our project webpage is available onhttps://isaaclabe.github.io/DGD-Website/</description><author>Isaac Labe, Noam Issachar, Itai Lang, Sagie Benaim</author><pubDate>Wed, 29 May 2024 18:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19321v1</guid></item><item><title>Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF</title><link>http://arxiv.org/abs/2405.19320v1</link><description>Reinforcement learning from human feedback (RLHF) has demonstrated greatpromise in aligning large language models (LLMs) with human preference.Depending on the availability of preference data, both online and offline RLHFare active areas of investigation. A key bottleneck is understanding how toincorporate uncertainty estimation in the reward function learned from thepreference data for RLHF, regardless of how the preference data is collected.While the principles of optimism or pessimism under uncertainty arewell-established in standard reinforcement learning (RL), apractically-implementable and theoretically-grounded form amenable to largelanguage models is not yet available, as standard techniques for constructingconfidence intervals become intractable under arbitrary policyparameterizations. In this paper, we introduce a unified approach to online and offline RLHF --value-incentivized preference optimization (VPO) -- which regularizes themaximum-likelihood estimate of the reward function with the corresponding valuefunction, modulated by a $\textit{sign}$ to indicate whether the optimism orpessimism is chosen. VPO also directly optimizes the policy with implicitreward modeling, and therefore shares a simpler RLHF pipeline similar to directpreference optimization. Theoretical guarantees of VPO are provided for bothonline and offline settings, matching the rates of their standard RLcounterparts. Moreover, experiments on text summarization and dialog verify thepracticality and effectiveness of VPO.</description><author>Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, Bo Dai</author><pubDate>Wed, 29 May 2024 18:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19320v1</guid></item><item><title>Adaptive Generalized Neyman Allocation: Local Asymptotic Minimax Optimal Best Arm Identification</title><link>http://arxiv.org/abs/2405.19317v1</link><description>This study investigates a local asymptotic minimax optimal strategy forfixed-budget best arm identification (BAI). We propose the Adaptive GeneralizedNeyman Allocation (AGNA) strategy and show that its worst-case upper bound ofthe probability of misidentifying the best arm aligns with the worst-case lowerbound under the small-gap regime, where the gap between the expected outcomesof the best and suboptimal arms is small. Our strategy corresponds to ageneralization of the Neyman allocation for two-armed bandits (Neyman, 1934;Kaufmann et al., 2016) and a refinement of existing strategies such as the onesproposed by Glynn &amp; Juneja (2004) and Shin et al. (2018). Compared to Komiyamaet al. (2022), which proposes a minimax rate-optimal strategy, our proposedstrategy has a tighter upper bound that exactly matches the lower bound,including the constant terms, by restricting the class of distributions to theclass of small-gap distributions. Our result contributes to the longstandingopen issue about the existence of asymptotically optimal strategies infixed-budget BAI, by presenting the local asymptotic minimax optimal strategy.</description><author>Masahiro Kato</author><pubDate>Wed, 29 May 2024 18:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19317v1</guid></item><item><title>Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention</title><link>http://arxiv.org/abs/2405.11616v2</link><description>In this paper, we introduce Era3D, a novel multiview diffusion method thatgenerates high-resolution multiview images from a single-view image. Despitesignificant advancements in multiview generation, existing methods still sufferfrom camera prior mismatch, inefficacy, and low resolution, resulting inpoor-quality multiview images. Specifically, these methods assume that theinput images should comply with a predefined camera type, e.g. a perspectivecamera with a fixed focal length, leading to distorted shapes when theassumption fails. Moreover, the full-image or dense multiview attention theyemploy leads to an exponential explosion of computational complexity as imageresolution increases, resulting in prohibitively expensive training costs. Tobridge the gap between assumption and reality, Era3D first proposes adiffusion-based camera prediction module to estimate the focal length andelevation of the input image, which allows our method to generate imageswithout shape distortions. Furthermore, a simple but efficient attention layer,named row-wise attention, is used to enforce epipolar priors in the multiviewdiffusion, facilitating efficient cross-view information fusion. Consequently,compared with state-of-the-art methods, Era3D generates high-quality multiviewimages with up to a 512*512 resolution while reducing computation complexity by12x times. Comprehensive experiments demonstrate that Era3D can reconstructhigh-quality and detailed 3D meshes from diverse single-view input images,significantly outperforming baseline multiview diffusion methods. Project page:https://penghtyx.github.io/Era3D/.</description><author>Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wenhan Luo, Ping Tan, Wenping Wang, Qifeng Liu, Yike Guo</author><pubDate>Wed, 29 May 2024 18:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11616v2</guid></item><item><title>Robust Preference Optimization through Reward Model Distillation</title><link>http://arxiv.org/abs/2405.19316v1</link><description>Language model (LM) post-training (or alignment) involves maximizing a rewardfunction that is derived from preference annotations. Direct PreferenceOptimization (DPO) is a popular offline alignment method that trains a policydirectly on preference data without the need to train a reward model or applyreinforcement learning. However, typical preference datasets have only asingle, or at most a few, annotation per preference pair, which causes DPO tooverconfidently assign rewards that trend towards infinite magnitude. Thisfrequently leads to degenerate policies, sometimes causing even theprobabilities of the preferred generations to go to zero. In this work, weanalyze this phenomenon and propose distillation to get a better proxy for thetrue preference distribution over generation pairs: we train the LM to produceprobabilities that match the distribution induced by a reward model trained onthe preference data. Moreover, to account for uncertainty in the reward modelwe are distilling from, we optimize against a family of reward models that, asa whole, is likely to include at least one reasonable proxy for the preferencedistribution. Our results show that distilling from such a family of rewardmodels leads to improved robustness to distribution shift in preferenceannotations, while preserving the simple supervised nature of DPO.</description><author>Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, Jonathan Berant</author><pubDate>Wed, 29 May 2024 18:39:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19316v1</guid></item><item><title>Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation</title><link>http://arxiv.org/abs/2402.11493v2</link><description>In recent years, substantial advancements have been made in the developmentof large language models, achieving remarkable performance across diversetasks. To evaluate the knowledge ability of language models, previous studieshave proposed lots of benchmarks based on question-answering pairs. We arguethat it is not reliable and comprehensive to evaluate language models with afixed question or limited paraphrases as the query, since language models aresensitive to prompt. Therefore, we introduce a novel concept named knowledgeboundary to encompass both prompt-agnostic and prompt-sensitive knowledgewithin language models. Knowledge boundary avoids prompt sensitivity inlanguage model evaluations, rendering them more dependable and robust. Toexplore the knowledge boundary for a given model, we propose projected gradientdescent method with semantic constraints, a new algorithm designed to identifythe optimal prompt for each piece of knowledge. Experiments demonstrate asuperior performance of our algorithm in computing the knowledge boundarycompared to existing methods. Furthermore, we evaluate the ability of multiplelanguage models in several domains with knowledge boundary.</description><author>Xunjian Yin, Xu Zhang, Jie Ruan, Xiaojun Wan</author><pubDate>Wed, 29 May 2024 18:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11493v2</guid></item><item><title>Matryoshka Query Transformer for Large Vision-Language Models</title><link>http://arxiv.org/abs/2405.19315v1</link><description>Large Vision-Language Models (LVLMs) typically encode an image into a fixednumber of visual tokens (e.g., 576) and process these tokens with a languagemodel. Despite their strong performance, LVLMs face challenges in adapting tovarying computational constraints. This raises the question: can we achieveflexibility in the number of visual tokens to suit different tasks andcomputational resources? We answer this with an emphatic yes. Inspired byMatryoshka Representation Learning, we introduce the Matryoshka QueryTransformer (MQT), capable of encoding an image into m visual tokens duringinference, where m can be any number up to a predefined maximum. This isachieved by employing a query transformer with M latent query tokens tocompress the visual embeddings. During each training step, we randomly select m&lt;= M latent query tokens and train the model using only these first m tokens,discarding the rest. Combining MQT with LLaVA, we train a single model once,and flexibly and drastically reduce the number of inference-time visual tokenswhile maintaining similar or better performance compared to trainingindependent models for each number of tokens. Our model, MQT-LLAVA, matchesLLaVA-1.5 performance across 11 benchmarks using a maximum of 256 tokensinstead of LLaVA's fixed 576. Reducing to 16 tokens (8x less TFLOPs) onlysacrifices the performance by 2.4 points on MMBench. On certain tasks such asScienceQA and MMMU, we can even go down to only 2 visual tokens withperformance drops of just 3% and 6% each. Our exploration of the trade-offbetween the accuracy and computational cost brought about by the number ofvisual tokens facilitates future research to achieve the best of both worlds.</description><author>Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, Kai-Wei Chang</author><pubDate>Wed, 29 May 2024 18:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19315v1</guid></item><item><title>Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice</title><link>http://arxiv.org/abs/2405.19313v1</link><description>The observed similarities in the behavior of humans and Large Language Models(LLMs) have prompted researchers to consider the potential of using LLMs asmodels of human cognition. However, several significant challenges must beaddressed before LLMs can be legitimately regarded as cognitive models. Forinstance, LLMs are trained on far more data than humans typically encounter,and may have been directly trained on human data in specific cognitive tasks oraligned with human preferences. Consequently, the origins of these behavioralsimilarities are not well understood. In this paper, we propose a novel way toenhance the utility of LLMs as cognitive models. This approach involves (i)leveraging computationally equivalent tasks that both an LLM and a rationalagent need to master for solving a cognitive problem and (ii) examining thespecific task distributions required for an LLM to exhibit human-likebehaviors. We apply this approach to decision-making -- specifically risky andintertemporal choice -- where the key computationally equivalent task is thearithmetic of expected value calculations. We show that an LLM pretrained on anecologically valid arithmetic dataset, which we call Arithmetic-GPT, predictshuman behavior better than many traditional cognitive models. Pretraining LLMson ecologically valid arithmetic datasets is sufficient to produce a strongcorrespondence between these models and human decision-making. Our results alsosuggest that LLMs used as cognitive models should be carefully investigated viaablation studies of the pretraining data.</description><author>Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths</author><pubDate>Wed, 29 May 2024 18:37:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19313v1</guid></item><item><title>Causal Inference from Slowly Varying Nonstationary Processes</title><link>http://arxiv.org/abs/2405.06902v2</link><description>Causal inference from observational data following the restricted structuralcausal models (SCM) framework hinges largely on the asymmetry between cause andeffect from the data generating mechanisms, such as non-Gaussianity ornon-linearity. This methodology can be adapted to stationary time series, yetinferring causal relationships from nonstationary time series remains achallenging task. In this work, we propose a new class of restricted SCM, via atime-varying filter and stationary noise, and exploit the asymmetry fromnonstationarity for causal identification in both bivariate and networksettings. We propose efficient procedures by leveraging powerful estimates ofthe bivariate evolutionary spectra for slowly varying processes. Varioussynthetic and real datasets that involve high-order and non-smooth filters areevaluated to demonstrate the effectiveness of our proposed methodology.</description><author>Kang Du, Yu Xiang</author><pubDate>Wed, 29 May 2024 18:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06902v2</guid></item><item><title>Real-Time Environment Condition Classification for Autonomous Vehicles</title><link>http://arxiv.org/abs/2405.19305v1</link><description>Current autonomous driving technologies are being rolled out in geo-fencedareas with well-defined operation conditions such as time of operation, area,weather conditions and road conditions. In this way, challenging conditions asadverse weather, slippery road or densely-populated city centers can beexcluded. In order to lift the geo-fenced restriction and allow a more dynamicavailability of autonomous driving functions, it is necessary for the vehicleto autonomously perform an environment condition assessment in real time toidentify when the system cannot operate safely and either stop operation orrequire the resting passenger to take control. In particular, adverse-weatherchallenges are a fundamental limitation as sensor performance degeneratesquickly, prohibiting the use of sensors such as cameras to locate and monitorroad signs, pedestrians or other vehicles. To address this issue, we train adeep learning model to identify outdoor weather and dangerous road conditions,enabling a quick reaction to new situations and environments. We achieve thisby introducing an improved taxonomy and label hierarchy for a state-of-the-artadverse-weather dataset, relabelling it with a novel semi-automated labelingpipeline. Using the novel proposed dataset and hierarchy, we train RECNet, adeep learning model for the classification of environment conditions from asingle RGB frame. We outperform baseline models by relative 16% in F1- Score,while maintaining a real-time capable performance of 20 Hz.</description><author>Marco Introvigne, Andrea Ramazzina, Stefanie Walz, Dominik Scheuble, Mario Bijelic</author><pubDate>Wed, 29 May 2024 18:29:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19305v1</guid></item><item><title>Measuring and Mitigating Bias for Tabular Datasets with Multiple Protected Attributes</title><link>http://arxiv.org/abs/2405.19300v1</link><description>Motivated by the recital (67) of the current corrigendum of the AI Act in theEuropean Union, we propose and present measures and mitigation strategies fordiscrimination in tabular datasets. We specifically focus on datasets thatcontain multiple protected attributes, such as nationality, age, and sex. Thismakes measuring and mitigating bias more challenging, as many existing methodsare designed for a single protected attribute. This paper comes with a twofoldcontribution: Firstly, new discrimination measures are introduced. Thesemeasures are categorized in our framework along with existing ones, guidingresearchers and practitioners in choosing the right measure to assess thefairness of the underlying dataset. Secondly, a novel application of anexisting bias mitigation method, FairDo, is presented. We show that thisstrategy can mitigate any type of discrimination, including intersectionaldiscrimination, by transforming the dataset. By conducting experiments onreal-world datasets (Adult, Bank, Compas), we demonstrate that de-biasingdatasets with multiple protected attributes is achievable. Further, thetransformed fair datasets do not compromise any of the tested machine learningmodels' performances significantly when trained on these datasets compared tothe original datasets. Discrimination was reduced by up to 83% in ourexperimentation. For most experiments, the disparity between protected groupswas reduced by at least 7% and 27% on average. Generally, the findings showthat the mitigation strategy used is effective, and this study contributes tothe ongoing discussion on the implementation of the European Union's AI Act.</description><author>Manh Khoi Duong, Stefan Conrad</author><pubDate>Wed, 29 May 2024 18:27:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19300v1</guid></item><item><title>Expert-Guided Extinction of Toxic Tokens for Debiased Generation</title><link>http://arxiv.org/abs/2405.19299v1</link><description>Large language models (LLMs) can elicit social bias during generations,especially when inference with toxic prompts. Controlling the sensitiveattributes in generation encounters challenges in data distribution,generalizability, and efficiency. Specifically, fine-tuning and retrievaldemand extensive unbiased corpus, while direct prompting requires meticulouslycurated instructions for correcting the output in multiple rounds of thoughtsbut poses challenges on memory and inference latency. In this work, we proposethe Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED)to eliminate the undesired harmful outputs for LLMs without the aforementionedrequirements. EXPOSED constructs a debiasing expert based on the abundant toxiccorpus to expose and elicit the potentially dangerous tokens. It then processesthe output to the LLMs and constructs a fair distribution by suppressing andattenuating the toxic tokens. EXPOSED is evaluated on fairness benchmarks overthree LLM families. Extensive experiments demonstrate that compared with otherbaselines, the proposed EXPOSED significantly reduces the potential social biaswhile balancing fairness and generation performance.</description><author>Xueyao Sun, Kaize Shi, Haoran Tang, Guandong Xu, Qing Li</author><pubDate>Wed, 29 May 2024 18:26:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19299v1</guid></item><item><title>Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare</title><link>http://arxiv.org/abs/2405.19298v1</link><description>While recent advancements in large multimodal models (LMMs) havesignificantly improved their abilities in image quality assessment (IQA)relying on absolute quality rating, how to transfer reliable relative qualitycomparison outputs to continuous perceptual quality scores remains largelyunexplored. To address this gap, we introduce Compare2Score-an all-aroundLMM-based no-reference IQA (NR-IQA) model, which is capable of producingqualitatively comparative responses and effectively translating these discretecomparative levels into a continuous quality score. Specifically, duringtraining, we present to generate scaled-up comparative instructions bycomparing images from the same IQA dataset, allowing for more flexibleintegration of diverse IQA datasets. Utilizing the established large-scaletraining corpus, we develop a human-like visual quality comparator. Duringinference, moving beyond binary choices, we propose a soft comparison methodthat calculates the likelihood of the test image being preferred over multiplepredefined anchor images. The quality score is further optimized by maximum aposteriori estimation with the resulting probability matrix. Extensiveexperiments on nine IQA datasets validate that the Compare2Score effectivelybridges text-defined comparative levels during training with converted singleimage quality score for inference, surpassing state-of-the-art IQA modelsacross diverse scenarios. Moreover, we verify that the probability-matrix-basedinference conversion not only improves the rating accuracy of Compare2Score butalso zero-shot general-purpose LMMs, suggesting its intrinsic effectiveness.</description><author>Hanwei Zhu, Haoning Wu, Yixuan Li, Zicheng Zhang, Baoliang Chen, Lingyu Zhu, Yuming Fang, Guangtao Zhai, Weisi Lin, Shiqi Wang</author><pubDate>Wed, 29 May 2024 18:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19298v1</guid></item><item><title>Neural Isometries: Taming Transformations for Equivariant ML</title><link>http://arxiv.org/abs/2405.19296v1</link><description>Real-world geometry and 3D vision tasks are replete with challengingsymmetries that defy tractable analytical expression. In this paper, weintroduce Neural Isometries, an autoencoder framework which learns to map theobservation space to a general-purpose latent space wherein encodings arerelated by isometries whenever their corresponding observations aregeometrically related in world space. Specifically, we regularize the latentspace such that maps between encodings preserve a learned inner product andcommute with a learned functional operator, in the same manner as rigid-bodytransformations commute with the Laplacian. This approach forms an effectivebackbone for self-supervised representation learning, and we demonstrate that asimple off-the-shelf equivariant network operating in the pre-trained latentspace can achieve results on par with meticulously-engineered, handcraftednetworks designed to handle complex, nonlinear symmetries. Furthermore,isometric maps capture information about the respective transformations inworld space, and we show that this allows us to regress camera poses directlyfrom the coefficients of the maps between encodings of adjacent views of ascene.</description><author>Thomas W. Mitchel, Michael Taylor, Vincent Sitzmann</author><pubDate>Wed, 29 May 2024 18:24:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19296v1</guid></item><item><title>3D Neural Edge Reconstruction</title><link>http://arxiv.org/abs/2405.19295v1</link><description>Real-world objects and environments are predominantly composed of edgefeatures, including straight lines and curves. Such edges are crucial elementsfor various applications, such as CAD modeling, surface meshing, lane mapping,etc. However, existing traditional methods only prioritize lines over curvesfor simplicity in geometric modeling. To this end, we introduce EMAP, a newmethod for learning 3D edge representations with a focus on both lines andcurves. Our method implicitly encodes 3D edge distance and direction inUnsigned Distance Functions (UDF) from multi-view edge maps. On top of thisneural representation, we propose an edge extraction algorithm that robustlyabstracts parametric 3D edges from the inferred edge points and theirdirections. Comprehensive evaluations demonstrate that our method achievesbetter 3D edge reconstruction on multiple challenging datasets. We further showthat our learned UDF field enhances neural surface reconstruction by capturingmore details.</description><author>Lei Li, Songyou Peng, Zehao Yu, Shaohui Liu, Rémi Pautrat, Xiaochuan Yin, Marc Pollefeys</author><pubDate>Wed, 29 May 2024 18:23:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19295v1</guid></item><item><title>Subgradient Convergence Implies Subdifferential Convergence on Weakly Convex Functions: With Uniform Rates Guarantees</title><link>http://arxiv.org/abs/2405.10289v3</link><description>In nonsmooth, nonconvex stochastic optimization, understanding the uniformconvergence of subdifferential mappings is crucial for analyzing stationarypoints of sample average approximations of risk as they approach the populationrisk. Yet, characterizing this convergence remains a fundamental challenge. This work introduces a novel perspective by connecting the uniformconvergence of subdifferential mappings to that of subgradient mappings asempirical risk converges to the population risk. We prove that, for stochasticweakly-convex objectives, and within any open set, a uniform bound on theconvergence of subgradients -- chosen arbitrarily from the correspondingsubdifferential sets -- translates to a uniform bound on the convergence of thesubdifferential sets itself, measured by the Hausdorff metric. Using this technique, we derive uniform convergence rates for subdifferentialsets of stochastic convex-composite objectives. Our results do not rely on keydistributional assumptions in the literature, which require the population andfinite sample subdifferentials to be continuous in the Hausdorff metric, yetstill provide tight convergence rates. These guarantees lead to new insightsinto the nonsmooth landscapes of such objectives within finite samples.</description><author>Feng Ruan</author><pubDate>Wed, 29 May 2024 18:20:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10289v3</guid></item><item><title>Integrating Multi-scale Contextualized Information for Byte-based Neural Machine Translation</title><link>http://arxiv.org/abs/2405.19290v1</link><description>Subword tokenization is a common method for vocabulary building in NeuralMachine Translation (NMT) models. However, increasingly complex tasks haverevealed its disadvantages. First, a vocabulary cannot be modified once it islearned, making it hard to adapt to new words. Second, in multilingualtranslation, the imbalance in data volumes across different languages spreadsto the vocabulary, exacerbating translations involving low-resource languages.While byte-based tokenization addresses these issues, byte-based modelsstruggle with the low information density inherent in UTF-8 byte sequences.Previous works enhance token semantics through local contextualization but failto select an appropriate contextualizing scope based on the input.Consequently, we propose the Multi-Scale Contextualization (MSC) method, whichlearns contextualized information of varying scales across different hiddenstate dimensions. It then leverages the attention module to dynamicallyintegrate the multi-scale contextualized information. Experiments show that MSCsignificantly outperforms subword-based and other byte-based methods in bothmultilingual and out-of-domain scenarios. Code can be found inhttps://github.com/ictnlp/Multiscale-Contextualization.</description><author>Langlin Huang, Yang Feng</author><pubDate>Wed, 29 May 2024 18:19:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19290v1</guid></item><item><title>MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection</title><link>http://arxiv.org/abs/2405.19285v1</link><description>Abstract Meaning Representation (AMR) is a semantic formalism that capturesthe core meaning of an utterance. There has been substantial work developingAMR corpora in English and more recently across languages, though the limitedsize of existing datasets and the cost of collecting more annotations areprohibitive. With both engineering and scientific questions in mind, weintroduce MASSIVE-AMR, a dataset with more than 84,000 text-to-graphannotations, currently the largest and most diverse of its kind: AMR graphs for1,685 information-seeking utterances mapped to 50+ typologically diverselanguages. We describe how we built our resource and its unique features beforereporting on experiments using large language models for multilingual AMR andSPARQL parsing as well as applying AMRs for hallucination detection in thecontext of knowledge base question answering, with results shedding light onpersistent issues using LLMs for structured parsing.</description><author>Michael Regan, Shira Wein, George Baker, Emilio Monti</author><pubDate>Wed, 29 May 2024 18:17:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19285v1</guid></item><item><title>Optimizing Foundation Model Inference on a Many-tiny-core Open-source RISC-V Platform</title><link>http://arxiv.org/abs/2405.19284v1</link><description>Transformer-based foundation models have become crucial for various domains,most notably natural language processing (NLP) or computer vision (CV). Thesemodels are predominantly deployed on high-performance GPUs or hardwiredaccelerators with highly customized, proprietary instruction sets. Until now,limited attention has been given to RISC-V-based general-purpose platforms. Inour work, we present the first end-to-end inference results of transformermodels on an open-source many-tiny-core RISC-V platform implementingdistributed Softmax primitives and leveraging ISA extensions for SIMDfloating-point operand streaming and instruction repetition, as well asspecialized DMA engines to minimize costly main memory accesses and to toleratetheir latency. We focus on two foundational transformer topologies,encoder-only and decoder-only models. For encoder-only models, we demonstrate aspeedup of up to 12.8x between the most optimized implementation and thebaseline version. We reach over 79% FPU utilization and 294 GFLOPS/W,outperforming State-of-the-Art (SoA) accelerators by more than 2x utilizing theHW platform while achieving comparable throughput per computational unit. Fordecoder-only topologies, we achieve 16.1x speedup in the Non-Autoregressive(NAR) mode and up to 35.6x speedup in the Autoregressive (AR) mode compared tothe baseline implementation. Compared to the best SoA dedicated accelerator, weachieve 2.04x higher FPU utilization.</description><author>Viviane Potocnik, Luca Colagrande, Tim Fischer, Luca Bertaccini, Daniele Jahier Pagliari, Alessio Burrello, Luca Benini</author><pubDate>Wed, 29 May 2024 18:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19284v1</guid></item><item><title>Lessons from the Trenches on Reproducible Evaluation of Language Models</title><link>http://arxiv.org/abs/2405.14782v2</link><description>Effective evaluation of language models remains an open challenge in NLP.Researchers and engineers face methodological issues such as the sensitivity ofmodels to evaluation setup, difficulty of proper comparisons across methods,and the lack of reproducibility and transparency. In this paper we draw onthree years of experience in evaluating large language models to provideguidance and lessons for researchers. First, we provide an overview of commonchallenges faced in language model evaluation. Second, we delineate bestpractices for addressing or lessening the impact of these challenges onresearch. Third, we present the Language Model Evaluation Harness (lm-eval): anopen source library for independent, reproducible, and extensible evaluation oflanguage models that seeks to address these issues. We describe the features ofthe library as well as case studies in which the library has been used toalleviate these methodological concerns.</description><author>Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Jeffrey Hsu, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata, François Yvon, Andy Zou</author><pubDate>Wed, 29 May 2024 18:15:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14782v2</guid></item><item><title>Programmable Motion Generation for Open-Set Motion Control Tasks</title><link>http://arxiv.org/abs/2405.19283v1</link><description>Character animation in real-world scenarios necessitates a variety ofconstraints, such as trajectories, key-frames, interactions, etc. Existingmethodologies typically treat single or a finite set of these constraint(s) asseparate control tasks. They are often specialized, and the tasks they addressare rarely extendable or customizable. We categorize these as solutions to theclose-set motion control problem. In response to the complexity of practicalmotion control, we propose and attempt to solve the open-set motion controlproblem. This problem is characterized by an open and fully customizable set ofmotion control tasks. To address this, we introduce a new paradigm,programmable motion generation. In this paradigm, any given motion control taskis broken down into a combination of atomic constraints. These constraints arethen programmed into an error function that quantifies the degree to which amotion sequence adheres to them. We utilize a pre-trained motion generationmodel and optimize its latent code to minimize the error function of thegenerated motion. Consequently, the generated motion not only inherits theprior of the generative model but also satisfies the required constraints.Experiments show that we can generate high-quality motions when addressing awide range of unseen tasks. These tasks encompass motion control by motiondynamics, geometric constraints, physical laws, interactions with scenes,objects or the character own body parts, etc. All of these are achieved in aunified approach, without the need for ad-hoc paired training data collectionor specialized network designs. During the programming of novel tasks, weobserved the emergence of new skills beyond those of the prior model. With theassistance of large language models, we also achieved automatic programming. Wehope that this work will pave the way for the motion control of general AIagents.</description><author>Hanchao Liu, Xiaohang Zhan, Shaoli Huang, Tai-Jiang Mu, Ying Shan</author><pubDate>Wed, 29 May 2024 18:14:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19283v1</guid></item><item><title>Understanding and Minimising Outlier Features in Neural Network Training</title><link>http://arxiv.org/abs/2405.19279v1</link><description>Outlier Features (OF) are neurons whose activation magnitudes significantlyexceed the average over a neural network's (NN) width. They are well known toemerge during standard transformer training and have the undesirable effect ofhindering quantisation in afflicted models. Despite their practical importance,little is known behind why OFs emerge during training, nor how one can minimisethem. Our work focuses on the above questions, first identifying severalquantitative metrics, such as the kurtosis over neuron activation norms, tomeasure OFs. With these metrics, we study how architectural and optimisationchoices influence OFs, and provide practical insights to minimise OFs duringtraining. As highlights, we emphasise the importance of controlling signalpropagation throughout training, and propose the Outlier Protected transformerblock, which removes standard Pre-Norm layers to mitigate OFs, without loss ofconvergence speed or training stability. Overall, our findings shed new lighton our understanding of, our ability to prevent, and the complexity of thisimportant facet in NN training dynamics.</description><author>Bobby He, Lorenzo Noci, Daniele Paliotta, Imanol Schlag, Thomas Hofmann</author><pubDate>Wed, 29 May 2024 18:11:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19279v1</guid></item><item><title>Ferrari: Federated Feature Unlearning via Optimizing Feature Sensitivity</title><link>http://arxiv.org/abs/2405.17462v2</link><description>The advent of Federated Learning (FL) highlights the practical necessity forthe 'right to be forgotten' for all clients, allowing them to request datadeletion from the machine learning model's service provider. This necessity hasspurred a growing demand for Federated Unlearning (FU). Feature unlearning hasgained considerable attention due to its applications in unlearning sensitivefeatures, backdoor features, and bias features. Existing methods employ theinfluence function to achieve feature unlearning, which is impractical for FLas it necessitates the participation of other clients in the unlearningprocess. Furthermore, current research lacks an evaluation of the effectivenessof feature unlearning. To address these limitations, we define featuresensitivity in the evaluation of feature unlearning according to Lipschitzcontinuity. This metric characterizes the rate of change or sensitivity of themodel output to perturbations in the input feature. We then propose aneffective federated feature unlearning framework called Ferrari, whichminimizes feature sensitivity. Extensive experimental results and theoreticalanalysis demonstrate the effectiveness of Ferrari across various featureunlearning scenarios, including sensitive, backdoor, and biased features.</description><author>Hanlin Gu, WinKent Ong, Chee Seng Chan, Lixin Fan</author><pubDate>Wed, 29 May 2024 18:11:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17462v2</guid></item><item><title>Deep Latent Variable Modeling of Physiological Signals</title><link>http://arxiv.org/abs/2405.19277v1</link><description>A deep latent variable model is a powerful method for capturing complexdistributions. These models assume that underlying structures, but unobserved,are present within the data. In this dissertation, we explore high-dimensionalproblems related to physiological monitoring using latent variable models.First, we present a novel deep state-space model to generate electricalwaveforms of the heart using optically obtained signals as inputs. This canbring about clinical diagnoses of heart disease via simple assessment throughwearable devices. Second, we present a brain signal modeling scheme thatcombines the strengths of probabilistic graphical models and deep adversariallearning. The structured representations can provide interpretability andencode inductive biases to reduce the data complexity of neural oscillations.The efficacy of the learned representations is further studied in epilepsyseizure detection formulated as an unsupervised learning problem. Third, wepropose a framework for the joint modeling of physiological measures andbehavior. Existing methods to combine multiple sources of brain data providedare limited. Direct analysis of the relationship between different types ofphysiological measures usually does not involve behavioral data. Our method canidentify the unique and shared contributions of brain regions to behavior andcan be used to discover new functions of brain regions. The success of theseinnovative computational methods would allow the translation of biomarkerfindings across species and provide insight into neurocognitive analysis innumerous biological studies and clinical diagnoses, as well as emergingconsumer applications.</description><author>Khuong Vo</author><pubDate>Wed, 29 May 2024 18:07:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19277v1</guid></item><item><title>A Recipe for Charge Density Prediction</title><link>http://arxiv.org/abs/2405.19276v1</link><description>In density functional theory, charge density is the core attribute of atomicsystems from which all chemical properties can be derived. Machine learningmethods are promising in significantly accelerating charge density prediction,yet existing approaches either lack accuracy or scalability. We propose arecipe that can achieve both. In particular, we identify three key ingredients:(1) representing the charge density with atomic and virtual orbitals (sphericalfields centered at atom/virtual coordinates); (2) using expressive andlearnable orbital basis sets (basis function for the spherical fields); and (3)using high-capacity equivariant neural network architecture. Our methodachieves state-of-the-art accuracy while being more than an order of magnitudefaster than existing methods. Furthermore, our method enables flexibleefficiency-accuracy trade-offs by adjusting the model/basis sizes.</description><author>Xiang Fu, Andrew Rosen, Kyle Bystrom, Rui Wang, Albert Musaelian, Boris Kozinsky, Tess Smidt, Tommi Jaakkola</author><pubDate>Wed, 29 May 2024 18:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19276v1</guid></item><item><title>Robust Emotion Recognition in Context Debiasing</title><link>http://arxiv.org/abs/2403.05963v2</link><description>Context-aware emotion recognition (CAER) has recently boosted the practicalapplications of affective computing techniques in unconstrained environments.Mainstream CAER methods invariably extract ensemble representations fromdiverse contexts and subject-centred characteristics to perceive the targetperson's emotional state. Despite advancements, the biggest challenge remainsdue to context bias interference. The harmful bias forces the models to rely onspurious correlations between background contexts and emotion labels inlikelihood estimation, causing severe performance bottlenecks and confoundingvaluable context priors. In this paper, we propose a counterfactual emotioninference (CLEF) framework to address the above issue. Specifically, we firstformulate a generalized causal graph to decouple the causal relationships amongthe variables in CAER. Following the causal graph, CLEF introduces anon-invasive context branch to capture the adverse direct effect caused by thecontext bias. During the inference, we eliminate the direct context effect fromthe total causal effect by comparing factual and counterfactual outcomes,resulting in bias mitigation and robust prediction. As a model-agnosticframework, CLEF can be readily integrated into existing methods, bringingconsistent performance gains.</description><author>Dingkang Yang, Kun Yang, Mingcheng Li, Shunli Wang, Shuaibing Wang, Lihua Zhang</author><pubDate>Wed, 29 May 2024 18:07:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05963v2</guid></item><item><title>Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning</title><link>http://arxiv.org/abs/2405.18386v2</link><description>Recent advances in text-to-music editing, which employ text queries to modifymusic (e.g.\ by changing its style or adjusting instrumental components),present unique challenges and opportunities for AI-assisted music creation.Previous approaches in this domain have been constrained by the necessity totrain specific editing models from scratch, which is both resource-intensiveand inefficient; other research uses large language models to predict editedmusic, resulting in imprecise audio reconstruction. To Combine the strengthsand address these limitations, we introduce Instruct-MusicGen, a novel approachthat finetunes a pretrained MusicGen model to efficiently follow editinginstructions such as adding, removing, or separating stems. Our approachinvolves a modification of the original MusicGen architecture by incorporatinga text fusion module and an audio fusion module, which allow the model toprocess instruction texts and audio inputs concurrently and yield the desirededited music. Remarkably, Instruct-MusicGen only introduces 8% new parametersto the original MusicGen model and only trains for 5K steps, yet it achievessuperior performance across all tasks compared to existing baselines, anddemonstrates performance comparable to the models trained for specific tasks.This advancement not only enhances the efficiency of text-to-music editing butalso broadens the applicability of music language models in dynamic musicproduction environments.</description><author>Yixiao Zhang, Yukara Ikemiya, Woosung Choi, Naoki Murata, Marco A. Martínez-Ramírez, Liwei Lin, Gus Xia, Wei-Hsiang Liao, Yuki Mitsufuji, Simon Dixon</author><pubDate>Wed, 29 May 2024 18:05:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18386v2</guid></item><item><title>Mitigating Disparate Impact of Differential Privacy in Federated Learning through Robust Clustering</title><link>http://arxiv.org/abs/2405.19272v1</link><description>Federated Learning (FL) is a decentralized machine learning (ML) approachthat keeps data localized and often incorporates Differential Privacy (DP) toenhance privacy guarantees. Similar to previous work on DP in ML, we observedthat differentially private federated learning (DPFL) introduces performancedisparities, particularly affecting minority groups. Recent work has attemptedto address performance fairness in vanilla FL through clustering, but thismethod remains sensitive and prone to errors, which are further exacerbated bythe DP noise in DPFL. To fill this gap, in this paper, we propose a novelclustered DPFL algorithm designed to effectively identify clients' clusters inhighly heterogeneous settings while maintaining high accuracy with DPguarantees. To this end, we propose to cluster clients based on both theirmodel updates and training loss values. Our proposed approach also addressesthe server's uncertainties in clustering clients' model updates by employinglarger batch sizes along with Gaussian Mixture Model (GMM) to alleviate theimpact of noise and potential clustering errors, especially inprivacy-sensitive scenarios. We provide theoretical analysis of theeffectiveness of our proposed approach. We also extensively evaluate ourapproach across diverse data distributions and privacy budgets and show itseffectiveness in mitigating the disparate impact of DP in FL settings with asmall computational cost.</description><author>Saber Malekmohammadi, Afaf Taik, Golnoosh Farnadi</author><pubDate>Wed, 29 May 2024 18:03:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19272v1</guid></item><item><title>Rich-Observation Reinforcement Learning with Continuous Latent Dynamics</title><link>http://arxiv.org/abs/2405.19269v1</link><description>Sample-efficiency and reliability remain major bottlenecks toward wideadoption of reinforcement learning algorithms in continuous settings withhigh-dimensional perceptual inputs. Toward addressing these challenges, weintroduce a new theoretical framework, RichCLD (Rich-Observation RL withContinuous Latent Dynamics), in which the agent performs control based onhigh-dimensional observations, but the environment is governed bylow-dimensional latent states and Lipschitz continuous dynamics. Our maincontribution is a new algorithm for this setting that is provably statisticallyand computationally efficient. The core of our algorithm is a newrepresentation learning objective; we show that prior representation learningschemes tailored to discrete dynamics do not naturally extend to the continuoussetting. Our new objective is amenable to practical implementation, andempirically, we find that it compares favorably to prior schemes in a standardevaluation protocol. We further provide several insights into the statisticalcomplexity of the RichCLD framework, in particular proving that certain notionsof Lipschitzness that admit sample-efficient learning in the absence of richobservations are insufficient in the rich-observation setting.</description><author>Yuda Song, Lili Wu, Dylan J. Foster, Akshay Krishnamurthy</author><pubDate>Wed, 29 May 2024 18:02:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19269v1</guid></item><item><title>LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition</title><link>http://arxiv.org/abs/2402.09989v4</link><description>Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodaltask that aims to identify named entities, entity types and their correspondingvisual regions. GMNER task exhibits two challenging properties: 1) The weakcorrelation between image-text pairs in social media results in a significantportion of named entities being ungroundable. 2) There exists a distinctionbetween coarse-grained referring expressions commonly used in similar tasks(e.g., phrase localization, referring expression comprehension) andfine-grained named entities. In this paper, we propose RiVEG, a unifiedframework that reformulates GMNER into a joint MNER-VE-VG task by leveraginglarge language models (LLMs) as a connecting bridge. This reformulation bringstwo benefits: 1) It maintains the optimal MNER performance and eliminates theneed for employing object detection methods to pre-extract regional features,thereby naturally addressing two major limitations of existing GMNER methods.2) The introduction of entity expansion expression and Visual Entailment (VE)module unifies Visual Grounding (VG) and Entity Grounding (EG). It enablesRiVEG to effortlessly inherit the Visual Entailment and Visual Groundingcapabilities of any current or prospective multimodal pretraining models.Extensive experiments demonstrate that RiVEG outperforms state-of-the-artmethods on the existing GMNER dataset and achieves absolute leads of 10.65%,6.21%, and 8.83% in all three subtasks.</description><author>Jinyuan Li, Han Li, Di Sun, Jiahao Wang, Wenkun Zhang, Zan Wang, Gang Pan</author><pubDate>Wed, 29 May 2024 17:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09989v4</guid></item><item><title>PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</title><link>http://arxiv.org/abs/2405.19266v1</link><description>Developing intelligent pediatric consultation systems offers promisingprospects for improving diagnostic efficiency, especially in China, wherehealthcare resources are scarce. Despite recent advances in Large LanguageModels (LLMs) for Chinese medicine, their performance is sub-optimal inpediatric applications due to inadequate instruction data and vulnerabletraining procedures. To address the above issues, this paper builds PedCorpus,a high-quality dataset of over 300,000 multi-task instructions from pediatrictextbooks, guidelines, and knowledge graph resources to fulfil diversediagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, thefirst Chinese pediatric LLM assistant built on a systematic and robust trainingpipeline. In the continuous pre-training phase, we introduce a hybridinstruction pre-training mechanism to mitigate the internal-injected knowledgeinconsistency of LLMs for medical domain adaptation. Immediately, thefull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate thegeneral medical knowledge schema into the models. After that, we devise adirect following preference optimization to enhance the generation ofpediatrician-like humanistic responses. In the parameter-efficient secondarySFT phase, a mixture of universal-specific experts strategy is presented toresolve the competency conflict between medical generalist and pediatricexpertise mastery. Extensive results based on the metrics, GPT-4, and doctorevaluations on distinct doctor downstream tasks show that PediatricsGPTconsistently outperforms previous Chinese medical LLMs. Our model and datasetwill be open-source for community development.</description><author>Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang</author><pubDate>Wed, 29 May 2024 17:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19266v1</guid></item><item><title>Efficient and Effective Time-Series Forecasting with Spiking Neural Networks</title><link>http://arxiv.org/abs/2402.01533v2</link><description>Spiking neural networks (SNNs), inspired by the spiking behavior ofbiological neurons, provide a unique pathway for capturing the intricacies oftemporal data. However, applying SNNs to time-series forecasting is challengingdue to difficulties in effective temporal alignment, complexities in encodingprocesses, and the absence of standardized guidelines for model selection. Inthis paper, we propose a framework for SNNs in time-series forecasting tasks,leveraging the efficiency of spiking neurons in processing temporalinformation. Through a series of experiments, we demonstrate that our proposedSNN-based approaches achieve comparable or superior results to traditionaltime-series forecasting methods on diverse benchmarks with much less energyconsumption. Furthermore, we conduct detailed analysis experiments to assessthe SNN's capacity to capture temporal dependencies within time-series data,offering valuable insights into its nuanced strengths and effectiveness inmodeling the intricate dynamics of temporal data. Our study contributes to theexpanding field of SNNs and offers a promising alternative for time-seriesforecasting tasks, presenting a pathway for the development of morebiologically inspired and temporally aware forecasting models. Our code isavailable at https://github.com/microsoft/SeqSNN.</description><author>Changze Lv, Yansen Wang, Dongqi Han, Xiaoqing Zheng, Xuanjing Huang, Dongsheng Li</author><pubDate>Wed, 29 May 2024 17:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01533v2</guid></item><item><title>AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data</title><link>http://arxiv.org/abs/2405.19265v1</link><description>Open-source Large Language Models (LLMs) and their specialized variants,particularly Code LLMs, have recently delivered impressive performance.However, previous Code LLMs are typically fine-tuned on single-source data withlimited quality and diversity, which may insufficiently elicit the potential ofpre-trained Code LLMs. In this paper, we present AlchemistCoder, a series ofCode LLMs with enhanced code generation and generalization capabilitiesfine-tuned on multi-source data. To achieve this, we pioneer to unveil inherentconflicts among the various styles and qualities in multi-source code corporaand introduce data-specific prompts with hindsight relabeling, termedAlchemistPrompts, to harmonize different data sources and instruction-responsepairs. Additionally, we propose incorporating the data construction processinto the fine-tuning data as code comprehension tasks, including instructionevolution, data filtering, and code review. Extensive experiments demonstratethat AlchemistCoder holds a clear lead among all models of the same size(6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasingthe efficacy of our method in refining instruction-following capabilities andadvancing the boundaries of code intelligence.</description><author>Zifan Song, Yudong Wang, Wenwei Zhang, Kuikun Liu, Chengqi Lyu, Demin Song, Qipeng Guo, Hang Yan, Dahua Lin, Kai Chen, Cairong Zhao</author><pubDate>Wed, 29 May 2024 17:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19265v1</guid></item><item><title>Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations</title><link>http://arxiv.org/abs/2405.18392v2</link><description>Scale has become a main ingredient in obtaining strong machine learningmodels. As a result, understanding a model's scaling properties is key toeffectively designing both the right training setup as well as futuregenerations of architectures. In this work, we argue that scale and trainingresearch has been needlessly complex due to reliance on the cosine schedule,which prevents training across different lengths for the same model size. Weinvestigate the training behavior of a direct alternative - constant learningrate and cooldowns - and find that it scales predictably and reliably similarto cosine. Additionally, we show that stochastic weight averaging yieldsimproved performance along the training trajectory, without additional trainingcosts, across different scales. Importantly, with these findings we demonstratethat scaling experiments can be performed with significantly reduced computeand GPU hours by utilizing fewer but reusable training runs. Our code isavailable at https://github.com/epfml/schedules-and-scaling.</description><author>Alexander Hägele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro Von Werra, Martin Jaggi</author><pubDate>Wed, 29 May 2024 17:56:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18392v2</guid></item><item><title>Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models</title><link>http://arxiv.org/abs/2405.19262v1</link><description>Large language models are usually fine-tuned to align with human preferences.However, fine-tuning a large language model can be challenging. In this work,we introduce $\textit{weak-to-strong search}$, framing the alignment of a largelanguage model as a test-time greedy search to maximize the log-likelihooddifference between small tuned and untuned models while sampling from thefrozen large model. This method serves both as (i) a compute-efficient modelup-scaling strategy that avoids directly tuning the large model and as (ii) aninstance of weak-to-strong generalization that enhances a strong model withweak test-time guidance. Empirically, we demonstrate the flexibility ofweak-to-strong search across different tasks. In controlled-sentimentgeneration and summarization, we use tuned and untuned $\texttt{gpt2}$s toeffectively improve the alignment of large models without additional training.Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0,we show that reusing off-the-shelf small model pairs (e.g.,$\texttt{zephyr-7b-beta}$ and its untuned version) can significantly improvethe length-controlled win rates of both white-box and black-box large modelsagainst $\texttt{gpt-4-turbo}$ (e.g., $34.4 \rightarrow 37.9$ for$\texttt{Llama-3-70B-Instruct}$ and $16.0 \rightarrow 20.1$ for$\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates$\approx 10.0$.</description><author>Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, Yu Qiao</author><pubDate>Wed, 29 May 2024 17:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19262v1</guid></item><item><title>Faster Cascades via Speculative Decoding</title><link>http://arxiv.org/abs/2405.19261v1</link><description>Cascades and speculative decoding are two common approaches to improvinglanguage models' inference efficiency. Both approaches involve interleavingmodels of different sizes, but via fundamentally distinct mechanisms: cascadesemploy a deferral rule that invokes the larger model only for "hard" inputs,while speculative decoding uses speculative execution to primarily invoke thelarger model in parallel verification mode. These mechanisms offer differentbenefits: empirically, cascades are often capable of yielding better qualitythan even the larger model, while theoretically, speculative decoding offers aguarantee of quality-neutrality. In this paper, we leverage the best of boththese approaches by designing new speculative cascading techniques thatimplement their deferral rule through speculative execution. We characterizethe optimal deferral rule for our speculative cascades, and employ a plug-inapproximation to the optimal rule. Through experiments with T5 models onbenchmark language tasks, we show that the proposed approach yields bettercost-quality trade-offs than cascading and speculative decoding baselines.</description><author>Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta, Aditya Krishna Menon, Sanjiv Kumar</author><pubDate>Wed, 29 May 2024 17:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19261v1</guid></item><item><title>Exploring Fairness in Educational Data Mining in the Context of the Right to be Forgotten</title><link>http://arxiv.org/abs/2405.16798v2</link><description>In education data mining (EDM) communities, machine learning has achievedremarkable success in discovering patterns and structures to tackle educationalchallenges. Notably, fairness and algorithmic bias have gained attention inlearning analytics of EDM. With the increasing demand for the right to beforgotten, there is a growing need for machine learning models to forgetsensitive data and its impact, particularly within the realm of EDM. Theparadigm of selective forgetting, also known as machine unlearning, has beenextensively studied to address this need by eliminating the influence ofspecific data from a pre-trained model without complete retraining. However,existing research assumes that interactive data removal operations areconducted in secure and reliable environments, neglecting potential maliciousunlearning requests to undermine the fairness of machine learning systems. Inthis paper, we introduce a novel class of selective forgetting attacks designedto compromise the fairness of learning models while maintaining theirpredictive accuracy, thereby preventing the model owner from detecting thedegradation in model performance. Additionally, we propose an innovativeoptimization framework for selective forgetting attacks, capable of generatingmalicious unlearning requests across various attack scenarios. We validate theeffectiveness of our proposed selective forgetting attacks on fairness throughextensive experiments using diverse EDM datasets.</description><author>Wei Qian, Aobo Chen, Chenxu Zhao, Yangyi Li, Mengdi Huai</author><pubDate>Wed, 29 May 2024 17:52:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16798v2</guid></item><item><title>Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks</title><link>http://arxiv.org/abs/2402.02036v2</link><description>Graph Neural Networks (GNNs) have become a building block in graph dataprocessing, with wide applications in critical domains. The growing needs todeploy GNNs in high-stakes applications necessitate explainability for users inthe decision-making processes. A popular paradigm for the explainability ofGNNs is to identify explainable subgraphs by comparing their labels with theones of original graphs. This task is challenging due to the substantialdistributional shift from the original graphs in the training set to the set ofexplainable subgraphs, which prevents accurate prediction of labels with thesubgraphs. To address it, in this paper, we propose a novel method thatgenerates proxy graphs for explainable subgraphs that are in the distributionof training data. We introduce a parametric method that employs graphgenerators to produce proxy graphs. A new training objective based oninformation theory is designed to ensure that proxy graphs not only adhere tothe distribution of training data but also preserve explanatory factors. Suchgenerated proxy graphs can be reliably used to approximate the predictions ofthe labels of explainable subgraphs. Empirical evaluations across variousdatasets demonstrate our method achieves more accurate explanations for GNNs.</description><author>Zhuomin Chen, Jiaxing Zhang, Jingchao Ni, Xiaoting Li, Yuchen Bian, Md Mezbahul Islam, Ananda Mohan Mondal, Hua Wei, Dongsheng Luo</author><pubDate>Wed, 29 May 2024 17:52:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02036v2</guid></item><item><title>UP5: Unbiased Foundation Model for Fairness-aware Recommendation</title><link>http://arxiv.org/abs/2305.12090v2</link><description>Recent advances in Foundation Models such as Large Language Models (LLMs)have propelled them to the forefront of Recommender Systems (RS). Despite theirutility, there is a growing concern that LLMs might inadvertently perpetuatesocietal stereotypes, resulting in unfair recommendations. Since fairness iscritical for RS as many users take it for decision-making and demandfulfillment, this paper focuses on user-side fairness for LLM-basedrecommendation where the users may require a recommender system to be fair onspecific sensitive features such as gender or age. In this paper, we dive intothe extent of unfairness exhibited by LLM-based recommender models based onboth T5 and LLaMA backbones, and discuss appropriate methods for promotingequitable treatment of users in LLM-based recommendation models. We introduce anovel Counterfactually-Fair-Prompt (CFP) method towards Unbiased FoundationmOdels (UFO) for fairness-aware LLM-based recommendation. Experiments areconducted on two real-world datasets, MovieLens-1M and Insurance, and comparedwith both matching-based and sequential-based fairness-aware recommendationmodels. Results show that CFP achieves better recommendation performance with ahigh level of fairness. Data and code are open-sourced athttps://github.com/agiresearch/UP5.</description><author>Wenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji, Yongfeng Zhang</author><pubDate>Wed, 29 May 2024 17:46:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12090v2</guid></item><item><title>HetCAN: A Heterogeneous Graph Cascade Attention Network with Dual-Level Awareness</title><link>http://arxiv.org/abs/2311.03275v2</link><description>Heterogeneous graph neural networks(HGNNs) have recently shown impressivecapability in modeling heterogeneous graphs that are ubiquitous in real-worldapplications. Most existing methods for heterogeneous graphs mainly learn nodeembeddings by stacking multiple convolutional or attentional layers, which canbe considered as capturing the high-order information from node-level aspect.However, different types of nodes in heterogeneous graphs have diversefeatures, it is also necessary to capture interactions among node features,namely the high-order information from feature-level aspect. In addition, mostmethods first align node features by mapping them into one same low-dimensionalspace, while they may lose some type information of nodes in this way. Toaddress these problems, in this paper, we propose a novel Heterogeneous graphCascade Attention Network (HetCAN) composed of multiple cascade blocks. Eachcascade block includes two components, the type-aware encoder and thedimension-aware encoder. Specifically, the type-aware encoder compensates forthe loss of node type information and aims to make full use of graphheterogeneity. The dimension-aware encoder is able to learn the feature-levelhigh-order information by capturing the interactions among node features. Withthe assistance of these components, HetCAN can comprehensively encodeinformation of node features, graph heterogeneity and graph structure in nodeembeddings. Extensive experiments demonstrate the superiority of HetCAN overadvanced competitors and also exhibit its efficiency and robustness.</description><author>Zeyuan Zhao, Qingqing Ge, Anfeng Cheng, Yiding Liu, Xiang Li, Shuaiqiang Wang</author><pubDate>Wed, 29 May 2024 17:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03275v2</guid></item><item><title>Weak Generative Sampler to Efficiently Sample Invariant Distribution of Stochastic Differential Equation</title><link>http://arxiv.org/abs/2405.19256v1</link><description>Sampling invariant distributions from an Ito diffusion process presents asignificant challenge in stochastic simulation. Traditional numerical solversfor stochastic differential equations require both a fine step size and alengthy simulation period, resulting in both biased and correlated samples.Current deep learning-based method solves the stationary Fokker--Planckequation to determine the invariant probability density function in form ofdeep neural networks, but they generally do not directly address the problem ofsampling from the computed density function. In this work, we introduce aframework that employs a weak generative sampler (WGS) to directly generateindependent and identically distributed (iid) samples induced by atransformation map derived from the stationary Fokker--Planck equation. Ourproposed loss function is based on the weak form of the Fokker--Planckequation, integrating normalizing flows to characterize the invariantdistribution and facilitate sample generation from the base distribution. Ourrandomized test function circumvents the need for mini-max optimization in thetraditional weak formulation. Distinct from conventional generative models, ourmethod neither necessitates the computationally intensive calculation of theJacobian determinant nor the invertibility of the transformation map. A crucialcomponent of our framework is the adaptively chosen family of test functions inthe form of Gaussian kernel functions with centres selected from the generateddata samples. Experimental results on several benchmark examples demonstratethe effectiveness of our method, which offers both low computational costs andexcellent capability in exploring multiple metastable states.</description><author>Zhiqiang Cai, Yu Cao, Yuanfei Huang, Xiang Zhou</author><pubDate>Wed, 29 May 2024 17:41:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19256v1</guid></item><item><title>Textureless Deformable Surface Reconstruction with Invisible Markers</title><link>http://arxiv.org/abs/2308.13678v2</link><description>Reconstructing and tracking deformable surface with little or no texture hasposed long-standing challenges. Fundamentally, the challenges stem fromtextureless surfaces lacking features for establishing cross-imagecorrespondences. In this work, we present a novel type of markers toproactively enrich the object's surface features, and thereby ease the 3Dsurface reconstruction and correspondence tracking. Our markers are made offluorescent dyes, visible only under the ultraviolet (UV) light and invisibleunder regular lighting condition. Leveraging the markers, we design amulti-camera system that captures surface deformation under the UV light andthe visible light in a time multiplexing fashion. Under the UV light, markerson the object emerge to enrich its surface texture, allowing high-quality 3Dshape reconstruction and tracking. Under the visible light, markers becomeinvisible, allowing us to capture the object's original untouched appearance.We perform experiments on various challenging scenes, including hand gestures,facial expressions, waving cloth, and hand-object interaction. In all thesecases, we demonstrate that our system is able to produce robust, high-quality3D reconstruction and tracking.</description><author>Xinyuan Li, Yu Guo, Yubei Tu, Yu Ji, Yanchen Liu, Jinwei Ye, Changxi Zheng</author><pubDate>Wed, 29 May 2024 17:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13678v2</guid></item><item><title>Towards Next-Generation Urban Decision Support Systems through AI-Powered Generation of Scientific Ontology using Large Language Models -- A Case in Optimizing Intermodal Freight Transportation</title><link>http://arxiv.org/abs/2405.19255v1</link><description>The incorporation of Artificial Intelligence (AI) models into variousoptimization systems is on the rise. Yet, addressing complex urban andenvironmental management problems normally requires in-depth domain science andinformatics expertise. This expertise is essential for deriving data andsimulation-driven for informed decision support. In this context, weinvestigate the potential of leveraging the pre-trained Large Language Models(LLMs). By adopting ChatGPT API as the reasoning core, we outline an integratedworkflow that encompasses natural language processing, methontology-basedprompt tuning, and transformers. This workflow automates the creation ofscenario-based ontology using existing research articles and technical manualsof urban datasets and simulations. The outcomes of our methodology areknowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL).These facilitate the development of urban decision support systems by enhancingthe data and metadata modeling, the integration of complex datasets, thecoupling of multi-domain simulation models, and the formulation ofdecision-making metrics and workflow. The feasibility of our methodology isevaluated through a comparative analysis that juxtaposes our AI-generatedontology with the well-known Pizza Ontology employed in tutorials for popularontology software (e.g., prot\'eg\'e). We close with a real-world case study ofoptimizing the complex urban system of multi-modal freight transportation bygenerating anthologies of various domain data and simulations to supportinformed decision-making.</description><author>Jose Tupayachi, Haowen Xu, Olufemi A. Omitaomu, Mustafa Can Camur, Aliza Sharmin, Xueping Li</author><pubDate>Wed, 29 May 2024 17:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19255v1</guid></item><item><title>ÚFAL LatinPipe at EvaLatin 2024: Morphosyntactic Analysis of Latin</title><link>http://arxiv.org/abs/2404.05839v2</link><description>We present LatinPipe, the winning submission to the EvaLatin 2024 DependencyParsing shared task. Our system consists of a fine-tuned concatenation of baseand large pre-trained LMs, with a dot-product attention head for parsing andsoftmax classification heads for morphology to jointly learn both dependencyparsing and morphological analysis. It is trained by sampling from sevenpublicly available Latin corpora, utilizing additional harmonization ofannotations to achieve a more unified annotation style. Before fine-tuning, wetrain the system for a few initial epochs with frozen weights. We also addadditional local relative contextualization by stacking the BiLSTM layers ontop of the Transformer(s). Finally, we ensemble output probabilitydistributions from seven randomly instantiated networks for the finalsubmission. The code is available athttps://github.com/ufal/evalatin2024-latinpipe.</description><author>Milan Straka, Jana Straková, Federica Gamba</author><pubDate>Wed, 29 May 2024 17:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05839v2</guid></item><item><title>Kotlin ML Pack: Technical Report</title><link>http://arxiv.org/abs/2405.19250v1</link><description>In this technical report, we present three novel datasets of Kotlin code:KStack, KStack-clean, and KExercises. We also describe the results offine-tuning CodeLlama and DeepSeek models on this data. Additionally, wepresent a version of the HumanEval benchmark rewritten by human experts intoKotlin - both the solutions and the tests. Our results demonstrate that small,high-quality datasets (KStack-clean and KExercises) can significantly improvemodel performance on code generation tasks, achieving up to a 16-point increasein pass rate on the HumanEval benchmark. Lastly, we discuss potential futurework in the field of improving language modeling for Kotlin, including the useof static analysis tools in the learning process and the introduction of moreintricate and realistic benchmarks.</description><author>Sergey Titov, Mikhail Evtikhiev, Anton Shapkin, Oleg Smirnov, Sergei Boytsov, Sergei Boytsov, Dariia Karaeva, Maksim Sheptyakov, Mikhail Arkhipov, Timofey Bryksin, Egor Bogomolov</author><pubDate>Wed, 29 May 2024 17:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19250v1</guid></item><item><title>More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms</title><link>http://arxiv.org/abs/2402.04054v2</link><description>We introduce a new framework for studying meta-learning methods usingPAC-Bayesian theory. Its main advantage over previous work is that it allowsfor more flexibility in how the transfer of knowledge between tasks isrealized. For previous approaches, this could only happen indirectly, by meansof learning prior distributions over models. In contrast, the newgeneralization bounds that we prove express the process of meta-learning muchmore directly as learning the learning algorithm that should be used for futuretasks. The flexibility of our framework makes it suitable to analyze a widerange of meta-learning mechanisms and even design new mechanisms. Other thanour theoretical contributions we also show empirically that our frameworkimproves the prediction quality in practical meta-learning mechanisms.</description><author>Hossein Zakerinia, Amin Behjati, Christoph H. Lampert</author><pubDate>Wed, 29 May 2024 17:32:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04054v2</guid></item><item><title>Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI</title><link>http://arxiv.org/abs/2402.00809v3</link><description>In the current landscape of deep learning research, there is a predominantemphasis on achieving high predictive accuracy in supervised tasks involvinglarge image and language datasets. However, a broader perspective reveals amultitude of overlooked metrics, tasks, and data types, such as uncertainty,active and continual learning, and scientific data, that demand attention.Bayesian deep learning (BDL) constitutes a promising avenue, offeringadvantages across these diverse settings. This paper posits that BDL canelevate the capabilities of deep learning. It revisits the strengths of BDL,acknowledges existing challenges, and highlights some exciting research avenuesaimed at addressing these obstacles. Looking ahead, the discussion focuses onpossible ways to combine large-scale foundation models with BDL to unlock theirfull potential.</description><author>Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, José Miguel Hernández-Lobato, Aliaksandr Hubin, Alexander Immer, Theofanis Karaletsos, Mohammad Emtiyaz Khan, Agustinus Kristiadi, Yingzhen Li, Stephan Mandt, Christopher Nemeth, Michael A. Osborne, Tim G. J. Rudner, David Rügamer, Yee Whye Teh, Max Welling, Andrew Gordon Wilson, Ruqi Zhang</author><pubDate>Wed, 29 May 2024 17:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00809v3</guid></item><item><title>Comparative Study of Neighbor-based Methods for Local Outlier Detection</title><link>http://arxiv.org/abs/2405.19247v1</link><description>The neighbor-based method has become a powerful tool to handle the outlierdetection problem, which aims to infer the abnormal degree of the sample basedon the compactness of the sample and its neighbors. However, the existingmethods commonly focus on designing different processes to locate outliers inthe dataset, while the contributions of different types neighbors to outlierdetection has not been well discussed. To this end, this paper studies theneighbor in the existing outlier detection algorithms and a taxonomy isintroduced, which uses the three-level components of information, neighbor andmethodology to define hybrid methods. This taxonomy can serve as a paradigmwhere a novel neighbor-based outlier detection method can be proposed bycombining different components in this taxonomy. A large number of comparativeexperiments were conducted on synthetic and real-world datasets in terms ofperformance comparison and case study, and the results show that reverseK-nearest neighbor based methods achieve promising performance and dynamicselection method is suitable for working in high-dimensional space. Notably, itis verified that rationally selecting components from this taxonomy may createan algorithms superior to existing methods.</description><author>Zhuang Qi, Junlin Zhang, Xiaming Chen, Xin Qi</author><pubDate>Wed, 29 May 2024 17:28:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19247v1</guid></item><item><title>Challenge-Device-Synthesis: A multi-disciplinary approach for the development of social innovation competences for students of Artificial Intelligence</title><link>http://arxiv.org/abs/2405.19243v1</link><description>The advent of Artificial Intelligence is expected to imply profound changesin the short-term. It is therefore imperative for Academia, and particularlyfor the Computer Science scope, to develop cross-disciplinary tools that bondAI developments to their social dimension. To this aim, we introduce theChallenge-Device-Synthesis methodology (CDS), in which a specific challenge ispresented to the students of AI, who are required to develop a device as asolution for the challenge. The device becomes the object of study for thedifferent dimensions of social transformation, and the conclusions addressed bythe students during the discussion around the device are presented in asynthesis piece in the shape of a 10-page scientific paper. The latter isevaluated taking into account both the depth of analysis and the level to whichit genuinely reflects the social transformations associated with the proposedAI-based device. We provide data obtained during the pilot for theimplementation phase of CDS within the subject of Social Innovation, a 6-ECTSsubject from the 6th semester of the Degree of Artificial Intelligence,UAB-Barcelona. We provide details on temporalisation, task distribution,methodological tools used and assessment delivery procedure, as well asqualitative analysis of the results obtained.</description><author>Matías Bilkis, Joan Moya Kohler, Fernando Vilariño</author><pubDate>Wed, 29 May 2024 17:24:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19243v1</guid></item><item><title>Explanation-based Belief Revision: Moving Beyond Minimalism to Explanatory Understanding</title><link>http://arxiv.org/abs/2405.19238v1</link><description>In belief revision, agents typically modify their beliefs when they receivesome new piece of information that is in conflict with them. The guidingprinciple behind most belief revision frameworks is that of minimalism, whichadvocates minimal changes to existing beliefs. However, minimalism may notnecessarily capture the nuanced ways in which human agents reevaluate andmodify their beliefs. In contrast, the explanatory hypothesis indicates thatpeople are inherently driven to seek explanations for inconsistencies, therebystriving for explanatory coherence rather than minimal changes when revisingbeliefs. Our contribution in this paper is two-fold. Motivated by theexplanatory hypothesis, we first present a novel, yet simple belief revisionoperator that, given a belief base and an explanation for an explanandum, itrevises the belief bases in a manner that preserves the explanandum and is notnecessarily minimal. We call this operator explanation-based belief revision.Second, we conduct two human-subject studies to empirically validate ourapproach and investigate belief revision behavior in real-world scenarios. Ourfindings support the explanatory hypothesis and provide insights into thestrategies people employ when resolving inconsistencies.</description><author>Stylianos Loukas Vasileiou, William Yeoh</author><pubDate>Wed, 29 May 2024 17:20:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19238v1</guid></item><item><title>ConceptPrune: Concept Editing in Diffusion Models via Skilled Neuron Pruning</title><link>http://arxiv.org/abs/2405.19237v1</link><description>While large-scale text-to-image diffusion models have demonstrated impressiveimage-generation capabilities, there are significant concerns about theirpotential misuse for generating unsafe content, violating copyright, andperpetuating societal biases. Recently, the text-to-image generation communityhas begun addressing these concerns by editing or unlearning undesired conceptsfrom pre-trained models. However, these methods often involve data-intensiveand inefficient fine-tuning or utilize various forms of token remapping,rendering them susceptible to adversarial jailbreaks. In this paper, we presenta simple and effective training-free approach, ConceptPrune, wherein we firstidentify critical regions within pre-trained models responsible for generatingundesirable concepts, thereby facilitating straightforward concept unlearningvia weight pruning. Experiments across a range of concepts including artisticstyles, nudity, object erasure, and gender debiasing demonstrate that targetconcepts can be efficiently erased by pruning a tiny fraction, approximately0.12% of total weights, enabling multi-concept erasure and robustness againstvarious white-box and black-box adversarial attacks.</description><author>Ruchika Chavhan, Da Li, Timothy Hospedales</author><pubDate>Wed, 29 May 2024 17:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19237v1</guid></item><item><title>Retrieval Augmented Generation for Domain-specific Question Answering</title><link>http://arxiv.org/abs/2404.14760v2</link><description>Question answering (QA) has become an important application in the advanceddevelopment of large language models. General pre-trained large language modelsfor question-answering are not trained to properly understand the knowledge orterminology for a specific domain, such as finance, healthcare, education, andcustomer service for a product. To better cater to domain-specificunderstanding, we build an in-house question-answering system for Adobeproducts. We propose a novel framework to compile a large question-answerdatabase and develop the approach for retrieval-aware finetuning of a LargeLanguage model. We showcase that fine-tuning the retriever leads to majorimprovements in the final generation. Our overall approach reduceshallucinations during generation while keeping in context the latest retrievalinformation for contextual grounding.</description><author>Sanat Sharma, David Seunghyun Yoon, Franck Dernoncourt, Dewang Sultania, Karishma Bagga, Mengjiao Zhang, Trung Bui, Varun Kotte</author><pubDate>Wed, 29 May 2024 17:18:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14760v2</guid></item><item><title>On the Error-Propagation of Inexact Hotelling's Deflation for Principal Component Analysis</title><link>http://arxiv.org/abs/2310.04283v2</link><description>Principal Component Analysis (PCA) aims to find subspaces spanned by theso-called principal components that best represent the variance in the dataset.The deflation method is a popular meta-algorithm that sequentially findsindividual principal components, starting from the most important ones andworking towards the less important ones. However, as deflation proceeds,numerical errors from the imprecise estimation of principal componentspropagate due to its sequential nature. This paper mathematically characterizesthe error propagation of the inexact Hotelling's deflation method. We considertwo scenarios: $i)$ when the sub-routine for finding the leading eigenvector isabstract and can represent various algorithms; and $ii)$ when power iterationis used as the sub-routine. In the latter case, the additional directionalinformation from power iteration allows us to obtain a tighter error bound thanthe sub-routine agnostic case. For both scenarios, we explicitly characterizehow the errors progress and affect subsequent principal component estimations.</description><author>Fangshuo Liao, Junhyung Lyle Kim, Cruz Barnum, Anastasios Kyrillidis</author><pubDate>Wed, 29 May 2024 17:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04283v2</guid></item><item><title>Exploring the impact of traffic signal control and connected and automated vehicles on intersections safety: A deep reinforcement learning approach</title><link>http://arxiv.org/abs/2405.19236v1</link><description>In transportation networks, intersections pose significant risks ofcollisions due to conflicting movements of vehicles approaching from differentdirections. To address this issue, various tools can exert influence on trafficsafety both directly and indirectly. This study focuses on investigating theimpact of adaptive signal control and connected and automated vehicles (CAVs)on intersection safety using a deep reinforcement learning approach. Theobjective is to assess the individual and combined effects of CAVs and adaptivetraffic signal control on traffic safety, considering rear-end and crossingconflicts. The study employs a Deep Q Network (DQN) to regulate traffic signalsand driving behaviors of both CAVs and Human Drive Vehicles (HDVs), and usesTime To Collision (TTC) metric to evaluate safety. The findings demonstrate asignificant reduction in rear-end and crossing conflicts through the combinedimplementation of CAVs and DQNs-based traffic signal control. Additionally, thelong-term positive effects of CAVs on safety are similar to the short-termeffects of combined CAVs and DQNs-based traffic signal control. Overall, thestudy emphasizes the potential benefits of integrating CAVs and adaptivetraffic signal control approaches in order to enhance traffic safety. Thefindings of this study could provide valuable insights for city officials andtransportation authorities in developing effective strategies to improve safetyat signalized intersections.</description><author>Amir Hossein Karbasi, Hao Yang, Saiedeh Razavi</author><pubDate>Wed, 29 May 2024 17:17:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19236v1</guid></item><item><title>Masked Autoencoders are PDE Learners</title><link>http://arxiv.org/abs/2403.17728v2</link><description>Neural solvers for partial differential equations (PDEs) have great potentialto generate fast and accurate physics solutions, yet their practicality iscurrently limited by their generalizability. PDEs evolve over broad scales andexhibit diverse behaviors; predicting these phenomena will require learningrepresentations across a wide variety of inputs which may encompass differentcoefficients, boundary conditions, resolutions, or even equations. As a steptowards generalizable PDE modeling, we adapt masked pretraining for physicsproblems. Through self-supervised learning across PDEs, masked autoencoders canconsolidate heterogeneous physics to learn meaningful latent representationsand perform latent PDE arithmetic in this space. Furthermore, we demonstratethat masked pretraining can improve PDE coefficient regression and theclassification of PDE features. Lastly, conditioning neural solvers on learnedlatent representations can improve time-stepping and super-resolutionperformance across a variety of coefficients, discretizations, or boundaryconditions, as well as on unseen PDEs. We hope that masked pretraining canemerge as a unifying method across large, unlabeled, and heterogeneous datasetsto learn latent physics at scale.</description><author>Anthony Zhou, Amir Barati Farimani</author><pubDate>Wed, 29 May 2024 17:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17728v2</guid></item><item><title>Forward-Backward Knowledge Distillation for Continual Clustering</title><link>http://arxiv.org/abs/2405.19234v1</link><description>Unsupervised Continual Learning (UCL) is a burgeoning field in machinelearning, focusing on enabling neural networks to sequentially learn taskswithout explicit label information. Catastrophic Forgetting (CF), where modelsforget previously learned tasks upon learning new ones, poses a significantchallenge in continual learning, especially in UCL, where labeled informationof data is not accessible. CF mitigation strategies, such as knowledgedistillation and replay buffers, often face memory inefficiency and privacyissues. Although current research in UCL has endeavored to refine datarepresentations and address CF in streaming data contexts, there is anoticeable lack of algorithms specifically designed for unsupervisedclustering. To fill this gap, in this paper, we introduce the concept ofUnsupervised Continual Clustering (UCC). We propose Forward-Backward KnowledgeDistillation for unsupervised Continual Clustering (FBCC) to counteract CFwithin the context of UCC. FBCC employs a single continual learner (the``teacher'') with a cluster projector, along with multiple student models, toaddress the CF issue. The proposed method consists of two phases: ForwardKnowledge Distillation, where the teacher learns new clusters while retainingknowledge from previous tasks with guidance from specialized student models,and Backward Knowledge Distillation, where a student model mimics the teacher'sbehavior to retain task-specific knowledge, aiding the teacher in subsequenttasks. FBCC marks a pioneering approach to UCC, demonstrating enhancedperformance and memory efficiency in clustering across various tasks,outperforming the application of clustering algorithms to the latent space ofstate-of-the-art UCL algorithms.</description><author>Mohammadreza Sadeghi, Zihan Wang, Narges Armanfard</author><pubDate>Wed, 29 May 2024 17:13:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19234v1</guid></item><item><title>Deconstructing In-Context Learning: Understanding Prompts via Corruption</title><link>http://arxiv.org/abs/2404.02054v2</link><description>The ability of large language models (LLMs) to $``$learn in context$"$ basedon the provided prompt has led to an explosive growth in their use, culminatingin the proliferation of AI assistants such as ChatGPT, Claude, and Bard. TheseAI assistants are known to be robust to minor prompt modifications, mostly dueto alignment techniques that use human feedback. In contrast, the underlyingpre-trained LLMs they use as a backbone are known to be brittle in thisrespect. Building high-quality backbone models remains a core challenge, and acommon approach to assessing their quality is to conduct few-shot evaluation.Such evaluation is notorious for being highly sensitive to minor promptmodifications, as well as the choice of specific in-context examples. Priorwork has examined how modifying different elements of the prompt can affectmodel performance. However, these earlier studies tended to concentrate on alimited number of specific prompt attributes and often produced contradictoryresults. Additionally, previous research either focused on models with fewerthan 15 billion parameters or exclusively examined black-box models like GPT-3or PaLM, making replication challenging. In the present study, we decompose theentire prompt into four components: task description, demonstration inputs,labels, and inline instructions provided for each demonstration. We investigatethe effects of structural and semantic corruptions of these elements on modelperformance. We study models ranging from 1.5B to 70B in size, using tendatasets covering classification and generation tasks. We find that repeatingtext within the prompt boosts model performance, and bigger models ($\geq$30B)are more sensitive to the semantics of the prompt. Finally, we observe thatadding task and inline instructions to the demonstrations enhances modelperformance even when the instructions are semantically corrupted.</description><author>Namrata Shivagunde, Vladislav Lialin, Sherin Muckatira, Anna Rumshisky</author><pubDate>Wed, 29 May 2024 17:12:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02054v2</guid></item><item><title>Incremental Object Detection with CLIP</title><link>http://arxiv.org/abs/2310.08815v2</link><description>In contrast to the incremental classification task, the incremental detectiontask is characterized by the presence of data ambiguity, as an image may havedifferently labeled bounding boxes across multiple continuous learning stages.This phenomenon often impairs the model's ability to effectively learn newclasses. However, existing research has paid less attention to the forwardcompatibility of the model, which limits its suitability for incrementallearning. To overcome this obstacle, we propose leveraging a visual-languagemodel such as CLIP to generate text feature embeddings for different classsets, which enhances the feature space globally. We then employ super-classesto replace the unavailable novel classes in the early learning stage tosimulate the incremental scenario. Finally, we utilize the CLIP image encoderto accurately identify potential objects. We incorporate the finely recognizeddetection boxes as pseudo-annotations into the training process, therebyfurther improving the detection performance. We evaluate our approach onvarious incremental learning settings using the PASCAL VOC 2007 dataset, andour approach outperforms state-of-the-art methods, particularly for recognizingthe new classes.</description><author>Ziyue Huang, Yupeng He, Qingjie Liu, Yunhong Wang</author><pubDate>Wed, 29 May 2024 17:11:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08815v2</guid></item><item><title>Feature Extraction for Generative Medical Imaging Evaluation: New Evidence Against an Evolving Trend</title><link>http://arxiv.org/abs/2311.13717v3</link><description>Fr\'echet Inception Distance (FID) is a widely used metric for assessingsynthetic image quality. It relies on an ImageNet-based feature extractor,making its applicability to medical imaging unclear. A recent trend is to adaptFID to medical imaging through feature extractors trained on medical images.Our study challenges this practice by demonstrating that ImageNet-basedextractors are more consistent and aligned with human judgment than theirRadImageNet counterparts. We evaluated sixteen StyleGAN2 networks across fourmedical imaging modalities and four data augmentation techniques with Fr\'echetdistances (FDs) computed using eleven ImageNet or RadImageNet-trained featureextractors. Comparison with human judgment via visual Turing tests revealedthat ImageNet-based extractors produced rankings consistent with humanjudgment, with the FD derived from the ImageNet-trained SwAV extractorsignificantly correlating with expert evaluations. In contrast,RadImageNet-based rankings were volatile and inconsistent with human judgment.Our findings challenge prevailing assumptions, providing novel evidence thatmedical image-trained feature extractors do not inherently improve FDs and caneven compromise their reliability. Our code is available athttps://github.com/mckellwoodland/fid-med-eval.</description><author>McKell Woodland, Austin Castelo, Mais Al Taie, Jessica Albuquerque Marques Silva, Mohamed Eltaher, Frank Mohn, Alexander Shieh, Austin Castelo, Suprateek Kundu, Joshua P. Yung, Ankit B. Patel, Kristy K. Brock</author><pubDate>Wed, 29 May 2024 17:09:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13717v3</guid></item><item><title>Track Anything Rapter(TAR)</title><link>http://arxiv.org/abs/2405.11655v2</link><description>Object tracking is a fundamental task in computer vision with broad practicalapplications across various domains, including traffic monitoring, robotics,and autonomous vehicle tracking. In this project, we aim to develop asophisticated aerial vehicle system known as Track Anything Rapter (TAR),designed to detect, segment, and track objects of interest based onuser-provided multimodal queries, such as text, images, and clicks. TARutilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimatethe relative pose of the queried object. The tracking problem is approached asa Visual Servoing task, enabling the UAV to consistently focus on the objectthrough advanced motion planning and control algorithms. We showcase how theintegration of these foundational models with a custom high-level controlalgorithm results in a highly stable and precise tracking system deployed on acustom-built PX4 Autopilot-enabled Voxl2 M500 drone. To validate the trackingalgorithm's performance, we compare it against Vicon-based ground truth.Additionally, we evaluate the reliability of the foundational models in aidingtracking in scenarios involving occlusions. Finally, we test and validate themodel's ability to work seamlessly with multiple modalities, such as click,bounding box, and image templates.</description><author>Tharun V. Puthanveettil, Fnu Obaid ur Rahman</author><pubDate>Wed, 29 May 2024 17:09:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11655v2</guid></item><item><title>Valid Conformal Prediction for Dynamic GNNs</title><link>http://arxiv.org/abs/2405.19230v1</link><description>Graph neural networks (GNNs) are powerful black-box models which have shownimpressive empirical performance. However, without any form of uncertaintyquantification, it can be difficult to trust such models in high-riskscenarios. Conformal prediction aims to address this problem, however, anassumption of exchangeability is required for its validity which has limitedits applicability to static graphs and transductive regimes. We propose to useunfolding, which allows any existing static GNN to output a dynamic graphembedding with exchangeability properties. Using this, we extend the validityof conformal prediction to dynamic GNNs in both transductive and semi-inductiveregimes. We provide a theoretical guarantee of valid conformal prediction inthese cases and demonstrate the empirical validity, as well as the performancegains, of unfolded GNNs against standard GNN architectures on both simulatedand real datasets.</description><author>Ed Davis, Ian Gallagher, Daniel John Lawson, Patrick Rubin-Delanchy</author><pubDate>Wed, 29 May 2024 17:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19230v1</guid></item><item><title>On Generating Monolithic and Model Reconciling Explanations in Probabilistic Scenarios</title><link>http://arxiv.org/abs/2405.19229v1</link><description>Explanation generation frameworks aim to make AI systems' decisionstransparent and understandable to human users. However, generating explanationsin uncertain environments characterized by incomplete information andprobabilistic models remains a significant challenge. In this paper, we proposea novel framework for generating probabilistic monolithic explanations andmodel reconciling explanations. Monolithic explanations provide self-containedreasons for an explanandum without considering the agent receiving theexplanation, while model reconciling explanations account for the knowledge ofthe agent receiving the explanation. For monolithic explanations, our approachintegrates uncertainty by utilizing probabilistic logic to increase theprobability of the explanandum. For model reconciling explanations, we proposea framework that extends the logic-based variant of the model reconciliationproblem to account for probabilistic human models, where the goal is to findexplanations that increase the probability of the explanandum while minimizingconflicts between the explanation and the probabilistic human model. Weintroduce explanatory gain and explanatory power as quantitative metrics toassess the quality of these explanations. Further, we present algorithms thatexploit the duality between minimal correction sets and minimal unsatisfiablesets to efficiently compute both types of explanations in probabilisticcontexts. Extensive experimental evaluations on various benchmarks demonstratethe effectiveness and scalability of our approach in generating explanationsunder uncertainty.</description><author>Stylianos Loukas Vasileiou, William Yeoh, Alessandro Previti, Tran Cao Son</author><pubDate>Wed, 29 May 2024 17:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19229v1</guid></item><item><title>ContextBLIP: Doubly Contextual Alignment for Contrastive Image Retrieval from Linguistically Complex Descriptions</title><link>http://arxiv.org/abs/2405.19226v1</link><description>Image retrieval from contextual descriptions (IRCD) aims to identify an imagewithin a set of minimally contrastive candidates based on linguisticallycomplex text. Despite the success of VLMs, they still significantly lag behindhuman performance in IRCD. The main challenges lie in aligning key contextualcues in two modalities, where these subtle cues are concealed in tiny areas ofmultiple contrastive images and within the complex linguistics of textualdescriptions. This motivates us to propose ContextBLIP, a simple yet effectivemethod that relies on a doubly contextual alignment scheme for challengingIRCD. Specifically, 1) our model comprises a multi-scale adapter, a matchingloss, and a text-guided masking loss. The adapter learns to capturefine-grained visual cues. The two losses enable iterative supervision for theadapter, gradually highlighting the focal patches of a single image to the keytextual cues. We term such a way as intra-contextual alignment. 2) Then,ContextBLIP further employs an inter-context encoder to learn dependenciesamong candidates, facilitating alignment between the text to multiple images.We term this step as inter-contextual alignment. Consequently, the nuanced cuesconcealed in each modality can be effectively aligned. Experiments on twobenchmarks show the superiority of our method. We observe that ContextBLIP canyield comparable results with GPT-4V, despite involving about 7,500 times fewerparameters.</description><author>Honglin Lin, Siyu Li, Guoshun Nan, Chaoyue Tang, Xueting Wang, Jingxin Xu, Rong Yankai, Zhili Zhou, Yutong Gao, Qimei Cui, Xiaofeng Tao</author><pubDate>Wed, 29 May 2024 17:06:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19226v1</guid></item><item><title>Synthetic Potential Outcomes for Mixtures of Treatment Effects</title><link>http://arxiv.org/abs/2405.19225v1</link><description>Modern data analysis frequently relies on the use of large datasets, oftenconstructed as amalgamations of diverse populations or data-sources.Heterogeneity across these smaller datasets constitutes two major challengesfor causal inference: (1) the source of each sample can introduce latentconfounding between treatment and effect, and (2) diverse populations mayrespond differently to the same treatment, giving rise to heterogeneoustreatment effects (HTEs). The issues of latent confounding and HTEs have beenstudied separately but not in conjunction. In particular, previous works onlyreport the conditional average treatment effect (CATE) among similarindividuals (with respect to the measured covariates). CATEs cannot resolvemixtures of potential treatment effects driven by latent heterogeneity, whichwe call mixtures of treatment effects (MTEs). Inspired by method of momentapproaches to mixture models, we propose "synthetic potential outcomes" (SPOs).Our new approach deconfounds heterogeneity while also guaranteeing theidentifiability of MTEs. This technique bypasses full recovery of a mixture,which significantly simplifies its requirements for identifiability. Wedemonstrate the efficacy of SPOs on synthetic data.</description><author>Bijan Mazaheri, Chandler Squires, Caroline Uhler</author><pubDate>Wed, 29 May 2024 17:05:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19225v1</guid></item><item><title>A study on the adequacy of common IQA measures for medical images</title><link>http://arxiv.org/abs/2405.19224v1</link><description>Image quality assessment (IQA) is standard practice in the development stageof novel machine learning algorithms that operate on images. The most commonlyused IQA measures have been developed and tested for natural images, but not inthe medical setting. Reported inconsistencies arising in medical images are notsurprising, as they have different properties than natural images. In thisstudy, we test the applicability of common IQA measures for medical image databy comparing their assessment to manually rated chest X-ray (5 experts) andphotoacoustic image data (1 expert). Moreover, we include supplementary studieson grayscale natural images and accelerated brain MRI data. The results of allexperiments show a similar outcome in line with previous findings for medicalimaging: PSNR and SSIM in the default setting are in the lower range of theresult list and HaarPSI outperforms the other tested measures in the overallperformance. Also among the top performers in our medical experiments are thefull reference measures DISTS, FSIM, LPIPS and MS-SSIM. Generally, the resultson natural images yield considerably higher correlations, suggesting that theadditional employment of tailored IQA measures for medical imaging algorithmsis needed.</description><author>Anna Breger, Clemens Karner, Ian Selby, Janek Gröhl, Sören Dittmer, Edward Lilley, Judith Babar, Jake Beckford, Timothy J Sadler, Shahab Shahipasand, Arthikkaa Thavakumar, Michael Roberts, Carola-Bibiane Schönlieb</author><pubDate>Wed, 29 May 2024 17:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19224v1</guid></item><item><title>Lower Bounds on the Expressivity of Recurrent Neural Language Models</title><link>http://arxiv.org/abs/2405.19222v1</link><description>The recent successes and spread of large neural language models (LMs) callfor a thorough understanding of their computational ability. Describing theircomputational abilities through LMs' \emph{representational capacity} is alively area of research. However, investigation into the representationalcapacity of neural LMs has predominantly focused on their ability to\emph{recognize} formal languages. For example, recurrent neural networks(RNNs) with Heaviside activations are tightly linked to regular languages,i.e., languages defined by finite-state automata (FSAs). Such results, however,fall short of describing the capabilities of RNN \emph{language models} (LMs),which are definitionally \emph{distributions} over strings. We take a freshlook at the representational capacity of RNN LMs by connecting them to\emph{probabilistic} FSAs and demonstrate that RNN LMs with linearly boundedprecision can express arbitrary regular LMs.</description><author>Anej Svete, Franz Nowak, Anisha Mohamed Sahabdeen, Ryan Cotterell</author><pubDate>Wed, 29 May 2024 17:02:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19222v1</guid></item><item><title>Semantic In-Domain Product Identification for Search Queries</title><link>http://arxiv.org/abs/2404.09091v2</link><description>Accurate explicit and implicit product identification in search queries iscritical for enhancing user experiences, especially at a company like Adobewhich has over 50 products and covers queries across hundreds of tools. In thiswork, we present a novel approach to training a product classifier from userbehavioral data. Our semantic model led to &gt;25% relative improvement in CTR(click through rate) across the deployed surfaces; a &gt;50% decrease in nullrate; a 2x increase in the app cards surfaced, which helps drive productvisibility.</description><author>Sanat Sharma, Jayant Kumar, Twisha Naik, Zhaoyu Lu, Arvind Srikantan, Tracy Holloway King</author><pubDate>Wed, 29 May 2024 17:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09091v2</guid></item><item><title>Domain adaptation in small-scale and heterogeneous biological datasets</title><link>http://arxiv.org/abs/2405.19221v1</link><description>Machine learning techniques are steadily becoming more important in modernbiology, and are used to build predictive models, discover patterns, andinvestigate biological problems. However, models trained on one dataset areoften not generalizable to other datasets from different cohorts orlaboratories, due to differences in the statistical properties of thesedatasets. These could stem from technical differences, such as the measurementtechnique used, or from relevant biological differences between the populationsstudied. Domain adaptation, a type of transfer learning, can alleviate thisproblem by aligning the statistical distributions of features and samples amongdifferent datasets so that similar models can be applied across them. However,a majority of state-of-the-art domain adaptation methods are designed to workwith large-scale data, mostly text and images, while biological datasets oftensuffer from small sample sizes, and possess complexities such as heterogeneityof the feature space. This Review aims to synthetically discuss domainadaptation methods in the context of small-scale and highly heterogeneousbiological data. We describe the benefits and challenges of domain adaptationin biological research and critically discuss some of its objectives,strengths, and weaknesses through key representative methodologies. We arguefor the incorporation of domain adaptation techniques to the computationalbiologist's toolkit, with further development of customized approaches.</description><author>Seyedmehdi Orouji, Martin C. Liu, Tal Korem, Megan A. K. Peters</author><pubDate>Wed, 29 May 2024 17:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19221v1</guid></item><item><title>WRDScore: New Metric for Evaluation of Natural Language Generation Models</title><link>http://arxiv.org/abs/2405.19220v1</link><description>The problem of natural language generation, and, more specifically, methodname prediction, faces significant difficulties when proposed models need to beevaluated on test data. Such a metric would need to consider the versatilitywith which a single method can be named, with respect to both semantics andsyntax. Measuring the direct overlap between the predicted and reference (true)sequences will not be able to capture these subtleties. Other existingembedding based metrics either do not measure precision and recall or imposestrict unrealistic assumptions on both sequences. To address these issues, wepropose a new metric that, on the one hand, is very simple and lightweight,and, on the other hand, is able to calculate precision and recall withoutresorting to any assumptions while obtaining good performance with respect tothe human judgement.</description><author>Ravil Mussabayev</author><pubDate>Wed, 29 May 2024 17:00:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19220v1</guid></item><item><title>LoByITFL: Low Communication Secure and Private Federated Learning</title><link>http://arxiv.org/abs/2405.19217v1</link><description>Federated Learning (FL) faces several challenges, such as the privacy of theclients data and security against Byzantine clients. Existing works treatingprivacy and security jointly make sacrifices on the privacy guarantee. In thiswork, we introduce LoByITFL, the first communication-efficientInformation-Theoretic (IT) private and secure FL scheme that makes nosacrifices on the privacy guarantees while ensuring security against Byzantineadversaries. The key ingredients are a small and representative datasetavailable to the federator, a careful transformation of the FLTrust algorithmand the use of a trusted third party only in a one-time preprocessing phasebefore the start of the learning algorithm. We provide theoretical guaranteeson privacy and Byzantine-resilience, and provide convergence guarantee andexperimental results validating our theoretical findings.</description><author>Yue Xia, Christoph Hofmeister, Maximilian Egger, Rawad Bitar</author><pubDate>Wed, 29 May 2024 17:00:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19217v1</guid></item><item><title>Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data</title><link>http://arxiv.org/abs/2401.15113v2</link><description>Accurate global glacier mapping is critical for understanding climate changeimpacts. Despite its importance, automated glacier mapping at a global scaleremains largely unexplored. Here we address this gap and proposeGlacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deeplearning model, and five strategies for multitemporal global-scale glaciermapping using open satellite imagery. Assessing the spatial, temporal andcross-sensor generalisation shows that our best strategy achieves intersectionover union &gt;0.85 on previously unobserved images in most cases, which drops to&gt;0.75 for debris-rich areas such as High-Mountain Asia and increases to &gt;0.90for regions dominated by clean ice. A comparative validation against humanexpert uncertainties in terms of area and distance deviations underscoresGlaViTU performance, approaching or matching expert-level delineation. Addingsynthetic aperture radar data, namely, backscatter and interferometriccoherence, increases the accuracy in all regions where available. Thecalibrated confidence for glacier extents is reported making the predictionsmore reliable and interpretable. We also release a benchmark dataset thatcovers 9% of glaciers worldwide. Our results support efforts towards automatedmultitemporal and global glacier mapping.</description><author>Konstantin A. Maslov, Claudio Persello, Thomas Schellenberger, Alfred Stein</author><pubDate>Wed, 29 May 2024 16:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15113v2</guid></item><item><title>HawkVision: Low-Latency Modeless Edge AI Serving</title><link>http://arxiv.org/abs/2405.19213v1</link><description>The trend of modeless ML inference is increasingly growing in popularity asit hides the complexity of model inference from users and caters to diverseuser and application accuracy requirements. Previous work mostly focuses onmodeless inference in data centers. To provide low-latency inference, in thispaper, we promote modeless inference at the edge. The edge environmentintroduces additional challenges related to low power consumption, limiteddevice memory, and volatile network environments. To address these challenges, we propose HawkVision, which provideslow-latency modeless serving of vision DNNs. HawkVision leverages a two-layeredge-DC architecture that employs confidence scaling to reduce the number ofmodel options while meeting diverse accuracy requirements. It also supportslossy inference under volatile network environments. Our experimental resultsshow that HawkVision outperforms current serving systems by up to 1.6X in P99latency for providing modeless service. Our FPGA prototype demonstrates similarperformance at certain accuracy levels with up to a 3.34X reduction in powerconsumption.</description><author>ChonLam Lao, Jiaqi Gao, Ganesh Ananthanarayanan, Aditya Akella, Minlan Yu</author><pubDate>Wed, 29 May 2024 16:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19213v1</guid></item><item><title>Intelligent Anomaly Detection for Lane Rendering Using Transformer with Self-Supervised Pre-Training and Customized Fine-Tuning</title><link>http://arxiv.org/abs/2312.04398v2</link><description>The burgeoning navigation services using digital maps provide greatconvenience to drivers. Nevertheless, the presence of anomalies in lanerendering map images occasionally introduces potential hazards, as suchanomalies can be misleading to human drivers and consequently contribute tounsafe driving conditions. In response to this concern and to accurately andeffectively detect the anomalies, this paper transforms lane rendering imageanomaly detection into a classification problem and proposes a four-phasepipeline consisting of data pre-processing, self-supervised pre-training withthe masked image modeling (MiM) method, customized fine-tuning usingcross-entropy based loss with label smoothing, and post-processing to tackle itleveraging state-of-the-art deep learning techniques, especially thoseinvolving Transformer models. Various experiments verify the effectiveness ofthe proposed pipeline. Results indicate that the proposed pipeline exhibitssuperior performance in lane rendering image anomaly detection, and notably,the self-supervised pre-training with MiM can greatly enhance the detectionaccuracy while significantly reducing the total training time. For instance,employing the Swin Transformer with Uniform Masking as self-supervisedpretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and animproved Area Under The Curve (AUC) score of 0.9743 compared with the pure SwinTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and anAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from theoriginal 280. In conclusion, the proposed pipeline, with its incorporation ofself-supervised pre-training using MiM and other advanced deep learningtechniques, emerges as a robust solution for enhancing the accuracy andefficiency of lane rendering image anomaly detection in digital navigationsystems.</description><author>Yongqi Dong, Xingmin Lu, Ruohan Li, Wei Song, Bart van Arem, Haneen Farah</author><pubDate>Wed, 29 May 2024 16:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04398v2</guid></item><item><title>Partial Information Decomposition for Data Interpretability and Feature Selection</title><link>http://arxiv.org/abs/2405.19212v1</link><description>In this paper, we introduce Partial Information Decomposition of Features(PIDF), a new paradigm for simultaneous data interpretability and featureselection. Contrary to traditional methods that assign a single importancevalue, our approach is based on three metrics per feature: the mutualinformation shared with the target variable, the feature's contribution tosynergistic information, and the amount of this information that is redundant.In particular, we develop a novel procedure based on these three metrics, whichreveals not only how features are correlated with the target but also theadditional and overlapping information provided by considering them incombination with other features. We extensively evaluate PIDF using bothsynthetic and real-world data, demonstrating its potential applications andeffectiveness, by considering case studies from genetics and neuroscience.</description><author>Charles Westphal, Stephen Hailes, Mirco Musolesi</author><pubDate>Wed, 29 May 2024 16:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19212v1</guid></item><item><title>Gone but Not Forgotten: Improved Benchmarks for Machine Unlearning</title><link>http://arxiv.org/abs/2405.19211v1</link><description>Machine learning models are vulnerable to adversarial attacks, includingattacks that leak information about the model's training data. There hasrecently been an increase in interest about how to best address privacyconcerns, especially in the presence of data-removal requests. Machineunlearning algorithms aim to efficiently update trained models to comply withdata deletion requests while maintaining performance and without having toresort to retraining the model from scratch, a costly endeavor. Severalalgorithms in the machine unlearning literature demonstrate some level ofprivacy gains, but they are often evaluated only on rudimentary membershipinference attacks, which do not represent realistic threats. In this paper wedescribe and propose alternative evaluation methods for three key shortcomingsin the current evaluation of unlearning algorithms. We show the utility of ouralternative evaluations via a series of experiments of state-of-the-artunlearning algorithms on different computer vision datasets, presenting a moredetailed picture of the state of the field.</description><author>Keltin Grimes, Collin Abidi, Cole Frank, Shannon Gallagher</author><pubDate>Wed, 29 May 2024 16:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19211v1</guid></item><item><title>REBEL: Reinforcement Learning via Regressing Relative Rewards</title><link>http://arxiv.org/abs/2404.16767v2</link><description>While originally developed for continuous control problems, Proximal PolicyOptimization (PPO) has emerged as the work-horse of a variety of reinforcementlearning (RL) applications, including the fine-tuning of generative models.Unfortunately, PPO requires multiple heuristics to enable stable convergence(e.g. value networks, clipping), and is notorious for its sensitivity to theprecise implementation of these components. In response, we take a step backand ask what a minimalist RL algorithm for the era of generative models wouldlook like. We propose REBEL, an algorithm that cleanly reduces the problem ofpolicy optimization to regressing the relative reward between two completionsto a prompt in terms of the policy, enabling strikingly lightweightimplementation. In theory, we prove that fundamental RL algorithms like NaturalPolicy Gradient can be seen as variants of REBEL, which allows us to match thestrongest known theoretical guarantees in terms of convergence and samplecomplexity in the RL literature. REBEL can also cleanly incorporate offlinedata and be extended to handle the intransitive preferences we frequently seein practice. Empirically, we find that REBEL provides a unified approach tolanguage modeling and image generation with stronger or similar performance asPPO and DPO, all while being simpler to implement and more computationallyefficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strongperformance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard.</description><author>Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, Wen Sun</author><pubDate>Wed, 29 May 2024 16:52:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16767v2</guid></item><item><title>Gradient Guided Hypotheses: A unified solution to enable machine learning models on scarce and noisy data regimes</title><link>http://arxiv.org/abs/2405.19210v1</link><description>Ensuring high-quality data is paramount for maximizing the performance ofmachine learning models and business intelligence systems. However, challengesin data quality, including noise in data capture, missing records, limited dataproduction, and confounding variables, significantly constrain the potentialperformance of these systems. In this study, we propose anarchitecture-agnostic algorithm, Gradient Guided Hypotheses (GGH), designed toaddress these challenges. GGH analyses gradients from hypotheses as a proxy ofdistinct and possibly contradictory patterns in the data. This frameworkentails an additional step in machine learning training, where gradients can beincluded or excluded from backpropagation. In this manner, missing and noisydata are addressed through a unified solution that perceives both challenges asfacets of the same overarching issue: the propagation of erroneous information.Experimental validation of GGH is conducted using real-world open-sourcedatasets, where records with missing rates of up to 98.5% are simulated.Comparative analysis with state-of-the-art imputation methods demonstrates asubstantial improvement in model performance achieved by GGH. Specifically invery high scarcity regimes, GGH was found to be the only viable solution.Additionally, GGH's noise detection capabilities are showcased by introducingsimulated noise into the datasets and observing enhanced model performanceafter filtering out the noisy data. This study presents GGH as a promisingsolution for improving data quality and model performance in variousapplications.</description><author>Paulo Neves, Joerg K. Wegner, Philippe Schwaller</author><pubDate>Wed, 29 May 2024 16:51:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19210v1</guid></item><item><title>A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry</title><link>http://arxiv.org/abs/2404.15777v4</link><description>Since the inception of the Transformer architecture in 2017, Large LanguageModels (LLMs) such as GPT and BERT have evolved significantly, impactingvarious industries with their advanced capabilities in language understandingand generation. These models have shown potential to transform the medicalfield, highlighting the necessity for specialized evaluation frameworks toensure their effective and ethical deployment. This comprehensive surveydelineates the extensive application and requisite evaluation of LLMs withinhealthcare, emphasizing the critical need for empirical validation to fullyexploit their capabilities in enhancing healthcare outcomes. Our survey isstructured to provide an in-depth analysis of LLM applications across clinicalsettings, medical text data processing, research, education, and public healthawareness. We begin by exploring the roles of LLMs in various medicalapplications, detailing their evaluation based on performance in tasks such asclinical diagnosis, medical text data processing, information retrieval, dataanalysis, and educational content generation. The subsequent sections offer acomprehensive discussion on the evaluation methods and metrics employed,including models, evaluators, and comparative experiments. We further examinethe benchmarks and datasets utilized in these evaluations, providing acategorized description of benchmarks for tasks like question answering,summarization, information extraction, bioinformatics, information retrievaland general comprehensive benchmarks. This structure ensures a thoroughunderstanding of how LLMs are assessed for their effectiveness, accuracy,usability, and ethical alignment in the medical domain. ...</description><author>Yining Huang, Keke Tang, Meilian Chen, Boyuan Wang</author><pubDate>Wed, 29 May 2024 16:50:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15777v4</guid></item><item><title>VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos</title><link>http://arxiv.org/abs/2405.19209v1</link><description>Video-language understanding tasks have focused on short video clips, oftenstruggling with long-form video understanding tasks. Recently, many longvideo-language understanding approaches have leveraged the reasoningcapabilities of Large Language Models (LLMs) to perform long video QA,transforming videos into densely sampled frame captions, and asking LLMs torespond to text queries over captions. However, the frames used for captioningare often redundant and contain irrelevant information, making dense samplinginefficient, and ignoring the fact that video QA requires varying levels ofgranularity, with some video segments being highly relevant to the question(needing more fine-grained detail) while others being less relevant. Thus,these LLM-based approaches are prone to missing information and operate onlarge numbers of irrelevant captions, lowering both performance and efficiency.To address these issues, we introduce VideoTree, a query-adaptive andhierarchical framework for long-video understanding with LLMs. VideoTreedynamically extracts query-related information from a video and builds atree-based representation for LLM reasoning. First, VideoTree adaptivelyselects frames for captioning by iteratively clustering frames based on theirvisual features and scoring clusters using their relevance to the query.Second, it organizes visual clusters into a query-adaptive and hierarchicaltree structure; the tree encodes varying levels of granularity, with higherresolution on relevant segments. Finally, VideoTree produces an answer bytraversing the tree's keyframes and passing their captions to an LLM answerer.Our method improves both reasoning accuracy and efficiency compared to existingmethods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselineson the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, whilereducing inference time by 40%.</description><author>Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, Mohit Bansal</author><pubDate>Wed, 29 May 2024 16:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19209v1</guid></item><item><title>A Multi-Source Retrieval Question Answering Framework Based on RAG</title><link>http://arxiv.org/abs/2405.19207v1</link><description>With the rapid development of large-scale language models,Retrieval-Augmented Generation (RAG) has been widely adopted. However, existingRAG paradigms are inevitably influenced by erroneous retrieval information,thereby reducing the reliability and correctness of generated results.Therefore, to improve the relevance of retrieval information, this studyproposes a method that replaces traditional retrievers with GPT-3.5, leveragingits vast corpus knowledge to generate retrieval information. We also propose aweb retrieval based method to implement fine-grained knowledge retrieval,Utilizing the powerful reasoning capability of GPT-3.5 to realize semanticpartitioning of problem.In order to mitigate the illusion of GPT retrieval andreduce noise in Web retrieval,we proposes a multi-source retrieval framework,named MSRAG, which combines GPT retrieval with web retrieval. Experiments onmultiple knowledge-intensive QA datasets demonstrate that the proposedframework in this study performs better than existing RAG framework inenhancing the overall efficiency and accuracy of QA systems.</description><author>Ridong Wu, Shuhong Chen, Xiangbiao Su, Yuankai Zhu, Yifei Liao, Jianming Wu</author><pubDate>Wed, 29 May 2024 16:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19207v1</guid></item><item><title>Matrix Manifold Neural Networks++</title><link>http://arxiv.org/abs/2405.19206v1</link><description>Deep neural networks (DNNs) on Riemannian manifolds have garnered increasinginterest in various applied areas. For instance, DNNs on spherical andhyperbolic manifolds have been designed to solve a wide range of computervision and nature language processing tasks. One of the key factors thatcontribute to the success of these networks is that spherical and hyperbolicmanifolds have the rich algebraic structures of gyrogroups and gyrovectorspaces. This enables principled and effective generalizations of the mostsuccessful DNNs to these manifolds. Recently, some works have shown that manyconcepts in the theory of gyrogroups and gyrovector spaces can also begeneralized to matrix manifolds such as Symmetric Positive Definite (SPD) andGrassmann manifolds. As a result, some building blocks for SPD and Grassmannneural networks, e.g., isometric models and multinomial logistic regression(MLR) can be derived in a way that is fully analogous to their spherical andhyperbolic counterparts. Building upon these works, we design fully-connected(FC) and convolutional layers for SPD neural networks. We also develop MLR onSymmetric Positive Semi-definite (SPSD) manifolds, and propose a method forperforming backpropagation with the Grassmann logarithmic map in the projectorperspective. We demonstrate the effectiveness of the proposed approach in thehuman action recognition and node classification tasks.</description><author>Xuan Son Nguyen, Shuo Yang, Aymeric Histace</author><pubDate>Wed, 29 May 2024 16:47:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19206v1</guid></item><item><title>TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms</title><link>http://arxiv.org/abs/2405.01242v3</link><description>We propose TRAMBA, a hybrid transformer and Mamba architecture for acousticand bone conduction speech enhancement, suitable for mobile and wearableplatforms. Bone conduction speech enhancement has been impractical to adopt inmobile and wearable platforms for several reasons: (i) data collection islabor-intensive, resulting in scarcity; (ii) there exists a performance gapbetween state of-art models with memory footprints of hundreds of MBs andmethods better suited for resource-constrained systems. To adapt TRAMBA tovibration-based sensing modalities, we pre-train TRAMBA with audio speechdatasets that are widely available. Then, users fine-tune with a small amountof bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% inPESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint andan inference speed up of up to 465 times. We integrate TRAMBA into real systemsand show that TRAMBA (i) improves battery life of wearables by up to 160% byrequiring less data sampling and transmission; (ii) generates higher qualityvoice in noisy environments than over-the-air speech; (iii) requires a memoryfootprint of less than 20.0 MB.</description><author>Yueyuan Sui, Minghui Zhao, Junxi Xia, Xiaofan Jiang, Stephen Xia</author><pubDate>Wed, 29 May 2024 16:46:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01242v3</guid></item><item><title>Contrastive-Adversarial and Diffusion: Exploring pre-training and fine-tuning strategies for sulcal identification</title><link>http://arxiv.org/abs/2405.19204v1</link><description>In the last decade, computer vision has witnessed the establishment ofvarious training and learning approaches. Techniques like adversarial learning,contrastive learning, diffusion denoising learning, and ordinary reconstructionlearning have become standard, representing state-of-the-art methodsextensively employed for fully training or pre-training networks across variousvision tasks. The exploration of fine-tuning approaches has emerged as acurrent focal point, addressing the need for efficient model tuning withreduced GPU memory usage and time costs while enhancing overall performance, asexemplified by methodologies like low-rank adaptation (LoRA). Key questionsarise: which pre-training technique yields optimal results - adversarial,contrastive, reconstruction, or diffusion denoising? How does the performanceof these approaches vary as the complexity of fine-tuning is adjusted? Thisstudy aims to elucidate the advantages of pre-training techniques andfine-tuning strategies to enhance the learning process of neural networks inindependent identical distribution (IID) cohorts. We underscore thesignificance of fine-tuning by examining various cases, including full tuning,decoder tuning, top-level tuning, and fine-tuning of linear parameters usingLoRA. Systematic summaries of model performance and efficiency are presented,leveraging metrics such as accuracy, time cost, and memory efficiency. Toempirically demonstrate our findings, we focus on a multi-tasksegmentation-classification challenge involving the paracingulate sulcus (PCS)using different 3D Convolutional Neural Network (CNN) architectures by usingthe TOP-OSLO cohort comprising 596 subjects.</description><author>Michail Mamalakis, Héloïse de Vareilles, Shun-Chin Jim Wu, Ingrid Agartz, Lynn Egeland Mørch-Johnsen, Jane Garrison, Jon Simons, Pietro Lio, John Suckling, Graham Murray</author><pubDate>Wed, 29 May 2024 16:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19204v1</guid></item><item><title>Continual Contrastive Spoken Language Understanding</title><link>http://arxiv.org/abs/2310.02699v2</link><description>Recently, neural networks have shown impressive progress across diversefields, with speech processing being no exception. However, recentbreakthroughs in this area require extensive offline training using largedatasets and tremendous computing resources. Unfortunately, these modelsstruggle to retain their previously acquired knowledge when learning new taskscontinually, and retraining from scratch is almost always impractical. In thispaper, we investigate the problem of learning sequence-to-sequence models forspoken language understanding in a class-incremental learning (CIL) setting andwe propose COCONUT, a CIL method that relies on the combination of experiencereplay and contrastive learning. Through a modified version of the standardsupervised contrastive loss applied only to the rehearsal samples, COCONUTpreserves the learned representations by pulling closer samples from the sameclass and pushing away the others. Moreover, we leverage a multimodalcontrastive loss that helps the model learn more discriminative representationsof the new data by aligning audio and text features. We also investigatedifferent contrastive designs to combine the strengths of the contrastive losswith teacher-student architectures used for distillation. Experiments on twoestablished SLU datasets reveal the effectiveness of our proposed approach andsignificant improvements over the baselines. We also show that COCONUT can becombined with methods that operate on the decoder side of the model, resultingin further metrics improvements.</description><author>Umberto Cappellazzo, Enrico Fini, Muqiao Yang, Daniele Falavigna, Alessio Brutti, Bhiksha Raj</author><pubDate>Wed, 29 May 2024 16:43:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02699v2</guid></item><item><title>$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation</title><link>http://arxiv.org/abs/2405.19203v1</link><description>This paper aims to introduce 3D Gaussian for efficient, expressive, andeditable digital avatar generation. This task faces two major challenges: (1)The unstructured nature of 3D Gaussian makes it incompatible with currentgeneration pipelines; (2) the expressive animation of 3D Gaussian in agenerative setting that involves training with multiple subjects remainsunexplored. In this paper, we propose a novel avatar generation method named$E^3$Gen, to effectively address these challenges. First, we propose a novelgenerative UV features plane representation that encodes unstructured 3DGaussian onto a structured 2D UV space defined by the SMPL-X parametric model.This novel representation not only preserves the representation ability of theoriginal 3D Gaussian but also introduces a shared structure among subjects toenable generative learning of the diffusion model. To tackle the secondchallenge, we propose a part-aware deformation module to achieve robust andaccurate full-body expressive pose control. Extensive experiments demonstratethat our method achieves superior performance in avatar generation and enablesexpressive full-body pose control and editing.</description><author>Weitian Zhang, Yichao Yan, Yunhui Liu, Xingdong Sheng, Xiaokang Yang</author><pubDate>Wed, 29 May 2024 16:43:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19203v1</guid></item><item><title>Vulnerable Road User Detection and Safety Enhancement: A Comprehensive Survey</title><link>http://arxiv.org/abs/2405.19202v1</link><description>Traffic incidents involving vulnerable road users (VRUs) constitute asignificant proportion of global road accidents. Advances in trafficcommunication ecosystems, coupled with sophisticated signal processing andmachine learning techniques, have facilitated the utilization of data fromdiverse sensors. Despite these advancements and the availability of extensivedatasets, substantial progress is required to mitigate traffic casualties. Thispaper provides a comprehensive survey of state-of-the-art technologies andmethodologies to enhance the safety of VRUs. The study delves into thecommunication networks between vehicles and VRUs, emphasizing the integrationof advanced sensors and the availability of relevant datasets. It explorespreprocessing techniques and data fusion methods to enhance sensor dataquality. Furthermore, our study assesses critical simulation environmentsessential for developing and testing VRU safety systems. Our research alsohighlights recent advances in VRU detection and classification algorithms,addressing challenges such as variable environmental conditions. Additionally,we cover cutting-edge research in predicting VRU intentions and behaviors,which is crucial for proactive collision avoidance strategies. Through thissurvey, we aim to provide a comprehensive understanding of the currentlandscape of VRU safety technologies, identifying areas of progress and areasneeding further research and development.</description><author>Renato M. Silva, Gregório F. Azevedo, Matheus V. V. Berto, Jean R. Rocha, Eduardo C. Fidelis, Matheus V. Nogueira, Pedro H. Lisboa, Tiago A. Almeida</author><pubDate>Wed, 29 May 2024 16:42:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19202v1</guid></item><item><title>Going beyond compositional generalization, DDPMs can produce zero-shot interpolation</title><link>http://arxiv.org/abs/2405.19201v1</link><description>Denoising Diffusion Probabilistic Models (DDPMs) exhibit remarkablecapabilities in image generation, with studies suggesting that they cangeneralize by composing latent factors learned from the training data. In thiswork, we go further and study DDPMs trained on strictly separate subsets of thedata distribution with large gaps on the support of the latent factors. We showthat such a model can effectively generate images in the unexplored,intermediate regions of the distribution. For instance, when trained on clearlysmiling and non-smiling faces, we demonstrate a sampling procedure which cangenerate slightly smiling faces without reference images (zero-shotinterpolation). We replicate these findings for other attributes as well asother datasets.$\href{https://github.com/jdeschena/ddpm-zero-shot-interpolation}{\text{Ourcode is available on GitHub.}}$</description><author>Justin Deschenaux, Igor Krawczuk, Grigorios Chrysos, Volkan Cevher</author><pubDate>Wed, 29 May 2024 16:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19201v1</guid></item><item><title>LOGO: Video Text Spotting with Language Collaboration and Glyph Perception Model</title><link>http://arxiv.org/abs/2405.19194v1</link><description>Video text spotting aims to simultaneously localize, recognize and track textinstances in videos. To address the limited recognition capability ofend-to-end methods, tracking the zero-shot results of state-of-the-art imagetext spotters directly can achieve impressive performance. However, owing tothe domain gap between different datasets, these methods usually obtain limitedtracking trajectories on extreme dataset. Fine-tuning transformer-based textspotters on specific datasets could yield performance enhancements, albeit atthe expense of considerable training resources. In this paper, we propose aLanguage Collaboration and Glyph Perception Model, termed LOGO to enhance theperformance of conventional text spotters through the integration of a synergymodule. To achieve this goal, a language synergy classifier (LSC) is designedto explicitly discern text instances from background noise in the recognitionstage. Specially, the language synergy classifier can output text content orbackground code based on the legibility of text regions, thus computinglanguage scores. Subsequently, fusion scores are computed by taking the averageof detection scores and language scores, and are utilized to re-score thedetection results before tracking. By the re-scoring mechanism, the proposedLSC facilitates the detection of low-resolution text instances while filteringout text-like regions. Besides, the glyph supervision and visual positionmixture module are proposed to enhance the recognition accuracy of noisy textregions, and acquire more discriminative tracking features, respectively.Extensive experiments on public benchmarks validate the effectiveness of theproposed method.</description><author>Hongen Liu, Yi Liu, Di Sun, Jiahao Wang, Gang Pan</author><pubDate>Wed, 29 May 2024 16:35:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19194v1</guid></item><item><title>Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem</title><link>http://arxiv.org/abs/2402.17606v2</link><description>Existing learning-based methods for solving job shop scheduling problems(JSSP) usually use off-the-shelf GNN models tailored to undirected graphs andneglect the rich and meaningful topological structures of disjunctive graphs(DGs). This paper proposes the topology-aware bidirectional graph attentionnetwork (TBGAT), a novel GNN architecture based on the attention mechanism, toembed the DG for solving JSSP in a local search framework. Specifically, TBGATembeds the DG from a forward and a backward view, respectively, where themessages are propagated by following the different topologies of the views andaggregated via graph attention. Then, we propose a novel operator based on themessage-passing mechanism to calculate the forward and backward topologicalsorts of the DG, which are the features for characterizing the topologicalstructures and exploited by our model. In addition, we theoretically andexperimentally show that TBGAT has linear computational complexity to thenumber of jobs and machines, respectively, strengthening our method's practicalvalue. Besides, extensive experiments on five synthetic datasets and sevenclassic benchmarks show that TBGAT achieves new SOTA results by outperforming awide range of neural methods by a large margin. All the code and data arepublicly available online at https://github.com/zcaicaros/TBGAT.</description><author>Cong Zhang, Zhiguang Cao, Yaoxin Wu, Wen Song, Jing Sun</author><pubDate>Wed, 29 May 2024 16:32:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17606v2</guid></item></channel></rss>