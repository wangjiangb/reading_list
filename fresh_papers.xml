<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 07 Aug 2024 13:00:24 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LLaVA-OneVision: Easy Visual Task Transfer</title><link>http://arxiv.org/abs/2408.03326v1</link><description>We present LLaVA-OneVision, a family of open large multimodal models (LMMs)developed by consolidating our insights into data, models, and visualrepresentations in the LLaVA-NeXT blog series. Our experimental resultsdemonstrate that LLaVA-OneVision is the first single model that cansimultaneously push the performance boundaries of open LMMs in three importantcomputer vision scenarios: single-image, multi-image, and video scenarios.Importantly, the design of LLaVA-OneVision allows strong transfer learningacross different modalities/scenarios, yielding new emerging capabilities. Inparticular, strong video understanding and cross-scenario capabilities aredemonstrated through task transfer from images to videos.</description><author>Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li</author><pubDate>Tue, 06 Aug 2024 17:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03326v1</guid></item><item><title>CoverBench: A Challenging Benchmark for Complex Claim Verification</title><link>http://arxiv.org/abs/2408.03325v1</link><description>There is a growing line of research on verifying the correctness of languagemodels' outputs. At the same time, LMs are being used to tackle complex queriesthat require reasoning. We introduce CoverBench, a challenging benchmarkfocused on verifying LM outputs in complex reasoning settings. Datasets thatcan be used for this purpose are often designed for other complex reasoningtasks (e.g., QA) targeting specific use-cases (e.g., financial tables),requiring transformations, negative sampling and selection of hard examples tocollect such a benchmark. CoverBench provides a diversified evaluation forcomplex claim verification in a variety of domains, types of reasoning,relatively long inputs, and a variety of standardizations, such as multiplerepresentations for tables where available, and a consistent schema. Wemanually vet the data for quality to ensure low levels of label noise. Finally,we report a variety of competitive baseline results to show CoverBench ischallenging and has very significant headroom. The data is available athttps://huggingface.co/datasets/google/coverbench .</description><author>Alon Jacovi, Moran Ambar, Eyal Ben-David, Uri Shaham, Amir Feder, Mor Geva, Dror Marcus, Avi Caciularu</author><pubDate>Tue, 06 Aug 2024 17:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03325v1</guid></item><item><title>ClassiFIM: An Unsupervised Method To Detect Phase Transitions</title><link>http://arxiv.org/abs/2408.03323v1</link><description>Estimation of the Fisher Information Metric (FIM-estimation) is an importanttask that arises in unsupervised learning of phase transitions, a problemproposed by physicists. This work completes the definition of the task bydefining rigorous evaluation metrics distMSE, distMSEPS, and distRE andintroduces ClassiFIM, a novel machine learning method designed to solve theFIM-estimation task. Unlike existing methods for unsupervised learning of phasetransitions, ClassiFIM directly estimates a well-defined quantity (the FIM),allowing it to be rigorously compared to any present and future other methodsthat estimate the same. ClassiFIM transforms a dataset for the FIM-estimationtask into a dataset for an auxiliary binary classification task and involvesselecting and training a model for the latter. We prove that the output ofClassiFIM approaches the exact FIM in the limit of infinite dataset size andunder certain regularity conditions. We implement ClassiFIM on multipledatasets, including datasets describing classical and quantum phasetransitions, and find that it achieves a good ground truth approximation withmodest computational resources. Furthermore, we independently implement twoalternative state-of-the-art methods for unsupervised estimation of phasetransition locations on the same datasets and find that ClassiFIM predicts suchlocations at least as well as these other methods. To emphasize the generalityof our method, we also propose and generate the MNIST-CNN dataset, whichconsists of the output of CNNs trained on MNIST for different hyperparameterchoices. Using ClassiFIM on this dataset suggests there is a phase transitionin the distribution of image-prediction pairs for CNNs trained on MNIST,demonstrating the broad scope of FIM-estimation beyond physics.</description><author>Victor Kasatkin, Evgeny Mozgunov, Nicholas Ezzell, Utkarsh Mishra, Itay Hen, Daniel Lidar</author><pubDate>Tue, 06 Aug 2024 17:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03323v1</guid></item><item><title>Segment Anything in Medical Images and Videos: Benchmark and Deployment</title><link>http://arxiv.org/abs/2408.03322v1</link><description>Recent advances in segmentation foundation models have enabled accurate andefficient segmentation across a wide range of natural images and videos, buttheir utility to medical data remains unclear. In this work, we first present acomprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11medical image modalities and videos and point out its strengths and weaknessesby comparing it to SAM1 and MedSAM. Then, we develop a transfer learningpipeline and demonstrate SAM2 can be quickly adapted to medical domain byfine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and GradioAPI for efficient 3D image and video segmentation. The code has been madepublicly available at \url{https://github.com/bowang-lab/MedSAM}.</description><author>Jun Ma, Sumin Kim, Feifei Li, Mohammed Baharoon, Reza Asakereh, Hongwei Lyu, Bo Wang</author><pubDate>Tue, 06 Aug 2024 17:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03322v1</guid></item><item><title>Getting it Right: Improving Spatial Consistency in Text-to-Image Models</title><link>http://arxiv.org/abs/2404.01197v2</link><description>One of the key shortcomings in current text-to-image (T2I) models is theirinability to consistently generate images which faithfully follow the spatialrelationships specified in the text prompt. In this paper, we offer acomprehensive investigation of this limitation, while also developing datasetsand methods that support algorithmic solutions to improve spatial reasoning inT2I models. We find that spatial relationships are under-represented in theimage descriptions found in current vision-language datasets. To alleviate thisdata bottleneck, we create SPRIGHT, the first spatially focused, large-scaledataset, by re-captioning 6 million images from 4 widely used vision datasetsand through a 3-fold evaluation and analysis pipeline, show that SPRIGHTimproves the proportion of spatial relationships in existing datasets. We showthe efficacy of SPRIGHT data by showing that using only $\sim$0.25% of SPRIGHTresults in a 22% improvement in generating spatially accurate images while alsoimproving FID and CMMD scores. We also find that training on images containinga larger number of objects leads to substantial improvements in spatialconsistency, including state-of-the-art results on T2I-CompBench with a spatialscore of 0.2133, by fine-tuning on &lt;500 images. Through a set of controlledexperiments and ablations, we document additional findings that could supportfuture work that seeks to understand factors that affect spatial consistency intext-to-image models.</description><author>Agneet Chatterjee, Gabriela Ben Melech Stan, Estelle Aflalo, Sayak Paul, Dhruba Ghosh, Tejas Gokhale, Ludwig Schmidt, Hannaneh Hajishirzi, Vasudev Lal, Chitta Baral, Yezhou Yang</author><pubDate>Tue, 06 Aug 2024 17:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01197v2</guid></item><item><title>MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts</title><link>http://arxiv.org/abs/2407.21770v2</link><description>We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)architecture designed for pre-training mixed-modal, early-fusion languagemodels. MoMa processes images and text in arbitrary sequences by dividingexpert modules into modality-specific groups. These groups exclusively processdesignated tokens while employing learned routing within each group to maintainsemantically informed adaptivity. Our empirical results reveal substantialpre-training efficiency gains through this modality-specific parameterallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,featuring 4 text experts and 4 image experts, achieves impressive FLOPssavings: 3.7x overall, with 2.6x for text and 5.2x for image processingcompared to a compute-equivalent dense baseline, measured by pre-training loss.This outperforms the standard expert-choice MoE with 8 mixed-modal experts,which achieves 3x overall FLOPs savings (3x for text, 2.8x for image).Combining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPssavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combinationhurts performance in causal inference due to increased sensitivity to routeraccuracy. These results demonstrate MoMa's potential to significantly advancethe efficiency of mixed-modal, early-fusion language model pre-training, pavingthe way for more resource-efficient and capable multimodal AI systems.</description><author>Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke Zettlemoyer, Armen Aghajanyan</author><pubDate>Tue, 06 Aug 2024 17:57:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21770v2</guid></item><item><title>LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization</title><link>http://arxiv.org/abs/2404.01282v2</link><description>Temporal Action Localization (TAL) involves localizing and classifying actionsnippets in an untrimmed video. The emergence of large video foundation modelshas led RGB-only video backbones to outperform previous methods needing bothRGB and optical flow modalities. Leveraging these large models is often limitedto training only the TAL head due to the prohibitively large GPU memoryrequired to adapt the video backbone for TAL. To overcome this limitation, weintroduce LoSA, the first memory-and-parameter-efficient backbone adapterdesigned specifically for TAL to handle untrimmed videos. LoSA specializes forTAL by introducing Long-Short-range Adapters that adapt the intermediate layersof the video backbone over different temporal ranges. These adapters runparallel to the video backbone to significantly reduce memory footprint. LoSAalso includes Long-Short-range Gated Fusion that strategically combines theoutput of these adapters from the video backbone layers to enhance the videofeatures provided to the TAL head. Experiments show that LoSA significantlyoutperforms all existing methods on standard TAL benchmarks, THUMOS-14 andActivityNet-v1.3, by scaling end-to-end backbone adaptation tobillion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging thembeyond head-only transfer learning.</description><author>Akshita Gupta, Gaurav Mittal, Ahmed Magooda, Ye Yu, Graham W. Taylor, Mei Chen</author><pubDate>Tue, 06 Aug 2024 17:56:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01282v2</guid></item><item><title>Hedge Fund Portfolio Construction Using PolyModel Theory and iTransformer</title><link>http://arxiv.org/abs/2408.03320v1</link><description>When constructing portfolios, a key problem is that a lot of financial timeseries data are sparse, making it challenging to apply machine learningmethods. Polymodel theory can solve this issue and demonstrate superiority inportfolio construction from various aspects. To implement the PolyModel theoryfor constructing a hedge fund portfolio, we begin by identifying an asset pool,utilizing over 10,000 hedge funds for the past 29 years' data. PolyModel theoryalso involves choosing a wide-ranging set of risk factors, which includesvarious financial indices, currencies, and commodity prices. This comprehensiveselection mirrors the complexities of the real-world environment. Leveraging onthe PolyModel theory, we create quantitative measures such as Long-term Alpha,Long-term Ratio, and SVaR. We also use more classical measures like the Sharperatio or Morningstar's MRAR. To enhance the performance of the constructedportfolio, we also employ the latest deep learning techniques (iTransformer) tocapture the upward trend, while efficiently controlling the downside, using allthe features. The iTransformer model is specifically designed to address thechallenges in high-dimensional time series forecasting and could largelyimprove our strategies. More precisely, our strategies achieve better Sharperatio and annualized return. The above process enables us to create multipleportfolio strategies aiming for high returns and low risks when compared tovarious benchmarks.</description><author>Siqiao Zhao, Zhikang Dong, Zeyu Cao, Raphael Douady</author><pubDate>Tue, 06 Aug 2024 17:55:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03320v1</guid></item><item><title>Training LLMs to Recognize Hedges in Spontaneous Narratives</title><link>http://arxiv.org/abs/2408.03319v1</link><description>Hedges allow speakers to mark utterances as provisional, whether to signalnon-prototypicality or "fuzziness", to indicate a lack of commitment to anutterance, to attribute responsibility for a statement to someone else, toinvite input from a partner, or to soften critical feedback in the service offace-management needs. Here we focus on hedges in an experimentallyparameterized corpus of 63 Roadrunner cartoon narratives spontaneously producedfrom memory by 21 speakers for co-present addressees, transcribed to text(Galati and Brennan, 2010). We created a gold standard of hedges annotated byhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-basedapproaches for hedge detection: fine-tuning BERT, and zero and few-shotprompting with GPT-4o and LLaMA-3. The best-performing approach was afine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis onthe top performing approaches, we used an LLM-in-the-Loop approach to improvethe gold standard coding, as well as to highlight cases in which hedges areambiguous in linguistically interesting ways that will guide future research.This is the first step in our research program to train LLMs to interpret andgenerate collateral signals appropriately and meaningfully in conversation.</description><author>Amie J. Paige, Adil Soubki, John Murzaku, Owen Rambow, Susan E. Brennan</author><pubDate>Tue, 06 Aug 2024 17:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03319v1</guid></item><item><title>Maximal Volume Matrix Cross Approximation for Image Compression and Least Squares Solution</title><link>http://arxiv.org/abs/2309.17403v2</link><description>We study the classic matrix cross approximation based on the maximal volumesubmatrices. Our main results consist of an improvement of the classic estimatefor matrix cross approximation and a greedy approach for finding the maximalvolume submatrices. More precisely, we present a new proof of the classicestimate of the inequality with an improved constant. Also, we present a familyof greedy maximal volume algorithms to improve the computational efficiency ofmatrix cross approximation. The proposed algorithms are shown to havetheoretical guarantees of convergence. Finally, we present two applications:image compression and the least squares approximation of continuous functions.Our numerical results at the end of the paper demonstrate the effectiveperformance of our approach.</description><author>Kenneth Allen, Ming-Jun Lai, Zhaiming Shen</author><pubDate>Tue, 06 Aug 2024 17:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17403v2</guid></item><item><title>Segment anything model 2: an application to 2D and 3D medical images</title><link>http://arxiv.org/abs/2408.00756v2</link><description>Segment Anything Model (SAM) has gained significant attention because of itsability to segment varous objects in images given a prompt. The recentlydeveloped SAM 2 has extended this ability to video inputs. This opens anopportunity to apply SAM to 3D images, one of the fundamental tasks in themedical imaging field. In this paper, we extensively evaluate SAM 2's abilityto segment both 2D and 3D medical images by first collecting 18 medical imagingdatasets, including common 3D modalities such as computed tomography (CT),magnetic resonance imaging (MRI), and positron emission tomography (PET) aswell as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines ofSAM 2 are considered: (1) multi-frame 3D segmentation, where prompts areprovided to one or multiple slice(s) selected from the volume, and (2)single-frame 2D segmentation, where prompts are provided to each slice. Theformer is only applicable to 3D modalities, while the latter applies to both 2Dand 3D modalities. Our results show that SAM 2 exhibits similar performance asSAM under single-frame 2D segmentation, and has variable performance undermulti-frame 3D segmentation depending on the choices of slices to annotate, thedirection of the propagation, the predictions utilized during the propagation,etc.</description><author>Haoyu Dong, Hanxue Gu, Yaqian Chen, Jichen Yang, Maciej A. Mazurowski</author><pubDate>Tue, 06 Aug 2024 17:40:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00756v2</guid></item><item><title>Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing</title><link>http://arxiv.org/abs/2402.00035v4</link><description>As deep neural networks (DNNs) are becoming the prominent solution for manycomputational problems, the aviation industry seeks to explore their potentialin alleviating pilot workload and in improving operational safety. However, theuse of DNNs in this type of safety-critical applications requires a thoroughcertification process. This need can be addressed through formal verification,which provides rigorous assurances -- e.g.,~by proving the absence of certainmispredictions. In this case-study paper, we demonstrate this process using animage-classifier DNN currently under development at Airbus and intended for useduring the aircraft taxiing phase. We use formal methods to assess this DNN'srobustness to three common image perturbation types: noise, brightness andcontrast, and some of their combinations. This process entails multipleinvocations of the underlying verifier, which might be computationallyexpensive; and we therefore propose a method that leverages the monotonicity ofthese robustness properties, as well as the results of past verificationqueries, in order to reduce the overall number of verification queries requiredby nearly 60%. Our results provide an indication of the level of robustnessachieved by the DNN classifier under study, and indicate that it isconsiderably more vulnerable to noise than to brightness or contrastperturbations.</description><author>Yizhak Elboher, Raya Elsaleh, Omri Isac, Mélanie Ducoffe, Audrey Galametz, Guillaume Povéda, Ryma Boumazouza, Noémie Cohen, Guy Katz</author><pubDate>Tue, 06 Aug 2024 17:36:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00035v4</guid></item><item><title>Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</title><link>http://arxiv.org/abs/2408.03314v1</link><description>Enabling LLMs to improve their outputs by using more test-time computation isa critical step towards building generally self-improving agents that canoperate on open-ended natural language. In this paper, we study the scaling ofinference-time computation in LLMs, with a focus on answering the question: ifan LLM is allowed to use a fixed but non-trivial amount of inference-timecompute, how much can it improve its performance on a challenging prompt?Answering this question has implications not only on the achievable performanceof LLMs, but also on the future of LLM pretraining and how one should tradeoffinference-time and pre-training compute. Despite its importance, littleresearch attempted to understand the scaling behaviors of various test-timeinference methods. Moreover, current work largely provides negative results fora number of these strategies. In this work, we analyze two primary mechanismsto scale test-time computation: (1) searching against dense, process-basedverifier reward models; and (2) updating the model's distribution over aresponse adaptively, given the prompt at test time. We find that in both cases,the effectiveness of different approaches to scaling test-time computecritically varies depending on the difficulty of the prompt. This observationmotivates applying a "compute-optimal" scaling strategy, which acts to mosteffectively allocate test-time compute adaptively per prompt. Using thiscompute-optimal strategy, we can improve the efficiency of test-time computescaling by more than 4x compared to a best-of-N baseline. Additionally, in aFLOPs-matched evaluation, we find that on problems where a smaller base modelattains somewhat non-trivial success rates, test-time compute can be used tooutperform a 14x larger model.</description><author>Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar</author><pubDate>Tue, 06 Aug 2024 17:35:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03314v1</guid></item><item><title>ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation</title><link>http://arxiv.org/abs/2402.04492v2</link><description>This paper introduces the ColorSwap dataset, designed to assess and improvethe proficiency of multimodal models in matching objects with their colors. Thedataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000examples. Each example includes a caption-image pair, along with a``color-swapped'' pair. We follow the Winoground schema: the two captions in anexample have the same words, but the color words have been rearranged to modifydifferent objects. The dataset was created through a novel blend of automatedcaption and image generation with humans in the loop. We evaluate image-textmatching (ITM) and visual language models (VLMs) and find that even the latestones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% onour main VLM metric, although they may improve with more advanced promptingtechniques. On the main ITM metric, contrastive models such as CLIP and SigLIPperform close to chance (at 12% and 30%, respectively), although thenon-contrastive BLIP ITM model is stronger (87%). We also find that finetuningon fewer than 2,000 examples yields significant performance gains on thisout-of-distribution word-order understanding task. The dataset is here:https://github.com/Top34051/colorswap and here:https://huggingface.co/datasets/stanfordnlp/colorswap.</description><author>Jirayu Burapacheep, Ishan Gaur, Agam Bhatia, Tristan Thrush</author><pubDate>Tue, 06 Aug 2024 17:31:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04492v2</guid></item><item><title>MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation</title><link>http://arxiv.org/abs/2408.03312v1</link><description>Recent advancements in the field of Diffusion Transformers have substantiallyimproved the generation of high-quality 2D images, 3D videos, and 3D shapes.However, the effectiveness of the Transformer architecture in the domain ofco-speech gesture generation remains relatively unexplored, as priormethodologies have predominantly employed the Convolutional Neural Network(CNNs) or simple a few transformer layers. In an attempt to bridge thisresearch gap, we introduce a novel Masked Diffusion Transformer for co-speechgesture generation, referred to as MDT-A2G, which directly implements thedenoising process on gesture sequences. To enhance the contextual reasoningcapability of temporally aligned speech-driven gestures, we incorporate a novelMasked Diffusion Transformer. This model employs a mask modeling schemespecifically designed to strengthen temporal relation learning among sequencegestures, thereby expediting the learning process and leading to coherent andrealistic motions. Apart from audio, Our MDT-A2G model also integratesmulti-modal information, encompassing text, emotion, and identity. Furthermore,we propose an efficient inference strategy that diminishes the denoisingcomputation by leveraging previously calculated results, thereby achieving aspeedup with negligible performance degradation. Experimental resultsdemonstrate that MDT-A2G excels in gesture generation, boasting a learningspeed that is over 6$\times$ faster than traditional diffusion transformers andan inference speed that is 5.7$\times$ than the standard diffusion model.</description><author>Xiaofeng Mao, Zhengkai Jiang, Qilin Wang, Chencan Fu, Jiangning Zhang, Jiafu Wu, Yabiao Wang, Chengjie Wang, Wei Li, Mingmin Chi</author><pubDate>Tue, 06 Aug 2024 17:29:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03312v1</guid></item><item><title>SteP: Stacked LLM Policies for Web Actions</title><link>http://arxiv.org/abs/2310.03720v3</link><description>Performing tasks on the web presents fundamental challenges to large languagemodels (LLMs), including combinatorially large open-world tasks and variationsacross web interfaces. Simply specifying a large prompt to handle all possiblebehaviors and states is extremely complex, and results in behavior leaksbetween unrelated behaviors. Decomposition to distinct policies can addressthis challenge, but requires carefully handing off control between policies. Wepropose Stacked LLM Policies for Web Actions (SteP), an approach to dynamicallycompose policies to solve a diverse set of web tasks. SteP defines a MarkovDecision Process where the state is a stack of policies representing thecontrol state, i.e., the chain of policy calls. Unlike traditional methods thatare restricted to static hierarchies, SteP enables dynamic control that adaptsto the complexity of the task. We evaluate SteP against multiple baselines andweb environments including WebArena, MiniWoB++, and a CRM. On WebArena, StePimproves (14.9\% to 33.5\%) over SOTA that use GPT-4 policies, while onMiniWob++, SteP is competitive with prior works while using significantly lessdata. Our code and data are available athttps://asappresearch.github.io/webagents-step.</description><author>Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, Ryan McDonald</author><pubDate>Tue, 06 Aug 2024 17:26:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.03720v3</guid></item><item><title>Comprehensive Attribution: Inherently Explainable Vision Model with Feature Detector</title><link>http://arxiv.org/abs/2407.19308v2</link><description>As deep vision models' popularity rapidly increases, there is a growingemphasis on explanations for model predictions. The inherently explainableattribution method aims to enhance the understanding of model behavior byidentifying the important regions in images that significantly contribute topredictions. It is achieved by cooperatively training a selector (generating anattribution map to identify important features) and a predictor (makingpredictions using the identified features). Despite many advancements, existingmethods suffer from the incompleteness problem, where discriminative featuresare masked out, and the interlocking problem, where the non-optimized selectorinitially selects noise, causing the predictor to fit on this noise andperpetuate the cycle. To address these problems, we introduce a new objectivethat discourages the presence of discriminative features in the masked-outregions thus enhancing the comprehensiveness of feature selection. Apre-trained detector is introduced to detect discriminative features in themasked-out region. If the selector selects noise instead of discriminativefeatures, the detector can observe and break the interlocking situation bypenalizing the selector. Extensive experiments show that our model makesaccurate predictions with higher accuracy than the regular black-box model, andproduces attribution maps with high feature coverage, localization ability,fidelity and robustness. Our code will be available at\href{https://github.com/Zood123/COMET}{https://github.com/Zood123/COMET}.</description><author>Xianren Zhang, Dongwon Lee, Suhang Wang</author><pubDate>Tue, 06 Aug 2024 17:22:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19308v2</guid></item><item><title>Pre-training and in-context learning IS Bayesian inference a la De Finetti</title><link>http://arxiv.org/abs/2408.03307v1</link><description>Accurately gauging uncertainty on the underlying environment is alongstanding goal of intelligent systems. We characterize which latent conceptspre-trained sequence models are naturally able to reason with. We go back to DeFinetti's predictive view of Bayesian reasoning: instead of modeling latentparameters through priors and likelihoods like topic models do, De Finetti haslong advocated for modeling exchangeable (permutation invariant) sequences ofobservables. According to this view, pre-training autoregressive modelsformulates informed beliefs based on prior observations ("empirical Bayes"),and forward generation is a simulated instantiation of an environment("posterior inference"). This connection allows extending in-context learning(ICL) beyond predictive settings, highlighting sequence models' ability toperform explicit statistical inference. In particular, we show the sequenceprediction loss over exchangeable documents controls performance on downstreamtasks where uncertainty quantification is key. Empirically, we propose anddemonstrate several approaches for encoding exchangeability in sequence modelarchitectures: data augmentation, regularization, and causal masking.</description><author>Naimeng Ye, Hanming Yang, Andrew Siah, Hongseok Namkoong</author><pubDate>Tue, 06 Aug 2024 17:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03307v1</guid></item><item><title>Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks</title><link>http://arxiv.org/abs/2408.03304v1</link><description>Etruscan mirrors constitute a significant category in Etruscan art,characterized by elaborate figurative illustrations featured on their backside.A laborious and costly aspect of their analysis and documentation is the taskof manually tracing these illustrations. In previous work, a methodology hasbeen proposed to automate this process, involving photometric-stereo scanningin combination with deep neural networks. While achieving quantitativeperformance akin to an expert annotator, some results still lack qualitativeprecision and, thus, require annotators for inspection and potentialcorrection, maintaining resource intensity. In response, we propose a deepneural network trained to interactively refine existing annotations based onhuman guidance. Our human-in-the-loop approach streamlines annotation,achieving equal quality with up to 75% less manual input required. Moreover,during the refinement process, the relative improvement of our methodology overpure manual labeling reaches peak values of up to 26%, attaining drasticallybetter quality quicker. By being tailored to the complex task of segmentingintricate lines, specifically distinguishing it from previous methods, ourapproach offers drastic improvements in efficacy, transferable to a broadspectrum of applications beyond Etruscan mirrors.</description><author>Rafael Sterzinger, Christian Stippel, Robert Sablatnig</author><pubDate>Tue, 06 Aug 2024 17:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03304v1</guid></item><item><title>Deep-learning Assisted Detection and Quantification of (oo)cysts of Giardia and Cryptosporidium on Smartphone Microscopy Images</title><link>http://arxiv.org/abs/2304.05339v2</link><description>The consumption of microbial-contaminated food and water is responsible forthe deaths of millions of people annually. Smartphone-based microscopy systemsare portable, low-cost, and more accessible alternatives for the detection ofGiardia and Cryptosporidium than traditional brightfield microscopes. However,the images from smartphone microscopes are noisier and require manual cystidentification by trained technicians, usually unavailable in resource-limitedsettings. Automatic detection of (oo)cysts using deep-learning-based objectdetection could offer a solution for this limitation. We evaluate theperformance of four state-of-the-art object detectors to detect (oo)cysts ofGiardia and Cryptosporidium on a custom dataset that includes both smartphoneand brightfield microscopic images from vegetable samples. Faster RCNN,RetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer(Deformable DETR) deep-learning models were employed to explore their efficacyand limitations. Our results show that while the deep-learning models performbetter with the brightfield microscopy image dataset than the smartphonemicroscopy image dataset, the smartphone microscopy predictions are stillcomparable to the prediction performance of non-experts. Also, we publiclyrelease brightfield and smartphone microscopy datasets with the benchmarkresults for the detection of Giardia and Cryptosporidium, independentlycaptured on reference (or standard lab setting) and vegetable samples. Our codeand dataset are available athttps://github.com/naamiinepal/smartphone_microscopy andhttps://doi.org/10.5281/zenodo.7813183, respectively.</description><author>Suprim Nakarmi, Sanam Pudasaini, Safal Thapaliya, Pratima Upretee, Retina Shrestha, Basant Giri, Bhanu Bhakta Neupane, Bishesh Khanal</author><pubDate>Tue, 06 Aug 2024 17:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05339v2</guid></item><item><title>Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges</title><link>http://arxiv.org/abs/2408.03303v1</link><description>Object recognition technologies hold the potential to support blind andlow-vision people in navigating the world around them. However, the gap betweenbenchmark performances and practical usability remains a significant challenge.This paper presents a study aimed at understanding blind users' interactionwith object recognition systems for identifying and avoiding errors. Leveraginga pre-existing object recognition system, URCam, fine-tuned for our experiment,we conducted a user study involving 12 blind and low-vision participants.Through in-depth interviews and hands-on error identification tasks, we gainedinsights into users' experiences, challenges, and strategies for identifyingerrors in camera-based assistive technologies and object recognition systems.During interviews, many participants preferred independent error review, whileexpressing apprehension toward misrecognitions. In the error identificationtask, participants varied viewpoints, backgrounds, and object sizes in theirimages to avoid and overcome errors. Even after repeating the task,participants identified only half of the errors, and the proportion of errorsidentified did not significantly differ from their first attempts. Based onthese insights, we offer implications for designing accessible interfacestailored to the needs of blind and low-vision users in identifying objectrecognition errors.</description><author>Jonggi Hong, Hernisa Kacorri</author><pubDate>Tue, 06 Aug 2024 17:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03303v1</guid></item><item><title>TextIM: Part-aware Interactive Motion Synthesis from Text</title><link>http://arxiv.org/abs/2408.03302v1</link><description>In this work, we propose TextIM, a novel framework for synthesizingTEXT-driven human Interactive Motions, with a focus on the precise alignment ofpart-level semantics. Existing methods often overlook the critical roles ofinteractive body parts and fail to adequately capture and align part-levelsemantics, resulting in inaccuracies and even erroneous movement outcomes. Toaddress these issues, TextIM utilizes a decoupled conditional diffusionframework to enhance the detailed alignment between interactive movements andcorresponding semantic intents from textual descriptions. Our approachleverages large language models, functioning as a human brain, to identifyinteracting human body parts and to comprehend interaction semantics togenerate complicated and subtle interactive motion. Guided by the refinedmovements of the interacting parts, TextIM further extends these movements intoa coherent whole-body motion. We design a spatial coherence module tocomplement the entire body movements while maintaining consistency and harmonyacross body parts using a part graph convolutional network. For training andevaluation, we carefully selected and re-labeled interactive motions fromHUMANML3D to develop a specialized dataset. Experimental results demonstratethat TextIM produces semantically accurate human interactive motions,significantly enhancing the realism and applicability of synthesizedinteractive motions in diverse scenarios, even including interactions withdeformable and dynamically changing objects.</description><author>Siyuan Fan, Bo Du, Xiantao Cai, Bo Peng, Longling Sun</author><pubDate>Tue, 06 Aug 2024 17:08:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03302v1</guid></item><item><title>PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single Highly-Ambiguous RGB Images</title><link>http://arxiv.org/abs/2405.11914v2</link><description>Generating 3D shapes from single RGB images is essential in variousapplications such as robotics. Current approaches typically target imagescontaining clear and complete visual descriptions of the object, withoutconsidering common realistic cases where observations of objects that arelargely occluded or truncated. We thus propose a transformer-basedautoregressive model to generate the probabilistic distribution of 3D shapesconditioned on an RGB image containing potentially highly ambiguousobservations of the object. To handle realistic scenarios such as occlusion orfield-of-view truncation, we create simulated image-to-shape training pairsthat enable improved fine-tuning for real-world scenarios. We then adoptcross-attention to effectively identify the most relevant region of interestfrom the input image for shape generation. This enables inference of sampledshapes with reasonable diversity and strong alignment with the input image. Wetrain and test our model on our synthetic data then fine-tune and test it onreal-world data. Experiments demonstrate that our model outperforms state ofthe art in both scenarios.</description><author>Yiheng Xiong, Angela Dai</author><pubDate>Tue, 06 Aug 2024 17:00:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11914v2</guid></item><item><title>KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models</title><link>http://arxiv.org/abs/2408.03297v1</link><description>By integrating external knowledge, Retrieval-Augmented Generation (RAG) hasbecome an effective strategy for mitigating the hallucination problems thatlarge language models (LLMs) encounter when dealing with knowledge-intensivetasks. However, in the process of integrating external non-parametricsupporting evidence with internal parametric knowledge, inevitable knowledgeconflicts may arise, leading to confusion in the model's responses. To enhancethe knowledge selection of LLMs in various contexts, some research has focusedon refining their behavior patterns through instruction-tuning. Nonetheless,due to the absence of explicit negative signals and comparative objectives,models fine-tuned in this manner may still exhibit undesirable behaviors in theintricate and realistic retrieval scenarios. To this end, we propose aKnowledge-aware Preference Optimization, dubbed KaPO, aimed at achievingcontrollable knowledge selection in real retrieval scenarios. Concretely, weexplore and simulate error types across diverse context combinations and learnhow to avoid these negative signals through preference optimization methods.Simultaneously, by adjusting the balance between response length and theproportion of preference data representing different behavior patterns, weenhance the adherence capabilities and noise robustness of LLMs in a balancedmanner. Experimental results show that KaPO outperforms previous methods forhandling knowledge conflicts by over 37%, while also exhibiting robustgeneralization across various out-of-distribution datasets.</description><author>Ruizhe Zhang, Yongxin Xu, Yuzhen Xiao, Runchuan Zhu, Xinke Jiang, Xu Chu, Junfeng Zhao, Yasha Wang</author><pubDate>Tue, 06 Aug 2024 16:55:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03297v1</guid></item><item><title>Static IR Drop Prediction with Attention U-Net and Saliency-Based Explainability</title><link>http://arxiv.org/abs/2408.03292v1</link><description>There has been significant recent progress to reduce the computational effortof static IR drop analysis using neural networks, and modeling as animage-to-image translation task. A crucial issue is the lack of sufficient datafrom real industry designs to train these networks. Additionally, there is nomethodology to explain a high-drop pixel in a predicted IR drop image to itsspecific root-causes. In this work, we first propose a U-Net neural networkmodel with attention gates which is specifically tailored to achieve fast andaccurate image-based static IR drop prediction. Attention gates allow selectiveemphasis on relevant parts of the input data without supervision which isdesired because of the often sparse nature of the IR drop map. We propose atwo-phase training process which utilizes a mix of artificially-generated dataand a limited number of points from real designs. The results are, on-average,18% (53%) better in MAE and 14% (113%) in F1 score compared to the winner ofthe ICCAD 2023 contest (and U-Net only) when tested on real designs. Second, wepropose a fast method using saliency maps which can explain a predicted IR dropin terms of specific input pixels contributing the most to a drop. In ourexperiments, we show the number of high IR drop pixels can be reducedon-average by 18% by mimicking upsize of a tiny portion of PDN's resistiveedges.</description><author>Lizi Zhang, Azadeh Davoodi</author><pubDate>Tue, 06 Aug 2024 16:41:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03292v1</guid></item><item><title>DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers</title><link>http://arxiv.org/abs/2408.03291v1</link><description>Vision transformers (ViTs) have garnered significant attention for theirperformance in vision tasks; however, the high computational cost andsignificant latency issues have hinder widespread adoption. Post-trainingquantization (PTQ), a promising method for model compression, still facesaccuracy degradation challenges with ViTs. There are two reasons for this: theexisting quantization paradigm does not fit the power-law distribution ofpost-Softmax activations well, and accuracy inevitably decreases afterreparameterizing post-LayerNorm activations. We propose a Distribution-Friendlyand Outlier-Aware Post-training Quantization method for Vision Transformers,named DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers andintroduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses moreon values near 1, more accurately preserving the power-law distribution ofpost-Softmax activations, and achieves favorable results. Moreover, whenreparameterizing post-LayerNorm activations from channel-wise to layer-wisequantization, the accuracy degradation is mainly due to the significant impactof outliers in the scaling factors. Therefore, DopQ-ViT proposes a method toSearch for the Optimal Scaling Factor, denoted as SOSF, which compensates forthe influence of outliers and preserves the performance of the quantizationmodel. DopQ-ViT has undergone extensive validation and demonstrates significantperformance improvements in quantization models, particularly in low-bitsettings.</description><author>Lianwei Yang, Haisong Gong</author><pubDate>Tue, 06 Aug 2024 16:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03291v1</guid></item><item><title>SARA: Singular-Value Based Adaptive Low-Rank Adaption</title><link>http://arxiv.org/abs/2408.03290v1</link><description>With the increasing number of parameters in large pre-trained models, LoRA asa parameter-efficient fine-tuning(PEFT) method is widely used for not addinginference overhead. The LoRA method assumes that weight changes duringfine-tuning can be approximated by low-rank matrices. However, the rank valuesneed to be manually verified to match different downstream tasks, and theycannot accommodate the varying importance of different layers in the model. Inthis work, we first analyze the relationship between the performance ofdifferent layers and their ranks using SVD. Based on this, we design theSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively findsthe rank during initialization by performing SVD on the pre-trained weights.Additionally, we explore the Mixture-of-SARA(Mo-SARA), which significantlyreduces the number of parameters by fine-tuning only multiple parallel sets ofsingular values controlled by a router. Extensive experiments on variouscomplex tasks demonstrate the simplicity and parameter efficiency of ourmethods. They can effectively and adaptively find the most suitable rank foreach layer of each model.</description><author>Jihao Gu, Shuai Chen, Zelin Wang, Yibo Zhang, Ping Gong</author><pubDate>Tue, 06 Aug 2024 16:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03290v1</guid></item><item><title>Position: Topological Deep Learning is the New Frontier for Relational Learning</title><link>http://arxiv.org/abs/2402.08871v3</link><description>Topological deep learning (TDL) is a rapidly evolving field that usestopological features to understand and design deep learning models. This paperposits that TDL is the new frontier for relational learning. TDL may complementgraph representation learning and geometric deep learning by incorporatingtopological concepts, and can thus provide a natural choice for various machinelearning settings. To this end, this paper discusses open problems in TDL,ranging from practical benefits to theoretical foundations. For each problem,it outlines potential solutions and future research opportunities. At the sametime, this paper serves as an invitation to the scientific community toactively participate in TDL research to unlock the potential of this emergingfield.</description><author>Theodore Papamarkou, Tolga Birdal, Michael Bronstein, Gunnar Carlsson, Justin Curry, Yue Gao, Mustafa Hajij, Roland Kwitt, Pietro Liò, Paolo Di Lorenzo, Vasileios Maroulas, Nina Miolane, Farzana Nasrin, Karthikeyan Natesan Ramamurthy, Bastian Rieck, Simone Scardapane, Michael T. Schaub, Petar Veličković, Bei Wang, Yusu Wang, Guo-Wei Wei, Ghada Zamzmi</author><pubDate>Tue, 06 Aug 2024 16:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08871v3</guid></item><item><title>RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with Occlusion Handling</title><link>http://arxiv.org/abs/2311.14242v2</link><description>In the domain of 3D Human Pose Estimation, which finds widespread dailyapplications, the requirement for convenient acquisition equipment continues togrow. To satisfy this demand, we set our sights on a short-baseline binocularsetting that offers both portability and a geometric measurement property thatradically mitigates depth ambiguity. However, as the binocular baselineshortens, two serious challenges emerge: first, the robustness of 3Dreconstruction against 2D errors deteriorates; and second, occlusion reoccursdue to the limited visual differences between two views. To address the firstchallenge, we propose the Stereo Co-Keypoints Estimation module to improve theview consistency of 2D keypoints and enhance the 3D robustness. In this module,the disparity is utilized to represent the correspondence of binocular 2Dpoints and the Stereo Volume Feature is introduced to contain binocularfeatures across different disparities. Through the regression of SVF, two-view2D keypoints are simultaneously estimated in a collaborative way whichrestricts their view consistency. Furthermore, to deal with occlusions, aPre-trained Pose Transformer module is introduced. Through this module, 3Dposes are refined by perceiving pose coherence, a representation of jointcorrelations. This perception is injected by the Pose Transformer network andlearned through a pre-training task that recovers iterative masked joints.Comprehensive experiments carried out on H36M and MHAD datasets, complementedby visualizations, validate the effectiveness of our approach in theshort-baseline binocular 3D Human Pose Estimation and occlusion handling.</description><author>Xiaoyue Wan, Zhuo Chen, Yiming Bao, Xu Zhao</author><pubDate>Tue, 06 Aug 2024 16:36:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14242v2</guid></item><item><title>GenAI Arena: An Open Evaluation Platform for Generative Models</title><link>http://arxiv.org/abs/2406.04485v3</link><description>Generative AI has made remarkable strides to revolutionize fields such asimage and video generation. These advancements are driven by innovativealgorithms, architecture, and data. However, the rapid proliferation ofgenerative models has highlighted a critical gap: the absence of trustworthyevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etcoften fail to capture the nuanced quality and user satisfaction associated withgenerative outputs. This paper proposes an open platform GenAI-Arena toevaluate different image and video generative models, where users can activelyparticipate in evaluating these models. By leveraging collective user feedbackand votes, GenAI-Arena aims to provide a more democratic and accurate measureof model performance. It covers three arenas for text-to-image generation,text-to-video generation, and image editing respectively. Currently, we cover atotal of 27 open-source generative models. GenAI-Arena has been operating forfour months, amassing over 6000 votes from the community. We describe ourplatform, analyze the data, and explain the statistical methods for ranking themodels. To further promote the research in building model-based evaluationmetrics, we release a cleaned version of our preference data for the threetasks, namely GenAI-Bench. We prompt the existing multi-modal models likeGemini, GPT-4o to mimic human voting. We compute the correlation between modelvoting with human voting to understand their judging abilities. Our resultsshow existing multimodal models are still lagging in assessing the generatedvisual content, even the best model GPT-4o only achieves a Pearson correlationof 0.22 in the quality subscore, and behaves like random guessing in others.</description><author>Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, Wenhu Chen</author><pubDate>Tue, 06 Aug 2024 16:35:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04485v3</guid></item><item><title>Malicious Internet Entity Detection Using Local Graph Inference</title><link>http://arxiv.org/abs/2408.03287v1</link><description>Detection of malicious behavior in a large network is a challenging problemfor machine learning in computer security, since it requires a model with highexpressive power and scalable inference. Existing solutions struggle to achievethis feat -- current cybersec-tailored approaches are still limited inexpressivity, and methods successful in other domains do not scale well forlarge volumes of data, rendering frequent retraining impossible. This workproposes a new perspective for learning from graph data that is modelingnetwork entity interactions as a large heterogeneous graph. High expressivityof the method is achieved with neural network architecture HMILnet thatnaturally models this type of data and provides theoretical guarantees. Thescalability is achieved by pursuing local graph inference, i.e., classifyingindividual vertices and their neighborhood as independent samples. Ourexperiments exhibit improvement over the state-of-the-art Probabilistic ThreatPropagation (PTP) algorithm, show a further threefold accuracy improvement whenadditional data is used, which is not possible with the PTP algorithm, anddemonstrate the generalization capabilities of the method to new, previouslyunseen entities.</description><author>Simon Mandlik, Tomas Pevny, Vaclav Smidl, Lukas Bajer</author><pubDate>Tue, 06 Aug 2024 16:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03287v1</guid></item><item><title>Biomedical SAM 2: Segment Anything in Biomedical Images and Videos</title><link>http://arxiv.org/abs/2408.03286v1</link><description>Medical image segmentation and video object segmentation are essential fordiagnosing and analyzing diseases by identifying and measuring biologicalstructures. Recent advances in natural domain have been driven by foundationmodels like the Segment Anything Model 2 (SAM 2). To explore the performance ofSAM 2 in biomedical applications, we designed two evaluation pipelines forsingle-frame image segmentation and multi-frame video segmentation with variedprompt designs, revealing SAM 2's limitations in medical contexts.Consequently, we developed BioSAM 2, an enhanced foundation model optimized forbiomedical data based on SAM 2. Our experiments show that BioSAM 2 not onlysurpasses the performance of existing state-of-the-art foundation models butalso matches or even exceeds specialist models, demonstrating its efficacy andpotential in the medical domain.</description><author>Zhiling Yan, Weixiang Sun, Rong Zhou, Zhengqing Yuan, Kai Zhang, Yiwei Li, Tianming Liu, Quanzheng Li, Xiang Li, Lifang He, Lichao Sun</author><pubDate>Tue, 06 Aug 2024 16:34:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03286v1</guid></item><item><title>Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI</title><link>http://arxiv.org/abs/2402.00809v5</link><description>In the current landscape of deep learning research, there is a predominantemphasis on achieving high predictive accuracy in supervised tasks involvinglarge image and language datasets. However, a broader perspective reveals amultitude of overlooked metrics, tasks, and data types, such as uncertainty,active and continual learning, and scientific data, that demand attention.Bayesian deep learning (BDL) constitutes a promising avenue, offeringadvantages across these diverse settings. This paper posits that BDL canelevate the capabilities of deep learning. It revisits the strengths of BDL,acknowledges existing challenges, and highlights some exciting research avenuesaimed at addressing these obstacles. Looking ahead, the discussion focuses onpossible ways to combine large-scale foundation models with BDL to unlock theirfull potential.</description><author>Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, José Miguel Hernández-Lobato, Aliaksandr Hubin, Alexander Immer, Theofanis Karaletsos, Mohammad Emtiyaz Khan, Agustinus Kristiadi, Yingzhen Li, Stephan Mandt, Christopher Nemeth, Michael A. Osborne, Tim G. J. Rudner, David Rügamer, Yee Whye Teh, Max Welling, Andrew Gordon Wilson, Ruqi Zhang</author><pubDate>Tue, 06 Aug 2024 16:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00809v5</guid></item><item><title>ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer</title><link>http://arxiv.org/abs/2408.03284v1</link><description>Lip-syncing videos with given audio is the foundation for variousapplications including the creation of virtual presenters or performers. Whilerecent studies explore high-fidelity lip-sync with different techniques, theirtask-orientated models either require long-term videos for clip-specifictraining or retain visible artifacts. In this paper, we propose a unified andeffective framework ReSyncer, that synchronizes generalized audio-visual facialinformation. The key design is revisiting and rewiring the Style-basedgenerator to efficiently adopt 3D facial dynamics predicted by a principledstyle-injected Transformer. By simply re-configuring the information insertionmechanisms within the noise and style space, our framework fuses motion andappearance with unified training. Extensive experiments demonstrate thatReSyncer not only produces high-fidelity lip-synced videos according to audio,but also supports multiple appealing properties that are suitable for creatingvirtual presenters and performers, including fast personalized fine-tuning,video-driven lip-syncing, the transfer of speaking styles, and even faceswapping. Resources can be found athttps://guanjz20.github.io/projects/ReSyncer.</description><author>Jiazhi Guan, Zhiliang Xu, Hang Zhou, Kaisiyuan Wang, Shengyi He, Zhanwang Zhang, Borong Liang, Haocheng Feng, Errui Ding, Jingtuo Liu, Jingdong Wang, Youjian Zhao, Ziwei Liu</author><pubDate>Tue, 06 Aug 2024 16:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03284v1</guid></item><item><title>AMES: Asymmetric and Memory-Efficient Similarity Estimation for Instance-level Retrieval</title><link>http://arxiv.org/abs/2408.03282v1</link><description>This work investigates the problem of instance-level image retrievalre-ranking with the constraint of memory efficiency, ultimately aiming to limitmemory usage to 1KB per image. Departing from the prevalent focus onperformance enhancements, this work prioritizes the crucial trade-off betweenperformance and memory requirements. The proposed model uses atransformer-based architecture designed to estimate image-to-image similarityby capturing interactions within and across images based on their localdescriptors. A distinctive property of the model is the capability forasymmetric similarity estimation. Database images are represented with asmaller number of descriptors compared to query images, enabling performanceimprovements without increasing memory consumption. To ensure adaptabilityacross different applications, a universal model is introduced that adjusts toa varying number of local descriptors during the testing phase. Results onstandard benchmarks demonstrate the superiority of our approach over bothhand-crafted and learned models. In particular, compared with currentstate-of-the-art methods that overlook their memory footprint, our approach notonly attains superior performance but does so with a significantly reducedmemory footprint. The code and pretrained models are publicly available at:https://github.com/pavelsuma/ames</description><author>Pavel Suma, Giorgos Kordopatis-Zilos, Ahmet Iscen, Giorgos Tolias</author><pubDate>Tue, 06 Aug 2024 16:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03282v1</guid></item><item><title>StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation</title><link>http://arxiv.org/abs/2408.03281v1</link><description>Evaluation is the baton for the development of large language models. Currentevaluations typically employ a single-item assessment paradigm for each atomictest objective, which struggles to discern whether a model genuinely possessesthe required capabilities or merely memorizes/guesses the answers to specificquestions. To this end, we propose a novel evaluation framework referred to asStructEval. Starting from an atomic test objective, StructEval deepens andbroadens the evaluation by conducting a structured assessment across multiplecognitive levels and critical concepts, and therefore offers a comprehensive,robust and consistent evaluation for LLMs. Experiments on three widely-usedbenchmarks demonstrate that StructEval serves as a reliable tool for resistingthe risk of data contamination and reducing the interference of potentialbiases, thereby providing more reliable and consistent conclusions regardingmodel capabilities. Our framework also sheds light on the design of futureprincipled and trustworthy LLM evaluation protocols.</description><author>Boxi Cao, Mengjie Ren, Hongyu Lin, Xianpei Han, Feng Zhang, Junfeng Zhan, Le Sun</author><pubDate>Tue, 06 Aug 2024 16:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03281v1</guid></item><item><title>Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments</title><link>http://arxiv.org/abs/2408.03274v1</link><description>To deploy machine learning models on-device, practitioners use compressionalgorithms to shrink and speed up models while maintaining their high-qualityoutput. A critical aspect of compression in practice is model comparison,including tracking many compression experiments, identifying subtle changes inmodel behavior, and negotiating complex accuracy-efficiency trade-offs.However, existing compression tools poorly support comparison, leading totedious and, sometimes, incomplete analyses spread across disjoint tools. Tosupport real-world comparative workflows, we develop an interactive visualsystem called Compress and Compare. Within a single interface, Compress andCompare surfaces promising compression strategies by visualizing provenancerelationships between compressed models and reveals compression-inducedbehavior changes by comparing models' predictions, weights, and activations. Wedemonstrate how Compress and Compare supports common compression analysis tasksthrough two case studies, debugging failed compression on generative languagemodels and identifying compression artifacts in image classification models. Wefurther evaluate Compress and Compare in a user study with eight compressionexperts, illustrating its potential to provide structure to compressionworkflows, help practitioners build intuition about compression, and encouragethorough analysis of compression's effect on model behavior. Through theseevaluations, we identify compression-specific challenges that future visualanalytics tools should consider and Compress and Compare visualizations thatmay generalize to broader model comparison tasks.</description><author>Angie Boggust, Venkatesh Sivaraman, Yannick Assogba, Donghao Ren, Dominik Moritz, Fred Hohman</author><pubDate>Tue, 06 Aug 2024 16:17:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03274v1</guid></item><item><title>CaloQVAE : Simulating high-energy particle-calorimeter interactions using hybrid quantum-classical generative models</title><link>http://arxiv.org/abs/2312.03179v4</link><description>The Large Hadron Collider's high luminosity era presents major computationalchallenges in the analysis of collision events. Large amounts of Monte Carlo(MC) simulation will be required to constrain the statistical uncertainties ofthe simulated datasets below these of the experimental data. Modelling ofhigh-energy particles propagating through the calorimeter section of thedetector is the most computationally intensive MC simulation task. We introducea technique combining recent advancements in generative models and quantumannealing for fast and efficient simulation of high-energy particle-calorimeterinteractions.</description><author>Sehmimul Hoque, Hao Jia, Abhishek Abhishek, Mojde Fadaie, J. Quetzalcoatl Toledo-Marín, Tiago Vale, Roger G. Melko, Maximilian Swiatlowski, Wojciech T. Fedorko</author><pubDate>Tue, 06 Aug 2024 16:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03179v4</guid></item><item><title>CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking</title><link>http://arxiv.org/abs/2403.15313v2</link><description>To enable self-driving vehicles accurate detection and tracking ofsurrounding objects is essential. While Light Detection and Ranging (LiDAR)sensors have set the benchmark for high-performance systems, the appeal ofcamera-only solutions lies in their cost-effectiveness. Notably, despite theprevalent use of Radio Detection and Ranging (RADAR) sensors in automotivesystems, their potential in 3D detection and tracking has been largelydisregarded due to data sparsity and measurement noise. As a recentdevelopment, the combination of RADARs and cameras is emerging as a promisingsolution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), acamera-RADAR fusion model for 3D object detection, and Multi-Object Tracking(MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-onlyBEVDet architecture, CR3DT demonstrates substantial improvements in bothdetection and tracking capabilities, by incorporating the spatial and velocityinformation of the RADAR sensor. Experimental results demonstrate an absoluteimprovement in detection performance of 5.3% in mean Average Precision (mAP)and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on thenuScenes dataset when leveraging both modalities. CR3DT bridges the gap betweenhigh-performance and cost-effective perception systems in autonomous driving,by capitalizing on the ubiquitous presence of RADAR in automotive applications.The code is available at: https://github.com/ETH-PBL/CR3DT.</description><author>Nicolas Baumann, Michael Baumgartner, Edoardo Ghignone, Jonas Kühne, Tobias Fischer, Yung-Hsu Yang, Marc Pollefeys, Michele Magno</author><pubDate>Tue, 06 Aug 2024 15:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15313v2</guid></item><item><title>GraphLearner: Graph Node Clustering with Fully Learnable Augmentation</title><link>http://arxiv.org/abs/2212.03559v3</link><description>Contrastive deep graph clustering (CDGC) leverages the power of contrastivelearning to group nodes into different clusters. The quality of contrastivesamples is crucial for achieving better performance, making augmentationtechniques a key factor in the process. However, the augmentation samples inexisting methods are always predefined by human experiences, and agnostic fromthe downstream task clustering, thus leading to high human resource costs andpoor performance. To overcome these limitations, we propose a Graph NodeClustering with Fully Learnable Augmentation, termed GraphLearner. Itintroduces learnable augmentors to generate high-quality and task-specificaugmented samples for CDGC. GraphLearner incorporates two learnable augmentorsspecifically designed for capturing attribute and structural information.Moreover, we introduce two refinement matrices, including the high-confidencepseudo-label matrix and the cross-view sample similarity matrix, to enhance thereliability of the learned affinity matrix. During the training procedure, wenotice the distinct optimization goals for training learnable augmentors andcontrastive learning networks. In other words, we should both guarantee theconsistency of the embeddings as well as the diversity of the augmentedsamples. To address this challenge, we propose an adversarial learningmechanism within our method. Besides, we leverage a two-stage training strategyto refine the high-confidence matrices. Extensive experimental results on sixbenchmark datasets validate the effectiveness of GraphLearner.The code andappendix of GraphLearner are available athttps://github.com/xihongyang1999/GraphLearner on Github.</description><author>Xihong Yang, Erxue Min, Ke Liang, Yue Liu, Siwei Wang, Sihang Zhou, Huijun Wu, Xinwang Liu, En Zhu</author><pubDate>Tue, 06 Aug 2024 15:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.03559v3</guid></item><item><title>Synthesizing Text-to-SQL Data from Weak and Strong LLMs</title><link>http://arxiv.org/abs/2408.03256v1</link><description>The capability gap between open-source and closed-source large languagemodels (LLMs) remains a challenge in text-to-SQL tasks. In this paper, weintroduce a synthetic data approach that combines data produced by larger, morepowerful models (strong models) with error information data generated bysmaller, not well-aligned models (weak models). The method not only enhancesthe domain generalization of text-to-SQL models but also explores the potentialof error data supervision through preference learning. Furthermore, we employthe synthetic data approach for instruction tuning on open-source LLMs,resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE isdemonstrated through state-of-the-art results on the SPIDER and BIRDbenchmarks, bridging the performance gap between open-source models and methodsprompted by closed-source models.</description><author>Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, Chang Zhou</author><pubDate>Tue, 06 Aug 2024 15:40:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03256v1</guid></item><item><title>Artificial Intelligence for Literature Reviews: Opportunities and Challenges</title><link>http://arxiv.org/abs/2402.08565v2</link><description>This manuscript presents a comprehensive review of the use of ArtificialIntelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorousand organised methodology that assesses and integrates previous research on agiven topic. Numerous tools have been developed to assist and partiallyautomate the SLR process. The increasing role of AI in this field shows greatpotential in providing more effective support for researchers, moving towardsthe semi-automatic creation of literature reviews. Our study focuses on how AItechniques are applied in the semi-automation of SLRs, specifically in thescreening and extraction phases. We examine 21 leading SLR tools using aframework that combines 23 traditional features with 11 AI features. We alsoanalyse 11 recent tools that leverage large language models for searching theliterature and assisting academic writing. Finally, the paper discusses currenttrends in the field, outlines key research challenges, and suggests directionsfor future research.</description><author>Francisco Bolanos, Angelo Salatino, Francesco Osborne, Enrico Motta</author><pubDate>Tue, 06 Aug 2024 15:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08565v2</guid></item><item><title>Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry Reconstruction in Field Conditions</title><link>http://arxiv.org/abs/2402.10344v3</link><description>We evaluate different Neural Radiance Fields (NeRFs) techniques for the 3Dreconstruction of plants in varied environments, from indoor settings tooutdoor fields. Traditional methods usually fail to capture the complexgeometric details of plants, which is crucial for phenotyping and breedingstudies. We evaluate the reconstruction fidelity of NeRFs in three scenarioswith increasing complexity and compare the results with the point cloudobtained using LiDAR as ground truth. In the most realistic field scenario, theNeRF models achieve a 74.6% F1 score after 30 minutes of training on the GPU,highlighting the efficacy of NeRFs for 3D reconstruction in challengingenvironments. Additionally, we propose an early stopping technique for NeRFtraining that almost halves the training time while achieving only a reductionof 7.4% in the average F1 score. This optimization process significantlyenhances the speed and efficiency of 3D reconstruction using NeRFs. Ourfindings demonstrate the potential of NeRFs in detailed and realistic 3D plantreconstruction and suggest practical approaches for enhancing the speed andefficiency of NeRFs in the 3D reconstruction process.</description><author>Muhammad Arbab Arshad, Talukder Jubery, James Afful, Anushrut Jignasu, Aditya Balu, Baskar Ganapathysubramanian, Soumik Sarkar, Adarsh Krishnamurthy</author><pubDate>Tue, 06 Aug 2024 15:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10344v3</guid></item><item><title>Training Compute Thresholds: Features and Functions in AI Regulation</title><link>http://arxiv.org/abs/2405.10799v2</link><description>Regulators in the US and EU are using thresholds based on trainingcompute--the number of computational operations used in training--to identifygeneral-purpose artificial intelligence (GPAI) models that may pose risks oflarge-scale societal harm. We argue that training compute currently is the mostsuitable metric to identify GPAI models that deserve regulatory oversight andfurther scrutiny. Training compute correlates with model capabilities andrisks, is quantifiable, can be measured early in the AI lifecycle, and can beverified by external actors, among other advantageous features. These featuresmake compute thresholds considerably more suitable than other proposed metricsto serve as an initial filter to trigger additional regulatory requirements andscrutiny. However, training compute is an imperfect proxy for risk. As such,compute thresholds should not be used in isolation to determine appropriatemitigation measures. Instead, they should be used to detect potentially riskyGPAI models that warrant regulatory oversight, such as through notificationrequirements, and further scrutiny, such as via model evaluations and riskassessments, the results of which may inform which mitigation measures areappropriate. In fact, this appears largely consistent with how computethresholds are used today. As GPAI technology and market structures evolve,regulators should update compute thresholds and complement them with othermetrics into regulatory review processes.</description><author>Lennart Heim, Leonie Koessler</author><pubDate>Tue, 06 Aug 2024 15:33:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10799v2</guid></item><item><title>GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation</title><link>http://arxiv.org/abs/2402.15745v2</link><description>The Large Vision-Language Models (LVLMs) have demonstrated great abilities inimage perception and language understanding. However, existing multimodalbenchmarks focus on primary perception abilities and commonsense knowledgewhich are insufficient to reflect the comprehensive capabilities of LVLMs. Wepropose GAOKAO-MM, a multimodal benchmark based on the Chinese College EntranceExamination (GAOKAO), comprising of 8 subjects and 12 types of images, such asdiagrams, function graphs, maps and photos. GAOKAO-MM derives from nativeChinese context and sets human-level requirements for the model's abilities,including perception, understanding, knowledge and reasoning. We evaluate 10LVLMs and find that the accuracies of all of them are lower than 50%, withGPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) rankingin the top three positions. The results of our multi-dimension analysisindicate that LVLMs have moderate distance towards Artificial GeneralIntelligence (AGI) and provide insights facilitating the development ofmultilingual LVLMs.</description><author>Yi Zong, Xipeng Qiu</author><pubDate>Tue, 06 Aug 2024 15:28:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15745v2</guid></item><item><title>Calo-VQ: Vector-Quantized Two-Stage Generative Model in Calorimeter Simulation</title><link>http://arxiv.org/abs/2405.06605v3</link><description>We introduce a novel machine learning method developed for the fastsimulation of calorimeter detector response, adapting vector-quantizedvariational autoencoder (VQ-VAE). Our model adopts a two-stage generationstrategy: initially compressing geometry-aware calorimeter data into a discretelatent space, followed by the application of a sequence model to learn andgenerate the latent tokens. Extensive experimentation on the Calo-challengedataset underscores the efficiency of our approach, showcasing a remarkableimprovement in the generation speed compared with conventional method by afactor of 2000. Remarkably, our model achieves the generation of calorimetershowers within milliseconds. Furthermore, comprehensive quantitativeevaluations across various metrics are performed to validate physicsperformance of generation.</description><author>Qibin Liu, Chase Shimmin, Xiulong Liu, Eli Shlizerman, Shu Li, Shih-Chieh Hsu</author><pubDate>Tue, 06 Aug 2024 15:20:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06605v3</guid></item><item><title>Tool Learning with Foundation Models</title><link>http://arxiv.org/abs/2304.08354v3</link><description>Humans possess an extraordinary ability to create and utilize tools, allowingthem to overcome physical limitations and explore new frontiers. With theadvent of foundation models, AI systems have the potential to be equally adeptin tool use as humans. This paradigm, i.e., tool learning with foundationmodels, combines the strengths of specialized tools and foundation models toachieve enhanced accuracy, efficiency, and automation in problem-solving.Despite its immense potential, there is still a lack of a comprehensiveunderstanding of key challenges, opportunities, and future endeavors in thisfield. To this end, we present a systematic investigation of tool learning inthis paper. We first introduce the background of tool learning, including itscognitive origins, the paradigm shift of foundation models, and thecomplementary roles of tools and models. Then we recapitulate existing toollearning research into tool-augmented and tool-oriented learning. We formulatea general tool learning framework: starting from understanding the userinstruction, models should learn to decompose a complex task into severalsubtasks, dynamically adjust their plan through reasoning, and effectivelyconquer each sub-task by selecting appropriate tools. We also discuss how totrain models for improved tool-use capabilities and facilitate thegeneralization in tool learning. Considering the lack of a systematic toollearning evaluation in prior works, we experiment with 18 representative toolsand show the potential of current foundation models in skillfully utilizingtools. Finally, we discuss several open problems that require furtherinvestigation for tool learning. In general, we hope this paper could inspirefuture research in integrating tools with foundation models.</description><author>Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, Maosong Sun</author><pubDate>Tue, 06 Aug 2024 15:14:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08354v3</guid></item><item><title>VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation</title><link>http://arxiv.org/abs/2407.03291v2</link><description>Complex human activity recognition (CHAR) remains a pivotal challenge withinubiquitous computing, especially in the context of smart environments. Existingstudies typically require meticulous labeling of both atomic and complexactivities, a task that is labor-intensive and prone to errors due to thescarcity and inaccuracies of available datasets. Most prior research hasfocused on datasets that either precisely label atomic activities or, atminimum, their sequence approaches that are often impractical in real worldsettings.In response, we introduce VCHAR (Variance-Driven Complex HumanActivity Recognition), a novel framework that treats the outputs of atomicactivities as a distribution over specified intervals. Leveraging generativemethodologies, VCHAR elucidates the reasoning behind complex activityclassifications through video-based explanations, accessible to users withoutprior machine learning expertise. Our evaluation across three publiclyavailable datasets demonstrates that VCHAR enhances the accuracy of complexactivity recognition without necessitating precise temporal or sequentiallabeling of atomic activities. Furthermore, user studies confirm that VCHAR'sexplanations are more intelligible compared to existing methods, facilitating abroader understanding of complex activity recognition among non-experts.</description><author>Yuan Sun, Navid Salami Pargoo, Taqiya Ehsan, Zhao Zhang, Jorge Ortiz</author><pubDate>Tue, 06 Aug 2024 15:14:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03291v2</guid></item><item><title>An Adaptive Second-order Method for a Class of Nonconvex Nonsmooth Composite Optimization</title><link>http://arxiv.org/abs/2407.17216v2</link><description>This paper explores a specific type of nonconvex sparsity-promotingregularization problems, namely those involving $\ell_p$-norm regularization,in conjunction with a twice continuously differentiable loss function. Wepropose a novel second-order algorithm designed to effectively address thisclass of challenging nonconvex and nonsmooth problems, showcasing severalinnovative features: (i) The use of an alternating strategy to solve areweighted $\ell_1$ regularized subproblem and the subspace approximate Newtonstep. (ii) The reweighted $\ell_1$ regularized subproblem relies on a convexapproximation to the nonconvex regularization term, enabling a closed-formsolution characterized by the soft-thresholding operator. This feature allowsour method to be applied to various nonconvex regularization problems. (iii)Our algorithm ensures that the iterates maintain their sign values and thatnonzero components are kept away from 0 for a sufficient number of iterations,eventually transitioning to a perturbed Newton method. (iv) We providetheoretical guarantees of global convergence, local superlinear convergence inthe presence of the Kurdyka-\L ojasiewicz (KL) property, and local quadraticconvergence when employing the exact Newton step in our algorithm. We alsoshowcase the effectiveness of our approach through experiments on a diverse setof model prediction problems.</description><author>Hao Wang, Xiangyu Yang, Yichen Zhu</author><pubDate>Tue, 06 Aug 2024 15:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17216v2</guid></item><item><title>Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons</title><link>http://arxiv.org/abs/2408.03247v1</link><description>In this paper, we investigate whether Large Language Models (LLMs) activelyrecall or retrieve their internal repositories of factual knowledge when facedwith reasoning tasks. Through an analysis of LLMs' internal factual recall ateach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harnessthe critical factual associations under certain circumstances. Instead, theytend to opt for alternative, shortcut-like pathways to answer reasoningquestions. By manually manipulating the recall process of parametric knowledgein LLMs, we demonstrate that enhancing this recall process directly improvesreasoning performance whereas suppressing it leads to notable degradation.Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, apowerful technique for addressing complex reasoning tasks. Our findingsindicate that CoT can intensify the recall of factual knowledge by encouragingLLMs to engage in orderly and reliable reasoning. Furthermore, we explored howcontextual conflicts affect the retrieval of facts during the reasoning processto gain a comprehensive understanding of the factual recall behaviors of LLMs.Code and data will be available soon.</description><author>Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, Daniel Dajun Zeng</author><pubDate>Tue, 06 Aug 2024 15:07:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03247v1</guid></item><item><title>Making Long-Context Language Models Better Multi-Hop Reasoners</title><link>http://arxiv.org/abs/2408.03246v1</link><description>Recent advancements in long-context modeling have enhanced language models(LMs) for complex tasks across multiple NLP applications. Despite thisprogress, we find that these models struggle with multi-hop reasoning andexhibit decreased performance in the presence of noisy contexts. In this paper,we introduce Reasoning with Attributions, a novel approach that prompts LMs tosupply attributions for each assertion during their reasoning. We validate ourapproach through experiments on three multi-hop datasets, employing bothproprietary and open-source models, and demonstrate its efficacy andresilience. Furthermore, we explore methods to augment reasoning capabilitiesvia fine-tuning and offer an attribution-annotated dataset and a specializedtraining strategy. Our fine-tuned model achieves competitive performance onmulti-hop reasoning benchmarks, closely paralleling proprietary LMs such asChatGPT and Claude-instant.</description><author>Yanyang Li, Shuo Liang, Michael R. Lyu, Liwei Wang</author><pubDate>Tue, 06 Aug 2024 15:06:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03246v1</guid></item><item><title>T-Explainer: A Model-Agnostic Explainability Framework Based on Gradients</title><link>http://arxiv.org/abs/2404.16495v2</link><description>The development of machine learning applications has increased significantlyin recent years, motivated by the remarkable ability of learning-poweredsystems to discover and generalize intricate patterns hidden in massivedatasets. Modern learning models, while powerful, often have a level ofcomplexity that renders them opaque black boxes, resulting in a notable lack oftransparency that hinders our ability to decipher their reasoning. Opacitychallenges the interpretability and practical application of machine learning,especially in critical domains where understanding the underlying reasons isessential for informed decision-making. Explainable Artificial Intelligence(XAI) rises to address that challenge, unraveling the complexity of black boxesby providing elucidating explanations. Among the various XAI approaches,feature attribution/importance stands out for its capacity to delineate thesignificance of input features in the prediction process. However, mostexisting attribution methods have limitations, such as instability, whendivergent explanations may result from similar or even the same instance. Thiswork introduces T-Explainer, a novel local additive attribution explainer basedon Taylor expansion. It has desirable properties, such as local accuracy andconsistency, making T-Explainer stable over multiple runs. We demonstrateT-Explainer's effectiveness in quantitative benchmark experiments againstwell-known attribution methods. Additionally, we provide several tools toevaluate and visualize explanations, turning T-Explainer into a comprehensiveXAI framework.</description><author>Evandro S. Ortigossa, Fábio F. Dias, Brian Barr, Claudio T. Silva, Luis Gustavo Nonato</author><pubDate>Tue, 06 Aug 2024 15:03:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16495v2</guid></item><item><title>Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph</title><link>http://arxiv.org/abs/2404.03623v2</link><description>Large Language Models (LLMs) demonstrate an impressive capacity to recall avast range of factual knowledge. However, understanding their underlyingreasoning and internal mechanisms in exploiting this knowledge remains a keyresearch area. This work unveils the factual information an LLM representsinternally for sentence-level claim verification. We propose an end-to-endframework to decode factual knowledge embedded in token representations from avector space to a set of ground predicates, showing its layer-wise evolutionusing a dynamic knowledge graph. Our framework employs activation patching, avector-level technique that alters a token representation during inference, toextract encoded knowledge. Accordingly, we neither rely on training norexternal models. Using factual and common-sense claims from two claimverification datasets, we showcase interpretability analyses at local andglobal levels. The local analysis highlights entity centrality in LLMreasoning, from claim-related information and multi-hop reasoning torepresentation errors causing erroneous evaluation. On the other hand, theglobal reveals trends in the underlying evolution, such as word-based knowledgeevolving into claim-related facts. By interpreting semantics from LLM latentrepresentations and enabling graph-related analyses, this work enhances theunderstanding of the factual knowledge resolution process.</description><author>Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini</author><pubDate>Tue, 06 Aug 2024 15:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03623v2</guid></item><item><title>Physics-guided Active Sample Reweighting for Urban Flow Prediction</title><link>http://arxiv.org/abs/2407.13605v2</link><description>Urban flow prediction is a spatio-temporal modeling task that estimates thethroughput of transportation services like buses, taxis, and ride-sharing,where data-driven models have become the most popular solution in the pastdecade. Meanwhile, the implicitly learned mapping between historicalobservations to the prediction targets tend to over-simplify the dynamics ofreal-world urban flows, leading to suboptimal predictions. Some recentspatio-temporal prediction solutions bring remedies with the notion ofphysics-guided machine learning (PGML), which describes spatio-temporal datawith nuanced and principled physics laws, thus enhancing both the predictionaccuracy and interpretability. However, these spatio-temporal PGML methods arebuilt upon a strong assumption that the observed data fully conforms to thedifferential equations that define the physical system, which can quicklybecome ill-posed in urban flow prediction tasks. The observed urban flow data,especially when sliced into time-dependent snapshots to facilitate predictions,is typically incomplete and sparse, and prone to inherent noise incurred in thecollection process. As a result, such physical inconsistency between the dataand PGML model significantly limits the predictive power and robustness of thesolution. Moreover, due to the interval-based predictions and intermittentnature of data filing in many transportation services, the instantaneousdynamics of urban flows can hardly be captured, rendering differentialequation-based continuous modeling a loose fit for this setting. To overcomethe challenges, we develop a discretized physics-guided network (PN), andpropose a data-aware framework Physics-guided Active Sample Reweighting(P-GASR) to enhance PN. Experimental results in four real-world datasetsdemonstrate that our method achieves state-of-the-art performance with ademonstrable improvement in robustness.</description><author>Wei Jiang, Tong Chen, Guanhua Ye, Wentao Zhang, Lizhen Cui, Zi Huang, Hongzhi Yin</author><pubDate>Tue, 06 Aug 2024 14:55:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13605v2</guid></item><item><title>Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool</title><link>http://arxiv.org/abs/2402.19135v2</link><description>In today's digital age, characterized by rapid news consumption andincreasing vulnerability to propaganda, fostering citizens' critical thinkingis crucial for stable democracies. This paper introduces the design ofClarifAI, a novel automated propaganda detection tool designed to nudge readerstowards more critical news consumption by activating the analytical mode ofthinking, following Kahneman's dual-system theory of cognition. Using LargeLanguage Models, ClarifAI detects propaganda in news articles and providescontext-rich explanations, enhancing users' understanding and criticalthinking. Our contribution is threefold: first, we propose the design ofClarifAI; second, in an online experiment, we demonstrate that this designeffectively encourages news readers to engage in more critical reading; andthird, we emphasize the value of explanations for fostering critical thinking.The study thus offers both a practical tool and useful design knowledge formitigating propaganda in digital news.</description><author>Liudmila Zavolokina, Kilian Sprenkamp, Zoya Katashinskaya, Daniel Gordon Jones, Gerhard Schwabe</author><pubDate>Tue, 06 Aug 2024 14:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19135v2</guid></item><item><title>LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for Accurate Robotic Grasping Under the Occlusion</title><link>http://arxiv.org/abs/2408.03238v1</link><description>This paper addresses the challenge of perceiving complete object shapesthrough visual perception. While prior studies have demonstrated encouragingoutcomes in segmenting the visible parts of objects within a scene, amodalsegmentation, in particular, has the potential to allow robots to infer theoccluded parts of objects. To this end, this paper introduces a new frameworkthat explores amodal segmentation for robotic grasping in cluttered scenes,thus greatly enhancing robotic grasping abilities. Initially, we use aconventional segmentation algorithm to detect the visible segments of thetarget object, which provides shape priors for completing the full object mask.Particularly, to explore how to utilize semantic features from RGB images andgeometric information from depth images, we propose a Linear-fusionAttention-guided Convolutional Network (LAC-Net). LAC-Net utilizes thelinear-fusion strategy to effectively fuse this cross-modal data, and then usesthe prior visible mask as attention map to guide the network to focus on targetfeature locations for further complete mask recovery. Using the amodal mask ofthe target object provides advantages in selecting more accurate and robustgrasp points compared to relying solely on the visible segments. The results ondifferent datasets show that our method achieves state-of-the-art performance.Furthermore, the robot experiments validate the feasibility and robustness ofthis method in the real world. Our code and demonstrations are available on theproject page: https://jrryzh.github.io/LAC-Net.</description><author>Jinyu Zhang, Yongchong Gu, Jianxiong Gao, Haitao Lin, Qiang Sun, Xinwei Sun, Xiangyang Xue, Yanwei Fu</author><pubDate>Tue, 06 Aug 2024 14:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03238v1</guid></item><item><title>Analysis of Partially-Calibrated Sparse Subarrays for Direction Finding with Extended Degrees of Freedom</title><link>http://arxiv.org/abs/2408.03236v1</link><description>This paper investigates the problem of direction-of-arrival (DOA) estimationusing multiple partially-calibrated sparse subarrays. In particular, we presentthe Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOAestimation algorithm to scenarios with partially-calibrated sparse subarrays.The proposed GCA-MUSIC algorithm exploits the difference coarray for eachsubarray, followed by a specific pseudo-spectrum merging rule that is based onthe intersection of the signal subspaces associated to each subarray. This ruleassumes that there is no a priori knowledge about the cross-covariance betweensubarrays. In that way, only the second-order statistics of each subarray areused to estimate the directions with increased degrees of freedom, i.e., theestimation procedure preserves the coarray Multiple Signal Classification andsparse arrays properties to estimate more sources than the number of physicalsensors in each subarray. Numerical simulations show that the proposedGCA-MUSIC has better performance than other similar strategies.</description><author>W. S. Leite, R. C. de Lamare</author><pubDate>Tue, 06 Aug 2024 14:48:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03236v1</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v7</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Tue, 06 Aug 2024 14:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v7</guid></item><item><title>The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities</title><link>http://arxiv.org/abs/2405.20089v2</link><description>Fine-tuning large language models (LLMs) for machine translation has shownimprovements in overall translation quality. However, it is unclear what is theimpact of fine-tuning on desirable LLM behaviors that are not present in neuralmachine translation models, such as steerability, inherent document-leveltranslation abilities, and the ability to produce less literal translations. Weperform an extensive translation evaluation on the LLaMA and Falcon family ofmodels with model size ranging from 7 billion up to 65 billion parameters. Ourresults show that while fine-tuning improves the general translation quality ofLLMs, several abilities degrade. In particular, we observe a decline in theability to perform formality steering, to produce technical translationsthrough few-shot examples, and to perform document-level translation. On theother hand, we observe that the model produces less literal translations afterfine-tuning on parallel data. We show that by including monolingual data aspart of the fine-tuning data we can maintain the abilities while simultaneouslyenhancing overall translation quality. Our findings emphasize the need forfine-tuning strategies that preserve the benefits of LLMs for machinetranslation.</description><author>David Stap, Eva Hasler, Bill Byrne, Christof Monz, Ke Tran</author><pubDate>Tue, 06 Aug 2024 14:46:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20089v2</guid></item><item><title>ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability</title><link>http://arxiv.org/abs/2404.14712v2</link><description>Earth system predictability is challenged by the complexity of environmentaldynamics and the multitude of variables involved. Current AI foundation models,although advanced by leveraging large and heterogeneous data, are oftenconstrained by their size and data integration, limiting their effectiveness inaddressing the full range of Earth system prediction challenges. To overcomethese limitations, we introduce the Oak Ridge Base Foundation Model for EarthSystem Predictability (ORBIT), an advanced vision transformer model that scalesup to 113 billion parameters using a novel hybrid tensor-data orthogonalparallelism technique. As the largest model of its kind, ORBIT surpasses thecurrent climate AI foundation model size by a thousandfold. Performance scalingtests conducted on the Frontier supercomputer have demonstrated that ORBITachieves 684 petaFLOPS to 1.6 exaFLOPS sustained throughput, with scalingefficiency maintained at 41% to 85% across 49,152 AMD GPUs. These breakthroughsestablish new advances in AI-driven climate modeling and demonstrate promise tosignificantly improve the Earth system predictability.</description><author>Xiao Wang, Siyan Liu, Aristeidis Tsaris, Jong-Youl Choi, Ashwin Aji, Ming Fan, Wei Zhang, Junqi Yin, Moetasim Ashfaq, Dan Lu, Prasanna Balaprakash</author><pubDate>Tue, 06 Aug 2024 14:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14712v2</guid></item><item><title>Contrastive Learning for Image Complexity Representation</title><link>http://arxiv.org/abs/2408.03230v1</link><description>Quantifying and evaluating image complexity can be instrumental in enhancingthe performance of various computer vision tasks. Supervised learning caneffectively learn image complexity features from well-annotated datasets.However, creating such datasets requires expensive manual annotation costs. Themodels may learn human subjective biases from it. In this work, we introducethe MoCo v2 framework. We utilize contrastive learning to represent imagecomplexity, named CLIC (Contrastive Learning for Image Complexity). We findthat there are complexity differences between different local regions of animage, and propose Random Crop and Mix (RCM), which can produce positivesamples consisting of multi-scale local crops. RCM can also expand the trainset and increase data diversity without introducing additional data. We conductextensive experiments with CLIC, comparing it with both unsupervised andsupervised methods. The results demonstrate that the performance of CLIC iscomparable to that of state-of-the-art supervised methods. In addition, weestablish the pipelines that can apply CLIC to computer vision tasks toeffectively improve their performance.</description><author>Shipeng Liu, Liang Zhao, Dengfeng Chen, Zhanping Song</author><pubDate>Tue, 06 Aug 2024 14:44:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03230v1</guid></item><item><title>Line-based 6-DoF Object Pose Estimation and Tracking With an Event Camera</title><link>http://arxiv.org/abs/2408.03225v1</link><description>Pose estimation and tracking of objects is a fundamental application in 3Dvision. Event cameras possess remarkable attributes such as high dynamic range,low latency, and resilience against motion blur, which enables them to addresschallenging high dynamic range scenes or high-speed motion. These features makeevent cameras an ideal complement over standard cameras for object poseestimation. In this work, we propose a line-based robust pose estimation andtracking method for planar or non-planar objects using an event camera.Firstly, we extract object lines directly from events, then provide an initialpose using a globally-optimal Branch-and-Bound approach, where 2D-3D linecorrespondences are not known in advance. Subsequently, we utilize event-linematching to establish correspondences between 2D events and 3D models.Furthermore, object poses are refined and continuously tracked by minimizingevent-line distances. Events are assigned different weights based on thesedistances, employing robust estimation algorithms. To evaluate the precision ofthe proposed methods in object pose estimation and tracking, we have devisedand established an event-based moving object dataset. Compared againststate-of-the-art methods, the robustness and accuracy of our methods have beenvalidated both on synthetic experiments and the proposed dataset. The sourcecode is available at https://github.com/Zibin6/LOPET.</description><author>Zibin Liu, Banglei Guan, Yang Shang, Qifeng Yu, Laurent Kneip</author><pubDate>Tue, 06 Aug 2024 14:36:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03225v1</guid></item><item><title>Don't Think It Twice: Exploit Shift Invariance for Efficient Online Streaming Inference of CNNs</title><link>http://arxiv.org/abs/2408.03223v1</link><description>Deep learning time-series processing often relies on convolutional neuralnetworks with overlapping windows. This overlap allows the network to producean output faster than the window length. However, it introduces additionalcomputations. This work explores the potential to optimize computationalefficiency during inference by exploiting convolution's shift-invarianceproperties to skip the calculation of layer activations between successiveoverlapping windows. Although convolutions are shift-invariant, zero-paddingand pooling operations, widely used in such networks, are not efficient andcomplicate efficient streaming inference. We introduce StreamiNNC, a strategyto deploy Convolutional Neural Networks for online streaming inference. Weexplore the adverse effects of zero padding and pooling on the accuracy ofstreaming inference, deriving theoretical error upper bounds for pooling duringstreaming. We address these limitations by proposing signal padding and poolingalignment and provide guidelines for designing and deploying models forStreamiNNC. We validate our method in simulated data and on three real-worldbiomedical signal processing applications. StreamiNNC achieves a low deviationbetween streaming output and normal inference for all three networks (2.03 -3.55% NRMSE). This work demonstrates that it is possible to linearly speed upthe inference of streaming CNNs processing overlapping windows, negating theadditional computation typically incurred by overlapping windows.</description><author>Christodoulos Kechris, Jonathan Dan, Jose Miranda, David Atienza</author><pubDate>Tue, 06 Aug 2024 14:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03223v1</guid></item><item><title>ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks</title><link>http://arxiv.org/abs/2402.09146v4</link><description>In this paper, we present a novel framework for enhancing the performance ofQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutionallayers and addressing the critical challenges associated with them. Traditionalquanvolutional layers, although beneficial for feature extraction, have largelybeen static, offering limited adaptability. Unlike state-of-the-art, ourresearch overcomes this limitation by enabling training within these layers,significantly increasing the flexibility and potential of QuNNs. However, theintroduction of multiple trainable quanvolutional layers induces complexitiesin gradient-based optimization, primarily due to the difficulty in accessinggradients across these layers. To resolve this, we propose a novelarchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveragingthe concept of residual learning, which facilitates the flow of gradients byadding skip connections between layers. By inserting residual blocks betweenquanvolutional layers, we ensure enhanced gradient access throughout thenetwork, leading to improved training performance. Moreover, we provideempirical evidence on the strategic placement of these residual blocks withinQuNNs. Through extensive experimentation, we identify an efficientconfiguration of residual blocks, which enables gradients across all the layersin the network that eventually results in efficient training. Our findingssuggest that the precise location of residual blocks plays a crucial role inmaximizing the performance gains in QuNNs. Our results mark a substantial stepforward in the evolution of quantum deep learning, offering new avenues forboth theoretical development and practical quantum computing applications.</description><author>Muhammad Kashif, Muhammad Shafique</author><pubDate>Tue, 06 Aug 2024 14:30:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09146v4</guid></item><item><title>Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data</title><link>http://arxiv.org/abs/2404.05530v2</link><description>Reinforcement Learning from Human Feedback (RLHF) is a popular method foraligning Language Models (LM) with human values and preferences. RLHF requiresa large number of preference pairs as training data, which are often used inboth the Supervised Fine-Tuning and Reward Model training and thereforepublicly available datasets are commonly used. In this work, we study to whatextent a malicious actor can manipulate the LMs generations by poisoning thepreferences, i.e., injecting poisonous preference pairs into these datasets andthe RLHF training process. We propose strategies to build poisonous preferencepairs and test their performance by poisoning two widely used preferencedatasets. Our results show that preference poisoning is highly effective:injecting a small amount of poisonous data (1-5\% of the original dataset), wecan effectively manipulate the LM to generate a target entity in a targetsentiment (positive or negative). The findings from our experiments also shedlight on strategies to defend against the preference poisoning attack.</description><author>Tim Baumgärtner, Yang Gao, Dana Alon, Donald Metzler</author><pubDate>Tue, 06 Aug 2024 14:30:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05530v2</guid></item><item><title>A Survey on Deep Learning for Theorem Proving</title><link>http://arxiv.org/abs/2404.09939v2</link><description>Theorem proving is a fundamental aspect of mathematics, spanning frominformal reasoning in natural language to rigorous derivations in formalsystems. In recent years, the advancement of deep learning, especially theemergence of large language models, has sparked a notable surge of researchexploring these techniques to enhance the process of theorem proving. Thispaper presents a comprehensive survey of deep learning for theorem proving byoffering (i) a thorough review of existing approaches across various tasks suchas autoformalization, premise selection, proofstep generation, and proofsearch; (ii) an extensive summary of curated datasets and strategies forsynthetic data generation; (iii) a detailed analysis of evaluation metrics andthe performance of state-of-the-art methods; and (iv) a critical discussion onthe persistent challenges and the promising avenues for future exploration. Oursurvey aims to serve as a foundational reference for deep learning approachesin theorem proving, inspiring and catalyzing further research endeavors in thisrapidly growing field. A curated list of papers is available athttps://github.com/zhaoyu-li/DL4TP.</description><author>Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, Xujie Si</author><pubDate>Tue, 06 Aug 2024 14:30:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09939v2</guid></item><item><title>Masked Random Noise for Communication Efficient Federaetd Learning</title><link>http://arxiv.org/abs/2408.03220v1</link><description>Federated learning is a promising distributed training paradigm thateffectively safeguards data privacy. However, it may involve significantcommunication costs, which hinders training efficiency. In this paper, we aimto enhance communication efficiency from a new perspective. Specifically, werequest the distributed clients to find optimal model updates relative toglobal model parameters within predefined random noise. For this purpose, wepropose Federated Masked Random Noise (FedMRN), a novel framework that enablesclients to learn a 1-bit mask for each model parameter and apply masked randomnoise (i.e., the Hadamard product of random noise and masks) to represent modelupdates. To make FedMRN feasible, we propose an advanced mask trainingstrategy, called progressive stochastic masking (PSM). After local training,each client only need to transmit local masks and a random seed to the server.Additionally, we provide theoretical guarantees for the convergence of FedMRNunder both strongly convex and non-convex assumptions. Extensive experimentsare conducted on four popular datasets. The results show that FedMRN exhibitssuperior convergence speed and test accuracy compared to relevant baselines,while attaining a similar level of accuracy as FedAvg.</description><author>Shiwei Li, Yingyi Cheng, Haozhao Wang, Xing Tang, Shijie Xu, Weihong Luo, Yuhua Li, Dugang Liu, Xiuqiang He, and Ruixuan Li</author><pubDate>Tue, 06 Aug 2024 14:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03220v1</guid></item><item><title>Learning to Learn without Forgetting using Attention</title><link>http://arxiv.org/abs/2408.03219v1</link><description>Continual learning (CL) refers to the ability to continually learn over timeby accommodating new knowledge while retaining previously learned experience.While this concept is inherent in human learning, current machine learningmethods are highly prone to overwrite previously learned patterns and thusforget past experience. Instead, model parameters should be updated selectivelyand carefully, avoiding unnecessary forgetting while optimally leveragingpreviously learned patterns to accelerate future learning. Since hand-craftingeffective update mechanisms is difficult, we propose meta-learning atransformer-based optimizer to enhance CL. This meta-learned optimizer usesattention to learn the complex relationships between model parameters across astream of tasks, and is designed to generate effective weight updates for thecurrent task while preventing catastrophic forgetting on previously encounteredtasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, andSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of bothforward and backward transfer, even on small sets of labeled data, highlightingthe advantages of integrating a meta-learned optimizer within the continuallearning framework.</description><author>Anna Vettoruzzo, Joaquin Vanschoren, Mohamed-Rafik Bouguelia, Thorsteinn Rögnvaldsson</author><pubDate>Tue, 06 Aug 2024 14:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03219v1</guid></item><item><title>FedBAT: Communication-Efficient Federated Learning via Learnable Binarization</title><link>http://arxiv.org/abs/2408.03215v1</link><description>Federated learning is a promising distributed machine learning paradigm thatcan effectively exploit large-scale data without exposing users' privacy.However, it may incur significant communication overhead, thereby potentiallyimpairing the training efficiency. To address this challenge, numerous studiessuggest binarizing the model updates. Nonetheless, traditional methods usuallybinarize model updates in a post-training manner, resulting in significantapproximation errors and consequent degradation in model accuracy. To this end,we propose Federated Binarization-Aware Training (FedBAT), a novel frameworkthat directly learns binary model updates during the local training process,thus inherently reducing the approximation errors. FedBAT incorporates aninnovative binarization operator, along with meticulously designed derivativesto facilitate efficient learning. In addition, we establish theoreticalguarantees regarding the convergence of FedBAT. Extensive experiments areconducted on four popular datasets. The results show that FedBAT significantlyaccelerates the convergence and exceeds the accuracy of baselines by up to 9\%,even surpassing that of FedAvg in some cases.</description><author>Shiwei Li, Wenchao Xu, Haozhao Wang, Xing Tang, Yining Qi, Shijie Xu, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li</author><pubDate>Tue, 06 Aug 2024 14:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03215v1</guid></item><item><title>Hummer: Towards Limited Competitive Preference Dataset</title><link>http://arxiv.org/abs/2405.11647v3</link><description>Preference datasets are essential for incorporating human preferences intopre-trained language models, playing a key role in the success of ReinforcementLearning from Human Feedback. However, these datasets often demonstrateconflicting alignment objectives, leading to increased vulnerability tojailbreak attacks and challenges in adapting downstream tasks to prioritizespecific alignment objectives without negatively impacting others. In thiswork, we introduce a novel statistical metric, Alignment Dimension Conflict, toquantify the degree of conflict within preference datasets. We then present\texttt{Hummer} and its fine-grained variant, \texttt{Hummer-F}, as innovativepairwise preference datasets with reduced-conflict alignment objectives.\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedbackfrom GPT-4, marking as the first preference dataset aimed at reducing thecompetition between alignment objectives. Furthermore, we develop rewardmodels, HummerRM and HummerRM-F, which employ a hybrid sampling approach tobalance diverse alignment objectives effectively. This sampling methodpositions HummerRM as an ideal model for domain-specific further fine-tuningand reducing vulnerabilities to attacks.</description><author>Li Jiang, Yusen Wu, Junwu Xiong, Jingqing Ruan, Yichuan Ding, Qingpei Guo, Zujie Wen, Jun Zhou, Xiaotie Deng</author><pubDate>Tue, 06 Aug 2024 14:12:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11647v3</guid></item><item><title>IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning using Instruct Prompts</title><link>http://arxiv.org/abs/2408.03209v1</link><description>Diffusion models continuously push the boundary of state-of-the-art imagegeneration, but the process is hard to control with any nuance: practice provesthat textual prompts are inadequate for accurately describing image style orfine structural details (such as faces). ControlNet and IPAdapter address thisshortcoming by conditioning the generative process on imagery instead, but eachindividual instance is limited to modeling a single conditional posterior: forpractical use-cases, where multiple different posteriors are desired within thesame workflow, training and using multiple adapters is cumbersome. We proposeIPAdapter-Instruct, which combines natural-image conditioning with ``Instruct''prompts to swap between interpretations for the same conditioning image: styletransfer, object extraction, both, or something else still? IPAdapterInstructefficiently learns multiple tasks with minimal loss in quality compared todedicated per-task models.</description><author>Ciara Rowles, Shimon Vainer, Dante De Nigris, Slava Elizarov, Konstantin Kutsy, Simon Donné</author><pubDate>Tue, 06 Aug 2024 14:08:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03209v1</guid></item><item><title>Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery</title><link>http://arxiv.org/abs/2408.03208v1</link><description>Personalized federated learning (PFL) for surgical instrument segmentation(SIS) is a promising approach. It enables multiple clinical sites tocollaboratively train a series of models in privacy, with each model tailoredto the individual distribution of each site. Existing PFL methods rarelyconsider the personalization of multi-headed self-attention, and do not accountfor appearance diversity and instrument shape similarity, both inherent insurgical scenes. We thus propose PFedSIS, a novel PFL method with visual traitpriors for SIS, incorporating global-personalized disentanglement (GPD),appearance-regulation personalized enhancement (APE), and shape-similarityglobal enhancement (SGE), to boost SIS performance in each site. GPD representsthe first attempt at head-wise assignment for multi-headed self-attentionpersonalization. To preserve the unique appearance representation of each siteand gradually leverage the inter-site difference, APE introduces appearanceregulation and provides customized layer-wise aggregation solutions viahypernetworks for each site's personalized parameters. The mutual shapeinformation of instruments is maintained and shared via SGE, which enhances thecross-style shape consistency on the image level and computes theshape-similarity contribution of each site on the prediction level for updatingthe global parameters. PFedSIS outperforms state-of-the-art methods with +1.51%Dice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The correspondingcode and models will be released at https://github.com/wzjialang/PFedSIS.</description><author>Jialang Xu, Jiacheng Wang, Lequan Yu, Danail Stoyanov, Yueming Jin, Evangelos B. Mazomenos</author><pubDate>Tue, 06 Aug 2024 14:06:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03208v1</guid></item><item><title>A Debiased Nearest Neighbors Framework for Multi-Label Text Classification</title><link>http://arxiv.org/abs/2408.03202v1</link><description>Multi-Label Text Classification (MLTC) is a practical yet challenging taskthat involves assigning multiple non-exclusive labels to each document.Previous studies primarily focus on capturing label correlations to assistlabel prediction by introducing special labeling schemes, designing specificmodel structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor($k$NN) framework has shown promise by retrieving labeled samples as referencesto mine label co-occurrence information in the embedding space. However, twocritical biases, namely embedding alignment bias and confidence estimationbias, are often overlooked, adversely affecting prediction performance. In thispaper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC,specifically designed to mitigate these biases. To address embedding alignmentbias, we propose a debiased contrastive learning strategy, enhancing neighborconsistency on label co-occurrence. For confidence estimation bias, we presenta debiased confidence estimation strategy, improving the adaptive combinationof predictions from $k$NN and inductive binary classifications. Extensiveexperiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2,Amazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method.Besides, our method does not introduce any extra parameters.</description><author>Zifeng Cheng, Zhiwei Jiang, Yafeng Yin, Zhaoling Chen, Cong Wang, Shiping Ge, Qiguo Huang, Qing Gu</author><pubDate>Tue, 06 Aug 2024 14:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03202v1</guid></item><item><title>Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors</title><link>http://arxiv.org/abs/2408.03200v1</link><description>Evaluating the decision-making system is indispensable in developingautonomous vehicles, while realistic and challenging safety-critical testscenarios play a crucial role. Obtaining these scenarios is non-trivial, thanksto the long-tailed distribution, sparsity, and rarity in real-world data sets.To tackle this problem, in this paper, we introduce a natural adversarialscenario generation solution using naturalistic human driving priors andreinforcement learning techniques. By doing this, we can obtain large-scaletest scenarios that are both diverse and realistic. Specifically, we build asimulation environment that mimics natural traffic interaction scenarios.Informed by this environment, we implement a two-stage procedure. The firststage incorporates conventional rule-based models, e.g., IDM~(IntelligentDriver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes)model, to coarsely and discretely capture and calibrate key control parametersfrom the real-world dataset. Next, we leverage GAIL~(Generative AdversarialImitation Learning) to represent driver behaviors continuously. The derivedGAIL can be further used to design a PPO~(Proximal Policy Optimization)-basedactor-critic network framework to fine-tune the reward function, and thenoptimizes our natural adversarial scenario generation solution. Extensiveexperiments have been conducted in the NGSIM dataset including the trajectoryof 3,000 vehicles. Essential traffic parameters were measured in comparisonwith the baseline model, e.g., the collision rate, accelerations, steering, andthe number of lane changes. Our findings demonstrate that the proposed modelcan generate realistic safety-critical test scenarios covering both naturalnessand adversariality, which can be a cornerstone for the development ofautonomous vehicles.</description><author>Kunkun Hao, Yonggang Luo, Wen Cui, Yuqiao Bai, Jucheng Yang, Songyang Yan, Yuxi Pan, Zijiang Yang</author><pubDate>Tue, 06 Aug 2024 13:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03200v1</guid></item><item><title>Convergence Conditions for Stochastic Line Search Based Optimization of Over-parametrized Models</title><link>http://arxiv.org/abs/2408.03199v1</link><description>In this paper, we deal with algorithms to solve the finite-sum problemsrelated to fitting over-parametrized models, that typically satisfy theinterpolation condition. In particular, we focus on approaches based onstochastic line searches and employing general search directions. We defineconditions on the sequence of search directions that guarantee finitetermination and bounds for the backtracking procedure. Moreover, we shed lighton the additional property of directions needed to prove fast (linear)convergence of the general class of algorithms when applied to PL functions inthe interpolation regime. From the point of view of algorithms design, theproposed analysis identifies safeguarding conditions that could be employed inrelevant algorithmic framework. In particular, it could be of interest tointegrate stochastic line searches within momentum, conjugate gradient oradaptive preconditioning methods.</description><author>Matteo Lapucci, Davide Pucci</author><pubDate>Tue, 06 Aug 2024 13:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03199v1</guid></item><item><title>RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning</title><link>http://arxiv.org/abs/2408.03195v1</link><description>The advent of the "pre-train, prompt" paradigm has recently extended itsgeneralization ability and data efficiency to graph representation learning,following its achievements in Natural Language Processing (NLP). Initial graphprompt tuning approaches tailored specialized prompting functions for GraphNeural Network (GNN) models pre-trained with specific strategies, such as edgeprediction, thus limiting their applicability. In contrast, another pioneeringline of research has explored universal prompting via adding prompts to theinput graph's feature space, thereby removing the reliance on specificpre-training strategies. However, the necessity to add feature prompts to allnodes remains an open question. Motivated by findings from prompt tuningresearch in the NLP domain, which suggest that highly capable pre-trainedmodels need less conditioning signal to achieve desired behaviors, we advocatefor strategically incorporating necessary and lightweight feature prompts tocertain graph nodes to enhance downstream task performance. This introduces acombinatorial optimization problem, requiring a policy to decide 1) which nodesto prompt and 2) what specific feature prompts to attach. We then address theproblem by framing the prompt incorporation process as a sequentialdecision-making problem and propose our method, RELIEF, which employsReinforcement Learning (RL) to optimize it. At each step, the RL agent selectsa node (discrete action) and determines the prompt content (continuous action),aiming to maximize cumulative performance gain. Extensive experiments on graphand node-level tasks with various pre-training strategies in few-shot scenariosdemonstrate that our RELIEF outperforms fine-tuning and other prompt-basedapproaches in classification performance and data efficiency.</description><author>Jiapeng Zhu, Zichen Ding, Jianxiang Yu, Jiaqi Tan, Xiang Li, Weining Qian</author><pubDate>Tue, 06 Aug 2024 13:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03195v1</guid></item><item><title>SGSR: Structure-Guided Multi-Contrast MRI Super-Resolution via Spatio-Frequency Co-Query Attention</title><link>http://arxiv.org/abs/2408.03194v1</link><description>Magnetic Resonance Imaging (MRI) is a leading diagnostic modality for a widerange of exams, where multiple contrast images are often acquired forcharacterizing different tissues. However, acquiring high-resolution MRItypically extends scan time, which can introduce motion artifacts.Super-resolution of MRI therefore emerges as a promising approach to mitigatethese challenges. Earlier studies have investigated the use of multiplecontrasts for MRI super-resolution (MCSR), whereas majority of them did notfully exploit the rich contrast-invariant structural information. To fullyutilize such crucial prior knowledge of multi-contrast MRI, in this work, wepropose a novel structure-guided MCSR (SGSR) framework based on a newspatio-frequency co-query attention (CQA) mechanism. Specifically, CQA performsattention on features of multiple contrasts with a shared structural query,which is particularly designed to extract, fuse, and refine the commonstructures from different contrasts. We further propose a novelfrequency-domain CQA module in addition to the spatial domain, to enable morefine-grained structural refinement. Extensive experiments on fastMRI knee dataand low-field brain MRI show that SGSR outperforms state-of-the-art MCSRmethods with statistical significance.</description><author>Shaoming Zheng, Yinsong Wang, Siyi Du, Chen Qin</author><pubDate>Tue, 06 Aug 2024 13:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03194v1</guid></item><item><title>Efficient NeRF Optimization -- Not All Samples Remain Equally Hard</title><link>http://arxiv.org/abs/2408.03193v1</link><description>We propose an application of online hard sample mining for efficient trainingof Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art qualityfor many 3D reconstruction and rendering tasks but require substantialcomputational resources. The encoding of the scene information within the NeRFnetwork parameters necessitates stochastic sampling. We observe that during thetraining, a major part of the compute time and memory usage is spent onprocessing already learnt samples, which no longer affect the model updatesignificantly. We identify the backward pass on the stochastic samples as thecomputational bottleneck during the optimization. We thus perform the firstforward pass in inference mode as a relatively low-cost search for hardsamples. This is followed by building the computational graph and updating theNeRF network parameters using only the hard samples. To demonstrate theeffectiveness of the proposed approach, we apply our method to Instant-NGP,resulting in significant improvements of the view-synthesis quality over thebaseline (1 dB improvement on average per training time, or 2x speedup to reachthe same PSNR level) along with approx. 40% memory savings coming from usingonly the hard samples to build the computational graph. As our method onlyinterfaces with the network module, we expect it to be widely applicable.</description><author>Juuso Korhonen, Goutham Rangu, Hamed R. Tavakoli, Juho Kannala</author><pubDate>Tue, 06 Aug 2024 13:49:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03193v1</guid></item><item><title>Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized Least-Squares Algorithm</title><link>http://arxiv.org/abs/2312.07186v5</link><description>We present the first optimal rates for infinite-dimensional vector-valuedridge regression on a continuous scale of norms that interpolate between $L_2$and the hypothesis space, which we consider as a vector-valued reproducingkernel Hilbert space. These rates allow to treat the misspecified case in whichthe true regression function is not contained in the hypothesis space. Wecombine standard assumptions on the capacity of the hypothesis space with anovel tensor product construction of vector-valued interpolation spaces inorder to characterize the smoothness of the regression function. Our upperbound not only attains the same rate as real-valued kernel ridge regression,but also removes the assumption that the target regression function is bounded.For the lower bound, we reduce the problem to the scalar setting using aprojection argument. We show that these rates are optimal in most cases andindependent of the dimension of the output space. We illustrate our results forthe special case of vector-valued Sobolev spaces.</description><author>Zhu Li, Dimitri Meunier, Mattes Mollenhauer, Arthur Gretton</author><pubDate>Tue, 06 Aug 2024 13:47:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07186v5</guid></item><item><title>Stability-Informed Initialization of Neural Ordinary Differential Equations</title><link>http://arxiv.org/abs/2311.15890v3</link><description>This paper addresses the training of Neural Ordinary Differential Equations(neural ODEs), and in particular explores the interplay between numericalintegration techniques, stability regions, step size, and initializationtechniques. It is shown how the choice of integration technique implicitlyregularizes the learned model, and how the solver's corresponding stabilityregion affects training and prediction performance. From this analysis, astability-informed parameter initialization technique is introduced. Theeffectiveness of the initialization method is displayed across several learningbenchmarks and industrial applications.</description><author>Theodor Westny, Arman Mohammadi, Daniel Jung, Erik Frisk</author><pubDate>Tue, 06 Aug 2024 13:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15890v3</guid></item><item><title>SentenceVAE: Enable Next-sentence Prediction for Large Language Models with Faster Speed, Higher Accuracy and Longer Context</title><link>http://arxiv.org/abs/2408.00655v3</link><description>Current large language models (LLMs) primarily utilize next-token predictionmethod for inference, which significantly impedes their processing speed. Inthis paper, we introduce a novel inference methodology termed next-sentenceprediction, aimed at enhancing the inference efficiency of LLMs. We presentSentence Variational Autoencoder (SentenceVAE), a tiny model consisting of aSentence Encoder and a Sentence Decoder. The Sentence Encoder can effectivelycondense the information within a sentence into a singular token, while theSentence Decoder can reconstruct this compressed token back into sentence. Byintegrating SentenceVAE into the input and output layers of LLMs, we developSentence-level LLMs (SLLMs) that employ a sentence-by-sentence inferencemethod. In addition, the SentenceVAE module of SLLMs can maintain the integrityof the original semantic content by segmenting the context into sentences,thereby improving accuracy while boosting inference speed. Moreover, comparedto previous LLMs, SLLMs process fewer tokens over equivalent context length,significantly reducing memory demands for self-attention computation andfacilitating the handling of longer context. Extensive experiments on Wanjuandataset have reveal that the proposed method can accelerate inference speed by204~365%, reduce perplexity (PPL) to 46~75% of its original metric, anddecrease memory overhead by 86~91% for the equivalent context length, comparedto the token-by-token method.</description><author>Hongjun An, Yifan Chen, Xiaozhen Qiao, Zhe Sun, Xuelong Li</author><pubDate>Tue, 06 Aug 2024 13:38:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00655v3</guid></item><item><title>DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for Fingerprint Presentation Attack Detection</title><link>http://arxiv.org/abs/2308.10015v2</link><description>Automatic fingerprint recognition systems suffer from the threat ofpresentation attacks due to their wide range of deployment in areas includingnational borders and commercial applications. A presentation attack can beperformed by creating a spoof of a user's fingerprint with or without theirconsent. This paper presents a dynamic ensemble of deep CNN and handcraftedfeatures to detect presentation attacks in known-material and unknown-materialprotocols of the livness detection competition. The proposed presentationattack detection model, in this way, utilizes the capabilities of both deep CNNand handcrafted features techniques and exhibits better performance than theirindividual performances. The proposed method is validated using benchmarkdatabases from the Liveness Detection Competition in 2015, 2017, and 2019,yielding overall accuracy of 96.10\%, 96.49\%, and 94.99\% on them,respectively. The proposed method outperforms state-of-the-art methods in termsof classification accuracy.</description><author>Anuj Rai, Parsheel Kumar Tiwari, Jyotishna Baishya, Ram Prakash Sharma, Somnath Dey</author><pubDate>Tue, 06 Aug 2024 13:25:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10015v2</guid></item><item><title>An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion</title><link>http://arxiv.org/abs/2408.03178v1</link><description>We introduce a new approach for generating realistic 3D models with UV mapsthrough a representation termed "Object Images." This approach encapsulatessurface geometry, appearance, and patch structures within a 64x64 pixel image,effectively converting complex 3D shapes into a more manageable 2D format. Bydoing so, we address the challenges of both geometric and semantic irregularityinherent in polygonal meshes. This method allows us to use image generationmodels, such as Diffusion Transformers, directly for 3D shape generation.Evaluated on the ABO dataset, our generated shapes with patch structuresachieve point cloud FID comparable to recent 3D generative models, whilenaturally supporting PBR material generation.</description><author>Xingguang Yan, Han-Hung Lee, Ziyu Wan, Angel X. Chang</author><pubDate>Tue, 06 Aug 2024 13:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03178v1</guid></item><item><title>Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition</title><link>http://arxiv.org/abs/2407.15706v5</link><description>Skeleton-based action recognition has garnered significant attention due tothe utilization of concise and resilient skeletons. Nevertheless, the absenceof detailed body information in skeletons restricts performance, while othermultimodal methods require substantial inference resources and are inefficientwhen using multimodal data during both training and inference stages. Toaddress this and fully harness the complementary multimodal features, wepropose a novel multi-modality co-learning (MMCL) framework by leveraging themultimodal large language models (LLMs) as auxiliary networks for efficientskeleton-based action recognition, which engages in multi-modality co-learningduring the training stage and keeps efficiency by employing only conciseskeletons in inference. Our MMCL framework primarily consists of two modules.First, the Feature Alignment Module (FAM) extracts rich RGB features from videoframes and aligns them with global skeleton features via contrastive learning.Second, the Feature Refinement Module (FRM) uses RGB images with temporalinformation and text instruction to generate instructive features based on thepowerful generalization of multimodal LLMs. These instructive text featureswill further refine the classification scores and the refined scores willenhance the model's robustness and generalization in a manner similar to softlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLAbenchmarks consistently verify the effectiveness of our MMCL, which outperformsthe existing skeleton-based action recognition methods. Meanwhile, experimentson UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalizationof our MMCL in zero-shot and domain-adaptive action recognition. Our code ispublicly available at: https://github.com/liujf69/MMCL-Action.</description><author>Jinfu Liu, Chen Chen, Mengyuan Liu</author><pubDate>Tue, 06 Aug 2024 13:20:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15706v5</guid></item><item><title>Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi</title><link>http://arxiv.org/abs/2408.03172v1</link><description>With the surge in digital content in low-resource languages, there is anescalating demand for advanced Natural Language Processing (NLP) techniquestailored to these languages. BERT (Bidirectional Encoder Representations fromTransformers), serving as the foundational framework for numerous NLParchitectures and language models, is increasingly employed for the developmentof low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a methodfor fine-tuning Large Language Models (LLMs) and reducing the trainingparameters to some extent to decrease the computational costs needed fortraining the model and achieve results comparable to a fully fine-tuned model.In this work, we present a study of PEFT methods for the Indic low-resourcelanguage Marathi. We conduct a comprehensive analysis of PEFT methods appliedto various monolingual and multilingual Marathi BERT models. These approachesare evaluated on prominent text classification datasets like MahaSent,MahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated tosignificantly expedite the training speed of the models, addressing a criticalaspect of model development and deployment. In this study, we explore Low-RankAdaptation of Large Language Models (LoRA) and adapter methods for low-resourcetext classification. We show that these methods are competitive with fullfine-tuning and can be used without loss in accuracy. This study contributesvaluable insights into the effectiveness of Marathi BERT models, offering afoundation for the continued advancement of NLP capabilities in Marathi andsimilar Indic languages.</description><author>Pranita Deshmukh, Nikita Kulkarni, Sanhita Kulkarni, Kareena Manghani, Raviraj Joshi</author><pubDate>Tue, 06 Aug 2024 13:16:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03172v1</guid></item><item><title>Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW</title><link>http://arxiv.org/abs/2408.03168v1</link><description>Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning(TinyML), such as nano-drones, are becoming an increasingly attractivetechnology. Their small form factor (i.e., ~10cm diameter) ensures vastapplicability, ranging from the exploration of narrow disaster scenarios tosafe human-robot interaction. Simple electronics make these CPSes inexpensive,but strongly limit the computational, memory, and sensing resources availableon board. In real-world applications, these limitations are further exacerbatedby domain shift. This fundamental machine learning problem implies that modelperception performance drops when moving from the training domain to adifferent deployment one. To cope with and mitigate this general problem, wepresent a novel on-device fine-tuning approach that relies only on the limitedultra-low power resources available aboard nano-drones. Then, to overcome thelack of ground-truth training labels aboard our CPS, we also employ aself-supervised method based on ego-motion consistency. Albeit our work buildson top of a specific real-world vision-based human pose estimation task, it iswidely applicable for many embedded TinyML use cases. Our 512-image on-devicetraining procedure is fully deployed aboard an ultra-low power GWT GAP9System-on-Chip and requires only 1MB of memory while consuming as low as 19mWor running in just 510ms (at 38mW). Finally, we demonstrate the benefits of ouron-device learning approach by field-testing our closed-loop CPS, showing areduction in horizontal position error of up to 26% vs. a non-fine-tunedstate-of-the-art baseline. In the most challenging never-seen-beforeenvironment, our on-device learning procedure makes the difference betweensucceeding or failing the mission.</description><author>Elia Cereda, Alessandro Giusti, Daniele Palossi</author><pubDate>Tue, 06 Aug 2024 13:11:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03168v1</guid></item><item><title>SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery Videos via Physics-embedded 3D Gaussians</title><link>http://arxiv.org/abs/2405.00956v3</link><description>Surgical scene simulation plays a crucial role in surgical education andsimulator-based robot learning. Traditional approaches for creating theseenvironments with surgical scene involve a labor-intensive process wheredesigners hand-craft tissues models with textures and geometries for soft bodysimulations. This manual approach is not only time-consuming but also limitedin the scalability and realism. In contrast, data-driven simulation offers acompelling alternative. It has the potential to automatically reconstruct 3Dsurgical scenes from real-world surgical video data, followed by theapplication of soft body physics. This area, however, is relatively uncharted.In our research, we introduce 3D Gaussian as a learnable representation forsurgical scene, which is learned from stereo endoscopic video. To preventover-fitting and ensure the geometrical correctness of these scenes, weincorporate depth supervision and anisotropy regularization into the Gaussianlearning process. Furthermore, we apply the Material Point Method, which isintegrated with physical properties, to the 3D Gaussians to achieve realisticscene deformations. Our method was evaluated on our collected in-house andpublic surgical videos datasets. Results show that it can reconstruct andsimulate surgical scenes from endoscopic videos efficiently-taking only a fewminutes to reconstruct the surgical scene-and produce both visually andphysically plausible deformations at a speed approaching real-time. The resultsdemonstrate great potential of our proposed method to enhance the efficiencyand variety of simulations available for surgical education and robot learning.</description><author>Zhenya Yang, Kai Chen, Yonghao Long, Qi Dou</author><pubDate>Tue, 06 Aug 2024 13:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00956v3</guid></item><item><title>IMAGDressing-v1: Customizable Virtual Dressing</title><link>http://arxiv.org/abs/2407.12705v2</link><description>Latest advances have achieved realistic virtual try-on (VTON) throughlocalized garment inpainting using latent diffusion models, significantlyenhancing consumers' online shopping experience. However, existing VTONtechnologies neglect the need for merchants to showcase garmentscomprehensively, including flexible control over garments, optional faces,poses, and scenes. To address this issue, we define a virtual dressing (VD)task focused on generating freely editable human images with fixed garments andoptional conditions. Meanwhile, we design a comprehensive affinity metric index(CAMI) to evaluate the consistency between generated images and referencegarments. Then, we propose IMAGDressing-v1, which incorporates a garment UNetthat captures semantic features from CLIP and texture features from VAE. Wepresent a hybrid attention module, including a frozen self-attention and atrainable cross-attention, to integrate garment features from the garment UNetinto a frozen denoising UNet, ensuring users can control different scenesthrough text. IMAGDressing-v1 can be combined with other extension plugins,such as ControlNet and IP-Adapter, to enhance the diversity and controllabilityof generated images. Furthermore, to address the lack of data, we release theinteractive garment pairing (IGPair) dataset, containing over 300,000 pairs ofclothing and dressed images, and establish a standard pipeline for dataassembly. Extensive experiments demonstrate that our IMAGDressing-v1 achievesstate-of-the-art human image synthesis performance under various controlledconditions. The code and model will be available athttps://github.com/muzishen/IMAGDressing.</description><author>Fei Shen, Xin Jiang, Xin He, Hu Ye, Cong Wang, Xiaoyu Du, Zechao Li, Jinhui Tang</author><pubDate>Tue, 06 Aug 2024 13:06:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12705v2</guid></item><item><title>Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study</title><link>http://arxiv.org/abs/2408.03164v1</link><description>Dilated Convolution with Learnable Spacing (DCLS) is a recent advancedconvolution method that allows enlarging the receptive fields (RF) withoutincreasing the number of parameters, like the dilated convolution, yet withoutimposing a regular grid. DCLS has been shown to outperform the standard anddilated convolutions on several computer vision benchmarks. Here, we show that,in addition, DCLS increases the models' interpretability, defined as thealignment with human visual strategies. To quantify it, we use the Spearmancorrelation between the models' GradCAM heatmaps and the ClickMe datasetheatmaps, which reflect human visual attention. We took eight reference models- ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and36) - and drop-in replaced the standard convolution layers with DCLS ones. Thisimproved the interpretability score in seven of them. Moreover, we observedthat Grad-CAM generated random heatmaps for two models in our study: CAFormerand ConvFormer models, leading to low interpretability scores. We addressedthis issue by introducing Threshold-Grad-CAM, a modification built on top ofGrad-CAM that enhanced interpretability across nearly all models. The code andcheckpoints to reproduce this study are available at:https://github.com/rabihchamas/DCLS-GradCAM-Eval.</description><author>Rabih Chamas, Ismail Khalfaoui-Hassani, Timothee Masquelier</author><pubDate>Tue, 06 Aug 2024 13:05:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03164v1</guid></item><item><title>Iterative CT Reconstruction via Latent Variable Optimization of Shallow Diffusion Models</title><link>http://arxiv.org/abs/2408.03156v1</link><description>Image generative AI has garnered significant attention in recent years. Inparticular, the diffusion model, a core component of recent generative AI,produces high-quality images with rich diversity. In this study, we propose anovel CT reconstruction method by combining the denoising diffusionprobabilistic model with iterative CT reconstruction. In sharp contrast toprevious studies, we optimize the fidelity loss of CT reconstruction withrespect to the latent variable of the diffusion model, instead of the image andmodel parameters. To suppress anatomical structure changes produced by thediffusion model, we shallow the diffusion and reverse processes, and fix a setof added noises in the reverse process to make it deterministic duringinference. We demonstrate the effectiveness of the proposed method throughsparse view CT reconstruction of 1/10 view projection data. Despite thesimplicity of the implementation, the proposed method shows the capability ofreconstructing high-quality images while preserving the patient's anatomicalstructure, and outperforms existing methods including iterative reconstruction,iterative reconstruction with total variation, and the diffusion model alone interms of quantitative indices such as SSIM and PSNR. We also explore furthersparse view CT using 1/20 view projection data with the same trained diffusionmodel. As the number of iterations increases, image quality improvementcomparable to that of 1/10 sparse view CT reconstruction is achieved. Inprinciple, the proposed method can be widely applied not only to CT but also toother imaging modalities such as MRI, PET, and SPECT.</description><author>Sho Ozaki, Shizuo Kaji, Toshikazu Imae, Kanabu Nawa, Hideomi Yamashita, Keiichi Nakagawa</author><pubDate>Tue, 06 Aug 2024 12:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03156v1</guid></item><item><title>DiffX: Guide Your Layout to Cross-Modal Generative Modeling</title><link>http://arxiv.org/abs/2407.15488v3</link><description>Diffusion models have made significant strides in language-driven andlayout-driven image generation. However, most diffusion models are limited tovisible RGB image generation. In fact, human perception of the world isenriched by diverse viewpoints, such as chromatic contrast, thermalillumination, and depth information. In this paper, we introduce a noveldiffusion model for general layout-guided cross-modal generation, called DiffX.Notably, DiffX presents a simple yet effective cross-modal generative modelingpipeline, which conducts diffusion and denoising processes in themodality-shared latent space. Moreover, we introduce the Joint-ModalityEmbedder (JME) to enhance interaction between layout and text conditions byincorporating a gated attention mechanism. Meanwhile, the advanced Long-CLIP isemployed for long caption embedding for user instruction. To facilitate theuser-instructed generative training, we construct the cross-modal imagedatasets with detailed text captions assisted by the Large-Multimodal Model(LMM). Through extensive experiments, DiffX demonstrates robustness incross-modal generation across three ``RGB+X'' datasets: FLIR, MFNet, andCOME15K, guided by various layout conditions. It also shows the potential forthe adaptive generation of ``RGB+X+Y+Z'' images or more diverse modalities onCOME15K and MCXFace datasets. Our code and constructed cross-modal imagedatasets are available at https://github.com/zeyuwang-zju/DiffX.</description><author>Zeyu Wang, Jingyu Lin, Yifei Qian, Yi Huang, Shicen Tian, Bosong Chai, Juncan Deng, Lan Du, Cunjian Chen, Yufei Guo, Kejie Huang</author><pubDate>Tue, 06 Aug 2024 12:54:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15488v3</guid></item><item><title>When a Relation Tells More Than a Concept: Exploring and Evaluating Classifier Decisions with CoReX</title><link>http://arxiv.org/abs/2405.01661v3</link><description>Explanations for Convolutional Neural Networks (CNNs) based on relevance ofinput pixels might be too unspecific to evaluate which and how input featuresimpact model decisions. Especially in complex real-world domains like biology,the presence of specific concepts and of relations between concepts might bediscriminating between classes. Pixel relevance is not expressive enough toconvey this type of information. In consequence, model evaluation is limitedand relevant aspects present in the data and influencing the model decisionsmight be overlooked. This work presents a novel method to explain and evaluateCNN models, which uses a concept- and relation-based explainer (CoReX). Itexplains the predictive behavior of a model on a set of images by masking(ir-)relevant concepts from the decision-making process and by constrainingrelations in a learned interpretable surrogate model. We test our approach withseveral image data sets and CNN architectures. Results show that CoReXexplanations are faithful to the CNN model in terms of predictive outcomes. Wefurther demonstrate through a human evaluation that CoReX is a suitable toolfor generating combined explanations that help assessing the classificationquality of CNNs. We further show that CoReX supports the identification andre-classification of incorrect or ambiguous classifications.</description><author>Bettina Finzel, Patrick Hilme, Johannes Rabold, Ute Schmid</author><pubDate>Tue, 06 Aug 2024 12:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01661v3</guid></item><item><title>TSC: A Simple Two-Sided Constraint against Over-Smoothing</title><link>http://arxiv.org/abs/2408.03152v1</link><description>Graph Convolutional Neural Network (GCN), a widely adopted method foranalyzing relational data, enhances node discriminability through theaggregation of neighboring information. Usually, stacking multiple layers canimprove the performance of GCN by leveraging information from high-orderneighbors. However, the increase of the network depth will induce theover-smoothing problem, which can be attributed to the quality and quantity ofneighbors changing: (a) neighbor quality, node's neighbors become overlappingin high order, leading to aggregated information becoming indistinguishable,(b) neighbor quantity, the exponentially growing aggregated neighbors submergesthe node's initial feature by recursively aggregating operations. Currentsolutions mainly focus on one of the above causes and seldom consider both atonce. Aiming at tackling both causes of over-smoothing in one shot, we introduce asimple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yetpotent techniques: random masking and contrastive constraint. The randommasking acts on the representation matrix's columns to regulate the degree ofinformation aggregation from neighbors, thus preventing the convergence of noderepresentations. Meanwhile, the contrastive constraint, applied to therepresentation matrix's rows, enhances the discriminability of the nodes.Designed as a plug-in module, TSC can be easily coupled with GCN or SGCarchitectures. Experimental analyses on diverse real-world graph datasetsverify that our approach markedly reduces the convergence of node'srepresentation and the performance degradation in deeper GCN.</description><author>Furong Peng, Kang Liu, Xuan Lu, Yuhua Qian, Hongren Yan, Chao Ma</author><pubDate>Tue, 06 Aug 2024 12:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03152v1</guid></item><item><title>Conditioning LLMs with Emotion in Neural Machine Translation</title><link>http://arxiv.org/abs/2408.03150v1</link><description>Large Language Models (LLMs) have shown remarkable performance in NaturalLanguage Processing tasks, including Machine Translation (MT). In this work, wepropose a novel MT pipeline that integrates emotion information extracted froma Speech Emotion Recognition (SER) model into LLMs to enhance translationquality. We first fine-tune five existing LLMs on the Libri-trans dataset andselect the most performant model. Subsequently, we augment LLM prompts withdifferent dimensional emotions and train the selected LLM under these differentconfigurations. Our experiments reveal that integrating emotion information,especially arousal, into LLM prompts leads to notable improvements intranslation quality.</description><author>Charles Brazier, Jean-Luc Rouas</author><pubDate>Tue, 06 Aug 2024 12:49:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03150v1</guid></item><item><title>Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization</title><link>http://arxiv.org/abs/2408.03149v1</link><description>The rapid increase in multimedia data has spurred advancements in MultimodalSummarization with Multimodal Output (MSMO), which aims to produce a multimodalsummary that integrates both text and relevant images. The inherentheterogeneity of content within multimodal inputs and outputs presents asignificant challenge to the execution of MSMO. Traditional approachestypically adopt a holistic perspective on coarse image-text data or individualvisual objects, overlooking the essential connections between objects and theentities they represent. To integrate the fine-grained entity knowledge, wepropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,building on BART, utilizes dual multimodal encoders with shared weights toprocess text-image and entity-image information concurrently. A gatingmechanism then combines visual data for enhanced textual summary generation,while image selection is refined through knowledge distillation from apre-trained vision-language model. Extensive experiments on public MSMO datasetvalidate the superiority of the EGMS method, which also prove the necessity toincorporate entity information into MSMO problem.</description><author>Yanghai Zhang, Ye Liu, Shiwei Wu, Kai Zhang, Xukai Liu, Qi Liu, Enhong Chen</author><pubDate>Tue, 06 Aug 2024 12:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03149v1</guid></item><item><title>Active Learning for Level Set Estimation Using Randomized Straddle Algorithms</title><link>http://arxiv.org/abs/2408.03144v1</link><description>Level set estimation (LSE), the problem of identifying the set of inputpoints where a function takes value above (or below) a given threshold, isimportant in practical applications. When the function is expensive-to-evaluateand black-box, the \textit{straddle} algorithm, which is a representativeheuristic for LSE based on Gaussian process models, and its extensions havingtheoretical guarantees have been developed. However, many of existing methodsinclude a confidence parameter $\beta^{1/2}_t$ that must be specified by theuser, and methods that choose $\beta^{1/2}_t$ heuristically do not providetheoretical guarantees. In contrast, theoretically guaranteed values of$\beta^{1/2}_t$ need to be increased depending on the number of iterations andcandidate points, and are conservative and not good for practical performance.In this study, we propose a novel method, the \textit{randomized straddle}algorithm, in which $\beta_t$ in the straddle algorithm is replaced by a randomsample from the chi-squared distribution with two degrees of freedom. Theconfidence parameter in the proposed method has the advantages of not needingadjustment, not depending on the number of iterations and candidate points, andnot being conservative. Furthermore, we show that the proposed method hastheoretical guarantees that depend on the sample complexity and the number ofiterations. Finally, we confirm the usefulness of the proposed method throughnumerical experiments using synthetic and real data.</description><author>Yu Inatsu, Shion Takeno, Kentaro Kutsukake, Ichiro Takeuchi</author><pubDate>Tue, 06 Aug 2024 12:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03144v1</guid></item><item><title>SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast and Reliable Surface Defect Detection</title><link>http://arxiv.org/abs/2408.03143v1</link><description>The aim of surface defect detection is to identify and localise abnormalregions on the surfaces of captured objects, a task that's increasinglydemanded across various industries. Current approaches frequently fail tofulfil the extensive demands of these industries, which encompass highperformance, consistency, and fast operation, along with the capacity toleverage the entirety of the available training data. Addressing these gaps, weintroduce SuperSimpleNet, an innovative discriminative model that evolved fromSimpleNet. This advanced model significantly enhances its predecessor'straining consistency, inference time, as well as detection performance.SuperSimpleNet operates in an unsupervised manner using only normal trainingimages but also benefits from labelled abnormal training images when they areavailable. SuperSimpleNet achieves state-of-the-art results in both thesupervised and the unsupervised settings, as demonstrated by experiments acrossfour challenging benchmark datasets. Code:https://github.com/blaz-r/SuperSimpleNet .</description><author>Blaž Rolih, Matic Fučka, Danijel Skočaj</author><pubDate>Tue, 06 Aug 2024 12:37:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03143v1</guid></item><item><title>Convergence Analysis of Natural Gradient Descent for Over-parameterized Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2408.00573v2</link><description>First-order methods, such as gradient descent (GD) and stochastic gradientdescent (SGD), have been proven effective in training neural networks. In thecontext of over-parameterization, there is a line of work demonstrating thatrandomly initialized (stochastic) gradient descent converges to a globallyoptimal solution at a linear convergence rate for the quadratic loss function.However, the learning rate of GD for training two-layer neural networksexhibits poor dependence on the sample size and the Gram matrix, leading to aslow training process. In this paper, we show that for the $L^2$ regressionproblems, the learning rate can be improved from $\mathcal{O}(\lambda_0/n^2)$to $\mathcal{O}(1/\|\bm{H}^{\infty}\|_2)$, which implies that GD actuallyenjoys a faster convergence rate. Furthermore, we generalize the method to GDin training two-layer Physics-Informed Neural Networks (PINNs), showing asimilar improvement for the learning rate. Although the improved learning ratehas a mild dependence on the Gram matrix, we still need to set it small enoughin practice due to the unknown eigenvalues of the Gram matrix. Moreimportantly, the convergence rate is tied to the least eigenvalue of the Grammatrix, which can lead to slow convergence. In this work, we provide theconvergence analysis of natural gradient descent (NGD) in training two-layerPINNs, demonstrating that the learning rate can be $\mathcal{O}(1)$, and atthis rate, the convergence rate is independent of the Gram matrix.</description><author>Xianliang Xu, Ting Du, Wang Kong, Ye Li, Zhongyi Huang</author><pubDate>Tue, 06 Aug 2024 12:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00573v2</guid></item><item><title>eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs</title><link>http://arxiv.org/abs/2407.21483v3</link><description>Over the past few years, we have seen the emergence of large knowledge graphscombining information from multiple sources. Sometimes, this information isprovided in the form of assertions about other assertions, defining contextswhere assertions are valid. A recent extension to RDF which admits statementsover statements, called RDF-star, is in revision to become a W3C standard.However, there is no proposal for a semantics of these RDF-star statements nora built-in facility to operate over them. In this paper, we propose a querylanguage for epistemic RDF-star metadata based on a four-valued logic, calledeSPARQL. Our proposed query language extends SPARQL-star, the query languagefor RDF-star, with a new type of FROM clause to facilitate operating withmultiple and sometimes conflicting beliefs. We show that the proposed querylanguage can express four use case queries, including the following features:(i) querying the belief of an individual, (ii) the aggregating of beliefs,(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs(i.e., nesting of beliefs).</description><author>Xinyi Pan, Daniel Hernández, Philipp Seifer, Ralf Lämmel, Steffen Staab</author><pubDate>Tue, 06 Aug 2024 12:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21483v3</guid></item><item><title>EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions</title><link>http://arxiv.org/abs/2402.17485v2</link><description>In this work, we tackle the challenge of enhancing the realism andexpressiveness in talking head video generation by focusing on the dynamic andnuanced relationship between audio cues and facial movements. We identify thelimitations of traditional techniques that often fail to capture the fullspectrum of human expressions and the uniqueness of individual facial styles.To address these issues, we propose EMO, a novel framework that utilizes adirect audio-to-video synthesis approach, bypassing the need for intermediate3D models or facial landmarks. Our method ensures seamless frame transitionsand consistent identity preservation throughout the video, resulting in highlyexpressive and lifelike animations. Experimental results demonsrate that EMO isable to produce not only convincing speaking videos but also singing videos invarious styles, significantly outperforming existing state-of-the-artmethodologies in terms of expressiveness and realism.</description><author>Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo</author><pubDate>Tue, 06 Aug 2024 12:33:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17485v2</guid></item></channel></rss>