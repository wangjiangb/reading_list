<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 04 Oct 2024 01:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models</title><link>http://arxiv.org/abs/2405.03869v4</link><description>A core data-centric learning challenge is the identification of trainingsamples that are detrimental to model performance. Influence functions serve asa prominent tool for this task and offer a robust framework for assessingtraining data influence on model predictions. Despite their widespread use,their high computational cost associated with calculating the inverse of theHessian matrix pose constraints, particularly when analyzing large-sized deepmodels. In this paper, we establish a bridge between identifying detrimentaltraining samples via influence functions and outlier gradient detection. Thistransformation not only presents a straightforward and Hessian-free formulationbut also provides insights into the role of the gradient in sample impact.Through systematic empirical evaluations, we first validate the hypothesis ofour proposed outlier gradient analysis approach on synthetic datasets. We thendemonstrate its effectiveness in detecting mislabeled samples in vision modelsand selecting data samples for improving performance of natural languageprocessing transformer models. We also extend its use to influential sampleidentification for fine-tuning Large Language Models.</description><author>Anshuman Chhabra, Bo Li, Jian Chen, Prasant Mohapatra, Hongfu Liu</author><pubDate>Wed, 02 Oct 2024 01:38:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.03869v4</guid></item><item><title>EZIGen: Enhancing zero-shot subject-driven image generation with precise subject encoding and decoupled guidance</title><link>http://arxiv.org/abs/2409.08091v2</link><description>Zero-shot subject-driven image generation aims to produce images thatincorporate a subject from a given example image. The challenge lies inpreserving the subject's identity while aligning with the text prompt whichoften requires modifying certain aspects of the subject's appearance. Despiteadvancements in diffusion model based methods, existing approaches stillstruggle to balance identity preservation with text prompt alignment. In thisstudy, we conducted an in-depth investigation into this issue and uncovered keyinsights for achieving effective identity preservation while maintaining astrong balance. Our key findings include: (1) the design of the subject imageencoder significantly impacts identity preservation quality, and (2) separatingtext and subject guidance is crucial for both text alignment and identitypreservation. Building on these insights, we introduce a new approach calledEZIGen, which employs two main strategies: a carefully crafted subject imageEncoder based on the pretrained UNet of the Stable Diffusion model to ensurehigh-quality identity transfer, following a process that decouples the guidancestages and iteratively refines the initial image layout. Through thesestrategies, EZIGen achieves state-of-the-art results on multiple subject-drivenbenchmarks with a unified model and 100 times less training data. The demo pageis available at: https://zichengduan.github.io/pages/EZIGen/index.html.</description><author>Zicheng Duan, Yuxuan Ding, Chenhui Gou, Ziqin Zhou, Ethan Smith, Lingqiao Liu</author><pubDate>Tue, 01 Oct 2024 17:52:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08091v2</guid></item><item><title>Characterizing Online Toxicity During the 2022 Mpox Outbreak: A Computational Analysis of Topical and Network Dynamics</title><link>http://arxiv.org/abs/2408.11962v3</link><description>Background: Online toxicity, encompassing behaviors such as harassment,bullying, hate speech, and the dissemination of misinformation, has become apressing social concern in the digital age. The 2022 Mpox outbreak, initiallytermed "Monkeypox" but subsequently renamed to mitigate associated stigmas andsocietal concerns, serves as a poignant backdrop to this issue. Objective: Inthis research, we undertake a comprehensive analysis of the toxic onlinediscourse surrounding the 2022 Mpox outbreak. Our objective is to dissect itsorigins, characterize its nature and content, trace its dissemination patterns,and assess its broader societal implications, with the goal of providinginsights that can inform strategies to mitigate such toxicity in future crises.Methods: We collected more than 1.6 million unique tweets and analyzed themfrom five dimensions, including context, extent, content, speaker, and intent.Utilizing BERT-based topic modeling and social network community clustering, wedelineated the toxic dynamics on Twitter. Results: We identified fivehigh-level topic categories in the toxic online discourse on Twitter, includingdisease (46.6%), health policy and healthcare (19.3%), homophobia (23.9%),politics (6.0%), and racism (4.1%). Through the toxicity diffusion networks ofmentions, retweets, and the top users, we found that retweets of toxic contentwere widespread, while influential users rarely engaged with or countered thistoxicity through retweets. Conclusions: By tracking topical dynamics, we cantrack the changing popularity of toxic content online, providing a betterunderstanding of societal challenges. Network dynamics spotlight key socialmedia influencers and their intents, indicating that addressing these centralfigures in toxic discourse can enhance crisis communication and informpolicy-making.</description><author>Lizhou Fan, Lingyao Li, Libby Hemphill</author><pubDate>Tue, 01 Oct 2024 17:50:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11962v3</guid></item><item><title>Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning</title><link>http://arxiv.org/abs/2409.12618v2</link><description>Iterative human engagement is a common and effective means of leveraging theadvanced language processing power of large language models (LLMs). Usingwell-structured prompts in a conversational manner, human users can effectivelyinfluence an LLM to develop more thoughtful and accurate responses. Motivatedby this insight, we propose the Iteration of Thought (IoT) framework forenhancing LLM responses by generating "thought"-provoking prompts vis a vis aninput query and the current iteration of an LLM's response. Unlike static orsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),IoT adapts its reasoning path dynamically, based on evolving context, andwithout generating alternate explorative thoughts which are ultimatelydiscarded. The three components of the IoT framework are (1) an Inner DialogueAgent (IDA) responsible for generating instructive, context-specific prompts;(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;and (3) an iterative prompting loop that implements a conversation between theformer two components. We introduce two variants of our framework: AutonomousIteration of Thought (AIoT), where an LLM decides when to stop iterating, andGuided Iteration of Thought (GIoT), which always forces a fixed numberiterations. We investigate the performance of IoT across various datasets,spanning complex reasoning tasks from the GPQA dataset, explorativeproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hopquestion answering from the HotpotQA dataset. Our results show that IoTrepresents a viable paradigm for autonomous response refinement in LLMs,showcasing significant improvements over CoT and thereby enabling more adaptiveand efficient reasoning systems that minimize human intervention.</description><author>Santosh Kumar Radha, Yasamin Nouri Jelyani, Ara Ghukasyan, Oktay Goktas</author><pubDate>Tue, 01 Oct 2024 17:50:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12618v2</guid></item><item><title>AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow</title><link>http://arxiv.org/abs/2409.18924v2</link><description>Simulated patient systems play a crucial role in modern medical education andresearch, providing safe, integrative learning environments and enablingclinical decision-making simulations. Large Language Models (LLM) could advancesimulated patient systems by replicating medical conditions and patient-doctorinteractions with high fidelity and low cost. However, ensuring theeffectiveness and trustworthiness of these systems remains a challenge, as theyrequire a large, diverse, and precise patient knowledgebase, along with arobust and stable knowledge diffusion to users. Here, we developed AIPatient,an advanced simulated patient system with AIPatient Knowledge Graph (AIPatientKG) as the input and the Reasoning Retrieval-Augmented Generation (ReasoningRAG) agentic workflow as the generation backbone. AIPatient KG samples datafrom Electronic Health Records (EHRs) in the Medical Information Mart forIntensive Care (MIMIC)-III database, producing a clinically diverse andrelevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).Reasoning RAG leverages six LLM powered agents spanning tasks includingretrieval, KG query generation, abstraction, checker, rewrite, andsummarization. This agentic framework reaches an overall accuracy of 94.15% inEHR-based medical Question Answering (QA), outperforming benchmarks that useeither no agent or only partial agent integration. Our system also presentshigh readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade5.6), robustness (ANOVA F-value 0.6126, p&gt;0.1), and stability (ANOVA F-value0.782, p&gt;0.1). The promising performance of the AIPatient system highlights itspotential to support a wide range of applications, including medical education,model evaluation, and system integration.</description><author>Huizi Yu, Jiayan Zhou, Lingyao Li, Shan Chen, Jack Gallifant, Anye Shi, Xiang Li, Wenyue Hua, Mingyu Jin, Guang Chen, Yang Zhou, Zhao Li, Trisha Gupte, Ming-Li Chen, Zahra Azizi, Yongfeng Zhang, Themistocles L. Assimes, Xin Ma, Danielle S. Bitterman, Lin Lu, Lizhou Fan</author><pubDate>Tue, 01 Oct 2024 17:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18924v2</guid></item><item><title>FLRT: Fluent Student-Teacher Redteaming</title><link>http://arxiv.org/abs/2407.17447v2</link><description>Many publicly available language models have been safety tuned to reduce thelikelihood of toxic or liability-inducing text. To redteam or jailbreak thesemodels for compliance with toxic requests, users and security analysts havedeveloped adversarial prompting techniques. One attack method is to applydiscrete optimization techniques to the prompt. However, the resulting attackstrings are often gibberish text, easily filtered by defenders due to highmeasured perplexity, and may fail for unseen tasks and/or well-tuned models. Inthis work, we improve existing algorithms (primarily GCG and BEAST) to developpowerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Ourtechnique centers around a new distillation-based approach that encourages thevictim model to emulate a toxified finetune, either in terms of outputprobabilities or internal activations. To encourage human-fluent attacks, weadd a multi-model perplexity penalty and a repetition penalty to the objective.We also enhance optimizer strength by allowing token insertions, token swaps,and token deletions and by using longer attack sequences. The resulting processis able to reliably jailbreak the most difficult target models with promptsthat appear similar to human-written prompts. On Advbench we achieve attacksuccess rates $&gt;93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, whilemaintaining model-measured perplexity $&lt;33$; we achieve $95$% attack successfor Phi-3, though with higher perplexity. We also find a universally-optimizedsingle fluent prompt that induces $&gt;88$% compliance on previously unseen tasksacross Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-boxmodels.</description><author>T. Ben Thompson, Michael Sklar</author><pubDate>Tue, 01 Oct 2024 17:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17447v2</guid></item><item><title>Measuring and Mitigating Bias for Tabular Datasets with Multiple Protected Attributes</title><link>http://arxiv.org/abs/2405.19300v3</link><description>Motivated by the recital (67) of the current corrigendum of the AI Act in theEuropean Union, we propose and present measures and mitigation strategies fordiscrimination in tabular datasets. We specifically focus on datasets thatcontain multiple protected attributes, such as nationality, age, and sex. Thismakes measuring and mitigating bias more challenging, as many existing methodsare designed for a single protected attribute. This paper comes with a twofoldcontribution: Firstly, new discrimination measures are introduced. Thesemeasures are categorized in our framework along with existing ones, guidingresearchers and practitioners in choosing the right measure to assess thefairness of the underlying dataset. Secondly, a novel application of anexisting bias mitigation method, FairDo, is presented. We show that thisstrategy can mitigate any type of discrimination, including intersectionaldiscrimination, by transforming the dataset. By conducting experiments onreal-world datasets (Adult, Bank, COMPAS), we demonstrate that de-biasingdatasets with multiple protected attributes is possible. All transformeddatasets show a reduction in discrimination, on average by 28%. Further, thesedatasets do not compromise any of the tested machine learning models'performances significantly compared to the original datasets. Conclusively,this study demonstrates the effectiveness of the mitigation strategy used andcontributes to the ongoing discussion on the implementation of the EuropeanUnion's AI Act.</description><author>Manh Khoi Duong, Stefan Conrad</author><pubDate>Tue, 01 Oct 2024 17:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19300v3</guid></item><item><title>Paths to Equilibrium in Games</title><link>http://arxiv.org/abs/2403.18079v2</link><description>In multi-agent reinforcement learning (MARL) and game theory, agentsrepeatedly interact and revise their strategies as new data arrives, producinga sequence of strategy profiles. This paper studies sequences of strategiessatisfying a pairwise constraint inspired by policy updating in reinforcementlearning, where an agent who is best responding in one period does not switchits strategy in the next period. This constraint merely requires thatoptimizing agents do not switch strategies, but does not constrain thenon-optimizing agents in any way, and thus allows for exploration. Sequenceswith this property are called satisficing paths, and arise naturally in manyMARL algorithms. A fundamental question about strategic dynamics is such: for agiven game and initial strategy profile, is it always possible to construct asatisficing path that terminates at an equilibrium? The resolution of thisquestion has implications about the capabilities or limitations of a class ofMARL algorithms. We answer this question in the affirmative for normal-formgames. Our analysis reveals a counterintuitive insight that rewarddeteriorating strategic updates are key to driving play to equilibrium along asatisficing path.</description><author>Bora Yongacoglu, Gürdal Arslan, Lacra Pavel, Serdar Yüksel</author><pubDate>Tue, 01 Oct 2024 17:33:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18079v2</guid></item><item><title>Generative models of MRI-derived neuroimaging features and associated dataset of 18,000 samples</title><link>http://arxiv.org/abs/2407.12897v2</link><description>Availability of large and diverse medical datasets is often challenged byprivacy and data sharing restrictions. For successful application of machinelearning techniques for disease diagnosis, prognosis, and precision medicine,large amounts of data are necessary for model building and optimization. Tohelp overcome such limitations in the context of brain MRI, we present GenMIND:a collection of generative models of normative regional volumetric featuresderived from structural brain imaging. GenMIND models are trained on real brainimaging regional volumetric measures from the iSTAGING consortium, whichencompasses over 40,000 MRI scans across 13 studies, incorporating covariatessuch as age, sex, and race. Leveraging GenMIND, we produce and offer 18,000synthetic samples spanning the adult lifespan (ages 22-90 years), alongside themodel's capability to generate unlimited data. Experimental results indicatethat samples generated from GenMIND agree with the distributions obtained fromreal data. Most importantly, the generated normative data significantly enhancethe accuracy of downstream machine learning models on tasks such as diseaseclassification. Data and models are available at:https://huggingface.co/spaces/rongguangw/GenMIND.</description><author>Sai Spandana Chintapalli, Rongguang Wang, Zhijian Yang, Vasiliki Tassopoulou, Fanyang Yu, Vishnu Bashyam, Guray Erus, Pratik Chaudhari, Haochang Shou, Christos Davatzikos</author><pubDate>Tue, 01 Oct 2024 17:26:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12897v2</guid></item><item><title>Conversational Complexity for Assessing Risk in Large Language Models</title><link>http://arxiv.org/abs/2409.01247v2</link><description>Large Language Models (LLMs) present a dual-use dilemma: they enablebeneficial applications while harboring potential for harm, particularlythrough conversational interactions. Despite various safeguards, advanced LLMsremain vulnerable. A watershed case was Kevin Roose's notable conversation withBing, which elicited harmful outputs after extended interaction. This contrastswith simpler early jailbreaks that produced similar content more easily,raising the question: How much conversational effort is needed to elicitharmful information from LLMs? We propose two measures: Conversational Length(CL), which quantifies the conversation length used to obtain a specificresponse, and Conversational Complexity (CC), defined as the Kolmogorovcomplexity of the user's instruction sequence leading to the response. Toaddress the incomputability of Kolmogorov complexity, we approximate CC using areference LLM to estimate the compressibility of user instructions. Applyingthis approach to a large red-teaming dataset, we perform a quantitativeanalysis examining the statistical distribution of harmful and harmlessconversational lengths and complexities. Our empirical findings suggest thatthis distributional analysis and the minimisation of CC serve as valuable toolsfor understanding AI safety, offering insights into the accessibility ofharmful information. This work establishes a foundation for a new perspectiveon LLM safety, centered around the algorithmic complexity of pathways to harm.</description><author>John Burden, Manuel Cebrian, Jose Hernandez-Orallo</author><pubDate>Tue, 01 Oct 2024 17:21:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01247v2</guid></item><item><title>Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis</title><link>http://arxiv.org/abs/2407.12857v2</link><description>In recent years, the rapid increase in scientific papers has overwhelmedtraditional review mechanisms, resulting in varying quality of publications.Although existing methods have explored the capabilities of Large LanguageModels (LLMs) for automated scientific reviewing, their generated contents areoften generic or partial. To address the issues above, we introduce anautomated paper reviewing framework SEA. It comprises of three modules:Standardization, Evaluation, and Analysis, which are represented by modelsSEA-S, SEA-E, and SEA-A, respectively. Initially, SEA-S distills datastandardization capabilities of GPT-4 for integrating multiple reviews for apaper. Then, SEA-E utilizes standardized data for fine-tuning, enabling it togenerate constructive reviews. Finally, SEA-A introduces a new evaluationmetric called mismatch score to assess the consistency between paper contentsand reviews. Moreover, we design a self-correction strategy to enhance theconsistency. Extensive experimental results on datasets collected from eightvenues show that SEA can generate valuable insights for authors to improvetheir papers.</description><author>Jianxiang Yu, Zichen Ding, Jiaqi Tan, Kangyang Luo, Zhenmin Weng, Chenghua Gong, Long Zeng, Renjing Cui, Chengcheng Han, Qiushi Sun, Zhiyong Wu, Yunshi Lan, Xiang Li</author><pubDate>Tue, 01 Oct 2024 17:13:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12857v2</guid></item><item><title>Generative Expansion of Small Datasets: An Expansive Graph Approach</title><link>http://arxiv.org/abs/2406.17238v2</link><description>Limited data availability in machine learning significantly impactsperformance and generalization. Traditional augmentation methods enhancemoderately sufficient datasets. GANs struggle with convergence when generatingdiverse samples. Diffusion models, while effective, have high computationalcosts. We introduce an Expansive Synthesis model generating large-scale,information-rich datasets from minimal samples. It uses expander graph mappingsand feature interpolation to preserve data distribution and featurerelationships. The model leverages neural networks' non-linear latent space,captured by a Koopman operator, to create a linear feature space for datasetexpansion. An autoencoder with self-attention layers and optimal transportrefines distributional consistency. We validate by comparing classifierstrained on generated data to those trained on original datasets. Results showcomparable performance, demonstrating the model's potential to augment trainingdata effectively. This work advances data generation, addressing scarcity inmachine learning applications.</description><author>Vahid Jebraeeli, Bo Jiang, Hamid Krim, Derya Cansever</author><pubDate>Tue, 01 Oct 2024 17:12:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17238v2</guid></item><item><title>Inference Optimization of Foundation Models on AI Accelerators</title><link>http://arxiv.org/abs/2407.09111v2</link><description>Powerful foundation models, including large language models (LLMs), withTransformer architectures have ushered in a new era of Generative AI acrossvarious industries. Industry and research community have witnessed a largenumber of new applications, based on those foundation models. Such applicationsinclude question and answer, customer services, image and video generation, andcode completions, among others. However, as the number of model parametersreaches to hundreds of billions, their deployment incurs prohibitive inferencecosts and high latency in real-world scenarios. As a result, the demand forcost-effective and fast inference using AI accelerators is ever more higher. Tothis end, our tutorial offers a comprehensive discussion on complementaryinference optimization techniques using AI accelerators. Beginning with anoverview of basic Transformer architectures and deep learning systemframeworks, we deep dive into system optimization techniques for fast andmemory-efficient attention computations and discuss how they can be implementedefficiently on AI accelerators. Next, we describe architectural elements thatare key for fast transformer inference. Finally, we examine various modelcompression and fast decoding strategies in the same context.</description><author>Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas Kübler, Jiaji Huang, Matthäus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, George Karypis</author><pubDate>Tue, 01 Oct 2024 17:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09111v2</guid></item><item><title>AXIAL: Attention-based eXplainability for Interpretable Alzheimer's Localized Diagnosis using 2D CNNs on 3D MRI brain scans</title><link>http://arxiv.org/abs/2407.02418v2</link><description>This study presents an innovative method for Alzheimer's disease diagnosisusing 3D MRI designed to enhance the explainability of model decisions. Ourapproach adopts a soft attention mechanism, enabling 2D CNNs to extractvolumetric representations. At the same time, the importance of each slice indecision-making is learned, allowing the generation of a voxel-level attentionmap to produce an explainable MRI. To test our method and ensure thereproducibility of our results, we chose a standardized collection of MRI datafrom the Alzheimer's Disease Neuroimaging Initiative (ADNI). On this dataset,our method significantly outperforms state-of-the-art methods in (i)distinguishing AD from cognitive normal (CN) with an accuracy of 0.856 andMatthew's correlation coefficient (MCC) of 0.712, representing improvements of2.4% and 5.3% respectively over the second-best, and (ii) in the prognostictask of discerning stable from progressive mild cognitive impairment (MCI) withan accuracy of 0.725 and MCC of 0.443, showing improvements of 10.2% and 20.5%respectively over the second-best. We achieved this prognostic result byadopting a double transfer learning strategy, which enhanced sensitivity tomorphological changes and facilitated early-stage AD detection. Withvoxel-level precision, our method identified which specific areas are beingpaid attention to, identifying these predominant brain regions: thehippocampus, the amygdala, the parahippocampal, and the inferior lateralventricles. All these areas are clinically associated with AD development.Furthermore, our approach consistently found the same AD-related areas acrossdifferent cross-validation folds, proving its robustness and precision inhighlighting areas that align closely with known pathological markers of thedisease.</description><author>Gabriele Lozupone, Alessandro Bria, Francesco Fontanella, Frederick J. A. Meijer, Claudio De Stefano</author><pubDate>Tue, 01 Oct 2024 17:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.02418v2</guid></item><item><title>MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model</title><link>http://arxiv.org/abs/2406.11193v2</link><description>Projecting visual features into word embedding space has become a significantfusion strategy adopted by Multimodal Large Language Models (MLLMs). However,its internal mechanisms have yet to be explored. Inspired by multilingualresearch, we identify domain-specific neurons in multimodal large languagemodels. Specifically, we investigate the distribution of domain-specificneurons and the mechanism of how MLLMs process features from diverse domains.Furthermore, we propose a three-stage mechanism for language model modules inMLLMs when handling projected image features, and verify this hypothesis usinglogit lens. Extensive experiments indicate that while current MLLMs exhibitVisual Question Answering (VQA) capability, they may not fully utilizedomain-specific information. Manipulating domain-specific neurons properly willresult in a 10% change of accuracy at most, shedding light on the developmentof cross-domain, all-encompassing MLLMs in the future. The source code isavailable at https://github.com/Z1zs/MMNeuron.</description><author>Jiahao Huo, Yibo Yan, Boren Hu, Yutao Yue, Xuming Hu</author><pubDate>Tue, 01 Oct 2024 17:04:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11193v2</guid></item><item><title>Multi-RoI Human Mesh Recovery with Camera Consistency and Contrastive Losses</title><link>http://arxiv.org/abs/2402.02074v2</link><description>Besides a 3D mesh, Human Mesh Recovery (HMR) methods usually need to estimatea camera for computing 2D reprojection loss. Previous approaches may encounterthe following problem: both the mesh and camera are not correct but thecombination of them can yield a low reprojection loss. To alleviate thisproblem, we define multiple RoIs (region of interest) containing the same humanand propose a multiple-RoI-based HMR method. Our key idea is that with multipleRoIs as input, we can estimate multiple local cameras and have the opportunityto design and apply additional constraints between cameras to improve theaccuracy of the cameras and, in turn, the accuracy of the corresponding 3Dmesh. To implement this idea, we propose a RoI-aware feature fusion network bywhich we estimate a 3D mesh shared by all RoIs as well as local camerascorresponding to the RoIs. We observe that local cameras can be converted tothe camera of the full image through which we construct a local cameraconsistency loss as the additional constraint imposed on local cameras. Anotherbenefit of introducing multiple RoIs is that we can encapsulate our networkinto a contrastive learning framework and apply a contrastive loss toregularize the training of our network. Experiments demonstrate theeffectiveness of our multi-RoI HMR method and superiority to recent prior arts.Our code is available at https://github.com/CptDiaos/Multi-RoI.</description><author>Yongwei Nie, Changzhen Liu, Chengjiang Long, Qing Zhang, Guiqing Li, Hongmin Cai</author><pubDate>Tue, 01 Oct 2024 16:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.02074v2</guid></item><item><title>Automated Segmentation and Analysis of Microscopy Images of Laser Powder Bed Fusion Melt Tracks</title><link>http://arxiv.org/abs/2409.18326v2</link><description>With the increasing adoption of metal additive manufacturing (AM),researchers and practitioners are turning to data-driven approaches to optimiseprinting conditions. Cross-sectional images of melt tracks provide valuableinformation for tuning process parameters, developing parameter scaling data,and identifying defects. Here we present an image segmentation neural networkthat automatically identifies and measures melt track dimensions from across-section image. We use a U-Net architecture to train on a data set of 62pre-labelled images obtained from different labs, machines, and materialscoupled with image augmentation. When neural network hyperparameters such asbatch size and learning rate are properly tuned, the learned model shows anaccuracy for classification of over 99% and an F1 score over 90%. The neuralnetwork exhibits robustness when tested on images captured by various users,printed on different machines, and acquired using different microscopes. Apost-processing module extracts the height and width of the melt pool, and thewetting angles. We discuss opportunities to improve model performance andavenues for transfer learning, such as extension to other AM processes such asdirected energy deposition.</description><author>Aagam Shah, Reimar Weissbach, David A. Griggs, A. John Hart, Elif Ertekin, Sameh Tawfick</author><pubDate>Tue, 01 Oct 2024 16:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18326v2</guid></item><item><title>Dual-Space Knowledge Distillation for Large Language Models</title><link>http://arxiv.org/abs/2406.17328v3</link><description>Knowledge distillation (KD) is known as a promising solution to compresslarge language models (LLMs) via transferring their knowledge to smallermodels. During this process, white-box KD methods usually minimize the distancebetween the output distributions of the two models so that more knowledge canbe transferred. However, in the current white-box KD framework, the outputdistributions are from the respective output spaces of the two models, usingtheir own prediction heads. We argue that the space discrepancy will lead tolow similarity between the teacher model and the student model on bothrepresentation and distribution levels. Furthermore, this discrepancy alsohinders the KD process between models with different vocabularies, which iscommon for current LLMs. To address these issues, we propose a dual-spaceknowledge distillation (DSKD) framework that unifies the output spaces of thetwo models for KD. On the basis of DSKD, we further develop a cross-modelattention mechanism, which can automatically align the representations of thetwo models with different vocabularies. Thus, our framework is not onlycompatible with various distance functions for KD (e.g., KL divergence) likethe current framework, but also supports KD between any two LLMs regardless oftheir vocabularies. Experiments on task-agnostic instruction-followingbenchmarks show that DSKD significantly outperforms the current white-box KDframework with various distance functions, and also surpasses existing KDmethods for LLMs with different vocabularies.</description><author>Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng Chen, Jinan Xu</author><pubDate>Tue, 01 Oct 2024 16:45:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17328v3</guid></item><item><title>Optimizing Synthetic Data for Enhanced Pancreatic Tumor Segmentation</title><link>http://arxiv.org/abs/2407.19284v2</link><description>Pancreatic cancer remains one of the leading causes of cancer-relatedmortality worldwide. Precise segmentation of pancreatic tumors from medicalimages is a bottleneck for effective clinical decision-making. However,achieving a high accuracy is often limited by the small size and availabilityof real patient data for training deep learning models. Recent approaches haveemployed synthetic data generation to augment training datasets. Whilepromising, these methods may not yet meet the performance benchmarks requiredfor real-world clinical use. This study critically evaluates the limitations ofexisting generative-AI based frameworks for pancreatic tumor segmentation. Weconduct a series of experiments to investigate the impact of synthetic\textit{tumor size} and \textit{boundary definition} precision on modelperformance. Our findings demonstrate that: (1) strategically selecting acombination of synthetic tumor sizes is crucial for optimal segmentationoutcomes, and (2) generating synthetic tumors with precise boundariessignificantly improves model accuracy. These insights highlight the importanceof utilizing refined synthetic data augmentation for enhancing the clinicalutility of segmentation models in pancreatic cancer decision making includingdiagnosis, prognosis, and treatment plans. Our code will be available athttps://github.com/lkpengcs/SynTumorAnalyzer.</description><author>Linkai Peng, Zheyuan Zhang, Gorkem Durak, Frank H. Miller, Alpay Medetalibeyoglu, Michael B. Wallace, Ulas Bagci</author><pubDate>Tue, 01 Oct 2024 16:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19284v2</guid></item><item><title>FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model</title><link>http://arxiv.org/abs/2402.12376v2</link><description>Nature is infinitely resolution-free. In the context of this reality,existing diffusion models, such as Diffusion Transformers, often facechallenges when processing image resolutions outside of their trained domain.To address this limitation, we conceptualize images as sequences of tokens withdynamic sizes, rather than traditional methods that perceive images asfixed-resolution grids. This perspective enables a flexible training strategythat seamlessly accommodates various aspect ratios during both training andinference, thus promoting resolution generalization and eliminating biasesintroduced by image cropping. On this basis, we present the Flexible VisionTransformer (FiT), a transformer architecture specifically designed forgenerating images with unrestricted resolutions and aspect ratios. We furtherupgrade the FiT to FiTv2 with several innovative designs, includingtheQuery-Key vector normalization, the AdaLN-LoRA module, a rectified flowscheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjustednetwork structure, FiTv2 exhibits 2x convergence speed of FiT. Whenincorporating advanced training-free extrapolation techniques, FiTv2demonstrates remarkable adaptability in both resolution extrapolation anddiverse resolution generation. Additionally, our exploration of the scalabilityof the FiTv2 model reveals that larger models exhibit better computationalefficiency. Furthermore, we introduce an efficient post-training strategy toadapt a pre-trained model for the high-resolution generation. Comprehensiveexperiments demonstrate the exceptional performance of FiTv2 across a broadrange of resolutions. We have released all the codes and models athttps://github.com/whlzy/FiT to promote the exploration of diffusiontransformer models for arbitrary-resolution image generation.</description><author>Zidong Wang, Zeyu Lu, Di Huang, Cai Zhou, Wanli Ouyang, Lei Bai</author><pubDate>Tue, 01 Oct 2024 16:38:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12376v2</guid></item><item><title>What is the Role of Large Language Models in the Evolution of Astronomy Research?</title><link>http://arxiv.org/abs/2409.20252v2</link><description>ChatGPT and other state-of-the-art large language models (LLMs) are rapidlytransforming multiple fields, offering powerful tools for a wide range ofapplications. These models, commonly trained on vast datasets, exhibithuman-like text generation capabilities, making them useful for research taskssuch as ideation, literature review, coding, drafting, and outreach. Weconducted a study involving 13 astronomers at different career stages andresearch fields to explore LLM applications across diverse tasks over severalmonths and to evaluate their performance in research-related activities. Thiswork was accompanied by an anonymous survey assessing participants' experiencesand attitudes towards LLMs. We provide a detailed analysis of the tasksattempted and the survey answers, along with specific output examples. Ourfindings highlight both the potential and limitations of LLMs in supportingresearch while also addressing general and research-specific ethicalconsiderations. We conclude with a series of recommendations, emphasizing theneed for researchers to complement LLMs with critical thinking and domainexpertise, ensuring these tools serve as aids rather than substitutes forrigorous scientific inquiry.</description><author>Morgan Fouesneau, Ivelina G. Momcheva, Urmila Chadayammuri, Mariia Demianenko, Antoine Dumont, Raphael E. Hviding, K. Angelique Kahle, Nadiia Pulatova, Bhavesh Rajpoot, Marten B. Scheuck, Rhys Seeburger, Dmitry Semenov, Jaime I. Villaseñor</author><pubDate>Tue, 01 Oct 2024 16:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20252v2</guid></item><item><title>Dynamic Pricing in Securities Lending Market: Application in Revenue Optimization for an Agent Lender Portfolio</title><link>http://arxiv.org/abs/2407.13687v3</link><description>Securities lending is an important part of the financial market structure,where agent lenders help long term institutional investors to lend out theirsecurities to short sellers in exchange for a lending fee. Agent lenders withinthe market seek to optimize revenue by lending out securities at the highestrate possible. Typically, this rate is set by hard-coded business rules orstandard supervised machine learning models. These approaches are oftendifficult to scale and are not adaptive to changing market conditions. Unlike atraditional stock exchange with a centralized limit order book, the securitieslending market is organized similarly to an e-commerce marketplace, where agentlenders and borrowers can transact at any agreed price in a bilateral fashion.This similarity suggests that the use of typical methods for addressing dynamicpricing problems in e-commerce could be effective in the securities lendingmarket. We show that existing contextual bandit frameworks can be successfullyutilized in the securities lending market. Using offline evaluation on realhistorical data, we show that the contextual bandit approach can consistentlyoutperform typical approaches by at least 15% in terms of total revenuegenerated.</description><author>Jing Xu, Yung-Cheng Hsu, William Biscarri</author><pubDate>Tue, 01 Oct 2024 16:33:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13687v3</guid></item><item><title>Does Vision Accelerate Hierarchical Generalization in Neural Language Learners?</title><link>http://arxiv.org/abs/2302.00667v2</link><description>Neural language models (LMs) are arguably less data-efficient than humansfrom a language acquisition perspective. One fundamental question is why thishuman-LM gap arises. This study explores the advantage of grounded languageacquisition, specifically the impact of visual information -- which humans canusually rely on but LMs largely do not have access to during languageacquisition -- on syntactic generalization in LMs. Our experiments, followingthe poverty of stimulus paradigm under two scenarios (using artificial vs.naturalistic images), demonstrate that if the alignments between the linguisticand visual components are clear in the input, access to vision data does helpwith the syntactic generalization of LMs, but if not, visual input does nothelp. This highlights the need for additional biases or signals, such as mutualgaze, to enhance cross-modal alignment and enable efficient syntacticgeneralization in multimodal LMs.</description><author>Tatsuki Kuribayashi, Timothy Baldwin</author><pubDate>Tue, 01 Oct 2024 16:29:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00667v2</guid></item><item><title>Camera Height Doesn't Change: Unsupervised Training for Metric Monocular Road-Scene Depth Estimation</title><link>http://arxiv.org/abs/2312.04530v3</link><description>In this paper, we introduce a novel training method for making any monoculardepth network learn absolute scale and estimate metric road-scene depth justfrom regular training data, i.e., driving videos. We refer to this trainingframework as FUMET. The key idea is to leverage cars found on the road assources of scale supervision and to incorporate them in network trainingrobustly. FUMET detects and estimates the sizes of cars in a frame andaggregates scale information extracted from them into an estimate of the cameraheight whose consistency across the entire video sequence is enforced as scalesupervision. This realizes robust unsupervised training of any, otherwisescale-oblivious, monocular depth network so that they become not onlyscale-aware but also metric-accurate without the need for auxiliary sensors andextra supervision. Extensive experiments on the KITTI and the Cityscapesdatasets show the effectiveness of FUMET, which achieves state-of-the-artaccuracy. We also show that FUMET enables training on mixed datasets ofdifferent camera heights, which leads to larger-scale training and bettergeneralization. Metric depth reconstruction is essential in any road-scenevisual modeling, and FUMET democratizes its deployment by establishing themeans to convert any model into a metric depth estimator.</description><author>Genki Kinoshita, Ko Nishino</author><pubDate>Tue, 01 Oct 2024 16:12:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04530v3</guid></item><item><title>Multi-Robot Informative Path Planning for Efficient Target Mapping using Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2409.16967v2</link><description>Autonomous robots are being employed in several mapping and data collectiontasks due to their efficiency and low labor costs. In these tasks, the robotsare required to map targets-of-interest in an unknown environment whileconstrained to a given resource budget such as path length or mission time.This is a challenging problem as each robot has to not only detect and avoidcollisions from static obstacles in the environment but also has to model otherrobots' trajectories to avoid inter-robot collisions. We propose a novel deepreinforcement learning approach for multi-robot informative path planning tomap targets-of-interest in an unknown 3D environment. A key aspect of ourapproach is an augmented graph that models other robots' trajectories to enableplanning for communication and inter-robot collision avoidance. We train ourdecentralized reinforcement learning policy via the centralized training anddecentralized execution paradigm. Once trained, our policy is also scalable tovarying number of robots and does not require re-training. Our approachoutperforms other state-of-the-art multi-robot target mapping approaches by33.75% in terms of the number of discovered targets-of-interest. We open-sourceour code and model at: https://github.com/AccGen99/marl_ipp</description><author>Apoorva Vashisth, Dipam Patel, Damon Conover, Aniket Bera</author><pubDate>Tue, 01 Oct 2024 16:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16967v2</guid></item><item><title>Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2311.09852v7</link><description>Swarms of autonomous interactive drones can provide compelling sensingcapabilities in Smart City applications, such as traffic monitoring. This paperfocuses on the task assignment problem for large-scale spatio-temporal sensingby a drone swarm. However, existing approaches have distinct challenges:distributed evolutionary optimization, such as collective learning, lackslong-term adaptability in dynamic environments, while deep reinforcementlearning (DRL) is limited to scale effectively due to the curse ofdimensionality. Therefore, this paper proposes a novel synergetic optimizationapproach by integrating long-term DRL and short-term collective learning.Through this approach, each drone independently and proactively determines itsflying direction and recharging location using DRL, while evolving theirnavigation and sensing policies through collective learning based on astructured tree communication model. Extensive experiments with datasetsgenerated from realistic urban mobility demonstrate an outstanding performanceof the proposed solution in complex scenarios. New insights show that thisapproach provides a win-win synthesis of short-term and long-term strategiesfor drone-based traffic monitoring, with short-term methods addressing trainingcomplexity and energy management, while long-term methods preserving highsensing performance.</description><author>Chuhao Qin, Evangelos Pournaras</author><pubDate>Tue, 01 Oct 2024 16:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09852v7</guid></item><item><title>Clustering Three-Way Data with Outliers</title><link>http://arxiv.org/abs/2310.05288v3</link><description>Matrix-variate distributions are a recent addition to the model-basedclustering field, thereby making it possible to analyze data in matrix formwith complex structure such as images and time series. Due to its recentappearance, there is limited literature on matrix-variate data, with even lesson dealing with outliers in these models. An approach for clusteringmatrix-variate normal data with outliers is discussed. The approach, which usesthe distribution of subset log-likelihoods, extends the OCLUST algorithm tomatrix-variate normal data and uses an iterative approach to detect and trimoutliers.</description><author>Katharine M. Clark, Paul D. McNicholas</author><pubDate>Tue, 01 Oct 2024 16:08:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05288v3</guid></item><item><title>Tight Lower Bounds under Asymmetric High-Order Hölder Smoothness and Uniform Convexity</title><link>http://arxiv.org/abs/2409.10773v2</link><description>In this paper, we provide tight lower bounds for the oracle complexity ofminimizing high-order H\"older smooth and uniformly convex functions.Specifically, for a function whose $p^{th}$-order derivatives are H\"oldercontinuous with degree $\nu$ and parameter $H$, and that is uniformly convexwith degree $q$ and parameter $\sigma$, we focus on two asymmetric cases: (1)$q &gt; p + \nu$, and (2) $q &lt; p+\nu$. Given up to $p^{th}$-order oracle access,we establish worst-case oracle complexities of $\Omega\left( \left(\frac{H}{\sigma}\right)^\frac{2}{3(p+\nu)-2}\left(\frac{\sigma}{\epsilon}\right)^\frac{2(q-p-\nu)}{q(3(p+\nu)-2)}\right)$ in thefirst case with an $\ell_\infty$-ball-truncated-Gaussian smoothed hard functionand $\Omega\left(\left(\frac{H}{\sigma}\right)^\frac{2}{3(p+\nu)-2}+\log^2\left(\frac{\sigma^{p+\nu}}{H^q}\right)^\frac{1}{p+\nu-q}\right)$ in thesecond case, for reaching an $\epsilon$-approximate solution in terms of theoptimality gap. Our analysis generalizes previous lower bounds for functionsunder first- and second-order smoothness as well as those for uniformly convexfunctions, and furthermore our results match the corresponding upper bounds inthe general setting.</description><author>Site Bai, Brian Bullins</author><pubDate>Tue, 01 Oct 2024 15:57:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10773v2</guid></item><item><title>Mitigating Shortcut Learning with Diffusion Counterfactuals and Diverse Ensembles</title><link>http://arxiv.org/abs/2311.16176v4</link><description>Spurious correlations in the data, where multiple cues are predictive of thetarget labels, often lead to a phenomenon known as shortcut learning, where amodel relies on erroneous, easy-to-learn cues while ignoring reliable ones. Inthis work, we propose DiffDiv an ensemble diversification framework exploitingDiffusion Probabilistic Models (DPMs) to mitigate this form of bias. We showthat at particular training intervals, DPMs can generate images with novelfeature combinations, even when trained on samples displaying correlated inputfeatures. We leverage this crucial property to generate syntheticcounterfactuals to increase model diversity via ensemble disagreement. We showthat DPM-guided diversification is sufficient to remove dependence on shortcutcues, without a need for additional supervised signals. We further empiricallyquantify its efficacy on several diversification objectives, and finally showimproved generalization and diversification on par with prior work that relieson auxiliary data collection.</description><author>Luca Scimeca, Alexander Rubinstein, Damien Teney, Seong Joon Oh, Armand Mihai Nicolicioiu, Yoshua Bengio</author><pubDate>Tue, 01 Oct 2024 15:50:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16176v4</guid></item><item><title>Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models</title><link>http://arxiv.org/abs/2407.06917v2</link><description>Large language models (LLMs) have been shown to propagate and amplify harmfulstereotypes, particularly those that disproportionately affect marginalisedcommunities. To understand the effect of these stereotypes morecomprehensively, we introduce GlobalBias, a dataset of 876k sentencesincorporating 40 distinct gender-by-ethnicity groups alongside descriptorstypically used in bias literature, which enables us to study a broad set ofstereotypes from around the world. We use GlobalBias to directly probe a suiteof LMs via perplexity, which we use as a proxy to determine how certainstereotypes are represented in the model's internal representations. Followingthis, we generate character profiles based on given names and evaluate theprevalence of stereotypes in model outputs. We find that the demographic groupsassociated with various stereotypes remain consistent across model likelihoodsand model outputs. Furthermore, larger models consistently display higherlevels of stereotypical outputs, even when explicitly instructed not to.</description><author>Zara Siddique, Liam D. Turner, Luis Espinosa-Anke</author><pubDate>Tue, 01 Oct 2024 15:50:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06917v2</guid></item><item><title>Atomic Inference for NLI with Generated Facts as Atoms</title><link>http://arxiv.org/abs/2305.13214v2</link><description>With recent advances, neural models can achieve human-level performance onvarious natural language tasks. However, there are no guarantees that anyexplanations from these models are faithful, i.e. that they reflect the innerworkings of the model. Atomic inference overcomes this issue, providinginterpretable and faithful model decisions. This approach involves makingpredictions for different components (or atoms) of an instance, before usinginterpretable and deterministic rules to derive the overall prediction based onthe individual atom-level predictions. We investigate the effectiveness ofusing LLM-generated facts as atoms, decomposing Natural Language Inferencepremises into lists of facts. While directly using generated facts in atomicinference systems can result in worse performance, with 1) a multi-stage factgeneration process, and 2) a training regime that incorporates the facts, ourfact-based method outperforms other approaches.</description><author>Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Oana-Maria Camburu, Marek Rei</author><pubDate>Tue, 01 Oct 2024 15:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13214v2</guid></item><item><title>DRIM: Learning Disentangled Representations from Incomplete Multimodal Healthcare Data</title><link>http://arxiv.org/abs/2409.17055v2</link><description>Real-life medical data is often multimodal and incomplete, fueling thegrowing need for advanced deep learning models capable of integrating themefficiently. The use of diverse modalities, including histopathology slides,MRI, and genetic data, offers unprecedented opportunities to improve prognosisprediction and to unveil new treatment pathways. Contrastive learning, widelyused for deriving representations from paired data in multimodal tasks, assumesthat different views contain the same task-relevant information and leveragesonly shared information. This assumption becomes restrictive when handlingmedical data since each modality also harbors specific knowledge relevant todownstream tasks. We introduce DRIM, a new multimodal method for capturingthese shared and unique representations, despite data sparsity. Morespecifically, given a set of modalities, we aim to encode a representation foreach one that can be divided into two components: one encapsulatingpatient-related information common across modalities and the other,encapsulating modality-specific details. This is achieved by increasing theshared information among different patient modalities while minimizing theoverlap between shared and unique components within each modality. Our methodoutperforms state-of-the-art algorithms on glioma patients survival predictiontasks, while being robust to missing modalities. To promote reproducibility,the code is made publicly available at https://github.com/Lucas-rbnt/DRIM</description><author>Lucas Robinet, Ahmad Berjaoui, Ziad Kheil, Elizabeth Cohen-Jonathan Moyal</author><pubDate>Tue, 01 Oct 2024 15:47:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17055v2</guid></item><item><title>The Use of Large Language Models (LLM) for Cyber Threat Intelligence (CTI) in Cybercrime Forums</title><link>http://arxiv.org/abs/2408.03354v3</link><description>Large language models (LLMs) can be used to analyze cyber threat intelligence(CTI) data from cybercrime forums, which contain extensive information and keydiscussions about emerging cyber threats. However, to date, the level ofaccuracy and efficiency of LLMs for such critical tasks has yet to bethoroughly evaluated. Hence, this study assesses the performance of an LLMsystem built on the OpenAI GPT-3.5-turbo model [8] to extract CTI information.To do so, a random sample of more than 700 daily conversations from threecybercrime forums - XSS, Exploit_in, and RAMP - was extracted, and the LLMsystem was instructed to summarize the conversations and predict 10 key CTIvariables, such as whether a large organization and/or a criticalinfrastructure is being targeted, with only simple human-language instructions.Then, two coders reviewed each conversation and evaluated whether theinformation extracted by the LLM was accurate. The LLM system performed well,with an average accuracy score of 96.23%, an average precision of 90% and anaverage recall of 88.2%. Various ways to enhance the model were uncovered, suchas the need to help the LLM distinguish between stories and past events, aswell as being careful with verb tenses in prompts. Nevertheless, the results ofthis study highlight the relevance of using LLMs for cyber threat intelligence.</description><author>Vanessa Clairoux-Trepanier, Isa-May Beauchamp, Estelle Ruellan, Masarah Paquet-Clouston, Serge-Olivier Paquette, Eric Clay</author><pubDate>Tue, 01 Oct 2024 15:41:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03354v3</guid></item><item><title>MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents</title><link>http://arxiv.org/abs/2404.10774v2</link><description>Recognizing if LLM output can be grounded in evidence is central to manytasks in NLP: retrieval-augmented generation, summarization, document-groundeddialogue, and more. Current approaches to this kind of fact-checking are basedon verifying each piece of a model generation against potential evidence usingan LLM. However, this process can be very computationally expensive, requiringmany calls to a model to check a single response. In this work, we show how tobuild small fact-checking models that have GPT-4-level performance but for 400xlower cost. We do this by constructing synthetic training data with GPT-4,which involves creating realistic yet challenging instances of factual errorsvia a structured generation procedure. Training on this data teaches models tocheck each fact in the claim and recognize synthesis of information acrosssentences. For evaluation, we unify datasets from recent work on fact-checkingand grounding LLM generations into a new benchmark, LLM-AggreFact. Our bestsystem MiniCheck-FT5 (770M parameters) outperforms all systems of comparablesize and reaches GPT-4 accuracy. We release LLM-AggreFact, code for datasynthesis, and models.</description><author>Liyan Tang, Philippe Laban, Greg Durrett</author><pubDate>Tue, 01 Oct 2024 15:39:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10774v2</guid></item><item><title>SPAMming Labels: Efficient Annotations for the Trackers of Tomorrow</title><link>http://arxiv.org/abs/2404.11426v3</link><description>Increasing the annotation efficiency of trajectory annotations from videoshas the potential to enable the next generation of data-hungry trackingalgorithms to thrive on large-scale datasets. Despite the importance of thistask, there are currently very few works exploring how to efficiently labeltracking datasets comprehensively. In this work, we introduce SPAM, a videolabel engine that provides high-quality labels with minimal human intervention.SPAM is built around two key insights: i) most tracking scenarios can be easilyresolved. To take advantage of this, we utilize a pre-trained model to generatehigh-quality pseudo-labels, reserving human involvement for a smaller subset ofmore difficult instances; ii) handling the spatiotemporal dependencies of trackannotations across time can be elegantly and efficiently formulated throughgraphs. Therefore, we use a unified graph formulation to address the annotationof both detections and identity association for tracks across time. Based onthese insights, SPAM produces high-quality annotations with a fraction ofground truth labeling cost. We demonstrate that trackers trained on SPAM labelsachieve comparable performance to those trained on human annotations whilerequiring only $3-20\%$ of the human labeling effort. Hence, SPAM paves the waytowards highly efficient labeling of large-scale tracking datasets. We releaseall models and code.</description><author>Orcun Cetintas, Tim Meinhardt, Guillem Brasó, Laura Leal-Taixé</author><pubDate>Tue, 01 Oct 2024 15:34:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11426v3</guid></item><item><title>Physics-Informed Latent Diffusion for Multimodal Brain MRI Synthesis</title><link>http://arxiv.org/abs/2409.13532v2</link><description>Recent advances in generative models for medical imaging have shown promisein representing multiple modalities. However, the variability in modalityavailability across datasets limits the general applicability of the syntheticdata they produce. To address this, we present a novel physics-informedgenerative model capable of synthesizing a variable number of brain MRImodalities, including those not present in the original dataset. Our approachutilizes latent diffusion models and a two-step generative process: first,unobserved physical tissue property maps are synthesized using a latentdiffusion model, and then these maps are combined with a physical signal modelto generate the final MRI scan. Our experiments demonstrate the efficacy ofthis approach in generating unseen MR contrasts and preserving physicalplausibility. Furthermore, we validate the distributions of generated tissueproperties by comparing them to those measured in real brain tissue.</description><author>Sven Lüpke, Yousef Yeganeh, Ehsan Adeli, Nassir Navab, Azade Farshad</author><pubDate>Tue, 01 Oct 2024 15:33:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.13532v2</guid></item><item><title>Observe Then Act: Asynchronous Active Vision-Action Model for Robotic Manipulation</title><link>http://arxiv.org/abs/2409.14891v2</link><description>In real-world scenarios, many robotic manipulation tasks are hindered byocclusions and limited fields of view, posing significant challenges forpassive observation-based models that rely on fixed or wrist-mounted cameras.In this paper, we investigate the problem of robotic manipulation under limitedvisual observation and propose a task-driven asynchronous active vision-actionmodel.Our model serially connects a camera Next-Best-View (NBV) policy with agripper Next-Best Pose (NBP) policy, and trains them in a sensor-motorcoordination framework using few-shot reinforcement learning. This approachallows the agent to adjust a third-person camera to actively observe theenvironment based on the task goal, and subsequently infer the appropriatemanipulation actions.We trained and evaluated our model on 8viewpoint-constrained tasks in RLBench. The results demonstrate that our modelconsistently outperforms baseline algorithms, showcasing its effectiveness inhandling visual constraints in manipulation tasks.</description><author>Guokang Wang, Hang Li, Shuyuan Zhang, Yanhong Liu, Huaping Liu</author><pubDate>Tue, 01 Oct 2024 15:31:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14891v2</guid></item><item><title>Large Language Models Are Unconscious of Unreasonability in Math Problems</title><link>http://arxiv.org/abs/2403.19346v3</link><description>Large language models (LLMs) demonstrate substantial capabilities in solvingmath problems. However, they tend to produce hallucinations when givenquestions containing unreasonable errors. In this paper, we study the behaviorof LLMs when faced with unreasonable math problems and further explore theirpotential to address these problems. We construct the Unreasonable Math Problem(UMP) benchmark to examine the error detection ability of LLMs. Experimentsshow that LLMs are able to detect unreasonable errors, but still fail ingenerating non-hallucinatory content. In order to improve their ability oferror detection and correction, we further design a strategic prompt templatecalled Critical Calculation and Conclusion(CCC). With CCC, LLMs can betterself-evaluate and detect unreasonable errors in math questions, making themmore reliable and safe in practical application scenarios.</description><author>Jingyuan Ma, Damai Dai, Lei Sha, Zhifang Sui</author><pubDate>Tue, 01 Oct 2024 15:28:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19346v3</guid></item><item><title>NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human Connectomes</title><link>http://arxiv.org/abs/2409.17510v2</link><description>Although modern imaging technologies allow us to study connectivity betweentwo distinct brain regions in-vivo, an in-depth understanding of how anatomicalstructure supports brain function and how spontaneous functional fluctuationsemerge remarkable cognition is still elusive. Meanwhile, tremendous effortshave been made in the realm of machine learning to establish the nonlinearmapping between neuroimaging data and phenotypic traits. However, the absenceof neuroscience insight in the current approaches poses significant challengesin understanding cognitive behavior from transient neural activities. Toaddress this challenge, we put the spotlight on the coupling mechanism ofstructural connectivity (SC) and functional connectivity (FC) by formulatingsuch network neuroscience question into an expressive graph representationlearning problem for high-order topology. Specifically, we introduce theconcept of topological detour to characterize how a ubiquitous instance of FC(direct link) is supported by neural pathways (detour) physically wired by SC,which forms a cyclic loop interacted by brain structure and function. In theclich\'e of machine learning, the multi-hop detour pathway underlying SC-FCcoupling allows us to devise a novel multi-head self-attention mechanism withinTransformer to capture multi-modal feature representation from paired graphs ofSC and FC. Taken together, we propose a biological-inspired deep model, coinedas NeuroPath, to find putative connectomic feature representations from theunprecedented amount of neuroimages, which can be plugged into variousdownstream applications such as task recognition and disease diagnosis. We haveevaluated NeuroPath on large-scale public datasets including HCP and UK Biobankunder supervised and zero-shot learning, where the state-of-the-art performanceby our NeuroPath indicates great potential in network neuroscience.</description><author>Ziquan Wei, Tingting Dan, Jiaqi Ding, Guorong Wu</author><pubDate>Tue, 01 Oct 2024 15:23:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17510v2</guid></item><item><title>Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach</title><link>http://arxiv.org/abs/2409.10932v2</link><description>Coronary heart disease (CHD) is a severe cardiac disease, and hence, itsearly diagnosis is essential as it improves treatment results and saves moneyon medical care. The prevailing development of quantum computing and machinelearning (ML) technologies may bring practical improvement to the performanceof CHD diagnosis. Quantum machine learning (QML) is receiving tremendousinterest in various disciplines due to its higher performance and capabilities.A quantum leap in the healthcare industry will increase processing power andoptimise multiple models. Techniques for QML have the potential to forecastcardiac disease and help in early detection. To predict the risk of coronaryheart disease, a hybrid approach utilizing an ensemble machine learning modelbased on QML classifiers is presented in this paper. Our approach, with itsunique ability to address multidimensional healthcare data, reassures themethod's robustness by fusing quantum and classical ML algorithms in amulti-step inferential framework. The marked rise in heart disease and deathrates impacts worldwide human health and the global economy. Reducing cardiacmorbidity and mortality requires early detection of heart disease. In thisresearch, a hybrid approach utilizes techniques with quantum computingcapabilities to tackle complex problems that are not amenable to conventionalmachine learning algorithms and to minimize computational expenses. Theproposed method has been developed in the Raspberry Pi 5 Graphics ProcessingUnit (GPU) platform and tested on a broad dataset that integrates clinical andimaging data from patients suffering from CHD and healthy controls. Compared toclassical machine learning models, the accuracy, sensitivity, F1 score, andspecificity of the proposed hybrid QML model used with CHD are manifold higher.</description><author>Mehroush Banday, Sherin Zafar, Parul Agarwal, M Afshar Alam, Abubeker K M</author><pubDate>Tue, 01 Oct 2024 15:21:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10932v2</guid></item><item><title>Divide And Conquer: Learning Chaotic Dynamical Systems With Multistep Penalty Neural Ordinary Differential Equations</title><link>http://arxiv.org/abs/2407.00568v4</link><description>Forecasting high-dimensional dynamical systems is a fundamental challenge invarious fields, such as geosciences and engineering. Neural OrdinaryDifferential Equations (NODEs), which combine the power of neural networks andnumerical solvers, have emerged as a promising algorithm for forecastingcomplex nonlinear dynamical systems. However, classical techniques used forNODE training are ineffective for learning chaotic dynamical systems. In thiswork, we propose a novel NODE-training approach that allows for robust learningof chaotic dynamical systems. Our method addresses the challenges ofnon-convexity and exploding gradients associated with underlying chaoticdynamics. Training data trajectories from such systems are split into multiple,non-overlapping time windows. In addition to the deviation from the trainingdata, the optimization loss term further penalizes the discontinuities of thepredicted trajectory between the time windows. The window size is selectedbased on the fastest Lyapunov time scale of the system. Multi-step penalty(MP)method is first demonstrated on Lorenz equation, to illustrate how it improvesthe loss landscape and thereby accelerates the optimization convergence. MPmethod can optimize chaotic systems in a manner similar to least-squaresshadowing with significantly lower computational costs. Our proposed algorithm,denoted the Multistep Penalty NODE, is applied to chaotic systems such as theKuramoto-Sivashinsky equation, the two-dimensional Kolmogorov flow, and ERA5reanalysis data for the atmosphere. It is observed that MP-NODE provide viableperformance for such chaotic systems, not only for short-term trajectorypredictions but also for invariant statistics that are hallmarks of the chaoticnature of these dynamics.</description><author>Dibyajyoti Chakraborty, Seung Whan Chung, Troy Arcomano, Romit Maulik</author><pubDate>Tue, 01 Oct 2024 15:19:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00568v4</guid></item><item><title>Visual Robustness Benchmark for Visual Question Answering (VQA)</title><link>http://arxiv.org/abs/2407.03386v3</link><description>Can Visual Question Answering (VQA) systems perform just as well whendeployed in the real world? Or are they susceptible to realistic corruptioneffects e.g. image blur, which can be detrimental in sensitive applications,such as medical VQA? While linguistic or textual robustness has been thoroughlyexplored in the VQA literature, there has yet to be any significant work on thevisual robustness of VQA models. We propose the first large-scale benchmarkcomprising 213,000 augmented images, challenging the visual robustness ofmultiple VQA models and assessing the strength of realistic visual corruptions.Additionally, we have designed several robustness evaluation metrics that canbe aggregated into a unified metric and tailored to fit a variety of use cases.Our experiments reveal several insights into the relationships between modelsize, performance, and robustness with the visual corruptions. Our benchmarkhighlights the need for a balanced approach in model development that considersmodel performance without compromising the robustness.</description><author>Md Farhan Ishmam, Ishmam Tashdeed, Talukder Asir Saadat, Md Hamjajul Ashmafee, Abu Raihan Mostofa Kamal, Md. Azam Hossain</author><pubDate>Tue, 01 Oct 2024 15:08:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.03386v3</guid></item><item><title>OmniHands: Towards Robust 4D Hand Mesh Recovery via A Versatile Transformer</title><link>http://arxiv.org/abs/2405.20330v3</link><description>In this paper, we introduce OmniHands, a universal approach to recoveringinteractive hand meshes and their relative movement from monocular ormulti-view inputs. Our approach addresses two major limitations of previousmethods: lacking a unified solution for handling various hand image inputs andneglecting the positional relationship of two hands within images. To overcomethese challenges, we develop a universal architecture with novel tokenizationand contextual feature fusion strategies, capable of adapting to a variety oftasks. Specifically, we propose a Relation-aware Two-Hand Tokenization (RAT)method to embed positional relation information into the hand tokens. In thisway, our network can handle both single-hand and two-hand inputs and explicitlyleverage relative hand positions, facilitating the reconstruction of intricatehand interactions in real-world scenarios. As such tokenization indicates therelative relationship of two hands, it also supports more effective featurefusion. To this end, we further develop a 4D Interaction Reasoning (FIR) moduleto fuse hand tokens in 4D with attention and decode them into 3D hand meshesand relative temporal movements. The efficacy of our approach is validated onseveral benchmark datasets. The results on in-the-wild videos and real-worldscenarios demonstrate the superior performances of our approach for interactivehand reconstruction. More video results can be found on the project page:https://OmniHand.github.io.</description><author>Dixuan Lin, Yuxiang Zhang, Mengcheng Li, Yebin Liu, Wei Jing, Qi Yan, Qianying Wang, Hongwen Zhang</author><pubDate>Tue, 01 Oct 2024 15:04:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20330v3</guid></item><item><title>OLAPH: Improving Factuality in Biomedical Long-form Question Answering</title><link>http://arxiv.org/abs/2405.12701v2</link><description>In the medical domain, numerous scenarios necessitate the long-formgeneration ability of large language models (LLMs). Specifically, whenaddressing patients' questions, it is essential that the model's responseconveys factual claims, highlighting the need for an automated method toevaluate those claims. Thus, we introduce MedLFQA, a benchmark datasetreconstructed using long-form question-answering datasets related to thebiomedical domain. We use MedLFQA to facilitate a cost-effective automaticevaluations of factuality. We also propose OLAPH, a simple and novel frameworkthat utilizes cost-effective and multifaceted automatic evaluation to constructa synthetic preference set and answers questions in our preferred manner. Ourframework leads us to train LLMs step-by-step to reduce hallucinations andinclude crucial medical claims. We highlight that, even on evaluation metricsnot used during training, LLMs trained with our OLAPH framework demonstratesignificant performance improvement in factuality. Our findings reveal that a7B LLM trained with our OLAPH framework can provide long answers comparable tothe medical experts' answers in terms of factuality. We believe that our workcould shed light on gauging the long-text generation ability of LLMs in themedical domain. Our code and datasets are available.</description><author>Minbyul Jeong, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, Jaewoo Kang</author><pubDate>Tue, 01 Oct 2024 15:03:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12701v2</guid></item><item><title>HOLA-Drone: Hypergraphic Open-ended Learning for Zero-Shot Multi-Drone Cooperative Pursuit</title><link>http://arxiv.org/abs/2409.08767v2</link><description>Zero-shot coordination (ZSC) is a significant challenge in multi-agentcollaboration, aiming to develop agents that can coordinate with unseenpartners they have not encountered before. Recent cutting-edge ZSC methods haveprimarily focused on two-player video games such as OverCooked!2 and Hanabi. Inthis paper, we extend the scope of ZSC research to the multi-drone cooperativepursuit scenario, exploring how to construct a drone agent capable ofcoordinating with multiple unseen partners to capture multiple evaders. Wepropose a novel Hypergraphic Open-ended Learning Algorithm (HOLA-Drone) thatcontinuously adapts the learning objective based on our hypergraphic-form gamemodeling, aiming to improve cooperative abilities with multiple unknown droneteammates. To empirically verify the effectiveness of HOLA-Drone, we build twodifferent unseen drone teammate pools to evaluate their performance incoordination with various unseen partners. The experimental results demonstratethat HOLA-Drone outperforms the baseline methods in coordination with unseendrone teammates. Furthermore, real-world experiments validate the feasibilityof HOLA-Drone in physical systems. Videos can be found on the projecthomepage~\url{https://sites.google.com/view/hola-drone}.</description><author>Yang Li, Dengyu Zhang, Junfan Chen, Ying Wen, Qingrui Zhang, Shaoshuai Mou, Wei Pan</author><pubDate>Tue, 01 Oct 2024 14:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08767v2</guid></item><item><title>Evidence Is All You Need: Ordering Imaging Studies via Language Model Alignment with the ACR Appropriateness Criteria</title><link>http://arxiv.org/abs/2409.19177v2</link><description>Diagnostic imaging studies are an increasingly important component of theworkup and management of acutely presenting patients. However, orderingappropriate imaging studies according to evidence-based medical guidelines is achallenging task with a high degree of variability between healthcareproviders. To address this issue, recent work has investigated if generative AIand large language models can be leveraged to help clinicians order relevantimaging studies for patients. However, it is challenging to ensure that thesetools are correctly aligned with medical guidelines, such as the AmericanCollege of Radiology's Appropriateness Criteria (ACR AC). In this study, weintroduce a framework to intelligently leverage language models by recommendingimaging studies for patient cases that are aligned with evidence-basedguidelines. We make available a novel dataset of patient "one-liner" scenariosto power our experiments, and optimize state-of-the-art language models toachieve an accuracy on par with clinicians in image ordering. Finally, wedemonstrate that our language model-based pipeline can be used as intelligentassistants by clinicians to support image ordering workflows and improve theaccuracy of imaging study ordering according to the ACR AC. Our workdemonstrates and validates a strategy to leverage AI-based software to improvetrustworthy clinical decision making in alignment with expert evidence-basedguidelines.</description><author>Michael S. Yao, Allison Chae, Charles E. Kahn Jr., Walter R. Witschey, James C. Gee, Hersh Sagreiya, Osbert Bastani</author><pubDate>Tue, 01 Oct 2024 14:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19177v2</guid></item><item><title>FELRec: Efficient Handling of Item Cold-Start With Dynamic Representation Learning in Recommender Systems</title><link>http://arxiv.org/abs/2210.16928v2</link><description>Recommender systems suffer from the cold-start problem whenever a new userjoins the platform or a new item is added to the catalog. To address itemcold-start, we propose to replace the embedding layer in sequentialrecommenders with a dynamic storage that has no learnable weights and can keepan arbitrary number of representations. In this paper, we present FELRec, alarge embedding network that refines the existing representations of users anditems in a recursive manner, as new information becomes available. In contrastto similar approaches, our model represents new users and items without sideinformation and time-consuming finetuning, instead it runs a single forwardpass over a sequence of existing representations. During item cold-start, ourmethod outperforms similar method by 29.50%-47.45%. Further, our proposed modelgeneralizes well to previously unseen datasets in zero-shot settings. Thesource code is publicly available at https://github.com/kweimann/FELRec .</description><author>Kuba Weimann, Tim O. F. Conrad</author><pubDate>Tue, 01 Oct 2024 14:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16928v2</guid></item><item><title>Ladder Bottom-up Convolutional Bidirectional Variational Autoencoder for Image Translation of Dotted Arabic Expiration Dates</title><link>http://arxiv.org/abs/2310.14069v2</link><description>This paper proposes an approach of Ladder Bottom-up ConvolutionalBidirectional Variational Autoencoder (LCBVAE) architecture for the encoder anddecoder, which is trained on the image translation of the dotted Arabicexpiration dates by reconstructing the Arabic dotted expiration dates intofilled-in expiration dates. We employed a customized and adapted version ofConvolutional Recurrent Neural Network CRNN model to meet our specificrequirements and enhance its performance in our context, and then trained thecustom CRNN model with the filled-in images from the year of 2019 to 2027 toextract the expiration dates and assess the model performance of LCBVAE on theexpiration date recognition. The pipeline of (LCBVAE+CRNN) can be thenintegrated into an automated sorting systems for extracting the expiry datesand sorting the products accordingly during the manufacture stage.Additionally, it can overcome the manual entry of expiration dates that can betime-consuming and inefficient at the merchants. Due to the lack of theavailability of the dotted Arabic expiration date images, we created an Arabicdot-matrix True Type Font (TTF) for the generation of the synthetic images. Wetrained the model with unrealistic synthetic dates of 60,000 images andperformed the testing on a realistic synthetic date of 3000 images from theyear of 2019 to 2027, represented as yyyy/mm/dd. In our study, we demonstratedthe significance of latent bottleneck layer with improving the generalizationwhen the size is increased up to 1024 in downstream transfer learning tasks asfor image translation. The proposed approach achieved an accuracy of 97% on theimage translation with using the LCBVAE architecture that can be generalizedfor any downstream learning tasks as for image translation and reconstruction.</description><author>Ahmed Zidane, Ghada Soliman</author><pubDate>Tue, 01 Oct 2024 14:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14069v2</guid></item><item><title>MobileMEF: Fast and Efficient Method for Multi-Exposure Fusion</title><link>http://arxiv.org/abs/2408.07932v2</link><description>Recent advances in camera design and imaging technology have enabled thecapture of high-quality images using smartphones. However, due to the limiteddynamic range of digital cameras, the quality of photographs captured inenvironments with highly imbalanced lighting often results in poor-qualityimages. To address this issue, most devices capture multi-exposure frames andthen use some multi-exposure fusion method to merge those frames into a finalfused image. Nevertheless, most traditional and current deep learningapproaches are unsuitable for real-time applications on mobile devices due totheir heavy computational and memory requirements. We propose a new method formulti-exposure fusion based on an encoder-decoder deep learning architecturewith efficient building blocks tailored for mobile devices. This efficientdesign makes our model capable of processing 4K resolution images in less than2 seconds on mid-range smartphones. Our method outperforms state-of-the-arttechniques regarding full-reference quality measures and computationalefficiency (runtime and memory usage), making it ideal for real-timeapplications on hardware-constrained devices. Our code is available at:https://github.com/LucasKirsten/MobileMEF.</description><author>Lucas Nedel Kirsten, Zhicheng Fu, Nikhil Ambha Madhusudhana</author><pubDate>Tue, 01 Oct 2024 14:26:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07932v2</guid></item><item><title>Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities</title><link>http://arxiv.org/abs/2403.04908v3</link><description>Recent advancements in Vision-Language (VL) models have sparked interest intheir deployment on edge devices, yet challenges in handling diverse visualmodalities, manual annotation, and computational constraints remain. Weintroduce EdgeVL, a novel framework that bridges this gap by seamlesslyintegrating dual-modality knowledge distillation and quantization-awarecontrastive learning. This approach enables the adaptation of large VL models,like CLIP, for efficient use with both RGB and non-RGB images onresource-limited devices without the need for manual annotations. EdgeVL notonly transfers visual language alignment capabilities to compact models butalso maintains feature quality post-quantization, significantly enhancingopen-vocabulary classification performance across various visual modalities.Our work represents the first systematic effort to adapt large VL models foredge deployment, showcasing up to 15.4% accuracy improvements on multipledatasets and up to 93-fold reduction in model size.</description><author>Kaiwen Cai, Zhekai Duan, Gaowen Liu, Charles Fleming, Chris Xiaoxuan Lu</author><pubDate>Tue, 01 Oct 2024 14:22:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04908v3</guid></item><item><title>Enhancing GANs with Contrastive Learning-Based Multistage Progressive Finetuning SNN and RL-Based External Optimization</title><link>http://arxiv.org/abs/2409.20340v2</link><description>The application of deep learning in cancer research, particularly in earlydiagnosis, case understanding, and treatment strategy design, emphasizes theneed for high-quality data. Generative AI, especially Generative AdversarialNetworks (GANs), has emerged as a leading solution to challenges like classimbalance, robust learning, and model training, while addressing issuesstemming from patient privacy and the scarcity of real data. Despite theirpromise, GANs face several challenges, both inherent and specific tohistopathology data. Inherent issues include training imbalance, mode collapse,linear learning from insufficient discriminator feedback, and hard boundaryconvergence due to stringent feedback. Histopathology data presents a uniquechallenge with its complex representation, high spatial resolution, andmultiscale features. To address these challenges, we propose a frameworkconsisting of two components. First, we introduce a contrastive learning-basedMultistage Progressive Finetuning Siamese Neural Network (MFT-SNN) forassessing the similarity between histopathology patches. Second, we implement aReinforcement Learning-based External Optimizer (RL-EO) within the GAN trainingloop, serving as a reward signal generator. The modified discriminator lossfunction incorporates a weighted reward, guiding the GAN to maximize thisreward while minimizing loss. This approach offers an external optimizationguide to the discriminator, preventing generator overfitting and ensuringsmooth convergence. Our proposed solution has been benchmarked againststate-of-the-art (SOTA) GANs and a Denoising Diffusion Probabilistic model,outperforming previous SOTA across various metrics, including FID score, KIDscore, Perceptual Path Length, and downstream classification tasks.</description><author>Osama Mustafa</author><pubDate>Tue, 01 Oct 2024 14:14:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20340v2</guid></item><item><title>Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity</title><link>http://arxiv.org/abs/2409.18708v3</link><description>We introduce a novel family of adversarial attacks that exploit the inabilityof language models to interpret ASCII art. To evaluate these attacks, wepropose the ToxASCII benchmark and develop two custom ASCII art fonts: oneleveraging special tokens and another using text-filled letter shapes. Ourattacks achieve a perfect 1.0 Attack Success Rate across ten models, includingOpenAI's o1-preview and LLaMA 3.1. Warning: this paper contains examples of toxic language used for researchpurposes.</description><author>Sergey Berezin, Reza Farahbakhsh, Noel Crespi</author><pubDate>Tue, 01 Oct 2024 08:50:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18708v3</guid></item><item><title>Instance-adaptive Zero-shot Chain-of-Thought Prompting</title><link>http://arxiv.org/abs/2409.20441v2</link><description>Zero-shot Chain-of-Thought (CoT) prompting emerges as a simple and effectivestrategy for enhancing the performance of large language models (LLMs) inreal-world reasoning tasks. Nonetheless, the efficacy of a singular, task-levelprompt uniformly applied across the whole of instances is inherently limitedsince one prompt cannot be a good partner for all, a more appropriate approachshould consider the interaction between the prompt and each instancemeticulously. This work introduces an instance-adaptive prompting algorithm asan alternative zero-shot CoT reasoning scheme by adaptively differentiatinggood and bad prompts. Concretely, we first employ analysis on LLMs through thelens of information flow to detect the mechanism under zero-shot CoT reasoning,in which we discover that information flows from question to prompt andquestion to rationale jointly influence the reasoning results most. We noticethat a better zero-shot CoT reasoning needs the prompt to obtain semanticinformation from the question then the rationale aggregates sufficientinformation from the question directly and via the prompt indirectly. On thecontrary, lacking any of those would probably lead to a bad one. Stem fromthat, we further propose an instance-adaptive prompting strategy (IAP) forzero-shot CoT reasoning. Experiments conducted with LLaMA-2, LLaMA-3, and Qwenon math, logic, and commonsense reasoning tasks (e.g., GSM8K, MMLU, CausalJudgement) obtain consistent improvement, demonstrating that theinstance-adaptive zero-shot CoT prompting performs better than other task-levelmethods with some curated prompts or sophisticated procedures, showing thesignificance of our findings in the zero-shot CoT reasoning mechanism.</description><author>Xiaosong Yuan, Chen Shen, Shaotian Yan, Xiaofeng Zhang, Liang Xie, Wenxiao Wang, Renchu Guan, Ying Wang, Jieping Ye</author><pubDate>Tue, 01 Oct 2024 06:03:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20441v2</guid></item><item><title>Continuously Improving Mobile Manipulation with Autonomous Real-World RL</title><link>http://arxiv.org/abs/2409.20568v1</link><description>We present a fully autonomous real-world RL framework for mobile manipulationthat can learn policies without extensive instrumentation or human supervision.This is enabled by 1) task-relevant autonomy, which guides exploration towardsobject interactions and prevents stagnation near goal states, 2) efficientpolicy learning by leveraging basic task knowledge in behavior priors, and 3)formulating generic rewards that combine human-interpretable semanticinformation with low-level, fine-grained observations. We demonstrate that ourapproach allows Spot robots to continually improve their performance on a setof four challenging mobile manipulation tasks, obtaining an average successrate of 80% across tasks, a 3-4 improvement over existing approaches. Videoscan be found at https://continual-mobile-manip.github.io/</description><author>Russell Mendonca, Emmanuel Panov, Bernadette Bucher, Jiuguang Wang, Deepak Pathak</author><pubDate>Mon, 30 Sep 2024 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20568v1</guid></item><item><title>MM1.5: Methods, Analysis &amp; Insights from Multimodal LLM Fine-tuning</title><link>http://arxiv.org/abs/2409.20566v1</link><description>We present MM1.5, a new family of multimodal large language models (MLLMs)designed to enhance capabilities in text-rich image understanding, visualreferring and grounding, and multi-image reasoning. Building upon the MM1architecture, MM1.5 adopts a data-centric approach to model training,systematically exploring the impact of diverse data mixtures across the entiremodel training lifecycle. This includes high-quality OCR data and syntheticcaptions for continual pre-training, as well as an optimized visualinstruction-tuning data mixture for supervised fine-tuning. Our models rangefrom 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE)variants, and demonstrate that careful data curation and training strategiescan yield strong performance even at small scales (1B and 3B). Additionally, weintroduce two specialized variants: MM1.5-Video, designed for videounderstanding, and MM1.5-UI, tailored for mobile UI understanding. Throughextensive empirical studies and ablations, we provide detailed insights intothe training processes and decisions that inform our final designs, offeringvaluable guidance for future research in MLLM development.</description><author>Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev, Mingze Xu, Hong-You Chen, Jean-Philippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, Afshin Dehghan, Peter Grasch, Yinfei Yang</author><pubDate>Mon, 30 Sep 2024 17:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20566v1</guid></item><item><title>Ranking Over Scoring: Towards Reliable and Robust Automated Evaluation of LLM-Generated Medical Explanatory Arguments</title><link>http://arxiv.org/abs/2409.20565v1</link><description>Evaluating LLM-generated text has become a key challenge, especially indomain-specific contexts like the medical field. This work introduces a novelevaluation methodology for LLM-generated medical explanatory arguments, relyingon Proxy Tasks and rankings to closely align results with human evaluationcriteria, overcoming the biases typically seen in LLMs used as judges. Wedemonstrate that the proposed evaluators are robust against adversarialattacks, including the assessment of non-argumentative text. Additionally, thehuman-crafted arguments needed to train the evaluators are minimized to justone example per Proxy Task. By examining multiple LLM-generated arguments, weestablish a methodology for determining whether a Proxy Task is suitable forevaluating LLM-generated medical explanatory arguments, requiring only fiveexamples and two human experts.</description><author>Iker De la Iglesia, Iakes Goenaga, Johanna Ramirez-Romero, Jose Maria Villa-Gonzalez, Josu Goikoetxea, Ander Barrena</author><pubDate>Mon, 30 Sep 2024 17:59:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20565v1</guid></item><item><title>DressRecon: Freeform 4D Human Reconstruction from Monocular Video</title><link>http://arxiv.org/abs/2409.20563v1</link><description>We present a method to reconstruct time-consistent human body models frommonocular videos, focusing on extremely loose clothing or handheld objectinteractions. Prior work in human reconstruction is either limited to tightclothing with no object interactions, or requires calibrated multi-viewcaptures or personalized template scans which are costly to collect at scale.Our key insight for high-quality yet flexible reconstruction is the carefulcombination of generic human priors about articulated body shape (learned fromlarge-scale training data) with video-specific articulated "bag-of-bones"deformation (fit to a single video via test-time optimization). We accomplishthis by learning a neural implicit model that disentangles body versus clothingdeformations as separate motion model layers. To capture subtle geometry ofclothing, we leverage image-based priors such as human body pose, surfacenormals, and optical flow during optimization. The resulting neural fields canbe extracted into time-consistent meshes, or further optimized as explicit 3DGaussians for high-fidelity interactive rendering. On datasets with highlychallenging clothing deformations and object interactions, DressRecon yieldshigher-fidelity 3D reconstructions than prior art. Project page:https://jefftan969.github.io/dressrecon/</description><author>Jeff Tan, Donglai Xiang, Shubham Tulsiani, Deva Ramanan, Gengshan Yang</author><pubDate>Mon, 30 Sep 2024 17:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20563v1</guid></item><item><title>SpaceMesh: A Continuous Representation for Learning Manifold Surface Meshes</title><link>http://arxiv.org/abs/2409.20562v1</link><description>Meshes are ubiquitous in visual computing and simulation, yet most existingmachine learning techniques represent meshes only indirectly, e.g. as the levelset of a scalar field or deformation of a template, or as a disordered trianglesoup lacking local structure. This work presents a scheme to directly generatemanifold, polygonal meshes of complex connectivity as the output of a neuralnetwork. Our key innovation is to define a continuous latent connectivity spaceat each mesh vertex, which implies the discrete mesh. In particular, our vertexembeddings generate cyclic neighbor relationships in a halfedge meshrepresentation, which gives a guarantee of edge-manifoldness and the ability torepresent general polygonal meshes. This representation is well-suited tomachine learning and stochastic optimization, without restriction onconnectivity or topology. We first explore the basic properties of thisrepresentation, then use it to fit distributions of meshes from large datasets.The resulting models generate diverse meshes with tessellation structurelearned from the dataset population, with concise details and high-quality meshelements. In applications, this approach not only yields high-quality outputsfrom generative models, but also enables directly learning challenging geometryprocessing tasks such as mesh repair.</description><author>Tianchang Shen, Zhaoshuo Li, Marc Law, Matan Atzmon, Sanja Fidler, James Lucas, Jun Gao, Nicholas Sharp</author><pubDate>Mon, 30 Sep 2024 17:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20562v1</guid></item><item><title>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and Planning with LM-Driven PDDL Planner</title><link>http://arxiv.org/abs/2409.20560v1</link><description>Language models (LMs) possess a strong capability to comprehend naturallanguage, making them effective in translating human instructions into detailedplans for simple robot tasks. Nevertheless, it remains a significant challengeto handle long-horizon tasks, especially in subtask identification andallocation for cooperative heterogeneous robot teams. To address this issue, wepropose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novelmulti-agent task planning framework that achieves state-of-the-art performanceon long-horizon tasks. LaMMA-P integrates the strengths of the LMs' reasoningcapability and the traditional heuristic search planner to achieve a highsuccess rate and efficiency while demonstrating strong generalization acrosstasks. Additionally, we create MAT-THOR, a comprehensive benchmark thatfeatures household tasks with two different levels of complexity based on theAI2-THOR environment. The experimental results demonstrate that LaMMA-Pachieves a 105% higher success rate and 36% higher efficiency than existingLM-based multi-agent planners. The experimental videos, code, and datasets ofthis work as well as the detailed prompts used in each module are available athttps://lamma-p.github.io.</description><author>Xiaopan Zhang, Hao Qin, Fuquan Wang, Yue Dong, Jiachen Li</author><pubDate>Mon, 30 Sep 2024 17:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20560v1</guid></item><item><title>Supervised Multi-Modal Fission Learning</title><link>http://arxiv.org/abs/2409.20559v1</link><description>Learning from multimodal datasets can leverage complementary information andimprove performance in prediction tasks. A commonly used strategy to accountfor feature correlations in high-dimensional datasets is the latent variableapproach. Several latent variable methods have been proposed for multimodaldatasets. However, these methods either focus on extracting the sharedcomponent across all modalities or on extracting both a shared component andindividual components specific to each modality. To address this gap, wepropose a Multi-Modal Fission Learning (MMFL) model that simultaneouslyidentifies globally joint, partially joint, and individual componentsunderlying the features of multimodal datasets. Unlike existing latent variablemethods, MMFL uses supervision from the response variable to identifypredictive latent components and has a natural extension for incorporatingincomplete multimodal data. Through simulation studies, we demonstrate thatMMFL outperforms various existing multimodal algorithms in both complete andincomplete modality settings. We applied MMFL to a real-world case study forearly prediction of Alzheimers Disease using multimodal neuroimaging andgenomics data from the Alzheimers Disease Neuroimaging Initiative (ADNI)dataset. MMFL provided more accurate predictions and better insights intowithin- and across-modality correlations compared to existing methods.</description><author>Lingchao Mao, Qi wang, Yi Su, Fleming Lure, Jing Li</author><pubDate>Mon, 30 Sep 2024 17:58:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20559v1</guid></item><item><title>Uni$^2$Det: Unified and Universal Framework for Prompt-Guided Multi-dataset 3D Detection</title><link>http://arxiv.org/abs/2409.20558v1</link><description>We present Uni$^2$Det, a brand new framework for unified and universalmulti-dataset training on 3D detection, enabling robust performance acrossdiverse domains and generalization to unseen domains. Due to substantialdisparities in data distribution and variations in taxonomy across diversedomains, training such a detector by simply merging datasets poses asignificant challenge. Motivated by this observation, we introduce multi-stageprompting modules for multi-dataset 3D detection, which leverages prompts basedon the characteristics of corresponding datasets to mitigate existingdifferences. This elegant design facilitates seamless plug-and-play integrationwithin various advanced 3D detection frameworks in a unified manner, while alsoallowing straightforward adaptation for universal applicability acrossdatasets. Experiments are conducted across multiple dataset consolidationscenarios involving KITTI, Waymo, and nuScenes, demonstrating that ourUni$^2$Det outperforms existing methods by a large margin in multi-datasettraining. Notably, results on zero-shot cross-dataset transfer validate thegeneralization capability of our proposed method.</description><author>Yubin Wang, Zhikang Zou, Xiaoqing Ye, Xiao Tan, Errui Ding, Cairong Zhao</author><pubDate>Mon, 30 Sep 2024 17:57:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20558v1</guid></item><item><title>Propose, Assess, Search: Harnessing LLMs for Goal-Oriented Planning in Instructional Videos</title><link>http://arxiv.org/abs/2409.20557v1</link><description>Goal-oriented planning, or anticipating a series of actions that transitionan agent from its current state to a predefined objective, is crucial fordeveloping intelligent assistants aiding users in daily procedural tasks. Theproblem presents significant challenges due to the need for comprehensiveknowledge of temporal and hierarchical task structures, as well as strongcapabilities in reasoning and planning. To achieve this, prior work typicallyrelies on extensive training on the target dataset, which often results insignificant dataset bias and a lack of generalization to unseen tasks. In thiswork, we introduce VidAssist, an integrated framework designed forzero/few-shot goal-oriented planning in instructional videos. VidAssistleverages large language models (LLMs) as both the knowledge base and theassessment tool for generating and evaluating action plans, thus overcoming thechallenges of acquiring procedural knowledge from small-scale, low-diversitydatasets. Moreover, VidAssist employs a breadth-first search algorithm foroptimal plan generation, in which a composite of value functions designed forgoal-oriented planning is utilized to assess the predicted actions at eachstep. Extensive experiments demonstrate that VidAssist offers a unifiedframework for different goal-oriented planning setups, e.g., visual planningfor assistance (VPA) and procedural planning (PP), and achieves remarkableperformance in zero-shot and few-shot setups. Specifically, our few-shot modeloutperforms the prior fully supervised state-of-the-art method by +7.7% in VPAand +4.81% PP task on the COIN dataset while predicting 4 future actions. Code,and models are publicly available at https://sites.google.com/view/vidassist.</description><author>Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Fu-Jen Chu, Kris Kitani, Gedas Bertasius, Xitong Yang</author><pubDate>Mon, 30 Sep 2024 17:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20557v1</guid></item><item><title>Inverse Painting: Reconstructing The Painting Process</title><link>http://arxiv.org/abs/2409.20556v1</link><description>Given an input painting, we reconstruct a time-lapse video of how it may havebeen painted. We formulate this as an autoregressive image generation problem,in which an initially blank "canvas" is iteratively updated. The model learnsfrom real artists by training on many painting videos. Our approachincorporates text and region understanding to define a set of painting"instructions" and updates the canvas with a novel diffusion-based renderer.The method extrapolates beyond the limited, acrylic style paintings on which ithas been trained, showing plausible results for a wide range of artistic stylesand genres.</description><author>Bowei Chen, Yifan Wang, Brian Curless, Ira Kemelmacher-Shlizerman, Steven M. Seitz</author><pubDate>Mon, 30 Sep 2024 17:56:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20556v1</guid></item><item><title>Maia-2: A Unified Model for Human-AI Alignment in Chess</title><link>http://arxiv.org/abs/2409.20553v1</link><description>There are an increasing number of domains in which artificial intelligence(AI) systems both surpass human ability and accurately model human behavior.This introduces the possibility of algorithmically-informed teaching in thesedomains through more relatable AI partners and deeper insights into humandecision-making. Critical to achieving this goal, however, is coherentlymodeling human behavior at various skill levels. Chess is an ideal model systemfor conducting research into this kind of human-AI alignment, with its richhistory as a pivotal testbed for AI research, mature superhuman AI systems likeAlphaZero, and precise measurements of skill via chess rating systems. Previouswork in modeling human decision-making in chess uses completely independentmodels to capture human style at different skill levels, meaning they lackcoherence in their ability to adapt to the full spectrum of human improvementand are ultimately limited in their effectiveness as AI partners and teachingtools. In this work, we propose a unified modeling approach for human-AIalignment in chess that coherently captures human style across different skilllevels and directly captures how people improve. Recognizing the complex,non-linear nature of human learning, we introduce a skill-aware attentionmechanism to dynamically integrate players' strengths with encoded chesspositions, enabling our model to be sensitive to evolving player skill. Ourexperimental results demonstrate that this unified framework significantlyenhances the alignment between AI and human players across a diverse range ofexpertise levels, paving the way for deeper insights into human decision-makingand AI-guided teaching tools.</description><author>Zhenwei Tang, Difan Jiao, Reid McIlroy-Young, Jon Kleinberg, Siddhartha Sen, Ashton Anderson</author><pubDate>Mon, 30 Sep 2024 17:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20553v1</guid></item><item><title>LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation</title><link>http://arxiv.org/abs/2409.20550v1</link><description>Code generation aims to automatically generate code from input requirements,significantly enhancing development efficiency. Recent large language models(LLMs) based approaches have shown promising results and revolutionized codegeneration task. Despite the promising performance, LLMs often generatecontents with hallucinations, especially for the code generation scenariorequiring the handling of complex contextual dependencies in practicaldevelopment process. Although previous study has analyzed hallucinations inLLM-powered code generation, the study is limited to standalone functiongeneration. In this paper, we conduct an empirical study to study thephenomena, mechanism, and mitigation of LLM hallucinations within morepractical and complex development contexts in repository-level generationscenario. First, we manually examine the code generation results from sixmainstream LLMs to establish a hallucination taxonomy of LLM-generated code.Next, we elaborate on the phenomenon of hallucinations, analyze theirdistribution across different models. We then analyze causes of hallucinationsand identify four potential factors contributing to hallucinations. Finally, wepropose an RAG-based mitigation method, which demonstrates consistenteffectiveness in all studied LLMs. The replication package including code,data, and experimental results is available athttps://github.com/DeepSoftwareAnalytics/LLMCodingHallucination</description><author>Ziyao Zhang, Yanlin Wang, Chong Wang, Jiachi Chen, Zibin Zheng</author><pubDate>Mon, 30 Sep 2024 17:51:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20550v1</guid></item><item><title>The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance</title><link>http://arxiv.org/abs/2406.11634v2</link><description>Cloze testing is a common method for measuring the behavior of large languagemodels on a number of benchmark tasks. Using the MMLU dataset, we show that thebase-rate probability (BRP) differences across answer tokens are significantand affect task performance ie. guess A if uncertain. We find thatcounterfactual prompting does sufficiently mitigate the BRP effect. The BRPeffect is found to have a similar effect to test taking strategies employed byhumans leading to the conflation of task performance and test-taking ability.We propose the Nvr-X-MMLU task, a variation of MMLU, which helps todisambiguate test-taking ability from task performance and reports the latter.</description><author>Kyle Moore, Jesse Roberts, Thao Pham, Oseremhen Ewaleifoh, Doug Fisher</author><pubDate>Mon, 30 Sep 2024 17:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11634v2</guid></item><item><title>Robi Butler: Remote Multimodal Interactions with Household Robot Assistant</title><link>http://arxiv.org/abs/2409.20548v1</link><description>In this paper, we introduce Robi Butler, a novel household robotic systemthat enables multimodal interactions with remote users. Building on theadvanced communication interfaces, Robi Butler allows users to monitor therobot's status, send text or voice instructions, and select target objects byhand pointing. At the core of our system is a high-level behavior module,powered by Large Language Models (LLMs), that interprets multimodalinstructions to generate action plans. These plans are composed of a set ofopen vocabulary primitives supported by Vision Language Models (VLMs) thathandle both text and pointing queries. The integration of the above componentsallows Robi Butler to ground remote multimodal instructions in the real-worldhome environment in a zero-shot manner. We demonstrate the effectiveness andefficiency of this system using a variety of daily household tasks that involveremote users giving multimodal instructions. Additionally, we conducted a userstudy to analyze how multimodal interactions affect efficiency and userexperience during remote human-robot interaction and discuss the potentialimprovements.</description><author>Anxing Xiao, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun Yu, David Hsu</author><pubDate>Mon, 30 Sep 2024 17:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20548v1</guid></item><item><title>Annealing Flow Generative Model Towards Sampling High-Dimensional and Multi-Modal Distributions</title><link>http://arxiv.org/abs/2409.20547v1</link><description>Sampling from high-dimensional, multi-modal distributions remains afundamental challenge across domains such as statistical Bayesian inference andphysics-based machine learning. In this paper, we propose Annealing Flow (AF),a continuous normalizing flow-based approach designed to sample fromhigh-dimensional and multi-modal distributions. The key idea is to learn acontinuous normalizing flow-based transport map, guided by annealing, totransition samples from an easy-to-sample distribution to the targetdistribution, facilitating effective exploration of modes in high-dimensionalspaces. Unlike many existing methods, AF training does not rely on samples fromthe target distribution. AF ensures effective and balanced mode exploration,achieves linear complexity in sample size and dimensions, and circumventsinefficient mixing times. We demonstrate the superior performance of AFcompared to state-of-the-art methods through extensive experiments on variouschallenging distributions and real-world datasets, particularly inhigh-dimensional and multi-modal settings. We also highlight the potential ofAF for sampling the least favorable distributions.</description><author>Dongze Wu, Yao Xie</author><pubDate>Mon, 30 Sep 2024 17:48:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20547v1</guid></item><item><title>FABLES: Evaluating faithfulness and content selection in book-length summarization</title><link>http://arxiv.org/abs/2404.01261v2</link><description>While long-context large language models (LLMs) can technically summarizebook-length documents (&gt;100K tokens), the length and complexity of thedocuments have so far prohibited evaluations of input-dependent aspects likefaithfulness. In this paper, we conduct the first large-scale human evaluationof faithfulness and content selection on LLM-generated summaries of fictionalbooks. Our study mitigates the issue of data contamination by focusing onsummaries of books published in 2023 or 2024, and we hire annotators who havefully read each book prior to the annotation task to minimize cost andcognitive burden. We collect FABLES, a dataset of annotations on 3,158 claimsmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, whichallows us to rank LLM summarizers based on faithfulness: Claude-3-Opussignificantly outperforms all closed-source LLMs, while the open-source Mixtralis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that mostunfaithful claims relate to events and character states, and they generallyrequire indirect reasoning over the narrative to invalidate. While LLM-basedauto-raters have proven reliable for factuality and coherence in othersettings, we implement several LLM raters of faithfulness and find that nonecorrelates strongly with human annotations, especially with regard to detectingunfaithful claims. Our experiments suggest that detecting unfaithful claims isan important future direction not only for summarization evaluation but also asa testbed for long-context understanding. Finally, we move beyond faithfulnessby exploring content selection errors in book-length summarization: we developa typology of omission errors related to crucial narrative elements and alsoidentify a systematic over-emphasis on events occurring towards the end of thebook.</description><author>Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, Mohit Iyyer</author><pubDate>Mon, 30 Sep 2024 17:39:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01261v2</guid></item><item><title>Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers</title><link>http://arxiv.org/abs/2409.20537v1</link><description>One of the roadblocks for training generalist robotic models today isheterogeneity. Previous robot learning methods often collect data to train withone specific embodiment for one task, which is expensive and prone tooverfitting. This work studies the problem of learning policy representationsthrough heterogeneous pre-training on robot data across different embodimentsand tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT),which pre-train a large, shareable trunk of a policy neural network to learn atask and embodiment agnostic shared representation. This general architecturealigns the specific proprioception and vision inputs from distinct embodimentsto a short sequence of tokens and then processes such tokens to map to controlrobots for different tasks. Leveraging the recent large-scale multi-embodimentreal-world robotic datasets as well as simulation, deployed robots, and humanvideo datasets, we investigate pre-training policies across heterogeneity. Weconduct experiments to investigate the scaling behaviors of trainingobjectives, to the extent of 52 datasets. HPTs outperform several baselines andenhance the fine-tuned policy performance by over 20% on unseen tasks inmultiple simulator benchmarks and real-world settings. See the project website(https://liruiw.github.io/hpt/) for code and videos.</description><author>Lirui Wang, Xinlei Chen, Jialiang Zhao, Kaiming He</author><pubDate>Mon, 30 Sep 2024 17:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20537v1</guid></item><item><title>Best Practices for Responsible Machine Learning in Credit Scoring</title><link>http://arxiv.org/abs/2409.20536v1</link><description>The widespread use of machine learning in credit scoring has broughtsignificant advancements in risk assessment and decision-making. However, ithas also raised concerns about potential biases, discrimination, and lack oftransparency in these automated systems. This tutorial paper performed anon-systematic literature review to guide best practices for developingresponsible machine learning models in credit scoring, focusing on fairness,reject inference, and explainability. We discuss definitions, metrics, andtechniques for mitigating biases and ensuring equitable outcomes acrossdifferent groups. Additionally, we address the issue of limited datarepresentativeness by exploring reject inference methods that incorporateinformation from rejected loan applications. Finally, we emphasize theimportance of transparency and explainability in credit models, discussingtechniques that provide insights into the decision-making process and enableindividuals to understand and potentially improve their creditworthiness. Byadopting these best practices, financial institutions can harness the power ofmachine learning while upholding ethical and responsible lending practices.</description><author>Giovani Valdrighi, Athyrson M. Ribeiro, Jansen S. B. Pereira, Vitoria Guardieiro, Arthur Hendricks, Décio Miranda Filho, Juan David Nieto Garcia, Felipe F. Bocca, Thalita B. Veronese, Lucas Wanner, Marcos Medeiros Raimundo</author><pubDate>Mon, 30 Sep 2024 17:39:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20536v1</guid></item><item><title>End-to-End Conformal Calibration for Optimization Under Uncertainty</title><link>http://arxiv.org/abs/2409.20534v1</link><description>Machine learning can significantly improve performance for decision-makingunder uncertainty in a wide range of domains. However, ensuring robustnessguarantees requires well-calibrated uncertainty estimates, which can bedifficult to achieve in high-capacity prediction models such as deep neuralnetworks. Moreover, in high-dimensional settings, there may be many validuncertainty estimates, each with their own performance profile - i.e., not alluncertainty is equally valuable for downstream decision-making. To address thisproblem, this paper develops an end-to-end framework to learn the uncertaintyestimates for conditional robust optimization, with robustness and calibrationguarantees provided by conformal prediction. In addition, we propose torepresent arbitrary convex uncertainty sets with partially input-convex neuralnetworks, which are learned as part of our framework. Our approach consistentlyimproves upon two-stage estimate-then-optimize baselines on concreteapplications in energy storage arbitrage and portfolio optimization.</description><author>Christopher Yeh, Nicolas Christianson, Alan Wu, Adam Wierman, Yisong Yue</author><pubDate>Mon, 30 Sep 2024 17:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20534v1</guid></item><item><title>On the Resilience of Multi-Agent Systems with Malicious Agents</title><link>http://arxiv.org/abs/2408.00989v2</link><description>Multi-agent systems, powered by large language models, have shown greatabilities across various tasks due to the collaboration of expert agents, eachfocusing on a specific domain. However, when agents are deployed separately,there is a risk that malicious users may introduce malicious agents whogenerate incorrect or irrelevant results that are too stealthy to be identifiedby other non-specialized agents. Therefore, this paper investigates twoessential questions: (1) What is the resilience of various multi-agent systemstructures (e.g., A$\rightarrow$B$\rightarrow$C,A$\leftrightarrow$B$\leftrightarrow$C) under malicious agents, on differentdownstream tasks? (2) How can we increase system resilience to defend againstmalicious agents? To simulate malicious agents, we devise two methods,AutoTransform and AutoInject, to transform any agent into a malicious one whilepreserving its functional integrity. We run comprehensive experiments on fourdownstream multi-agent systems tasks, namely code generation, math problems,translation, and text evaluation. Results suggest that the "hierarchical"multi-agent structure, i.e., A$\rightarrow$(B$\leftrightarrow$C), exhibitssuperior resilience with the lowest performance drop of $23.6\%$, compared to$46.4\%$ and $49.8\%$ of other two structures. Additionally, we show thepromise of improving multi-agent system resilience by demonstrating that twodefense methods, introducing a mechanism for each agent to challenge others'outputs, or an additional agent to review and correct messages, can enhancesystem resilience. Our code and data are available athttps://github.com/CUHK-ARISE/MAS-Resilience.</description><author>Jen-tse Huang, Jiaxu Zhou, Tailin Jin, Xuhui Zhou, Zixi Chen, Wenxuan Wang, Youliang Yuan, Maarten Sap, Michael R. Lyu</author><pubDate>Mon, 30 Sep 2024 17:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00989v2</guid></item><item><title>Can Large Language Models Address Open-Target Stance Detection?</title><link>http://arxiv.org/abs/2409.00222v4</link><description>Stance detection (SD) identifies a text's position towards a target,typically labeled as favor, against, or none. We introduce Open-Target StanceDetection (OTSD), the most realistic task where targets are neither seen duringtraining nor provided as input. We evaluate Large Language Models (LLMs)GPT-4o, GPT-3.5, Llama-3, and Mistral, comparing their performance to the onlyexisting work, Target-Stance Extraction (TSE), which benefits from predefinedtargets. Unlike TSE, OTSD removes the dependency of a predefined list, makingtarget generation and evaluation more challenging. We also provide a metric forevaluating target quality that correlates well with human judgment. Ourexperiments reveal that LLMs outperform TSE in target generation when the realtarget is explicitly and not explicitly mentioned in the text. Likewise, forstance detection, LLMs excel in explicit cases with comparable performance innon-explicit in general.</description><author>Abu Ubaida Akash, Ahmed Fahmy, Amine Trabelsi</author><pubDate>Mon, 30 Sep 2024 17:37:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00222v4</guid></item><item><title>Dual Encoder GAN Inversion for High-Fidelity 3D Head Reconstruction from Single Images</title><link>http://arxiv.org/abs/2409.20530v1</link><description>3D GAN inversion aims to project a single image into the latent space of a 3DGenerative Adversarial Network (GAN), thereby achieving 3D geometryreconstruction. While there exist encoders that achieve good results in 3D GANinversion, they are predominantly built on EG3D, which specializes insynthesizing near-frontal views and is limiting in synthesizing comprehensive3D scenes from diverse viewpoints. In contrast to existing approaches, wepropose a novel framework built on PanoHead, which excels in synthesizingimages from a 360-degree perspective. To achieve realistic 3D modeling of theinput image, we introduce a dual encoder system tailored for high-fidelityreconstruction and realistic generation from different viewpoints. Accompanyingthis, we propose a stitching framework on the triplane domain to get the bestpredictions from both. To achieve seamless stitching, both encoders must outputconsistent results despite being specialized for different tasks. For thisreason, we carefully train these encoders using specialized losses, includingan adversarial loss based on our novel occlusion-aware triplane discriminator.Experiments reveal that our approach surpasses the existing encoder trainingmethods qualitatively and quantitatively. Please visit the project page:https://berkegokmen1.github.io/dual-enc-3d-gan-inv.</description><author>Bahri Batuhan Bilecen, Ahmet Berke Gokmen, Aysegul Dundar</author><pubDate>Mon, 30 Sep 2024 17:30:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20530v1</guid></item><item><title>Formally Verified Physics-Informed Neural Control Lyapunov Functions</title><link>http://arxiv.org/abs/2409.20528v1</link><description>Control Lyapunov functions are a central tool in the design and analysis ofstabilizing controllers for nonlinear systems. Constructing such functions,however, remains a significant challenge. In this paper, we investigatephysics-informed learning and formal verification of neural network controlLyapunov functions. These neural networks solve a transformedHamilton-Jacobi-Bellman equation, augmented by data generated usingPontryagin's maximum principle. Similar to how Zubov's equation characterizesthe domain of attraction for autonomous systems, this equation characterizesthe null-controllability set of a controlled system. This principled learningof neural network control Lyapunov functions outperforms alternativeapproaches, such as sum-of-squares and rational control Lyapunov functions, asdemonstrated by numerical examples. As an intermediate step, we also presentresults on the formal verification of quadratic control Lyapunov functions,which, aided by satisfiability modulo theories solvers, can performsurprisingly well compared to more sophisticated approaches and efficientlyproduce global certificates of null-controllability.</description><author>Jun Liu, Maxwell Fitzsimmons, Ruikun Zhou, Yiming Meng</author><pubDate>Mon, 30 Sep 2024 17:27:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20528v1</guid></item><item><title>Word Sense Disambiguation in Native Spanish: A Comprehensive Lexical Evaluation Resource</title><link>http://arxiv.org/abs/2409.20524v1</link><description>Human language, while aimed at conveying meaning, inherently carriesambiguity. It poses challenges for speech and language processing, but alsoserves crucial communicative functions. Efficiently solve ambiguity is both adesired and a necessary characteristic. The lexical meaning of a word incontext can be determined automatically by Word Sense Disambiguation (WSD)algorithms that rely on external knowledge often limited and biased towardEnglish. When adapting content to other languages, automated translations arefrequently inaccurate and a high degree of expert human validation is necessaryto ensure both accuracy and understanding. The current study addresses previouslimitations by introducing a new resource for Spanish WSD. It includes a senseinventory and a lexical dataset sourced from the Diccionario de la LenguaEspa\~nola which is maintained by the Real Academia Espa\~nola. We also reviewcurrent resources for Spanish and report metrics on them by a state-of-the-artsystem.</description><author>Pablo Ortega, Jordi Luque, Luis Lamiable, Rodrigo López, Richard Benjamins</author><pubDate>Mon, 30 Sep 2024 17:22:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20524v1</guid></item><item><title>Health-LLM: Personalized Retrieval-Augmented Disease Prediction System</title><link>http://arxiv.org/abs/2402.00746v7</link><description>Recent advancements in artificial intelligence (AI), especially largelanguage models (LLMs), have significantly advanced healthcare applications anddemonstrated potentials in intelligent medical treatment. However, there areconspicuous challenges such as vast data volumes and inconsistent symptomcharacterization standards, preventing full integration of healthcare AIsystems with individual patients' needs. To promote professional andpersonalized healthcare, we propose an innovative framework, Heath-LLM, whichcombines large-scale feature extraction and medical knowledge trade-offscoring. Compared to traditional health management applications, our system hasthree main advantages: (1) It integrates health reports and medical knowledgeinto a large model to ask relevant questions to large language model fordisease prediction; (2) It leverages a retrieval augmented generation (RAG)mechanism to enhance feature extraction; (3) It incorporates a semi-automatedfeature updating framework that can merge and delete features to improveaccuracy of disease prediction. We experiment on a large number of healthreports to assess the effectiveness of Health-LLM system. The results indicatethat the proposed system surpasses the existing ones and has the potential tosignificantly advance disease prediction and personalized health management.</description><author>Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Lizhou Fan, Wenyue Hua, Suiyuan Zhu, Yanda Meng, Zhenting Wang, Mengnan Du, Yongfeng Zhang</author><pubDate>Mon, 30 Sep 2024 17:22:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00746v7</guid></item><item><title>Upper and Lower Bounds for Distributionally Robust Off-Dynamics Reinforcement Learning</title><link>http://arxiv.org/abs/2409.20521v1</link><description>We study off-dynamics Reinforcement Learning (RL), where the policy trainingand deployment environments are different. To deal with this environmentalperturbation, we focus on learning policies robust to uncertainties intransition dynamics under the framework of distributionally robust Markovdecision processes (DRMDPs), where the nominal and perturbed dynamics arelinear Markov Decision Processes. We propose a novel algorithm We-DRIVE-U thatenjoys an average suboptimality $\widetilde{\mathcal{O}}\big({d H \cdot \min\{1/{\rho}, H\}/\sqrt{K} }\big)$, where $K$ is the number of episodes, $H$ isthe horizon length, $d$ is the feature dimension and $\rho$ is the uncertaintylevel. This result improves the state-of-the-art by$\mathcal{O}(dH/\min\{1/\rho,H\})$. We also construct a novel hard instance andderive the first information-theoretic lower bound in this setting, whichindicates our algorithm is near-optimal up to $\mathcal{O}(\sqrt{H})$ for anyuncertainty level $\rho\in(0,1]$. Our algorithm also enjoys a 'rare-switching'design, and thus only requires $\mathcal{O}(dH\log(1+H^2K))$ policy switchesand $\mathcal{O}(d^2H\log(1+H^2K))$ calls for oracle to solve dual optimizationproblems, which significantly improves the computational efficiency of existingalgorithms for DRMDPs, whose policy switch and oracle complexities are both$\mathcal{O}(K)$.</description><author>Zhishuai Liu, Weixin Wang, Pan Xu</author><pubDate>Mon, 30 Sep 2024 17:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20521v1</guid></item><item><title>Accelerating Non-Maximum Suppression: A Graph Theory Perspective</title><link>http://arxiv.org/abs/2409.20520v1</link><description>Non-maximum suppression (NMS) is an indispensable post-processing step inobject detection. With the continuous optimization of network models, NMS hasbecome the ``last mile'' to enhance the efficiency of object detection. Thispaper systematically analyzes NMS from a graph theory perspective for the firsttime, revealing its intrinsic structure. Consequently, we propose twooptimization methods, namely QSI-NMS and BOE-NMS. The former is a fastrecursive divide-and-conquer algorithm with negligible mAP loss, and itsextended version (eQSI-NMS) achieves optimal complexity of $\mathcal{O}(n\logn)$. The latter, concentrating on the locality of NMS, achieves an optimizationat a constant level without an mAP loss penalty. Moreover, to facilitate rapidevaluation of NMS methods for researchers, we introduce NMS-Bench, the firstbenchmark designed to comprehensively assess various NMS methods. Taking theYOLOv8-N model on MS COCO 2017 as the benchmark setup, our method QSI-NMSprovides $6.2\times$ speed of original NMS on the benchmark, with a $0.1\%$decrease in mAP. The optimal eQSI-NMS, with only a $0.3\%$ mAP decrease,achieves $10.7\times$ speed. Meanwhile, BOE-NMS exhibits $5.1\times$ speed withno compromise in mAP.</description><author>King-Siong Si, Lu Sun, Weizhan Zhang, Tieliang Gong, Jiahao Wang, Jiang Liu, Hao Sun</author><pubDate>Mon, 30 Sep 2024 17:20:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20520v1</guid></item><item><title>SMLE: Safe Machine Learning via Embedded Overapproximation</title><link>http://arxiv.org/abs/2409.20517v1</link><description>Despite the extent of recent advances in Machine Learning (ML) and NeuralNetworks, providing formal guarantees on the behavior of these systems is stillan open problem, and a crucial requirement for their adoption in regulated orsafety-critical scenarios. We consider the task of training differentiable MLmodels guaranteed to satisfy designer-chosen properties, stated as input-outputimplications. This is very challenging, due to the computational complexity ofrigorously verifying and enforcing compliance in modern neural models. Weprovide an innovative approach based on three components: 1) a general, simplearchitecture enabling efficient verification with a conservative semantic; 2) arigorous training algorithm based on the Projected Gradient Method; 3) aformulation of the problem of searching for strong counterexamples. Theproposed framework, being only marginally affected by model complexity, scaleswell to practical applications, and produces models that provide full propertysatisfaction guarantees. We evaluate our approach on properties defined bylinear inequalities in regression, and on mutually exclusive classes inmultilabel classification. Our approach is competitive with a baseline thatincludes property enforcement during preprocessing, i.e. on the training data,as well as during postprocessing, i.e. on the model predictions. Finally, ourcontributions establish a framework that opens up multiple research directionsand potential improvements.</description><author>Matteo Francobaldi, Michele Lombardi</author><pubDate>Mon, 30 Sep 2024 17:19:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20517v1</guid></item><item><title>Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity</title><link>http://arxiv.org/abs/2409.18708v2</link><description>We introduce a novel family of adversarial attacks that exploit the inabilityof language models to interpret ASCII art. To evaluate these attacks, wepropose the ToxASCII benchmark and develop two custom ASCII art fonts: oneleveraging special tokens and another using text-filled letter shapes. Ourattacks achieve a perfect 1.0 Attack Success Rate across ten models, includingOpenAI's o1-preview and LLaMA 3.1. Warning: this paper contains examples of toxic language used for researchpurposes.</description><author>Sergey Berezin, Reza Farahbakhsh, Noel Crespi</author><pubDate>Mon, 30 Sep 2024 17:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18708v2</guid></item><item><title>Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties</title><link>http://arxiv.org/abs/2311.17041v3</link><description>A major reason behind the recent success of large language models (LLMs) istheir \textit{in-context learning} capability, which makes it possible torapidly adapt them to downstream text-based tasks by prompting them with asmall number of relevant demonstrations. While large vision-language models(VLMs) have recently been developed for tasks requiring both text and images,they largely lack in-context learning over visual information, especially inunderstanding and generating text about videos. In this work, we implement\textbf{E}mergent \textbf{I}n-context \textbf{Le}arning on \textbf{V}ideos(\eilev{}), a novel training paradigm that induces in-context learning overvideo and text by capturing key properties of pre-training data found by priorwork to be essential for in-context learning in transformers. In ourexperiments, we show that \eilev-trained models outperform other off-the-shelfVLMs in few-shot video narration for novel, rare actions. Furthermore, wedemonstrate that these key properties of bursty distributions, skewed marginaldistributions, and dynamic meaning each contribute to varying degrees to VLMs'in-context learning capability in narrating procedural videos. Our results,analysis, and \eilev{}-trained models yield numerous insights about theemergence of in-context learning over video and text, creating a foundation forfuture work to optimize and scale VLMs for open-domain video understanding andreasoning. Our code and demo are available at\url{https://github.com/yukw777/EILEV}.</description><author>Keunwoo Peter Yu, Zheyuan Zhang, Fengyuan Hu, Shane Storks, Joyce Chai</author><pubDate>Mon, 30 Sep 2024 17:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17041v3</guid></item><item><title>Ensemble WSINDy for Data Driven Discovery of Governing Equations from Laser-based Full-field Measurements</title><link>http://arxiv.org/abs/2409.20510v1</link><description>This work leverages laser vibrometry and the weak form of the sparseidentification of nonlinear dynamics (WSINDy) for partial differentialequations to learn macroscale governing equations from full-field experimentaldata. In the experiments, two beam-like specimens, one aluminum and oneIDOX/Estane composite, are subjected to shear wave excitation in the lowfrequency regime and the response is measured in the form of particle velocityon the specimen surface. The WSINDy for PDEs algorithm is applied to theresulting spatio-temporal data to discover the effective dynamics of thespecimens from a family of potential PDEs. The discovered PDE is of therecognizable Euler-Bernoulli beam model form, from which the Young's modulusfor the two materials are estimated. An ensemble version of the WSINDyalgorithm is also used which results in information about the uncertainty inthe PDE coefficients and Young's moduli. The discovered PDEs are also simulatedwith a finite element code to compare against the experimental data withreasonable accuracy. Using full-field experimental data and WSINDy together isa powerful non-destructive approach for learning unknown governing equationsand gaining insights about mechanical systems in the dynamic regime.</description><author>Abigail C. Schmid, Alireza Doostan, Fatemeh Pourahmadian</author><pubDate>Mon, 30 Sep 2024 17:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20510v1</guid></item><item><title>NUTRIVISION: A System for Automatic Diet Management in Smart Healthcare</title><link>http://arxiv.org/abs/2409.20508v1</link><description>Maintaining health and fitness through a balanced diet is essential forpreventing non communicable diseases such as heart disease, diabetes, andcancer. NutriVision combines smart healthcare with computer vision and machinelearning to address the challenges of nutrition and dietary management. Thispaper introduces a novel system that can identify food items, estimatequantities, and provide comprehensive nutritional information. NutriVisionemploys the Faster Region based Convolutional Neural Network, a deep learningalgorithm that improves object detection by generating region proposals andthen classifying those regions, making it highly effective for accurate andfast food identification even in complex and disorganized meal settings.Through smartphone based image capture, NutriVision delivers instantnutritional data, including macronutrient breakdown, calorie count, andmicronutrient details. One of the standout features of NutriVision is itspersonalized nutritional analysis and diet recommendations, which are tailoredto each user's dietary preferences, nutritional needs, and health history. Byproviding customized advice, NutriVision helps users achieve specific healthand fitness goals, such as managing dietary restrictions or controlling weight.In addition to offering precise food detection and nutritional assessment,NutriVision supports smarter dietary decisions by integrating user data withrecommendations that promote a balanced, healthful diet. This system presents apractical and advanced solution for nutrition management and has the potentialto significantly influence how people approach their dietary choices, promotinghealthier eating habits and overall well being. This paper discusses thedesign, performance evaluation, and prospective applications of the NutriVisionsystem.</description><author>Madhumita Veeramreddy, Ashok Kumar Pradhan, Swetha Ghanta, Laavanya Rachakonda, Saraju P Mohanty</author><pubDate>Mon, 30 Sep 2024 17:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20508v1</guid></item><item><title>What Information Contributes to Log-based Anomaly Detection? Insights from a Configurable Transformer-Based Approach</title><link>http://arxiv.org/abs/2409.20503v1</link><description>Log data are generated from logging statements in the source code, providinginsights into the execution processes of software applications and systems.State-of-the-art log-based anomaly detection approaches typically leverage deeplearning models to capture the semantic or sequential information in the logdata and detect anomalous runtime behaviors. However, the impacts of thesedifferent types of information are not clear. In addition, existing approacheshave not captured the timestamps in the log data, which can potentially providemore fine-grained temporal information than sequential information. In thiswork, we propose a configurable transformer-based anomaly detection model thatcan capture the semantic, sequential, and temporal information in the log dataand allows us to configure the different types of information as the model'sfeatures. Additionally, we train and evaluate the proposed model using logsequences of different lengths, thus overcoming the constraint of existingmethods that rely on fixed-length or time-windowed log sequences as inputs.With the proposed model, we conduct a series of experiments with differentcombinations of input features to evaluate the roles of different types ofinformation in anomaly detection. When presented with log sequences of varyinglengths, the model can attain competitive and consistently stable performancecompared to the baselines. The results indicate that the event occurrenceinformation plays a key role in identifying anomalies, while the impact of thesequential and temporal information is not significant for anomaly detection inthe studied public datasets. On the other hand, the findings also reveal thesimplicity of the studied public datasets and highlight the importance ofconstructing new datasets that contain different types of anomalies to betterevaluate the performance of anomaly detection models.</description><author>Xingfang Wu, Heng Li, Foutse Khomh</author><pubDate>Mon, 30 Sep 2024 17:03:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20503v1</guid></item><item><title>MemBench: Memorized Image Trigger Prompt Dataset for Diffusion Models</title><link>http://arxiv.org/abs/2407.17095v2</link><description>Diffusion models have achieved remarkable success in Text-to-Image generationtasks, leading to the development of many commercial models. However, recentstudies have reported that diffusion models often generate replicated images intrain data when triggered by specific prompts, potentially raising socialissues ranging from copyright to privacy concerns. To sidestep thememorization, there have been recent studies for developing memorizationmitigation methods for diffusion models. Nevertheless, the lack of benchmarksimpedes the assessment of the true effectiveness of these methods. In thiswork, we present MemBench, the first benchmark for evaluating imagememorization mitigation methods. Our benchmark includes a large number ofmemorized image trigger prompts in various Text-to-Image diffusion models.Furthermore, in contrast to the prior work evaluating mitigation performanceonly on trigger prompts, we present metrics evaluating on both trigger promptsand general prompts, so that we can see whether mitigation methods address thememorization issue while maintaining performance for general prompts. This isan important development considering the practical applications which previousworks have overlooked. Through evaluation on MemBench, we verify that theperformance of existing image memorization mitigation methods is stillinsufficient for application to diffusion models. The code and datasets areavailable at https://github.com/chunsanHong/MemBench\_code.</description><author>Chunsan Hong, Tae-Hyun Oh, Minhyuk Sung</author><pubDate>Mon, 30 Sep 2024 17:02:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17095v2</guid></item><item><title>COLLAGE: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models</title><link>http://arxiv.org/abs/2409.20502v1</link><description>We propose a novel framework COLLAGE for generating collaborativeagent-object-agent interactions by leveraging large language models (LLMs) andhierarchical motion-specific vector-quantized variational autoencoders(VQ-VAEs). Our model addresses the lack of rich datasets in this domain byincorporating the knowledge and reasoning abilities of LLMs to guide agenerative diffusion model. The hierarchical VQ-VAE architecture capturesdifferent motion-specific characteristics at multiple levels of abstraction,avoiding redundant concepts and enabling efficient multi-resolutionrepresentation. We introduce a diffusion model that operates in the latentspace and incorporates LLM-generated motion planning cues to guide thedenoising process, resulting in prompt-specific motion generation with greatercontrol and diversity. Experimental results on the CORE-4D, and InterHumandatasets demonstrate the effectiveness of our approach in generating realisticand diverse collaborative human-object-human interactions, outperformingstate-of-the-art methods. Our work opens up new possibilities for modelingcomplex interactions in various domains, such as robotics, graphics andcomputer vision.</description><author>Divyanshu Daiya, Damon Conover, Aniket Bera</author><pubDate>Mon, 30 Sep 2024 17:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20502v1</guid></item><item><title>FreeMask: Rethinking the Importance of Attention Masks for Zero-Shot Video Editing</title><link>http://arxiv.org/abs/2409.20500v1</link><description>Text-to-video diffusion models have made remarkable advancements. Driven bytheir ability to generate temporally coherent videos, research on zero-shotvideo editing using these fundamental models has expanded rapidly. To enhanceediting quality, structural controls are frequently employed in video editing.Among these techniques, cross-attention mask control stands out for itseffectiveness and efficiency. However, when cross-attention masks are naivelyapplied to video editing, they can introduce artifacts such as blurring andflickering. Our experiments uncover a critical factor overlooked in previousvideo editing research: cross-attention masks are not consistently clear butvary with model structure and denoising timestep. To address this issue, wepropose the metric Mask Matching Cost (MMC) that quantifies this variabilityand propose FreeMask, a method for selecting optimal masks tailored to specificvideo editing tasks. Using MMC-selected masks, we further improve the maskedfusion mechanism within comprehensive attention features, e.g., temp, cross,and self-attention modules. Our approach can be seamlessly integrated intoexisting zero-shot video editing frameworks with better performance, requiringno control assistance or parameter fine-tuning but enabling adaptive decouplingof unedited semantic layouts with mask precision control. Extensive experimentsdemonstrate that FreeMask achieves superior semantic fidelity, temporalconsistency, and editing quality compared to state-of-the-art methods.</description><author>Lingling Cai, Kang Zhao, Hangjie Yuan, Yingya Zhang, Shiwei Zhang, Kejie Huang</author><pubDate>Mon, 30 Sep 2024 17:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20500v1</guid></item><item><title>Enhancing Romanian Offensive Language Detection through Knowledge Distillation, Multi-Task Learning, and Data Augmentation</title><link>http://arxiv.org/abs/2409.20498v1</link><description>This paper highlights the significance of natural language processing (NLP)within artificial intelligence, underscoring its pivotal role in comprehendingand modeling human language. Recent advancements in NLP, particularly inconversational bots, have garnered substantial attention and adoption amongdevelopers. This paper explores advanced methodologies for attaining smallerand more efficient NLP models. Specifically, we employ three key approaches:(1) training a Transformer-based neural network to detect offensive language,(2) employing data augmentation and knowledge distillation techniques toincrease performance, and (3) incorporating multi-task learning with knowledgedistillation and teacher annealing using diverse datasets to enhanceefficiency. The culmination of these methods has yielded demonstrably improvedoutcomes.</description><author>Vlad-Cristian Matei, Iulian-Marius Tăiatu, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel</author><pubDate>Mon, 30 Sep 2024 16:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20498v1</guid></item><item><title>Online Decision Deferral under Budget Constraints</title><link>http://arxiv.org/abs/2409.20489v1</link><description>Machine Learning (ML) models are increasingly used to support or substitutedecision making. In applications where skilled experts are a limited resource,it is crucial to reduce their burden and automate decisions when theperformance of an ML model is at least of equal quality. However, models areoften pre-trained and fixed, while tasks arrive sequentially and theirdistribution may shift. In that case, the respective performance of thedecision makers may change, and the deferral algorithm must remain adaptive. Wepropose a contextual bandit model of this online decision making problem. Ourframework includes budget constraints and different types of partial feedbackmodels. Beyond the theoretical guarantees of our algorithm, we proposeefficient extensions that achieve remarkable performance on real-worlddatasets.</description><author>Mirabel Reid, Tom Sühr, Claire Vernade, Samira Samadi</author><pubDate>Mon, 30 Sep 2024 16:53:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20489v1</guid></item><item><title>"What" x "When" working memory representations using Laplace Neural Manifolds</title><link>http://arxiv.org/abs/2409.20484v1</link><description>Working memory $\unicode{x2013}$ the ability to remember recent events asthey recede continuously into the past $\unicode{x2013}$ requires the abilityto represent any stimulus at any time delay. This property requires neuronscoding working memory to show mixed selectivity, with conjunctive receptivefields (RFs) for stimuli and time, forming a representation of 'what' $\times$'when'. We study the properties of such a working memory in simple experimentswhere a single stimulus must be remembered for a short time. The requirement ofconjunctive receptive fields allows the covariance matrix of the network todecouple neatly, allowing an understanding of the low-dimensional dynamics ofthe population. Different choices of temporal basis functions lead toqualitatively different dynamics. We study a specific choice $\unicode{x2013}$a Laplace space with exponential basis functions for time coupled to an"Inverse Laplace" space with circumscribed basis functions in time. We refer tothis choice with basis functions that evenly tile log time as a Laplace NeuralManifold. Despite the fact that they are related to one another by a linearprojection, the Laplace population shows a stable stimulus-specific subspacewhereas the Inverse Laplace population shows rotational dynamics. The growth ofthe rank of the covariance matrix with time depends on the density of thetemporal basis set; logarithmic tiling shows good agreement with data. Wesketch a continuous attractor CANN that constructs a Laplace Neural Manifold.The attractor in the Laplace space appears as an edge; the attractor for theinverse space appears as a bump. This work provides a map for going from moreabstract cognitive models of WM to circuit-level implementation usingcontinuous attractor neural networks, and places constraints on the types ofneural dynamics that support working memory.</description><author>Aakash Sarkar, Chenyu Wang, Shangfu Zuo, Marc W. Howard</author><pubDate>Mon, 30 Sep 2024 16:47:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20484v1</guid></item><item><title>RecSys Challenge 2024: Balancing Accuracy and Editorial Values in News Recommendations</title><link>http://arxiv.org/abs/2409.20483v1</link><description>The RecSys Challenge 2024 aims to advance news recommendation by addressingboth the technical and normative challenges inherent in designing effective andresponsible recommender systems for news publishing. This paper describes thechallenge, including its objectives, problem setting, and the dataset providedby the Danish news publishers Ekstra Bladet and JP/Politikens Media Group("Ekstra Bladet"). The challenge explores the unique aspects of newsrecommendation, such as modeling user preferences based on behavior, accountingfor the influence of the news agenda on user interests, and managing the rapiddecay of news items. Additionally, the challenge embraces normativecomplexities, investigating the effects of recommender systems on news flow andtheir alignment with editorial values. We summarize the challenge setup,dataset characteristics, and evaluation metrics. Finally, we announce thewinners and highlight their contributions. The dataset is available at:https://recsys.eb.dk.</description><author>Johannes Kruse, Kasper Lindskow, Saikishore Kalloori, Marco Polignano, Claudio Pomo, Abhishek Srivastava, Anshuk Uppal, Michael Riis Andersen, Jes Frellsen</author><pubDate>Mon, 30 Sep 2024 16:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20483v1</guid></item><item><title>The African Woman is Rhythmic and Soulful: An Investigation of Implicit Biases in LLM Open-ended Text Generation</title><link>http://arxiv.org/abs/2407.01270v2</link><description>This paper investigates the subtle and often concealed biases present inLarge Language Models (LLMs), focusing on implicit biases that may remaindespite passing explicit bias tests. Implicit biases are significant becausethey influence the decisions made by these systems, potentially perpetuatingstereotypes and discrimination, even when LLMs appear to function fairly.Traditionally, explicit bias tests or embedding-based methods are employed todetect bias, but these approaches can overlook more nuanced, implicit forms ofbias. To address this, we introduce two novel psychological-inspiredmethodologies: the LLM Implicit Association Test (IAT) Bias and the LLMDecision Bias, designed to reveal and measure implicit biases throughprompt-based and decision-making tasks. Additionally, open-ended generationtasks with thematic analysis of word generations and storytelling providequalitative insights into the model's behavior. Our findings demonstrate thatthe LLM IAT Bias correlates with traditional methods and more effectivelypredicts downstream behaviors, as measured by the LLM Decision Bias, offering amore comprehensive framework for detecting subtle biases in AI systems. Thisresearch advances the field of AI ethics by proposing new methods tocontinually assess and mitigate biases in LLMs, highlighting the importance ofqualitative and decision-focused evaluations to address challenges thatprevious approaches have not fully captured.</description><author>Serene Lim, María Pérez-Ortiz</author><pubDate>Mon, 30 Sep 2024 16:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01270v2</guid></item><item><title>VideoPatchCore: An Effective Method to Memorize Normality for Video Anomaly Detection</title><link>http://arxiv.org/abs/2409.16225v4</link><description>Video anomaly detection (VAD) is a crucial task in video analysis andsurveillance within computer vision. Currently, VAD is gaining attention withmemory techniques that store the features of normal frames. The stored featuresare utilized for frame reconstruction, identifying an abnormality when asignificant difference exists between the reconstructed and input frames.However, this approach faces several challenges due to the simultaneousoptimization required for both the memory and encoder-decoder model. Thesechallenges include increased optimization difficulty, complexity ofimplementation, and performance variability depending on the memory size. Toaddress these challenges,we propose an effective memory method for VAD, calledVideoPatchCore. Inspired by PatchCore, our approach introduces a structure thatprioritizes memory optimization and configures three types of memory tailoredto the characteristics of video data. This method effectively addresses thelimitations of existing memory-based methods, achieving good performancecomparable to state-of-the-art methods. Furthermore, our method requires notraining and is straightforward to implement, making VAD tasks more accessible.Our code is available online at github.com/SkiddieAhn/Paper-VideoPatchCore.</description><author>Sunghyun Ahn, Youngwan Jo, Kijung Lee, Sanghyun Park</author><pubDate>Mon, 30 Sep 2024 16:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16225v4</guid></item><item><title>IRFusionFormer: Enhancing Pavement Crack Segmentation with RGB-T Fusion and Topological-Based Loss</title><link>http://arxiv.org/abs/2409.20474v1</link><description>Crack segmentation is crucial in civil engineering, particularly forassessing pavement integrity and ensuring the durability of infrastructure.While deep learning has advanced RGB-based segmentation, performance degradesunder adverse conditions like low illumination or motion blur. Thermal imagingoffers complementary information by capturing emitted radiation, improvingcrack detection in challenging environments. Combining RGB and thermal images(RGB-T) for crack segmentation shows promise in complex real-world conditions,such as adverse weather, yet research in this area remains limited. CurrentRGB-T segmentation methods often fail to fully exploit the complementaryrelationships between modalities at various levels of interaction. To addressthis, we propose IRFusionFormer, a novel model for crack segmentation thateffectively integrates RGB and thermal data. Our Efficient RGB-T Cross FusionModule captures multi-scale relationships and long-range dependencies betweenmodalities without significant computational overhead. Additionally, weintroduce the Interaction-Hybrid-Branch-Supervision framework, which enhancesinteraction between modalities by distributing fused features across brancheswith joint supervision. To maintain the topological structure of cracks, weintroduce a novel topology-based loss function that preserves connectivityduring training. Our method achieves state-of-the-art performance, with a Dicescore of 90.01% and an IoU of 81.83%, significantly improving robustness andaccuracy in varying environmental conditions. These advancements address keychallenges in pavement crack segmentation, offering a more reliable andefficient solution. For access to the codes, data, and models from this study,visit https://github.com/sheauhuu/IRFusionFormer</description><author>Ruiqiang Xiao, Xiaohu Chen</author><pubDate>Mon, 30 Sep 2024 16:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20474v1</guid></item><item><title>Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey</title><link>http://arxiv.org/abs/2409.18169v2</link><description>Recent research demonstrates that the nascent fine-tuning-as-a-servicebusiness model exposes serious safety concerns -- fine-tuning over a fewharmful data uploaded by the users can compromise the safety alignment of themodel. The attack, known as harmful fine-tuning, has raised a broad researchinterest among the community. However, as the attack is still new, \textbf{weobserve from our miserable submission experience that there are generalmisunderstandings within the research community.} We in this paper aim to clearsome common concerns for the attack setting, and formally establish theresearch problem. Specifically, we first present the threat model of theproblem, and introduce the harmful fine-tuning attack and its variants. Then wesystematically survey the existing literature on attacks/defenses/mechanicalanalysis of the problem. Finally, we outline future research directions thatmight contribute to the development of the field. Additionally, we present alist of questions of interest, which might be useful to refer to when reviewersin the peer review process question the realism of theexperiment/attack/defense setting. A curated list of relevant papers ismaintained and made accessible at:\url{https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers}.</description><author>Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu</author><pubDate>Mon, 30 Sep 2024 16:29:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18169v2</guid></item><item><title>Continual Human Pose Estimation for Incremental Integration of Keypoints and Pose Variations</title><link>http://arxiv.org/abs/2409.20469v1</link><description>This paper reformulates cross-dataset human pose estimation as a continuallearning task, aiming to integrate new keypoints and pose variations intoexisting models without losing accuracy on previously learned datasets. Webenchmark this formulation against established regularization-based methods formitigating catastrophic forgetting, including EWC, LFL, and LwF. Moreover, wepropose a novel regularization method called Importance-Weighted Distillation(IWD), which enhances conventional LwF by introducing a layer-wise distillationpenalty and dynamic temperature adjustment based on layer importance forpreviously learned knowledge. This allows for a controlled adaptation to newtasks that respects the stability-plasticity balance critical in continuallearning. Through extensive experiments across three datasets, we demonstratethat our approach outperforms existing regularization-based continual learningstrategies. IWD shows an average improvement of 3.60\% over thestate-of-the-art LwF method. The results highlight the potential of our methodto serve as a robust framework for real-world applications where models mustevolve with new data without forgetting past knowledge.</description><author>Muhammad Saif Ullah Khan, Muhammad Ahmed Ullah Khan, Muhammad Zeshan Afzal, Didier Stricker</author><pubDate>Mon, 30 Sep 2024 16:29:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20469v1</guid></item><item><title>A Weakly Supervised Data Labeling Framework for Machine Lexical Normalization in Vietnamese Social Media</title><link>http://arxiv.org/abs/2409.20467v1</link><description>This study introduces an innovative automatic labeling framework to addressthe challenges of lexical normalization in social media texts for low-resourcelanguages like Vietnamese. Social media data is rich and diverse, but theevolving and varied language used in these contexts makes manual labelinglabor-intensive and expensive. To tackle these issues, we propose a frameworkthat integrates semi-supervised learning with weak supervision techniques. Thisapproach enhances the quality of training dataset and expands its size whileminimizing manual labeling efforts. Our framework automatically labels rawdata, converting non-standard vocabulary into standardized forms, therebyimproving the accuracy and consistency of the training data. Experimentalresults demonstrate the effectiveness of our weak supervision framework innormalizing Vietnamese text, especially when utilizing Pre-trained LanguageModels. The proposed framework achieves an impressive F1-score of 82.72% andmaintains vocabulary integrity with an accuracy of up to 99.22%. Additionally,it effectively handles undiacritized text under various conditions. Thisframework significantly enhances natural language normalization quality andimproves the accuracy of various NLP tasks, leading to an average accuracyincrease of 1-3%.</description><author>Dung Ha Nguyen, Anh Thi Hoang Nguyen, Kiet Van Nguyen</author><pubDate>Mon, 30 Sep 2024 16:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20467v1</guid></item><item><title>Language Resources in Spanish for Automatic Text Simplification across Domains</title><link>http://arxiv.org/abs/2409.20466v1</link><description>This work describes the language resources and models developed for automaticsimplification of Spanish texts in three domains: Finance, Medicine and Historystudies. We created several corpora in each domain, annotation andsimplification guidelines, a lexicon of technical and simplified medical terms,datasets used in shared tasks for the financial domain, and two simplificationtools. The methodology, resources and companion publications are sharedpublicly on the web-site: https://clara-nlp.uned.es/.</description><author>Antonio Moreno-Sandoval, Leonardo Campillos-Llanos, Ana García-Serrano</author><pubDate>Mon, 30 Sep 2024 16:26:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.20466v1</guid></item></channel></rss>