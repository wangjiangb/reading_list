<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 06 Dec 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ReconFusion: 3D Reconstruction with Diffusion Priors</title><link>http://arxiv.org/abs/2312.02981v1</link><description>3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel atrendering photorealistic novel views of complex scenes. However, recovering ahigh-quality NeRF typically requires tens to hundreds of input images,resulting in a time-consuming capture process. We present ReconFusion toreconstruct real-world scenes using only a few photos. Our approach leverages adiffusion prior for novel view synthesis, trained on synthetic and multiviewdatasets, which regularizes a NeRF-based 3D reconstruction pipeline at novelcamera poses beyond those captured by the set of input images. Our methodsynthesizes realistic geometry and texture in underconstrained regions whilepreserving the appearance of observed regions. We perform an extensiveevaluation across various real-world datasets, including forward-facing and360-degree scenes, demonstrating significant performance improvements overprevious few-view NeRF reconstruction approaches.</description><author>Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, Aleksander Holynski</author><pubDate>Tue, 05 Dec 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02981v1</guid></item><item><title>GPT4Point: A Unified Framework for Point-Language Understanding and Generation</title><link>http://arxiv.org/abs/2312.02980v1</link><description>Multimodal Large Language Models (MLLMs) have excelled in 2D image-textcomprehension and image generation, but their understanding of the 3D world isnotably deficient, limiting progress in 3D language understanding andgeneration. To solve this problem, we introduce GPT4Point, an innovativegroundbreaking point-language multimodal model designed specifically forunified 3D object understanding and generation within the MLLM framework.GPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-textreference tasks such as point-cloud captioning and Q&amp;A. Additionally, GPT4Pointis equipped with advanced capabilities for controllable 3D generation, it canget high-quality results through a low-quality point-text feature maintainingthe geometric shapes and colors. To support the expansive needs of 3Dobject-text pairs, we develop Pyramid-XL, a point-language dataset annotationengine. It constructs a large-scale database over 1M objects of varied textgranularity levels from the Objaverse-XL dataset, essential for trainingGPT4Point. A comprehensive benchmark has been proposed to evaluate 3Dpoint-language understanding capabilities. In extensive evaluations, GPT4Pointhas demonstrated superior performance in understanding and generation.</description><author>Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, Hengshuang Zhao</author><pubDate>Tue, 05 Dec 2023 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02980v1</guid></item><item><title>Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World</title><link>http://arxiv.org/abs/2312.02976v1</link><description>Reinforcement learning (RL) with dense rewards and imitation learning (IL)with human-generated trajectories are the most widely used approaches fortraining modern embodied agents. RL requires extensive reward shaping andauxiliary losses and is often too slow and ineffective for long-horizon tasks.While IL with human supervision is effective, collecting human trajectories atscale is extremely expensive. In this work, we show that imitatingshortest-path planners in simulation produces agents that, given a languageinstruction, can proficiently navigate, explore, and manipulate objects in bothsimulation and in the real world using only RGB sensors (no depth map or GPScoordinates). This surprising result is enabled by our end-to-end,transformer-based, SPOC architecture, powerful visual encoders paired withextensive image augmentation, and the dramatic scale and diversity of ourtraining data: millions of frames of shortest-path-expert trajectoriescollected inside approximately 200,000 procedurally generated houses containing40,000 unique 3D assets. Our models, data, training code, and newly proposed10-task benchmarking suite CHORES will be open-sourced.</description><author>Kiana Ehsani, Tanmay Gupta, Rose Hendrix, Jordi Salvador, Luca Weihs, Kuo-Hao Zeng, Kunal Pratap Singh, Yejin Kim, Winson Han, Alvaro Herrasti, Ranjay Krishna, Dustin Schwenk, Eli VanderBilt, Aniruddha Kembhavi</author><pubDate>Tue, 05 Dec 2023 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02976v1</guid></item><item><title>Dexterous Functional Grasping</title><link>http://arxiv.org/abs/2312.02975v1</link><description>While there have been significant strides in dexterous manipulation, most ofit is limited to benchmark tasks like in-hand reorientation which are oflimited utility in the real world. The main benefit of dexterous hands overtwo-fingered ones is their ability to pickup tools and other objects (includingthin ones) and grasp them firmly to apply force. However, this task requiresboth a complex understanding of functional affordances as well as preciselow-level control. While prior work obtains affordances from human data thisapproach doesn't scale to low-level control. Similarly, simulation trainingcannot give the robot an understanding of real-world semantics. In this paper,we aim to combine the best of both worlds to accomplish functional grasping forin-the-wild objects. We use a modular approach. First, affordances are obtainedby matching corresponding regions of different objects and then a low-levelpolicy trained in sim is run to grasp it. We propose a novel application ofeigengrasps to reduce the search space of RL using a small amount of human dataand find that it leads to more stable and physically realistic motion. We findthat eigengrasp action space beats baselines in simulation and outperformshardcoded grasping in real and matches or outperforms a trained humanteleoperator. Results visualizations and videos at https://dexfunc.github.io/</description><author>Ananye Agarwal, Shagun Uppal, Kenneth Shaw, Deepak Pathak</author><pubDate>Tue, 05 Dec 2023 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02975v1</guid></item><item><title>Describing Differences in Image Sets with Natural Language</title><link>http://arxiv.org/abs/2312.02974v1</link><description>How do two sets of images differ? Discerning set-level differences is crucialfor understanding model behaviors and analyzing datasets, yet manually siftingthrough thousands of images is impractical. To aid in this discovery process,we explore the task of automatically describing the differences between two$\textbf{sets}$ of images, which we term Set Difference Captioning. This tasktakes in image sets $D_A$ and $D_B$, and outputs a description that is moreoften true on $D_A$ than $D_B$. We outline a two-stage approach that firstproposes candidate difference descriptions from image sets and then re-ranksthe candidates by checking how well they can differentiate the two sets. Weintroduce VisDiff, which first captions the images and prompts a language modelto propose candidate descriptions, then re-ranks these descriptions using CLIP.To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired imagesets with ground truth difference descriptions. We apply VisDiff to variousdomains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparingclassification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizingmodel failure modes (supervised ResNet), characterizing differences betweengenerative models (e.g., StableDiffusionV1 and V2), and discovering what makesimages memorable. Using VisDiff, we are able to find interesting and previouslyunknown differences in datasets and models, demonstrating its utility inrevealing nuanced insights.</description><author>Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, Serena Yeung-Levy</author><pubDate>Tue, 05 Dec 2023 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02974v1</guid></item><item><title>GauHuman: Articulated Gaussian Splatting from Monocular Human Videos</title><link>http://arxiv.org/abs/2312.02973v1</link><description>We present, GauHuman, a 3D human model with Gaussian Splatting for both fasttraining (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared withexisting NeRF-based implicit representation modelling frameworks demandinghours of training and seconds of rendering per frame. Specifically, GauHumanencodes Gaussian Splatting in the canonical space and transforms 3D Gaussiansfrom canonical space to posed space with linear blend skinning (LBS), in whicheffective pose and LBS refinement modules are designed to learn fine details of3D humans under negligible computational cost. Moreover, to enable fastoptimization of GauHuman, we initialize and prune 3D Gaussians with 3D humanprior, while splitting/cloning via KL divergence guidance, along with a novelmerge operation for further speeding up. Extensive experiments on ZJU_Mocap andMonoCap datasets demonstrate that GauHuman achieves state-of-the-artperformance quantitatively and qualitatively with fast training and real-timerendering speed. Notably, without sacrificing rendering quality, GauHuman canfast model the 3D human performer with ~13k 3D Gaussians.</description><author>Shoukang Hu, Ziwei Liu</author><pubDate>Tue, 05 Dec 2023 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02973v1</guid></item><item><title>Alchemist: Parametric Control of Material Properties with Diffusion Models</title><link>http://arxiv.org/abs/2312.02970v1</link><description>We propose a method to control material attributes of objects like roughness,metallic, albedo, and transparency in real images. Our method capitalizes onthe generative prior of text-to-image models known for photorealism, employinga scalar value and instructions to alter low-level material properties.Addressing the lack of datasets with controlled material attributes, wegenerated an object-centric synthetic dataset with physically-based materials.Fine-tuning a modified pre-trained text-to-image model on this syntheticdataset enables us to edit material properties in real-world images whilepreserving all other attributes. We show the potential application of our modelto material edited NeRFs.</description><author>Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William T. Freeman, Mark Matthews</author><pubDate>Tue, 05 Dec 2023 18:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02970v1</guid></item><item><title>Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models</title><link>http://arxiv.org/abs/2312.02969v1</link><description>Listwise rerankers based on large language models (LLM) are the zero-shotstate-of-the-art. However, current works in this direction all depend on theGPT models, making it a single point of failure in scientific reproducibility.Moreover, it raises the concern that the current research findings only holdfor GPT models but not LLM in general. In this work, we lift this pre-conditionand build for the first time effective listwise rerankers without any form ofdependency on GPT. Our passage retrieval experiments show that our best list sereranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves97% effectiveness of the ones built on GPT-4. Our results also show that theexisting training datasets, which were expressly constructed for pointwiseranking, are insufficient for building such listwise rerankers. Instead,high-quality listwise ranking data is required and crucial, calling for furtherwork on building human-annotated listwise data resources.</description><author>Xinyu Zhang, Sebastian Hofstätter, Patrick Lewis, Raphael Tang, Jimmy Lin</author><pubDate>Tue, 05 Dec 2023 18:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02969v1</guid></item><item><title>Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal Inference</title><link>http://arxiv.org/abs/2311.18826v2</link><description>This manuscript enriches the framework of continuous normalizing flows (CNFs)within causal inference, primarily to augment the geometric properties ofparametric submodels used in targeted maximum likelihood estimation (TMLE). Byintroducing an innovative application of CNFs, we construct a refined series ofparametric submodels that enable a directed interpolation between the priordistribution $p_0$ and the empirical distribution $p_1$. This proposedmethodology serves to optimize the semiparametric efficiency bound in causalinference by orchestrating CNFs to align with Wasserstein gradient flows. Ourapproach not only endeavors to minimize the mean squared error in theestimation but also imbues the estimators with geometric sophistication,thereby enhancing robustness against misspecification. This robustness iscrucial, as it alleviates the dependence on the standard $n^{\frac{1}{4}}$ ratefor a doubly-robust perturbation direction in TMLE. By incorporating robustoptimization principles and differential geometry into the estimators, thedeveloped geometry-aware CNFs represent a significant advancement in thepursuit of doubly robust causal inference.</description><author>Kaiwen Hou</author><pubDate>Tue, 05 Dec 2023 18:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18826v2</guid></item><item><title>AmbiGen: Generating Ambigrams from Pre-trained Diffusion Model</title><link>http://arxiv.org/abs/2312.02967v1</link><description>Ambigrams are calligraphic designs that have different meanings depending onthe viewing orientation. Creating ambigrams is a challenging task even forskilled artists, as it requires maintaining the meaning under two differentviewpoints at the same time. In this work, we propose to generate ambigrams bydistilling a large-scale vision and language diffusion model, namely DeepFloydIF, to optimize the letters' outline for legibility in the two viewingorientations. Empirically, we demonstrate that our approach outperformsexisting ambigram generation methods. On the 500 most common words in English,our method achieves more than an 11.6% increase in word accuracy and at least a41.9% reduction in edit distance.</description><author>Boheng Zhao, Rana Hanocka, Raymond A. Yeh</author><pubDate>Tue, 05 Dec 2023 18:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02967v1</guid></item><item><title>Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection</title><link>http://arxiv.org/abs/2312.02966v1</link><description>Semi-supervised object detection is crucial for 3D scene understanding,efficiently addressing the limitation of acquiring large-scale 3D bounding boxannotations. Existing methods typically employ a teacher-student framework withpseudo-labeling to leverage unlabeled point clouds. However, producing reliablepseudo-labels in a diverse 3D space still remains challenging. In this work, wepropose Diffusion-SS3D, a new perspective of enhancing the quality ofpseudo-labels via the diffusion model for semi-supervised 3D object detection.Specifically, we include noises to produce corrupted 3D object size and classlabel distributions, and then utilize the diffusion model as a denoisingprocess to obtain bounding box outputs. Moreover, we integrate the diffusionmodel into the teacher-student framework, so that the denoised bounding boxescan be used to improve pseudo-label generation, as well as the entiresemi-supervised learning process. We conduct experiments on the ScanNet and SUNRGB-D benchmark datasets to demonstrate that our approach achievesstate-of-the-art performance against existing methods. We also presentextensive analysis to understand how our diffusion model design affectsperformance in semi-supervised learning.</description><author>Cheng-Ju Ho, Chen-Hsuan Tai, Yen-Yu Lin, Ming-Hsuan Yang, Yi-Hsuan Tsai</author><pubDate>Tue, 05 Dec 2023 18:54:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02966v1</guid></item><item><title>MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures</title><link>http://arxiv.org/abs/2312.02963v1</link><description>In this era, the success of large language models and text-to-image modelscan be attributed to the driving force of large-scale datasets. However, in therealm of 3D vision, while remarkable progress has been made with models trainedon large-scale synthetic and real-captured object data like Objaverse andMVImgNet, a similar level of progress has not been observed in the domain ofhuman-centric tasks partially due to the lack of a large-scale human dataset.Existing datasets of high-fidelity 3D human capture continue to be mid-sizeddue to the significant challenges in acquiring large-scale high-quality 3Dhuman data. To bridge this gap, we present MVHumanNet, a dataset that comprisesmulti-view human action sequences of 4,500 human identities. The primary focusof our work is on collecting human data that features a large number of diverseidentities and everyday clothing using a multi-view human capture system, whichfacilitates easily scalable data collection. Our dataset contains 9,000 dailyoutfits, 60,000 motion sequences and 645 million frames with extensiveannotations, including human masks, camera parameters, 2D and 3D keypoints,SMPL/SMPLX parameters, and corresponding textual descriptions. To explore thepotential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilotstudies on view-consistent action recognition, human NeRF reconstruction,text-driven view-unconstrained human image generation, as well as 2Dview-unconstrained human image and 3D avatar generation. Extensive experimentsdemonstrate the performance improvements and effective applications enabled bythe scale provided by MVHumanNet. As the current largest-scale 3D humandataset, we hope that the release of MVHumanNet data with annotations willfoster further innovations in the domain of 3D human-centric tasks at scale.</description><author>Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, Shuguang Cui, Xiaoguang Han</author><pubDate>Tue, 05 Dec 2023 18:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02963v1</guid></item><item><title>Detecting algorithmic bias in medical AI-models</title><link>http://arxiv.org/abs/2312.02959v1</link><description>With the growing prevalence of machine learning and artificialintelligence-based medical decision support systems, it is equally important toensure that these systems provide patient outcomes in a fair and equitablefashion. This paper presents an innovative framework for detecting areas ofalgorithmic bias in medical-AI decision support systems. Our approachefficiently identifies potential biases in medical-AI models, specifically inthe context of sepsis prediction, by employing the Classification andRegression Trees (CART) algorithm. We verify our methodology by conducting aseries of synthetic data experiments, showcasing its ability to estimate areasof bias in controlled settings precisely. The effectiveness of the concept isfurther validated by experiments using electronic medical records from GradyMemorial Hospital in Atlanta, Georgia. These tests demonstrate the practicalimplementation of our strategy in a clinical environment, where it can functionas a vital instrument for guaranteeing fairness and equity in AI-based medicaldecisions.</description><author>Jeffrey Smith, Andre Holder, Rishikesan Kamaleswaran, Yao Xie</author><pubDate>Tue, 05 Dec 2023 18:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02959v1</guid></item><item><title>Harnessing Discrete Representations For Continual Reinforcement Learning</title><link>http://arxiv.org/abs/2312.01203v2</link><description>Reinforcement learning (RL) agents make decisions using nothing butobservations from the environment, and consequently, heavily rely on therepresentations of those observations. Though some recent breakthroughs haveused vector-based categorical representations of observations, often referredto as discrete representations, there is little work explicitly assessing thesignificance of such a choice. In this work, we provide a thorough empiricalinvestigation of the advantages of representing observations as vectors ofcategorical values within the context of reinforcement learning. We performevaluations on world-model learning, model-free RL, and ultimately continual RLproblems, where the benefits best align with the needs of the problem setting.We find that, when compared to traditional continuous representations, worldmodels learned over discrete representations accurately model more of the worldwith less capacity, and that agents trained with discrete representations learnbetter policies with less data. In the context of continual RL, these benefitstranslate into faster adapting agents. Additionally, our analysis suggests thatthe observed performance improvements can be attributed to the informationcontained within the latent vectors and potentially the encoding of thediscrete representation itself.</description><author>Edan Meyer, Adam White, Marlos C. Machado</author><pubDate>Tue, 05 Dec 2023 18:45:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01203v2</guid></item><item><title>Classification for everyone : Building geography agnostic models for fairer recognition</title><link>http://arxiv.org/abs/2312.02957v1</link><description>In this paper, we analyze different methods to mitigate inherent geographicalbiases present in state of the art image classification models. We firstquantitatively present this bias in two datasets - The Dollar Street Datasetand ImageNet, using images with location information. We then present differentmethods which can be employed to reduce this bias. Finally, we analyze theeffectiveness of the different techniques on making these models more robust togeographical locations of the images.</description><author>Akshat Jindal, Shreya Singh, Soham Gadgil</author><pubDate>Tue, 05 Dec 2023 18:41:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02957v1</guid></item><item><title>Choroidalyzer: An open-source, end-to-end pipeline for choroidal analysis in optical coherence tomography</title><link>http://arxiv.org/abs/2312.02956v1</link><description>Purpose: To develop Choroidalyzer, an open-source, end-to-end pipeline forsegmenting the choroid region, vessels, and fovea, and deriving choroidalthickness, area, and vascular index. Methods: We used 5,600 OCT B-scans (233 subjects, 6 systemic disease cohorts,3 device types, 2 manufacturers). To generate region and vessel ground-truths,we used state-of-the-art automatic methods following manual correction ofinaccurate segmentations, with foveal positions manually annotated. We traineda U-Net deep-learning model to detect the region, vessels, and fovea tocalculate choroid thickness, area, and vascular index in a fovea-centred regionof interest. We analysed segmentation agreement (AUC, Dice) and choroid metricsagreement (Pearson, Spearman, mean absolute error (MAE)) in internal andexternal test sets. We compared Choroidalyzer to two manual graders on a smallsubset of external test images and examined cases of high error. Results: Choroidalyzer took 0.299 seconds per image on a standard laptop andachieved excellent region (Dice: internal 0.9789, external 0.9749), very goodvessel segmentation performance (Dice: internal 0.8817, external 0.8703) andexcellent fovea location prediction (MAE: internal 3.9 pixels, external 3.4pixels). For thickness, area, and vascular index, Pearson correlations were0.9754, 0.9815, and 0.8285 (internal) / 0.9831, 0.9779, 0.7948 (external),respectively (all p&lt;0.0001). Choroidalyzer's agreement with graders wascomparable to the inter-grader agreement across all metrics. Conclusions: Choroidalyzer is an open-source, end-to-end pipeline thataccurately segments the choroid and reliably extracts thickness, area, andvascular index. Especially choroidal vessel segmentation is a difficult andsubjective task, and fully-automatic methods like Choroidalyzer could provideobjectivity and standardisation.</description><author>Justin Engelmann, Jamie Burke, Charlene Hamid, Megan Reid-Schachter, Dan Pugh, Neeraj Dhaun, Diana Moukaddem, Lyle Gray, Niall Strang, Paul McGraw, Amos Storkey, Paul J. Steptoe, Stuart King, Tom MacGillivray, Miguel O. Bernabeu, Ian J. C. MacCormick</author><pubDate>Tue, 05 Dec 2023 18:40:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02956v1</guid></item><item><title>Investigation of UAV Detection in Images with Complex Backgrounds and Rainy Artifacts</title><link>http://arxiv.org/abs/2305.16450v2</link><description>To detect unmanned aerial vehicles (UAVs) in real-time, computer vision anddeep learning approaches are evolving research areas. Interest in this problemhas grown due to concerns regarding the possible hazards and misuse ofemploying UAVs in many applications. These include potential privacyviolations. To address the concerns, vision-based object detection methods havebeen developed for UAV detection. However, UAV detection in images with complexbackgrounds and weather artifacts like rain has yet to be reasonably studied.Hence, for this purpose, we prepared two training datasets. The first datasethas the sky as its background and is called the Sky Background Dataset (SBD).The second training dataset has more complex scenes (with diverse backgrounds)and is named the Complex Background Dataset (CBD). Additionally, two test setswere prepared: one containing clear images and the other with images with threerain artifacts, named the Rainy Test Set (RTS). This work also focuses onbenchmarking state-of-the-art object detection models, and to the best of ourknowledge, it is the first to investigate the performance of recent and popularvision-based object detection methods for UAV detection under challengingconditions such as complex backgrounds, varying UAV sizes, and low-to-heavyrainy conditions. The findings presented in the paper shall help provideinsights concerning the performance of the selected models for UAV detectionunder challenging conditions and pave the way to develop more robust UAVdetection methods. The codes and datasets are available at:https://github.com/AdnanMunir294/UAVD-CBRA.</description><author>Adnan Munir, Abdul Jabbar Siddiqui, Saeed Anwar</author><pubDate>Tue, 05 Dec 2023 18:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16450v2</guid></item><item><title>LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models</title><link>http://arxiv.org/abs/2312.02949v1</link><description>With the recent significant advancements in large multi-modal models (LMMs),the importance of their grounding capability in visual chat is increasinglyrecognized. Despite recent efforts to enable LMMs to support grounding, theircapabilities for grounding and chat are usually separate, and their chatperformance drops dramatically when asked to ground. The problem is the lack ofa dataset for grounded visual chat (GVC). Existing grounding datasets onlycontain short captions. To address this issue, we have created GVC data thatallows for the combination of grounding and chat capabilities. To betterevaluate the GVC capabilities, we have introduced a benchmark calledGrounding-Bench. Additionally, we have proposed a model design that can supportGVC and various types of visual prompts by connecting segmentation models withlanguage models. Experimental results demonstrate that our model outperformsother LMMs on Grounding-Bench. Furthermore, our model achieves competitiveperformance on classic grounding benchmarks like RefCOCO/+/g and Flickr30KEntities. Our code will be released athttps://github.com/UX-Decoder/LLaVA-Grounding .</description><author>Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, Jianwei Yang</author><pubDate>Tue, 05 Dec 2023 18:29:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02949v1</guid></item><item><title>Calibrating dimension reduction hyperparameters in the presence of noise</title><link>http://arxiv.org/abs/2312.02946v1</link><description>The goal of dimension reduction tools is to construct a low-dimensionalrepresentation of high-dimensional data. These tools are employed for a varietyof reasons such as noise reduction, visualization, and to lower computationalcosts. However, there is a fundamental issue that is highly discussed in othermodeling problems, but almost entirely ignored in the dimension reductionliterature: overfitting. If we interpret data as a combination of signal andnoise, prior works judge dimension reduction techniques on their ability tocapture the entirety of the data, i.e. both the signal and the noise. In thecontext of other modeling problems, techniques such as feature-selection,cross-validation, and regularization are employed to combat overfitting, but nosuch precautions are taken when performing dimension reduction. In this paper,we present a framework that models dimension reduction problems in the presenceof noise and use this framework to explore the role perplexity and number ofneighbors play in overfitting data when applying t-SNE and UMAP. We alsopresent a workflow others may use to calibrate perplexity or number ofneighbors in the presence of noise.</description><author>Justin Lin, Julia Fukuyama</author><pubDate>Tue, 05 Dec 2023 18:16:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02946v1</guid></item><item><title>Fast CT anatomic localization algorithm</title><link>http://arxiv.org/abs/2312.02941v1</link><description>Automatically determining the position of every slice in a CT scan is a basicyet powerful capability allowing fast retrieval of region of interest forvisual inspection and automated analysis. Unlike conventional localizationapproaches which work at the slice level, we directly localize only a fractionof the slices and and then fit a linear model which maps slice index to itsestimated axial anatomical position based on those slices. The model is thenused to assign axial position to every slices of the scan. This approach provesto be both computationally efficient, with a typical processing time of lessthan a second per scan (regardless of its size), accurate, with a typicalmedian localization error of 1 cm, and robust to different noise sources,imaging protocols, metal induced artifacts, anatomical deformations etc.Another key element of our approach is the introduction of a mapping confidencescore. This score acts as a fail safe mechanism which allows a rejection ofunreliable localization results in rare cases of anomalous scans. Our algorithmsets new State Of The Art results in terms of localization accuracy. It alsooffers a decrease of two orders of magnitude in processing time with respect toall published processing times. It was designed to be invariant to various scanresolutions, scan protocols, patient orientations, strong artifacts and variousdeformations and abnormalities. Additionally, our algorithm is the first one tothe best of our knowledge which supports the entire body from head to feet andis not confined to specific anatomical region. This algorithm was tested onthousands of scans and proves to be very reliable and useful as a preprocessingstage for many applications.</description><author>Amit Oved</author><pubDate>Tue, 05 Dec 2023 18:09:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02941v1</guid></item><item><title>Drag-A-Video: Non-rigid Video Editing with Point-based Interaction</title><link>http://arxiv.org/abs/2312.02936v1</link><description>Video editing is a challenging task that requires manipulating videos on boththe spatial and temporal dimensions. Existing methods for video editing mainlyfocus on changing the appearance or style of the objects in the video, whilekeeping their structures unchanged. However, there is no existing method thatallows users to interactively ``drag'' any points of instances on the firstframe to precisely reach the target points with other frames consistentlydeformed. In this paper, we propose a new diffusion-based method forinteractive point-based video manipulation, called Drag-A-Video. Our methodallows users to click pairs of handle points and target points as well as maskson the first frame of an input video. Then, our method transforms the inputsinto point sets and propagates these sets across frames. To precisely modifythe contents of the video, we employ a new video-level motion supervision toupdate the features of the video and introduce the latent offsets to achievethis update at multiple denoising timesteps. We propose a temporal-consistentpoint tracking module to coordinate the movement of the points in the handlepoint sets. We demonstrate the effectiveness and flexibility of our method onvarious videos. The website of our work is available here:https://drag-a-video.github.io/.</description><author>Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, Xihui Liu</author><pubDate>Tue, 05 Dec 2023 18:05:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02936v1</guid></item><item><title>WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation</title><link>http://arxiv.org/abs/2312.02934v1</link><description>Generating multi-camera street-view videos is critical for augmentingautonomous driving datasets, addressing the urgent demand for extensive andvaried data. Due to the limitations in diversity and challenges in handlinglighting conditions, traditional rendering-based methods are increasingly beingsupplanted by diffusion-based methods. However, a significant challenge indiffusion-based methods is ensuring that the generated sensor data preserveboth intra-world consistency and inter-sensor coherence. To address thesechallenges, we combine an additional explicit world volume and propose theWorld Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This systemis specifically designed to leverage 4D world volume as a foundational elementfor video generation. Our model operates in two distinct phases: (i)envisioning the future 4D temporal world volume based on vehicle controlsequences, and (ii) generating multi-camera videos, informed by this envisioned4D temporal world volume and sensor interconnectivity. The incorporation of the4D world volume empowers WoVoGen not only to generate high-quality street-viewvideos in response to vehicle control inputs but also to facilitate sceneediting tasks.</description><author>Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, Li Zhang</author><pubDate>Tue, 05 Dec 2023 18:05:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02934v1</guid></item><item><title>WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words</title><link>http://arxiv.org/abs/2312.02931v1</link><description>Training on multiple modalities of input can augment the capabilities of alanguage model. Here, we ask whether such a training regime can improve thequality and efficiency of these systems as well. We focus on text--audio andintroduce Whisbert, which is inspired by the text--image approach of FLAVA\citep{singh_flava_2022}. In accordance with Babylm \citep{warstadt2023papers}guidelines, we pretrain Whisbert on a dataset comprising only 100 million wordsplus their corresponding speech from the word-aligned version of the People'sSpeech dataset \citep{galvez_peoples_2021}. To assess the impact ofmultimodality, we compare versions of the model that are trained on text onlyand on both audio and text simultaneously. We find that while Whisbert is ableto perform well on multimodal masked modeling and surpasses the Babylmbaselines in most benchmark tasks, it struggles to optimize its complexobjective and outperform its text-only Whisbert baseline.</description><author>Lukas Wolf, Klemen Kotar, Greta Tuckute, Eghbal Hosseini, Tamar Regev, Ethan Wilcox, Alex Warstadt</author><pubDate>Tue, 05 Dec 2023 18:03:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02931v1</guid></item><item><title>Jellyfish: A Large Language Model for Data Preprocessing</title><link>http://arxiv.org/abs/2312.01678v2</link><description>In this paper, we present Jellyfish, an open-source LLM as a universal tasksolver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tunedwith the datasets of several typical DP tasks including error detection, dataimputation, schema matching, and entity matching, and delivers generalizabilityto other tasks. Remarkably, Jellyfish can operate on a local, single, andlow-priced GPU with its 13 billion parameters, ensuring data security andenabling further tuning. Its proficiency in understanding natural languageallows users to manually craft instructions for DP tasks. Unlike many existingmethods that heavily rely on prior knowledge, Jellyfish acquires domainknowledge during its tuning process and integrates optional knowledge injectionduring inference. A distinctive feature of Jellyfish is its interpreter, whichelucidates its output decisions. To construct Jellyfish, we develop a series ofpre-tuning and DP-tuning techniques. Jellyfish is equipped with an instanceserializer, which automatically translates raw data into model prompts, and aknowledge injector, which optionally introduces task- and dataset-specificknowledge to enhance DP performance. Our evaluation of Jellyfish, using a rangeof real datasets, shows its competitiveness compared to state-of-the-artmethods and its strong generalizability to unseen tasks. Jellyfish'sperformance rivals that of GPT series models, and its interpreter offersenhanced reasoning capabilities compared to GPT-3.5. Furthermore, ourevaluation highlights the effectiveness of the techniques employed inconstructing Jellyfish. Our model is available at Hugging Face:https://huggingface.co/NECOUDBFM/Jellyfish .</description><author>Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada</author><pubDate>Tue, 05 Dec 2023 18:02:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01678v2</guid></item><item><title>Uncertainty Quantification in Multivariable Regression for Material Property Prediction with Bayesian Neural Networks</title><link>http://arxiv.org/abs/2311.02495v2</link><description>With the increased use of data-driven approaches and machine learning-basedmethods in material science, the importance of reliable uncertaintyquantification (UQ) of the predicted variables for informed decision-makingcannot be overstated. UQ in material property prediction poses uniquechallenges, including the multi-scale and multi-physics nature of advancedmaterials, intricate interactions between numerous factors, limitedavailability of large curated datasets for model training, etc. Recently,Bayesian Neural Networks (BNNs) have emerged as a promising approach for UQ,offering a probabilistic framework for capturing uncertainties within neuralnetworks. In this work, we introduce an approach for UQ within physics-informedBNNs, which integrates knowledge from governing laws in material modeling toguide the models toward physically consistent predictions. To evaluate theeffectiveness of this approach, we present case studies for predicting thecreep rupture life of steel alloys. Experimental validation with three datasetsof collected measurements from creep tests demonstrates the ability of BNNs toproduce accurate point and uncertainty estimates that are competitive or exceedthe performance of the conventional method of Gaussian Process Regression.Similarly, we evaluated the suitability of BNNs for UQ in an active learningapplication and reported competitive performance. The most promising frameworkfor creep life prediction is BNNs based on Markov Chain Monte Carloapproximation of the posterior distribution of network parameters, as itprovided more reliable results in comparison to BNNs based on variationalinference approximation or related NNs with probabilistic outputs. The codesare available at:https://github.com/avakanski/Creep-uncertainty-quantification.</description><author>Longze Li, Jiang Chang, Aleksandar Vakanski, Yachun Wang, Tiankai Yao, Min Xian</author><pubDate>Tue, 05 Dec 2023 18:00:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02495v2</guid></item><item><title>LivePhoto: Real Image Animation with Text-guided Motion Control</title><link>http://arxiv.org/abs/2312.02928v1</link><description>Despite the recent progress in text-to-video generation, existing studiesusually overlook the issue that only spatial contents but not temporal motionsin synthesized videos are under the control of text. Towards such a challenge,this work presents a practical system, named LivePhoto, which allows users toanimate an image of their interest with text descriptions. We first establish astrong baseline that helps a well-learned text-to-image generator (i.e., StableDiffusion) take an image as a further input. We then equip the improvedgenerator with a motion module for temporal modeling and propose a carefullydesigned training pipeline to better link texts and motions. In particular,considering the facts that (1) text can only describe motions roughly (e.g.,regardless of the moving speed) and (2) text may include both content andmotion descriptions, we introduce a motion intensity estimation module as wellas a text re-weighting module to reduce the ambiguity of text-to-motionmapping. Empirical evidence suggests that our approach is capable of welldecoding motion-related textual instructions into videos, such as actions,camera movements, or even conjuring new contents from thin air (e.g., pouringwater into an empty glass). Interestingly, thanks to the proposed intensitylearning mechanism, our system offers users an additional control signal (i.e.,the motion intensity) besides text for video customization.</description><author>Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, Hengshuang Zhao</author><pubDate>Tue, 05 Dec 2023 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02928v1</guid></item><item><title>Split &amp; Merge: Unlocking the Potential of Visual Adapters via Sparse Training</title><link>http://arxiv.org/abs/2312.02923v1</link><description>With the rapid growth in the scale of pre-trained foundation models,parameter-efficient fine-tuning techniques have gained significant attention,among which Adapter Tuning is the most widely used. Despite achievingefficiency, Adapter Tuning still underperforms full fine-tuning, and theperformance improves at the cost of an increase in parameters. Recent effortsaddress this issue by pruning the original adapters, but it also introducestraining instability and suboptimal performance on certain datasets. Motivatedby this, we propose Mixture of Sparse Adapters, or MoSA, as a novel AdapterTuning method to fully unleash the potential of each parameter in the adapter.We first split the standard adapter into multiple non-overlapping modules, thenstochastically activate modules for sparse training, and finally merge them toform a complete adapter after tuning. In this way, MoSA can achievesignificantly better performance than standard adapters without any additionalcomputational or storage overhead. Furthermore, we propose a hierarchicalsparse strategy to better leverage limited training data. Extensive experimentson a series of 27 visual tasks demonstrate that MoSA consistently outperformsother Adapter Tuning methods as well as other baselines by a significantmargin. Furthermore, in two challenging scenarios with low-resource andmulti-task settings, MoSA achieves satisfactory results, further demonstratingthe effectiveness of our design. Our code will be released.</description><author>Qizhe Zhang, Bocheng Zou, Ruichuan An, Jiaming Liu, Shanghang Zhang</author><pubDate>Tue, 05 Dec 2023 17:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02923v1</guid></item><item><title>Variability of echo state network prediction horizon for partially observed dynamical systems</title><link>http://arxiv.org/abs/2306.10797v3</link><description>Study of dynamical systems using partial state observation is an importantproblem due to its applicability to many real-world systems. We address theproblem by studying an echo state network (ESN) framework with partial stateinput with partial or full state output. Application to the Lorenz system andChua's oscillator (both numerically simulated and experimental systems)demonstrate the effectiveness of our method. We show that the ESN, as anautonomous dynamical system, is capable of making short-term predictions up toa few Lyapunov times. However, the prediction horizon has high variabilitydepending on the initial condition-an aspect that we explore in detail usingthe distribution of the prediction horizon. Further, using a variety ofstatistical metrics to compare the long-term dynamics of the ESN predictionswith numerically simulated or experimental dynamics and observed similarresults, we show that the ESN can effectively learn the system's dynamics evenwhen trained with noisy numerical or experimental datasets. Thus, wedemonstrate the potential of ESNs to serve as cheap surrogate models forsimulating the dynamics of systems where complete observations are unavailable.</description><author>Ajit Mahata, Reetish Padhi, Amit Apte</author><pubDate>Tue, 05 Dec 2023 17:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10797v3</guid></item><item><title>Fine-grained Controllable Video Generation via Object Appearance and Context</title><link>http://arxiv.org/abs/2312.02919v1</link><description>Text-to-video generation has shown promising results. However, by taking onlynatural languages as input, users often face difficulties in providing detailedinformation to precisely control the model's output. In this work, we proposefine-grained controllable video generation (FACTOR) to achieve detailedcontrol. Specifically, FACTOR aims to control objects' appearances and context,including their location and category, in conjunction with the text prompt. Toachieve detailed control, we propose a unified framework to jointly injectcontrol signals into the existing text-to-video model. Our model consists of ajoint encoder and adaptive cross-attention layers. By optimizing the encoderand the inserted layer, we adapt the model to generate videos that are alignedwith both text prompts and fine-grained control. Compared to existing methodsrelying on dense control signals such as edge maps, we provide a more intuitiveand user-friendly interface to allow object-level fine-grained control. Ourmethod achieves controllability of object appearances without finetuning, whichreduces the per-subject optimization efforts for the users. Extensiveexperiments on standard benchmark datasets and user-provided inputs validatethat our model obtains a 70% improvement in controllability metrics overcompetitive baselines.</description><author>Hsin-Ping Huang, Yu-Chuan Su, Deqing Sun, Lu Jiang, Xuhui Jia, Yukun Zhu, Ming-Hsuan Yang</author><pubDate>Tue, 05 Dec 2023 17:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02919v1</guid></item><item><title>Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration</title><link>http://arxiv.org/abs/2312.02918v1</link><description>Despite substantial progress, all-in-one image restoration (IR) grapples withpersistent challenges in handling intricate real-world degradations. This paperintroduces MPerceiver: a novel multimodal prompt learning approach thatharnesses Stable Diffusion (SD) priors to enhance adaptiveness,generalizability and fidelity for all-in-one image restoration. Specifically,we develop a dual-branch module to master two types of SD prompts: textual forholistic representation and visual for multiscale detail representation. Bothprompts are dynamically adjusted by degradation predictions from the CLIP imageencoder, enabling adaptive responses to diverse unknown degradations. Moreover,a plug-in detail refinement module improves restoration fidelity via directencoder-to-decoder information transformation. To assess our method, MPerceiveris trained on 9 tasks for all-in-one IR and outperforms state-of-the-arttask-specific methods across most tasks. Post multitask pre-training,MPerceiver attains a generalized representation in low-level vision, exhibitingremarkable zero-shot and few-shot capabilities in unseen tasks. Extensiveexperiments on 16 IR tasks and 26 benchmarks underscore the superiority ofMPerceiver in terms of adaptiveness, generalizability and fidelity.</description><author>Yuang Ai, Huaibo Huang, Xiaoqiang Zhou, Jiexiang Wang, Ran He</author><pubDate>Tue, 05 Dec 2023 17:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02918v1</guid></item><item><title>MIND: Multi-Task Incremental Network Distillation</title><link>http://arxiv.org/abs/2312.02916v1</link><description>The recent surge in pervasive devices generating dynamic data streams hasunderscored the necessity for learning systems to adapt to data distributionalshifts continually. To tackle this challenge, the research community has putforth a spectrum of methodologies, including the demanding pursuit ofclass-incremental learning without replay data. In this study, we present MIND,a parameter isolation method that aims to significantly enhance the performanceof replay-free solutions and achieve state-of-the-art results on several widelystudied datasets. Our approach introduces two main contributions: twoalternative distillation procedures that significantly improve the efficiencyof MIND increasing the accumulated knowledge of each sub-network, and theoptimization of the BachNorm layers across tasks inside the sub-networks.Overall, MIND outperforms all the state-of-the-art methods for rehearsal-freeClass-Incremental learning (with an increment in classification accuracy ofapprox. +6% on CIFAR-100/10 and +10% on TinyImageNet/10) reaching up to approx.+40% accuracy in Domain-Incremental scenarios. Moreover, we ablated eachcontribution to demonstrate its impact on performance improvement. Our resultsshowcase the superior performance of MIND indicating its potential foraddressing the challenges posed by Class-incremental and Domain-Incrementallearning in resource-constrained environments.</description><author>Jacopo Bonato, Francesco Pelosin, Luigi Sabetta, Alessandro Nicolosi</author><pubDate>Tue, 05 Dec 2023 17:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02916v1</guid></item><item><title>Multi-task Image Restoration Guided By Robust DINO Features</title><link>http://arxiv.org/abs/2312.01677v2</link><description>Multi-task image restoration has gained significant interest due to itsinherent versatility and efficiency compared to its single-task counterpart.Despite its potential, performance degradation is observed with an increase inthe number of tasks, primarily attributed to the distinct nature of eachrestoration task. Addressing this challenge, we introduce\mbox{\textbf{DINO-IR}}, a novel multi-task image restoration approachleveraging robust features extracted from DINOv2. Our empirical analysis showsthat while shallow features of DINOv2 capture rich low-level imagecharacteristics, the deep features ensure a robust semantic representationinsensitive to degradations while preserving high-frequency contour details.Building on these features, we devise specialized components, includingmulti-layer semantic fusion module, DINO-Restore adaption and fusion module,and DINO perception contrastive loss, to integrate DINOv2 features into therestoration paradigm. Equipped with the aforementioned components, our DINO-IRperforms favorably against existing multi-task image restoration approaches invarious tasks by a large margin, indicating the superiority and necessity ofreinforcing the robust features for multi-task image restoration.</description><author>Xin Lin, Chao Ren, Kelvin C. K. Chan, Lu Qi, Jinshan Pan, Ming-Hsuan Yang</author><pubDate>Tue, 05 Dec 2023 17:46:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.01677v2</guid></item><item><title>Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training</title><link>http://arxiv.org/abs/2312.02914v1</link><description>In this work, we tackle the problem of unsupervised domain adaptation (UDA)for video action recognition. Our approach, which we call UNITE, uses an imageteacher model to adapt a video student model to the target domain. UNITE firstemploys self-supervised pre-training to promote discriminative feature learningon target domain videos using a teacher-guided masked distillation objective.We then perform self-training on masked target data, using the video studentmodel and image teacher model together to generate improved pseudolabels forunlabeled target videos. Our self-training process successfully leverages thestrengths of both models to achieve strong transfer performance across domains.We evaluate our approach on multiple video domain adaptation benchmarks andobserve significant improvements upon previously reported results.</description><author>Arun Reddy, William Paul, Corban Rivera, Ketul Shah, Celso M. de Melo, Rama Chellappa</author><pubDate>Tue, 05 Dec 2023 17:39:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02914v1</guid></item><item><title>Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions</title><link>http://arxiv.org/abs/2312.02913v1</link><description>Conversational question-answering (CQA) systems aim to create interactivesearch systems that effectively retrieve information by interacting with users.To replicate human-to-human conversations, existing work uses human annotatorsto play the roles of the questioner (student) and the answerer (teacher).Despite its effectiveness, challenges exist as human annotation istime-consuming, inconsistent, and not scalable. To address this issue andinvestigate the applicability of large language models (LLMs) in CQAsimulation, we propose a simulation framework that employs zero-shot learnerLLMs for simulating teacher-student interactions. Our framework involves twoLLMs interacting on a specific topic, with the first LLM acting as a student,generating questions to explore a given search topic. The second LLM plays therole of a teacher by answering questions and is equipped with additionalinformation, including a text on the given topic. We implement both the studentand teacher by zero-shot prompting the GPT-4 model. To assess the effectivenessof LLMs in simulating CQA interactions and understand the disparities betweenLLM- and human-generated conversations, we evaluate the simulated data fromvarious perspectives. We begin by evaluating the teacher's performance throughboth automatic and human assessment. Next, we evaluate the performance of thestudent, analyzing and comparing the disparities between questions generated bythe LLM and those generated by humans. Furthermore, we conduct extensiveanalyses to thoroughly examine the LLM performance by benchmarkingstate-of-the-art reading comprehension models on both datasets. Our resultsreveal that the teacher LLM generates lengthier answers that tend to be moreaccurate and complete. The student LLM generates more diverse questions,covering more aspects of a given topic.</description><author>Zahra Abbasiantaeb, Yifei Yuan, Evangelos Kanoulas, Mohammad Aliannejadi</author><pubDate>Tue, 05 Dec 2023 17:38:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02913v1</guid></item><item><title>Realistic Scatterer Based Adversarial Attacks on SAR Image Classifiers</title><link>http://arxiv.org/abs/2312.02912v1</link><description>Adversarial attacks have highlighted the vulnerability of classifiers basedon machine learning for Synthetic Aperture Radar (SAR) Automatic TargetRecognition (ATR) tasks. An adversarial attack perturbs SAR images of on-groundtargets such that the classifiers are misled into making incorrect predictions.However, many existing attacking techniques rely on arbitrary manipulation ofSAR images while overlooking the feasibility of executing the attacks onreal-world SAR imagery. Instead, adversarial attacks should be able to beimplemented by physical actions, for example, placing additional false objectsas scatterers around the on-ground target to perturb the SAR image and fool theSAR ATR. In this paper, we propose the On-Target Scatterer Attack (OTSA), ascatterer-based physical adversarial attack. To ensure the feasibility of itsphysical execution, we enforce a constraint on the positioning of thescatterers. Specifically, we restrict the scatterers to be placed only on thetarget instead of in the shadow regions or the background. To achieve this, weintroduce a positioning score based on Gaussian kernels and formulate anoptimization problem for our OTSA attack. Using a gradient ascent method tosolve the optimization problem, the OTSA can generate a vector of parametersdescribing the positions, shapes, sizes and amplitudes of the scatterers toguide the physical execution of the attack that will mislead SAR imageclassifiers. The experimental results show that our attack obtainssignificantly higher success rates under the positioning constraint comparedwith the existing method.</description><author>Tian Ye, Rajgopal Kannan, Viktor Prasanna, Carl Busart, Lance Kaplan</author><pubDate>Tue, 05 Dec 2023 17:36:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02912v1</guid></item><item><title>Rare Galaxy Classes Identified In Foundation Model Representations</title><link>http://arxiv.org/abs/2312.02910v1</link><description>We identify rare and visually distinctive galaxy populations by searching forstructure within the learned representations of pretrained models. We show thatthese representations arrange galaxies by appearance in patterns beyond thoseneeded to predict the pretraining labels. We design a clustering approach toisolate specific local patterns, revealing groups of galaxies with rare andscientifically-interesting morphologies.</description><author>Mike Walmsley, Anna M. M. Scaife</author><pubDate>Tue, 05 Dec 2023 17:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02910v1</guid></item><item><title>Deep Learning Segmentation of Spiral Arms and Bars</title><link>http://arxiv.org/abs/2312.02908v1</link><description>We present the first deep learning model for segmenting galactic spiral armsand bars. In a blinded assessment by expert astronomers, our predicted spiralarm masks are preferred over both current automated methods (99% ofevaluations) and our original volunteer labels (79% of evaluations). Expertsrated our spiral arm masks as `mostly good' to `perfect' in 89% of evaluations.Bar lengths trivially derived from our predicted bar masks are in excellentagreement with a dedicated crowdsourcing project. The pixelwise precision ofour masks, previously impossible at scale, will underpin new research into howspiral arms and bars evolve.</description><author>Mike Walmsley, Ashley Spindler</author><pubDate>Tue, 05 Dec 2023 17:30:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02908v1</guid></item><item><title>End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes</title><link>http://arxiv.org/abs/2305.15930v3</link><description>Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency ofBayesian optimisation by leveraging data from related tasks. While previousmethods successfully meta-learn either a surrogate model or an acquisitionfunction independently, joint training of both components remains an openchallenge. This paper proposes the first end-to-end differentiable meta-BOframework that generalises neural processes to learn acquisition functions viatransformer architectures. We enable this end-to-end framework withreinforcement learning (RL) to tackle the lack of labelled acquisition data.Early on, we notice that training transformer-based neural processes fromscratch with RL is challenging due to insufficient supervision, especially whenrewards are sparse. We formalise this claim with a combinatorial analysisshowing that the widely used notion of regret as a reward signal exhibits alogarithmic sparsity pattern in trajectory lengths. To tackle this problem, weaugment the RL objective with an auxiliary task that guides part of thearchitecture to learn a valid probabilistic model as an inductive bias. Wedemonstrate that our method achieves state-of-the-art regret results againstvarious baselines in experiments on standard hyperparameter optimisation tasksand also outperforms others in the real-world problems of mixed-integerprogramming tuning, antibody design, and logic synthesis for electronic designautomation.</description><author>Alexandre Maraval, Matthieu Zimmer, Antoine Grosnit, Haitham Bou Ammar</author><pubDate>Tue, 05 Dec 2023 17:21:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15930v3</guid></item><item><title>HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2312.02902v1</link><description>3D head animation has seen major quality and runtime improvements over thelast few years, particularly empowered by the advances in differentiablerendering and neural radiance fields. Real-time rendering is a highly desirablegoal for real-world applications. We propose HeadGaS, the first model to use 3DGaussian Splats (3DGS) for 3D head reconstruction and animation. In this paperwe introduce a hybrid model that extends the explicit representation from 3DGSwith a base of learnable latent features, which can be linearly blended withlow-dimensional parameters from parametric head models to obtainexpression-dependent final color and opacity values. We demonstrate thatHeadGaS delivers state-of-the-art results in real-time inference frame rates,which surpasses baselines by up to ~2dB, while accelerating rendering speed byover x10.</description><author>Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero</author><pubDate>Tue, 05 Dec 2023 17:19:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02902v1</guid></item><item><title>Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems</title><link>http://arxiv.org/abs/2305.01090v2</link><description>While many phenomena in physics and engineering are formallyhigh-dimensional, their long-time dynamics often live on a lower-dimensionalmanifold. The present work introduces an autoencoder framework that combinesimplicit regularization with internal linear layers and $L_2$ regularization(weight decay) to automatically estimate the underlying dimensionality of adata set, produce an orthogonal manifold coordinate system, and provide themapping functions between the ambient space and manifold space, allowing forout-of-sample projections. We validate our framework's ability to estimate themanifold dimension for a series of datasets from dynamical systems of varyingcomplexities and compare to other state-of-the-art estimators. We analyze thetraining dynamics of the network to glean insight into the mechanism oflow-rank learning and find that collectively each of the implicit regularizinglayers compound the low-rank representation and even self-correct duringtraining. Analysis of gradient descent dynamics for this architecture in thelinear case reveals the role of the internal linear layers in leading to fasterdecay of a "collective weight variable" incorporating all layers, and the roleof weight decay in breaking degeneracies and thus driving convergence alongdirections in which no decay would occur in its absence. We show that thisframework can be naturally extended for applications of state-space modelingand forecasting by generating a data-driven dynamic model of a spatiotemporallychaotic partial differential equation using only the manifold coordinates.Finally, we demonstrate that our framework is robust to hyperparameter choices.</description><author>Kevin Zeng, Carlos E. Pérez De Jesús, Andrew J. Fox, Michael D. Graham</author><pubDate>Tue, 05 Dec 2023 17:16:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01090v2</guid></item><item><title>FroSSL: Frobenius Norm Minimization for Self-Supervised Learning</title><link>http://arxiv.org/abs/2310.02903v2</link><description>Self-supervised learning (SSL) is an increasingly popular paradigm forrepresentation learning. Recent methods can be classified assample-contrastive, dimension-contrastive, or asymmetric network-based, witheach family having its own approach to avoiding informational collapse. Whiledimension-contrastive methods converge to similar solutions assample-contrastive methods, it can be empirically shown that some methodsrequire more epochs of training to converge. Motivated by closing this divide,we present the objective function FroSSL which is both sample- anddimension-contrastive up to embedding normalization. FroSSL works by minimizingcovariance Frobenius norms for avoiding collapse and minimizing mean-squarederror for augmentation invariance. We show that FroSSL converges more quicklythan a variety of other SSL methods and provide theoretical and empiricalsupport that this faster convergence is due to how FroSSL affects theeigenvalues of the embedding covariance matrices. We also show that FroSSLlearns competitive representations on linear probe evaluation when used totrain a ResNet18 on the CIFAR-10, CIFAR-100, STL-10, and ImageNet datasets.</description><author>Oscar Skean, Aayush Dhakal, Nathan Jacobs, Luis Gonzalo Sanchez Giraldo</author><pubDate>Tue, 05 Dec 2023 17:16:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02903v2</guid></item><item><title>Concept Drift Adaptation in Text Stream Mining Settings: A Comprehensive Review</title><link>http://arxiv.org/abs/2312.02901v1</link><description>Due to the advent and increase in the popularity of the Internet, people havebeen producing and disseminating textual data in several ways, such as reviews,social media posts, and news articles. As a result, numerous researchers havebeen working on discovering patterns in textual data, especially because socialmedia posts function as social sensors, indicating peoples' opinions,interests, etc. However, most tasks regarding natural language processing areaddressed using traditional machine learning methods and static datasets. Thissetting can lead to several problems, such as an outdated dataset, which maynot correspond to reality, and an outdated model, which has its performancedegrading over time. Concept drift is another aspect that emphasizes theseissues, which corresponds to data distribution and pattern changes. In a textstream scenario, it is even more challenging due to its characteristics, suchas the high speed and data arriving sequentially. In addition, models for thistype of scenario must adhere to the constraints mentioned above while learningfrom the stream by storing texts for a limited time and consuming low memory.In this study, we performed a systematic literature review regarding conceptdrift adaptation in text stream scenarios. Considering well-defined criteria,we selected 40 papers to unravel aspects such as text drift categories, typesof text drift detection, model update mechanism, the addressed stream miningtasks, types of text representations, and text representation update mechanism.In addition, we discussed drift visualization and simulation and listedreal-world datasets used in the selected papers. Therefore, this papercomprehensively reviews the concept drift adaptation in text stream miningscenarios.</description><author>Cristiano Mesquita Garcia, Ramon Simoes Abilio, Alessandro Lameiras Koerich, Alceu de Souza Britto Jr., Jean Paul Barddal</author><pubDate>Tue, 05 Dec 2023 17:15:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02901v1</guid></item><item><title>VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence</title><link>http://arxiv.org/abs/2312.02087v2</link><description>Current diffusion-based video editing primarily focuses onstructure-preserved editing by utilizing various dense correspondences toensure temporal consistency and motion alignment. However, these approaches areoften ineffective when the target edit involves a shape change. To embark onvideo editing with shape change, we explore customized video subject swappingin this work, where we aim to replace the main subject in a source video with atarget subject having a distinct identity and potentially different shape. Incontrast to previous methods that rely on dense correspondences, we introducethe VideoSwap framework that exploits semantic point correspondences, inspiredby our observation that only a small number of semantic points are necessary toalign the subject's motion trajectory and modify its shape. We also introducevarious user-point interactions (\eg, removing points and dragging points) toaddress various semantic point correspondence. Extensive experimentsdemonstrate state-of-the-art video subject swapping results across a variety ofreal-world videos.</description><author>Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, Kevin Tang</author><pubDate>Tue, 05 Dec 2023 17:14:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02087v2</guid></item><item><title>BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models</title><link>http://arxiv.org/abs/2312.02896v1</link><description>Large Multimodal Models (LMMs) such as GPT-4V and LLaVA have shown remarkablecapabilities in visual reasoning with common image styles. However, theirrobustness against diverse style shifts, crucial for practical applications,remains largely unexplored. In this paper, we propose a new benchmark,BenchLMM, to assess the robustness of LMMs against three different styles:artistic image style, imaging sensor style, and application style, where eachstyle has five sub-styles. Utilizing BenchLMM, we comprehensively evaluatestate-of-the-art LMMs and reveal: 1) LMMs generally suffer performancedegradation when working with other styles; 2) An LMM performs better thananother model in common style does not guarantee its superior performance inother styles; 3) LMMs' reasoning capability can be enhanced by prompting LMMsto predict the style first, based on which we propose a versatile andtraining-free method for improving LMMs; 4) An intelligent LMM is expected tointerpret the causes of its errors when facing stylistic variations. We hopethat our benchmark and analysis can shed new light on developing moreintelligent and versatile LMMs.</description><author>Rizhao Cai, Zirui Song, Dayan Guan, Zhenhao Chen, Xing Luo, Chenyu Yi, Alex Kot</author><pubDate>Tue, 05 Dec 2023 17:06:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02896v1</guid></item><item><title>ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?</title><link>http://arxiv.org/abs/2311.16989v3</link><description>Upon its release in late 2022, ChatGPT has brought a seismic shift in theentire landscape of AI, both in research and commerce. Throughinstruction-tuning a large language model (LLM) with supervised fine-tuning andreinforcement learning from human feedback, it showed that a model could answerhuman questions and follow instructions on a broad panel of tasks. Followingthis success, interests in LLMs have intensified, with new LLMs flourishing atfrequent interval across academia and industry, including many start-upsfocused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic'sClaude) generally outperform their open-source counterparts, the progress onthe latter has been rapid with claims of achieving parity or even better oncertain tasks. This has crucial implications not only on research but also onbusiness. In this work, on the first anniversary of ChatGPT, we provide anexhaustive overview of this success, surveying all tasks where an open-sourceLLM has claimed to be on par or better than ChatGPT.</description><author>Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, Shafiq Joty</author><pubDate>Tue, 05 Dec 2023 16:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16989v3</guid></item><item><title>Towards More Practical Group Activity Detection: A New Benchmark and Model</title><link>http://arxiv.org/abs/2312.02878v1</link><description>Group activity detection (GAD) is the task of identifying members of eachgroup and classifying the activity of the group at the same time in a video.While GAD has been studied recently, there is still much room for improvementin both dataset and methodology due to their limited capability to addresspractical GAD scenarios. To resolve these issues, we first present a newdataset, dubbed Caf\'e. Unlike existing datasets, Caf\'e is constructedprimarily for GAD and presents more practical evaluation scenarios and metrics,as well as being large-scale and providing rich annotations. Along with thedataset, we propose a new GAD model that deals with an unknown number of groupsand latent group members efficiently and effectively. We evaluated our model onthree datasets including Caf\'e, where it outperformed previous work in termsof both accuracy and inference speed. Both our dataset and code base will beopen to the public to promote future research on GAD.</description><author>Dongkeun Kim, Youngkil Song, Minsu Cho, Suha Kwak</author><pubDate>Tue, 05 Dec 2023 16:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02878v1</guid></item><item><title>A Dynamic Network for Efficient Point Cloud Registration</title><link>http://arxiv.org/abs/2312.02877v1</link><description>For the point cloud registration task, a significant challenge arises fromnon-overlapping points that consume extensive computational resources whilenegatively affecting registration accuracy. In this paper, we introduce adynamic approach, widely utilized to improve network efficiency in computervision tasks, to the point cloud registration task. We employ an iterativeregistration process on point cloud data multiple times to identify regionswhere matching points cluster, ultimately enabling us to remove noisy points.Specifically, we begin with deep global sampling to perform coarse globalregistration. Subsequently, we employ the proposed refined node proposal moduleto further narrow down the registration region and perform local registration.Furthermore, we utilize a spatial consistency-based classifier to evaluate theresults of each registration stage. The model terminates once it reachessufficient confidence, avoiding unnecessary computations. Extended experimentsdemonstrate that our model significantly reduces time consumption compared toother methods with similar results, achieving a speed improvement of over 41%on indoor dataset (3DMatch) and 33% on outdoor datasets (KITTI) whilemaintaining competitive registration recall requirements.</description><author>Yang Ai, Xi Yang</author><pubDate>Tue, 05 Dec 2023 16:47:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02877v1</guid></item><item><title>On the Identifiability of Quantized Factors</title><link>http://arxiv.org/abs/2306.16334v2</link><description>Disentanglement aims to recover meaningful latent ground-truth factors fromthe observed distribution solely, and is formalized through the theory ofidentifiability. The identifiability of independent latent factors is proven tobe impossible in the unsupervised i.i.d. setting under a general nonlinear mapfrom factors to observations. In this work, however, we demonstrate that it ispossible to recover quantized latent factors under a generic nonlineardiffeomorphism. We only assume that the latent factors have independentdiscontinuities in their density, without requiring the factors to bestatistically independent. We introduce this novel form of identifiability,termed quantized factor identifiability, and provide a comprehensive proof ofthe recovery of the quantized factors.</description><author>Vitória Barin-Pacela, Kartik Ahuja, Simon Lacoste-Julien, Pascal Vincent</author><pubDate>Tue, 05 Dec 2023 16:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16334v2</guid></item><item><title>A Practical Approach to Novel Class Discovery in Tabular Data</title><link>http://arxiv.org/abs/2311.05440v2</link><description>The problem of Novel Class Discovery (NCD) consists in extracting knowledgefrom a labeled set of known classes to accurately partition an unlabeled set ofnovel classes. While NCD has recently received a lot of attention from thecommunity, it is often solved on computer vision problems and under unrealisticconditions. In particular, the number of novel classes is usually assumed to beknown in advance, and their labels are sometimes used to tune hyperparameters.Methods that rely on these assumptions are not applicable in real-worldscenarios. In this work, we focus on solving NCD in tabular data when no priorknowledge of the novel classes is available. To this end, we propose to tunethe hyperparameters of NCD methods by adapting the $k$-fold cross-validationprocess and hiding some of the known classes in each fold. Since we have foundthat methods with too many hyperparameters are likely to overfit these hiddenclasses, we define a simple deep NCD model. This method is composed of only theessential elements necessary for the NCD problem and performs impressively wellunder realistic conditions. Furthermore, we find that the latent space of thismethod can be used to reliably estimate the number of novel classes.Additionally, we adapt two unsupervised clustering algorithms ($k$-means andSpectral Clustering) to leverage the knowledge of the known classes. Extensiveexperiments are conducted on 7 tabular datasets and demonstrate theeffectiveness of the proposed method and hyperparameter tuning process, andshow that the NCD problem can be solved without relying on knowledge from thenovel classes.</description><author>Colin Troisemaine, Alexandre Reiffers-Masson, Stéphane Gosselin, Vincent Lemaire, Sandrine Vaton</author><pubDate>Tue, 05 Dec 2023 16:46:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05440v2</guid></item><item><title>Toward autocorrection of chemical process flowsheets using large language models</title><link>http://arxiv.org/abs/2312.02873v1</link><description>The process engineering domain widely uses Process Flow Diagrams (PFDs) andProcess and Instrumentation Diagrams (P&amp;IDs) to represent process flows andequipment configurations. However, the P&amp;IDs and PFDs, hereafter calledflowsheets, can contain errors causing safety hazards, inefficient operation,and unnecessary expenses. Correcting and verifying flowsheets is a tedious,manual process. We propose a novel generative AI methodology for automaticallyidentifying errors in flowsheets and suggesting corrections to the user, i.e.,autocorrecting flowsheets. Inspired by the breakthrough of Large LanguageModels (LLMs) for grammatical autocorrection of human language, we investigateLLMs for the autocorrection of flowsheets. The input to the model is apotentially erroneous flowsheet and the output of the model are suggestions fora corrected flowsheet. We train our autocorrection model on a synthetic datasetin a supervised manner. The model achieves a top-1 accuracy of 80% and a top-5accuracy of 84% on an independent test dataset of synthetically generatedflowsheets. The results suggest that the model can learn to autocorrect thesynthetic flowsheets. We envision that flowsheet autocorrection will become auseful tool for chemical engineers.</description><author>Lukas Schulze Balhorn, Marc Caballero, Artur M. Schweidtmann</author><pubDate>Tue, 05 Dec 2023 16:39:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02873v1</guid></item><item><title>Experimental Insights Towards Explainable and Interpretable Pedestrian Crossing Prediction</title><link>http://arxiv.org/abs/2312.02872v1</link><description>In the context of autonomous driving, pedestrian crossing prediction is a keycomponent for improving road safety. Presently, the focus of these predictionsextends beyond achieving trustworthy results; it is shifting towards theexplainability and interpretability of these predictions. This researchintroduces a novel neuro-symbolic approach that combines deep learning andfuzzy logic for an explainable and interpretable pedestrian crossingprediction. We have developed an explainable predictor (ExPedCross), whichutilizes a set of explainable features and employs a fuzzy inference system topredict whether the pedestrian will cross or not. Our approach was evaluated onboth the PIE and JAAD datasets. The results offer experimental insights intoachieving explainability and interpretability in the pedestrian crossingprediction task. Furthermore, the testing results yield a set of guidelines andrecommendations regarding the process of dataset selection, feature selection,and explainability.</description><author>Angie Nataly Melo, Carlota Salinas, Miguel Angel Sotelo</author><pubDate>Tue, 05 Dec 2023 16:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02872v1</guid></item><item><title>Grounding Everything: Emerging Localization Properties in Vision-Language Transformers</title><link>http://arxiv.org/abs/2312.00878v2</link><description>Vision-language foundation models have shown remarkable performance invarious zero-shot settings such as image retrieval, classification, orcaptioning. But so far, those models seem to fall behind when it comes tozero-shot localization of referential expressions and objects in images. As aresult, they need to be fine-tuned for this task. In this paper, we show thatpretrained vision-language (VL) models allow for zero-shot open-vocabularyobject localization without any fine-tuning. To leverage those capabilities, wepropose a Grounding Everything Module (GEM) that generalizes the idea ofvalue-value attention introduced by CLIPSurgery to a self-self attention path.We show that the concept of self-self attention corresponds to clustering, thusenforcing groups of tokens arising from the same object to be similar whilepreserving the alignment with the language space. To further guide the groupformation, we propose a set of regularizations that allows the model to finallygeneralize across datasets and backbones. We evaluate the proposed GEMframework on various benchmark tasks and datasets for semantic segmentation. Itshows that GEM not only outperforms other training-free open-vocabularylocalization methods, but also achieves state-of-the-art results on therecently proposed OpenImagesV7 large-scale segmentation benchmark.</description><author>Walid Bousselham, Felix Petersen, Vittorio Ferrari, Hilde Kuehne</author><pubDate>Tue, 05 Dec 2023 16:39:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00878v2</guid></item><item><title>Attention-enhanced neural differential equations for physics-informed deep learning of ion transport</title><link>http://arxiv.org/abs/2312.02871v1</link><description>Species transport models typically combine partial differential equations(PDEs) with relations from hindered transport theory to quantifyelectromigrative, convective, and diffusive transport through complexnanoporous systems; however, these formulations are frequently substantialsimplifications of the governing dynamics, leading to the poor generalizationperformance of PDE-based models. Given the growing interest in deep learningmethods for the physical sciences, we develop a machine learning-based approachto characterize ion transport across nanoporous membranes. Our proposedframework centers around attention-enhanced neural differential equations thatincorporate electroneutrality-based inductive biases to improve generalizationperformance relative to conventional PDE-based methods. In addition, we studythe role of the attention mechanism in illuminating physically-meaningfulion-pairing relationships across diverse mixture compositions. Further, weinvestigate the importance of pre-training on simulated data from PDE-basedmodels, as well as the performance benefits from hard vs. soft inductivebiases. Our results indicate that physics-informed deep learning solutions canoutperform their classical PDE-based counterparts and provide promising avenuesfor modelling complex transport phenomena across diverse applications.</description><author>Danyal Rehman, John H. Lienhard</author><pubDate>Tue, 05 Dec 2023 16:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02871v1</guid></item><item><title>Can a Tabula Recta provide security in the XXI century?</title><link>http://arxiv.org/abs/2312.02869v1</link><description>In the not so unlikely scenario of total compromise of computers accessibleto a group of users, they might be tempted to resort to human-computablepaper-and-pencil cryptographic methods aided by a classic Tabula Recta, whichhelps to perform addition and subtraction directly with letters. But do theseclassic algorithms, or some new ones using the same simple tools, have anychance against computer-aided cryptanalysis? In this paper I discuss how somehuman-computable algorithms can indeed afford sufficient security in thissituation, drawing conclusions from computer-based statistical analysis. Threekinds of algorithms are discussed: those that concentrate entropy from sharedtext sources, stream ciphers based on arithmetic of non-binary spaces, andhash-like algorithms that may be used to generate a password from a challengetext.</description><author>Francisco Ruiz</author><pubDate>Tue, 05 Dec 2023 16:36:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02869v1</guid></item><item><title>Semi-Supervised Health Index Monitoring with Feature Generation and Fusion</title><link>http://arxiv.org/abs/2312.02867v1</link><description>The Health Index (HI) is crucial for evaluating system health, aiding taskslike anomaly detection and predicting remaining useful life for systemsdemanding high safety and reliability. Tight monitoring is crucial forachieving high precision at a lower cost, with applications such as spraycoating. Obtaining HI labels in real-world applications is oftencost-prohibitive, requiring continuous, precise health measurements. Therefore,it is more convenient to leverage run-to failure datasets that may providepotential indications of machine wear condition, making it necessary to applysemi-supervised tools for HI construction. In this study, we adapt the DeepSemi-supervised Anomaly Detection (DeepSAD) method for HI construction. We usethe DeepSAD embedding as a condition indicators to address interpretabilitychallenges and sensitivity to system-specific factors. Then, we introduce adiversity loss to enrich condition indicators. We employ an alternatingprojection algorithm with isotonic constraints to transform the DeepSADembedding into a normalized HI with an increasing trend. Validation on the PHME2010 milling dataset, a recognized benchmark with ground truth HIs demonstratesmeaningful HIs estimations. Our methodology is then applied to monitor wearstates of thermal spray coatings using high-frequency voltage. Ourcontributions create opportunities for more accessible and reliable HIestimation, particularly in cases where obtaining ground truth HI labels isunfeasible.</description><author>Gaëtan Frusque, Ismail Nejjar, Majid Nabavi, Olga Fink</author><pubDate>Tue, 05 Dec 2023 16:27:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02867v1</guid></item><item><title>DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models</title><link>http://arxiv.org/abs/2310.20138v2</link><description>Large language models pretrained on a huge amount of data capture richknowledge and information in the training data. The ability of datamemorization and regurgitation in pretrained language models, revealed inprevious studies, brings the risk of data leakage. In order to effectivelyreduce these risks, we propose a framework DEPN to Detect and Edit PrivacyNeurons in pretrained language models, partially inspired by knowledge neuronsand model editing. In DEPN, we introduce a novel method, termed as privacyneuron detector, to locate neurons associated with private information, andthen edit these detected privacy neurons by setting their activations to zero.Furthermore, we propose a privacy neuron aggregator dememorize privateinformation in a batch processing manner. Experimental results show that ourmethod can significantly and efficiently reduce the exposure of private dataleakage without deteriorating the performance of the model. Additionally, weempirically demonstrate the relationship between model memorization and privacyneurons, from multiple perspectives, including model size, training time,prompts, privacy neuron distribution, illustrating the robustness of ourapproach.</description><author>Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong</author><pubDate>Tue, 05 Dec 2023 16:14:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20138v2</guid></item><item><title>Lessons from Usable ML Deployments and Application to Wind Turbine Monitoring</title><link>http://arxiv.org/abs/2312.02859v1</link><description>Through past experiences deploying what we call usable ML (one step beyondexplainable ML, including both explanations and other augmenting information)to real-world domains, we have learned three key lessons. First, manyorganizations are beginning to hire people who we call ``bridges'' because theybridge the gap between ML developers and domain experts, and these people filla valuable role in developing usable ML applications. Second, a configurablesystem that enables easily iterating on usable ML interfaces duringcollaborations with bridges is key. Finally, there is a need for continuous,in-deployment evaluations to quantify the real-world impact of usable ML.Throughout this paper, we apply these lessons to the task of wind turbinemonitoring, an essential task in the renewable energy domain. Turbine engineersand data analysts must decide whether to perform costly in-personinvestigations on turbines to prevent potential cases of brakepad failure, andwell-tuned usable ML interfaces can aid with this decision-making process.Through the applications of our lessons to this task, we hope to demonstratethe potential real-world impact of usable ML in the renewable energy domain.</description><author>Alexandra Zytek, Wei-En Wang, Sofia Koukoura, Kalyan Veeramachaneni</author><pubDate>Tue, 05 Dec 2023 16:13:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02859v1</guid></item><item><title>Towards Causal Representations of Climate Model Data</title><link>http://arxiv.org/abs/2312.02858v1</link><description>Climate models, such as Earth system models (ESMs), are crucial forsimulating future climate change based on projected Shared SocioeconomicPathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticatedand invaluable, machine learning-based emulators trained on existing simulationdata can project additional climate scenarios much faster and arecomputationally efficient. However, they often lack generalizability andinterpretability. This work delves into the potential of causal representationlearning, specifically the \emph{Causal Discovery with Single-parent Decoding}(CDSD) method, which could render climate model emulation efficient\textit{and} interpretable. We evaluate CDSD on multiple climate datasets,focusing on emissions, temperature, and precipitation. Our findings shed lighton the challenges, limitations, and promise of using CDSD as a stepping stonetowards more interpretable and robust climate model emulation.</description><author>Julien Boussard, Chandni Nagda, Julia Kaltenborn, Charlotte Emilie Elektra Lange, Philippe Brouillard, Yaniv Gurwicz, Peer Nowack, David Rolnick</author><pubDate>Tue, 05 Dec 2023 16:13:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02858v1</guid></item><item><title>Exploring Error Bits for Memory Failure Prediction: An In-Depth Correlative Study</title><link>http://arxiv.org/abs/2312.02855v1</link><description>In large-scale datacenters, memory failure is a common cause of servercrashes, with uncorrectable errors (UEs) being a major indicator of Dual InlineMemory Module (DIMM) defects. Existing approaches primarily focus on predictingUEs using correctable errors (CEs), without fully considering the informationprovided by error bits. However, error bit patterns have a strong correlationwith the occurrence of uncorrectable errors (UEs). In this paper, we present acomprehensive study on the correlation between CEs and UEs, specificallyemphasizing the importance of spatio-temporal error bit information. Ouranalysis reveals a strong correlation between spatio-temporal error bits and UEoccurrence. Through evaluations using real-world datasets, we demonstrate thatour approach significantly improves prediction performance by 15% in F1-scorecompared to the state-of-the-art algorithms. Overall, our approach effectivelyreduces the number of virtual machine interruptions caused by UEs byapproximately 59%.</description><author>Qiao Yu, Wengui Zhang, Jorge Cardoso, Odej Kao</author><pubDate>Tue, 05 Dec 2023 16:11:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02855v1</guid></item><item><title>Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems</title><link>http://arxiv.org/abs/2312.02852v1</link><description>Domain experts often possess valuable physical insights that are overlookedin fully automated decision-making processes such as Bayesian optimisation. Inthis article we apply high-throughput (batch) Bayesian optimisation alongsideanthropological decision theory to enable domain experts to influence theselection of optimal experiments. Our methodology exploits the hypothesis thathumans are better at making discrete choices than continuous ones and enablesexperts to influence critical early decisions. At each iteration we solve anaugmented multi-objective optimisation problem across a number of alternatesolutions, maximising both the sum of their utility function values and thedeterminant of their covariance matrix, equivalent to their total variability.By taking the solution at the knee point of the Pareto front, we return a setof alternate solutions at each iteration that have both high utility values andare reasonably distinct, from which the expert selects one for evaluation. Wedemonstrate that even in the case of an uninformed practitioner, our algorithmrecovers the regret of standard Bayesian optimisation.</description><author>Tom Savage, Ehecatl Antonio del Rio Chanona</author><pubDate>Tue, 05 Dec 2023 16:09:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02852v1</guid></item><item><title>A Unified Theory of Diversity in Ensemble Learning</title><link>http://arxiv.org/abs/2301.03962v2</link><description>We present a theory of ensemble diversity, explaining the nature of diversityfor a wide range of supervised learning scenarios. This challenge, ofunderstanding ensemble diversity, has been referred to as the "holy grail" ofensemble learning, an open research issue for over 30 years. Our frameworkreveals that diversity is in fact a hidden dimension in the bias-variancedecomposition of the ensemble loss. We prove a family of exactbias-variance-diversity decompositions, for both regression and classification,e.g., squared, cross-entropy, and Poisson losses. For losses where an additivebias-variance decomposition is not available (e.g., 0/1 loss) we present analternative approach, which precisely quantifies the effects of diversity,turning out to be dependent on the label distribution. Experiments show how wecan use our framework to understand the diversity-encouraging mechanisms ofpopular methods: Bagging, Boosting, and Random Forests.</description><author>Danny Wood, Tingting Mu, Andrew Webb, Henry Reeve, Mikel Lujan, Gavin Brown</author><pubDate>Tue, 05 Dec 2023 16:09:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.03962v2</guid></item><item><title>One-step Diffusion with Distribution Matching Distillation</title><link>http://arxiv.org/abs/2311.18828v3</link><description>Diffusion models generate high-quality images but require dozens of forwardpasses. We introduce Distribution Matching Distillation (DMD), a procedure totransform a diffusion model into a one-step image generator with minimal impacton image quality. We enforce the one-step image generator match the diffusionmodel at distribution level, by minimizing an approximate KL divergence whosegradient can be expressed as the difference between 2 score functions, one ofthe target distribution and the other of the synthetic distribution beingproduced by our one-step generator. The score functions are parameterized astwo diffusion models trained separately on each distribution. Combined with asimple regression loss matching the large-scale structure of the multi-stepdiffusion outputs, our method outperforms all published few-step diffusionapproaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shotCOCO-30k, comparable to Stable Diffusion but orders of magnitude faster.Utilizing FP16 inference, our model generates images at 20 FPS on modernhardware.</description><author>Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T. Freeman, Taesung Park</author><pubDate>Tue, 05 Dec 2023 16:08:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18828v3</guid></item><item><title>A Kernel-Based Neural Network Test for High-dimensional Sequencing Data Analysis</title><link>http://arxiv.org/abs/2312.02850v1</link><description>The recent development of artificial intelligence (AI) technology, especiallythe advance of deep neural network (DNN) technology, has revolutionized manyfields. While DNN plays a central role in modern AI technology, it has beenrarely used in sequencing data analysis due to challenges brought byhigh-dimensional sequencing data (e.g., overfitting). Moreover, due to thecomplexity of neural networks and their unknown limiting distributions,building association tests on neural networks for genetic association analysisremains a great challenge. To address these challenges and fill the importantgap of using AI in high-dimensional sequencing data analysis, we introduce anew kernel-based neural network (KNN) test for complex association analysis ofsequencing data. The test is built on our previously developed KNN framework,which uses random effects to model the overall effects of high-dimensionalgenetic data and adopts kernel-based neural network structures to model complexgenotype-phenotype relationships. Based on KNN, a Wald-type test is thenintroduced to evaluate the joint association of high-dimensional genetic datawith a disease phenotype of interest, considering non-linear and non-additiveeffects (e.g., interaction effects). Through simulations, we demonstrated thatour proposed method attained higher power compared to the sequence kernelassociation test (SKAT), especially in the presence of non-linear andinteraction effects. Finally, we apply the methods to the whole genomesequencing (WGS) dataset from the Alzheimer's Disease Neuroimaging Initiative(ADNI) study, investigating new genes associated with the hippocampal volumechange over time.</description><author>Tingting Hou, Chang Jiang, Qing Lu</author><pubDate>Tue, 05 Dec 2023 16:06:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02850v1</guid></item><item><title>Balance is Essence: Accelerating Sparse Training via Adaptive Gradient Correction</title><link>http://arxiv.org/abs/2301.03573v2</link><description>Despite impressive performance, deep neural networks require significantmemory and computation costs, prohibiting their application inresource-constrained scenarios. Sparse training is one of the most commontechniques to reduce these costs, however, the sparsity constraints adddifficulty to the optimization, resulting in an increase in training time andinstability. In this work, we aim to overcome this problem and achievespace-time co-efficiency. To accelerate and stabilize the convergence of sparsetraining, we analyze the gradient changes and develop an adaptive gradientcorrection method. Specifically, we approximate the correlation between thecurrent and previous gradients, which is used to balance the two gradients toobtain a corrected gradient. Our method can be used with the most popularsparse training pipelines under both standard and adversarial setups.Theoretically, we prove that our method can accelerate the convergence rate ofsparse training. Extensive experiments on multiple datasets, modelarchitectures, and sparsities demonstrate that our method outperforms leadingsparse training methods by up to \textbf{5.0\%} in accuracy given the samenumber of training epochs, and reduces the number of training epochs by up to\textbf{52.1\%} to achieve the same accuracy. Our code is available on:\url{https://github.com/StevenBoys/AGENT}.</description><author>Bowen Lei, Dongkuan Xu, Ruqi Zhang, Shuren He, Bani K. Mallick</author><pubDate>Tue, 05 Dec 2023 16:05:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.03573v2</guid></item><item><title>AlignBench: Benchmarking Chinese Alignment of Large Language Models</title><link>http://arxiv.org/abs/2311.18743v3</link><description>Alignment has become a critical step for instruction-tuned Large LanguageModels (LLMs) to become helpful assistants. However, effective evaluation ofalignment for emerging Chinese LLMs is still significantly lacking, calling forreal-scenario grounded, open-ended, challenging and automatic evaluationstailored for alignment. To fill in this gap, we introduce AlignBench, acomprehensive multi-dimensional benchmark for evaluating LLMs' alignment inChinese. Equipped with a human-in-the-loop data curation pipeline, ourbenchmark employs a rule-calibrated multi-dimensional LLM-as-Judge withChain-of-Thought to generate explanations and final ratings as evaluations,ensuring high reliability and interpretability. Furthermore, we reportAlignBench evaluated by CritiqueLLM, a dedicated Chinese evaluator LLM thatrecovers 95% of GPT-4's evaluation ability. We will provide public APIs forevaluating AlignBench with CritiqueLLM to facilitate the evaluation of LLMs'Chinese alignment. All evaluation codes, data, and LLM generations areavailable at \url{https://github.com/THUDM/AlignBench}.</description><author>Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, Jie Tang</author><pubDate>Tue, 05 Dec 2023 16:04:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18743v3</guid></item><item><title>Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space</title><link>http://arxiv.org/abs/2312.02849v1</link><description>We develop a theory of finite-dimensional polyhedral subsets over theWasserstein space and optimization of functionals over them via first-ordermethods. Our main application is to the problem of mean-field variationalinference, which seeks to approximate a distribution $\pi$ over $\mathbb{R}^d$by a product measure $\pi^\star$. When $\pi$ is strongly log-concave andlog-smooth, we provide (1) approximation rates certifying that $\pi^\star$ isclose to the minimizer $\pi^\star_\diamond$ of the KL divergence over a\emph{polyhedral} set $\mathcal{P}_\diamond$, and (2) an algorithm forminimizing $\text{KL}(\cdot\|\pi)$ over $\mathcal{P}_\diamond$ with acceleratedcomplexity $O(\sqrt \kappa \log(\kappa d/\varepsilon^2))$, where $\kappa$ isthe condition number of $\pi$.</description><author>Yiheng Jiang, Sinho Chewi, Aram-Alexandre Pooladian</author><pubDate>Tue, 05 Dec 2023 16:02:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02849v1</guid></item><item><title>Stable Segment Anything Model</title><link>http://arxiv.org/abs/2311.15776v2</link><description>The Segment Anything Model (SAM) achieves remarkable promptable segmentationgiven high-quality prompts which, however, often require good skills tospecify. To make SAM robust to casual prompts, this paper presents the firstcomprehensive analysis on SAM's segmentation stability across a diversespectrum of prompt qualities, notably imprecise bounding boxes and insufficientpoints. Our key finding reveals that given such low-quality prompts, SAM's maskdecoder tends to activate image features that are biased towards the backgroundor confined to specific object parts. To mitigate this issue, our key ideaconsists of calibrating solely SAM's mask attention by adjusting the samplinglocations and amplitudes of image features, while the original SAM modelarchitecture and weights remain unchanged. Consequently, our deformablesampling plugin (DSP) enables SAM to adaptively shift attention to the promptedtarget regions in a data-driven manner, facilitated by our effective robusttraining strategy (RTS). During inference, dynamic routing plugin (DRP) isproposed that toggles SAM between the deformable and regular grid samplingmodes, conditioned on the input prompt quality. Thus, our solution, termedStable-SAM, offers several advantages: 1) improved SAM's segmentation stabilityacross a wide range of prompt qualities, while 2) retaining SAM's powerfulpromptable segmentation efficiency and generality, with 3) minimal learnableparameters (0.08 M) and fast adaptation (by 1 training epoch). Extensiveexperiments across multiple datasets validate the effectiveness and advantagesof our approach, underscoring Stable-SAM as a more robust solution forsegmenting anything. Codes will be released upon acceptance.https://github.com/fanq15/Stable-SAM</description><author>Qi Fan, Xin Tao, Lei Ke, Mingqiao Ye, Yuan Zhang, Pengfei Wan, Zhongyuan Wang, Yu-Wing Tai, Chi-Keung Tang</author><pubDate>Tue, 05 Dec 2023 15:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15776v2</guid></item><item><title>Are Vision Transformers More Data Hungry Than Newborn Visual Systems?</title><link>http://arxiv.org/abs/2312.02843v1</link><description>Vision transformers (ViTs) are top performing models on many computer visionbenchmarks and can accurately predict human behavior on object recognitiontasks. However, researchers question the value of using ViTs as models ofbiological learning because ViTs are thought to be more data hungry thanbrains, with ViTs requiring more training data to reach similar levels ofperformance. To test this assumption, we directly compared the learningabilities of ViTs and animals, by performing parallel controlled rearingexperiments on ViTs and newborn chicks. We first raised chicks in impoverishedvisual environments containing a single object, then simulated the trainingdata available in those environments by building virtual animal chambers in avideo game engine. We recorded the first-person images acquired by agentsmoving through the virtual chambers and used those images to train selfsupervised ViTs that leverage time as a teaching signal, akin to biologicalvisual systems. When ViTs were trained through the eyes of newborn chicks, theViTs solved the same view invariant object recognition tasks as the chicks.Thus, ViTs were not more data hungry than newborn visual systems: both learnedview invariant object representations in impoverished visual environments. Theflexible and generic attention based learning mechanism in ViTs combined withthe embodied data streams available to newborn animals appears sufficient todrive the development of animal-like object recognition.</description><author>Lalit Pandey, Samantha M. W. Wood, Justin N. Wood</author><pubDate>Tue, 05 Dec 2023 15:53:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02843v1</guid></item><item><title>Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach</title><link>http://arxiv.org/abs/2308.08410v2</link><description>The eikonal equation has become an indispensable tool for modeling cardiacelectrical activation accurately and efficiently. In principle, by matchingclinically recorded and eikonal-based electrocardiograms (ECGs), it is possibleto build patient-specific models of cardiac electrophysiology in a purelynon-invasive manner. Nonetheless, the fitting procedure remains a challengingtask. The present study introduces a novel method, Geodesic-BP, to solve theinverse eikonal problem. Geodesic-BP is well-suited for GPU-accelerated machinelearning frameworks, allowing us to optimize the parameters of the eikonalequation to reproduce a given ECG. We show that Geodesic-BP can reconstruct asimulated cardiac activation with high accuracy in a synthetic test case, evenin the presence of modeling inaccuracies. Furthermore, we apply our algorithmto a publicly available dataset of a biventricular rabbit model, with promisingresults. Given the future shift towards personalized medicine, Geodesic-BP hasthe potential to help in future functionalizations of cardiac models meetingclinical time constraints while maintaining the physiological accuracy ofstate-of-the-art cardiac models.</description><author>Thomas Grandits, Jan Verhülsdonk, Gundolf Haase, Alexander Effland, Simone Pezzuto</author><pubDate>Tue, 05 Dec 2023 15:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08410v2</guid></item><item><title>MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition</title><link>http://arxiv.org/abs/2312.02829v1</link><description>With the advent of deep learning, progressively larger neural networks havebeen designed to solve complex tasks. We take advantage of these capacity-richmodels to lower the cost of inference by exploiting computation insuperposition. To reduce the computational burden per input, we proposeMultiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handlingmany inputs at once. MIMONets augment various deep neural network architectureswith variable binding mechanisms to represent an arbitrary number of inputs ina compositional data structure via fixed-width distributed representations.Accordingly, MIMONets adapt nonlinear neural transformations to process thedata structure holistically, leading to a speedup nearly proportional to thenumber of superposed input items in the data structure. After processing insuperposition, an unbinding mechanism recovers each transformed input ofinterest. MIMONets also provide a dynamic trade-off between accuracy andthroughput by an instantaneous on-demand switching between a set ofaccuracy-throughput operating points, yet within a single set of fixedparameters. We apply the concept of MIMONets to both CNN and Transformerarchitectures resulting in MIMOConv and MIMOFormer, respectively. Empiricalevaluations show that MIMOConv achieves about 2-4 x speedup at an accuracydelta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 andCIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaininga high average accuracy within a [-1.07, -3.43]% delta on the long range arenabenchmark. Finally, we provide mathematical bounds on the interference betweensuperposition channels in MIMOFormer. Our code is available athttps://github.com/IBM/multiple-input-multiple-output-nets.</description><author>Nicolas Menet, Michael Hersche, Geethan Karunaratne, Luca Benini, Abu Sebastian, Abbas Rahimi</author><pubDate>Tue, 05 Dec 2023 15:25:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02829v1</guid></item><item><title>Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications</title><link>http://arxiv.org/abs/2312.02828v1</link><description>The Stochastic Approximation (SA) algorithm introduced by Robbins and Monroin 1951 has been a standard method for solving equations of the form$\mathbf{f}({\boldsymbol {\theta}}) = \mathbf{0}$, when only noisy measurementsof $\mathbf{f}(\cdot)$ are available. If $\mathbf{f}({\boldsymbol {\theta}}) =\nabla J({\boldsymbol {\theta}})$ for some function $J(\cdot)$, then SA canalso be used to find a stationary point of $J(\cdot)$. In much of theliterature, it is assumed that the error term ${\boldsymbol {xi}}_{t+1}$ haszero conditional mean, and that its conditional variance is bounded as afunction of $t$ (though not necessarily with respect to ${\boldsymbol{\theta}}_t$). Also, for the most part, the emphasis has been on``synchronous'' SA, whereby, at each time $t$, \textit{every} component of${\boldsymbol {\theta}}_t$ is updated. Over the years, SA has been applied to avariety of areas, out of which two are the focus in this paper: Convex andnonconvex optimization, and Reinforcement Learning (RL). As it turns out, inthese applications, the above-mentioned assumptions do not always hold. Inzero-order methods, the error neither has zero mean nor bounded conditionalvariance. In the present paper, we extend SA theory to encompass errors withnonzero conditional mean and/or unbounded conditional variance, and alsoasynchronous SA. In addition, we derive estimates for the rate of convergenceof the algorithm. Then we apply the new results to problems in nonconvexoptimization, and to Markovian SA, a recently emerging area in RL. We provethat SA converges in these situations, and compute the ``optimal step sizesequences'' to maximize the estimated rate of convergence.</description><author>Rajeeva L. Karandikar, M. Vidyasagar</author><pubDate>Tue, 05 Dec 2023 15:22:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02828v1</guid></item><item><title>Calibrated Adaptive Teacher for Domain Adaptive Intelligent Fault Diagnosis</title><link>http://arxiv.org/abs/2312.02826v1</link><description>Intelligent Fault Diagnosis (IFD) based on deep learning has proven to be aneffective and flexible solution, attracting extensive research. Deep neuralnetworks can learn rich representations from vast amounts of representativelabeled data for various applications. In IFD, they achieve high classificationperformance from signals in an end-to-end manner, without requiring extensivedomain knowledge. However, deep learning models usually only perform well onthe data distribution they have been trained on. When applied to a differentdistribution, they may experience performance drops. This is also observed inIFD, where assets are often operated in working conditions different from thosein which labeled data have been collected. Unsupervised domain adaptation (UDA)deals with the scenario where labeled data are available in a source domain,and only unlabeled data are available in a target domain, where domains maycorrespond to operating conditions. Recent methods rely on training withconfident pseudo-labels for target samples. However, the confidence-basedselection of pseudo-labels is hindered by poorly calibrated confidenceestimates in the target domain, primarily due to over-confident predictions,which limits the quality of pseudo-labels and leads to error accumulation. Inthis paper, we propose a novel UDA method called Calibrated Adaptive Teacher(CAT), where we propose to calibrate the predictions of the teacher networkthroughout the self-training process, leveraging post-hoc calibrationtechniques. We evaluate CAT on domain-adaptive IFD and perform extensiveexperiments on the Paderborn benchmark for bearing fault diagnosis undervarying operating conditions. Our proposed method achieves state-of-the-artperformance on most transfer tasks.</description><author>Florent Forest, Olga Fink</author><pubDate>Tue, 05 Dec 2023 15:19:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02826v1</guid></item><item><title>On the instrumental variable estimation with many weak and invalid instruments</title><link>http://arxiv.org/abs/2207.03035v2</link><description>We discuss the fundamental issue of identification in linear instrumentalvariable (IV) models with unknown IV validity. With the assumption of the"sparsest rule", which is equivalent to the plurality rule but becomesoperational in computation algorithms, we investigate and prove the advantagesof non-convex penalized approaches over other IV estimators based on two-stepselections, in terms of selection consistency and accommodation forindividually weak IVs. Furthermore, we propose a surrogate sparsest penaltythat aligns with the identification condition and provides oracle sparsestructure simultaneously. Desirable theoretical properties are derived for theproposed estimator with weaker IV strength conditions compared to the previousliterature. Finite sample properties are demonstrated using simulations and theselection and estimation method is applied to an empirical study concerning theeffect of BMI on diastolic blood pressure.</description><author>Yiqi Lin, Frank Windmeijer, Xinyuan Song, Qingliang Fan</author><pubDate>Tue, 05 Dec 2023 15:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.03035v2</guid></item><item><title>RotaTR: Detection Transformer for Dense and Rotated Object</title><link>http://arxiv.org/abs/2312.02821v1</link><description>Detecting the objects in dense and rotated scenes is a challenging task.Recent works on this topic are mostly based on Faster RCNN or Retinanet. Asthey are highly dependent on the pre-set dense anchors and the NMS operation,the approach is indirect and suboptimal.The end-to-end DETR-based detectorshave achieved great success in horizontal object detection and many other areaslike segmentation, tracking, action recognition and etc.However, the DETR-baseddetectors perform poorly on dense rotated target tasks and perform worse thanmost modern CNN-based detectors. In this paper, we find the most significantreason for the poor performance is that the original attention can notaccurately focus on the oriented targets. Accordingly, we propose Rotatedobject detection TRansformer (RotaTR) as an extension of DETR to orienteddetection. Specifically, we design Rotation Sensitive deformable (RSDeform)attention to enhance the DETR's ability to detect oriented targets. It is usedto build the feature alignment module and rotation-sensitive decoder for ourmodel. We test RotaTR on four challenging-oriented benchmarks. It shows a greatadvantage in detecting dense and oriented objects compared to the originalDETR. It also achieves competitive results when compared to thestate-of-the-art.</description><author>Zhu Yuke, Ruan Yumeng, Yang Lei, Guo Sheng</author><pubDate>Tue, 05 Dec 2023 15:06:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02821v1</guid></item><item><title>Revitalizing Legacy Video Content: Deinterlacing with Bidirectional Information Propagation</title><link>http://arxiv.org/abs/2310.19535v2</link><description>Due to old CRT display technology and limited transmission bandwidth, earlyfilm and TV broadcasts commonly used interlaced scanning. This meant each fieldcontained only half of the information. Since modern displays require fullframes, this has spurred research into deinterlacing, i.e. restoring themissing information in legacy video content. In this paper, we present adeep-learning-based method for deinterlacing animated and live-action content.Our proposed method supports bidirectional spatio-temporal informationpropagation across multiple scales to leverage information in both space andtime. More specifically, we design a Flow-guided Refinement Block (FRB) whichperforms feature refinement including alignment, fusion, and rectification.Additionally, our method can process multiple fields simultaneously, reducingper-frame processing time, and potentially enabling real-time processing. Ourexperimental results demonstrate that our proposed method achieves superiorperformance compared to existing methods.</description><author>Zhaowei Gao, Mingyang Song, Christopher Schroers, Yang Zhang</author><pubDate>Tue, 05 Dec 2023 15:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19535v2</guid></item><item><title>ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems</title><link>http://arxiv.org/abs/2306.04743v2</link><description>Natural Language to SQL systems (NL-to-SQL) have recently shown a significantincrease in accuracy for natural language to SQL query translation. Thisimprovement is due to the emergence of transformer-based language models, andthe popularity of the Spider benchmark - the de-facto standard for evaluatingNL-to-SQL systems. The top NL-to-SQL systems reach accuracies of up to 85\%.However, Spider mainly contains simple databases with few tables, columns, andentries, which does not reflect a realistic setting. Moreover, complexreal-world databases with domain-specific content have little to no trainingdata available in the form of NL/SQL-pairs leading to poor performance ofexisting NL-to-SQL systems. In this paper, we introduce ScienceBenchmark, a new complex NL-to-SQLbenchmark for three real-world, highly domain-specific databases. For this newbenchmark, SQL experts and domain experts created high-quality NL/SQL-pairs foreach domain. To garner more data, we extended the small amount ofhuman-generated data with synthetic data generated using GPT-3. We show thatour benchmark is highly challenging, as the top performing systems on Spiderachieve a very low performance on our benchmark. Thus, the challenge ismany-fold: creating NL-to-SQL systems for highly complex domains with a smallamount of hand-made training data augmented with synthetic data. To ourknowledge, ScienceBenchmark is the first NL-to-SQL benchmark designed withcomplex real-world scientific databases, containing challenging training andtest data carefully validated by domain experts.</description><author>Yi Zhang, Jan Deriu, George Katsogiannis-Meimarakis, Catherine Kosten, Georgia Koutrika, Kurt Stockinger</author><pubDate>Tue, 05 Dec 2023 15:05:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04743v2</guid></item><item><title>Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix</title><link>http://arxiv.org/abs/2312.02820v1</link><description>In multilingual translation research, the comprehension and utilization oflanguage families are of paramount importance. Nevertheless, clusteringlanguages based solely on their ancestral families can yield suboptimal resultsdue to variations in the datasets employed during the model's training phase.To mitigate this challenge, we introduce an innovative method that leveragesthe fisher information matrix (FIM) to cluster language families, anchored onthe multilingual translation model's characteristics. We hypothesize thatlanguage pairs with similar effects on model parameters exhibit a considerabledegree of linguistic congruence and should thus be grouped cohesively. Thisconcept has led us to define pseudo language families. We provide an in-depthdiscussion regarding the inception and application of these pseudo languagefamilies. Empirical evaluations reveal that employing these pseudo languagefamilies enhances performance over conventional language families in adapting amultilingual translation model to unfamiliar language pairs. The proposedmethodology may also be extended to scenarios requiring language similaritymeasurements. The source code and associated scripts can be accessed athttps://github.com/ecoli-hit/PseudoFamily.</description><author>Xinyu Ma, Xuebo Liu, Min Zhang</author><pubDate>Tue, 05 Dec 2023 15:03:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02820v1</guid></item><item><title>Deterministic Guidance Diffusion Model for Probabilistic Weather Forecasting</title><link>http://arxiv.org/abs/2312.02819v1</link><description>Weather forecasting requires not only accuracy but also the ability toperform probabilistic prediction. However, deterministic weather forecastingmethods do not support probabilistic predictions, and conversely, probabilisticmodels tend to be less accurate. To address these challenges, in this paper, weintroduce the \textbf{\textit{D}}eterministic \textbf{\textit{G}}uidance\textbf{\textit{D}}iffusion \textbf{\textit{M}}odel (DGDM) for probabilisticweather forecasting, integrating benefits of both deterministic andprobabilistic approaches. During the forward process, both the deterministicand probabilistic models are trained end-to-end. In the reverse process,weather forecasting leverages the predicted result from the deterministicmodel, using as an intermediate starting point for the probabilistic model. Byfusing deterministic models with probabilistic models in this manner, DGDM iscapable of providing accurate forecasts while also offering probabilisticpredictions. To evaluate DGDM, we assess it on the global weather forecastingdataset (WeatherBench) and the common video frame prediction benchmark (MovingMNIST). We also introduce and evaluate the Pacific Northwest Windstorm(PNW)-Typhoon weather satellite dataset to verify the effectiveness of DGDM inhigh-resolution regional forecasting. As a result of our experiments, DGDMachieves state-of-the-art results not only in global forecasting but also inregional forecasting. The code is available at:\url{https://github.com/DongGeun-Yoon/DGDM}.</description><author>Donggeun Yoon, Minseok Seo, Doyi Kim, Yeji Choi, Donghyeon Cho</author><pubDate>Tue, 05 Dec 2023 15:03:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02819v1</guid></item><item><title>BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models</title><link>http://arxiv.org/abs/2312.02813v1</link><description>Diffusion models have made tremendous progress in text-driven image and videogeneration. Now text-to-image foundation models are widely applied to variousdownstream image synthesis tasks, such as controllable image generation andimage editing, while downstream video synthesis tasks are less explored forseveral reasons. First, it requires huge memory and compute overhead to train avideo generation foundation model. Even with video foundation models,additional costly training is still required for downstream video synthesistasks. Second, although some works extend image diffusion models into videos ina training-free manner, temporal consistency cannot be well kept. Finally,these adaption methods are specifically designed for one task and fail togeneralize to different downstream video synthesis tasks. To mitigate theseissues, we propose a training-free general-purpose video synthesis framework,coined as BIVDiff, via bridging specific image diffusion models and generaltext-to-video foundation diffusion models. Specifically, we first use an imagediffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise videogeneration, then perform Mixed Inversion on the generated video, and finallyinput the inverted latents into the video diffusion model for temporalsmoothing. Decoupling image and video models enables flexible image modelselection for different purposes, which endows the framework with strong taskgeneralization and high efficiency. To validate the effectiveness and generaluse of BIVDiff, we perform a wide range of video generation tasks, includingcontrollable video generation video editing, video inpainting and outpainting.Our project page is available at https://bivdiff.github.io.</description><author>Fengyuan Shi, Jiaxi Gu, Hang Xu, Songcen Xu, Wei Zhang, Limin Wang</author><pubDate>Tue, 05 Dec 2023 14:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02813v1</guid></item><item><title>Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and Qualitative Analysis</title><link>http://arxiv.org/abs/2311.15218v2</link><description>The application of Machine learning to finance has become a familiarapproach, even more so in stock market forecasting. The stock market is highlyvolatile and huge amounts of data are generated every minute globally. Theextraction of effective intelligence from this data is of critical importance.However, a collaboration of numerical stock data with qualitative text data canbe a challenging task. In this work, we accomplish this and provide anunprecedented, publicly available dataset with technical and fundamental data,sentiment that we gathered from News Archives, TV news captions, RadioTranscripts, Tweets, Daily financial newspapers, etc. The text data entriesused for sentiment extraction total more than 1.4 Million. The dataset consistsof daily entries from January 2018 to December 2022 for 8 companiesrepresenting diverse industrial sectors and the Dow Jones Industrial Average(DJIA) as a whole. Holistic Fundamental and Technical data is provided trainingready for Model learning and deployment. The data generated could be used forIncremental online learning with real-time data points retrieved daily, sincethere was no stagnant data utilized, all the data was retired from APIs orself-designed scripts. Moreover, the utilization of Spearman's rank correlationover real-time data, linking stock returns with sentiment analysis has producednoteworthy results for the DJIA achieving accuracy levels surpassing 60\%. Thedataset is made available at https://github.com/batking24/Huge-Stock-Dataset</description><author>Sai Akash Bathini, Dagli Cihan</author><pubDate>Tue, 05 Dec 2023 14:49:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15218v2</guid></item><item><title>Score-Aware Policy-Gradient Methods and Performance Guarantees using Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks and Queueing Systems</title><link>http://arxiv.org/abs/2312.02804v1</link><description>Stochastic networks and queueing systems often lead to Markov decisionprocesses (MDPs) with large state and action spaces as well as nonconvexobjective functions, which hinders the convergence of many reinforcementlearning (RL) algorithms. Policy-gradient methods perform well on MDPs withlarge state and action spaces, but they sometimes experience slow convergencedue to the high variance of the gradient estimator. In this paper, we show thatsome of these difficulties can be circumvented by exploiting the structure ofthe underlying MDP. We first introduce a new family of gradient estimatorscalled score-aware gradient estimators (SAGEs). When the stationarydistribution of the MDP belongs to an exponential family parametrized by thepolicy parameters, SAGEs allow us to estimate the policy gradient withoutrelying on value-function estimation, contrary to classical policy-gradientmethods like actor-critic. To demonstrate their applicability, we examine twocommon control problems arising in stochastic networks and queueing systemswhose stationary distributions have a product-form, a special case ofexponential families. As a second contribution, we show that, under appropriateassumptions, the policy under a SAGE-based policy-gradient method has a largeprobability of converging to an optimal policy, provided that it startssufficiently close to it, even with a nonconvex objective function and multiplemaximizers. Our key assumptions are that, locally around a maximizer, anondegeneracy property of the Hessian of the objective function holds and aLyapunov function exists. Finally, we conduct a numerical comparison between aSAGE-based policy-gradient method and an actor-critic algorithm. The resultsdemonstrate that the SAGE-based method finds close-to-optimal policies morerapidly, highlighting its superior performance over the traditionalactor-critic method.</description><author>Céline Comte, Matthieu Jonckheere, Jaron Sanders, Albert Senen-Cerda</author><pubDate>Tue, 05 Dec 2023 14:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02804v1</guid></item><item><title>Leveraging Domain Adaptation and Data Augmentation to Improve Qur'anic IR in English and Arabic</title><link>http://arxiv.org/abs/2312.02803v1</link><description>In this work, we approach the problem of Qur'anic information retrieval (IR)in Arabic and English. Using the latest state-of-the-art methods in neural IR,we research what helps to tackle this task more efficiently. Training retrievalmodels requires a lot of data, which is difficult to obtain for trainingin-domain. Therefore, we commence with training on a large amount of generaldomain data and then continue training on in-domain data. To handle the lack ofin-domain data, we employed a data augmentation technique, which considerablyimproved results in MRR@10 and NDCG@5 metrics, setting the state-of-the-art inQur'anic IR for both English and Arabic. The absence of an Islamic corpus anddomain-specific model for IR task in English motivated us to address this lackof resources and take preliminary steps of the Islamic corpus compilation anddomain-specific language model (LM) pre-training, which helped to improve theperformance of the retrieval models that use the domain-specific LM as theshared backbone. We examined several language models (LMs) in Arabic to selectone that efficiently deals with the Qur'anic IR task. Besides transferringsuccessful experiments from English to Arabic, we conducted additionalexperiments with retrieval task in Arabic to amortize the scarcity of generaldomain datasets used to train the retrieval models. Handling Qur'anic IR taskcombining English and Arabic allowed us to enhance the comparison and sharevaluable insights across models and languages.</description><author>Vera Pavlova</author><pubDate>Tue, 05 Dec 2023 14:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02803v1</guid></item><item><title>NeuroMixGDP: A Neural Collapse-Inspired Random Mixup for Private Data Release</title><link>http://arxiv.org/abs/2202.06467v2</link><description>Privacy-preserving data release algorithms have gained increasing attentionfor their ability to protect user privacy while enabling downstream machinelearning tasks. However, the utility of current popular algorithms is notalways satisfactory. Mixup of raw data provides a new way of data augmentation,which can help improve utility. However, its performance drasticallydeteriorates when differential privacy (DP) noise is added. To address thisissue, this paper draws inspiration from the recently observed Neural Collapse(NC) phenomenon, which states that the last layer features of a neural networkconcentrate on the vertices of a simplex as Equiangular Tight Frame (ETF). Wepropose a scheme to mixup the Neural Collapse features to exploit the ETFsimplex structure and release noisy mixed features to enhance the utility ofthe released data. By using Gaussian Differential Privacy (GDP), we obtain anasymptotic rate for the optimal mixup degree. To further enhance the utilityand address the label collapse issue when the mixup degree is large, we proposea Hierarchical sampling method to stratify the mixup samples on a small numberof classes. This method remarkably improves utility when the number of classesis large. Extensive experiments demonstrate the effectiveness of our proposedmethod in protecting against attacks and improving utility. In particular, ourapproach shows significantly improved utility compared to directly trainingclassification networks with DPSGD on CIFAR100 and MiniImagenet datasets,highlighting the benefits of using privacy-preserving data release. We releasereproducible code in https://github.com/Lidonghao1996/NeuroMixGDP.</description><author>Donghao Li, Yang Cao, Yuan Yao</author><pubDate>Tue, 05 Dec 2023 14:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.06467v2</guid></item><item><title>Weakly Supervised Detection of Hallucinations in LLM Activations</title><link>http://arxiv.org/abs/2312.02798v1</link><description>We propose an auditing method to identify whether a large language model(LLM) encodes patterns such as hallucinations in its internal states, which maypropagate to downstream tasks. We introduce a weakly supervised auditingtechnique using a subset scanning approach to detect anomalous patterns in LLMactivations from pre-trained models. Importantly, our method does not needknowledge of the type of patterns a-priori. Instead, it relies on a referencedataset devoid of anomalies during testing. Further, our approach enables theidentification of pivotal nodes responsible for encoding these patterns, whichmay offer crucial insights for fine-tuning specific sub-networks for biasmitigation. We introduce two new scanning methods to handle LLM activations foranomalous sentences that may deviate from the expected distribution in eitherdirection. Our results confirm prior findings of BERT's limited internalcapacity for encoding hallucinations, while OPT appears capable of encodinghallucination information internally. Importantly, our scanning approach,without prior exposure to false statements, performs comparably to a fullysupervised out-of-distribution classifier.</description><author>Miriam Rateike, Celia Cintas, John Wamburu, Tanya Akumu, Skyler Speakman</author><pubDate>Tue, 05 Dec 2023 14:35:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02798v1</guid></item><item><title>Materials Expert-Artificial Intelligence for Materials Discovery</title><link>http://arxiv.org/abs/2312.02796v1</link><description>The advent of material databases provides an unprecedented opportunity touncover predictive descriptors for emergent material properties from vast dataspace. However, common reliance on high-throughput ab initio data necessarilyinherits limitations of such data: mismatch with experiments. On the otherhand, experimental decisions are often guided by an expert's intuition honedfrom experiences that are rarely articulated. We propose using machine learningto "bottle" such operational intuition into quantifiable descriptors usingexpertly curated measurement-based data. We introduce "MaterialsExpert-Artificial Intelligence" (ME-AI) to encapsulate and articulate thishuman intuition. As a first step towards such a program, we focus on thetopological semimetal (TSM) among square-net materials as the property inspiredby the expert-identified descriptor based on structural information: thetolerance factor. We start by curating a dataset encompassing 12 primaryfeatures of 879 square-net materials, using experimental data wheneverpossible. We then use Dirichlet-based Gaussian process regression using aspecialized kernel to reveal composite descriptors for square-net topologicalsemimetals. The ME-AI learned descriptors independently reproduce expertintuition and expand upon it. Specifically, new descriptors point tohypervalency as a critical chemical feature predicting TSM within square-netcompounds. Our success with a carefully defined problem points to the "machinebottling human insight" approach as promising for machine learning-aidedmaterial discovery.</description><author>Yanjun Liu, Milena Jovanovic, Krishnanand Mallayya, Wesley J. Maddox, Andrew Gordon Wilson, Sebastian Klemenz, Leslie M. Schoop, Eun-Ah Kim</author><pubDate>Tue, 05 Dec 2023 14:29:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02796v1</guid></item><item><title>Machine Learning Driven Sensitivity Analysis of E3SM Land Model Parameters for Wetland Methane Emissions</title><link>http://arxiv.org/abs/2312.02786v1</link><description>Methane (CH4) is the second most critical greenhouse gas after carbondioxide, contributing to 16-25% of the observed atmospheric warming. Wetlandsare the primary natural source of methane emissions globally. However, wetlandmethane emission estimates from biogeochemistry models contain considerableuncertainty. One of the main sources of this uncertainty arises from thenumerous uncertain model parameters within various physical, biological, andchemical processes that influence methane production, oxidation, and transport.Sensitivity Analysis (SA) can help identify critical parameters for methaneemission and achieve reduced biases and uncertainties in future projections.This study performs SA for 19 selected parameters responsible for criticalbiogeochemical processes in the methane module of the Energy Exascale EarthSystem Model (E3SM) land model (ELM). The impact of these parameters on variousCH4 fluxes is examined at 14 FLUXNET- CH4 sites with diverse vegetation types.Given the extensive number of model simulations needed for globalvariance-based SA, we employ a machine learning (ML) algorithm to emulate thecomplex behavior of ELM methane biogeochemistry. ML enables the computationaltime to be shortened significantly from 6 CPU hours to 0.72 milliseconds,achieving reduced computational costs. We found that parameters linked to CH4production and diffusion generally present the highest sensitivities despiteapparent seasonal variation. Comparing simulated emissions from perturbedparameter sets against FLUXNET-CH4 observations revealed that betterperformances can be achieved at each site compared to the default parametervalues. This presents a scope for further improving simulated emissions usingparameter calibration with advanced optimization techniques like Bayesianoptimization.</description><author>Sandeep Chinta, Xiang Gao, Qing Zhu</author><pubDate>Tue, 05 Dec 2023 14:16:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02786v1</guid></item><item><title>Large Language Models on Graphs: A Comprehensive Survey</title><link>http://arxiv.org/abs/2312.02783v1</link><description>Large language models (LLMs), such as ChatGPT and LLaMA, are creatingsignificant advancements in natural language processing, due to their strongtext encoding/decoding ability and newly found emergent capability (e.g.,reasoning). While LLMs are mainly designed to process pure texts, there aremany real-world scenarios where text data are associated with rich structureinformation in the form of graphs (e.g., academic networks, and e-commercenetworks) or scenarios where graph data are paired with rich textualinformation (e.g., molecules with descriptions). Besides, although LLMs haveshown their pure text-based reasoning ability, it is underexplored whether suchability can be generalized to graph scenarios (i.e., graph-based reasoning). Inthis paper, we provide a systematic review of scenarios and techniques relatedto large language models on graphs. We first summarize potential scenarios ofadopting LLMs on graphs into three categories, namely pure graphs, text-richgraphs, and text-paired graphs. We then discuss detailed techniques forutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLMas Aligner, and compare the advantages and disadvantages of different schoolsof models. Furthermore, we mention the real-world applications of such methodsand summarize open-source codes and benchmark datasets. Finally, we concludewith potential future research directions in this fast-growing field. Therelated source can be found athttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.</description><author>Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, Jiawei Han</author><pubDate>Tue, 05 Dec 2023 14:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02783v1</guid></item><item><title>PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo Multi-modal Features</title><link>http://arxiv.org/abs/2312.02781v1</link><description>Speech-driven 3D facial animation has improved a lot recently while mostrelated works only utilize acoustic modality and neglect the influence ofvisual and textual cues, leading to unsatisfactory results in terms ofprecision and coherence. We argue that visual and textual cues are not trivialinformation. Therefore, we present a novel framework, namely PMMTalk, usingcomplementary Pseudo Multi-Modal features for improving the accuracy of facialanimation. The framework entails three modules: PMMTalk encoder, cross-modalalignment module, and PMMTalk decoder. Specifically, the PMMTalk encoderemploys the off-the-shelf talking head generation architecture and speechrecognition technology to extract visual and textual information from speech,respectively. Subsequently, the cross-modal alignment module aligns theaudio-image-text features at temporal and semantic levels. Then PMMTalk decoderis employed to predict lip-syncing facial blendshape coefficients. Contrary toprior methods, PMMTalk only requires an additional random reference face imagebut yields more accurate results. Additionally, it is artist-friendly as itseamlessly integrates into standard animation production workflows byintroducing facial blendshape coefficients. Finally, given the scarcity of 3Dtalking face datasets, we introduce a large-scale 3D Chinese Audio-VisualFacial Animation (3D-CAVFA) dataset. Extensive experiments and user studiesshow that our approach outperforms the state of the art. We recommend watchingthe supplementary video.</description><author>Tianshun Han, Shengnan Gui, Yiqing Huang, Baihui Li, Lijian Liu, Benjia Zhou, Ning Jiang, Quan Lu, Ruicong Zhi, Yanyan Liang, Du Zhang, Jun Wan</author><pubDate>Tue, 05 Dec 2023 14:12:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02781v1</guid></item><item><title>Scaling Laws for Adversarial Attacks on Language Model Activations</title><link>http://arxiv.org/abs/2312.02780v1</link><description>We explore a class of adversarial attacks targeting the activations oflanguage models. By manipulating a relatively small subset of modelactivations, $a$, we demonstrate the ability to control the exact prediction ofa significant number (in some cases up to 1000) of subsequent tokens $t$. Weempirically verify a scaling law where the maximum number of target tokens$t_\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whoseactivations the attacker controls as $t_\mathrm{max} = \kappa a$. We find thatthe number of bits of control in the input space needed to control a single bitin the output space (what we call attack resistance $\chi$) is remarkablyconstant between $\approx 16$ and $\approx 25$ over 2 orders of magnitude ofmodel sizes for different language models. Compared to attacks on tokens,attacks on activations are predictably much stronger, however, we identify asurprising regularity where one bit of input steered either via activations orvia tokens is able to exert control over a similar amount of output bits. Thisgives support for the hypothesis that adversarial attacks are a consequence ofdimensionality mismatch between the input and output spaces. A practicalimplication of the ease of attacking language model activations instead oftokens is for multi-modal and selected retrieval models, where additional datasources are added as activations directly, sidestepping the tokenized input.This opens up a new, broad attack surface. By using language models as acontrollable test-bed to study adversarial attacks, we were able to experimentwith input-output dimensions that are inaccessible in computer vision,especially where the output dimension dominates.</description><author>Stanislav Fort</author><pubDate>Tue, 05 Dec 2023 14:12:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02780v1</guid></item><item><title>Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions</title><link>http://arxiv.org/abs/2312.02772v1</link><description>Recently, significant progress has been made in text-based motion generation,enabling the generation of diverse and high-quality human motions that conformto textual descriptions. However, it remains challenging to generatefine-grained or stylized motions due to the lack of datasets annotated withdetailed textual descriptions. By adopting a divide-and-conquer strategy, wepropose a new framework named Fine-Grained Human Motion Diffusion Model(FG-MDM) for human motion generation. Specifically, we first parse previousvague textual annotation into fine-grained description of different body partsby leveraging a large language model (GPT-3.5). We then use these fine-graineddescriptions to guide a transformer-based diffusion model. FG-MDM can generatefine-grained and stylized motions even outside of the distribution of thetraining data. Our experimental results demonstrate the superiority of FG-MDMover previous methods, especially the strong generalization capability. We willrelease our fine-grained textual annotations for HumanML3D and KIT.</description><author>Xu Shi, Chuanchen Luo, Junran Peng, Hongwen Zhang, Yunlian Sun</author><pubDate>Tue, 05 Dec 2023 14:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02772v1</guid></item><item><title>Learning "Look-Ahead" Nonlocal Traffic Dynamics in a Ring Road</title><link>http://arxiv.org/abs/2312.02770v1</link><description>The macroscopic traffic flow model is widely used for traffic control andmanagement. To incorporate drivers' anticipative behaviors and to removeimpractical speed discontinuity inherent in the classicLighthill-Whitham-Richards (LWR) traffic model, nonlocal partial differentialequation (PDE) models with ``look-ahead" dynamics have been proposed, whichassume that the speed is a function of weighted downstream traffic density.However, it lacks data validation on two important questions: whether thereexist nonlocal dynamics, and how the length and weight of the ``look-ahead"window affect the spatial temporal propagation of traffic densities. In thispaper, we adopt traffic trajectory data from a ring-road experiment and designa physics-informed neural network to learn the fundamental diagram andlook-ahead kernel that best fit the data, and reinvent a data-enhanced nonlocalLWR model via minimizing the loss function combining the data discrepancy andthe nonlocal model discrepancy. Results show that the learned nonlocal LWRyields a more accurate prediction of traffic wave propagation in threedifferent scenarios: stop-and-go oscillations, congested, and free traffic. Wefirst demonstrate the existence of ``look-ahead" effect with real traffic data.The optimal nonlocal kernel is found out to take a length of around 35 to 50meters, and the kernel weight within 5 meters accounts for the majority of thenonlocal effect. Our results also underscore the importance of choosing apriori physics in machine learning models.</description><author>Chenguang Zhao, Huan Yu</author><pubDate>Tue, 05 Dec 2023 14:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02770v1</guid></item><item><title>Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps</title><link>http://arxiv.org/abs/2305.15583v4</link><description>Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in thesynthesis of high-quality images. However, their inference processcharacteristically requires numerous, potentially hundreds, of iterative steps,which could exaggerate the problem of exposure bias due to the training andinference discrepancy. Previous work has attempted to mitigate this issue byperturbing inputs during training, which consequently mandates the retrainingof the DPM. In this work, we conduct a systematic study of exposure bias in DPMand, intriguingly, we find that the exposure bias could be alleviated with anovel sampling method that we propose, without retraining the model. Weempirically and theoretically show that, during inference, for each backwardtime step $t$ and corresponding state $\hat{x}_t$, there might exist anothertime step $t_s$ which exhibits superior coupling with $\hat{x}_t$. Based onthis finding, we introduce a sampling method named Time-Shift Sampler. Ourframework can be seamlessly integrated to existing sampling algorithms, such asDDPM, DDIM and other high-order solvers, inducing merely minimal additionalcomputations. Experimental results show our method brings significant andconsistent improvements in FID scores on different datasets and samplingmethods. For example, integrating Time-Shift Sampler to F-PNDM yields aFID=3.88, achieving 44.49\% improvements as compared to F-PNDM, on CIFAR-10with 10 sampling steps, which is more performant than the vanilla DDIM with 100sampling steps. We will release the code upon acceptance.</description><author>Mingxiao Li, Tingyu Qu, Ruicong Yao, Wei Sun, Marie-Francine Moens</author><pubDate>Tue, 05 Dec 2023 13:48:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15583v4</guid></item><item><title>Learning Cortical Anomaly through Masked Encoding for Unsupervised Heterogeneity Mapping</title><link>http://arxiv.org/abs/2312.02762v1</link><description>The detection of heterogeneous mental disorders based on brain readoutsremains challenging due to the complexity of symptoms and the absence ofreliable biomarkers. This paper introduces CAM (Cortical Anomaly Detectionthrough Masked Image Modeling), a novel self-supervised framework designed forthe unsupervised detection of complex brain disorders using cortical surfacefeatures. We employ this framework for the detection of individuals on thepsychotic spectrum and demonstrate its capabilities compared to state-ofthe-artmethods, achieving an AUC of 0.696 for Schizoaffective and 0.769 forSchizophreniform, without the need for any labels. Furthermore, the analysis ofatypical cortical regions includes Pars Triangularis and several frontal areas,often implicated in schizophrenia, provide further confidence in our approach.Altogether, we demonstrate a scalable approach for anomaly detection of complexbrain disorders based on cortical abnormalities.</description><author>Hao-Chun Yang, Ole Andreassen, Lars Tjelta Westlye, Andre F. Marquand, Christian F. Beckmann, Thomas Wolfers</author><pubDate>Tue, 05 Dec 2023 13:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02762v1</guid></item><item><title>Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization</title><link>http://arxiv.org/abs/2207.02016v4</link><description>Reinforcement learning (RL) is recognized as lacking generalization androbustness under environmental perturbations, which excessively restricts itsapplication for real-world robotics. Prior work claimed that addingregularization to the value function is equivalent to learning a robust policywith uncertain transitions. Although the regularization-robustnesstransformation is appealing for its simplicity and efficiency, it is stilllacking in continuous control tasks. In this paper, we propose a newregularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer(USR), by formulating the uncertainty set on the parameter space of thetransition function. In particular, USR is flexible enough to be plugged intoany existing RL framework. To deal with unknown uncertainty sets, we furtherpropose a novel adversarial approach to generate them based on the valuefunction. We evaluate USR on the Real-world Reinforcement Learning (RWRL)benchmark, demonstrating improvements in the robust performance for perturbedtesting environments.</description><author>Yuan Zhang, Jianhong Wang, Joschka Boedecker</author><pubDate>Tue, 05 Dec 2023 13:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.02016v4</guid></item><item><title>Topological Graph Signal Compression</title><link>http://arxiv.org/abs/2308.11068v2</link><description>Recently emerged Topological Deep Learning (TDL) methods aim to extendcurrent Graph Neural Networks (GNN) by naturally processing higher-orderinteractions, going beyond the pairwise relations and local neighborhoodsdefined by graph representations. In this paper we propose a novel TDL-basedmethod for compressing signals over graphs, consisting in two main steps:first, disjoint sets of higher-order structures are inferred based on theoriginal signal --by clustering $N$ datapoints into $K\ll N$ collections; then,a topological-inspired message passing gets a compressed representation of thesignal within those multi-element sets. Our results show that our frameworkimproves both standard GNN and feed-forward architectures in compressingtemporal link-based signals from two real-word Internet Service ProviderNetworks' datasets --from $30\%$ up to $90\%$ better reconstruction errorsacross all evaluation scenarios--, suggesting that it better captures andexploits spatial and temporal correlations over the whole graph-based networkstructure.</description><author>Guillermo Bernárdez, Lev Telyatnikov, Eduard Alarcón, Albert Cabellos-Aparicio, Pere Barlet-Ros, Pietro Liò</author><pubDate>Tue, 05 Dec 2023 13:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11068v2</guid></item><item><title>C3: High-performance and low-complexity neural compression from a single image or video</title><link>http://arxiv.org/abs/2312.02753v1</link><description>Most neural compression models are trained on large datasets of images orvideos in order to generalize to unseen data. Such generalization typicallyrequires large and expressive architectures with a high decoding complexity.Here we introduce C3, a neural compression method with strong rate-distortion(RD) performance that instead overfits a small model to each image or videoseparately. The resulting decoding complexity of C3 can be an order ofmagnitude lower than neural baselines with similar RD performance. C3 builds onCOOL-CHIC (Ladune et al.) and makes several simple and effective improvementsfor images. We further develop new methodology to apply C3 to videos. On theCLIC2020 image benchmark, we match the RD performance of VTM, the referenceimplementation of the H.266 codec, with less than 3k MACs/pixel for decoding.On the UVG video benchmark, we match the RD performance of the VideoCompression Transformer (Mentzer et al.), a well-established neural videocodec, with less than 5k MACs/pixel for decoding.</description><author>Hyunjik Kim, Matthias Bauer, Lucas Theis, Jonathan Richard Schwarz, Emilien Dupont</author><pubDate>Tue, 05 Dec 2023 13:28:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02753v1</guid></item><item><title>C-NERF: Representing Scene Changes as Directional Consistency Difference-based NeRF</title><link>http://arxiv.org/abs/2312.02751v1</link><description>In this work, we aim to detect the changes caused by object variations in ascene represented by the neural radiance fields (NeRFs). Given an arbitraryview and two sets of scene images captured at different timestamps, we canpredict the scene changes in that view, which has significant potentialapplications in scene monitoring and measuring. We conducted preliminarystudies and found that such an exciting task cannot be easily achieved byutilizing existing NeRFs and 2D change detection methods with many false ormissing detections. The main reason is that the 2D change detection is based onthe pixel appearance difference between spatial-aligned image pairs andneglects the stereo information in the NeRF. To address the limitations, wepropose the C-NERF to represent scene changes as directional consistencydifference-based NeRF, which mainly contains three modules. We first performthe spatial alignment of two NeRFs captured before and after changes. Then, weidentify the change points based on the direction-consistent constraint; thatis, real change points have similar change representations across viewdirections, but fake change points do not. Finally, we design the change maprendering process based on the built NeRFs and can generate the change map ofan arbitrarily specified view direction. To validate the effectiveness, webuild a new dataset containing ten scenes covering diverse scenarios withdifferent changing objects. Our approach surpasses state-of-the-art 2D changedetection and NeRF-based methods by a significant margin.</description><author>Rui Huang, Binbin Jiang, Qingyi Zhao, William Wang, Yuxiang Zhang, Qing Guo</author><pubDate>Tue, 05 Dec 2023 13:27:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02751v1</guid></item><item><title>On minimizing the training set fill distance in machine learning regression</title><link>http://arxiv.org/abs/2307.10988v2</link><description>For regression tasks one often leverages large datasets for trainingpredictive machine learning models. However, using large datasets may not befeasible due to computational limitations or high data labelling costs.Therefore, suitably selecting small training sets from large pools ofunlabelled data points is essential to maximize model performance whilemaintaining efficiency. In this work, we study Farthest Point Sampling (FPS), adata selection approach that aims to minimize the fill distance of the selectedset. We derive an upper bound for the maximum expected prediction error,conditional to the location of the unlabelled data points, that linearlydepends on the training set fill distance. For empirical validation, we performexperiments using two regression models on three datasets. We empirically showthat selecting a training set by aiming to minimize the fill distance, therebyminimizing our derived bound, significantly reduces the maximum predictionerror of various regression models, outperforming alternative samplingapproaches by a large margin. Furthermore, we show that selecting training setswith the FPS can also increase model stability for the specific case ofGaussian kernel regression approaches.</description><author>Paolo Climaco, Jochen Garcke</author><pubDate>Tue, 05 Dec 2023 13:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10988v2</guid></item><item><title>Compositional Generalization for Data-to-Text Generation</title><link>http://arxiv.org/abs/2312.02748v1</link><description>Data-to-text generation involves transforming structured data, oftenrepresented as predicate-argument tuples, into coherent textual descriptions.Despite recent advances, systems still struggle when confronted with unseencombinations of predicates, producing unfaithful descriptions (e.g.hallucinations or omissions). We refer to this issue as compositionalgeneralisation, and it encouraged us to create a benchmark for assessing theperformance of different approaches on this specific problem. Furthermore, wepropose a novel model that addresses compositional generalization by clusteringpredicates into groups. Our model generates text in a sentence-by-sentencemanner, relying on one cluster of predicates at a time. This approachsignificantly outperforms T5~baselines across all evaluation metrics.Notably,it achieved a 31% improvement over T5 in terms of a metric focused onmaintaining faithfulness to the input.</description><author>Xinnuo Xu, Ivan Titov, Mirella Lapata</author><pubDate>Tue, 05 Dec 2023 13:23:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02748v1</guid></item><item><title>GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models</title><link>http://arxiv.org/abs/2310.08529v2</link><description>In recent times, the generation of 3D assets from text prompts has shownimpressive results. Both 2D and 3D diffusion models can help generate decent 3Dobjects based on prompts. 3D diffusion models have good 3D consistency, buttheir quality and generalization are limited as trainable 3D data is expensiveand hard to obtain. 2D diffusion models enjoy strong abilities ofgeneralization and fine generation, but 3D consistency is hard to guarantee.This paper attempts to bridge the power from the two types of diffusion modelsvia the recent explicit and efficient 3D Gaussian splatting representation. Afast 3D object generation framework, named as GaussianDreamer, is proposed,where the 3D diffusion model provides priors for initialization and the 2Ddiffusion model enriches the geometry and appearance. Operations of noisy pointgrowing and color perturbation are introduced to enhance the initializedGaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3Davatar within 15 minutes on one GPU, much faster than previous methods, whilethe generated instances can be directly rendered in real time. Demos and codeare available at https://taoranyi.com/gaussiandreamer/.</description><author>Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang</author><pubDate>Tue, 05 Dec 2023 13:22:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08529v2</guid></item></channel></rss>