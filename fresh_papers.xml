<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 21 Jun 2023 06:00:48 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Learning Profitable NFT Image Diffusions via Multiple Visual-Policy Guided Reinforcement Learning</title><link>http://arxiv.org/abs/2306.11731v1</link><description>We study the task of generating profitable Non-Fungible Token (NFT) imagesfrom user-input texts. Recent advances in diffusion models have shown greatpotential for image generation. However, existing works can fall short ingenerating visually-pleasing and highly-profitable NFT images, mainly due tothe lack of 1) plentiful and fine-grained visual attribute prompts for an NFTimage, and 2) effective optimization metrics for generating high-quality NFTimages. To solve these challenges, we propose a Diffusion-based generationframework with Multiple Visual-Policies as rewards (i.e., Diffusion-MVP) forNFT images. The proposed framework consists of a large language model (LLM), adiffusion-based image generator, and a series of visual rewards by design.First, the LLM enhances a basic human input (such as "panda") by generatingmore comprehensive NFT-style prompts that include specific visual attributes,such as "panda with Ninja style and green background." Second, thediffusion-based image generator is fine-tuned using a large-scale NFT datasetto capture fine-grained image styles and accessory compositions of popular NFTelements. Third, we further propose to utilize multiple visual-policies asoptimization goals, including visual rarity levels, visual aesthetic scores,and CLIP-based text-image relevances. This design ensures that our proposedDiffusion-MVP is capable of minting NFT images with high visual quality andmarket value. To facilitate this research, we have collected the largestpublicly available NFT image dataset to date, consisting of 1.5 millionhigh-quality images with corresponding texts and market values. Extensiveexperiments including objective evaluations and user studies demonstrate thatour framework can generate NFT images showing more visually engaging elementsand higher market value, compared with SOTA approaches.</description><author>Huiguo He, Tianfu Wang, Huan Yang, Jianlong Fu, Nicholas Jing Yuan, Jian Yin, Hongyang Chao, Qi Zhang</author><pubDate>Tue, 20 Jun 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11731v1</guid></item><item><title>Segment Anything Model (SAM) for Radiation Oncology</title><link>http://arxiv.org/abs/2306.11730v1</link><description>In this study, we evaluate the performance of the Segment Anything Model(SAM) model in clinical radiotherapy. We collected real clinical cases fromfour regions at the Mayo Clinic: prostate, lung, gastrointestinal, and head \&amp;neck, which are typical treatment sites in radiation oncology. For each case,we selected the OARs of concern in radiotherapy planning and compared the Diceand Jaccard outcomes between clinical manual delineation, automaticsegmentation using SAM's "segment anything" mode, and automatic segmentationusing SAM with box prompt. Our results indicate that SAM performs better inautomatic segmentation for the prostate and lung regions, while its performancein the gastrointestinal and head \&amp; neck regions was relatively inferior. Whenconsidering the size of the organ and the clarity of its boundary, SAM displaysbetter performance for larger organs with clear boundaries, such as the lungand liver, and worse for smaller organs with unclear boundaries, like theparotid and cochlea. These findings align with the generally acceptedvariations in difficulty level associated with manual delineation of differentorgans at different sites in clinical radiotherapy. Given that SAM, a singletrained model, could handle the delineation of OARs in four regions, theseresults also demonstrate SAM's robust generalization capabilities in automaticsegmentation for radiotherapy, i.e., achieving delineation of differentradiotherapy OARs using a generic automatic segmentation model. SAM'sgeneralization capabilities across different regions make it technicallyfeasible to develop a generic model for automatic segmentation in radiotherapy.</description><author>Lian Zhang, Zhengliang Liu, Lu Zhang, Zihao Wu, Xiaowei Yu, Jason Holmes, Hongying Feng, Haixing Dai, Xiang Li, Quanzheng Li, Dajiang Zhu, Tianming Liu, Wei Liu</author><pubDate>Tue, 20 Jun 2023 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11730v1</guid></item><item><title>Multimodal Fusion Transformer for Remote Sensing Image Classification</title><link>http://arxiv.org/abs/2203.16952v2</link><description>Vision transformers (ViTs) have been trending in image classification tasksdue to their promising performance when compared to convolutional neuralnetworks (CNNs). As a result, many researchers have tried to incorporate ViTsin hyperspectral image (HSI) classification tasks. To achieve satisfactoryperformance, close to that of CNNs, transformers need fewer parameters. ViTsand other similar transformers use an external classification (CLS) token whichis randomly initialized and often fails to generalize well, whereas othersources of multimodal datasets, such as light detection and ranging (LiDAR)offer the potential to improve these models by means of a CLS. In this paper,we introduce a new multimodal fusion transformer (MFT) network which comprisesa multihead cross patch attention (mCrossPA) for HSI land-cover classification.Our mCrossPA utilizes other sources of complementary information in addition tothe HSI in the transformer encoder to achieve better generalization. Theconcept of tokenization is used to generate CLS and HSI patch tokens, helpingto learn a {distinctive representation} in a reduced and hierarchical featurespace. Extensive experiments are carried out on {widely used benchmark}datasets {i.e.,} the University of Houston, Trento, University of SouthernMississippi Gulfpark (MUUFL), and Augsburg. We compare the results of theproposed MFT model with other state-of-the-art transformers, classical CNNs,and conventional classifiers models. The superior performance achieved by theproposed model is due to the use of multihead cross patch attention. The sourcecode will be made available publicly at\url{https://github.com/AnkurDeria/MFT}.}</description><author>Swalpa Kumar Roy, Ankur Deria, Danfeng Hong, Behnood Rasti, Antonio Plaza, Jocelyn Chanussot</author><pubDate>Tue, 20 Jun 2023 18:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.16952v2</guid></item><item><title>Error correction and extraction in request dialogs</title><link>http://arxiv.org/abs/2004.04243v4</link><description>We propose a dialog system utility component that gets the last twoutterances of a user and can detect whether the last utterance is an errorcorrection of the second last utterance. If yes, it corrects the second lastutterance according to the error correction in the last utterance and outputsthe extracted pairs of reparandum and repair entity. This component offers twoadvantages, learning the concept of corrections to avoid collecting correctionsfor every new domain and extracting reparandum and repair pairs, which offersthe possibility to learn out of it. For the error correction one sequence labeling and two sequence to sequenceapproaches are presented. For the error correction detection these three errorcorrection approaches can also be used and in addition, we present a sequenceclassification approach. One error correction detection and one errorcorrection approach can be combined to a pipeline or the error correctionapproaches can be trained and used end-to-end to avoid two components. Wemodified the EPIC-KITCHENS-100 dataset to evaluate the approaches forcorrecting entity phrases in request dialogs. For error correction detectionand correction, we got an accuracy of 96.40 % on synthetic validation data andan accuracy of 77.81 % on human-created real-world test data.</description><author>Stefan Constantin, Alex Waibel</author><pubDate>Tue, 20 Jun 2023 18:58:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2004.04243v4</guid></item><item><title>Dense Video Object Captioning from Disjoint Supervision</title><link>http://arxiv.org/abs/2306.11729v1</link><description>We propose a new task and model for dense video object captioning --detecting, tracking, and captioning trajectories of all objects in a video.This task unifies spatial and temporal understanding of the video, and requiresfine-grained language description. Our model for dense video object captioningis trained end-to-end and consists of different modules for spatiallocalization, tracking, and captioning. As such, we can train our model with amixture of disjoint tasks, and leverage diverse, large-scale datasets whichsupervise different parts of our model. This results in noteworthy zero-shotperformance. Moreover, by finetuning a model from this initialization, we canfurther improve our performance, surpassing strong image-based baselines by asignificant margin. Although we are not aware of other work performing thistask, we are able to repurpose existing video grounding datasets for our task,namely VidSTG and VLN. We show our task is more general than grounding, andmodels trained on our task can directly be applied to grounding by finding thebounding box with the maximum likelihood of generating the query sentence. Ourmodel outperforms dedicated, state-of-the-art models for spatial grounding onboth VidSTG and VLN.</description><author>Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid</author><pubDate>Tue, 20 Jun 2023 18:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11729v1</guid></item><item><title>How can objects help action recognition?</title><link>http://arxiv.org/abs/2306.11726v1</link><description>Current state-of-the-art video models process a video clip as a long sequenceof spatio-temporal tokens. However, they do not explicitly model objects, theirinteractions across the video, and instead process all the tokens in the video.In this paper, we investigate how we can use knowledge of objects to designbetter video models, namely to process fewer tokens and to improve recognitionaccuracy. This is in contrast to prior works which either drop tokens at thecost of accuracy, or increase accuracy whilst also increasing the computationrequired. First, we propose an object-guided token sampling strategy thatenables us to retain a small fraction of the input tokens with minimal impacton accuracy. And second, we propose an object-aware attention module thatenriches our feature representation with object information and improvesoverall accuracy. Our resulting framework achieves better performance whenusing fewer tokens than strong baselines. In particular, we match our baselinewith 30%, 40%, and 60% of the input tokens on SomethingElse,Something-something v2, and Epic-Kitchens, respectively. When we use our modelto process the same number of tokens as our baseline, we improve by 0.6 to 4.2points on these datasets.</description><author>Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid</author><pubDate>Tue, 20 Jun 2023 18:56:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11726v1</guid></item><item><title>Low-complexity Multidimensional DCT Approximations</title><link>http://arxiv.org/abs/2306.11724v1</link><description>In this paper, we introduce low-complexity multidimensional discrete cosinetransform (DCT) approximations. Three dimensional DCT (3D DCT) approximationsare formalized in terms of high-order tensor theory. The formulation isextended to higher dimensions with arbitrary lengths. Several multiplierless$8\times 8\times 8$ approximate methods are proposed and the computationalcomplexity is discussed for the general multidimensional case. The proposedmethods complexity cost was assessed, presenting considerably lower arithmeticoperations when compared with the exact 3D DCT. The proposed approximationswere embedded into 3D DCT-based video coding scheme and a modified quantizationstep was introduced. The simulation results showed that the approximate 3D DCTcoding methods offer almost identical output visual quality when compared withexact 3D DCT scheme. The proposed 3D approximations were also employed as atool for visual tracking. The approximate 3D DCT-based proposed system performssimilarly to the original exact 3D DCT-based method. In general, the suggestedmethods showed competitive performance at a considerably lower computationalcost.</description><author>V. A. Coutinho, R. J. Cintra, F. M. Bayer</author><pubDate>Tue, 20 Jun 2023 18:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11724v1</guid></item><item><title>Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision</title><link>http://arxiv.org/abs/2306.11719v1</link><description>Denoising diffusion models are a powerful type of generative models used tocapture complex distributions of real-world signals. However, theirapplicability is limited to scenarios where training samples are readilyavailable, which is not always the case in real-world applications. Forexample, in inverse graphics, the goal is to generate samples from adistribution of 3D scenes that align with a given image, but ground-truth 3Dscenes are unavailable and only 2D images are accessible. To address thislimitation, we propose a novel class of denoising diffusion probabilisticmodels that learn to sample from distributions of signals that are neverdirectly observed. Instead, these signals are measured indirectly through aknown differentiable forward model, which produces partial observations of theunknown signal. Our approach involves integrating the forward model directlyinto the denoising process. This integration effectively connects thegenerative modeling of observations with the generative modeling of theunderlying signals, allowing for end-to-end training of a conditionalgenerative model over signals. During inference, our approach enables samplingfrom the distribution of underlying signals that are consistent with a givenpartial observation. We demonstrate the effectiveness of our method on threechallenging computer vision tasks. For instance, in the context of inversegraphics, our model enables direct sampling from the distribution of 3D scenesthat align with a single 2D input image.</description><author>Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Frédo Durand, William T. Freeman, Vincent Sitzmann</author><pubDate>Tue, 20 Jun 2023 18:53:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11719v1</guid></item><item><title>ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation</title><link>http://arxiv.org/abs/2212.03588v3</link><description>Recently, CLIP has been applied to pixel-level zero-shot learning tasks via atwo-stage scheme. The general idea is to first generate class-agnostic regionproposals and then feed the cropped proposal regions to CLIP to utilize itsimage-level zero-shot classification capability. While effective, such a schemerequires two image encoders, one for proposal generation and one for CLIP,leading to a complicated pipeline and high computational cost. In this work, wepursue a simpler-and-efficient one-stage solution that directly extends CLIP'szero-shot prediction capability from image to pixel level. Our investigationstarts with a straightforward extension as our baseline that generates semanticmasks by comparing the similarity between text and patch embeddings extractedfrom CLIP. However, such a paradigm could heavily overfit the seen classes andfail to generalize to unseen classes. To handle this issue, we propose threesimple-but-effective designs and figure out that they can significantly retainthe inherent zero-shot capacity of CLIP and improve pixel-level generalizationability. Incorporating those modifications leads to an efficient zero-shotsemantic segmentation system called ZegCLIP. Through extensive experiments onthree public benchmarks, ZegCLIP demonstrates superior performance,outperforming the state-of-the-art methods by a large margin under both"inductive" and "transductive" zero-shot settings. In addition, compared withthe two-stage method, our one-stage ZegCLIP achieves a speedup of about 5 timesfaster during inference. We release the code athttps://github.com/ZiqinZhou66/ZegCLIP.git.</description><author>Ziqin Zhou, Bowen Zhang, Yinjie Lei, Lingqiao Liu, Yifan Liu</author><pubDate>Tue, 20 Jun 2023 18:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.03588v3</guid></item><item><title>Multi-Fidelity Active Learning with GFlowNets</title><link>http://arxiv.org/abs/2306.11715v1</link><description>In the last decades, the capacity to generate large amounts of data inscience and engineering applications has been growing steadily. Meanwhile, theprogress in machine learning has turned it into a suitable tool to process andutilise the available data. Nonetheless, many relevant scientific andengineering problems present challenges where current machine learning methodscannot yet efficiently leverage the available data and resources. For example,in scientific discovery, we are often faced with the problem of exploring verylarge, high-dimensional spaces, where querying a high fidelity, black-boxobjective function is very expensive. Progress in machine learning methods thatcan efficiently tackle such problems would help accelerate currently crucialareas such as drug and materials discovery. In this paper, we propose the useof GFlowNets for multi-fidelity active learning, where multiple approximationsof the black-box function are available at lower fidelity and cost. GFlowNetsare recently proposed methods for amortised probabilistic inference that haveproven efficient for exploring large, high-dimensional spaces and can hence bepractical in the multi-fidelity setting too. Here, we describe our algorithmfor multi-fidelity active learning with GFlowNets and evaluate its performancein both well-studied synthetic tasks and practically relevant applications ofmolecular discovery. Our results show that multi-fidelity active learning withGFlowNets can efficiently leverage the availability of multiple oracles withdifferent costs and fidelities to accelerate scientific discovery andengineering design.</description><author>Alex Hernandez-Garcia, Nikita Saxena, Moksh Jain, Cheng-Hao Liu, Yoshua Bengio</author><pubDate>Tue, 20 Jun 2023 18:43:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11715v1</guid></item><item><title>Meta-Analysis of Transfer Learning for Segmentation of Brain Lesions</title><link>http://arxiv.org/abs/2306.11714v1</link><description>A major challenge in stroke research and stroke recovery predictions is thedetermination of a stroke lesion's extent and its impact on relevant brainsystems. Manual segmentation of stroke lesions from 3D magnetic resonance (MR)imaging volumes, the current gold standard, is not only very time-consuming,but its accuracy highly depends on the operator's experience. As a result,there is a need for a fully automated segmentation method that can efficientlyand objectively measure lesion extent and the impact of each lesion to predictimpairment and recovery potential which might be beneficial for clinical,translational, and research settings. We have implemented and tested a fullyautomatic method for stroke lesion segmentation which was developed using eightdifferent 2D-model architectures trained via transfer learning (TL) and mixeddata approaches. Additionally, the final prediction was made using a novelensemble method involving stacking and agreement window. Our novel method wasevaluated in a novel in-house dataset containing 22 T1w brain MR images, whichwere challenging in various perspectives, but mostly because they included T1wMR images from the subacute (which typically less well defined T1 lesions) andchronic stroke phase (which typically means well defined T1-lesions).Cross-validation results indicate that our new method can efficiently andautomatically segment lesions fast and with high accuracy compared to groundtruth. In addition to segmentation, we provide lesion volume and weightedlesion load of relevant brain systems based on the lesions' overlap with acanonical structural motor system that stretches from the cortical motor regionto the lowest end of the brain stem.</description><author>Sovesh Mohapatra, Advait Gosai, Anant Shinde, Aleksei Rutkovskii, Sirisha Nouduri, Gottfried Schlaug</author><pubDate>Tue, 20 Jun 2023 18:42:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11714v1</guid></item><item><title>Data-Driven but Privacy-Conscious: Pedestrian Dataset De-identification via Full-Body Person Synthesis</title><link>http://arxiv.org/abs/2306.11710v1</link><description>The advent of data-driven technology solutions is accompanied by anincreasing concern with data privacy. This is of particular importance forhuman-centered image recognition tasks, such as pedestrian detection,re-identification, and tracking. To highlight the importance of privacy issuesand motivate future research, we motivate and introduce the Pedestrian DatasetDe-Identification (PDI) task. PDI evaluates the degree of de-identification anddownstream task training performance for a given de-identification method. As afirst baseline, we propose IncogniMOT, a two-stage full-body de-identificationpipeline based on image synthesis via generative adversarial networks. Thefirst stage replaces target pedestrians with synthetic identities. To improvedownstream task performance, we then apply stage two, which blends and adaptsthe synthetic image parts into the data. To demonstrate the effectiveness ofIncogniMOT, we generate a fully de-identified version of the MOT17 pedestriantracking dataset and analyze its application as training data for pedestrianre-identification, detection, and tracking models. Furthermore, we show how ourdata is able to narrow the synthetic-to-real performance gap in aprivacy-conscious manner.</description><author>Maxim Maximov, Tim Meinhardt, Ismail Elezi, Zoe Papakipos, Caner Hazirbas, Cristian Canton, Laura Leal-Taixé</author><pubDate>Tue, 20 Jun 2023 18:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11710v1</guid></item><item><title>The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement</title><link>http://arxiv.org/abs/2306.09633v2</link><description>Reinforcement learning (RL) for physical design of silicon chips in a Google2021 Nature paper stirred controversy due to poorly documented claims thatraised eyebrows and attracted critical media coverage. The Nature paperwithheld most inputs needed to produce reported results and some critical stepsin the methodology. But two separate evaluations filled in the gaps anddemonstrated that Google RL lags behind human designers, behind a well-knownalgorithm (Simulated Annealing), and also behind generally-available commercialsoftware. Crosschecked data indicate that the integrity of the Nature paper issubstantially undermined owing to errors in the conduct, analysis andreporting.</description><author>Igor L. Markov</author><pubDate>Tue, 20 Jun 2023 18:39:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09633v2</guid></item><item><title>RoboCat: A Self-Improving Foundation Agent for Robotic Manipulation</title><link>http://arxiv.org/abs/2306.11706v1</link><description>The ability to leverage heterogeneous robotic experience from differentrobots and tasks to quickly master novel skills and embodiments has thepotential to transform robot learning. Inspired by recent advances infoundation models for vision and language, we propose a foundation agent forrobotic manipulation. This agent, named RoboCat, is a visual goal-conditioneddecision transformer capable of consuming multi-embodiment action-labelledvisual experience. This data spans a large repertoire of motor control skillsfrom simulated and real robotic arms with varying sets of observations andactions. With RoboCat, we demonstrate the ability to generalise to new tasksand robots, both zero-shot as well as through adaptation using only 100--1000examples for the target task. We also show how a trained model itself can beused to generate data for subsequent training iterations, thus providing abasic building block for an autonomous improvement loop. We investigate theagent's capabilities, with large-scale evaluations both in simulation and onthree different real robot embodiments. We find that as we grow and diversifyits training data, RoboCat not only shows signs of cross-task transfer, butalso becomes more efficient at adapting to new tasks.</description><author>Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Żołna, Scott Reed, Sergio Gómez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Tom Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, Nicolas Heess</author><pubDate>Tue, 20 Jun 2023 18:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11706v1</guid></item><item><title>CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search</title><link>http://arxiv.org/abs/2306.10008v2</link><description>The success of deep learning based face recognition systems has given rise toserious privacy concerns due to their ability to enable unauthorized trackingof users in the digital world. Existing methods for enhancing privacy fail togenerate naturalistic images that can protect facial privacy withoutcompromising user experience. We propose a novel two-step approach for facialprivacy protection that relies on finding adversarial latent codes in thelow-dimensional manifold of a pretrained generative model. The first stepinverts the given face image into the latent space and finetunes the generativemodel to achieve an accurate reconstruction of the given image from its latentcode. This step produces a good initialization, aiding the generation ofhigh-quality faces that resemble the given identity. Subsequently, user-definedmakeup text prompts and identity-preserving regularization are used to guidethe search for adversarial codes in the latent space. Extensive experimentsdemonstrate that faces generated by our approach have stronger black-boxtransferability with an absolute gain of 12.06% over the state-of-the-artfacial privacy protection approach under the face verification task. Finally,we demonstrate the effectiveness of the proposed approach for commercial facerecognition systems. Our code is available athttps://github.com/fahadshamshad/Clip2Protect.</description><author>Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar</author><pubDate>Tue, 20 Jun 2023 18:33:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10008v2</guid></item><item><title>Lingua Manga: A Generic Large Language Model Centric System for Data Curation</title><link>http://arxiv.org/abs/2306.11702v1</link><description>Data curation is a wide-ranging area which contains many critical buttime-consuming data processing tasks. However, the diversity of such tasksmakes it challenging to develop a general-purpose data curation system. Toaddress this issue, we present Lingua Manga, a user-friendly and versatilesystem that utilizes pre-trained large language models. Lingua Manga offersautomatic optimization for achieving high performance and label efficiencywhile facilitating flexible and rapid development. Through three exampleapplications with distinct objectives and users of varying levels of technicalproficiency, we demonstrate that Lingua Manga can effectively assist bothskilled programmers and low-code or even no-code users in addressing datacuration challenges.</description><author>Zui Chen, Lei Cao, Sam Madden</author><pubDate>Tue, 20 Jun 2023 18:30:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11702v1</guid></item><item><title>Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs</title><link>http://arxiv.org/abs/2306.11700v1</link><description>We study the problem of computing an optimal policy of an infinite-horizondiscounted constrained Markov decision process (constrained MDP). Despite thepopularity of Lagrangian-based policy search methods used in practice, theoscillation of policy iterates in these methods has not been fully understood,bringing out issues such as violation of constraints and sensitivity tohyper-parameters. To fill this gap, we employ the Lagrangian method to cast aconstrained MDP into a constrained saddle-point problem in which max/minplayers correspond to primal/dual variables, respectively, and develop twosingle-time-scale policy-based primal-dual algorithms with non-asymptoticconvergence of their policy iterates to an optimal constrained policy.Specifically, we first propose a regularized policy gradient primal-dual(RPG-PD) method that updates the policy using an entropy-regularized policygradient, and the dual via a quadratic-regularized gradient ascent,simultaneously. We prove that the policy primal-dual iterates of RPG-PDconverge to a regularized saddle point with a sublinear rate, while the policyiterates converge sublinearly to an optimal constrained policy. We furtherinstantiate RPG-PD in large state or action spaces by including functionapproximation in policy parametrization, and establish similar sublinearlast-iterate policy convergence. Second, we propose an optimistic policygradient primal-dual (OPG-PD) method that employs the optimistic gradientmethod to update primal/dual variables, simultaneously. We prove that thepolicy primal-dual iterates of OPG-PD converge to a saddle point that containsan optimal constrained policy, with a linear rate. To the best of ourknowledge, this work appears to be the first non-asymptotic policy last-iterateconvergence result for single-time-scale algorithms in constrained MDPs.</description><author>Dongsheng Ding, Chen-Yu Wei, Kaiqing Zhang, Alejandro Ribeiro</author><pubDate>Tue, 20 Jun 2023 18:27:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11700v1</guid></item><item><title>GenPlot: Increasing the Scale and Diversity of Chart Derendering Data</title><link>http://arxiv.org/abs/2306.11699v1</link><description>Vertical bars, horizontal bars, dot, scatter, and line plots provide adiverse set of visualizations to represent data. To understand these plots, onemust be able to recognize textual components, locate data points in a plot, andprocess diverse visual contexts to extract information. In recent works such asPix2Struct, Matcha, and Deplot, OCR-free chart-to-text translation has achievedstate-of-the-art results on visual language tasks. These results outline theimportance of chart-derendering as a pre-training objective, yet existingdatasets provide a fixed set of training examples. In this paper, we proposeGenPlot; a plot generator that can generate billions of additional plots forchart-derendering using synthetic data.</description><author>Brendan Artley</author><pubDate>Tue, 20 Jun 2023 18:25:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11699v1</guid></item><item><title>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models</title><link>http://arxiv.org/abs/2306.11698v1</link><description>Generative Pre-trained Transformer (GPT) models have exhibited excitingprogress in capabilities, capturing the interest of practitioners and thepublic alike. Yet, while the literature on the trustworthiness of GPT modelsremains limited, practitioners have proposed employing capable GPT models forsensitive applications to healthcare and finance - where mistakes can becostly. To this end, this work proposes a comprehensive trustworthinessevaluation for large language models with a focus on GPT-4 and GPT-3.5,considering diverse perspectives - including toxicity, stereotype bias,adversarial robustness, out-of-distribution robustness, robustness onadversarial demonstrations, privacy, machine ethics, and fairness. Based on ourevaluations, we discover previously unpublished vulnerabilities totrustworthiness threats. For instance, we find that GPT models can be easilymisled to generate toxic and biased outputs and leak private information inboth training data and conversation history. We also find that although GPT-4is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is morevulnerable given jailbreaking system or user prompts, potentially due to thereason that GPT-4 follows the (misleading) instructions more precisely. Ourwork illustrates a comprehensive trustworthiness evaluation of GPT models andsheds light on the trustworthiness gaps. Our benchmark is publicly available athttps://decodingtrust.github.io/.</description><author>Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li</author><pubDate>Tue, 20 Jun 2023 18:24:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11698v1</guid></item><item><title>Individual Treatment Effects in Extreme Regimes</title><link>http://arxiv.org/abs/2306.11697v1</link><description>Understanding individual treatment effects in extreme regimes is importantfor characterizing risks associated with different interventions. This ishindered by the fact that extreme regime data may be hard to collect, as it isscarcely observed in practice. In addressing this issue, we propose a newframework for estimating the individual treatment effect in extreme regimes(ITE$_2$). Specifically, we quantify this effect by the changes in the taildecay rates of potential outcomes in the presence or absence of the treatment.Subsequently, we establish conditions under which ITE$_2$ may be calculated anddevelop algorithms for its computation. We demonstrate the efficacy of ourproposed method on various synthetic and semi-synthetic datasets.</description><author>Ahmed Aloui, Ali Hasan, Yuting Ng, Miroslav Pajic, Vahid Tarokh</author><pubDate>Tue, 20 Jun 2023 18:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11697v1</guid></item><item><title>RoTaR: Efficient Row-Based Table Representation Learning via Teacher-Student Training</title><link>http://arxiv.org/abs/2306.11696v1</link><description>We propose RoTaR, a row-based table representation learning method, toaddress the efficiency and scalability issues faced by existing tablerepresentation learning methods. The key idea of RoTaR is to generatequery-agnostic row representations that could be re-used via query-specificaggregation. In addition to the row-based architecture, we introduce severaltechniques: cell-aware position embedding, teacher-student training paradigm,and selective backward to improve the performance of RoTaR model.</description><author>Zui Chen, Lei Cao, Sam Madden</author><pubDate>Tue, 20 Jun 2023 18:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11696v1</guid></item><item><title>A Simple and Effective Pruning Approach for Large Language Models</title><link>http://arxiv.org/abs/2306.11695v1</link><description>As their size increases, Large Languages Models (LLMs) are natural candidatesfor network pruning methods: approaches that drop a subset of network weightswhile striving to preserve performance. Existing methods, however, requireeither retraining, which is rarely affordable for billion-scale LLMs, orsolving a weight reconstruction problem reliant on second-order information,which may also be computationally expensive. In this paper, we introduce anovel, straightforward yet effective pruning method, termed Wanda (Pruning byWeights and activations), designed to induce sparsity in pretrained LLMs.Motivated by the recent observation of emergent large magnitude features inLLMs, our approach prune weights with the smallest magnitudes multiplied by thecorresponding input activations, on a per-output basis. Notably, Wanda requiresno retraining or weight update, and the pruned LLM can be used as is. Weconduct a thorough evaluation of our method on LLaMA across various languagebenchmarks. Wanda significantly outperforms the established baseline ofmagnitude pruning and competes favorably against recent methods involvingintensive weight update. Code is available athttps://github.com/locuslab/wanda.</description><author>Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter</author><pubDate>Tue, 20 Jun 2023 18:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11695v1</guid></item><item><title>Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers</title><link>http://arxiv.org/abs/2207.10170v3</link><description>Autonomous agents deployed in the real world need to be robust againstadversarial attacks on sensory inputs. Robustifying agent policies requiresanticipating the strongest attacks possible. We demonstrate that existingobservation-space attacks on reinforcement learning agents have a commonweakness: while effective, their lack of temporal consistency makes themdetectable using automated means or human inspection. Detectability isundesirable to adversaries as it may trigger security escalations. We introduceperfect illusory attacks, a novel form of adversarial attack on sequentialdecision-makers that is both effective and provably statistically undetectable.We then propose the more versatile R-attacks, which result in observationtransitions that are consistent with the state-transition function of theadversary-free environment and can be learned end-to-end. Compared to existingattacks, we empirically find R-attacks to be significantly harder to detectwith automated methods, and a small study with human subjects suggests they aresimilarly harder to detect for humans. We propose that undetectability shouldbe a central concern in the study of adversarial attacks on mixed-autonomysettings.</description><author>Tim Franzmeyer, Stephen McAleer, João F. Henriques, Jakob N. Foerster, Philip H. S. Torr, Adel Bibi, Christian Schroeder de Witt</author><pubDate>Tue, 20 Jun 2023 18:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.10170v3</guid></item><item><title>Statistical Tests for Replacing Human Decision Makers with Algorithms</title><link>http://arxiv.org/abs/2306.11689v1</link><description>This paper proposes a statistical framework with which artificialintelligence can improve human decision making. The performance of each humandecision maker is first benchmarked against machine predictions; we thenreplace the decisions made by a subset of the decision makers with therecommendation from the proposed artificial intelligence algorithm. Using alarge nationwide dataset of pregnancy outcomes and doctor diagnoses fromprepregnancy checkups of reproductive age couples, we experimented with both aheuristic frequentist approach and a Bayesian posterior loss function approachwith an application to abnormal birth detection. We find that our algorithm ona test dataset results in a higher overall true positive rate and a lower falsepositive rate than the diagnoses made by doctors only. We also find that thediagnoses of doctors from rural areas are more frequently replaceable,suggesting that artificial intelligence assisted decision making tends toimprove precision more in less developed regions.</description><author>Kai Feng, Han Hong, Ke Tang, Jingyuan Wang</author><pubDate>Tue, 20 Jun 2023 18:09:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11689v1</guid></item><item><title>SkyGPT: Probabilistic Short-term Solar Forecasting Using Synthetic Sky Videos from Physics-constrained VideoGPT</title><link>http://arxiv.org/abs/2306.11682v1</link><description>In recent years, deep learning-based solar forecasting using all-sky imageshas emerged as a promising approach for alleviating uncertainty in PV powergeneration. However, the stochastic nature of cloud movement remains a majorchallenge for accurate and reliable solar forecasting. With the recent advancesin generative artificial intelligence, the synthesis of visually plausible yetdiversified sky videos has potential for aiding in forecasts. In this study, weintroduce \emph{SkyGPT}, a physics-informed stochastic video prediction modelthat is able to generate multiple possible future images of the sky withdiverse cloud motion patterns, by using past sky image sequences as input.Extensive experiments and comparison with benchmark video prediction modelsdemonstrate the effectiveness of the proposed model in capturing cloud dynamicsand generating future sky images with high realism and diversity. Furthermore,we feed the generated future sky images from the video prediction models for15-minute-ahead probabilistic solar forecasting for a 30-kW roof-top PV system,and compare it with an end-to-end deep learning baseline model SUNSET and asmart persistence model. Better PV output prediction reliability and sharpnessis observed by using the predicted sky images generated with SkyGPT comparedwith other benchmark models, achieving a continuous ranked probability score(CRPS) of 2.81 (13\% better than SUNSET and 23\% better than smart persistence)and a Winkler score of 26.70 for the test set. Although an arbitrary number offutures can be generated from a historical sky image sequence, the resultssuggest that 10 future scenarios is a good choice that balances probabilisticsolar forecasting performance and computational cost.</description><author>Yuhao Nie, Eric Zelikman, Andea Scott, Quentin Paletta, Adam Brandt</author><pubDate>Tue, 20 Jun 2023 17:58:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11682v1</guid></item><item><title>MoleCLUEs: Optimizing Molecular Conformers by Minimization of Differentiable Uncertainty</title><link>http://arxiv.org/abs/2306.11681v1</link><description>Structure-based models in the molecular sciences can be highly sensitive toinput geometries and give predictions with large variance under subtlecoordinate perturbations. We present an approach to mitigate this failure modeby generating conformations that explicitly minimize uncertainty in apredictive model. To achieve this, we compute differentiable estimates ofaleatoric \textit{and} epistemic uncertainties directly from learnedembeddings. We then train an optimizer that iteratively samples embeddings toreduce these uncertainties according to their gradients. As our predictivemodel is constructed as a variational autoencoder, the new embeddings can bedecoded to their corresponding inputs, which we call \textit{MoleCLUEs}, or(molecular) counterfactual latent uncertainty explanations\citep{antoran2020getting}. We provide results of our algorithm for the task ofpredicting drug properties with maximum confidence as well as analysis of thedifferentiable structure simulations.</description><author>Michael Maser, Natasa Tagasovska, Jae Hyeon Lee, Andrew Watkins</author><pubDate>Tue, 20 Jun 2023 17:58:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11681v1</guid></item><item><title>The Implicit Bias of Batch Normalization in Linear Models and Two-layer Linear Convolutional Neural Networks</title><link>http://arxiv.org/abs/2306.11680v1</link><description>We study the implicit bias of batch normalization trained by gradientdescent. We show that when learning a linear model with batch normalization forbinary classification, gradient descent converges to a uniform marginclassifier on the training data with an $\exp(-\Omega(\log^2 t))$ convergencerate. This distinguishes linear models with batch normalization from thosewithout batch normalization in terms of both the type of implicit bias and theconvergence rate. We further extend our result to a class of two-layer,single-filter linear convolutional neural networks, and show that batchnormalization has an implicit bias towards a patch-wise uniform margin. Basedon two examples, we demonstrate that patch-wise uniform margin classifiers canoutperform the maximum margin classifiers in certain learning problems. Ourresults contribute to a better theoretical understanding of batchnormalization.</description><author>Yuan Cao, Difan Zou, Yuanzhi Li, Quanquan Gu</author><pubDate>Tue, 20 Jun 2023 17:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11680v1</guid></item><item><title>Causal Falsification of Digital Twins</title><link>http://arxiv.org/abs/2301.07210v3</link><description>Digital twins hold substantial promise in many applications, but rigorousprocedures for assessing their accuracy are essential for their widespreaddeployment in safety-critical settings. By formulating this task within theframework of causal inference, we show that attempts to certify the correctnessof a twin using real-world observational data are unsound unless potentiallytenuous assumptions are made about the data-generating process. To avoid theseassumptions, we propose an assessment strategy that instead aims to find caseswhere the twin is not correct, and present a general-purpose statisticalprocedure for doing so that may be used across a wide variety of applicationsand twin models. Our approach yields reliable and actionable information aboutthe twin under minimal assumptions about the twin and the real-world process ofinterest. We demonstrate the effectiveness of our methodology via a large-scalecase study involving sepsis modelling within the Pulse Physiology Engine, whichwe assess using the MIMIC-III dataset of ICU patients.</description><author>Rob Cornish, Muhammad Faaiz Taufiq, Arnaud Doucet, Chris Holmes</author><pubDate>Tue, 20 Jun 2023 17:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07210v3</guid></item><item><title>QGNN: Value Function Factorisation with Graph Neural Networks</title><link>http://arxiv.org/abs/2205.13005v2</link><description>In multi-agent reinforcement learning, the use of a global objective is apowerful tool for incentivising cooperation. Unfortunately, it is notsample-efficient to train individual agents with a global reward, because itdoes not necessarily correlate with an agent's individual actions. This problemcan be solved by factorising the global value function into local valuefunctions. Early work in this domain performed factorisation by conditioninglocal value functions purely on local information. Recently, it has been shownthat providing both local information and an encoding of the global state canpromote cooperative behaviour. In this paper we propose QGNN, the first valuefactorisation method to use a graph neural network (GNN) based model. Themulti-layer message passing architecture of QGNN provides more representationalcomplexity than models in prior work, allowing it to produce a more effectivefactorisation. QGNN also introduces a permutation invariant mixer which is ableto match the performance of other methods, even with significantly fewerparameters. We evaluate our method against several baselines, includingQMIX-Att, GraphMIX, QMIX, VDN, and hybrid architectures. Our experimentsinclude Starcraft, the standard benchmark for credit assignment; Estimate Game,a custom environment that explicitly models inter-agent dependencies; andCoalition Structure Generation, a foundational problem with real-worldapplications. The results show that QGNN outperforms state-of-the-art valuefactorisation baselines consistently.</description><author>Ryan Kortvelesy, Amanda Prorok</author><pubDate>Tue, 20 Jun 2023 17:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.13005v2</guid></item><item><title>MedNgage: A Dataset for Understanding Engagement in Patient-Nurse Conversations</title><link>http://arxiv.org/abs/2305.19981v2</link><description>Patients who effectively manage their symptoms often demonstrate higherlevels of engagement in conversations and interventions with healthcarepractitioners. This engagement is multifaceted, encompassing cognitive andsocio-affective dimensions. Consequently, it is crucial for AI systems tounderstand the engagement in natural conversations between patients andpractitioners to better contribute toward patient care. In this paper, wepresent a novel dataset (MedNgage), which consists of patient-nurseconversations about cancer symptom management. We manually annotate the datasetwith a novel framework of categories of patient engagement from two differentangles, namely: i) socio-affective (3.1K spans), and ii) cognitive use oflanguage (1.8K spans). Through statistical analysis of the data that isannotated using our framework, we show a positive correlation between patientsymptom management outcomes and their engagement in conversations.Additionally, we demonstrate that pre-trained transformer models fine-tuned onour dataset can reliably predict engagement classes in patient-nurseconversations. Lastly, we use LIME (Ribeiro et al., 2016) to analyze theunderlying challenges of the tasks that state-of-the-art transformer modelsencounter. The de-identified data is available for research purposes uponrequest.</description><author>Yan Wang, Heidi Ann Scharf Donovan, Sabit Hassan, Mailhe Alikhani</author><pubDate>Tue, 20 Jun 2023 17:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19981v2</guid></item><item><title>Learning to Rank when Grades Matter</title><link>http://arxiv.org/abs/2306.08650v2</link><description>Graded labels are ubiquitous in real-world learning-to-rank applications,especially in human rated relevance data. Traditional learning-to-ranktechniques aim to optimize the ranked order of documents. They typically,however, ignore predicting actual grades. This prevents them from being adoptedin applications where grades matter, such as filtering out ``poor'' documents.Achieving both good ranking performance and good grade prediction performanceis still an under-explored problem. Existing research either focuses only onranking performance by not calibrating model outputs, or treats grades asnumerical values, assuming labels are on a linear scale and failing to leveragethe ordinal grade information. In this paper, we conduct a rigorous study oflearning to rank with grades, where both ranking performance and gradeprediction performance are important. We provide a formal discussion on how toperform ranking with non-scalar predictions for grades, and propose amultiobjective formulation to jointly optimize both ranking and gradepredictions. In experiments, we verify on several public datasets that ourmethods are able to push the Pareto frontier of the tradeoff between rankingand grade prediction performance, showing the benefit of leveraging ordinalgrade information.</description><author>Le Yan, Zhen Qin, Gil Shamir, Dong Lin, Xuanhui Wang, Mike Bendersky</author><pubDate>Tue, 20 Jun 2023 17:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08650v2</guid></item><item><title>GIO: Gradient Information Optimization for Training Dataset Selection</title><link>http://arxiv.org/abs/2306.11670v1</link><description>It is often advantageous to train models on a subset of the available trainexamples, because the examples are of variable quality or because one wouldlike to train with fewer examples, without sacrificing performance. We presentGradient Information Optimization (GIO), a scalable, task-agnostic approach tothis data selection problem that requires only a small set of (unlabeled)examples representing a target distribution. GIO begins from a natural,information-theoretic objective that is intractable in practice. Ourcontribution is in showing that it can be made highly scalable through a simplerelaxation of the objective and a highly efficient implementation. Inexperiments with machine translation, spelling correction, and imagerecognition, we show that GIO delivers outstanding results with very smalltrain sets. These findings are robust to different representation models andhyperparameters for GIO itself. GIO is task- and domain-agnostic and can beapplied out-of-the-box to new datasets and domains.</description><author>Dante Everaert, Christopher Potts</author><pubDate>Tue, 20 Jun 2023 17:43:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11670v1</guid></item><item><title>Principles for Initialization and Architecture Selection in Graph Neural Networks with ReLU Activations</title><link>http://arxiv.org/abs/2306.11668v1</link><description>This article derives and validates three principles for initialization andarchitecture selection in finite width graph neural networks (GNNs) with ReLUactivations. First, we theoretically derive what is essentially the uniquegeneralization to ReLU GNNs of the well-known He-initialization. Ourinitialization scheme guarantees that the average scale of network outputs andgradients remains order one at initialization. Second, we prove in finite widthvanilla ReLU GNNs that oversmoothing is unavoidable at large depth when usingfixed aggregation operator, regardless of initialization. We then prove thatusing residual aggregation operators, obtained by interpolating a fixedaggregation operator with the identity, provably alleviates oversmoothing atinitialization. Finally, we show that the common practice of using residualconnections with a fixup-type initialization provably avoids correlationcollapse in final layer features at initialization. Through ablation studies wefind that using the correct initialization, residual aggregation operators, andresidual connections in the forward pass significantly and reliably speeds upearly training dynamics in deep ReLU GNNs on a variety of tasks.</description><author>Gage DeZoort, Boris Hanin</author><pubDate>Tue, 20 Jun 2023 17:40:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11668v1</guid></item><item><title>G-NM: A Group of Numerical Time Series Prediction Models</title><link>http://arxiv.org/abs/2306.11667v1</link><description>In this study, we focus on the development and implementation of acomprehensive ensemble of numerical time series forecasting models,collectively referred to as the Group of Numerical Time Series Prediction Model(G-NM). This inclusive set comprises traditional models such as AutoregressiveIntegrated Moving Average (ARIMA), Holt-Winters' method, and Support VectorRegression (SVR), in addition to modern neural network models includingRecurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM isexplicitly constructed to augment our predictive capabilities related topatterns and trends inherent in complex natural phenomena. By utilizing timeseries data relevant to these events, G-NM facilitates the prediction of suchphenomena over extended periods. The primary objective of this research is toboth advance our understanding of such occurrences and to significantly enhancethe accuracy of our forecasts. G-NM encapsulates both linear and non-lineardependencies, seasonalities, and trends present in time series data. Each ofthese models contributes distinct strengths, from ARIMA's resilience inhandling linear trends and seasonality, SVR's proficiency in capturingnon-linear patterns, to LSTM's adaptability in modeling various components oftime series data. Through the exploitation of the G-NM potential, we strive toadvance the state-of-the-art in large-scale time series forecasting models. Weanticipate that this research will represent a significant stepping stone inour ongoing endeavor to comprehend and forecast the complex events thatconstitute the natural world.</description><author>Juyoung Yun</author><pubDate>Tue, 20 Jun 2023 17:39:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11667v1</guid></item><item><title>Bridging the Gap: Differentially Private Equivariant Deep Learning for Medical Image Analysis</title><link>http://arxiv.org/abs/2209.04338v2</link><description>Machine learning with formal privacy-preserving techniques like DifferentialPrivacy (DP) allows one to derive valuable insights from sensitive medicalimaging data while promising to protect patient privacy, but it usually comesat a sharp privacy-utility trade-off. In this work, we propose to use steerableequivariant convolutional networks for medical image analysis with DP. Theirimproved feature quality and parameter efficiency yield remarkable accuracygains, narrowing the privacy-utility gap.</description><author>Florian A. Hölzl, Daniel Rueckert, Georgios Kaissis</author><pubDate>Tue, 20 Jun 2023 17:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.04338v2</guid></item><item><title>Neural Astrophysical Wind Models</title><link>http://arxiv.org/abs/2306.11666v1</link><description>The bulk kinematics and thermodynamics of hot supernovae-driven galacticwinds is critically dependent on both the amount of swept up cool clouds andnon-spherical collimated flow geometry. However, accurately parameterizingthese physics is difficult because their functional forms are often unknown,and because the coupled non-linear flow equations contain singularities. Weshow that deep neural networks embedded as individual terms in the governingcoupled ordinary differential equations (ODEs) can robustly discover both ofthese physics, without any prior knowledge of the true function structure, as asupervised learning task. We optimize a loss function based on the Mach number,rather than the explicitly solved-for 3 conserved variables, and apply apenalty term towards near-diverging solutions. The same neural networkarchitecture is used for learning both the hidden mass-loading and surface areaexpansion rates. This work further highlights the feasibility of neural ODEs asa promising discovery tool with mechanistic interpretability for non-linearinverse problems.</description><author>Dustin D. Nguyen</author><pubDate>Tue, 20 Jun 2023 17:37:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11666v1</guid></item><item><title>Multi-Concept Customization of Text-to-Image Diffusion</title><link>http://arxiv.org/abs/2212.04488v2</link><description>While generative models produce high-quality images of concepts learned froma large-scale database, a user often wishes to synthesize instantiations oftheir own concepts (for example, their family, pets, or items). Can we teach amodel to quickly acquire a new concept, given a few examples? Furthermore, canwe compose multiple new concepts together? We propose Custom Diffusion, anefficient method for augmenting existing text-to-image models. We find thatonly optimizing a few parameters in the text-to-image conditioning mechanism issufficiently powerful to represent new concepts while enabling fast tuning (~6minutes). Additionally, we can jointly train for multiple concepts or combinemultiple fine-tuned models into one via closed-form constrained optimization.Our fine-tuned model generates variations of multiple new concepts andseamlessly composes them with existing concepts in novel settings. Our methodoutperforms or performs on par with several baselines and concurrent works inboth qualitative and quantitative evaluations while being memory andcomputationally efficient.</description><author>Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu</author><pubDate>Tue, 20 Jun 2023 17:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04488v2</guid></item><item><title>Regularization Through Simultaneous Learning: A Case Study on Plant Classification</title><link>http://arxiv.org/abs/2305.13447v4</link><description>In response to the prevalent challenge of overfitting in deep neuralnetworks, this paper introduces Simultaneous Learning, a regularizationapproach drawing on principles of Transfer Learning and Multi-task Learning. Weleverage auxiliary datasets with the target dataset, the UFOP-HVD, tofacilitate simultaneous classification guided by a customized loss functionfeaturing an inter-group penalty. This experimental configuration allows for adetailed examination of model performance across similar (PlantNet) anddissimilar (ImageNet) domains, thereby enriching the generalizability ofConvolutional Neural Network models. Remarkably, our approach demonstratessuperior performance over models without regularization and those applyingdropout regularization exclusively, enhancing accuracy by 5 to 22 percentagepoints. Moreover, when combined with dropout, the proposed approach improvesgeneralization, securing state-of-the-art results for the UFOP-HVD challenge.The method also showcases efficiency with significantly smaller sample sizes,suggesting its broad applicability across a spectrum of related tasks. Inaddition, an interpretability approach is deployed to evaluate feature qualityby analyzing class feature correlations within the network's convolutionallayers. The findings of this study provide deeper insights into the efficacy ofSimultaneous Learning, particularly concerning its interaction with theauxiliary and target datasets.</description><author>Pedro Henrique Nascimento Castro, Gabriel Cássia Fortuna, Rafael Alves Bonfim de Queiroz, Gladston Juliano Prates Moreira, Eduardo José da Silva Luz</author><pubDate>Tue, 20 Jun 2023 17:18:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13447v4</guid></item><item><title>FedNoisy: Federated Noisy Label Learning Benchmark</title><link>http://arxiv.org/abs/2306.11650v1</link><description>Federated learning has gained popularity for distributed learning withoutaggregating sensitive data from clients. But meanwhile, the distributed andisolated nature of data isolation may be complicated by data quality, making itmore vulnerable to noisy labels. Many efforts exist to defend against thenegative impacts of noisy labels in centralized or federated settings. However,there is a lack of a benchmark that comprehensively considers the impact ofnoisy labels in a wide variety of typical FL settings. In this work, we servethe first standardized benchmark that can help researchers fully explorepotential federated noisy settings. Also, we conduct comprehensive experimentsto explore the characteristics of these data settings and unravel challengingscenarios on the federated noisy label learning, which may guide methoddevelopment in the future. We highlight the 20 basic settings for more than 5datasets proposed in our benchmark and standardized simulation pipeline forfederated noisy label learning. We hope this benchmark can facilitate ideaverification in federated learning with noisy labels. \texttt{FedNoisy} isavailable at \codeword{https://github.com/SMILELab-FL/FedNoisy}.</description><author>Siqi Liang, Jintao Huang, Dun Zeng, Junyuan Hong, Jiayu Zhou, Zenglin Xu</author><pubDate>Tue, 20 Jun 2023 17:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11650v1</guid></item><item><title>Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy</title><link>http://arxiv.org/abs/2306.11648v1</link><description>This study investigates the application of Large Language Models (LLMs),specifically GPT-4, within Astronomy. We employ in-context prompting, supplyingthe model with up to 1000 papers from the NASA Astrophysics Data System, toexplore the extent to which performance can be improved by immersing the modelin domain-specific literature. Our findings point towards a substantial boostin hypothesis generation when using in-context prompting, a benefit that isfurther accentuated by adversarial prompting. We illustrate how adversarialprompting empowers GPT-4 to extract essential details from a vast knowledgebase to produce meaningful hypotheses, signaling an innovative step towardsemploying LLMs for scientific research in Astronomy.</description><author>Ioana Ciucă, Yuan-Sen Ting, Sandor Kruk, Kartheik Iyer</author><pubDate>Tue, 20 Jun 2023 17:16:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11648v1</guid></item><item><title>Recent Advances in Direct Speech-to-text Translation</title><link>http://arxiv.org/abs/2306.11646v1</link><description>Recently, speech-to-text translation has attracted more and more attentionand many studies have emerged rapidly. In this paper, we present acomprehensive survey on direct speech translation aiming to summarize thecurrent state-of-the-art techniques. First, we categorize the existing researchwork into three directions based on the main challenges -- modeling burden,data scarcity, and application issues. To tackle the problem of modelingburden, two main structures have been proposed, encoder-decoder framework(Transformer and the variants) and multitask frameworks. For the challenge ofdata scarcity, recent work resorts to many sophisticated techniques, such asdata augmentation, pre-training, knowledge distillation, and multilingualmodeling. We analyze and summarize the application issues, which includereal-time, segmentation, named entity, gender bias, and code-switching.Finally, we discuss some promising directions for future work.</description><author>Chen Xu, Rong Ye, Qianqian Dong, Chengqi Zhao, Tom Ko, Mingxuan Wang, Tong Xiao, Jingbo Zhu</author><pubDate>Tue, 20 Jun 2023 17:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11646v1</guid></item><item><title>Textbooks Are All You Need</title><link>http://arxiv.org/abs/2306.11644v1</link><description>We introduce phi-1, a new large language model for code, with significantlysmaller size than competing models: phi-1 is a Transformer-based model with1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbookquality" data from the web (6B tokens) and synthetically generated textbooksand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attainspass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displayssurprising emergent properties compared to phi-1-base, our model before ourfinetuning stage on a dataset of coding exercises, and phi-1-small, a smallermodel with 350M parameters trained with the same pipeline as phi-1 that stillachieves 45% on HumanEval.</description><author>Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, Yuanzhi Li</author><pubDate>Tue, 20 Jun 2023 17:14:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11644v1</guid></item><item><title>Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning</title><link>http://arxiv.org/abs/2206.02670v4</link><description>The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agentsoperating in public are increasing. Adopting AI-based techniques and, morespecifically, Deep Learning (DL) approaches to control and guide these UAVs canbe beneficial in terms of performance but can add concerns regarding the safetyof those techniques and their vulnerability against adversarial attacks.Confusion in the agent's decision-making process caused by these attacks canseriously affect the safety of the UAV. This paper proposes an innovativeapproach based on the explainability of DL methods to build an efficientdetector that will protect these DL schemes and the UAVs adopting them fromattacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme forguidance and planning. The agent is trained with a Deep Deterministic PolicyGradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme thatutilises Artificial Potential Field (APF) to improve training times andobstacle avoidance performance. A simulated environment for UAV explainableDRL-based planning and guidance, including obstacles and adversarial attacks,is built. The adversarial attacks are generated by the Basic Iterative Method(BIM) algorithm and reduced obstacle course completion rates from 97\% to 35\%.Two adversarial attack detectors are proposed to counter this reduction. Thefirst one is a Convolutional Neural Network Adversarial Detector (CNN-AD),which achieves accuracy in the detection of 80\%. The second detector utilisesa Long Short Term Memory (LSTM) network. It achieves an accuracy of 91\% withfaster computing times compared to the CNN-AD, allowing for real-timeadversarial detection.</description><author>Thomas Hickling, Nabil Aouf, Phillippa Spencer</author><pubDate>Tue, 20 Jun 2023 17:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02670v4</guid></item><item><title>Collision Avoidance Detour for Multi-Agent Trajectory Forecasting</title><link>http://arxiv.org/abs/2306.11638v1</link><description>We present our approach, Collision Avoidance Detour (CAD), which won the 3rdplace award in the 2023 Waymo Open Dataset Challenge - Sim Agents, held at the2023 CVPR Workshop on Autonomous Driving. To satisfy the motion predictionfactorization requirement, we partition all the valid objects into threemutually exclusive sets: Autonomous Driving Vehicle (ADV),World-tracks-to-predict, and World-others. We use different motion models toforecast their future trajectories independently. Furthermore, we also applycollision avoidance detour resampling, additive Gaussian noise, andvelocity-based heading estimation to improve the realism of our simulationresult.</description><author>Hsu-kuang Chiu, Stephen F. Smith</author><pubDate>Tue, 20 Jun 2023 17:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11638v1</guid></item><item><title>SeFNet: Bridging Tabular Datasets with Semantic Feature Nets</title><link>http://arxiv.org/abs/2306.11636v1</link><description>Machine learning applications cover a wide range of predictive tasks in whichtabular datasets play a significant role. However, although they often addresssimilar problems, tabular datasets are typically treated as standalone tasks.The possibilities of using previously solved problems are limited due to thelack of structured contextual information about their features and the lack ofunderstanding of the relations between them. To overcome this limitation, wepropose a new approach called Semantic Feature Net (SeFNet), capturing thesemantic meaning of the analyzed tabular features. By leveraging existingontologies and domain knowledge, SeFNet opens up new opportunities for sharinginsights between diverse predictive tasks. One such opportunity is the DatasetOntology-based Semantic Similarity (DOSS) measure, which quantifies thesimilarity between datasets using relations across their features. In thispaper, we present an example of SeFNet prepared for a collection of predictivetasks in healthcare, with the features' relations derived from the SNOMED-CTontology. The proposed SeFNet framework and the accompanying DOSS measureaddress the issue of limited contextual information in tabular datasets. Byincorporating domain knowledge and establishing semantic relations betweenfeatures, we enhance the potential for meta-learning and enable valuableinsights to be shared across different predictive tasks.</description><author>Katarzyna Woźnica, Piotr Wilczyński, Przemysław Biecek</author><pubDate>Tue, 20 Jun 2023 17:02:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11636v1</guid></item><item><title>Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models</title><link>http://arxiv.org/abs/2302.10289v7</link><description>We use concept-based interpretable models to mitigate shortcut learning.Existing methods lack interpretability. Beginning with a Blackbox, weiteratively carve out a mixture of interpretable experts (MoIE) and a residualnetwork. Each expert explains a subset of data using First Order Logic (FOL).While explaining a sample, the FOL from biased BB-derived MoIE detects theshortcut effectively. Finetuning the BB with Metadata Normalization (MDN)eliminates the shortcut. The FOLs from the finetuned-BB-derived MoIE verify theelimination of the shortcut. Our experiments show that MoIE does not hurt theaccuracy of the original BB and eliminates shortcuts effectively.</description><author>Shantanu Ghosh, Ke Yu, Forough Arabshahi, Kayhan Batmanghelich</author><pubDate>Tue, 20 Jun 2023 16:56:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10289v7</guid></item><item><title>PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated Catalyst Design</title><link>http://arxiv.org/abs/2211.12020v2</link><description>Mitigating the climate crisis requires a rapid transition towards lowercarbon energy. Catalyst materials play a crucial role in the electrochemicalreactions involved in a great number of industrial processes key to thistransition, such as renewable energy storage and electrofuel synthesis. Toreduce the amount of energy spent on such processes, we must quickly discovermore efficient catalysts to drive the electrochemical reactions. Machinelearning (ML) holds the potential to efficiently model the properties ofmaterials from large amounts of data, and thus to accelerate electrocatalystdesign. The Open Catalyst Project OC20 data set was constructed to that end.However, most existing ML models trained on OC20 are still neither scalable noraccurate enough for practical applications. Here, we propose severaltask-specific innovations, applicable to most architectures, which increaseboth computational efficiency and accuracy. In particular, we proposeimprovements in (1) the graph creation step, (2) atom representations and (3)the energy prediction head. We describe these contributions and evaluate themon several architectures, showing up to 5$\times$ reduction in inference timewithout sacrificing accuracy.</description><author>Alexandre Duval, Victor Schmidt, Santiago Miret, Yoshua Bengio, Alex Hernández-García, David Rolnick</author><pubDate>Tue, 20 Jun 2023 16:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.12020v2</guid></item><item><title>Regularized Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity</title><link>http://arxiv.org/abs/2306.11626v1</link><description>This paper focuses on reinforcement learning for the regularized robustMarkov decision process (MDP) problem, an extension of the robust MDPframework. We first introduce the risk-sensitive MDP and establish theequivalence between risk-sensitive MDP and regularized robust MDP. Thisequivalence offers an alternative perspective for addressing the regularizedRMDP and enables the design of efficient learning algorithms. Given thisequivalence, we further derive the policy gradient theorem for the regularizedrobust MDP problem and prove the global convergence of the exact policygradient method under the tabular setting with direct parameterization. We alsopropose a sample-based offline learning algorithm, namely the robust fitted-Ziteration (RFZI), for a specific regularized robust MDP problem with aKL-divergence regularization term and analyze the sample complexity of thealgorithm. Our results are also supported by numerical simulations.</description><author>Runyu Zhang, Yang Hu, Na Li</author><pubDate>Tue, 20 Jun 2023 16:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11626v1</guid></item><item><title>Deep Learning and Ethics</title><link>http://arxiv.org/abs/2305.15239v2</link><description>This article appears as chapter 21 of Prince (2023, Understanding DeepLearning); a complete draft of the textbook is available here:http://udlbook.com. This chapter considers potential harms arising from thedesign and use of AI systems. These include algorithmic bias, lack ofexplainability, data privacy violations, militarization, fraud, andenvironmental concerns. The aim is not to provide advice on being more ethical.Instead, the goal is to express ideas and start conversations in key areas thathave received attention in philosophy, political science, and the broadersocial sciences.</description><author>Travis LaCroix, Simon J. D. Prince</author><pubDate>Tue, 20 Jun 2023 16:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15239v2</guid></item><item><title>Mean-field Analysis of Generalization Errors</title><link>http://arxiv.org/abs/2306.11623v1</link><description>We propose a novel framework for exploring weak and $L_2$ generalizationerrors of algorithms through the lens of differential calculus on the space ofprobability measures. Specifically, we consider the KL-regularized empiricalrisk minimization problem and establish generic conditions under which thegeneralization error convergence rate, when training on a sample of size $n$,is $\mathcal{O}(1/n)$. In the context of supervised learning with a one-hiddenlayer neural network in the mean-field regime, these conditions are reflectedin suitable integrability and regularity assumptions on the loss and activationfunctions.</description><author>Gholamali Aminian, Samuel N. Cohen, Łukasz Szpruch</author><pubDate>Tue, 20 Jun 2023 16:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11623v1</guid></item><item><title>Causal Analysis for Robust Interpretability of Neural Networks</title><link>http://arxiv.org/abs/2305.08950v2</link><description>Interpreting the inner function of neural networks is crucial for thetrustworthy development and deployment of these black-box models. Priorinterpretability methods focus on correlation-based measures to attribute modeldecisions to individual examples. However, these measures are susceptible tonoise and spurious correlations encoded in the model during the training phase(e.g., biased inputs, model overfitting, or misspecification). Moreover, thisprocess has proven to result in noisy and unstable attributions that preventany transparent understanding of the model's behavior. In this paper, wedevelop a robust interventional-based method grounded by causal analysis tocapture cause-effect mechanisms in pre-trained neural networks and theirrelation to the prediction. Our novel approach relies on path interventions toinfer the causal mechanisms within hidden layers and isolate relevant andnecessary information (to model prediction), avoiding noisy ones. The result istask-specific causal explanatory graphs that can audit model behavior andexpress the actual causes underlying its performance. We apply our method tovision models trained on classification tasks. On image classification tasks,we provide extensive quantitative experiments to show that our approach cancapture more stable and faithful explanations than standard attribution-basedmethods. Furthermore, the underlying causal graphs reveal the neuralinteractions in the model, making it a valuable tool in other applications(e.g., model repair).</description><author>Ola Ahmad, Nicolas Bereux, Loïc Baret, Vahid Hashemi, Freddy Lecue</author><pubDate>Tue, 20 Jun 2023 16:43:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08950v2</guid></item><item><title>Towards mutual synchronization of serially connected Spin Torque Oscillators based on magnetic tunnel junctions</title><link>http://arxiv.org/abs/2306.11608v1</link><description>Multiple neuromorphic applications require the tuning of two or more devicesto a common signal. Various types of neuromorphic computation can be realizedusing spintronic oscillators, where the DC current induces magnetizationprecession, which turns into an AC voltage generator. However, in spintronics,synchronization of two oscillators using a DC signal is still a challengingproblem because it requires a certain degree of similarity between devices thatare to be synchronized, which may be difficult to achieve due to deviceparameter distribution during the fabrication process. In this work, we presentexperimental results on the mechanisms of synchronization of spin-torqueoscillators. Devices are based on magnetic tunnel junction with aperpendicularly magnetized free layer and take advantage of a uniformmagnetization precision in the presence of the magnetic field and a DC bias. Byusing an external microwave source, we show the optimal condition for thesynchronization of the magnetic tunnel junctions. Finally, we present resultson the in-series connection of two junctions and discuss the possible pathtowards improving oscillation power and linewidth. In addition, using numericalsimulations of the coupled oscillators model, we aim to reproduce theconditions of the experiments and determine the tolerance for achievingsynchronization.</description><author>Piotr Rzeszut, Jakub Mojsiejuk, Witold Skowroński, Sumito Tsunegi, Hitoshi Kubota, Shinji Yuasa</author><pubDate>Tue, 20 Jun 2023 16:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11608v1</guid></item><item><title>Annotation Cost Efficient Active Learning for Content Based Image Retrieval</title><link>http://arxiv.org/abs/2306.11605v1</link><description>Deep metric learning (DML) based methods have been found very effective forcontent-based image retrieval (CBIR) in remote sensing (RS). For accuratelylearning the model parameters of deep neural networks, most of the DML methodsrequire a high number of annotated training images, which can be costly togather. To address this problem, in this paper we present an annotation costefficient active learning (AL) method (denoted as ANNEAL). The proposed methodaims to iteratively enrich the training set by annotating the most informativeimage pairs as similar or dissimilar, %answering a simple yes/no question,while accurately modelling a deep metric space. This is achieved by twoconsecutive steps. In the first step the pairwise image similarity is modelledbased on the available training set. Then, in the second step the mostuncertain and diverse (i.e., informative) image pairs are selected to beannotated. Unlike the existing AL methods for CBIR, at each AL iteration ofANNEAL a human expert is asked to annotate the most informative image pairs assimilar/dissimilar. This significantly reduces the annotation cost compared toannotating images with land-use/land cover class labels. Experimental resultsshow the effectiveness of our method. The code of ANNEAL is publicly availableat https://git.tu-berlin.de/rsim/ANNEAL.</description><author>Julia Henkel, Genc Hoxha, Gencer Sumbul, Lars Möllenbrok, Begüm Demir</author><pubDate>Tue, 20 Jun 2023 16:33:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11605v1</guid></item><item><title>Correlated Time Series Self-Supervised Representation Learning via Spatiotemporal Bootstrapping</title><link>http://arxiv.org/abs/2306.06994v2</link><description>Correlated time series analysis plays an important role in many real-worldindustries. Learning an efficient representation of this large-scale data forfurther downstream tasks is necessary but challenging. In this paper, wepropose a time-step-level representation learning framework for individualinstances via bootstrapped spatiotemporal representation prediction. Weevaluated the effectiveness and flexibility of our representation learningframework on correlated time series forecasting and cold-start transferring theforecasting model to new instances with limited data. A linear regression modeltrained on top of the learned representations demonstrates our model performsbest in most cases. Especially compared to representation learning models, wereduce the RMSE, MAE, and MAPE by 37%, 49%, and 48% on the PeMS-BAY dataset,respectively. Furthermore, in real-world metro passenger flow data, ourframework demonstrates the ability to transfer to infer future information ofnew cold-start instances, with gains of 15%, 19%, and 18%. The source code willbe released under the GitHubhttps://github.com/bonaldli/Spatiotemporal-TS-Representation-Learning</description><author>Luxuan Wang, Lei Bai, Ziyue Li, Rui Zhao, Fugee Tsung</author><pubDate>Tue, 20 Jun 2023 16:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06994v2</guid></item><item><title>Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*</title><link>http://arxiv.org/abs/2305.06721v2</link><description>To advance the neural encoding of Portuguese (PT), and a fortiori thetechnological preparation of this language for the digital age, we developed aTransformer-based foundation model that sets a new state of the art in thisrespect for two of its variants, namely European Portuguese from Portugal(PT-PT) and American Portuguese from Brazil (PT-BR). To develop this encoder, which we named Albertina PT-*, a strong model wasused as a starting point, DeBERTa, and its pre-training was done over data setsof Portuguese, namely over data sets we gathered for PT-PT and PT-BR, and overthe brWaC corpus for PT-BR. The performance of Albertina and competing modelswas assessed by evaluating them on prominent downstream language processingtasks adapted for Portuguese. Both Albertina PT-PT and PT-BR versions are distributed free of charge andunder the most permissive license possible and can be run on consumer-gradehardware, thus seeking to contribute to the advancement of research andinnovation in language technology for Portuguese.</description><author>João Rodrigues, Luís Gomes, João Silva, António Branco, Rodrigo Santos, Henrique Lopes Cardoso, Tomás Osório</author><pubDate>Tue, 20 Jun 2023 16:22:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06721v2</guid></item><item><title>BEVScope: Enhancing Self-Supervised Depth Estimation Leveraging Bird's-Eye-View in Dynamic Scenarios</title><link>http://arxiv.org/abs/2306.11598v1</link><description>Depth estimation is a cornerstone of perception in autonomous driving androbotic systems. The considerable cost and relatively sparse data acquisitionof LiDAR systems have led to the exploration of cost-effective alternatives,notably, self-supervised depth estimation. Nevertheless, currentself-supervised depth estimation methods grapple with several limitations: (1)the failure to adequately leverage informative multi-camera views. (2) thelimited capacity to handle dynamic objects effectively. To address thesechallenges, we present BEVScope, an innovative approach to self-superviseddepth estimation that harnesses Bird's-Eye-View (BEV) features. Concurrently,we propose an adaptive loss function, specifically designed to mitigate thecomplexities associated with moving objects. Empirical evaluations conducted onthe Nuscenes dataset validate our approach, demonstrating competitiveperformance. Code will be released at https://github.com/myc634/BEVScope.</description><author>Yucheng Mao, Ruowen Zhao, Tianbao Zhang, Hang Zhao</author><pubDate>Tue, 20 Jun 2023 16:16:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11598v1</guid></item><item><title>Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion</title><link>http://arxiv.org/abs/2306.11593v1</link><description>State-of-The-Art (SoTA) image captioning models often rely on the MicrosoftCOCO (MS-COCO) dataset for training. This dataset contains annotations providedby human annotators, who typically produce captions averaging around tentokens. However, this constraint presents a challenge in effectively capturingcomplex scenes and conveying detailed information. Furthermore, captioningmodels tend to exhibit bias towards the ``average'' caption, which capturesonly the more general aspects. What would happen if we were able toautomatically generate longer captions, thereby making them more detailed?Would these captions, evaluated by humans, be more or less representative ofthe image content compared to the original MS-COCO captions? In this paper, wepresent a novel approach to address previous challenges by showcasing howcaptions generated from different SoTA models can be effectively fused,resulting in richer captions. Our proposed method leverages existing modelsfrom the literature, eliminating the need for additional training. Instead, itutilizes an image-text based metric to rank the captions generated by SoTAmodels for a given image. Subsequently, the top two captions are fused using aLarge Language Model (LLM). Experimental results demonstrate the effectivenessof our approach, as the captions generated by our model exhibit higherconsistency with human judgment when evaluated on the MS-COCO test set. Bycombining the strengths of various SoTA models, our method enhances the qualityand appeal of image captions, bridging the gap between automated systems andthe rich, informative nature of human-generated descriptions. This advanceopens up new possibilities for generating captions that are more suitable forthe training of both vision-language and captioning models.</description><author>Simone Bianco, Luigi Celona, Marco Donzella, Paolo Napoletano</author><pubDate>Tue, 20 Jun 2023 16:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11593v1</guid></item><item><title>Deep Double Self-Expressive Subspace Clustering</title><link>http://arxiv.org/abs/2306.11592v1</link><description>Deep subspace clustering based on auto-encoder has received wide attention.However, most subspace clustering based on auto-encoder does not utilize thestructural information in the self-expressive coefficient matrix, which limitsthe clustering performance. In this paper, we propose a double self-expressivesubspace clustering algorithm. The key idea of our solution is to view theself-expressive coefficient as a feature representation of the example to getanother coefficient matrix. Then, we use the two coefficient matrices toconstruct the affinity matrix for spectral clustering. We find that it canreduce the subspace-preserving representation error and improve connectivity.To further enhance the clustering performance, we proposed a self-supervisedmodule based on contrastive learning, which can further improve the performanceof the trained network. Experiments on several benchmark datasets demonstratethat the proposed algorithm can achieve better clustering than state-of-the-artmethods.</description><author>Ling Zhao, Yunpeng Ma, Shanxiong Chen, Jun Zhou</author><pubDate>Tue, 20 Jun 2023 16:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11592v1</guid></item><item><title>Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent</title><link>http://arxiv.org/abs/2306.11589v1</link><description>Gaussian processes are a powerful framework for quantifying uncertainty andfor sequential decision-making but are limited by the requirement of solvinglinear systems. In general, this has a cubic cost in dataset size and issensitive to conditioning. We explore stochastic gradient algorithms as acomputationally efficient method of approximately solving these linear systems:we develop low-variance optimization objectives for sampling from the posteriorand extend these to inducing points. Counterintuitively, stochastic gradientdescent often produces accurate predictions, even in cases where it does notconverge quickly to the optimum. We explain this through a spectralcharacterization of the implicit bias from non-convergence. We show thatstochastic gradient descent produces predictive distributions close to the trueposterior both in regions with sufficient data coverage, and in regionssufficiently far away from the data. Experimentally, stochastic gradientdescent achieves state-of-the-art performance on sufficiently large-scale orill-conditioned regression tasks. Its uncertainty estimates match theperformance of significantly more expensive baselines on a large-scaleBayesian~optimization~task.</description><author>Jihao Andreas Lin, Javier Antorán, Shreyas Padhy, David Janz, José Miguel Hernández-Lobato, Alexander Terenin</author><pubDate>Tue, 20 Jun 2023 16:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11589v1</guid></item><item><title>Domain-Aware Few-Shot Learning for Optical Coherence Tomography Noise Reduction</title><link>http://arxiv.org/abs/2306.08102v2</link><description>Speckle noise has long been an extensively studied problem in medicalimaging. In recent years, there have been significant advances in leveragingdeep learning methods for noise reduction. Nevertheless, adaptation ofsupervised learning models to unseen domains remains a challenging problem.Specifically, deep neural networks (DNNs) trained for computational imagingtasks are vulnerable to changes in the acquisition system's physicalparameters, such as: sampling space, resolution, and contrast. Even within thesame acquisition system, performance degrades across datasets of differentbiological tissues. In this work, we propose a few-shot supervised learningframework for optical coherence tomography (OCT) noise reduction, that offers adramatic increase in training speed and requires only a single image, or partof an image, and a corresponding speckle suppressed ground truth, for training.Furthermore, we formulate the domain shift problem for OCT diverse imagingsystems, and prove that the output resolution of a despeckling trained model isdetermined by the source domain resolution. We also provide possible remedies.We propose different practical implementations of our approach, verify andcompare their applicability, robustness, and computational efficiency. Ourresults demonstrate significant potential for generally improving samplecomplexity, generalization, and time efficiency, for coherent and non-coherentnoise reduction via supervised learning models, that can also be leveraged forother real-time computer vision applications.</description><author>Deborah Pereg</author><pubDate>Tue, 20 Jun 2023 16:04:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08102v2</guid></item><item><title>Provably Powerful Graph Neural Networks for Directed Multigraphs</title><link>http://arxiv.org/abs/2306.11586v1</link><description>This paper proposes a set of simple adaptations to transform standardmessage-passing Graph Neural Networks (GNN) into provably powerful directedmultigraph neural networks. The adaptations include multigraph port numbering,ego IDs, and reverse message passing. We prove that the combination of thesetheoretically enables the detection of any directed subgraph pattern. Tovalidate the effectiveness of our proposed adaptations in practice, we conductexperiments on synthetic subgraph detection tasks, which demonstrateoutstanding performance with almost perfect results. Moreover, we apply our proposed adaptations to two financial crime analysistasks. We observe dramatic improvements in detecting money launderingtransactions, improving the minority-class F1 score of a standardmessage-passing GNN by up to 45%, and clearly outperforming tree-based and GNNbaselines. Similarly impressive results are observed on a real-world phishingdetection dataset, boosting a standard GNN's F1 score by over 15% andoutperforming all baselines.</description><author>Béni Egressy, Luc von Niederhäusern, Jovan Blanusa, Erik Altman, Roger Wattenhofer, Kubilay Atasu</author><pubDate>Tue, 20 Jun 2023 16:03:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11586v1</guid></item><item><title>FAIR: A Causal Framework for Accurately Inferring Judgments Reversals</title><link>http://arxiv.org/abs/2306.11585v1</link><description>Artificial intelligence researchers have made significant advances in legalintelligence in recent years. However, the existing studies have not focused onthe important value embedded in judgments reversals, which limits theimprovement of the efficiency of legal intelligence. In this paper, we proposea causal Framework for Accurately Inferring case Reversals (FAIR), which modelsthe problem of judgments reversals based on real Chinese judgments. We mine thecauses of judgments reversals by causal inference methods and inject theobtained causal relationships into the neural network as a priori knowledge.And then, our framework is validated on a challenging dataset as a legaljudgment prediction task. The experimental results show that our framework cantap the most critical factors in judgments reversal, and the obtained causalrelationships can effectively improve the neural network's performance. Inaddition, we discuss the generalization ability of large language models forlegal intelligence tasks using ChatGPT as an example. Our experiment has foundthat the generalization ability of large language models still has defects, andmining causal relationships can effectively improve the accuracy and explainability of model predictions.</description><author>Minghua He, Nanfei Gu, Yuntao Shi, Qionghui Zhang, Yaying Chen</author><pubDate>Tue, 20 Jun 2023 16:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11585v1</guid></item><item><title>Computing a human-like reaction time metric from stable recurrent vision models</title><link>http://arxiv.org/abs/2306.11582v1</link><description>The meteoric rise in the adoption of deep neural networks as computationalmodels of vision has inspired efforts to "align" these models with humans. Onedimension of interest for alignment includes behavioral choices, but movingbeyond characterizing choice patterns to capturing temporal aspects of visualdecision-making has been challenging. Here, we sketch a general-purposemethodology to construct computational accounts of reaction times from astimulus-computable, task-optimized model. Specifically, we introduce a novelmetric leveraging insights from subjective logic theory summarizing evidenceaccumulation in recurrent vision models. We demonstrate that our metric alignswith patterns of human reaction times for stimulus manipulations across fourdisparate visual decision-making tasks spanning perceptual grouping, mentalsimulation, and scene categorization. This work paves the way for exploring thetemporal alignment of model and human visual strategies in the context ofvarious other cognitive tasks toward generating testable hypotheses forneuroscience.</description><author>Lore Goetschalckx, Lakshmi Narasimhan Govindarajan, Alekh Karkada Ashok, Aarit Ahuja, David L. Sheinberg, Thomas Serre</author><pubDate>Tue, 20 Jun 2023 15:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11582v1</guid></item><item><title>A Trustworthiness Score to Evaluate DNN Predictions</title><link>http://arxiv.org/abs/2301.08839v6</link><description>Due to the black box nature of deep neural networks (DNN), the continuousvalidation of DNN during operation is challenging with the absence of a humanmonitor. As a result this makes it difficult for developers and regulators togain confidence in the deployment of autonomous systems employing DNN. It iscritical for safety during operation to know when DNN's predictions aretrustworthy or suspicious. With the absence of a human monitor, the basicapproach is to use the model's output confidence score to assess if predictionsare trustworthy or suspicious. However, the model's confidence score is aresult of computations coming from a black box, therefore lacks transparencyand makes it challenging to automatedly credit trustworthiness to predictions.We introduce the trustworthiness score (TS), a simple metric that provides amore transparent and effective way of providing confidence in DNN predictionscompared to model's confidence score. The metric quantifies the trustworthinessin a prediction by checking for the existence of certain features in thepredictions made by the DNN. We also use the underlying idea of the TS metric,to provide a suspiciousness score (SS) in the overall input frame to help inthe detection of suspicious frames where false negatives exist. We conduct acase study using YOLOv5 on persons detection to demonstrate our method andusage of TS and SS. The case study shows that using our method consistentlyimproves the precision of predictions compared to relying on model confidencescore alone, for both 1) approving of trustworthy predictions (~20%improvement) and 2) detecting suspicious frames (~5% improvement).</description><author>Abanoub Ghobrial, Darryl Hond, Hamid Asgari, Kerstin Eder</author><pubDate>Tue, 20 Jun 2023 15:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.08839v6</guid></item><item><title>Deep Learning Methods for Retinal Blood Vessel Segmentation: Evaluation on Images with Retinopathy of Prematurity</title><link>http://arxiv.org/abs/2306.11576v1</link><description>Automatic blood vessel segmentation from retinal images plays an importantrole in the diagnosis of many systemic and eye diseases, including retinopathyof prematurity. Current state-of-the-art research in blood vessel segmentationfrom retinal images is based on convolutional neural networks. The solutionsproposed so far are trained and tested on images from a few available retinalblood vessel segmentation datasets, which might limit their performance whengiven an image with retinopathy of prematurity signs. In this paper, weevaluate the performance of three high-performing convolutional neural networksfor retinal blood vessel segmentation in the context of blood vesselsegmentation on retinopathy of prematurity retinal images. The main motivebehind the study is to test if existing public datasets suffice to develop ahigh-performing predictor that could assist an ophthalmologist in retinopathyof prematurity diagnosis. To do so, we create a dataset consisting solely ofretinopathy of prematurity images with retinal blood vessel annotationsmanually labeled by two observers, where one is the ophthalmologist experiencedin retinopathy of prematurity treatment. Experimental results show that allthree solutions have difficulties in detecting the retinal blood vessels ofinfants due to a lower contrast compared to images from public datasets asdemonstrated by a significant drop in classification sensitivity. All threesolutions segment alongside retinal also choroidal blood vessels which are notused to diagnose retinopathy of prematurity, but instead represent noise andare confused with retinal blood vessels. By visual and numerical observations,we observe that existing solutions for retinal blood vessel segmentation needimprovement toward more detailed datasets or deeper models in order to assistthe ophthalmologist in retinopathy of prematurity diagnosis.</description><author>Gorana Gojić, Veljko Petrović, Radovan Turović, Dinu Dragan, Ana Oros, Dušan Gajić, Nebojša Horvat</author><pubDate>Tue, 20 Jun 2023 15:46:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11576v1</guid></item><item><title>PromptNER: Prompting For Named Entity Recognition</title><link>http://arxiv.org/abs/2305.15444v2</link><description>In a surprising turn, Large Language Models (LLMs) together with a growingarsenal of prompt-based heuristics now offer powerful off-the-shelf approachesproviding few-shot solutions to myriad classic NLP problems. However, despitepromising early results, these LLM-based few-shot methods remain far from thestate of the art in Named Entity Recognition (NER), where prevailing methodsinclude learning representations via end-to-end structural understanding andfine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt toany new NER task PromptNER requires a set of entity definitions in addition tothe standard few-shot examples. Given a sentence, PromptNER prompts an LLM toproduce a list of potential entities along with corresponding explanationsjustifying their compatibility with the provided entity type definitions.Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER,achieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9%(absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement onthe FewNERD dataset. PromptNER also moves the state of the art on Cross DomainNER, outperforming prior methods (including those not limited to the few-shotsetting), setting a new mark on 3/5 CrossNER target domains, with an average F1gain of 3%, despite using less than 2% of the available data.</description><author>Dhananjay Ashok, Zachary C. Lipton</author><pubDate>Tue, 20 Jun 2023 15:41:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15444v2</guid></item><item><title>Graph Kalman Filters</title><link>http://arxiv.org/abs/2303.12021v2</link><description>The well-known Kalman filters model dynamical systems by relying onstate-space representations with the next state updated, and its uncertaintycontrolled, by fresh information associated with newly observed system outputs.This paper generalizes, for the first time in the literature, Kalman andextended Kalman filters to discrete-time settings where inputs, states, andoutputs are represented as attributed graphs whose topology and attributes canchange with time. The setup allows us to adapt the framework to cases where theoutput is a vector or a scalar too (node/graph level tasks). Within theproposed theoretical framework, the unknown state-transition and the readoutfunctions are learned end-to-end along with the downstream prediction task.</description><author>Cesare Alippi, Daniele Zambon</author><pubDate>Tue, 20 Jun 2023 15:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12021v2</guid></item><item><title>HomeRobot: Open-Vocabulary Mobile Manipulation</title><link>http://arxiv.org/abs/2306.11565v1</link><description>HomeRobot (noun): An affordable compliant robot that navigates homes andmanipulates a wide range of objects in order to complete everyday tasks.Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any objectin any unseen environment, and placing it in a commanded location. This is afoundational challenge for robots to be useful assistants in humanenvironments, because it involves tackling sub-problems from across robotics:perception, language understanding, navigation, and manipulation are allessential to OVMM. In addition, integration of the solutions to thesesub-problems poses its own substantial challenges. To drive research in thisarea, we introduce the HomeRobot OVMM benchmark, where an agent navigateshousehold environments to grasp novel objects and place them on targetreceptacles. HomeRobot has two components: a simulation component, which uses alarge and diverse curated object set in new, high-quality multi-room homeenvironments; and a real-world component, providing a software stack for thelow-cost Hello Robot Stretch to encourage replication of real-world experimentsacross labs. We implement both reinforcement learning and heuristic(model-based) baselines and show evidence of sim-to-real transfer. Ourbaselines achieve a 20% success rate in the real world; our experimentsidentify ways future research work improve performance. See videos on ourwebsite: https://ovmm.github.io/.</description><author>Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, Zsolt Kira, Manolis Savva, Angel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton</author><pubDate>Tue, 20 Jun 2023 15:30:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11565v1</guid></item><item><title>MILD: Modeling the Instance Learning Dynamics for Learning with Noisy Labels</title><link>http://arxiv.org/abs/2306.11560v1</link><description>Despite deep learning has achieved great success, it often relies on a largeamount of training data with accurate labels, which are expensive andtime-consuming to collect. A prominent direction to reduce the cost is to learnwith noisy labels, which are ubiquitous in the real-world applications. Acritical challenge for such a learning task is to reduce the effect of networkmemorization on the falsely-labeled data. In this work, we propose an iterativeselection approach based on the Weibull mixture model, which identifies cleandata by considering the overall learning dynamics of each data instance. Incontrast to the previous small-loss heuristics, we leverage the observationthat deep network is easy to memorize and hard to forget clean data. Inparticular, we measure the difficulty of memorization and forgetting for eachinstance via the transition times between being misclassified and beingmemorized in training, and integrate them into a novel metric for selection.Based on the proposed metric, we retain a subset of identified clean data andrepeat the selection procedure to iteratively refine the clean subset, which isfinally used for model training. To validate our method, we perform extensiveexperiments on synthetic noisy datasets and real-world web data, and ourstrategy outperforms existing noisy-label learning methods.</description><author>Chuanyang Hu, Shipeng Yan, Zhitong Gao, Xuming He</author><pubDate>Tue, 20 Jun 2023 15:26:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11560v1</guid></item><item><title>Deep neural network techniques for monaural speech enhancement: state of the art analysis</title><link>http://arxiv.org/abs/2212.00369v2</link><description>Deep neural networks (DNN) techniques have become pervasive in domains suchas natural language processing and computer vision. They have achieved greatsuccess in these domains in task such as machine translation and imagegeneration. Due to their success, these data driven techniques have beenapplied in audio domain. More specifically, DNN models have been applied inspeech enhancement domain to achieve denosing, dereverberation andmulti-speaker separation in monaural speech enhancement. In this paper, wereview some dominant DNN techniques being employed to achieve speechseparation. The review looks at the whole pipeline of speech enhancement fromfeature extraction, how DNN based tools are modelling both global and localfeatures of speech and model training (supervised and unsupervised). We alsoreview the use of speech-enhancement pre-trained models to boost speechenhancement process. The review is geared towards covering the dominant trendswith regards to DNN application in speech enhancement in speech obtained via asingle speaker.</description><author>Peter Ochieng</author><pubDate>Tue, 20 Jun 2023 15:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00369v2</guid></item><item><title>The Ecological Fallacy in Annotation: Modelling Human Label Variation goes beyond Sociodemographics</title><link>http://arxiv.org/abs/2306.11559v1</link><description>Many NLP tasks exhibit human label variation, where different annotators givedifferent labels to the same texts. This variation is known to depend, at leastin part, on the sociodemographics of annotators. Recent research aims to modelindividual annotator behaviour rather than predicting aggregated labels, and wewould expect that sociodemographic information is useful for these models. Onthe other hand, the ecological fallacy states that aggregate group behaviour,such as the behaviour of the average female annotator, does not necessarilyexplain individual behaviour. To account for sociodemographics in models ofindividual annotator behaviour, we introduce group-specific layers tomulti-annotator models. In a series of experiments for toxic content detection,we find that explicitly accounting for sociodemographic attributes in this waydoes not significantly improve model performance. This result shows thatindividual annotation behaviour depends on much more than justsociodemographics.</description><author>Matthias Orlikowski, Paul Röttger, Philipp Cimiano, Dirk Hovy</author><pubDate>Tue, 20 Jun 2023 15:23:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11559v1</guid></item><item><title>Matched Pair Calibration for Ranking Fairness</title><link>http://arxiv.org/abs/2306.03775v2</link><description>We propose a test of fairness in score-based ranking systems called matchedpair calibration. Our approach constructs a set of matched item pairs withminimal confounding differences between subgroups before computing anappropriate measure of ranking error over the set. The matching step ensuresthat we compare subgroup outcomes between identically scored items so thatmeasured performance differences directly imply unfairness in subgroup-levelexposures. We show how our approach generalizes the fairness intuitions ofcalibration from a binary classification setting to ranking and connect ourapproach to other proposals for ranking fairness measures. Moreover, ourstrategy shows how the logic of marginal outcome tests extends to cases wherethe analyst has access to model scores. Lastly, we provide an example ofapplying matched pair calibration to a real-word ranking data set todemonstrate its efficacy in detecting ranking bias.</description><author>Hannah Korevaar, Chris McConnell, Edmund Tong, Erik Brinkman, Alana Shine, Misam Abbas, Blossom Metevier, Sam Corbett-Davies, Khalid El-Arini</author><pubDate>Tue, 20 Jun 2023 15:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03775v2</guid></item><item><title>Unifying Large Language Models and Knowledge Graphs: A Roadmap</title><link>http://arxiv.org/abs/2306.08302v2</link><description>Large language models (LLMs), such as ChatGPT and GPT4, are making new wavesin the field of natural language processing and artificial intelligence, due totheir emergent ability and generalizability. However, LLMs are black-boxmodels, which often fall short of capturing and accessing factual knowledge. Incontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, arestructured knowledge models that explicitly store rich factual knowledge. KGscan enhance LLMs by providing external knowledge for inference andinterpretability. Meanwhile, KGs are difficult to construct and evolving bynature, which challenges the existing methods in KGs to generate new facts andrepresent unseen knowledge. Therefore, it is complementary to unify LLMs andKGs together and simultaneously leverage their advantages. In this article, wepresent a forward-looking roadmap for the unification of LLMs and KGs. Ourroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,which incorporate KGs during the pre-training and inference phases of LLMs, orfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,completion, construction, graph-to-text generation, and question answering; and3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in amutually beneficial way to enhance both LLMs and KGs for bidirectionalreasoning driven by both data and knowledge. We review and summarize existingefforts within these three frameworks in our roadmap and pinpoint their futureresearch directions.</description><author>Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu</author><pubDate>Tue, 20 Jun 2023 15:18:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08302v2</guid></item><item><title>NeRF synthesis with shading guidance</title><link>http://arxiv.org/abs/2306.11556v1</link><description>The emerging Neural Radiance Field (NeRF) shows great potential inrepresenting 3D scenes, which can render photo-realistic images from novel viewwith only sparse views given. However, utilizing NeRF to reconstruct real-worldscenes requires images from different viewpoints, which limits its practicalapplication. This problem can be even more pronounced for large scenes. In thispaper, we introduce a new task called NeRF synthesis that utilizes thestructural content of a NeRF patch exemplar to construct a new radiance fieldof large size. We propose a two-phase method for synthesizing new scenes thatare continuous in geometry and appearance. We also propose a boundaryconstraint method to synthesize scenes of arbitrary size without artifacts.Specifically, we control the lighting effects of synthesized scenes usingshading guidance instead of decoupling the scene. We have demonstrated that ourmethod can generate high-quality results with consistent geometry andappearance, even for scenes with complex lighting. We can also synthesize newscenes on curved surface with arbitrary lighting effects, which enhances thepracticality of our proposed NeRF synthesis approach.</description><author>Chenbin Li, Yu Xin, Gaoyi Liu, Xiang Zeng, Ligang Liu</author><pubDate>Tue, 20 Jun 2023 15:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11556v1</guid></item><item><title>Inter-Cell Network Slicing With Transfer Learning Empowered Multi-Agent Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2306.11552v1</link><description>Network slicing enables operators to efficiently support diverse applicationson a common physical infrastructure. The ever-increasing densification ofnetwork deployment leads to complex and non-trivial inter-cell interference,which requires more than inaccurate analytic models to dynamically optimizeresource management for network slices. In this paper, we develop a DIRPalgorithm with multiple deep reinforcement learning (DRL) agents tocooperatively optimize resource partition in individual cells to fulfill therequirements of each slice, based on two alternative reward functions.Nevertheless, existing DRL approaches usually tie the pretrained modelparameters to specific network environments with poor transferability, whichraises practical deployment concerns in large-scale mobile networks. Hence, wedesign a novel transfer learning-aided DIRP (TL-DIRP) algorithm to ease thetransfer of DIRP agents across different network environments in terms ofsample efficiency, model reproducibility, and algorithm scalability. TheTL-DIRP algorithm first centrally trains a generalized model and then transfersthe "generalist" to each local agent as "specialist" with distributedfinetuning and execution. TL-DIRP consists of two steps: 1) centralizedtraining of a generalized distributed model, 2) transferring the "generalist"to each "specialist" with distributed finetuning and execution. The numericalresults show that not only DIRP outperforms existing baseline approaches interms of faster convergence and higher reward, but more importantly, TL-DIRPsignificantly improves the service performance, with reduced exploration cost,accelerated convergence rate, and enhanced model reproducibility. As comparedto a traffic-aware baseline, TL-DIRP provides about 15% less violation ratio ofthe quality of service (QoS) for the worst slice service and 8.8% lessviolation on the average service QoS.</description><author>Tianlun Hu, Qi Liao, Qiang Liu, Georg Carle</author><pubDate>Tue, 20 Jun 2023 15:14:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11552v1</guid></item><item><title>Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Markov Chains</title><link>http://arxiv.org/abs/2306.09332v2</link><description>Score matching is an approach to learning probability distributionsparametrized up to a constant of proportionality (e.g. Energy-Based Models).The idea is to fit the score of the distribution, rather than the likelihood,thus avoiding the need to evaluate the constant of proportionality. Whilethere's a clear algorithmic benefit, the statistical "cost'' can be steep:recent work by Koehler et al. 2022 showed that for distributions that have poorisoperimetric properties (a large Poincar\'e or log-Sobolev constant), scorematching is substantially statistically less efficient than maximum likelihood.However, many natural realistic distributions, e.g. multimodal distributions assimple as a mixture of two Gaussians in one dimension -- have a poor Poincar\'econstant. In this paper, we show a close connection between the mixing time of anarbitrary Markov process with generator $\mathcal{L}$ and an appropriatelychosen generalized score matching loss that tries to fit $\frac{\mathcal{O}p}{p}$. If $\mathcal{L}$ corresponds to a Markov process corresponding to acontinuous version of simulated tempering, we show the correspondinggeneralized score matching loss is a Gaussian-convolution annealed scorematching loss, akin to the one proposed in Song and Ermon 2019. Moreover, weshow that if the distribution being learned is a finite mixture of Gaussians in$d$ dimensions with a shared covariance, the sample complexity of annealedscore matching is polynomial in the ambient dimension, the diameter the means,and the smallest and largest eigenvalues of the covariance -- obviating thePoincar\'e constant-based lower bounds of the basic score matching loss shownin Koehler et al. 2022. This is the first result characterizing the benefits ofannealing for score matching -- a crucial component in more sophisticatedscore-based approaches like Song and Ermon 2019.</description><author>Yilong Qin, Andrej Risteski</author><pubDate>Tue, 20 Jun 2023 15:12:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09332v2</guid></item><item><title>IMP-MARL: a Suite of Environments for Large-scale Infrastructure Management Planning via MARL</title><link>http://arxiv.org/abs/2306.11551v1</link><description>We introduce IMP-MARL, an open-source suite of multi-agent reinforcementlearning (MARL) environments for large-scale Infrastructure Management Planning(IMP), offering a platform for benchmarking the scalability of cooperative MARLmethods in real-world engineering applications. In IMP, a multi-componentengineering system is subject to a risk of failure due to its components'damage condition. Specifically, each agent plans inspections and repairs for aspecific system component, aiming to minimise maintenance costs whilecooperating to minimise system failure risk. With IMP-MARL, we release severalenvironments including one related to offshore wind structural systems, in aneffort to meet today's needs to improve management strategies to supportsustainable and reliable energy systems. Supported by IMP practical engineeringenvironments featuring up to 100 agents, we conduct a benchmark campaign, wherethe scalability and performance of state-of-the-art cooperative MARL methodsare compared against expert-based heuristic policies. The results reveal thatcentralised training with decentralised execution methods scale better with thenumber of agents than fully centralised or decentralised RL approaches, whilealso outperforming expert-based heuristic policies in most IMP environments.Based on our findings, we additionally outline remaining cooperation andscalability challenges that future MARL methods should still address. ThroughIMP-MARL, we encourage the implementation of new environments and the furtherdevelopment of MARL methods.</description><author>Pascal Leroy, Pablo G. Morato, Jonathan Pisane, Athanasios Kolios, Damien Ernst</author><pubDate>Tue, 20 Jun 2023 15:12:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11551v1</guid></item><item><title>A Low-Delay MAC for IoT Applications: Decentralized Optimal Scheduling of Queues without Explicit State Information Sharing</title><link>http://arxiv.org/abs/2105.11213v2</link><description>We consider a system of several collocated nodes sharing a time slottedwireless channel, and seek a MAC (medium access control) that (i) provides lowmean delay, (ii) has distributed control (i.e., there is no central scheduler),and (iii) does not require explicit exchange of state information or controlsignals. The design of such MAC protocols must keep in mind the need forcontention access at light traffic, and scheduled access in heavy traffic,leading to the long-standing interest in hybrid, adaptive MACs. Working in the discrete time setting, for the distributed MAC design, weconsider a practical information structure where each node has localinformation and some common information obtained from overhearing. In thissetting, "ZMAC" is an existing protocol that is hybrid and adaptive. Weapproach the problem via two steps (1) We show that it is sufficient for thepolicy to be "greedy" and "exhaustive". Limiting the policy to this classreduces the problem to obtaining a queue switching policy at queue emptinessinstants. (2) Formulating the delay optimal scheduling as a POMDP (partiallyobserved Markov decision process), we show that the optimal switching rule isStochastic Largest Queue (SLQ). Using this theory as the basis, we then develop a practical distributedscheduler, QZMAC, which is also tunable. We implement QZMAC on standardoff-the-shelf TelosB motes and also use simulations to compare QZMAC with thefull-knowledge centralized scheduler, and with ZMAC. We use our implementationto study the impact of false detection while overhearing the commoninformation, and the efficiency of QZMAC. Our simulation results show that themean delay with QZMAC is close that of the full-knowledge centralizedscheduler.</description><author>Avinash Mohan, Arpan Chattopadhyay, Shivam Vinayak Vatsa, Anurag Kumar</author><pubDate>Tue, 20 Jun 2023 15:03:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2105.11213v2</guid></item><item><title>Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events</title><link>http://arxiv.org/abs/2306.11547v1</link><description>Generative, pre-trained transformers (GPTs, a.k.a. "Foundation Models") havereshaped natural language processing (NLP) through their versatility in diversedownstream tasks. However, their potential extends far beyond NLP. This paperprovides a software utility to help realize this potential, extending theapplicability of GPTs to continuous-time sequences of complex events withinternal dependencies, such as medical record datasets. Despite theirpotential, the adoption of foundation models in these domains has been hamperedby the lack of suitable tools for model construction and evaluation. To bridgethis gap, we introduce Event Stream GPT (ESGPT), an open-source librarydesigned to streamline the end-to-end process for building GPTs forcontinuous-time event sequences. ESGPT allows users to (1) build flexible,foundation-model scale input datasets by specifying only a minimalconfiguration file, (2) leverage a Hugging Face compatible modeling API forGPTs over this modality that incorporates intra-event causal dependencystructures and autoregressive generation capabilities, and (3) evaluate modelsvia standardized processes that can assess few and even zero-shot performanceof pre-trained models on user-specified fine-tuning tasks.</description><author>Matthew B. A. McDermott, Bret Nestor, Peniel Argaw, Isaac Kohane</author><pubDate>Tue, 20 Jun 2023 15:01:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11547v1</guid></item><item><title>AugOp: Inject Transformation into Neural Operator</title><link>http://arxiv.org/abs/2211.12514v2</link><description>In this paper, we propose a simple and general approach to augment regularconvolution operator by injecting extra group-wise transformation duringtraining and recover it during inference. Extra transformation is carefullyselected to ensure it can be merged with regular convolution in each group andwill not change the topological structure of regular convolution duringinference. Compared with regular convolution operator, our approach (AugConv)can introduce larger learning capacity to improve model performance duringtraining but will not increase extra computational overhead for modeldeployment. Based on ResNet, we utilize AugConv to build convolutional neuralnetworks named AugResNet. Result on image classification dataset Cifar-10 showsthat AugResNet outperforms its baseline in terms of model performance.</description><author>Longqing Ye</author><pubDate>Tue, 20 Jun 2023 15:01:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.12514v2</guid></item><item><title>Trustworthy Multi-phase Liver Tumor Segmentation via Evidence-based Uncertainty</title><link>http://arxiv.org/abs/2305.05344v2</link><description>Multi-phase liver contrast-enhanced computed tomography (CECT) images conveythe complementary multi-phase information for liver tumor segmentation (LiTS),which are crucial to assist the diagnosis of liver cancer clinically. However,the performances of existing multi-phase liver tumor segmentation(MPLiTS)-based methods suffer from redundancy and weak interpretability, % ofthe fused result, resulting in the implicit unreliability of clinicalapplications. In this paper, we propose a novel trustworthy multi-phase livertumor segmentation (TMPLiTS), which is a unified framework jointly conductingsegmentation and uncertainty estimation. The trustworthy results could assistthe clinicians to make a reliable diagnosis. Specifically, Dempster-ShaferEvidence Theory (DST) is introduced to parameterize the segmentation anduncertainty as evidence following Dirichlet distribution. The reliability ofsegmentation results among multi-phase CECT images is quantified explicitly.Meanwhile, a multi-expert mixture scheme (MEMS) is proposed to fuse themulti-phase evidences, which can guarantee the effect of fusion procedure basedon theoretical analysis. Experimental results demonstrate the superiority ofTMPLiTS compared with the state-of-the-art methods. Meanwhile, the robustnessof TMPLiTS is verified, where the reliable performance can be guaranteedagainst the perturbations.</description><author>Chuanfei Hu, Tianyi Xia, Ying Cui, Quchen Zou, Yuancheng Wang, Wenbo Xiao, Shenghong Ju, Xinde Li</author><pubDate>Tue, 20 Jun 2023 15:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05344v2</guid></item><item><title>Bullying10K: A Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition</title><link>http://arxiv.org/abs/2306.11546v1</link><description>The prevalence of violence in daily life poses significant threats toindividuals' physical and mental well-being. Using surveillance cameras inpublic spaces has proven effective in proactively deterring and preventing suchincidents. However, concerns regarding privacy invasion have emerged due totheir widespread deployment. To address the problem, we leverage Dynamic VisionSensors (DVS) cameras to detect violent incidents and preserve privacy since itcaptures pixel brightness variations instead of static imagery. We introducethe Bullying10K dataset, encompassing various actions, complex movements, andocclusions from real-life scenarios. It provides three benchmarks forevaluating different tasks: action recognition, temporal action localization,and pose estimation. With 10,000 event segments, totaling 12 billion events and255 GB of data, Bullying10K contributes significantly by balancing violencedetection and personal privacy persevering. And it also poses a challenge tothe neuromorphic dataset. It will serve as a valuable resource for training anddeveloping privacy-protecting video systems. The Bullying10K opens newpossibilities for innovative approaches in these domains.</description><author>Yiting Dong, Yang Li, Dongcheng Zhao, Guobin Shen, Yi Zeng</author><pubDate>Tue, 20 Jun 2023 14:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11546v1</guid></item><item><title>Audio-Driven 3D Facial Animation from In-the-Wild Videos</title><link>http://arxiv.org/abs/2306.11541v1</link><description>Given an arbitrary audio clip, audio-driven 3D facial animation aims togenerate lifelike lip motions and facial expressions for a 3D head. Existingmethods typically rely on training their models using limited public 3Ddatasets that contain a restricted number of audio-3D scan pairs. Consequently,their generalization capability remains limited. In this paper, we propose anovel method that leverages in-the-wild 2D talking-head videos to train our 3Dfacial animation model. The abundance of easily accessible 2D talking-headvideos equips our model with a robust generalization capability. By combiningthese videos with existing 3D face reconstruction methods, our model excels ingenerating consistent and high-fidelity lip synchronization. Additionally, ourmodel proficiently captures the speaking styles of different individuals,allowing it to generate 3D talking-heads with distinct personal styles.Extensive qualitative and quantitative experimental results demonstrate thesuperiority of our method.</description><author>Liying Lu, Tianke Zhang, Yunfei Liu, Xuangeng Chu, Yu Li</author><pubDate>Tue, 20 Jun 2023 14:53:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11541v1</guid></item><item><title>Human or Machine: Reflections on Turing-Inspired Testing for the Everyday</title><link>http://arxiv.org/abs/2305.04312v3</link><description>In his seminal paper "Computing Machinery and Intelligence", Alan Turingintroduced the "imitation game" as part of exploring the concept of machineintelligence. The Turing Test has since been the subject of much analysis,debate, refinement and extension. Here we sidestep the question of whether aparticular machine can be labeled intelligent, or can be said to match humancapabilities in a given context. Instead, but inspired by Turing, we drawattention to the seemingly simpler challenge of determining whether one isinteracting with a human or with a machine, in the context of everyday life. Weare interested in reflecting upon the importance of this Human-or-Machinequestion and the use one may make of a reliable answer thereto. WhereasTuring's original test is widely considered to be more of a thought experiment,the Human-or-Machine question as discussed here has obvious practicalsignificance. And while the jury is still not in regarding the possibility ofmachines that can mimic human behavior with high fidelity in everyday contexts,we argue that near-term exploration of the issues raised here can contribute todevelopment methods for computerized systems, and may also improve ourunderstanding of human behavior in general.</description><author>David Harel, Assaf Marron</author><pubDate>Tue, 20 Jun 2023 14:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04312v3</guid></item><item><title>Improving visual image reconstruction from human brain activity using latent diffusion models via multiple decoded inputs</title><link>http://arxiv.org/abs/2306.11536v1</link><description>The integration of deep learning and neuroscience has been advancing rapidly,which has led to improvements in the analysis of brain activity and theunderstanding of deep learning models from a neuroscientific perspective. Thereconstruction of visual experience from human brain activity is an area thathas particularly benefited: the use of deep learning models trained on largeamounts of natural images has greatly improved its quality, and approaches thatcombine the diverse information contained in visual experiences haveproliferated rapidly in recent years. In this technical paper, by takingadvantage of the simple and generic framework that we proposed (Takagi andNishimoto, CVPR 2023), we examine the extent to which various additionaldecoding techniques affect the performance of visual experience reconstruction.Specifically, we combined our earlier work with the following three techniques:using decoded text from brain activity, nonlinear optimization for structuralimage reconstruction, and using decoded depth information from brain activity.We confirmed that these techniques contributed to improving accuracy over thebaseline. We also discuss what researchers should consider when performingvisual reconstruction using deep generative models trained on large datasets.Please check our webpage athttps://sites.google.com/view/stablediffusion-with-brain/. Code is alsoavailable at https://github.com/yu-takagi/StableDiffusionReconstruction.</description><author>Yu Takagi, Shinji Nishimoto</author><pubDate>Tue, 20 Jun 2023 14:48:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11536v1</guid></item><item><title>Studying Generalization on Memory-Based Methods in Continual Learning</title><link>http://arxiv.org/abs/2306.09890v2</link><description>One of the objectives of Continual Learning is to learn new conceptscontinually over a stream of experiences and at the same time avoidcatastrophic forgetting. To mitigate complete knowledge overwriting,memory-based methods store a percentage of previous data distributions to beused during training. Although these methods produce good results, few studieshave tested their out-of-distribution generalization properties, as well aswhether these methods overfit the replay memory. In this work, we show thatalthough these methods can help in traditional in-distribution generalization,they can strongly impair out-of-distribution generalization by learningspurious features and correlations. Using a controlled environment, the Synbolbenchmark generator (Lacoste et al., 2020), we demonstrate that this lack ofout-of-distribution generalization mainly occurs in the linear classifier.</description><author>Felipe del Rio, Julio Hurtado, Cristian Buc, Alvaro Soto, Vincenzo Lomonaco</author><pubDate>Tue, 20 Jun 2023 14:47:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09890v2</guid></item><item><title>Online List Labeling with Predictions</title><link>http://arxiv.org/abs/2305.10536v2</link><description>A growing line of work shows how learned predictions can be used to breakthrough worst-case barriers to improve the running time of an algorithm.However, incorporating predictions into data structures with strong theoreticalguarantees remains underdeveloped. This paper takes a step in this direction byshowing that predictions can be leveraged in the fundamental online listlabeling problem. In the problem, n items arrive over time and must be storedin sorted order in an array of size Theta(n). The array slot of an element isits label and the goal is to maintain sorted order while minimizing the totalnumber of elements moved (i.e., relabeled). We design a new list labeling datastructure and bound its performance in two models. In the worst-caselearning-augmented model, we give guarantees in terms of the error in thepredictions. Our data structure provides strong guarantees: it is optimal forany prediction error and guarantees the best-known worst-case bound even whenthe predictions are entirely erroneous. We also consider a stochastic errormodel and bound the performance in terms of the expectation and variance of theerror. Finally, the theoretical results are demonstrated empirically. Inparticular, we show that our data structure has strong performance on realtemporal data sets where predictions are constructed from elements that arrivedin the past, as is typically done in a practical use case.</description><author>Samuel McCauley, Benjamin Moseley, Aidin Niaparast, Shikha Singh</author><pubDate>Tue, 20 Jun 2023 14:45:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10536v2</guid></item><item><title>Evolutionary Strategy Guided Reinforcement Learning via MultiBuffer Communication</title><link>http://arxiv.org/abs/2306.11535v1</link><description>Evolutionary Algorithms and Deep Reinforcement Learning have bothsuccessfully solved control problems across a variety of domains. Recently,algorithms have been proposed which combine these two methods, aiming toleverage the strengths and mitigate the weaknesses of both approaches. In thispaper we introduce a new Evolutionary Reinforcement Learning model whichcombines a particular family of Evolutionary algorithm called EvolutionaryStrategies with the off-policy Deep Reinforcement Learning algorithm TD3. Theframework utilises a multi-buffer system instead of using a single sharedreplay buffer. The multi-buffer system allows for the Evolutionary Strategy tosearch freely in the search space of policies, without running the risk ofoverpopulating the replay buffer with poorly performing trajectories whichlimit the number of desirable policy behaviour examples thus negativelyimpacting the potential of the Deep Reinforcement Learning within the sharedframework. The proposed algorithm is demonstrated to perform competitively withcurrent Evolutionary Reinforcement Learning algorithms on MuJoCo control tasks,outperforming the well known state-of-the-art CEM-RL on 3 of the 4 environmentstested.</description><author>Adam Callaghan, Karl Mason, Patrick Mannion</author><pubDate>Tue, 20 Jun 2023 14:41:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11535v1</guid></item><item><title>Multi-aspect Multilingual and Cross-lingual Parliamentary Speech Analysis</title><link>http://arxiv.org/abs/2207.01054v2</link><description>Parliamentary and legislative debate transcripts provide informative insightinto elected politicians' opinions, positions, and policy preferences. They areinteresting for political and social sciences as well as linguistics andnatural language processing (NLP) research. While existing research studiedindividual parliaments, we apply advanced NLP methods to a joint andcomparative analysis of six national parliaments (Bulgarian, Czech, French,Slovene, Spanish, and United Kingdom) between 2017 and 2020. We analyzeemotions and sentiment in the transcripts from the ParlaMint dataset collectionand assess if the age, gender, and political orientation of speakers can bedetected from their speeches. The results show some commonalities and manysurprising differences among the analyzed countries.</description><author>Kristian Miok, Encarnacion Hidalgo-Tenorio, Petya Osenova, Miguel-Angel Benitez-Castro, Marko Robnik-Sikonja</author><pubDate>Tue, 20 Jun 2023 14:32:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.01054v2</guid></item><item><title>3D Keypoint Estimation Using Implicit Representation Learning</title><link>http://arxiv.org/abs/2306.11529v1</link><description>In this paper, we tackle the challenging problem of 3D keypoint estimation ofgeneral objects using a novel implicit representation. Previous works havedemonstrated promising results for keypoint prediction through directcoordinate regression or heatmap-based inference. However, these methods arecommonly studied for specific subjects, such as human bodies and faces, whichpossess fixed keypoint structures. They also suffer in several practicalscenarios where explicit or complete geometry is not given, including imagesand partial point clouds. Inspired by the recent success of advanced implicitrepresentation in reconstruction tasks, we explore the idea of using animplicit field to represent keypoints. Specifically, our key idea is employingspheres to represent 3D keypoints, thereby enabling the learnability of thecorresponding signed distance field. Explicit keypoints can be extractedsubsequently by our algorithm based on the Hough transform. Quantitative andqualitative evaluations also show the superiority of our representation interms of prediction accuracy.</description><author>Xiangyu Zhu, Dong Du, Haibin Huang, Chongyang Ma, Xiaoguang Han</author><pubDate>Tue, 20 Jun 2023 14:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11529v1</guid></item><item><title>TransRef: Multi-Scale Reference Embedding Transformer for Reference-Guided Image Inpainting</title><link>http://arxiv.org/abs/2306.11528v1</link><description>Image inpainting for completing complicated semantic environments and diversehole patterns of corrupted images is challenging even for state-of-the-artlearning-based inpainting methods trained on large-scale data. A referenceimage capturing the same scene of a corrupted image offers informative guidancefor completing the corrupted image as it shares similar texture and structurepriors to that of the holes of the corrupted image. In this work, we propose atransformer-based encoder-decoder network, named TransRef, for reference-guidedimage inpainting. Specifically, the guidance is conducted progressively througha reference embedding procedure, in which the referencing features aresubsequently aligned and fused with the features of the corrupted image. Forprecise utilization of the reference features for guidance, a reference-patchalignment (Ref-PA) module is proposed to align the patch features of thereference and corrupted images and harmonize their style differences, while areference-patch transformer (Ref-PT) module is proposed to refine the embeddedreference feature. Moreover, to facilitate the research of reference-guidedimage restoration tasks, we construct a publicly accessible benchmark datasetcontaining 50K pairs of input and reference images. Both quantitative andqualitative evaluations demonstrate the efficacy of the reference informationand the proposed method over the state-of-the-art methods in completing complexholes. Code and dataset can be accessed at https://github.com/Cameltr/TransRef.</description><author>Liang Liao, Taorong Liu, Delin Chen, Jing Xiao, Zheng Wang, Chia-Wen Lin</author><pubDate>Tue, 20 Jun 2023 14:31:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11528v1</guid></item><item><title>Understanding Contrastive Learning Through the Lens of Margins</title><link>http://arxiv.org/abs/2306.11526v1</link><description>Self-supervised learning, or SSL, holds the key to expanding the usage ofmachine learning in real-world tasks by alleviating heavy human supervision.Contrastive learning and its varieties have been SSL strategies in variousfields. We use margins as a stepping stone for understanding how contrastivelearning works at a deeper level and providing potential directions to improverepresentation learning. Through gradient analysis, we found that margins scalegradients in three different ways: emphasizing positive samples, de-emphasizingpositive samples when angles of positive samples are wide, and attenuating thediminishing gradients as the estimated probability approaches the targetprobability. We separately analyze each and provide possible directions forimproving SSL frameworks. Our experimental results demonstrate that theseproperties can contribute to acquiring better representations, which canenhance performance in both seen and unseen datasets.</description><author>Daniel Rho, TaeSoo Kim, Sooill Park, Jaehyun Park, JaeHan Park</author><pubDate>Tue, 20 Jun 2023 14:28:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11526v1</guid></item><item><title>Inhomogeneous graph trend filtering via a l2,0 cardinality penalty</title><link>http://arxiv.org/abs/2304.05223v2</link><description>We study estimation of piecewise smooth signals over a graph. We propose a$\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimatepiecewise smooth graph signals that exhibit inhomogeneous levels of smoothnessacross the nodes. We prove that the proposed GTF model is simultaneously ak-means clustering on the signal over the nodes and a minimum graph cut on theedges of the graph, where the clustering and the cut share the same assignmentmatrix. We propose two methods to solve the proposed GTF model: a spectraldecomposition method and a method based on simulated annealing. In theexperiment on synthetic and real-world datasets, we show that the proposed GTFmodel has a better performances compared with existing approaches on the tasksof denoising, support recovery and semi-supervised classification. We also showthat the proposed GTF model can be solved more efficiently than existing modelsfor the dataset with a large edge set.</description><author>Xiaoqing Huang, Andersen Ang, Kun Huang, Jie Zhang, Yijie Wang</author><pubDate>Tue, 20 Jun 2023 14:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05223v2</guid></item><item><title>Hallucination is the last thing you need</title><link>http://arxiv.org/abs/2306.11520v1</link><description>The legal profession necessitates a multidimensional approach that involvessynthesizing an in-depth comprehension of a legal issue with insightfulcommentary based on personal experience, combined with a comprehensiveunderstanding of pertinent legislation, regulation, and case law, in order todeliver an informed legal solution. The present offering with generative AIpresents major obstacles in replicating this, as current models struggle tointegrate and navigate such a complex interplay of understanding, experience,and fact-checking procedures. It is noteworthy that where generative AI outputsunderstanding and experience, which reflect the aggregate of various subjectiveviews on similar topics, this often deflects the model's attention from thecrucial legal facts, thereby resulting in hallucination. Hence, this paperdelves into the feasibility of three independent LLMs, each focused onunderstanding, experience, and facts, synthesising as one single ensemble modelto effectively counteract the current challenges posed by the existingmonolithic generative AI models. We introduce an idea of mutli-lengthtokenisation to protect key information assets like common law judgements, andfinally we interrogate the most advanced publicly available models for legalhallucination, with some interesting results.</description><author>Shawn Curran, Sam Lansley, Oliver Bethell</author><pubDate>Tue, 20 Jun 2023 14:14:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11520v1</guid></item><item><title>One model to rule them all: ranking Slovene summarizers</title><link>http://arxiv.org/abs/2306.11518v1</link><description>Text summarization is an essential task in natural language processing, andresearchers have developed various approaches over the years, ranging fromrule-based systems to neural networks. However, there is no single model orapproach that performs well on every type of text. We propose a system thatrecommends the most suitable summarization model for a given text. The proposedsystem employs a fully connected neural network that analyzes the input contentand predicts which summarizer should score the best in terms of ROUGE score fora given input. The meta-model selects among four different summarizationmodels, developed for the Slovene language, using different properties of theinput, in particular its Doc2Vec document representation. The four Slovenesummarization models deal with different challenges associated with textsummarization in a less-resourced language. We evaluate the proposed SloMetaSummodel performance automatically and parts of it manually. The results show thatthe system successfully automates the step of manually selecting the bestmodel.</description><author>Aleš Žagar, Marko Robnik-Šikonja</author><pubDate>Tue, 20 Jun 2023 14:12:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11518v1</guid></item><item><title>Mirror Sinkhorn: Fast Online Optimization on Transport Polytopes</title><link>http://arxiv.org/abs/2211.10420v3</link><description>Optimal transport is an important tool in machine learning, allowing tocapture geometric properties of the data through a linear program on transportpolytopes. We present a single-loop optimization algorithm for minimizinggeneral convex objectives on these domains, utilizing the principles ofSinkhorn matrix scaling and mirror descent. The proposed algorithm is robust tonoise, and can be used in an online setting. We provide theoretical guaranteesfor convex objectives and experimental results showcasing it effectiveness onboth synthetic and real-world data.</description><author>Marin Ballu, Quentin Berthet</author><pubDate>Tue, 20 Jun 2023 14:01:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10420v3</guid></item><item><title>Pushing the Limits of 3D Shape Generation at Scale</title><link>http://arxiv.org/abs/2306.11510v1</link><description>We present a significant breakthrough in 3D shape generation by scaling it tounprecedented dimensions. Through the adaptation of the Auto-Regressive modeland the utilization of large language models, we have developed a remarkablemodel with an astounding 3.6 billion trainable parameters, establishing it asthe largest 3D shape generation model to date, named Argus-3D. Our approachaddresses the limitations of existing methods by enhancing the quality anddiversity of generated 3D shapes. To tackle the challenges of high-resolution3D shape generation, our model incorporates tri-plane features as latentrepresentations, effectively reducing computational complexity. Additionally,we introduce a discrete codebook for efficient quantization of theserepresentations. Leveraging the power of transformers, we enable multi-modalconditional generation, facilitating the production of diverse and visuallyimpressive 3D shapes. To train our expansive model, we leverage an ensemble ofpublicly-available 3D datasets, consisting of a comprehensive collection ofapproximately 900,000 objects from renowned repositories such as ModelNet40,ShapeNet, Pix3D, 3D-Future, and Objaverse. This diverse dataset empowers ourmodel to learn from a wide range of object variations, bolstering its abilityto generate high-quality and diverse 3D shapes. Extensive experimentationdemonstrate the remarkable efficacy of our approach in significantly improvingthe visual quality of generated 3D shapes. By pushing the boundaries of 3Dgeneration, introducing novel methods for latent representation learning, andharnessing the power of transformers for multi-modal conditional generation,our contributions pave the way for substantial advancements in the field. Ourwork unlocks new possibilities for applications in gaming, virtual reality,product design, and other domains that demand high-quality and diverse 3Dobjects.</description><author>Wang Yu, Xuelin Qian, Jingyang Huo, Tiejun Huang, Bo Zhao, Yanwei Fu</author><pubDate>Tue, 20 Jun 2023 14:01:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11510v1</guid></item><item><title>Implicit neural representation with physics-informed neural networks for the reconstruction of the early part of room impulse responses</title><link>http://arxiv.org/abs/2306.11509v1</link><description>Recently deep learning and machine learning approaches have been widelyemployed for various applications in acoustics. Nonetheless, in the area ofsound field processing and reconstruction classic methods based on thesolutions of wave equation are still widespread. Recently, physics-informedneural networks have been proposed as a deep learning paradigm for solvingpartial differential equations which govern physical phenomena, bridging thegap between purely data-driven and model based methods. Here, we exploitphysics-informed neural networks to reconstruct the early part of missing roomimpulse responses in an uniform linear array. This methodology allows us toexploit the underlying law of acoustics, i.e., the wave equation, forcing theneural network to generate physically meaningful solutions given only a limitednumber of data points. The results on real measurements show that the proposedmodel achieves accurate reconstruction and performance in line with respect tostate-of-the-art deep-learning and compress sensing techniques whilemaintaining a lightweight architecture.</description><author>Mirco Pezzoli, Fabio Antonacci, Augusto Sarti</author><pubDate>Tue, 20 Jun 2023 14:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11509v1</guid></item><item><title>TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models</title><link>http://arxiv.org/abs/2306.11507v1</link><description>Large Language Models (LLMs) such as ChatGPT, have gained significantattention due to their impressive natural language processing capabilities. Itis crucial to prioritize human-centered principles when utilizing these models.Safeguarding the ethical and moral compliance of LLMs is of utmost importance.However, individual ethical issues have not been well studied on the latestLLMs. Therefore, this study aims to address these gaps by introducing a newbenchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs inthree crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPTexamines toxicity in language models by employing toxic prompt templatesderived from social norms. It then quantifies the extent of bias in models bymeasuring quantifiable toxicity values across different groups. Lastly,TrustGPT assesses the value of conversation generation models from both activevalue-alignment and passive value-alignment tasks. Through the implementationof TrustGPT, this research aims to enhance our understanding of the performanceof conversation generation models and promote the development of languagemodels that are more ethical and socially responsible.</description><author>Yue Huang, Qihui Zhang, Philip S. Y, Lichao Sun</author><pubDate>Tue, 20 Jun 2023 13:53:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11507v1</guid></item><item><title>Align, Adapt and Inject: Sound-guided Unified Image Generation</title><link>http://arxiv.org/abs/2306.11504v1</link><description>Text-guided image generation has witnessed unprecedented progress due to thedevelopment of diffusion models. Beyond text and image, sound is a vitalelement within the sphere of human perception, offering vivid representationsand naturally coinciding with corresponding scenes. Taking advantage of soundtherefore presents a promising avenue for exploration within image generationresearch. However, the relationship between audio and image supervision remainssignificantly underdeveloped, and the scarcity of related, high-qualitydatasets brings further obstacles. In this paper, we propose a unifiedframework 'Align, Adapt, and Inject' (AAI) for sound-guided image generation,editing, and stylization. In particular, our method adapts input sound into asound token, like an ordinary word, which can plug and play with existingpowerful diffusion-based Text-to-Image (T2I) models. Specifically, we firsttrain a multi-modal encoder to align audio representation with the pre-trainedtextual manifold and visual manifold, respectively. Then, we propose the audioadapter to adapt audio representation into an audio token enriched withspecific semantics, which can be injected into a frozen T2I model flexibly. Inthis way, we are able to extract the dynamic information of varied sounds,while utilizing the formidable capability of existing T2I models to facilitatesound-guided image generation, editing, and stylization in a convenient andcost-effective manner. The experiment results confirm that our proposed AAIoutperforms other text and sound-guided state-of-the-art methods. And ouraligned multi-modal encoder is also competitive with other approaches in theaudio-visual retrieval and audio-text retrieval tasks.</description><author>Yue Yang, Kaipeng Zhang, Yuying Ge, Wenqi Shao, Zeyue Xue, Yu Qiao, Ping Luo</author><pubDate>Tue, 20 Jun 2023 13:50:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11504v1</guid></item></channel></rss>