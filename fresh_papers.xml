<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 28 Feb 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning</title><link>http://arxiv.org/abs/2402.17768v1</link><description>A common failure mode for policies trained with imitation is compoundingexecution errors at test time. When the learned policy encounters states thatwere not present in the expert demonstrations, the policy fails, leading todegenerate behavior. The Dataset Aggregation, or DAgger approach to thisproblem simply collects more data to cover these failure states. However, inpractice, this is often prohibitively expensive. In this work, we proposeDiffusion Meets DAgger (DMD), a method to reap the benefits of DAgger withoutthe cost for eye-in-hand imitation learning problems. Instead of collecting newsamples to cover out-of-distribution states, DMD uses recent advances indiffusion models to create these samples with diffusion models. This leads torobust performance from few demonstrations. In experiments conducted fornon-prehensile pushing on a Franka Research 3, we show that DMD can achieve asuccess rate of 80% with as few as 8 expert demonstrations, where naivebehavior cloning reaches only 20%. DMD also outperform competing NeRF-basedaugmentation schemes by 50%.</description><author>Xiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh Gupta</author><pubDate>Tue, 27 Feb 2024 18:59:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17768v1</guid></item><item><title>Stochastic positional embeddings improve masked image modeling</title><link>http://arxiv.org/abs/2308.00566v2</link><description>Masked Image Modeling (MIM) is a promising self-supervised learning approachthat enables learning from unlabeled images. Despite its recent success,learning good representations through MIM remains challenging because itrequires predicting the right semantic content in accurate locations. Forexample, given an incomplete picture of a dog, we can guess that there is atail, but we cannot determine its exact location. In this work, we propose toincorporate location uncertainty into MIM by using stochastic positionalembeddings (StoP). Specifically, we condition the model on stochastic maskedtoken positions drawn from a Gaussian distribution. StoP reduces overfitting tolocation features and guides the model toward learning features that are morerobust to location uncertainties. Quantitatively, StoP improves downstream MIMperformance on a variety of downstream tasks, including $+1.7\%$ on ImageNetlinear probing using ViT-B, and $+2.5\%$ for ViT-H using $1\%$ of the data.</description><author>Amir Bar, Florian Bordes, Assaf Shocher, Mahmoud Assran, Pascal Vincent, Nicolas Ballas, Trevor Darrell, Amir Globerson, Yann LeCun</author><pubDate>Tue, 27 Feb 2024 18:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00566v2</guid></item><item><title>Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator</title><link>http://arxiv.org/abs/2402.17767v1</link><description>Pulling open cabinets and drawers presents many difficult technicalchallenges in perception (inferring articulation parameters for objects fromonboard sensors), planning (producing motion plans that conform to tight taskconstraints), and control (making and maintaining contact while applying forceson the environment). In this work, we build an end-to-end system that enables acommodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers indiverse previously unseen real world environments. We conduct 4 days of realworld testing of this system spanning 31 different objects from across 13different real world environments. Our system achieves a success rate of 61% onopening novel cabinets and drawers in unseen environments zero-shot. Ananalysis of the failure modes suggests that errors in perception are the mostsignificant challenge for our system. We will open source code and models forothers to replicate and build upon our system.</description><author>Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta</author><pubDate>Tue, 27 Feb 2024 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17767v1</guid></item><item><title>ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</title><link>http://arxiv.org/abs/2402.17766v1</link><description>This paper presents ShapeLLM, the first 3D Multimodal Large Language Model(LLM) designed for embodied interaction, exploring a universal 3D objectunderstanding with 3D point clouds and languages. ShapeLLM is built upon animproved 3D encoder by extending ReCon to ReCon++ that benefits from multi-viewimage distillation for enhanced geometry understanding. By utilizing ReCon++ asthe 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructedinstruction-following data and tested on our newly human-curated evaluationbenchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performancein 3D geometry understanding and language-unified 3D interaction tasks, such asembodied visual grounding.</description><author>Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, Kaisheng Ma</author><pubDate>Tue, 27 Feb 2024 18:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17766v1</guid></item><item><title>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</title><link>http://arxiv.org/abs/2402.17764v1</link><description>Recent research, such as BitNet, is paving the way for a new era of 1-bitLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,namely BitNet b1.58, in which every single parameter (or weight) of the LLM isternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)Transformer LLM with the same model size and training tokens in terms of bothperplexity and end-task performance, while being significantly morecost-effective in terms of latency, memory, throughput, and energy consumption.More profoundly, the 1.58-bit LLM defines a new scaling law and recipe fortraining new generations of LLMs that are both high-performance andcost-effective. Furthermore, it enables a new computation paradigm and opensthe door for designing specific hardware optimized for 1-bit LLMs.</description><author>Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei</author><pubDate>Tue, 27 Feb 2024 18:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17764v1</guid></item><item><title>Accelerating Cutting-Plane Algorithms via Reinforcement Learning Surrogates</title><link>http://arxiv.org/abs/2307.08816v2</link><description>Discrete optimization belongs to the set of $\mathcal{NP}$-hard problems,spanning fields such as mixed-integer programming and combinatorialoptimization. A current standard approach to solving convex discreteoptimization problems is the use of cutting-plane algorithms, which reachoptimal solutions by iteratively adding inequalities known as \textit{cuts} torefine a feasible set. Despite the existence of a number of general-purposecut-generating algorithms, large-scale discrete optimization problems continueto suffer from intractability. In this work, we propose a method foraccelerating cutting-plane algorithms via reinforcement learning. Our approachuses learned policies as surrogates for $\mathcal{NP}$-hard elements of the cutgenerating procedure in a way that (i) accelerates convergence, and (ii)retains guarantees of optimality. We apply our method on two types of problemswhere cutting-plane algorithms are commonly used: stochastic optimization, andmixed-integer quadratic programming. We observe the benefits of our method whenapplied to Benders decomposition (stochastic optimization) and iterative lossapproximation (quadratic programming), achieving up to $45\%$ faster averageconvergence when compared to modern alternative algorithms.</description><author>Kyle Mana, Fernando Acero, Stephen Mak, Parisa Zehtabi, Michael Cashmore, Daniele Magazzeni, Manuela Veloso</author><pubDate>Tue, 27 Feb 2024 18:55:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08816v2</guid></item><item><title>Massive Activations in Large Language Models</title><link>http://arxiv.org/abs/2402.17762v1</link><description>We observe an empirical phenomenon in Large Language Models (LLMs) -- veryfew activations exhibit significantly larger values than others (e.g., 100,000times larger). We call them massive activations. First, we demonstrate thewidespread existence of massive activations across various LLMs andcharacterize their locations. Second, we find their values largely stayconstant regardless of the input, and they function as indispensable bias termsin LLMs. Third, these massive activations lead to the concentration ofattention probabilities to their corresponding tokens, and further, implicitbias terms in the self-attention output. Last, we also study massiveactivations in Vision Transformers.</description><author>Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu</author><pubDate>Tue, 27 Feb 2024 18:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17762v1</guid></item><item><title>Feedback Efficient Online Fine-Tuning of Diffusion Models</title><link>http://arxiv.org/abs/2402.16359v2</link><description>Diffusion models excel at modeling complex data distributions, includingthose of images, proteins, and small molecules. However, in many cases, ourgoal is to model parts of the distribution that maximize certain properties:for example, we may want to generate images with high aesthetic quality, ormolecules with high bioactivity. It is natural to frame this as a reinforcementlearning (RL) problem, in which the objective is to fine-tune a diffusion modelto maximize a reward function that corresponds to some property. Even withaccess to online queries of the ground-truth reward function, efficientlydiscovering high-reward samples can be challenging: they might have a lowprobability in the initial distribution, and there might be many infeasiblesamples that do not even have a well-defined reward (e.g., unnatural images orphysically impossible molecules). In this work, we propose a novelreinforcement learning procedure that efficiently explores on the manifold offeasible samples. We present a theoretical analysis providing a regretguarantee, as well as empirical validation across three domains: images,biological sequences, and molecules.</description><author>Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Sergey Levine, Tommaso Biancalani</author><pubDate>Tue, 27 Feb 2024 18:54:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16359v2</guid></item><item><title>Learning to Program Variational Quantum Circuits with Fast Weights</title><link>http://arxiv.org/abs/2402.17760v1</link><description>Quantum Machine Learning (QML) has surfaced as a pioneering frameworkaddressing sequential control tasks and time-series modeling. It hasdemonstrated empirical quantum advantages notably within domains such asReinforcement Learning (RL) and time-series prediction. A significantadvancement lies in Quantum Recurrent Neural Networks (QRNNs), specificallytailored for memory-intensive tasks encompassing partially observableenvironments and non-linear time-series prediction. Nevertheless, QRNN-basedmodels encounter challenges, notably prolonged training duration stemming fromthe necessity to compute quantum gradients using backpropagation-through-time(BPTT). This predicament exacerbates when executing the complete model onquantum devices, primarily due to the substantial demand for circuit evaluationarising from the parameter-shift rule. This paper introduces the Quantum FastWeight Programmers (QFWP) as a solution to the temporal or sequential learningchallenge. The QFWP leverages a classical neural network (referred to as the'slow programmer') functioning as a quantum programmer to swiftly modify theparameters of a variational quantum circuit (termed the 'fast programmer').Instead of completely overwriting the fast programmer at each time-step, theslow programmer generates parameter changes or updates for the quantum circuitparameters. This approach enables the fast programmer to incorporate pastobservations or information. Notably, the proposed QFWP model achieves learningof temporal dependencies without necessitating the use of quantum recurrentneural networks. Numerical simulations conducted in this study showcase theefficacy of the proposed QFWP model in both time-series prediction and RLtasks. The model exhibits performance levels either comparable to or surpassingthose achieved by QLSTM-based models.</description><author>Samuel Yen-Chi Chen</author><pubDate>Tue, 27 Feb 2024 18:53:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17760v1</guid></item><item><title>Towards Optimal Learning of Language Models</title><link>http://arxiv.org/abs/2402.17759v1</link><description>This work studies the general principles of improving the learning oflanguage models (LMs), which aims at reducing the necessary training steps forachieving superior performance. Specifically, we present a theory for theoptimal learning of LMs. We first propose an objective that optimizes LMlearning by maximizing the data compression ratio in an"LM-training-as-lossless-compression" view. Then, we derive a theorem, namedLearning Law, to reveal the properties of the dynamics in the optimal learningprocess under our objective. The theorem is then validated by experiments on alinear classification and a real-world language modeling task. Finally, weempirically verify that the optimal learning of LMs essentially stems from theimprovement of the coefficients in the scaling law of LMs, indicating greatpromise and significance for designing practical learning acceleration methods.Our code can be found at https://aka.ms/LearningLaw.</description><author>Yuxian Gu, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, Furu Wei</author><pubDate>Tue, 27 Feb 2024 18:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17759v1</guid></item><item><title>ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living</title><link>http://arxiv.org/abs/2402.17758v1</link><description>Hand-Object Interactions (HOIs) are conditioned on spatial and temporalcontexts like surrounding objects, pre- vious actions, and future intents (forexample, grasping and handover actions vary greatly based on objects proximityand trajectory obstruction). However, existing datasets for 4D HOI (3D HOI overtime) are limited to one subject inter- acting with one object only. Thisrestricts the generalization of learning-based HOI methods trained on thosedatasets. We introduce ADL4D, a dataset of up to two subjects inter- actingwith different sets of objects performing Activities of Daily Living (ADL) likebreakfast or lunch preparation ac- tivities. The transition between multipleobjects to complete a certain task over time introduces a unique contextlacking in existing datasets. Our dataset consists of 75 sequences with a totalof 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained actionannotations. We develop an automatic system for multi-view multi-hand 3D posean- notation capable of tracking hand poses over time. We inte- grate and testit against publicly available datasets. Finally, we evaluate our dataset on thetasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).</description><author>Marsil Zakour, Partha Pratim Nath, Ludwig Lohmer, Emre Faik Gökçe, Martin Piccolrovazzi, Constantin Patsch, Yuankai Wu, Rahul Chaudhari, Eckehard Steinbach</author><pubDate>Tue, 27 Feb 2024 18:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17758v1</guid></item><item><title>Robustly Learning Single-Index Models via Alignment Sharpness</title><link>http://arxiv.org/abs/2402.17756v1</link><description>We study the problem of learning Single-Index Models under the $L_2^2$ lossin the agnostic model. We give an efficient learning algorithm, achieving aconstant factor approximation to the optimal loss, that succeeds under a rangeof distributions (including log-concave distributions) and a broad class ofmonotone and Lipschitz link functions. This is the first efficient constantfactor approximate agnostic learner, even for Gaussian data and for anynontrivial class of link functions. Prior work for the case of unknown linkfunction either works in the realizable setting or does not attain constantfactor approximation. The main technical ingredient enabling our algorithm andanalysis is a novel notion of a local error bound in optimization that we termalignment sharpness and that may be of broader interest.</description><author>Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, Jelena Diakonikolas</author><pubDate>Tue, 27 Feb 2024 18:48:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17756v1</guid></item><item><title>Ricci flow-guided autoencoders in learning time-dependent dynamics</title><link>http://arxiv.org/abs/2401.14591v4</link><description>We present a manifold-based autoencoder method for learning nonlineardynamics in time, notably partial differential equations (PDEs), in which themanifold latent space evolves according to Ricci flow. This can be accomplishedby simulating Ricci flow in a physics-informed setting, and manifold quantitiescan be matched so that Ricci flow is empirically achieved. With ourmethodology, the manifold is learned as part of the training procedure, soideal geometries may be discerned, while the evolution simultaneously induces amore accommodating latent representation over static methods. We present ourmethod on a range of numerical experiments consisting of PDEs that encompassdesirable characteristics such as periodicity and randomness, remarking erroron in-distribution and extrapolation scenarios.</description><author>Andrew Gracyk</author><pubDate>Tue, 27 Feb 2024 18:46:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14591v4</guid></item><item><title>Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model</title><link>http://arxiv.org/abs/2402.14035v2</link><description>Recent advancements in foundation models have yielded impressive performanceacross a wide range of tasks. Meanwhile, for specific applications,practitioners have been developing specialized application models. To enjoy thebenefits of both kinds of models, one natural path is to transfer the knowledgein foundation models into specialized application models, which are generallymore efficient for serving. Techniques from knowledge distillation may beapplied here, where the application model learns to mimic the foundation model.However, specialized application models and foundation models have substantialgaps in capacity, employing distinct architectures, using different inputfeatures from different modalities, and being optimized on differentdistributions. These differences in model characteristics lead to significantchallenges for distillation methods. In this work, we propose creating ateaching committee comprising both foundation model teachers and complementaryteachers. Complementary teachers possess model characteristics akin to thestudent's, aiming to bridge the gap between the foundation model andspecialized application models for a smoother knowledge transfer. Further, toaccommodate the dissimilarity among the teachers in the committee, we introduceDiverseDistill, which allows the student to understand the expertise of eachteacher and extract task knowledge. Our evaluations demonstrate that addingcomplementary teachers enhances student performance. Finally, DiverseDistillconsistently outperforms baseline distillation methods, regardless of theteacher choices, resulting in significantly improved student performance.</description><author>Zichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe Zhao</author><pubDate>Tue, 27 Feb 2024 18:44:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14035v2</guid></item><item><title>Dynamic fairness-aware recommendation through multi-agent social choice</title><link>http://arxiv.org/abs/2303.00968v3</link><description>Algorithmic fairness in the context of personalized recommendation presentssignificantly different challenges to those commonly encountered inclassification tasks. Researchers studying classification have generallyconsidered fairness to be a matter of achieving equality of outcomes between aprotected and unprotected group, and built algorithmic interventions on thisbasis. We argue that fairness in real-world application settings in general,and especially in the context of personalized recommendation, is much morecomplex and multi-faceted, requiring a more general approach. We propose amodel to formalize multistakeholder fairness in recommender systems as a twostage social choice problem. In particular, we express recommendation fairnessas a novel combination of an allocation and an aggregation problem, whichintegrate both fairness concerns and personalized recommendation provisions,and derive new recommendation techniques based on this formulation. Simulationsdemonstrate the ability of the framework to integrate multiple fairnessconcerns in a dynamic way.</description><author>Amanda Aird, Paresha Farastu, Joshua Sun, Elena Štefancová, Cassidy All, Amy Voida, Nicholas Mattei, Robin Burke</author><pubDate>Tue, 27 Feb 2024 18:44:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.00968v3</guid></item><item><title>Preference Ranking Optimization for Human Alignment</title><link>http://arxiv.org/abs/2306.17492v2</link><description>Large language models (LLMs) often contain misleading content, emphasizingthe need to align them with human values to ensure secure AI systems.Reinforcement learning from human feedback (RLHF) has been employed to achievethis alignment. However, it encompasses two main drawbacks: (1) RLHF exhibitscomplexity, instability, and sensitivity to hyperparameters in contrast to SFT.(2) Despite massive trial-and-error, multiple sampling is reduced to pair-wisecontrast, thus lacking contrasts from a macro perspective. In this paper, wepropose Preference Ranking Optimization (PRO) as an efficient SFT algorithm todirectly fine-tune LLMs for human alignment. PRO extends the pair-wise contrastto accommodate preference rankings of any length. By iteratively contrastingcandidates, PRO instructs the LLM to prioritize the best response whileprogressively ranking the rest responses. In this manner, PRO effectivelytransforms human alignment into aligning the probability ranking of n responsesgenerated by LLM with the preference ranking of humans towards these responses.Experiments have shown that PRO outperforms baseline algorithms, achievingcomparable results to ChatGPT and human responses through automatic-based,reward-based, GPT-4, and human evaluations.</description><author>Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, Houfeng Wang</author><pubDate>Tue, 27 Feb 2024 18:42:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17492v2</guid></item><item><title>Evaluating Very Long-Term Conversational Memory of LLM Agents</title><link>http://arxiv.org/abs/2402.17753v1</link><description>Existing works on long-term open-domain dialogues focus on evaluating modelresponses within contexts spanning no more than five chat sessions. Despiteadvancements in long-context large language models (LLMs) and retrievalaugmented generation (RAG) techniques, their efficacy in very long-termdialogues remains unexplored. To address this research gap, we introduce amachine-human pipeline to generate high-quality, very long-term dialogues byleveraging LLM-based agent architectures and grounding their dialogues onpersonas and temporal event graphs. Moreover, we equip each agent with thecapability of sharing and reacting to images. The generated conversations areverified and edited by human annotators for long-range consistency andgrounding to the event graphs. Using this pipeline, we collect LoCoMo, adataset of very long-term conversations, each encompassing 300 turns and 9Ktokens on avg., over up to 35 sessions. Based on LoCoMo, we present acomprehensive evaluation benchmark to measure long-term memory in models,encompassing question answering, event summarization, and multi-modal dialoguegeneration tasks. Our experimental results indicate that LLMs exhibitchallenges in understanding lengthy conversations and comprehending long-rangetemporal and causal dynamics within dialogues. Employing strategies likelong-context LLMs or RAG can offer improvements but these models stillsubstantially lag behind human performance.</description><author>Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang</author><pubDate>Tue, 27 Feb 2024 18:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17753v1</guid></item><item><title>FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using Segment Anything Model with Fair Error-Bound Scaling</title><link>http://arxiv.org/abs/2311.02189v3</link><description>Fairness in artificial intelligence models has gained significantly moreattention in recent years, especially in the area of medicine, as fairness inmedical models is critical to people's well-being and lives. High-qualitymedical fairness datasets are needed to promote fairness learning research.Existing medical fairness datasets are all for classification tasks, and nofairness datasets are available for medical segmentation, while medicalsegmentation is an equally important clinical task as classifications, whichcan provide detailed spatial information on organ abnormalities ready to beassessed by clinicians. In this paper, we propose the first fairness datasetfor medical segmentation named Harvard-FairSeg with 10,000 subject samples. Inaddition, we propose a fair error-bound scaling approach to reweight the lossfunction with the upper error-bound in each identity group, using the segmentanything model (SAM). We anticipate that the segmentation performance equitycan be improved by explicitly tackling the hard cases with high training errorsin each identity group. To facilitate fair comparisons, we utilize a novelequity-scaled segmentation performance metric to compare segmentation metricsin the context of fairness, such as the equity-scaled Dice coefficient. Throughcomprehensive experiments, we demonstrate that our fair error-bound scalingapproach either has superior or comparable fairness performance to thestate-of-the-art fairness learning models. The dataset and code are publiclyaccessible via https://ophai.hms.harvard.edu/harvard-fairseg10k.</description><author>Yu Tian, Min Shi, Yan Luo, Ava Kouhana, Tobias Elze, Mengyu Wang</author><pubDate>Tue, 27 Feb 2024 18:38:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02189v3</guid></item><item><title>Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation</title><link>http://arxiv.org/abs/2402.17750v1</link><description>On-chip photonic processors for neural networks have potential benefits inboth speed and energy efficiency but have not yet reached the scale at whichthey can outperform electronic processors. The dominant paradigm for designingon-chip photonics is to make networks of relatively bulky discrete componentsconnected by one-dimensional waveguides. A far more compact alternative is toavoid explicitly defining any components and instead sculpt the continuoussubstrate of the photonic processor to directly perform the computation usingwaves freely propagating in two dimensions. We propose and demonstrate a devicewhose refractive index as a function of space, $n(x,z)$, can be rapidlyreprogrammed, allowing arbitrary control over the wave propagation in thedevice. Our device, a 2D-programmable waveguide, combines photoconductive gainwith the electro-optic effect to achieve massively parallel modulation of therefractive index of a slab waveguide, with an index modulation depth of$10^{-3}$ and approximately $10^4$ programmable degrees of freedom. We used aprototype device with a functional area of $12\,\text{mm}^2$ to performneural-network inference with up to 49-dimensional input vectors in a singlepass, achieving 96% accuracy on vowel classification and 86% accuracy on $7\times 7$-pixel MNIST handwritten-digit classification. This is a scale beyondthat of previous photonic chips relying on discrete components, illustratingthe benefit of the continuous-waves paradigm. In principle, with large enoughchip area, the reprogrammability of the device's refractive index distributionenables the reconfigurable realization of any passive, linear photonic circuitor device. This promises the development of more compact and versatile photonicsystems for a wide range of applications, including optical processing, smartsensing, spectroscopy, and optical communications.</description><author>Tatsuhiro Onodera, Martin M. Stein, Benjamin A. Ash, Mandar M. Sohoni, Melissa Bosch, Ryotatsu Yanagimoto, Marc Jankowski, Timothy P. McKenna, Tianyu Wang, Gennady Shvets, Maxim R. Shcherbakov, Logan G. Wright, Peter L. McMahon</author><pubDate>Tue, 27 Feb 2024 18:37:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17750v1</guid></item><item><title>When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning</title><link>http://arxiv.org/abs/2402.17747v1</link><description>Past analyses of reinforcement learning from human feedback (RLHF) assumethat the human fully observes the environment. What happens when human feedbackis based only on partial observations? We formally define two failure cases:deception and overjustification. Modeling the human as Boltzmann-rationalw.r.t. a belief over trajectories, we prove conditions under which RLHF isguaranteed to result in policies that deceptively inflate their performance,overjustify their behavior to make an impression, or both. To help addressthese issues, we mathematically characterize how partial observability of theenvironment translates into (lack of) ambiguity in the learned return function.In some cases, accounting for partial observability makes it theoreticallypossible to recover the return function and thus the optimal policy, while inother cases, there is irreducible ambiguity. We caution against blindlyapplying RLHF in partially observable settings and propose research directionsto help tackle these challenges.</description><author>Leon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner, Scott Emmons</author><pubDate>Tue, 27 Feb 2024 18:32:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17747v1</guid></item><item><title>Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional layers for 3D abdominal organ segmentation</title><link>http://arxiv.org/abs/2402.16825v2</link><description>Filter-decomposition-based group equivariant convolutional neural networksshow promising stability and data efficiency for 3D image feature extraction.However, the existing filter-decomposition-based 3D group equivariant neuralnetworks rely on parameter-sharing designs and are mostly limited to rotationtransform groups, where the chosen spherical harmonic filter bases consideronly angular orthogonality. These limitations hamper its application to deepneural network architectures for medical image segmentation. To address theseissues, this paper describes a non-parameter-sharing affine group equivariantneural network for 3D medical image segmentation based on an adaptiveaggregation of Monte Carlo augmented spherical Fourier Bessel filter bases. Theefficiency and flexibility of the adopted non-parameter strategy enable for thefirst time an efficient implementation of 3D affine group equivariantconvolutional neural networks for volumetric data. The introduced sphericalBessel Fourier filter basis combines both angular and radial orthogonality forbetter feature extraction. The 3D image segmentation experiments on twoabdominal image sets, BTCV and the NIH Pancreas datasets, show that theproposed methods excel the state-of-the-art 3D neural networks with hightraining stability and data efficiency. The code will be available athttps://github.com/ZhaoWenzhao/WVMS.</description><author>Wenzhao Zhao, Steffen Albert, Barbara D. Wichtmann, Angelika Maurer, Ulrike Attenberger, Frank G. Zöllner, Jürgen Hesser</author><pubDate>Tue, 27 Feb 2024 18:30:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16825v2</guid></item><item><title>Solving PDEs on Unknown Manifolds with Machine Learning</title><link>http://arxiv.org/abs/2106.06682v4</link><description>This paper proposes a mesh-free computational framework and machine learningtheory for solving elliptic PDEs on unknown manifolds, identified with pointclouds, based on diffusion maps (DM) and deep learning. The PDE solver isformulated as a supervised learning task to solve a least-squares regressionproblem that imposes an algebraic equation approximating a PDE (and boundaryconditions if applicable). This algebraic equation involves a graph-Laplaciantype matrix obtained via DM asymptotic expansion, which is a consistentestimator of second-order elliptic differential operators. The resultingnumerical method is to solve a highly non-convex empirical risk minimizationproblem subjected to a solution from a hypothesis space of neural networks(NNs). In a well-posed elliptic PDE setting, when the hypothesis space consistsof neural networks with either infinite width or depth, we show that the globalminimizer of the empirical loss function is a consistent solution in the limitof large training data. When the hypothesis space is a two-layer neuralnetwork, we show that for a sufficiently large width, gradient descent canidentify a global minimizer of the empirical loss function. Supportingnumerical examples demonstrate the convergence of the solutions, ranging fromsimple manifolds with low and high co-dimensions, to rough surfaces with andwithout boundaries. We also show that the proposed NN solver can robustlygeneralize the PDE solution on new data points with generalization errors thatare almost identical to the training errors, superseding a Nystrom-basedinterpolation method.</description><author>Senwei Liang, Shixiao W. Jiang, John Harlim, Haizhao Yang</author><pubDate>Tue, 27 Feb 2024 18:29:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.06682v4</guid></item><item><title>LoDIP: Low light phase retrieval with deep image prior</title><link>http://arxiv.org/abs/2402.17745v1</link><description>Phase retrieval (PR) is a fundamental challenge in scientific imaging,enabling nanoscale techniques like coherent diffractive imaging (CDI). Imagingat low radiation doses becomes important in applications where samples aresusceptible to radiation damage. However, most PR methods struggle in low dosescenario due to the presence of very high shot noise. Advancements in theoptical data acquisition setup, exemplified by in-situ CDI, have shownpotential for low-dose imaging. But these depend on a time series ofmeasurements, rendering them unsuitable for single-image applications.Similarly, on the computational front, data-driven phase retrieval techniquesare not readily adaptable to the single-image context. Deep learning basedsingle-image methods, such as deep image prior, have been effective for variousimaging tasks but have exhibited limited success when applied to PR. In thiswork, we propose LoDIP which combines the in-situ CDI setup with the power ofimplicit neural priors to tackle the problem of single-image low-dose phaseretrieval. Quantitative evaluations demonstrate the superior performance ofLoDIP on this task as well as applicability to real experimental scenarios.</description><author>Raunak Manekar, Elisa Negrini, Minh Pham, Daniel Jacobs, Jaideep Srivastava</author><pubDate>Tue, 27 Feb 2024 18:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17745v1</guid></item><item><title>Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using Contrastive Learning and Geometric Unfolding</title><link>http://arxiv.org/abs/2402.17744v1</link><description>Understanding the cortical organization of the human brain requiresinterpretable descriptors for distinct structural and functional imaging data.3D polarized light imaging (3D-PLI) is an imaging modality for visualizingfiber architecture in postmortem brains with high resolution that also capturesthe presence of cell bodies, for example, to identify hippocampal subfields.The rich texture in 3D-PLI images, however, makes this modality particularlydifficult to analyze and best practices for characterizing architectonicpatterns still need to be established. In this work, we demonstrate a novelmethod to analyze the regional organization of the human hippocampus in 3D-PLIby combining recent advances in unfolding methods with deep texture featuresobtained using a self-supervised contrastive learning approach. We identifyclusters in the representations that correspond well with classicaldescriptions of hippocampal subfields, lending validity to the developedmethodology.</description><author>Alexander Oberstrass, Jordan DeKraker, Nicola Palomero-Gallagher, Sascha E. A. Muenzing, Alan C. Evans, Markus Axer, Katrin Amunts, Timo Dickscheid</author><pubDate>Tue, 27 Feb 2024 18:25:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17744v1</guid></item><item><title>reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use</title><link>http://arxiv.org/abs/2402.17739v1</link><description>The escalating prevalence of cannabis use, and associated cannabis-usedisorder (CUD), poses a significant public health challenge globally. With anotably wide treatment gap, especially among emerging adults (EAs; ages 18-25),addressing cannabis use and CUD remains a pivotal objective within the 2030United Nations Agenda for Sustainable Development Goals (SDG). In this work, wedevelop an online reinforcement learning (RL) algorithm called reBandit whichwill be utilized in a mobile health study to deliver personalized mobile healthinterventions aimed at reducing cannabis use among EAs. reBandit utilizesrandom effects and informative Bayesian priors to learn quickly and efficientlyin noisy mobile health environments. Moreover, reBandit employs Empirical Bayesand optimization techniques to autonomously update its hyper-parameters online.To evaluate the performance of our algorithm, we construct a simulation testbedusing data from a prior study, and compare against commonly used algorithms inmobile health studies. We show that reBandit performs equally well or betterthan all the baseline algorithms, and the performance gap widens as populationheterogeneity increases in the simulation environment, proving its adeptness toadapt to diverse population of study participants.</description><author>Susobhan Ghosh, Yongyi Guo, Pei-Yao Hung, Lara Coughlin, Erin Bonar, Inbal Nahum-Shani, Maureen Walton, Susan Murphy</author><pubDate>Tue, 27 Feb 2024 18:18:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17739v1</guid></item><item><title>Learning-Based Algorithms for Graph Searching Problems</title><link>http://arxiv.org/abs/2402.17736v1</link><description>We consider the problem of graph searching with prediction recentlyintroduced by Banerjee et al. (2022). In this problem, an agent, starting atsome vertex $r$ has to traverse a (potentially unknown) graph $G$ to find ahidden goal node $g$ while minimizing the total distance travelled. We study asetting in which at any node $v$, the agent receives a noisy estimate of thedistance from $v$ to $g$. We design algorithms for this search task on unknowngraphs. We establish the first formal guarantees on unknown weighted graphs andprovide lower bounds showing that the algorithms we propose have optimal ornearly-optimal dependence on the prediction error. Further, we performnumerical experiments demonstrating that in addition to being robust toadversarial error, our algorithms perform well in typical instances in whichthe error is stochastic. Finally, we provide alternative simpler performancebounds on the algorithms of Banerjee et al. (2022) for the case of searching ona known graph, and establish new lower bounds for this setting.</description><author>Adela Frances DePavia, Erasmo Tani, Ali Vakilian</author><pubDate>Tue, 27 Feb 2024 18:12:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17736v1</guid></item><item><title>Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information</title><link>http://arxiv.org/abs/2310.04311v2</link><description>We consider low-latency image transmission over a noisy wireless channel whencorrelated side information is present only at the receiver side (the Wyner-Zivscenario). In particular, we are interested in developing practical schemesusing a data-driven joint source-channel coding (JSCC) approach, which has beenpreviously shown to outperform conventional separation-based approaches in thepractical finite blocklength regimes, and to provide graceful degradation withchannel quality. We propose a novel neural network architecture thatincorporates the decoder-only side information at multiple stages at thereceiver side. Our results demonstrate that the proposed method succeeds inintegrating the side information, yielding improved performance at all channelconditions in terms of the various quality measures considered here, especiallyat low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs).We have made the source code of the proposed method public to enable furtherresearch, and the reproducibility of the results.</description><author>Selim F. Yilmaz, Ezgi Ozyilkan, Deniz Gunduz, Elza Erkip</author><pubDate>Tue, 27 Feb 2024 18:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04311v2</guid></item><item><title>Tower: An Open Multilingual Large Language Model for Translation-Related Tasks</title><link>http://arxiv.org/abs/2402.17733v1</link><description>While general-purpose large language models (LLMs) demonstrate proficiency onmultiple tasks within the domain of translation, approaches based on open LLMsare competitive only when specializing on a single task. In this paper, wepropose a recipe for tailoring LLMs to multiple tasks present in translationworkflows. We perform continued pretraining on a multilingual mixture ofmonolingual and parallel data, creating TowerBase, followed by finetuning oninstructions relevant for translation processes, creating TowerInstruct. Ourfinal model surpasses open alternatives on several tasks relevant totranslation workflows and is competitive with general-purpose closed LLMs. Tofacilitate future research, we release the Tower models, our specializationdataset, an evaluation framework for LLMs focusing on the translationecosystem, and a collection of model generations, including ours, on ourbenchmark.</description><author>Duarte M. Alves, José Pombal, Nuno M. Guerreiro, Pedro H. Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, André F. T. Martins</author><pubDate>Tue, 27 Feb 2024 18:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17733v1</guid></item><item><title>Asymmetry in Low-Rank Adapters of Foundation Models</title><link>http://arxiv.org/abs/2402.16842v2</link><description>Parameter-efficient fine-tuning optimizes large, pre-trained foundationmodels by updating a subset of parameters; in this class, Low-Rank Adaptation(LoRA) is particularly effective. Inspired by an effort to investigate thedifferent roles of LoRA matrices during fine-tuning, this paper characterizesand leverages unexpected asymmetry in the importance of low-rank adaptermatrices. Specifically, when updating the parameter matrices of a neuralnetwork by adding a product $BA$, we observe that the $B$ and $A$ matrices havedistinct functions: $A$ extracts features from the input, while $B$ uses thesefeatures to create the desired output. Based on this observation, wedemonstrate that fine-tuning $B$ is inherently more effective than fine-tuning$A$, and that a random untrained $A$ should perform nearly as well as afine-tuned one. Using an information-theoretic lens, we also bound thegeneralization of low-rank adapters, showing that the parameter savings ofexclusively training $B$ improves the bound. We support our conclusions withexperiments on RoBERTa, BART-Large, LLaMA-2, and ViTs.</description><author>Jiacheng Zhu, Kristjan Greenewald, Kimia Nadjahi, Haitz Sáez de Ocáriz Borde, Rickard Brüel Gabrielsson, Leshem Choshen, Marzyeh Ghassemi, Mikhail Yurochkin, Justin Solomon</author><pubDate>Tue, 27 Feb 2024 18:06:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16842v2</guid></item><item><title>Batched Nonparametric Contextual Bandits</title><link>http://arxiv.org/abs/2402.17732v1</link><description>We study nonparametric contextual bandits under batch constraints, where theexpected reward for each action is modeled as a smooth function of covariates,and the policy updates are made at the end of each batch of observations. Weestablish a minimax regret lower bound for this setting and propose BatchedSuccessive Elimination with Dynamic Binning (BaSEDB) that achieves optimalregret (up to logarithmic factors). In essence, BaSEDB dynamically splits thecovariate space into smaller bins, carefully aligning their widths with thebatch size. We also show the suboptimality of static binning under batchconstraints, highlighting the necessity of dynamic binning. Additionally, ourresults suggest that a nearly constant number of policy updates can attainoptimal regret in the fully online setting.</description><author>Rong Jiang, Cong Ma</author><pubDate>Tue, 27 Feb 2024 18:06:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17732v1</guid></item><item><title>Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures</title><link>http://arxiv.org/abs/2402.17730v1</link><description>Sequential data naturally arises from user engagement on digital platformslike social media, music streaming services, and web navigation, encapsulatingevolving user preferences and behaviors through continuous information streams.A notable unresolved query in stochastic processes is learning mixtures ofcontinuous-time Markov chains (CTMCs). While there is progress in learningmixtures of discrete-time Markov chains with recovery guarantees[GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexploredchallenges. The intrigue in CTMC mixtures stems from their potential to modelintricate continuous-time stochastic processes prevalent in various fieldsincluding social media, finance, and biology. In this study, we introduce a novel framework for exploring CTMCs,emphasizing the influence of observed trails' length and mixture parameters onproblem regimes, which demands specific algorithms. Through thoroughexperimentation, we examine the impact of discretizing continuous-time trailson the learnability of the continuous-time mixture, given that these processesare often observed via discrete, resource-demanding observations. Ourcomparative analysis with leading methods explores sample complexity and thetrade-off between the number of trails and their lengths, offering crucialinsights for method selection in different problem instances. We apply ouralgorithms on an extensive collection of Lastfm's user-generated trailsspanning three years, demonstrating the capability of our algorithms todifferentiate diverse user preferences. We pioneer the use of CTMC mixtures ona basketball passing dataset to unveil intricate offensive tactics of NBAteams. This underscores the pragmatic utility and versatility of our proposedframework. All results presented in this study are replicable, and we providethe implementations to facilitate reproducibility.</description><author>Fabian Spaeh, Charalampos E. Tsourakakis</author><pubDate>Tue, 27 Feb 2024 18:04:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17730v1</guid></item><item><title>Towards Fairness-Aware Adversarial Learning</title><link>http://arxiv.org/abs/2402.17729v1</link><description>Although adversarial training (AT) has proven effective in enhancing themodel's robustness, the recently revealed issue of fairness in robustness hasnot been well addressed, i.e. the robust accuracy varies significantly amongdifferent categories. In this paper, instead of uniformly evaluating themodel's average class performance, we delve into the issue of robust fairness,by considering the worst-case distribution across various classes. We propose anovel learning paradigm, named Fairness-Aware Adversarial Learning (FAAL). As ageneralization of conventional AT, we re-define the problem of adversarialtraining as a min-max-max framework, to ensure both robustness and fairness ofthe trained model. Specifically, by taking advantage of distributional robustoptimization, our method aims to find the worst distribution among differentcategories, and the solution is guaranteed to obtain the upper boundperformance with high probability. In particular, FAAL can fine-tune an unfairrobust model to be fair within only two epochs, without compromising theoverall clean and robust accuracies. Extensive experiments on various imagedatasets validate the superior performance and efficiency of the proposed FAALcompared to other state-of-the-art methods.</description><author>Yanghao Zhang, Tianle Zhang, Ronghui Mu, Xiaowei Huang, Wenjie Ruan</author><pubDate>Tue, 27 Feb 2024 18:01:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17729v1</guid></item><item><title>VRP-SAM: SAM with Visual Reference Prompt</title><link>http://arxiv.org/abs/2402.17726v1</link><description>In this paper, we propose a novel Visual Reference Prompt (VRP) encoder thatempowers the Segment Anything Model (SAM) to utilize annotated reference imagesas prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAMcan utilize annotated reference images to comprehend specific objects andperform segmentation of specific objects in target image. It is note that theVRP encoder can support a variety of annotation formats for reference images,including \textbf{point}, \textbf{box}, \textbf{scribble}, and \textbf{mask}.VRP-SAM achieves a breakthrough within the SAM framework by extending itsversatility and applicability while preserving SAM's inherent strengths, thusenhancing user-friendliness. To enhance the generalization ability of VRP-SAM,the VRP encoder adopts a meta-learning strategy. To validate the effectivenessof VRP-SAM, we conducted extensive empirical studies on the Pascal and COCOdatasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visualreference segmentation with minimal learnable parameters. Furthermore, VRP-SAMdemonstrates strong generalization capabilities, allowing it to performsegmentation of unseen objects and enabling cross-domain segmentation.</description><author>Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, Gang Zhang, Errui Ding, Jingdong Wang, Zechao Li</author><pubDate>Tue, 27 Feb 2024 17:58:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17726v1</guid></item><item><title>MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation</title><link>http://arxiv.org/abs/2402.17725v1</link><description>Volumetric medical segmentation is a critical component of 3D medical imageanalysis that delineates different semantic regions. Deep neural networks havesignificantly improved volumetric medical segmentation, but they generallyrequire large-scale annotated data to achieve better performance, which can beexpensive and prohibitive to obtain. To address this limitation, existing workstypically perform transfer learning or design dedicated pretraining-finetuningstages to learn representative features. However, the mismatch between thesource and target domain can make it challenging to learn optimalrepresentation for volumetric data, while the multi-stage training demandshigher compute as well as careful selection of stage-specific design choices.In contrast, we propose a universal training framework called MedContext thatis architecture-agnostic and can be incorporated into any existing trainingframework for 3D medical segmentation. Our approach effectively learns selfsupervised contextual cues jointly with the supervised voxel segmentation taskwithout requiring large-scale annotated volumetric medical data or dedicatedpretraining-finetuning stages. The proposed approach induces contextualknowledge in the network by learning to reconstruct the missing organ or partsof an organ in the output segmentation space. The effectiveness of MedContextis validated across multiple 3D medical datasets and four state-of-the-artmodel architectures. Our approach demonstrates consistent gains in segmentationperformance across datasets and different architectures even in few-shot datascenarios. Our code and pretrained models are available athttps://github.com/hananshafi/MedContext</description><author>Hanan Gani, Muzammal Naseer, Fahad Khan, Salman Khan</author><pubDate>Tue, 27 Feb 2024 17:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17725v1</guid></item><item><title>Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners</title><link>http://arxiv.org/abs/2402.17723v1</link><description>Video and audio content creation serves as the core technique for the movieindustry and professional users. Recently, existing diffusion-based methodstackle video and audio generation separately, which hinders the techniquetransfer from academia to industry. In this work, we aim at filling the gap,with a carefully designed optimization-based framework for cross-visual-audioand joint-visual-audio generation. We observe the powerful generation abilityof off-the-shelf video or audio generation models. Thus, instead of trainingthe giant models from scratch, we propose to bridge the existing strong modelswith a shared latent representation space. Specifically, we propose amultimodality latent aligner with the pre-trained ImageBind model. Our latentaligner shares a similar core as the classifier guidance that guides thediffusion denoising process during inference time. Through carefully designedoptimization strategy and loss functions, we show the superior performance ofour method on joint video-audio generation, visual-steered audio generation,and audio-steered visual generation tasks. The project website can be found athttps://yzxing87.github.io/Seeing-and-Hearing/</description><author>Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, Qifeng Chen</author><pubDate>Tue, 27 Feb 2024 17:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17723v1</guid></item><item><title>Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence</title><link>http://arxiv.org/abs/2402.17722v1</link><description>This paper revisits the convergence of Stochastic Mirror Descent (SMD) in thecontemporary nonconvex optimization setting. Existing results for batch-freenonconvex SMD restrict the choice of the distance generating function (DGF) tobe differentiable with Lipschitz continuous gradients, thereby excludingimportant setups such as Shannon entropy. In this work, we present a newconvergence analysis of nonconvex SMD supporting general DGF, that overcomesthe above limitations and relies solely on the standard assumptions. Moreover,our convergence is established with respect to the Bregman Forward-Backwardenvelope, which is a stronger measure than the commonly used squared norm ofgradient mapping. We further extend our results to guarantee high probabilityconvergence under sub-Gaussian noise and global convergence under thegeneralized Bregman Proximal Polyak-{\L}ojasiewicz condition. Additionally, weillustrate the advantages of our improved SMD theory in various nonconvexmachine learning tasks by harnessing nonsmooth DGFs. Notably, in the context ofnonconvex differentially private (DP) learning, our theory yields a simplealgorithm with a (nearly) dimension-independent utility bound. For the problemof training linear neural networks, we develop provably convergent stochasticalgorithms.</description><author>Ilyas Fatkhullin, Niao He</author><pubDate>Tue, 27 Feb 2024 17:56:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17722v1</guid></item><item><title>CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models</title><link>http://arxiv.org/abs/2402.15968v2</link><description>Federated Learning (FL) enables collaborative optimization of machinelearning models across decentralized data by aggregating model parameters. Ourapproach extends this concept by aggregating "knowledge" derived from models,instead of model parameters. We present a novel framework called CoDream, whereclients collaboratively optimize randomly initialized data using federatedoptimization in the input data space, similar to how randomly initialized modelparameters are optimized in FL. Our key insight is that jointly optimizing thisdata can effectively capture the properties of the global data distribution.Sharing knowledge in data space offers numerous benefits: (1) model-agnosticcollaborative learning, i.e., different clients can have different modelarchitectures; (2) communication that is independent of the model size,eliminating scalability concerns with model parameters; (3) compatibility withsecure aggregation, thus preserving the privacy benefits of federated learning;(4) allowing of adaptive optimization of knowledge shared for personalizedlearning. We empirically validate CoDream on standard FL tasks, demonstratingcompetitive performance despite not sharing model parameters. Our code:https://mitmedialab.github.io/codream.github.io/</description><author>Abhishek Singh, Gauri Gupta, Ritvik Kapila, Yichuan Shi, Alex Dang, Sheshank Shankar, Mohammed Ehab, Ramesh Raskar</author><pubDate>Tue, 27 Feb 2024 17:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15968v2</guid></item><item><title>The SMART approach to instance-optimal online learning</title><link>http://arxiv.org/abs/2402.17720v1</link><description>We devise an online learning algorithm -- titled Switching via MonotoneAdapted Regret Traces (SMART) -- that adapts to the data and achieves regretthat is instance optimal, i.e., simultaneously competitive on every inputsequence compared to the performance of the follow-the-leader (FTL) policy andthe worst case guarantee of any other input policy. We show that the regret ofthe SMART policy on any input sequence is within a multiplicative factor$e/(e-1) \approx 1.58$ of the smaller of: 1) the regret obtained by FTL on thesequence, and 2) the upper bound on regret guaranteed by the given worst-casepolicy. This implies a strictly stronger guarantee than typical`best-of-both-worlds' bounds as the guarantee holds for every input sequenceregardless of how it is generated. SMART is simple to implement as it begins byplaying FTL and switches at most once during the time horizon to the worst-casealgorithm. Our approach and results follow from an operational reduction ofinstance optimal online learning to competitive analysis for the ski-rentalproblem. We complement our competitive ratio upper bounds with a fundamentallower bound showing that over all input sequences, no algorithm can get betterthan a $1.43$-fraction of the minimum regret achieved by FTL and theminimax-optimal policy. We also present a modification of SMART that combinesFTL with a ``small-loss" algorithm to achieve instance optimality between theregret of FTL and the small loss regret bound.</description><author>Siddhartha Banerjee, Alankrita Bhatt, Christina Lee Yu</author><pubDate>Tue, 27 Feb 2024 17:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17720v1</guid></item><item><title>Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization</title><link>http://arxiv.org/abs/2402.17718v1</link><description>Laser-directed-energy deposition (DED) offers advantages in additivemanufacturing (AM) for creating intricate geometries and material grading. Yet,challenges like material inconsistency and part variability remain, mainly dueto its layer-wise fabrication. A key issue is heat accumulation during DED,which affects the material microstructure and properties. While closed-loopcontrol methods for heat management are common in DED research, few integratereal-time monitoring, physics-based modeling, and control in a unifiedframework. Our work presents a digital twin (DT) framework for real-timepredictive control of DED process parameters to meet specific designobjectives. We develop a surrogate model using Long Short-Term Memory(LSTM)-based machine learning with Bayesian Inference to predict temperaturesin DED parts. This model predicts future temperature states in real time. Wealso introduce Bayesian Optimization (BO) for Time Series Process Optimization(BOTSPO), based on traditional BO but featuring a unique time series processprofile generator with reduced dimensions. BOTSPO dynamically optimizesprocesses, identifying optimal laser power profiles to attain desiredmechanical properties. The established process trajectory guides onlineoptimizations, aiming to enhance performance. This paper outlines the digitaltwin framework's components, promoting its integration into a comprehensivesystem for AM.</description><author>Vispi Karkaria, Anthony Goeckner, Rujing Zha, Jie Chen, Jianjing Zhang, Qi Zhu, Jian Cao, Robert X. Gao, Wei Chen</author><pubDate>Tue, 27 Feb 2024 17:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17718v1</guid></item><item><title>AmbigNLG: Addressing Task Ambiguity in Instruction for NLG</title><link>http://arxiv.org/abs/2402.17717v1</link><description>In this study, we introduce AmbigNLG, a new task designed to tackle thechallenge of task ambiguity in instructions for Natural Language Generation(NLG) tasks. Despite the impressive capabilities of Large Language Models(LLMs) in understanding and executing a wide range of tasks through naturallanguage interaction, their performance is significantly hindered by theambiguity present in real-world instructions. To address this, AmbigNLG seeksto identify and mitigate such ambiguities, aiming to refine instructions tomatch user expectations better. We introduce a dataset, AmbigSNI-NLG,consisting of 2,500 instances, and develop an ambiguity taxonomy forcategorizing and annotating instruction ambiguities. Our approach demonstratessubstantial improvements in text generation quality, highlighting the criticalrole of clear and specific instructions in enhancing LLM performance in NLGtasks.</description><author>Ayana Niwa, Hayate Iso</author><pubDate>Tue, 27 Feb 2024 17:52:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17717v1</guid></item><item><title>Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers</title><link>http://arxiv.org/abs/2402.17710v1</link><description>In neural network binarization, BinaryConnect (BC) and its variants areconsidered the standard. These methods apply the sign function in their forwardpass and their respective gradients are backpropagated to update the weights.However, the derivative of the sign function is zero whenever defined, whichconsequently freezes training. Therefore, implementations of BC (e.g., BNN)usually replace the derivative of sign in the backward computation withidentity or other approximate gradient alternatives. Although such practiceworks well empirically, it is largely a heuristic or ''training trick.'' We aimat shedding some light on these training tricks from the optimizationperspective. Building from existing theory on ProxConnect (PC, a generalizationof BC), we (1) equip PC with different forward-backward quantizers and obtainProxConnect++ (PC++) that includes existing binarization techniques as specialcases; (2) derive a principled way to synthesize forward-backward quantizerswith automatic theoretical guarantees; (3) illustrate our theory by proposingan enhanced binarization algorithm BNN++; (4) conduct image classificationexperiments on CNNs and vision transformers, and empirically verify that BNN++generally achieves competitive results on binarizing these models.</description><author>Yiwei Lu, Yaoliang Yu, Xinlin Li, Vahid Partovi Nia</author><pubDate>Tue, 27 Feb 2024 17:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17710v1</guid></item><item><title>Case-Based or Rule-Based: How Do Transformers Do the Math?</title><link>http://arxiv.org/abs/2402.17709v1</link><description>Despite the impressive performance in a variety of complex tasks, modernlarge language models (LLMs) still have trouble dealing with some math problemsthat are simple and intuitive for humans, such as addition. While we can easilylearn basic rules of addition and apply them to new problems of any length,LLMs struggle to do the same. Instead, they may rely on similar "cases" seen inthe training corpus for help. We define these two different reasoningmechanisms as "rule-based reasoning" and "case-based reasoning". Sincerule-based reasoning is essential for acquiring the systematic generalizationability, we aim to explore exactly whether transformers use rule-based orcase-based reasoning for math problems. Through carefully designed interventionexperiments on five math tasks, we confirm that transformers are performingcase-based reasoning, no matter whether scratchpad is used, which aligns withthe previous observations that transformers use subgraph matching/shortcutlearning to reason. To mitigate such problems, we propose a Rule-FollowingFine-Tuning (RFFT) technique to teach transformers to perform rule-basedreasoning. Specifically, we provide explicit rules in the input and theninstruct transformers to recite and follow the rules step by step. ThroughRFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition togeneralize to up to 12-digit addition with over 95% accuracy, which is over 40%higher than scratchpad. The significant improvement demonstrates that teachingLLMs to explicitly use rules helps them learn rule-based reasoning andgeneralize better in length.</description><author>Yi Hu, Xiaojuan Tang, Haotong Yang, Muhan Zhang</author><pubDate>Tue, 27 Feb 2024 17:41:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17709v1</guid></item><item><title>Adaptive quantization with mixed-precision based on low-cost proxy</title><link>http://arxiv.org/abs/2402.17706v1</link><description>It is critical to deploy complicated neural network models on hardware withlimited resources. This paper proposes a novel model quantization method, namedthe Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ),which contains three key modules. The hardware-aware module is designed byconsidering the hardware limitations, while an adaptive mixed-precisionquantization module is developed to evaluate the quantization sensitivity byusing the Hessian matrix and Pareto frontier techniques. Integer linearprogramming is used to fine-tune the quantization across different layers. Thenthe low-cost proxy neural architecture search module efficiently explores theideal quantization hyperparameters. Experiments on the ImageNet demonstratethat the proposed LCPAQ achieves comparable or superior quantization accuracyto existing mixed-precision models. Notably, LCPAQ achieves 1/200 of the searchtime compared with existing methods, which provides a shortcut in practicalquantization use for resource-limited devices.</description><author>Junzhe Chen, Qiao Yang, Senmao Tian, Shunli Zhang</author><pubDate>Tue, 27 Feb 2024 17:36:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17706v1</guid></item><item><title>OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments</title><link>http://arxiv.org/abs/2306.08649v2</link><description>Cognitive science and psychology suggest that object-centric representationsof complex scenes are a promising step towards enabling efficient abstractreasoning from low-level perceptual features. Yet, most deep reinforcementlearning approaches only rely on pixel-based representations that do notcapture the compositional properties of natural scenes. For this, we needenvironments and datasets that allow us to work and evaluate object-centricapproaches. In our work, we extend the Atari Learning Environments, themost-used evaluation framework for deep RL approaches, by introducing OCAtari,that performs resource-efficient extractions of the object-centric states forthese games. Our framework allows for object discovery, object representationlearning, as well as object-centric RL. We evaluate OCAtari's detectioncapabilities and resource efficiency. Our source code is available atgithub.com/k4ntz/OC_Atari.</description><author>Quentin Delfosse, Jannis Blüml, Bjarne Gregori, Sebastian Sztwiertnia, Kristian Kersting</author><pubDate>Tue, 27 Feb 2024 17:34:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08649v2</guid></item><item><title>Federated Learning for Estimating Heterogeneous Treatment Effects</title><link>http://arxiv.org/abs/2402.17705v1</link><description>Machine learning methods for estimating heterogeneous treatment effects (HTE)facilitate large-scale personalized decision-making across various domains suchas healthcare, policy making, education, and more. Current machine learningapproaches for HTE require access to substantial amounts of data per treatment,and the high costs associated with interventions makes centrally collecting somuch data for each intervention a formidable challenge. To overcome thisobstacle, in this work, we propose a novel framework for collaborative learningof HTE estimators across institutions via Federated Learning. We show that evenunder a diversity of interventions and subject populations across clients, onecan jointly learn a common feature representation, while concurrently andprivately learning the specific predictive functions for outcomes underdistinct interventions across institutions. Our framework and the associatedalgorithm are based on this insight, and leverage tabular transformers to mapmultiple input data to feature representations which are then used for outcomeprediction via multi-task learning. We also propose a novel way of federatedtraining of personalised transformers that can work with heterogeneous inputfeature spaces. Experimental results on real-world clinical trial datademonstrate the effectiveness of our method.</description><author>Disha Makhija, Joydeep Ghosh, Yejin Kim</author><pubDate>Tue, 27 Feb 2024 17:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17705v1</guid></item><item><title>Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays</title><link>http://arxiv.org/abs/2402.17704v1</link><description>With the rise in engineered biomolecular devices, there is an increased needfor tailor-made biological sequences. Often, many similar biological sequencesneed to be made for a specific application meaning numerous, sometimesprohibitively expensive, lab experiments are necessary for their optimization.This paper presents a transfer learning design of experiments workflow to makethis development feasible. By combining a transfer learning surrogate modelwith Bayesian optimization, we show how the total number of experiments can bereduced by sharing information between optimization tasks. We demonstrate thereduction in the number of experiments using data from the development of DNAcompetitors for use in an amplification-based diagnostic assay. We usecross-validation to compare the predictive accuracy of different transferlearning models, and then compare the performance of the models for both singleobjective and penalized optimization tasks.</description><author>Ruby Sedgwick, John P. Goertz, Molly M. Stevens, Ruth Misener, Mark van der Wilk</author><pubDate>Tue, 27 Feb 2024 17:30:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17704v1</guid></item><item><title>Real-time Low-latency Music Source Separation using Hybrid Spectrogram-TasNet</title><link>http://arxiv.org/abs/2402.17701v1</link><description>There have been significant advances in deep learning for music demixing inrecent years. However, there has been little attention given to how theseneural networks can be adapted for real-time low-latency applications, whichcould be helpful for hearing aids, remixing audio streams and live shows. Inthis paper, we investigate the various challenges involved in adapting currentdemixing models in the literature for this use case. Subsequently, inspired bythe Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domainAudio Separation Network HS-TasNet, which utilises the advantages of spectraland waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overallsignal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increasesto 5.55 with additional training data. These results demonstrate the potentialof efficient demixing for real-time low-latency music applications.</description><author>Satvik Venkatesh, Arthur Benilov, Philip Coleman, Frederic Roskam</author><pubDate>Tue, 27 Feb 2024 17:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17701v1</guid></item><item><title>RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations</title><link>http://arxiv.org/abs/2402.17700v1</link><description>Individual neurons participate in the representation of multiple high-levelconcepts. To what extent can different interpretability methods successfullydisentangle these roles? To help address this question, we introduce RAVEL(Resolving Attribute-Value Entanglements in Language Models), a dataset thatenables tightly controlled, quantitative comparisons between a variety ofexisting interpretability methods. We use the resulting conceptual framework todefine the new method of Multi-task Distributed Alignment Search (MDAS), whichallows us to find distributed representations satisfying multiple causalcriteria. With Llama2-7B as the target language model, MDAS achievesstate-of-the-art results on RAVEL, demonstrating the importance of going beyondneuron-level analyses to identify features distributed across activations. Werelease our benchmark at https://github.com/explanare/ravel.</description><author>Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, Atticus Geiger</author><pubDate>Tue, 27 Feb 2024 17:25:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17700v1</guid></item><item><title>Gradient-based Discrete Sampling with Automatic Cyclical Scheduling</title><link>http://arxiv.org/abs/2402.17699v1</link><description>Discrete distributions, particularly in high-dimensional deep models, areoften highly multimodal due to inherent discontinuities. While gradient-baseddiscrete sampling has proven effective, it is susceptible to becoming trappedin local modes due to the gradient information. To tackle this challenge, wepropose an automatic cyclical scheduling, designed for efficient and accuratesampling in multimodal discrete distributions. Our method contains three keycomponents: (1) a cyclical step size schedule where large steps discover newmodes and small steps exploit each mode; (2) a cyclical balancing schedule,ensuring ``balanced" proposals for given step sizes and high efficiency of theMarkov chain; and (3) an automatic tuning scheme for adjusting thehyperparameters in the cyclical schedules, allowing adaptability across diversedatasets with minimal tuning. We prove the non-asymptotic convergence andinference guarantee for our method in general discrete distributions. Extensiveexperiments demonstrate the superiority of our method in sampling complexmultimodal discrete distributions.</description><author>Patrick Pynadath, Riddhiman Bhattacharya, Arun Hariharan, Ruqi Zhang</author><pubDate>Tue, 27 Feb 2024 17:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17699v1</guid></item><item><title>Learning reduced-order Quadratic-Linear models in Process Engineering using Operator Inference</title><link>http://arxiv.org/abs/2402.17698v1</link><description>In this work, we address the challenge of efficiently modeling dynamicalsystems in process engineering. We use reduced-order model learning,specifically operator inference. This is a non-intrusive, data-driven methodfor learning dynamical systems from time-domain data. The application in ourstudy is carbon dioxide methanation, an important reaction within thePower-to-X framework, to demonstrate its potential. The numerical results showthe ability of the reduced-order models constructed with operator inference toprovide a reduced yet accurate surrogate solution. This represents an importantmilestone towards the implementation of fast and reliable digital twinarchitectures.</description><author>Ion Victor Gosea, Luisa Peterson, Pawan Goyal, Jens Bremer, Kai Sundmacher, Peter Benner</author><pubDate>Tue, 27 Feb 2024 17:21:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17698v1</guid></item><item><title>Multimodal Federated Learning in Healthcare: a Review</title><link>http://arxiv.org/abs/2310.09650v2</link><description>Recent advancements in multimodal machine learning have empowered thedevelopment of accurate and robust AI systems in the medical domain, especiallywithin centralized database systems. Simultaneously, Federated Learning (FL)has progressed, providing a decentralized mechanism where data need not beconsolidated, thereby enhancing the privacy and security of sensitivehealthcare data. The integration of these two concepts supports the ongoingprogress of multimodal learning in healthcare while ensuring the security andprivacy of patient records within local data-holding agencies. This paperoffers a concise overview of the significance of FL in healthcare and outlinesthe current state-of-the-art approaches to Multimodal Federated Learning (MMFL)within the healthcare domain. It comprehensively examines the existingchallenges in the field, shedding light on the limitations of present models.Finally, the paper outlines potential directions for future advancements in thefield, aiming to bridge the gap between cutting-edge AI technology and theimperative need for patient data privacy in healthcare applications.</description><author>Jacob Thrasher, Alina Devkota, Prasiddha Siwakotai, Rohit Chivukula, Pranav Poudel, Chaunbo Hu, Binod Bhattarai, Prashnna Gyawali</author><pubDate>Tue, 27 Feb 2024 17:16:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09650v2</guid></item><item><title>Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution</title><link>http://arxiv.org/abs/2402.11525v3</link><description>Faithfulness, expressiveness, and elegance is the constant pursuit in machinetranslation. However, traditional metrics like \textit{BLEU} do not strictlyalign with human preference of translation quality. In this paper, we exploreleveraging reinforcement learning with human feedback (\textit{RLHF}) toimprove translation quality. It is non-trivial to collect a large high-qualitydataset of human comparisons between translations, especially for low-resourcelanguages. To address this issue, we propose a cost-effective preferencelearning strategy, optimizing reward models by distinguishing between human andmachine translations. In this manner, the reward model learns the deficienciesof machine translation compared to human and guides subsequent improvements inmachine translation. Experimental results demonstrate that \textit{RLHF} caneffectively enhance translation quality and this improvement benefits othertranslation directions not trained with \textit{RLHF}. Further analysisindicates that the model's language capabilities play a crucial role inpreference learning. A reward model with strong language capabilities can moresensitively learn the subtle differences in translation quality and alignbetter with real human translation preferences.</description><author>Nuo Xu, Jun Zhao, Can Zu, Sixian Li, Lu Chen, Zhihao Zhang, Rui Zheng, Shihan Dou, Wenjuan Qin, Tao Gui, Qi Zhang, Xuanjing Huang</author><pubDate>Tue, 27 Feb 2024 17:12:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11525v3</guid></item><item><title>Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning</title><link>http://arxiv.org/abs/2306.00003v2</link><description>Aortic stenosis (AS) is a degenerative valve condition that causessubstantial morbidity and mortality. This condition is under-diagnosed andunder-treated. In clinical practice, AS is diagnosed with expert review oftransthoracic echocardiography, which produces dozens of ultrasound images ofthe heart. Only some of these views show the aortic valve. To automatescreening for AS, deep networks must learn to mimic a human expert's ability toidentify views of the aortic valve then aggregate across these relevant imagesto produce a study-level diagnosis. We find previous approaches to AS detectionyield insufficient accuracy due to relying on inflexible averages acrossimages. We further find that off-the-shelf attention-based multiple instancelearning (MIL) performs poorly. We contribute a new end-to-end MIL approachwith two key methodological innovations. First, a supervised attentiontechnique guides the learned attention mechanism to favor relevant views.Second, a novel self-supervised pretraining strategy applies contrastivelearning on the representation of the whole study instead of individual imagesas commonly done in prior literature. Experiments on an open-access dataset andan external validation set show that our approach yields higher accuracy whilereducing model size.</description><author>Zhe Huang, Benjamin S. Wessler, Michael C. Hughes</author><pubDate>Tue, 27 Feb 2024 17:12:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00003v2</guid></item><item><title>Geometric Deep Learning for Computer-Aided Design: A Survey</title><link>http://arxiv.org/abs/2402.17695v1</link><description>Geometric Deep Learning techniques have become a transformative force in thefield of Computer-Aided Design (CAD), and have the potential to revolutionizehow designers and engineers approach and enhance the design process. Byharnessing the power of machine learning-based methods, CAD designers canoptimize their workflows, save time and effort while making better informeddecisions, and create designs that are both innovative and practical. Theability to process the CAD designs represented by geometric data and to analyzetheir encoded features enables the identification of similarities among diverseCAD models, the proposition of alternative designs and enhancements, and eventhe generation of novel design alternatives. This survey offers a comprehensiveoverview of learning-based methods in computer-aided design across variouscategories, including similarity analysis and retrieval, 2D and 3D CAD modelsynthesis, and CAD generation from point clouds. Additionally, it provides acomplete list of benchmark datasets and their characteristics, along withopen-source codes that have propelled research in this domain. The finaldiscussion delves into the challenges prevalent in this field, followed bypotential future research directions in this rapidly evolving field.</description><author>Negar Heidari, Alexandros Iosifidis</author><pubDate>Tue, 27 Feb 2024 17:11:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17695v1</guid></item><item><title>EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria</title><link>http://arxiv.org/abs/2309.13633v2</link><description>By simply composing prompts, developers can prototype novel generativeapplications with Large Language Models (LLMs). To refine prototypes intoproducts, however, developers must iteratively revise prompts by evaluatingoutputs to diagnose weaknesses. Formative interviews (N=8) revealed thatdevelopers invest significant effort in manually evaluating outputs as theyassess context-specific and subjective criteria. We present EvalLM, aninteractive system for iteratively refining prompts by evaluating multipleoutputs on user-defined criteria. By describing criteria in natural language,users can employ the system's LLM-based evaluator to get an overview of whereprompts excel or fail, and improve these based on the evaluator's feedback. Acomparative study (N=12) showed that EvalLM, when compared to manualevaluation, helped participants compose more diverse criteria, examine twice asmany outputs, and reach satisfactory prompts with 59% fewer revisions. Beyondprompts, our work can be extended to augment model evaluation and alignment inspecific application contexts.</description><author>Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim</author><pubDate>Tue, 27 Feb 2024 17:10:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13633v2</guid></item><item><title>Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts</title><link>http://arxiv.org/abs/2305.13300v4</link><description>By providing external information to large language models (LLMs), toolaugmentation (including retrieval augmentation) has emerged as a promisingsolution for addressing the limitations of LLMs' static parametric memory.However, how receptive are LLMs to such external evidence, especially when theevidence conflicts with their parametric memory? We present the firstcomprehensive and controlled investigation into the behavior of LLMs whenencountering knowledge conflicts. We propose a systematic framework to elicithigh-quality parametric memory from LLMs and construct the correspondingcounter-memory, which enables us to conduct a series of controlled experiments.Our investigation reveals seemingly contradicting behaviors of LLMs. On the onehand, different from prior wisdom, we find that LLMs can be highly receptive toexternal evidence even when that conflicts with their parametric memory, giventhat the external evidence is coherent and convincing. On the other hand, LLMsalso demonstrate a strong confirmation bias when the external evidence containssome information that is consistent with their parametric memory, despite beingpresented with conflicting evidence at the same time. These results poseimportant implications that are worth careful consideration for the furtherdevelopment and deployment of tool- and retrieval-augmented LLMs. Resources areavailable at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.</description><author>Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su</author><pubDate>Tue, 27 Feb 2024 17:08:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13300v4</guid></item><item><title>OneLog: Towards End-to-End Training in Software Log Anomaly Detection</title><link>http://arxiv.org/abs/2104.07324v2</link><description>With the growth of online services, IoT devices, and DevOps-oriented softwaredevelopment, software log anomaly detection is becoming increasingly important.Prior works mainly follow a traditional four-staged architecture (Preprocessor,Parser, Vectorizer, and Classifier). This paper proposes OneLog, which utilizesa single Deep Neural Network (DNN) instead of multiple separate components.OneLog harnesses Convolutional Neural Networks (CNN) at the character level totake digits, numbers, and punctuations, which were removed in prior works, intoaccount alongside the main natural language text. We evaluate our approach insix message- and sequence-based data sets: HDFS, Hadoop, BGL, Thunderbird,Spirit, and Liberty. We experiment with Onelog with single-, multi-, andcross-project setups. Onelog offers state-of-the-art performance in ourdatasets. Onelog can utilize multi-project datasets simultaneously duringtraining, which suggests our model can generalize between datasets.Multi-project training also improves Onelog performance making it ideal whenlimited training data is available for an individual project. We also foundthat cross-project anomaly detection is possible with a single project pair(Liberty and Spirit). Analysis of model internals shows that one log hasmultiple modes of detecting anomalies and that the model learns manuallyvalidated parsing rules for the log messages. We conclude that character-basedCNNs are a promising approach toward end-to-end learning in log anomalydetection. They offer good performance and generalization over multipledatasets. We will make our scripts publicly available upon the acceptance ofthis paper.</description><author>Shayan Hashemi, Mika Mäntylä</author><pubDate>Tue, 27 Feb 2024 17:07:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2104.07324v2</guid></item><item><title>Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms</title><link>http://arxiv.org/abs/2402.17690v1</link><description>The advent of autonomous vehicles has heralded a transformative era intransportation, reshaping the landscape of mobility through cutting-edgetechnologies. Central to this evolu- tion is the integration of ArtificialIntelligence (AI) and learning algorithms, propelling vehicles into realms ofunprecedented autonomy. This paper provides a comprehensive exploration of theevolutionary trajectory of AI within autonomous vehicles, tracing the journeyfrom foundational principles to the most recent advancements. Commencing with acurrent landscape overview, the paper delves into the fundamental role of AI inshaping the autonomous decision-making capabilities of vehicles. It elucidatesthe steps involved in the AI-powered development life cycle in vehicles,addressing ethical considerations and bias in AI-driven software developmentfor autonomous vehicles. The study presents statis- tical insights into theusage and types of AI/learning algorithms over the years, showcasing theevolving research landscape within the automotive industry. Furthermore, thepaper highlights the pivotal role of parameters in refining algorithms for bothtrucks and cars, facilitating vehicles to adapt, learn, and improve performanceover time. It concludes by outlining different levels of autonomy, elucidatingthe nuanced usage of AI and learning algorithms, and automating key tasks ateach level. Additionally, the document discusses the variation in softwarepackage sizes across different autonomy levels</description><author>Sneha Sudhir Shetiya, Divya Garikapati</author><pubDate>Tue, 27 Feb 2024 17:07:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17690v1</guid></item><item><title>QoS prediction in radio vehicular environments via prior user information</title><link>http://arxiv.org/abs/2402.17689v1</link><description>Reliable wireless communications play an important role in the automotiveindustry as it helps to enhance current use cases and enable new ones such asconnected autonomous driving, platooning, cooperative maneuvering, teleoperateddriving, and smart navigation. These and other use cases often rely on specificquality of service (QoS) levels for communication. Recently, the area ofpredictive quality of service (QoS) has received a great deal of attention as akey enabler to forecast communication quality well enough in advance. However,predicting QoS in a reliable manner is a notoriously difficult task. In thispaper, we evaluate ML tree-ensemble methods to predict QoS in the range ofminutes with data collected from a cellular test network. We discuss radioenvironment characteristics and we showcase how these can be used to improve MLperformance and further support the uptake of ML in commercial networks.Specifically, we use the correlations of the measurements coming from the radioenvironment by including information of prior vehicles to enhance theprediction of the target vehicles. Moreover, we are extending prior art byshowing how longer prediction horizons can be supported.</description><author>Noor Ul Ain, Rodrigo Hernangómez, Alexandros Palaios, Martin Kasparick, Sławomir Stańczak</author><pubDate>Tue, 27 Feb 2024 17:05:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17689v1</guid></item><item><title>Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces</title><link>http://arxiv.org/abs/2402.17686v1</link><description>Uncertainty quantification (UQ) to detect samples with large expected errors(outliers) is applied to reactive molecular potential energy surfaces (PESs).Three methods - Ensembles, Deep Evidential Regression (DER), and GaussianMixture Models (GMM) - were applied to the H-transfer reaction between ${\itsyn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemblemodels provide the best results for detecting outliers, followed by GMM. Forexample, from a pool of 1000 structures with the largest uncertainty, thedetection quality for outliers is $\sim 90$ \% and $\sim 50$ \%, respectively,if 25 or 1000 structures with large errors are sought. On the contrary, thelimitations of the statistical assumptions of DER greatly impacted itsprediction capabilities. Finally, a structure-based indicator was found to becorrelated with large average error, which may help to rapidly classify newstructures into those that provide an advantage for refining the neuralnetwork.</description><author>Luis Itza Vazquez-Salazar, Silvan Käser, Markus Meuwly</author><pubDate>Tue, 27 Feb 2024 17:01:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17686v1</guid></item><item><title>NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation</title><link>http://arxiv.org/abs/2402.15852v2</link><description>Vision-and-Language Navigation (VLN) stands as a key research problem ofEmbodied AI, aiming at enabling agents to navigate in unseen environmentsfollowing linguistic instructions. In this field, generalization is along-standing challenge, either to out-of-distribution scenes or from Sim toReal. In this paper, we propose NaVid, a video-based large vision languagemodel (VLM), to mitigate such a generalization gap. NaVid makes the firstendeavour to showcase the capability of VLMs to achieve state-of-the-art levelnavigation performance without any maps, odometer and depth inputs. Followinghuman instruction, NaVid only requires an on-the-fly video stream from amonocular RGB camera equipped on the robot to output the next-step action. Ourformulation mimics how humans navigate and naturally gets rid of the problemsintroduced by odometer noises, and the Sim2Real gaps from map or depth inputs.Moreover, our video-based approach can effectively encode the historicalobservations of robots as spatio-temporal contexts for decision-making andinstruction following. We train NaVid with 550k navigation samples collectedfrom VLN-CE trajectories, including action-planning and instruction-reasoningsamples, along with 665k large-scale web data. Extensive experiments show thatNaVid achieves SOTA performance in simulation environments and the real world,demonstrating superior cross-dataset and Sim2Real transfer. We thus believe ourproposed VLM approach plans the next step for not only the navigation agentsbut also this research field.</description><author>Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, Wang He</author><pubDate>Tue, 27 Feb 2024 17:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15852v2</guid></item><item><title>Killer Apps: Low-Speed, Large-Scale AI Weapons</title><link>http://arxiv.org/abs/2402.01663v2</link><description>The accelerating advancements in Artificial Intelligence (AI) and MachineLearning (ML), highlighted by the development of cutting-edge GenerativePre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, andAnthropic, present new challenges and opportunities in warfare and security.Much of the current focus is on AI's integration within weapons systems and itsrole in rapid decision-making in kinetic conflict. However, an equallyimportant but often overlooked aspect is the potential of AI-basedpsychological manipulation at internet scales within the information domain.These capabilities could pose significant threats to individuals,organizations, and societies globally. This paper explores the concept of AIweapons, their deployment, detection, and potential countermeasures.</description><author>Philip Feldman, Aaron Dant, James R. Foulds</author><pubDate>Tue, 27 Feb 2024 16:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01663v2</guid></item><item><title>NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents</title><link>http://arxiv.org/abs/2402.17682v1</link><description>While (large) language models have significantly improved over the lastyears, they still struggle to sensibly process long sequences found, e.g., inbooks, due to the quadratic scaling of the underlying attention mechanism. Toaddress this, we propose NextLevelBERT, a Masked Language Model operating noton tokens, but on higher-level semantic representations in the form of textembeddings. We pretrain NextLevelBERT to predict the vector representation ofentire masked text chunks and evaluate the effectiveness of the resultingdocument vectors on three task types: 1) Semantic Textual Similarity viazero-shot document embeddings, 2) Long document classification, 3)Multiple-choice question answering. We find that next level Masked LanguageModeling is an effective technique to tackle long-document use cases and canoutperform much larger embedding models as long as the required level of detailis not too high. We make model and code available.</description><author>Tamara Czinczoll, Christoph Hönes, Maximilian Schall, Gerard de Melo</author><pubDate>Tue, 27 Feb 2024 16:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17682v1</guid></item><item><title>MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning</title><link>http://arxiv.org/abs/2402.17680v1</link><description>To address the problem of catastrophic forgetting due to the invisibility ofold categories in sequential input, existing work based on relatively simplecategorization tasks has made some progress. In contrast, video captioning is amore complex task in multimodal scenario, which has not been explored in thefield of incremental learning. After identifying this stability-plasticityproblem when analyzing video with sequential input, we originally propose amethod to Mitigate Catastrophic Forgetting in class-incremental learning formultimodal Video Captioning (MCF-VC). As for effectively maintaining goodperformance on old tasks at the macro level, we design Fine-grained SensitivitySelection (FgSS) based on the Mask of Linear's Parameters and FisherSensitivity to pick useful knowledge from old tasks. Further, in order tobetter constrain the knowledge characteristics of old and new tasks at thespecific feature level, we have created the Two-stage Knowledge Distillation(TsKD), which is able to learn the new task well while weighing the old task.Specifically, we design two distillation losses, which constrain the crossmodal semantic information of semantic attention feature map and the textualinformation of the final outputs respectively, so that the inter-model andintra-model stylized knowledge of the old class is retained while learning thenew class. In order to illustrate the ability of our model to resistforgetting, we designed a metric CIDER_t to detect the stage forgetting rate.Our experiments on the public dataset MSR-VTT show that the proposed methodsignificantly resists the forgetting of previous tasks without replaying oldsamples, and performs well on the new task.</description><author>Huiyu Xiong, Lanxiao Wang, Heqian Qiu, Taijin Zhao, Benliu Qiu, Hongliang Li</author><pubDate>Tue, 27 Feb 2024 16:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17680v1</guid></item><item><title>CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise Sketch Instance Guided Attention</title><link>http://arxiv.org/abs/2402.17678v1</link><description>Reverse engineering in the realm of Computer-Aided Design (CAD) has been alongstanding aspiration, though not yet entirely realized. Its primary aim isto uncover the CAD process behind a physical object given its 3D scan. Wepropose CAD-SIGNet, an end-to-end trainable and auto-regressive architecture torecover the design history of a CAD model represented as a sequence ofsketch-and-extrusion from an input point cloud. Our model learnsvisual-language representations by layer-wise cross-attention between pointcloud and CAD language embedding. In particular, a new Sketch instance GuidedAttention (SGA) module is proposed in order to reconstruct the fine-graineddetails of the sketches. Thanks to its auto-regressive nature, CAD-SIGNet notonly reconstructs a unique full design history of the corresponding CAD modelgiven an input point cloud but also provides multiple plausible design choices.This allows for an interactive reverse engineering scenario by providingdesigners with multiple next-step choices along with the design process.Extensive experiments on publicly available CAD datasets showcase theeffectiveness of our approach against existing baseline models in two settings,namely, full design history recovery and conditional auto-completion from pointclouds.</description><author>Mohammad Sadil Khan, Elona Dupont, Sk Aziz Ali, Kseniya Cherenkova, Anis Kacem, Djamila Aouada</author><pubDate>Tue, 27 Feb 2024 16:53:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17678v1</guid></item><item><title>Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling</title><link>http://arxiv.org/abs/2401.14556v2</link><description>Pre-trained language models based on masked language modeling (MLM) excel innatural language understanding (NLU) tasks. While fine-tuned MLM-based encodersconsistently outperform causal language modeling decoders of comparable size,recent decoder-only large language models (LLMs) perform on par with smallerMLM-based encoders. Although their performance improves with scale, LLMs fallshort of achieving state-of-the-art results in information extraction (IE)tasks, many of which are formulated as sequence labeling (SL). We hypothesizethat LLMs' poor SL performance stems from causal masking, which prevents themodel from attending to tokens on the right of the current token. Yet, howexactly and to what extent LLMs' performance on SL can be improved remainsunclear. We explore techniques for improving the SL performance of open LLMs onIE tasks by applying layer-wise removal of the causal mask (CM) during LLMfine-tuning. This approach yields performance gains competitive withstate-of-the-art SL models, matching or outperforming the results of CM removalfrom all blocks. Our findings hold for diverse SL tasks, demonstrating thatopen LLMs with layer-dependent CM removal outperform strong MLM-based encodersand even instruction-tuned LLMs.</description><author>David Dukić, Jan Šnajder</author><pubDate>Tue, 27 Feb 2024 16:48:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14556v2</guid></item><item><title>SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification</title><link>http://arxiv.org/abs/2402.17672v1</link><description>Polarimetric synthetic aperture radar (PolSAR) images encompass valuableinformation that can facilitate extensive land cover interpretation andgenerate diverse output products. Extracting meaningful features from PolSARdata poses challenges distinct from those encountered in optical imagery. Deeplearning (DL) methods offer effective solutions for overcoming these challengesin PolSAR feature extraction. Convolutional neural networks (CNNs) play acrucial role in capturing PolSAR image characteristics by leveraging kernelcapabilities to consider local information and the complex-valued nature ofPolSAR data. In this study, a novel three-branch fusion of complex-valued CNN,named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed forPolSAR image classification. To validate the performance of the proposedmethod, classification results are compared against multiple state-of-the-artapproaches using the airborne synthetic aperture radar (AIRSAR) datasets ofFlevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. Theresults indicate that the proposed approach demonstrates improvements inoverallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland dataunderscore the effectiveness of the SDF2Net model, revealing a promisingoverall accuracy of 96.01% even with only a 1% sampling ratio.</description><author>Mohammed Q. Alkhatib, M. Sami Zitouni, Mina Al-Saad, Nour Aburaed, Hussain Al-Ahmad</author><pubDate>Tue, 27 Feb 2024 16:46:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17672v1</guid></item><item><title>Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models</title><link>http://arxiv.org/abs/2402.17671v1</link><description>As foundation models (FMs) continue to shape the landscape of AI, thein-context learning (ICL) paradigm thrives but also encounters issues such astoxicity, hallucination, disparity, adversarial vulnerability, andinconsistency. Ensuring the reliability and responsibility of FMs is crucialfor the sustainable development of the AI ecosystem. In this concise overview,we investigate recent advancements in enhancing the reliability andtrustworthiness of FMs within ICL frameworks, focusing on four keymethodologies, each with its corresponding subgoals. We sincerely hope thispaper can provide valuable insights for researchers and practitionersendeavoring to build safe and dependable FMs and foster a stable and consistentICL environment, thereby unlocking their vast potential.</description><author>Yunpeng Huang, Yaonan Gu, Jingwei Xu, Zhihong Zhu, Zhaorun Chen, Xiaoxing Ma</author><pubDate>Tue, 27 Feb 2024 16:44:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17671v1</guid></item><item><title>Consistency-guided Prompt Learning for Vision-Language Models</title><link>http://arxiv.org/abs/2306.01195v3</link><description>We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuningmethod for vision-language models. Our approach improves the generalization oflarge foundation models when fine-tuned on downstream tasks in a few-shotsetting. The basic idea of CoPrompt is to enforce a consistency constraint inthe prediction of the trainable and pre-trained models to prevent overfittingon the downstream task. Additionally, we introduce the following two componentsinto our consistency constraint to further boost the performance: enforcingconsistency on two perturbed inputs and combining two dominant paradigms oftuning, prompting and adapter. Enforcing consistency on perturbed input servesto further regularize the consistency constraint, thereby improvinggeneralization. Moreover, the integration of adapters and prompts not onlyenhances performance on downstream tasks but also offers increased tuningflexibility in both input and output spaces. This facilitates more effectiveadaptation to downstream tasks in a few-shot learning setting. Experiments showthat CoPrompt outperforms existing methods on a range of evaluation suites,including base-to-novel generalization, domain generalization, andcross-dataset evaluation. On generalization, CoPrompt improves thestate-of-the-art on zero-shot tasks and the overall harmonic mean over 11datasets. Detailed ablation studies show the effectiveness of each of thecomponents in CoPrompt. We make our code available athttps://github.com/ShuvenduRoy/CoPrompt.</description><author>Shuvendu Roy, Ali Etemad</author><pubDate>Tue, 27 Feb 2024 16:40:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01195v3</guid></item><item><title>Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework</title><link>http://arxiv.org/abs/2311.18460v2</link><description>Fairness for machine learning predictions is widely required in practice forlegal, ethical, and societal reasons. Existing work typically focuses onsettings without unobserved confounding, even though unobserved confounding canlead to severe violations of causal fairness and, thus, unfair predictions. Inthis work, we analyze the sensitivity of causal fairness to unobservedconfounding. Our contributions are three-fold. First, we derive bounds forcausal fairness metrics under different sources of unobserved confounding. Thisenables practitioners to examine the sensitivity of their machine learningmodels to unobserved confounding in fairness-critical applications. Second, wepropose a novel neural framework for learning fair predictions, which allows usto offer worst-case guarantees of the extent to which causal fairness can beviolated due to unobserved confounding. Third, we demonstrate the effectivenessof our framework in a series of experiments, including a real-world case studyabout predicting prison sentences. To the best of our knowledge, ours is thefirst work to study causal fairness under unobserved confounding. To this end,our work is of direct practical value as a refutation strategy to ensure thefairness of predictions in high-stakes applications.</description><author>Maresa Schröder, Dennis Frauen, Stefan Feuerriegel</author><pubDate>Tue, 27 Feb 2024 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18460v2</guid></item><item><title>Overcoming Dimensional Collapse in Self-supervised Contrastive Learning for Medical Image Segmentation</title><link>http://arxiv.org/abs/2402.14611v2</link><description>Self-supervised learning (SSL) approaches have achieved great success whenthe amount of labeled data is limited. Within SSL, models learn robust featurerepresentations by solving pretext tasks. One such pretext task is contrastivelearning, which involves forming pairs of similar and dissimilar input samples,guiding the model to distinguish between them. In this work, we investigate theapplication of contrastive learning to the domain of medical image analysis.Our findings reveal that MoCo v2, a state-of-the-art contrastive learningmethod, encounters dimensional collapse when applied to medical images. This isattributed to the high degree of inter-image similarity shared between themedical images. To address this, we propose two key contributions: localfeature learning and feature decorrelation. Local feature learning improves theability of the model to focus on the local regions of the image, while featuredecorrelation removes the linear dependence among the features. Ourexperimental findings demonstrate that our contributions significantly enhancethe model's performance in the downstream task of medical segmentation, both inthe linear evaluation and full fine-tuning settings. This work illustrates theimportance of effectively adapting SSL techniques to the characteristics ofmedical imaging tasks. The source code will be made publicly available at:https://github.com/CAMMA-public/med-moco</description><author>Jamshid Hassanpour, Vinkle Srivastav, Didier Mutter, Nicolas Padoy</author><pubDate>Tue, 27 Feb 2024 16:39:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14611v2</guid></item><item><title>Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing</title><link>http://arxiv.org/abs/2402.17666v1</link><description>This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL)approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Eachsatellite is an independent decision-making agent with a partial knowledge ofthe environment, and supported by feedback received from the nearby agents.Building on our previous work that introduced a Q-routing solution, thecontribution of this paper is to extend it to a deep learning framework able toquickly adapt to the network and traffic changes, and based on two phases: (1)An offline exploration learning phase that relies on a global Deep NeuralNetwork (DNN) to learn the optimal paths at each possible position andcongestion level; (2) An online exploitation phase with local, on-board,pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routesoffline that are then loaded for an efficient distributed routing online.</description><author>Federico Lozano-Cuadra, Beatriz Soret</author><pubDate>Tue, 27 Feb 2024 16:36:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17666v1</guid></item><item><title>Anatomy of Neural Language Models</title><link>http://arxiv.org/abs/2401.03797v2</link><description>The fields of generative AI and transfer learning have experienced remarkableadvancements in recent years especially in the domain of Natural LanguageProcessing (NLP). Transformers have been at the heart of these advancementswhere the cutting-edge transformer-based Language Models (LMs) have led to newstate-of-the-art results in a wide spectrum of applications. While the numberof research works involving neural LMs is exponentially increasing, their vastmajority are high-level and far from self-contained. Consequently, a deepunderstanding of the literature in this area is a tough task especially in theabsence of a unified mathematical framework explaining the main types of neuralLMs. We address the aforementioned problem in this tutorial where the objectiveis to explain neural LMs in a detailed, simplified and unambiguous mathematicalframework accompanied by clear graphical illustrations. Concrete examples onwidely used models like BERT and GPT2 are explored. Finally, since transformerspretrained on language-modeling-like tasks have been widely adopted in computervision and time series applications, we briefly explore some examples of suchsolutions in order to enable readers to understand how transformers work in theaforementioned domains and compare this use with the original one in NLP.</description><author>Majd Saleh, Stéphane Paquelet</author><pubDate>Tue, 27 Feb 2024 16:35:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03797v2</guid></item><item><title>Bayesian Differentiable Physics for Cloth Digitalization</title><link>http://arxiv.org/abs/2402.17664v1</link><description>We propose a new method for cloth digitalization. Deviating from existingmethods which learn from data captured under relatively casual settings, wepropose to learn from data captured in strictly tested measuring protocols, andfind plausible physical parameters of the cloths. However, such data iscurrently absent, so we first propose a new dataset with accurate clothmeasurements. Further, the data size is considerably smaller than the ones incurrent deep learning, due to the nature of the data capture process. To learnfrom small data, we propose a new Bayesian differentiable cloth model toestimate the complex material heterogeneity of real cloths. It can providehighly accurate digitalization from very limited data samples. Throughexhaustive evaluation and comparison, we show our method is accurate in clothdigitalization, efficient in learning from limited data samples, and general incapturing material variations. Code and data are availablehttps://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization</description><author>Deshan Gong, Ningtao Mao, He Wang</author><pubDate>Tue, 27 Feb 2024 16:35:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17664v1</guid></item><item><title>TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations</title><link>http://arxiv.org/abs/2402.17660v1</link><description>Achieving a balance between computational speed, prediction accuracy, anduniversal applicability in molecular simulations has been a persistentchallenge. This paper presents substantial advancements in the TorchMD-Netsoftware, a pivotal step forward in the shift from conventional force fields toneural network-based potentials. The evolution of TorchMD-Net into a morecomprehensive and versatile framework is highlighted, incorporatingcutting-edge architectures such as TensorNet. This transformation is achievedthrough a modular design approach, encouraging customized applications withinthe scientific community. The most notable enhancement is a significantimprovement in computational efficiency, achieving a very remarkableacceleration in the computation of energy and forces for TensorNet models, withperformance gains ranging from 2-fold to 10-fold over previous iterations.Other enhancements include highly optimized neighbor search algorithms thatsupport periodic boundary conditions and the smooth integration with existingmolecular dynamics frameworks. Additionally, the updated version introduces thecapability to integrate physical priors, further enriching its applicationspectrum and utility in research. The software is available athttps://github.com/torchmd/torchmd-net.</description><author>Raul P. Pelaez, Guillem Simeon, Raimondas Galvelis, Antonio Mirarchi, Peter Eastman, Stefan Doerr, Philipp Thölke, Thomas E. Markland, Gianni De Fabritiis</author><pubDate>Tue, 27 Feb 2024 16:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17660v1</guid></item><item><title>Confidence-Aware Multi-Field Model Calibration</title><link>http://arxiv.org/abs/2402.17655v1</link><description>Accurately predicting the probabilities of user feedback, such as clicks andconversions, is critical for ad ranking and bidding. However, there often existunwanted mismatches between predicted probabilities and true likelihoods due tothe shift of data distributions and intrinsic model biases. Calibration aims toaddress this issue by post-processing model predictions, and field-awarecalibration can adjust model output on different feature field values tosatisfy fine-grained advertising demands. Unfortunately, the observed samplescorresponding to certain field values can be too limited to make confidentcalibrations, which may yield bias amplification and online disturbance. Inthis paper, we propose a confidence-aware multi-field calibration method, whichadaptively adjusts the calibration intensity based on the confidence levelsderived from sample statistics. It also utilizes multiple feature fields forjoint model calibration with awareness of their importance to mitigate the datasparsity effect of a single field. Extensive offline and online experimentsshow the superiority of our method in boosting advertising performance andreducing prediction deviations.</description><author>Yuang Zhao, Chuhan Wu, Qinglin Jia, Hong Zhu, Jia Yan, Libin Zong, Linxuan Zhang, Zhenhua Dong, Muyu Zhang</author><pubDate>Tue, 27 Feb 2024 16:24:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17655v1</guid></item><item><title>Mitigating Distributional Shift in Semantic Segmentation via Uncertainty Estimation from Unlabelled Data</title><link>http://arxiv.org/abs/2402.17653v1</link><description>Knowing when a trained segmentation model is encountering data that isdifferent to its training data is important. Understanding and mitigating theeffects of this play an important part in their application from a performanceand assurance perspective - this being a safety concern in applications such asautonomous vehicles (AVs). This work presents a segmentation network that candetect errors caused by challenging test domains without any additionalannotation in a single forward pass. As annotation costs limit the diversity oflabelled datasets, we use easy-to-obtain, uncurated and unlabelled data tolearn to perform uncertainty estimation by selectively enforcing consistencyover data augmentation. To this end, a novel segmentation benchmark based onthe SAX Dataset is used, which includes labelled test data spanning threeautonomous-driving domains, ranging in appearance from dense urban to off-road.The proposed method, named Gamma-SSL, consistently outperforms uncertaintyestimation and Out-of-Distribution (OoD) techniques on this difficult benchmark- by up to 10.7% in area under the receiver operating characteristic (ROC)curve and 19.2% in area under the precision-recall (PR) curve in the mostchallenging of the three scenarios.</description><author>David S. W. Williams, Daniele De Martini, Matthew Gadd, Paul Newman</author><pubDate>Tue, 27 Feb 2024 16:23:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17653v1</guid></item><item><title>Navigator: A Decentralized Scheduler for Latency-Sensitive ML Workflows</title><link>http://arxiv.org/abs/2402.17652v1</link><description>We consider ML query processing in distributed systems where GPU-enabledworkers coordinate to execute complex queries: a computing style often seen inapplications that interact with users in support of image processing andnatural language processing. In such systems, coscheduling of GPU memorymanagement and task placement represents a promising opportunity. We proposeNavigator, a novel framework that unifies these functions to reduce job latencywhile using resources efficiently, placing tasks where data dependencies willbe satisfied, collocating tasks from the same job (when this will not overloadthe host or its GPU), and efficiently managing GPU memory. Comparison withother state of the art schedulers shows a significant reduction in completiontimes while requiring the same amount or even fewer resources. In one case,just half the servers were needed for processing the same workload.</description><author>Yuting Yang, Andrea Merlina, Weijia Song, Tiancheng Yuan, Ken Birman, Roman Vitenberg</author><pubDate>Tue, 27 Feb 2024 16:21:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17652v1</guid></item><item><title>Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs</title><link>http://arxiv.org/abs/2402.17649v1</link><description>Due to the widespread use of large language models (LLMs) in ubiquitoussystems, we need to understand whether they embed a specific worldview and whatthese views reflect. Recent studies report that, prompted with politicalquestionnaires, LLMs show left-liberal leanings. However, it is as yet unclearwhether these leanings are reliable (robust to prompt variations) and whetherthe leaning is consistent across policies and political leaning. We propose aseries of tests which assess the reliability and consistency of LLMs' stanceson political statements based on a dataset of voting-advice questionnairescollected from seven EU countries and annotated for policy domains. We studyLLMs ranging in size from 7B to 70B parameters and find that their reliabilityincreases with parameter count. Larger models show overall stronger alignmentwith left-leaning parties but differ among policy programs: They evince a(left-wing) positive stance towards environment protection, social welfare butalso (right-wing) law and order, with no consistent preferences in foreignpolicy, migration, and economy.</description><author>Tanise Ceron, Neele Falk, Ana Barić, Dmitry Nikolaev, Sebastian Padó</author><pubDate>Tue, 27 Feb 2024 16:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17649v1</guid></item><item><title>Variational Gaussian Process Diffusion Processes</title><link>http://arxiv.org/abs/2306.02066v3</link><description>Diffusion processes are a class of stochastic differential equations (SDEs)providing a rich family of expressive models that arise naturally in dynamicmodelling tasks. Probabilistic inference and learning under generative modelswith latent processes endowed with a non-linear diffusion process prior areintractable problems. We build upon work within variational inference,approximating the posterior process as a linear diffusion process, and pointout pathologies in the approach. We propose an alternative parameterization ofthe Gaussian variational process using a site-based exponential familydescription. This allows us to trade a slow inference algorithm withfixed-point iterations for a fast algorithm for convex optimization akin tonatural gradient descent, which also provides a better objective for learningmodel parameters.</description><author>Prakhar Verma, Vincent Adam, Arno Solin</author><pubDate>Tue, 27 Feb 2024 16:18:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02066v3</guid></item><item><title>SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation</title><link>http://arxiv.org/abs/2402.17645v1</link><description>We present SongComposer, an innovative LLM designed for song composition. Itcould understand and generate melodies and lyrics in symbolic songrepresentations, by leveraging the capability of LLM. Existing music-relatedLLM treated the music as quantized audio signals, while such implicit encodingleads to inefficient encoding and poor flexibility. In contrast, we resort tosymbolic song representation, the mature and efficient way humans designed formusic, and enable LLM to explicitly compose songs like humans. In practice, wedesign a novel tuple design to format lyric and three note attributes (pitch,duration, and rest duration) in the melody, which guarantees the correct LLMunderstanding of musical symbols and realizes precise alignment between lyricsand melody. To impart basic music understanding to LLM, we carefully collectedSongCompose-PT, a large-scale song pretraining dataset that includes lyrics,melodies, and paired lyrics-melodies in either Chinese or English. Afteradequate pre-training, 10K carefully crafted QA pairs are used to empower theLLM with the instruction-following capability and solve diverse tasks. Withextensive experiments, SongComposer demonstrates superior performance inlyric-to-melody generation, melody-to-lyric generation, song continuation, andtext-to-song creation, outperforming advanced LLMs like GPT-4.</description><author>Shuangrui Ding, Zihan Liu, Xiaoyi Dong, Pan Zhang, Rui Qian, Conghui He, Dahua Lin, Jiaqi Wang</author><pubDate>Tue, 27 Feb 2024 16:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17645v1</guid></item><item><title>Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data</title><link>http://arxiv.org/abs/2402.17644v1</link><description>Quantitative reasoning is a critical skill to analyze data, yet theassessment of such ability remains limited. To address this gap, we introducethe Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluateLarge Language Models' capability in statistical and causal reasoning withreal-world data. The benchmark comprises a carefully constructed dataset of 411questions accompanied by data sheets from textbooks, online learning materials,and academic papers. To compare models' quantitative reasoning abilities ondata and text, we enrich the benchmark with an auxiliary set of 290 text-onlyquestions, namely QRText. We evaluate natural language reasoning, program-basedreasoning, and agent reasoning methods including Chain-of-Thought,Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models.The strongest model GPT-4 achieves an accuracy of 58%, which has a large roomfor improvement. Among open-source models, Deepseek-coder-instruct, a code LLMpretrained on 2T tokens, gets the highest accuracy of 37%. Analysis revealsthat models encounter difficulties in data analysis and causal reasoning, andstruggle in using causal knowledge and provided data simultaneously. Code anddata are in https://github.com/xxxiaol/QRData.</description><author>Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng</author><pubDate>Tue, 27 Feb 2024 16:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17644v1</guid></item><item><title>Variational Learning is Effective for Large Deep Networks</title><link>http://arxiv.org/abs/2402.17641v1</link><description>We give extensive empirical evidence against the common belief thatvariational learning is ineffective for large neural networks. We show that anoptimizer called Improved Variational Online Newton (IVON) consistently matchesor outperforms Adam for training large networks such as GPT-2 and ResNets fromscratch. IVON's computational costs are nearly identical to Adam but itspredictive uncertainty is better. We show several new use cases of IVON wherewe improve fine-tuning and model merging in Large Language Models, accuratelypredict generalization error, and faithfully estimate sensitivity to data. Wefind overwhelming evidence in support of effectiveness of variational learning.</description><author>Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, Thomas Möllenhoff</author><pubDate>Tue, 27 Feb 2024 16:11:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17641v1</guid></item><item><title>DoubleML -- An Object-Oriented Implementation of Double Machine Learning in R</title><link>http://arxiv.org/abs/2103.09603v5</link><description>The R package DoubleML implements the double/debiased machine learningframework of Chernozhukov et al. (2018). It provides functionalities toestimate parameters in causal models based on machine learning methods. Thedouble machine learning framework consist of three key ingredients: Neymanorthogonality, high-quality machine learning estimation and sample splitting.Estimation of nuisance components can be performed by various state-of-the-artmachine learning methods that are available in the mlr3 ecosystem. DoubleMLmakes it possible to perform inference in a variety of causal models, includingpartially linear and interactive regression models and their extensions toinstrumental variable estimation. The object-oriented implementation ofDoubleML enables a high flexibility for the model specification and makes iteasily extendable. This paper serves as an introduction to the double machinelearning framework and the R package DoubleML. In reproducible code exampleswith simulated and real data sets, we demonstrate how DoubleML users canperform valid inference based on machine learning methods.</description><author>Philipp Bach, Victor Chernozhukov, Malte S. Kurz, Martin Spindler, Sven Klaassen</author><pubDate>Tue, 27 Feb 2024 16:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.09603v5</guid></item><item><title>UVDoc: Neural Grid-based Document Unwarping</title><link>http://arxiv.org/abs/2302.02887v2</link><description>Restoring the original, flat appearance of a printed document from casualphotographs of bent and wrinkled pages is a common everyday problem. In thispaper we propose a novel method for grid-based single-image document unwarping.Our method performs geometric distortion correction via a fully convolutionaldeep neural network that learns to predict the 3D grid mesh of the document andthe corresponding 2D unwarping grid in a dual-task fashion, implicitly encodingthe coupling between the shape of a 3D piece of paper and its 2D image. Inorder to allow unwarping models to train on data that is more realistic inappearance than the commonly used synthetic Doc3D dataset, we create andpublish our own dataset, called UVDoc, which combines pseudo-photorealisticdocument images with physically accurate 3D shape and unwarping functionannotations. Our dataset is labeled with all the information necessary to trainour unwarping network, without having to engineer separate loss functions thatcan deal with the lack of ground-truth typically found in document in the wilddatasets. We perform an in-depth evaluation that demonstrates that with theinclusion of our novel pseudo-photorealistic dataset, our relatively smallnetwork architecture achieves state-of-the-art results on the DocUNetbenchmark. We show that the pseudo-photorealistic nature of our UVDoc datasetallows for new and better evaluation methods, such as lighting-correctedMS-SSIM. We provide a novel benchmark dataset that facilitates suchevaluations, and propose a metric that quantifies line straightness afterunwarping. Our code, results and UVDoc dataset are available athttps://github.com/tanguymagne/UVDoc.</description><author>Floor Verhoeven, Tanguy Magne, Olga Sorkine-Hornung</author><pubDate>Tue, 27 Feb 2024 15:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02887v2</guid></item><item><title>From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions</title><link>http://arxiv.org/abs/2402.17633v1</link><description>Text segmentation is a fundamental task in natural language processing, wheredocuments are split into contiguous sections. However, prior research in thisarea has been constrained by limited datasets, which are either small in scale,synthesized, or only contain well-structured documents. In this paper, weaddress these limitations by introducing a novel benchmark YTSeg focusing onspoken content that is inherently more unstructured and both topically andstructurally diverse. As part of this work, we introduce an efficienthierarchical segmentation model MiniSeg, that outperforms state-of-the-artbaselines. Lastly, we expand the notion of text segmentation to a morepractical "smart chaptering" task that involves the segmentation ofunstructured content, the generation of meaningful segment titles, and apotential real-time application of the models.</description><author>Fabian Retkowski, Alexander Waibel</author><pubDate>Tue, 27 Feb 2024 15:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17633v1</guid></item><item><title>Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks</title><link>http://arxiv.org/abs/2402.17630v1</link><description>We study existing approaches to leverage off-the-shelf Natural LanguageInference (NLI) models for the evaluation of summary faithfulness and arguethat these are sub-optimal due to the granularity level considered for premisesand hypotheses. That is, the smaller content unit considered as hypothesis is asentence and premises are made up of a fixed number of document sentences. Wepropose a novel approach, namely InFusE, that uses a variable premise size andsimplifies summary sentences into shorter hypotheses. Departing from previousstudies which focus on single short document summarisation, we analyse NLIbased faithfulness evaluation for diverse summarisation tasks. We introduceDiverSumm, a new benchmark comprising long form summarisation (long documentsand summaries) and diverse summarisation tasks (e.g., meeting andmulti-document summarisation). In experiments, InFusE obtains superiorperformance across the different summarisation tasks. Our code and data areavailable at https://github.com/HJZnlp/infuse.</description><author>Huajian Zhang, Yumo Xu, Laura Perez-Beltrachini</author><pubDate>Tue, 27 Feb 2024 15:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17630v1</guid></item><item><title>CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing</title><link>http://arxiv.org/abs/2402.17624v1</link><description>Personalization techniques for large text-to-image (T2I) models allow usersto incorporate new concepts from reference images. However, existing methodsprimarily rely on textual descriptions, leading to limited control overcustomized images and failing to support fine-grained and local editing (e.g.,shape, pose, and details). In this paper, we identify sketches as an intuitiveand versatile representation that can facilitate such control, e.g., contourlines capturing shape information and flow lines representing texture. Thismotivates us to explore a novel task of sketch concept extraction: given one ormore sketch-image pairs, we aim to extract a special sketch concept thatbridges the correspondence between the images and sketches, thus enablingsketch-based image synthesis and editing at a fine-grained level. To accomplishthis, we introduce CustomSketching, a two-stage framework for extracting novelsketch concepts. Considering that an object can often be depicted by a contourfor general shapes and additional strokes for internal details, we introduce adual-sketch representation to reduce the inherent ambiguity in sketchdepiction. We employ a shape loss and a regularization loss to balance fidelityand editability during optimization. Through extensive experiments, a userstudy, and several applications, we show our method is effective and superiorto the adapted baselines.</description><author>Chufeng Xiao, Hongbo Fu</author><pubDate>Tue, 27 Feb 2024 15:52:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17624v1</guid></item><item><title>Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image Modeling</title><link>http://arxiv.org/abs/2402.17622v1</link><description>This work proposes a semantic segmentation network that produces high-qualityuncertainty estimates in a single forward pass. We exploit generalrepresentations from foundation models and unlabelled datasets through a MaskedImage Modeling (MIM) approach, which is robust to augmentation hyper-parametersand simpler than previous techniques. For neural networks used insafety-critical applications, bias in the training data can lead to errors;therefore it is crucial to understand a network's limitations at run time andact accordingly. To this end, we test our proposed method on a number of testdomains including the SAX Segmentation benchmark, which includes labelled testdata from dense urban, rural and off-road driving domains. The proposed methodconsistently outperforms uncertainty estimation and Out-of-Distribution (OoD)techniques on this difficult benchmark.</description><author>David S. W. Williams, Matthew Gadd, Paul Newman, Daniele De Martini</author><pubDate>Tue, 27 Feb 2024 15:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17622v1</guid></item><item><title>Supervised machine learning for microbiomics: bridging the gap between current and best practices</title><link>http://arxiv.org/abs/2402.17621v1</link><description>Machine learning (ML) is set to accelerate innovations in clinicalmicrobiomics, such as in disease diagnostics and prognostics. This will requirehigh-quality, reproducible, interpretable workflows whose predictivecapabilities meet or exceed the high thresholds set for clinical tools byregulatory agencies. Here, we capture a snapshot of current practices in theapplication of supervised ML to microbiomics data, through an in-depth analysisof 100 peer-reviewed journal articles published in 2021-2022. We apply adata-driven approach to steer discussion of the merits of varied approaches toexperimental design, including key considerations such as how to mitigate theeffects of small dataset size while avoiding data leakage. We further provideguidance on how to avoid common experimental design pitfalls that can hurtmodel performance, trustworthiness, and reproducibility. Discussion isaccompanied by an interactive online tutorial that demonstrates foundationalprinciples of ML experimental design, tailored to the microbiomics community.Formalizing community best practices for supervised ML in microbiomics is animportant step towards improving the success and efficiency of clinicalresearch, to the benefit of patients and other stakeholders.</description><author>Natasha K. Dudek, Mariam Chakhvadze, Saba Kobakhidze, Omar Kantidze, Yuriy Gankin</author><pubDate>Tue, 27 Feb 2024 15:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17621v1</guid></item><item><title>Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation</title><link>http://arxiv.org/abs/2402.17614v1</link><description>Few-shot segmentation performance declines substantially when facing imagesfrom a domain different than the training domain, effectively limitingreal-world use cases. To alleviate this, recently cross-domain few-shotsegmentation (CD-FSS) has emerged. Works that address this task mainlyattempted to learn segmentation on a source domain in a manner that generalizesacross domains. Surprisingly, we can outperform these approaches whileeliminating the training stage and removing their main segmentation network. Weshow test-time task-adaption is the key for successful CD-FSS instead.Task-adaption is achieved by appending small networks to the feature pyramid ofa conventionally classification-pretrained backbone. To avoid overfitting tothe few labeled samples in supervised fine-tuning, consistency across augmentedviews of input images serves as guidance while learning the parameters of theattached layers. Despite our self-restriction not to use any images other thanthe few labeled samples at test time, we achieve new state-of-the-artperformance in CD-FSS, evidencing the need to rethink approaches for the task.</description><author>Jonas Herzog</author><pubDate>Tue, 27 Feb 2024 15:43:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17614v1</guid></item><item><title>All the Feels: A dexterous hand with large-area tactile sensing</title><link>http://arxiv.org/abs/2210.15658v3</link><description>High cost and lack of reliability has precluded the widespread adoption ofdexterous hands in robotics. Furthermore, the lack of a viable tactile sensorcapable of sensing over the entire area of the hand impedes the rich, low-levelfeedback that would improve learning of dexterous manipulation skills. Thispaper introduces an inexpensive, modular, robust, and scalable platform -- theDManus -- aimed at resolving these challenges while satisfying the large-scaledata collection capabilities demanded by deep robot learning paradigms. Studieson human manipulation point to the criticality of low-level tactile feedback inperforming everyday dexterous tasks. The DManus comes with ReSkin sensing onthe entire surface of the palm as well as the fingertips. We demonstrateeffectiveness of the fully integrated system in a tactile aware task -- binpicking and sorting. Code, documentation, design files, detailed assemblyinstructions, trained models, task videos, and all supplementary materialsrequired to recreate the setup can be found onhttps://sites.google.com/view/roboticsbenchmarks/platforms/dmanus.</description><author>Raunaq Bhirangi, Abigail DeFranco, Jacob Adkins, Carmel Majidi, Abhinav Gupta, Tess Hellebrekers, Vikash Kumar</author><pubDate>Tue, 27 Feb 2024 15:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.15658v3</guid></item><item><title>Neural Automated Writing Evaluation with Corrective Feedback</title><link>http://arxiv.org/abs/2402.17613v1</link><description>The utilization of technology in second language learning and teaching hasbecome ubiquitous. For the assessment of writing specifically, automatedwriting evaluation (AWE) and grammatical error correction (GEC) have becomeimmensely popular and effective methods for enhancing writing proficiency anddelivering instant and individualized feedback to learners. By leveraging thepower of natural language processing (NLP) and machine learning algorithms, AWEand GEC systems have been developed separately to provide language learnerswith automated corrective feedback and more accurate and unbiased scoring thatwould otherwise be subject to examiners. In this paper, we propose anintegrated system for automated writing evaluation with corrective feedback asa means of bridging the gap between AWE and GEC results for second languagelearners. This system enables language learners to simulate the essay writingtests: a student writes and submits an essay, and the system returns theassessment of the writing along with suggested grammatical error corrections.Given that automated scoring and grammatical correction are more efficient andcost-effective than human grading, this integrated system would also alleviatethe burden of manually correcting innumerable essays.</description><author>Izia Xiaoxiao Wang, Xihan Wu, Edith Coates, Min Zeng, Jiexin Kuang, Siliang Liu, Mengyang Qiu, Jungyeul Park</author><pubDate>Tue, 27 Feb 2024 15:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17613v1</guid></item><item><title>A Large-scale Evaluation of Pretraining Paradigms for the Detection of Defects in Electroluminescence Solar Cell Images</title><link>http://arxiv.org/abs/2402.17611v1</link><description>Pretraining has been shown to improve performance in many domains, includingsemantic segmentation, especially in domains with limited labelled data. Inthis work, we perform a large-scale evaluation and benchmarking of variouspretraining methods for Solar Cell Defect Detection (SCDD) inelectroluminescence images, a field with limited labelled datasets. We coversupervised training with semantic segmentation, semi-supervised learning, andtwo self-supervised techniques. We also experiment with both in-distributionand out-of-distribution (OOD) pretraining and observe how this affectsdownstream performance. The results suggest that supervised training on a largeOOD dataset (COCO), self-supervised pretraining on a large OOD dataset(ImageNet), and semi-supervised pretraining (CCT) all yield statisticallyequivalent performance for mean Intersection over Union (mIoU). We achieve anew state-of-the-art for SCDD and demonstrate that certain pretraining schemesresult in superior performance on underrepresented classes. Additionally, weprovide a large-scale unlabelled EL image dataset of $22000$ images, and a$642$-image labelled semantic segmentation EL dataset, for further research indeveloping self- and semi-supervised training techniques in this domain.</description><author>David Torpey, Lawrence Pratt, Richard Klein</author><pubDate>Tue, 27 Feb 2024 15:37:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17611v1</guid></item><item><title>Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)</title><link>http://arxiv.org/abs/2402.17608v1</link><description>In this paper, we explore the impact of augmenting pre-trainedEncoder-Decoder models, specifically T5, with linguistic knowledge for theprediction of a target task. In particular, we investigate whether fine-tuninga T5 model on an intermediate task that predicts structural linguisticproperties of sentences modifies its performance in the target task ofpredicting sentence-level complexity. Our study encompasses diverse experimentsconducted on Italian and English datasets, employing both monolingual andmultilingual T5 models at various sizes. Results obtained for both languagesand in cross-lingual configurations show that linguistically motivatedintermediate fine-tuning has generally a positive impact on target taskperformance, especially when applied to smaller models and in scenarios withlimited data availability.</description><author>Alessio Miaschi, Felice Dell'Orletta, Giulia Venturi</author><pubDate>Tue, 27 Feb 2024 15:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17608v1</guid></item><item><title>NECO: NEural Collapse Based Out-of-distribution detection</title><link>http://arxiv.org/abs/2310.06823v3</link><description>Detecting out-of-distribution (OOD) data is a critical challenge in machinelearning due to model overconfidence, often without awareness of theirepistemological limits. We hypothesize that ``neural collapse'', a phenomenonaffecting in-distribution data for models trained beyond loss convergence, alsoinfluences OOD data. To benefit from this interplay, we introduce NECO, a novelpost-hoc method for OOD detection, which leverages the geometric properties of``neural collapse'' and of principal component spaces to identify OOD data. Ourextensive experiments demonstrate that NECO achieves state-of-the-art resultson both small and large-scale OOD detection tasks while exhibiting stronggeneralization capabilities across different network architectures.Furthermore, we provide a theoretical explanation for the effectiveness of ourmethod in OOD detection. Code is available at https://gitlab.com/drti/neco</description><author>Mouïn Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine Manzanera, Gianni Franchi</author><pubDate>Tue, 27 Feb 2024 15:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06823v3</guid></item><item><title>Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem</title><link>http://arxiv.org/abs/2402.17606v1</link><description>Existing learning-based methods for solving job shop scheduling problem(JSSP) usually use off-the-shelf GNN models tailored to undirected graphs andneglect the rich and meaningful topological structures of disjunctive graphs(DGs). This paper proposes the topology-aware bidirectional graph attentionnetwork (TBGAT), a novel GNN architecture based on the attention mechanism, toembed the DG for solving JSSP in a local search framework. Specifically, TBGATembeds the DG from a forward and a backward view, respectively, where themessages are propagated by following the different topologies of the views andaggregated via graph attention. Then, we propose a novel operator based on themessage-passing mechanism to calculate the forward and backward topologicalsorts of the DG, which are the features for characterizing the topologicalstructures and exploited by our model. In addition, we theoretically andexperimentally show that TBGAT has linear computational complexity to thenumber of jobs and machines, respectively, which strengthens the practicalvalue of our method. Besides, extensive experiments on five synthetic datasetsand seven classic benchmarks show that TBGAT achieves new SOTA results byoutperforming a wide range of neural methods by a large margin.</description><author>Cong Zhang, Zhiguang Cao, Yaoxin Wu, Wen Song, Jing Sun</author><pubDate>Tue, 27 Feb 2024 15:33:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17606v1</guid></item><item><title>Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach</title><link>http://arxiv.org/abs/2402.17601v1</link><description>Understanding sleep and activity patterns plays a crucial role in physicaland mental health. This study introduces a novel approach for sleep detectionusing weakly supervised learning for scenarios where reliable ground truthlabels are unavailable. The proposed method relies on a set of weak labels,derived from the predictions generated by conventional sleep detectionalgorithms. Introducing a novel approach, we suggest a novel generalisednon-linear statistical model in which the number of weak sleep labels ismodelled as outcome of a binomial distribution. The probability of sleep in thebinomial distribution is linked to the outcomes of neural networks trained todetect sleep based on actigraphy. We show that maximizing the likelihoodfunction of the model, is equivalent to minimizing the soft cross-entropy loss.Additionally, we explored the use of the Brier score as a loss function forweak labels. The efficacy of the suggested modelling framework was demonstratedusing the Multi-Ethnic Study of Atherosclerosis dataset. A \gls{lstm} trainedon the soft cross-entropy outperformed conventional sleep detection algorithms,other neural network architectures and loss functions in accuracy and modelcalibration. This research not only advances sleep detection techniques inscenarios where ground truth data is scarce but also contributes to the broaderfield of weakly supervised learning by introducing innovative approach inmodelling sets of weak labels.</description><author>Matthias Boeker, Vajira Thambawita, Michael Riegler, Pål Halvorsen, Hugo L. Hammer</author><pubDate>Tue, 27 Feb 2024 15:30:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17601v1</guid></item><item><title>Implicit Regularization via Spectral Neural Networks and Non-linear Matrix Sensing</title><link>http://arxiv.org/abs/2402.17595v1</link><description>The phenomenon of implicit regularization has attracted interest in recentyears as a fundamental aspect of the remarkable generalizing ability of neuralnetworks. In a nutshell, it entails that gradient descent dynamics in manyneural nets, even without any explicit regularizer in the loss function,converges to the solution of a regularized learning problem. However, knownresults attempting to theoretically explain this phenomenon focusoverwhelmingly on the setting of linear neural nets, and the simplicity of thelinear structure is particularly crucial to existing arguments. In this paper,we explore this problem in the context of more realistic neural networks with ageneral class of non-linear activation functions, and rigorously demonstratethe implicit regularization phenomenon for such networks in the setting ofmatrix sensing problems, together with rigorous rate guarantees that ensureexponentially fast convergence of gradient descent.In this vein, we contributea network architecture called Spectral Neural Networks (abbrv. SNN) that isparticularly suitable for matrix learning problems. Conceptually, this entailscoordinatizing the space of matrices by their singular values and singularvectors, as opposed to by their entries, a potentially fruitful perspective formatrix learning. We demonstrate that the SNN architecture is inherently muchmore amenable to theoretical analysis than vanilla neural nets and confirm itseffectiveness in the context of matrix sensing, via both mathematicalguarantees and empirical investigations. We believe that the SNN architecturehas the potential to be of wide applicability in a broad class of matrixlearning scenarios.</description><author>Hong T. M. Chu, Subhro Ghosh, Chi Thanh Lam, Soumendu Sundar Mukherjee</author><pubDate>Tue, 27 Feb 2024 15:28:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17595v1</guid></item><item><title>DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes</title><link>http://arxiv.org/abs/2312.07920v2</link><description>We present DrivingGaussian, an efficient and effective framework forsurrounding dynamic autonomous driving scenes. For complex scenes with movingobjects, we first sequentially and progressively model the static background ofthe entire scene with incremental static 3D Gaussians. We then leverage acomposite dynamic Gaussian graph to handle multiple moving objects,individually reconstructing each object and restoring their accurate positionsand occlusion relationships within the scene. We further use a LiDAR prior forGaussian Splatting to reconstruct scenes with greater details and maintainpanoramic consistency. DrivingGaussian outperforms existing methods in drivingscene reconstruction and enables photorealistic surround-view synthesis withhigh-fidelity and multi-camera consistency. Our project page is at:https://github.com/VDIGPKU/DrivingGaussian.</description><author>Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</author><pubDate>Tue, 27 Feb 2024 15:26:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07920v2</guid></item></channel></rss>