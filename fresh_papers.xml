<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 12 Sep 2023 06:00:13 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Robot Parkour Learning</title><link>http://arxiv.org/abs/2309.05665v1</link><description>Parkour is a grand challenge for legged locomotion that requires robots toovercome various obstacles rapidly in complex environments. Existing methodscan generate either diverse but blind locomotion skills or vision-based butspecialized skills by using reference animal data or complex rewards. However,autonomous parkour requires robots to learn generalizable skills that are bothvision-based and diverse to perceive and react to various scenarios. In thiswork, we propose a system for learning a single end-to-end vision-based parkourpolicy of diverse parkour skills using a simple reward without any referencemotion data. We develop a reinforcement learning method inspired by directcollocation to generate parkour skills, including climbing over high obstacles,leaping over large gaps, crawling beneath low barriers, squeezing through thinslits, and running. We distill these skills into a single vision-based parkourpolicy and transfer it to a quadrupedal robot using its egocentric depthcamera. We demonstrate that our system can empower two different low-costrobots to autonomously select and execute appropriate parkour skills totraverse challenging real-world environments.</description><author>Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, Hang Zhao</author><pubDate>Mon, 11 Sep 2023 18:59:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05665v1</guid></item><item><title>Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips</title><link>http://arxiv.org/abs/2309.05663v1</link><description>We tackle the task of reconstructing hand-object interactions from shortvideo clips. Given an input video, our approach casts 3D inference as aper-video optimization and recovers a neural 3D representation of the objectshape, as well as the time-varying motion and hand articulation. While theinput video naturally provides some multi-view cues to guide 3D inference,these are insufficient on their own due to occlusions and limited viewpointvariations. To obtain accurate 3D, we augment the multi-view signals withgeneric data-driven priors to guide reconstruction. Specifically, we learn adiffusion network to model the conditional distribution of (geometric)renderings of objects conditioned on hand configuration and category label, andleverage it as a prior to guide the novel-view renderings of the reconstructedscene. We empirically evaluate our approach on egocentric videos across 6object categories, and observe significant improvements over prior single-viewand multi-view methods. Finally, we demonstrate our system's ability toreconstruct arbitrary clips from YouTube, showing both 1st and 3rd personinteractions.</description><author>Yufei Ye, Poorvi Hebbar, Abhinav Gupta, Shubham Tulsiani</author><pubDate>Mon, 11 Sep 2023 18:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05663v1</guid></item><item><title>ViHOPE: Visuotactile In-Hand Object 6D Pose Estimation with Shape Completion</title><link>http://arxiv.org/abs/2309.05662v1</link><description>In this letter, we introduce ViHOPE, a novel framework for estimating the 6Dpose of an in-hand object using visuotactile perception. Our key insight isthat the accuracy of the 6D object pose estimate can be improved by explicitlycompleting the shape of the object. To this end, we introduce a novelvisuotactile shape completion module that uses a conditional GenerativeAdversarial Network to complete the shape of an in-hand object based onvolumetric representation. This approach improves over prior works thatdirectly regress visuotactile observations to a 6D pose. By explicitlycompleting the shape of the in-hand object and jointly optimizing the shapecompletion and pose estimation tasks, we improve the accuracy of the 6D objectpose estimate. We train and test our model on a synthetic dataset and compareit with the state-of-the-art. In the visuotactile shape completion task, weoutperform the state-of-the-art by 265% using the Intersection of Union metricand achieve 88% lower Chamfer Distance. In the visuotactile pose estimationtask, we present results that suggest our framework reduces position andangular errors by 35% and 64%, respectively. Furthermore, we ablate ourframework to confirm the gain on the 6D object pose estimate from explicitlycompleting the shape. Ultimately, we show that our framework produces modelsthat are robust to sim-to-real transfer on a real-world robot platform.</description><author>Hongyu Li, Snehal Dikhale, Soshi Iba, Nawid Jamali</author><pubDate>Mon, 11 Sep 2023 18:58:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05662v1</guid></item><item><title>Tell me what you see: A zero-shot action recognition method based on natural language descriptions</title><link>http://arxiv.org/abs/2112.09976v2</link><description>This paper presents a novel approach to Zero-Shot Action Recognition. Recentworks have explored the detection and classification of objects to obtainsemantic information from videos with remarkable performance. Inspired by them,we propose using video captioning methods to extract semantic information aboutobjects, scenes, humans, and their relationships. To the best of our knowledge,this is the first work to represent both videos and labels with descriptivesentences. More specifically, we represent videos using sentences generated viavideo captioning methods and classes using sentences extracted from documentsacquired through search engines on the Internet. Using these representations,we build a shared semantic space employing BERT-based embedders pre-trained inthe paraphrasing task on multiple text datasets. The projection of both visualand semantic information onto this space is straightforward, as they aresentences, enabling classification using the nearest neighbor rule. Wedemonstrate that representing videos and labels with sentences alleviates thedomain adaptation problem. Additionally, we show that word vectors areunsuitable for building the semantic embedding space of our descriptions. Ourmethod outperforms the state-of-the-art performance on the UCF101 dataset by3.3 p.p. in accuracy under the TruZe protocol and achieves competitive resultson both the UCF101 and HMDB51 datasets under the conventional protocol (0/50\%- training/testing split). Our code is available athttps://github.com/valterlej/zsarcap.</description><author>Valter Estevam, Rayson Laroca, David Menotti, Helio Pedrini</author><pubDate>Mon, 11 Sep 2023 18:57:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.09976v2</guid></item><item><title>Hypothesis Search: Inductive Reasoning with Language Models</title><link>http://arxiv.org/abs/2309.05660v1</link><description>Inductive reasoning is a core problem-solving capacity: humans can identifyunderlying principles from a few examples, which can then be robustlygeneralized to novel scenarios. Recent work has evaluated large language models(LLMs) on inductive reasoning tasks by directly prompting them yielding "incontext learning." This can work well for straightforward inductive tasks, butperforms very poorly on more complex tasks such as the Abstraction andReasoning Corpus (ARC). In this work, we propose to improve the inductivereasoning ability of LLMs by generating explicit hypotheses at multiple levelsof abstraction: we prompt the LLM to propose multiple abstract hypotheses aboutthe problem, in natural language, then implement the natural languagehypotheses as concrete Python programs. These programs can be directly verifiedby running on the observed examples and generalized to novel inputs. Because ofthe prohibitive cost of generation with state-of-the-art LLMs, we consider amiddle step to filter the set of hypotheses that will be implemented intoprograms: we either ask the LLM to summarize into a smaller set of hypotheses,or ask human annotators to select a subset of the hypotheses. We verify ourpipeline's effectiveness on the ARC visual inductive reasoning benchmark, itsvariant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problemsubset of ARC, our automated pipeline using LLM summaries achieves 27.5%accuracy, significantly outperforming the direct prompting baseline (accuracyof 12.5%). With the minimal human input of selecting from LLM-generatedcandidates, the performance is boosted to 37.5%. (And we argue this is a lowerbound on the performance of our approach without filtering.) Our ablationstudies show that abstract hypothesis generation and concrete programrepresentations are both beneficial for LLMs to perform inductive reasoningtasks.</description><author>Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D. Goodman</author><pubDate>Mon, 11 Sep 2023 18:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05660v1</guid></item><item><title>The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification</title><link>http://arxiv.org/abs/2305.04940v2</link><description>The use of modern Natural Language Processing (NLP) techniques has shown tobe beneficial for software engineering tasks, such as vulnerability detectionand type inference. However, training deep NLP models requires significantcomputational resources. This paper explores techniques that aim at achievingthe best usage of resources and available information in these models. We propose a generic approach, EarlyBIRD, to build composite representationsof code from the early layers of a pre-trained transformer model. Weempirically investigate the viability of this approach on the CodeBERT model bycomparing the performance of 12 strategies for creating compositerepresentations with the standard practice of only using the last encoderlayer. Our evaluation on four datasets shows that several early layer combinationsyield better performance on defect detection, and some combinations improvemulti-class classification. More specifically, we obtain a +2 averageimprovement of detection accuracy on Devign with only 3 out of 12 layers ofCodeBERT and a 3.3x speed-up of fine-tuning. These findings show that earlylayers can be used to obtain better results using the same resources, as wellas to reduce resource usage during fine-tuning and inference.</description><author>Anastasiia Grishina, Max Hort, Leon Moonen</author><pubDate>Mon, 11 Sep 2023 18:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04940v2</guid></item><item><title>Distribution-Aligned Diffusion for Human Mesh Recovery</title><link>http://arxiv.org/abs/2308.13369v2</link><description>Recovering a 3D human mesh from a single RGB image is a challenging task dueto depth ambiguity and self-occlusion, resulting in a high degree ofuncertainty. Meanwhile, diffusion models have recently seen much success ingenerating high-quality outputs by progressively denoising noisy inputs.Inspired by their capability, we explore a diffusion-based approach for humanmesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework whichframes mesh recovery as a reverse diffusion process. We also propose aDistribution Alignment Technique (DAT) that infuses prior distributioninformation into the mesh distribution diffusion process, and provides usefulprior knowledge to facilitate the mesh recovery task. Our method achievesstate-of-the-art performance on three widely used datasets. Project page:https://gongjia0208.github.io/HMDiff/.</description><author>Lin Geng Foo, Jia Gong, Hossein Rahmani, Jun Liu</author><pubDate>Mon, 11 Sep 2023 18:55:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13369v2</guid></item><item><title>An Overview of Catastrophic AI Risks</title><link>http://arxiv.org/abs/2306.12001v5</link><description>Rapid advancements in artificial intelligence (AI) have sparked growingconcerns among experts, policymakers, and world leaders regarding the potentialfor increasingly advanced AI systems to pose catastrophic risks. Althoughnumerous risks have been detailed separately, there is a pressing need for asystematic discussion and illustration of the potential dangers to betterinform efforts to mitigate them. This paper provides an overview of the mainsources of catastrophic AI risks, which we organize into four categories:malicious use, in which individuals or groups intentionally use AIs to causeharm; AI race, in which competitive environments compel actors to deploy unsafeAIs or cede control to AIs; organizational risks, highlighting how humanfactors and complex systems can increase the chances of catastrophic accidents;and rogue AIs, describing the inherent difficulty in controlling agents farmore intelligent than humans. For each category of risk, we describe specifichazards, present illustrative stories, envision ideal scenarios, and proposepractical suggestions for mitigating these dangers. Our goal is to foster acomprehensive understanding of these risks and inspire collective and proactiveefforts to ensure that AIs are developed and deployed in a safe manner.Ultimately, we hope this will allow us to realize the benefits of this powerfultechnology while minimizing the potential for catastrophic outcomes.</description><author>Dan Hendrycks, Mantas Mazeika, Thomas Woodside</author><pubDate>Mon, 11 Sep 2023 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12001v5</guid></item><item><title>On the quality of randomized approximations of Tukey's depth</title><link>http://arxiv.org/abs/2309.05657v1</link><description>Tukey's depth (or halfspace depth) is a widely used measure of centrality formultivariate data. However, exact computation of Tukey's depth is known to be ahard problem in high dimensions. As a remedy, randomized approximations ofTukey's depth have been proposed. In this paper we explore when such randomizedalgorithms return a good approximation of Tukey's depth. We study the case whenthe data are sampled from a log-concave isotropic distribution. We prove that,if one requires that the algorithm runs in polynomial time in the dimension,the randomized algorithm correctly approximates the maximal depth $1/2$ anddepths close to zero. On the other hand, for any point of intermediate depth,any good approximation requires exponential complexity.</description><author>Simon Briend, Gábor Lugosi, Roberto Imbuzeiro Oliveira</author><pubDate>Mon, 11 Sep 2023 18:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05657v1</guid></item><item><title>Dynamic Handover: Throw and Catch with Bimanual Hands</title><link>http://arxiv.org/abs/2309.05655v1</link><description>Humans throw and catch objects all the time. However, such a seemingly commonskill introduces a lot of challenges for robots to achieve: The robots need tooperate such dynamic actions at high-speed, collaborate precisely, and interactwith diverse objects. In this paper, we design a system with two multi-fingerhands attached to robot arms to solve this problem. We train our system usingMulti-Agent Reinforcement Learning in simulation and perform Sim2Real transferto deploy on the real robots. To overcome the Sim2Real gap, we provide multiplenovel algorithm designs including learning a trajectory prediction model forthe object. Such a model can help the robot catcher has a real-time estimationof where the object will be heading, and then react accordingly. We conduct ourexperiments with multiple objects in the real-world system, and showsignificant improvements over multiple baselines. Our project page is availableat \url{https://binghao-huang.github.io/dynamic_handover/}.</description><author>Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Qin, Yaodong Yang, Nikolay Atanasov, Xiaolong Wang</author><pubDate>Mon, 11 Sep 2023 18:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05655v1</guid></item><item><title>MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning</title><link>http://arxiv.org/abs/2309.05653v1</link><description>We introduce MAmmoTH, a series of open-source large language models (LLMs)specifically tailored for general math problem-solving. The MAmmoTH models aretrained on MathInstruct, our meticulously curated instruction tuning dataset.MathInstruct is compiled from 13 math datasets with intermediate rationales,six of which have rationales newly curated by us. It presents a unique hybridof chain-of-thought (CoT) and program-of-thought (PoT) rationales, and alsoensures extensive coverage of diverse fields in math. The hybrid of CoT and PoTnot only unleashes the potential of tool use but also allows different thoughtprocesses for different math problems. As a result, the MAmmoTH seriessubstantially outperform existing open-source models on nine mathematicalreasoning datasets across all scales with an average accuracy gain between 13%and 29%. Remarkably, our MAmmoTH-7B model reaches 35% on MATH (acompetition-level dataset), which exceeds the best open-source 7B model(WizardMath) by 25%, and the MAmmoTH-34B model achieves 46% accuracy on MATH,even surpassing GPT-4's CoT result. Our work underscores the importance ofdiverse problem coverage and the use of hybrid rationales in developingsuperior math generalist models.</description><author>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen</author><pubDate>Mon, 11 Sep 2023 18:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05653v1</guid></item><item><title>An Effective Two-stage Training Paradigm Detector for Small Dataset</title><link>http://arxiv.org/abs/2309.05652v1</link><description>Learning from the limited amount of labeled data to the pre-train model hasalways been viewed as a challenging task. In this report, an effective androbust solution, the two-stage training paradigm YOLOv8 detector (TP-YOLOv8),is designed for the object detection track in VIPriors Challenge 2023. First,the backbone of YOLOv8 is pre-trained as the encoder using the masked imagemodeling technique. Then the detector is fine-tuned with elaborateaugmentations. During the test stage, test-time augmentation (TTA) is used toenhance each model, and weighted box fusion (WBF) is implemented to furtherboost the performance. With the well-designed structure, our approach hasachieved 30.4% average precision from 0.50 to 0.95 on the DelftBikes test set,ranking 4th on the leaderboard.</description><author>Zheng Wang, Dong Xie, Hanzhi Wang, Jiang Tian</author><pubDate>Mon, 11 Sep 2023 18:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05652v1</guid></item><item><title>A Novel Supervised Deep Learning Solution to Detect Distributed Denial of Service (DDoS) attacks on Edge Systems using Convolutional Neural Networks (CNN)</title><link>http://arxiv.org/abs/2309.05646v1</link><description>Cybersecurity attacks are becoming increasingly sophisticated and pose agrowing threat to individuals, and private and public sectors. DistributedDenial of Service attacks are one of the most harmful of these threats intoday's internet, disrupting the availability of essential services. Thisproject presents a novel deep learning-based approach for detecting DDoSattacks in network traffic using the industry-recognized DDoS evaluationdataset from the University of New Brunswick, which contains packet capturesfrom real-time DDoS attacks, creating a broader and more applicable model forthe real world. The algorithm employed in this study exploits the properties ofConvolutional Neural Networks (CNN) and common deep learning algorithms tobuild a novel mitigation technique that classifies benign and malicioustraffic. The proposed model preprocesses the data by extracting packet flowsand normalizing them to a fixed length which is fed into a custom architecturecontaining layers regulating node dropout, normalization, and a sigmoidactivation function to out a binary classification. This allows for the modelto process the flows effectively and look for the nodes that contribute to DDoSattacks while dropping the "noise" or the distractors. The results of thisstudy demonstrate the effectiveness of the proposed algorithm in detecting DDOSattacks, achieving an accuracy of .9883 on 2000 unseen flows in networktraffic, while being scalable for any network environment.</description><author>Vedanth Ramanathan, Krish Mahadevan, Sejal Dua</author><pubDate>Mon, 11 Sep 2023 18:37:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05646v1</guid></item><item><title>CitDet: A Benchmark Dataset for Citrus Fruit Detection</title><link>http://arxiv.org/abs/2309.05645v1</link><description>In this letter, we present a new dataset to advance the state of the art indetecting citrus fruit and accurately estimate yield on trees affected by theHuanglongbing (HLB) disease in orchard environments via imaging. Despite thefact that significant progress has been made in solving the fruit detectionproblem, the lack of publicly available datasets has complicated directcomparison of results. For instance, citrus detection has long been of interestin the agricultural research community, yet there is an absence of work,particularly involving public datasets of citrus affected by HLB. To addressthis issue, we enhance state-of-the-art object detection methods for use intypical orchard settings. Concretely, we provide high-resolution images ofcitrus trees located in an area known to be highly affected by HLB, along withhigh-quality bounding box annotations of citrus fruit. Fruit on both the treesand the ground are labeled to allow for identification of fruit location, whichcontributes to advancements in yield estimation and potential measure of HLBimpact via fruit drop. The dataset consists of over 32,000 bounding boxannotations for fruit instances contained in 579 high-resolution images. Insummary, our contributions are the following: (i) we introduce a novel datasetalong with baseline performance benchmarks on multiple contemporary objectdetection algorithms, (ii) we show the ability to accurately capture fruitlocation on tree or on ground, and finally (ii) we present a correlation of ourresults with yield estimations.</description><author>Jordan A. James, Heather K. Manching, Matthew R. Mattia, Kim D. Bowman, Amanda M. Hulse-Kemp, William J. Beksi</author><pubDate>Mon, 11 Sep 2023 18:37:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05645v1</guid></item><item><title>Combinative Cumulative Knowledge Processes</title><link>http://arxiv.org/abs/2309.05638v1</link><description>We analyze Cumulative Knowledge Processes, introduced by Ben-Eliezer,Mikulincer, Mossel, and Sudan (ITCS 2023), in the setting of "directed acyclicgraphs", i.e., when new units of knowledge may be derived by combining multipleprevious units of knowledge. The main considerations in this model are the roleof errors (when new units may be erroneous) and local checking (where a fewantecedent units of knowledge are checked when a new unit of knowledge isdiscovered). The aforementioned work defined this model but only analyzed anidealized and simplified "tree-like" setting, i.e., a setting where new unitsof knowledge only depended directly on one previously generated unit ofknowledge. The main goal of our work is to understand when the general process is safe,i.e., when the effect of errors remains under control. We provide somenecessary and some sufficient conditions for safety. As in the earlier work, wedemonstrate that the frequency of checking as well as the depth of the checksplay a crucial role in determining safety. A key new parameter in the currentwork is the $\textit{combination factor}$ which is the distribution of thenumber of units $M$ of old knowledge that a new unit of knowledge depends on.Our results indicate that a large combination factor can compensate for a smalldepth of checking. The dependency of the safety on the combination factor isfar from trivial. Indeed some of our main results are stated in terms of$\mathbb{E}\{1/M\}$ while others depend on $\mathbb{E}\{M\}$.</description><author>Anna Brandenberger, Cassandra Marcussen, Elchanan Mossel, Madhu Sudan</author><pubDate>Mon, 11 Sep 2023 18:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05638v1</guid></item><item><title>Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback</title><link>http://arxiv.org/abs/2307.15217v2</link><description>Reinforcement learning from human feedback (RLHF) is a technique for trainingAI systems to align with human goals. RLHF has emerged as the central methodused to finetune state-of-the-art large language models (LLMs). Despite thispopularity, there has been relatively little public work systematizing itsflaws. In this paper, we (1) survey open problems and fundamental limitationsof RLHF and related methods; (2) overview techniques to understand, improve,and complement RLHF in practice; and (3) propose auditing and disclosurestandards to improve societal oversight of RLHF systems. Our work emphasizesthe limitations of RLHF and highlights the importance of a multi-facetedapproach to the development of safer AI systems.</description><author>Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell</author><pubDate>Mon, 11 Sep 2023 18:25:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15217v2</guid></item><item><title>Task-Based MoE for Multitask Multilingual Machine Translation</title><link>http://arxiv.org/abs/2308.15772v2</link><description>Mixture-of-experts (MoE) architecture has been proven a powerful method fordiverse tasks in training deep models in many applications. However, currentMoE implementations are task agnostic, treating all tokens from different tasksin the same manner. In this work, we instead design a novel method thatincorporates task information into MoE models at different granular levels withshared dynamic task-based adapters. Our experiments and analysis show theadvantages of our approaches over the dense and canonical MoE models onmulti-task multilingual machine translations. With task-specific adapters, ourmodels can additionally generalize to new tasks efficiently.</description><author>Hai Pham, Young Jin Kim, Subhabrata Mukherjee, David P. Woodruff, Barnabas Poczos, Hany Hassan Awadalla</author><pubDate>Mon, 11 Sep 2023 18:22:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15772v2</guid></item><item><title>Boundary Peeling: Outlier Detection Method Using One-Class Peeling</title><link>http://arxiv.org/abs/2309.05630v1</link><description>Unsupervised outlier detection constitutes a crucial phase within dataanalysis and remains a dynamic realm of research. A good outlier detectionalgorithm should be computationally efficient, robust to tuning parameterselection, and perform consistently well across diverse underlying datadistributions. We introduce One-Class Boundary Peeling, an unsupervised outlierdetection algorithm. One-class Boundary Peeling uses the average signeddistance from iteratively-peeled, flexible boundaries generated by one-classsupport vector machines. One-class Boundary Peeling has robust hyperparametersettings and, for increased flexibility, can be cast as an ensemble method. Insynthetic data simulations One-Class Boundary Peeling outperforms all state ofthe art methods when no outliers are present while maintaining comparable orsuperior performance in the presence of outliers, as compared to benchmarkmethods. One-Class Boundary Peeling performs competitively in terms of correctclassification, AUC, and processing time using common benchmark data sets.</description><author>Sheikh Arafat, Na Sun, Maria L. Weese, Waldyn G. Martinez</author><pubDate>Mon, 11 Sep 2023 18:19:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05630v1</guid></item><item><title>A soft nearest-neighbor framework for continual semi-supervised learning</title><link>http://arxiv.org/abs/2212.05102v3</link><description>Despite significant advances, the performance of state-of-the-art continuallearning approaches hinges on the unrealistic scenario of fully labeled data.In this paper, we tackle this challenge and propose an approach for continualsemi-supervised learning--a setting where not all the data samples are labeled.A primary issue in this scenario is the model forgetting representations ofunlabeled data and overfitting the labeled samples. We leverage the power ofnearest-neighbor classifiers to nonlinearly partition the feature space andflexibly model the underlying data distribution thanks to its non-parametricnature. This enables the model to learn a strong representation for the currenttask, and distill relevant information from previous tasks. We perform athorough experimental evaluation and show that our method outperforms all theexisting approaches by large margins, setting a solid state of the art on thecontinual semi-supervised learning paradigm. For example, on CIFAR-100 wesurpass several others even when using at least 30 times less supervision (0.8%vs. 25% of annotations). Finally, our method works well on both low and highresolution images and scales seamlessly to more complex datasets such asImageNet-100. The code is publicly available onhttps://github.com/kangzhiq/NNCSL</description><author>Zhiqi Kang, Enrico Fini, Moin Nabi, Elisa Ricci, Karteek Alahari</author><pubDate>Mon, 11 Sep 2023 18:09:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05102v3</guid></item><item><title>InVAErt networks: a data-driven framework for model synthesis and identifiability analysis</title><link>http://arxiv.org/abs/2307.12586v2</link><description>Use of generative models and deep learning for physics-based systems iscurrently dominated by the task of emulation. However, the remarkableflexibility offered by data-driven architectures would suggest to extend thisrepresentation to other aspects of system synthesis including model inversionand identifiability. We introduce inVAErt (pronounced "invert") networks, acomprehensive framework for data-driven analysis and synthesis of parametricphysical systems which uses a deterministic encoder and decoder to representthe forward and inverse solution maps, a normalizing flow to capture theprobabilistic distribution of system outputs, and a variational encoderdesigned to learn a compact latent representation for the lack of bijectivitybetween inputs and outputs. We formally investigate the selection of penaltycoefficients in the loss function and strategies for latent space sampling,since we find that these significantly affect both training and testingperformance. We validate our framework through extensive numerical examples,including simple linear, nonlinear, and periodic maps, dynamical systems, andspatio-temporal PDEs.</description><author>Guoxiang Grayson Tong, Carlos A. Sing Long, Daniele E. Schiavazzi</author><pubDate>Mon, 11 Sep 2023 18:08:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12586v2</guid></item><item><title>Effective Proxy for Human Labeling: Ensemble Disagreement Scores in Large Language Models for Industrial NLP</title><link>http://arxiv.org/abs/2309.05619v1</link><description>Large language models (LLMs) have demonstrated significant capability togeneralize across a large number of NLP tasks. For industry applications, it isimperative to assess the performance of the LLM on unlabeled production datafrom time to time to validate for a real-world setting. Human labeling toassess model error requires considerable expense and time delay. Here wedemonstrate that ensemble disagreement scores work well as a proxy for humanlabeling for language models in zero-shot, few-shot, and fine-tuned settings,per our evaluation on keyphrase extraction (KPE) task. We measure fidelity ofthe results by comparing to true error measured from human labeled groundtruth. We contrast with the alternative of using another LLM as a source ofmachine labels, or silver labels. Results across various languages and domainsshow disagreement scores provide a better estimation of model performance withmean average error (MAE) as low as 0.4% and on average 13.8% better than usingsilver labels.</description><author>Wei Du, Laksh Advani, Yashmeet Gambhir, Daniel J Perry, Prashant Shiralkar, Zhengzheng Xing, Aaron Colak</author><pubDate>Mon, 11 Sep 2023 18:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05619v1</guid></item><item><title>SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling</title><link>http://arxiv.org/abs/2303.17368v2</link><description>Synthetic data has emerged as a promising source for 3D human research as itoffers low-cost access to large-scale human datasets. To advance the diversityand annotation quality of human models, we introduce a new synthetic dataset,SynBody, with three appealing features: 1) a clothed parametric human modelthat can generate a diverse range of subjects; 2) the layered humanrepresentation that naturally offers high-quality 3D annotations to supportmultiple tasks; 3) a scalable system for producing realistic data to facilitatereal-world tasks. The dataset comprises 1.2M images with corresponding accurate3D annotations, covering 10,000 human body models, 1,187 actions, and variousviewpoints. The dataset includes two subsets for human pose and shapeestimation as well as human neural rendering. Extensive experiments on SynBodyindicate that it substantially enhances both SMPL and SMPL-X estimation.Furthermore, the incorporation of layered annotations offers a valuabletraining resource for investigating the Human Neural Radiance Fields (NeRF).</description><author>Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai, Wayne Wu, Chen Qian, Dahua Lin, Ziwei Liu, Lei Yang</author><pubDate>Mon, 11 Sep 2023 18:06:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17368v2</guid></item><item><title>Understanding Sinusoidal Neural Networks</title><link>http://arxiv.org/abs/2212.01833v2</link><description>In this work, we investigate the structure and representation capacity ofsinusoidal MLPs - multilayer perceptron networks that use sine as theactivation function. These neural networks (known as neural fields) have becomefundamental in representing common signals in computer graphics, such asimages, signed distance functions, and radiance fields. This success can beprimarily attributed to two key properties of sinusoidal MLPs: smoothness andcompactness. These functions are smooth because they arise from the compositionof affine maps with the sine function. This work provides theoretical resultsto justify the compactness property of sinusoidal MLPs and provides controlmechanisms in the definition and training of these networks. We propose to study a sinusoidal MLP by expanding it as a harmonic sum.First, we observe that its first layer can be seen as a harmonic dictionary,which we call the input sinusoidal neurons. Then, a hidden layer combines thisdictionary using an affine map and modulates the outputs using the sine, thisresults in a special dictionary of sinusoidal neurons. We prove that each ofthese sinusoidal neurons expands as a harmonic sum producing a large number ofnew frequencies expressed as integer linear combinations of the inputfrequencies. Thus, each hidden neuron produces the same frequencies, and thecorresponding amplitudes are completely determined by the hidden affine map. Wealso provide an upper bound and a way of sorting these amplitudes that cancontrol the resulting approximation, allowing us to truncate the correspondingseries. Finally, we present applications for training and initialization ofsinusoidal MLPs. Additionally, we show that if the input neurons are periodic,then the entire network will be periodic with the same period. We relate theseperiodic networks with the Fourier series representation.</description><author>Tiago Novello</author><pubDate>Mon, 11 Sep 2023 18:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01833v2</guid></item><item><title>Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy</title><link>http://arxiv.org/abs/2210.17546v3</link><description>Studying data memorization in neural language models helps us understand therisks (e.g., to privacy or copyright) associated with models regurgitatingtraining data and aids in the development of countermeasures. Many prior works-- and some recently deployed defenses -- focus on "verbatim memorization",defined as a model generation that exactly matches a substring from thetraining set. We argue that verbatim memorization definitions are toorestrictive and fail to capture more subtle forms of memorization.Specifically, we design and implement an efficient defense that perfectlyprevents all verbatim memorization. And yet, we demonstrate that this "perfect"filter does not prevent the leakage of training data. Indeed, it is easilycircumvented by plausible and minimally modified "style-transfer" prompts --and in some cases even the non-modified original prompts -- to extractmemorized information. We conclude by discussing potential alternativedefinitions and why defining memorization is a difficult yet crucial openquestion for neural language models.</description><author>Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A. Choquette-Choo, Nicholas Carlini</author><pubDate>Mon, 11 Sep 2023 17:58:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.17546v3</guid></item><item><title>Learning the Geodesic Embedding with Graph Neural Networks</title><link>http://arxiv.org/abs/2309.05613v1</link><description>We present GeGnn, a learning-based method for computing the approximategeodesic distance between two arbitrary points on discrete polyhedra surfaceswith constant time complexity after fast precomputation. Previous relevantmethods either focus on computing the geodesic distance between a single sourceand all destinations, which has linear complexity at least or require a longprecomputation time. Our key idea is to train a graph neural network to embedan input mesh into a high-dimensional embedding space and compute the geodesicdistance between a pair of points using the corresponding embedding vectors anda lightweight decoding function. To facilitate the learning of the embedding,we propose novel graph convolution and graph pooling modules that incorporatelocal geodesic information and are verified to be much more effective thanprevious designs. After training, our method requires only one forward pass ofthe network per mesh as precomputation. Then, we can compute the geodesicdistance between a pair of points using our decoding function, which requiresonly several matrix multiplications and can be massively parallelized on GPUs.We verify the efficiency and effectiveness of our method on ShapeNet anddemonstrate that our method is faster than existing methods by orders ofmagnitude while achieving comparable or better accuracy. Additionally, ourmethod exhibits robustness on noisy and incomplete meshes and stronggeneralization ability on out-of-distribution meshes. The code and pretrainedmodel can be found on https://github.com/IntelligentGeometry/GeGnn.</description><author>Bo Pang, Zhongtian Zheng, Guoping Wang, Peng-Shuai Wang</author><pubDate>Mon, 11 Sep 2023 17:54:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05613v1</guid></item><item><title>Privacy Side Channels in Machine Learning Systems</title><link>http://arxiv.org/abs/2309.05610v1</link><description>Most current approaches for protecting privacy in machine learning (ML)assume that models exist in a vacuum, when in reality, ML models are part oflarger systems that include components for training data filtering, outputmonitoring, and more. In this work, we introduce privacy side channels: attacksthat exploit these system-level components to extract private information atfar higher rates than is otherwise possible for standalone models. We proposefour categories of side channels that span the entire ML lifecycle (trainingdata filtering, input preprocessing, output post-processing, and queryfiltering) and allow for either enhanced membership inference attacks or evennovel threats such as extracting users' test queries. For example, we show thatdeduplicating training data before applying differentially-private trainingcreates a side-channel that completely invalidates any provable privacyguarantees. Moreover, we show that systems which block language models fromregenerating training data can be exploited to allow exact reconstruction ofprivate keys contained in the training set -- even if the model did notmemorize these keys. Taken together, our results demonstrate the need for aholistic, end-to-end privacy analysis of machine learning.</description><author>Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini, Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, Florian Tramèr</author><pubDate>Mon, 11 Sep 2023 17:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05610v1</guid></item><item><title>Incorporating Pre-trained Model Prompting in Multimodal Stock Volume Movement Prediction</title><link>http://arxiv.org/abs/2309.05608v1</link><description>Multimodal stock trading volume movement prediction with stock-related newsis one of the fundamental problems in the financial area. Existing multimodalworks that train models from scratch face the problem of lacking universalknowledge when modeling financial news. In addition, the models ability may belimited by the lack of domain-related knowledge due to insufficient data in thedatasets. To handle this issue, we propose the Prompt-based MUltimodal StockvolumE prediction model (ProMUSE) to process text and time series modalities.We use pre-trained language models for better comprehension of financial newsand adopt prompt learning methods to leverage their capability in universalknowledge to model textual information. Besides, simply fusing two modalitiescan cause harm to the unimodal representations. Thus, we propose a novelcross-modality contrastive alignment while reserving the unimodal heads besidethe fusion head to mitigate this problem. Extensive experiments demonstratethat our proposed ProMUSE outperforms existing baselines. Comprehensiveanalyses further validate the effectiveness of our architecture compared topotential variants and learning mechanisms.</description><author>Ruibo Chen, Zhiyuan Zhang, Yi Liu, Ruihan Bao, Keiko Harimoto, Xu Sun</author><pubDate>Mon, 11 Sep 2023 17:47:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05608v1</guid></item><item><title>Deep grading for MRI-based differential diagnosis of Alzheimer's disease and Frontotemporal dementia</title><link>http://arxiv.org/abs/2211.14096v2</link><description>Alzheimer's disease and Frontotemporal dementia are common forms ofneurodegenerative dementia. Behavioral alterations and cognitive impairmentsare found in the clinical courses of both diseases and their differentialdiagnosis is sometimes difficult for physicians. Therefore, an accurate tooldedicated to this diagnostic challenge can be valuable in clinical practice.However, current structural imaging methods mainly focus on the detection ofeach disease but rarely on their differential diagnosis. In this paper, wepropose a deep learning based approach for both problems of disease detectionand differential diagnosis. We suggest utilizing two types of biomarkers forthis application: structure grading and structure atrophy. First, we propose totrain a large ensemble of 3D U-Nets to locally determine the anatomicalpatterns of healthy people, patients with Alzheimer's disease and patients withFrontotemporal dementia using structural MRI as input. The output of theensemble is a 2-channel disease's coordinate map able to be transformed into a3D grading map which is easy to interpret for clinicians. This 2-channel map iscoupled with a multi-layer perceptron classifier for different classificationtasks. Second, we propose to combine our deep learning framework with atraditional machine learning strategy based on volume to improve the modeldiscriminative capacity and robustness. After both cross-validation andexternal validation, our experiments based on 3319 MRI demonstrated competitiveresults of our method compared to the state-of-the-art methods for both diseasedetection and differential diagnosis.</description><author>Huy-Dung Nguyen, Michaël Clément, Vincent Planche, Boris Mansencal, Pierrick Coupé</author><pubDate>Mon, 11 Sep 2023 17:44:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14096v2</guid></item><item><title>Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models</title><link>http://arxiv.org/abs/2309.05605v1</link><description>Answering multi-hop reasoning questions requires retrieving and synthesizinginformation from diverse sources. Large Language Models (LLMs) struggle toperform such reasoning consistently. Here we propose an approach to pinpointand rectify multi-hop reasoning failures through targeted memory injections onLLM attention heads. First, we analyze the per-layer activations of GPT-2models in response to single and multi-hop prompts. We then propose a mechanismthat allows users to inject pertinent prompt-specific information, which werefer to as "memories," at critical LLM locations during inference. By thusenabling the LLM to incorporate additional relevant information duringinference, we enhance the quality of multi-hop prompt completions. We showempirically that a simple, efficient, and targeted memory injection into a keyattention layer can often increase the probability of the desired next token inmulti-hop tasks, by up to 424%.</description><author>Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, Ian Foster</author><pubDate>Mon, 11 Sep 2023 17:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05605v1</guid></item><item><title>Robust Feature-Level Adversaries are Interpretability Tools</title><link>http://arxiv.org/abs/2110.03605v7</link><description>The literature on adversarial attacks in computer vision typically focuses onpixel-level perturbations. These tend to be very difficult to interpret. Recentwork that manipulates the latent representations of image generators to create"feature-level" adversarial perturbations gives us an opportunity to exploreperceptible, interpretable adversarial attacks. We make three contributions.First, we observe that feature-level attacks provide useful classes of inputsfor studying representations in models. Second, we show that these adversariesare uniquely versatile and highly robust. We demonstrate that they can be usedto produce targeted, universal, disguised, physically-realizable, and black-boxattacks at the ImageNet scale. Third, we show how these adversarial images canbe used as a practical interpretability tool for identifying bugs in networks.We use these adversaries to make predictions about spurious associationsbetween features and classes which we then test by designing "copy/paste"attacks in which one natural image is pasted into another to cause a targetedmisclassification. Our results suggest that feature-level attacks are apromising approach for rigorous interpretability research. They support thedesign of tools to better understand what a model has learned and diagnosebrittle feature associations. Code is available athttps://github.com/thestephencasper/feature_level_adv</description><author>Stephen Casper, Max Nadeau, Dylan Hadfield-Menell, Gabriel Kreiman</author><pubDate>Mon, 11 Sep 2023 17:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.03605v7</guid></item><item><title>Temporal Action Localization with Enhanced Instant Discriminability</title><link>http://arxiv.org/abs/2309.05590v1</link><description>Temporal action detection (TAD) aims to detect all action boundaries andtheir corresponding categories in an untrimmed video. The unclear boundaries ofactions in videos often result in imprecise predictions of action boundaries byexisting methods. To resolve this issue, we propose a one-stage framework namedTriDet. First, we propose a Trident-head to model the action boundary via anestimated relative probability distribution around the boundary. Then, weanalyze the rank-loss problem (i.e. instant discriminability deterioration) intransformer-based methods and propose an efficient scalable-granularityperception (SGP) layer to mitigate this issue. To further push the limit ofinstant discriminability in the video backbone, we leverage the strongrepresentation capability of pretrained large models and investigate theirperformance on TAD. Last, considering the adequate spatial-temporal context forclassification, we design a decoupled feature pyramid network with separatefeature pyramids to incorporate rich spatial context from the large model forlocalization. Experimental results demonstrate the robustness of TriDet and itsstate-of-the-art performance on multiple TAD datasets, including hierarchical(multilabel) TAD datasets.</description><author>Dingfeng Shi, Qiong Cao, Yujie Zhong, Shan An, Jian Cheng, Haogang Zhu, Dacheng Tao</author><pubDate>Mon, 11 Sep 2023 17:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05590v1</guid></item><item><title>Quantitative Analysis of Forecasting Models:In the Aspect of Online Political Bias</title><link>http://arxiv.org/abs/2309.05589v1</link><description>Understanding and mitigating political bias in online social media platformsare crucial tasks to combat misinformation and echo chamber effects. However,characterizing political bias temporally using computational methods presentschallenges due to the high frequency of noise in social media datasets. Whileexisting research has explored various approaches to political biascharacterization, the ability to forecast political bias and anticipate howpolitical conversations might evolve in the near future has not beenextensively studied. In this paper, we propose a heuristic approach to classifysocial media posts into five distinct political leaning categories. Since thereis a lack of prior work on forecasting political bias, we conduct an in-depthanalysis of existing baseline models to identify which model best fits toforecast political leaning time series. Our approach involves utilizingexisting time series forecasting models on two social media datasets withdifferent political ideologies, specifically Twitter and Gab. Through ourexperiments and analyses, we seek to shed light on the challenges andopportunities in forecasting political bias in social media platforms.Ultimately, our work aims to pave the way for developing more effectivestrategies to mitigate the negative impact of political bias in the digitalrealm.</description><author>Srinath Sai Tripuraneni, Sadia Kamal, Arunkumar Bagavathi</author><pubDate>Mon, 11 Sep 2023 17:17:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05589v1</guid></item><item><title>Mind the Uncertainty: Risk-Aware and Actively Exploring Model-Based Reinforcement Learning</title><link>http://arxiv.org/abs/2309.05582v1</link><description>We introduce a simple but effective method for managing risk in model-basedreinforcement learning with trajectory sampling that involves probabilisticsafety constraints and balancing of optimism in the face of epistemicuncertainty and pessimism in the face of aleatoric uncertainty of an ensembleof stochastic neural networks.Various experiments indicate that the separationof uncertainties is essential to performing well with data-driven MPCapproaches in uncertain and safety-critical control environments.</description><author>Marin Vlastelica, Sebastian Blaes, Cristina Pineri, Georg Martius</author><pubDate>Mon, 11 Sep 2023 17:10:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05582v1</guid></item><item><title>Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test</title><link>http://arxiv.org/abs/2307.05775v2</link><description>The expressive power of graph neural networks is usually measured bycomparing how many pairs of graphs or nodes an architecture can possiblydistinguish as non-isomorphic to those distinguishable by the $k$-dimensionalWeisfeiler-Lehman ($k$-WL) test. In this paper, we uncover misalignmentsbetween graph machine learning practitioners' conceptualizations of expressivepower and $k$-WL through a systematic analysis of the reliability and validityof $k$-WL. We conduct a survey ($n = 18$) of practitioners to surface theirconceptualizations of expressive power and their assumptions about $k$-WL. Incontrast to practitioners' opinions, our analysis (which draws from graphtheory and benchmark auditing) reveals that $k$-WL does not guarantee isometry,can be irrelevant to real-world graph tasks, and may not promote generalizationor trustworthiness. We argue for extensional definitions and measurement ofexpressive power based on benchmarks. We further contribute guiding questionsfor constructing such benchmarks, which is critical for graph machine learningpractitioners to develop and transparently communicate our understandings ofexpressive power.</description><author>Arjun Subramonian, Adina Williams, Maximilian Nickel, Yizhou Sun, Levent Sagun</author><pubDate>Mon, 11 Sep 2023 17:06:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05775v2</guid></item><item><title>Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations</title><link>http://arxiv.org/abs/2309.05575v1</link><description>Anisotropic diffusion processes with a diffusion tensor are important inimage analysis, physics, and engineering. However, their numericalapproximation has a strong impact on dissipative artefacts and deviations fromrotation invariance. In this work, we study a large family of finite differencediscretisations on a 3 x 3 stencil. We derive it by splitting 2-D anisotropicdiffusion into four 1-D diffusions. The resulting stencil class involves onefree parameter and covers a wide range of existing discretisations. Itcomprises the full stencil family of Weickert et al. (2013) and shows thattheir two parameters contain redundancy. Furthermore, we establish a bound onthe spectral norm of the matrix corresponding to the stencil. This gives timestep size limits that guarantee stability of an explicit scheme in theEuclidean norm. Our directional splitting also allows a very naturaltranslation of the explicit scheme into ResNet blocks. Employing neural networklibraries enables simple and highly efficient parallel implementations on GPUs.</description><author>Karl Schrader, Joachim Weickert, Michael Krause</author><pubDate>Mon, 11 Sep 2023 17:03:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05575v1</guid></item><item><title>UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase</title><link>http://arxiv.org/abs/2309.05573v1</link><description>Point-, voxel-, and range-views are three representative forms of pointclouds. All of them have accurate 3D measurements but lack color and textureinformation. RGB images are a natural complement to these point cloud views andfully utilizing the comprehensive information of them benefits more robustperceptions. In this paper, we present a unified multi-modal LiDAR segmentationnetwork, termed UniSeg, which leverages the information of RGB images and threeviews of the point cloud, and accomplishes semantic segmentation and panopticsegmentation simultaneously. Specifically, we first design the Learnablecross-Modal Association (LMA) module to automatically fuse voxel-view andrange-view features with image features, which fully utilize the rich semanticinformation of images and are robust to calibration errors. Then, the enhancedvoxel-view and range-view features are transformed to the point space,wherethree views of point cloud features are further fused adaptively by theLearnable cross-View Association module (LVA). Notably, UniSeg achievespromising results in three public benchmarks, i.e., SemanticKITTI, nuScenes,and Waymo Open Dataset (WOD); it ranks 1st on two challenges of two benchmarks,including the LiDAR semantic segmentation challenge of nuScenes and panopticsegmentation challenges of SemanticKITTI. Besides, we construct the OpenPCSegcodebase, which is the largest and most comprehensive outdoor LiDARsegmentation codebase. It contains most of the popular outdoor LiDARsegmentation algorithms and provides reproducible implementations. TheOpenPCSeg codebase will be made publicly available athttps://github.com/PJLab-ADG/PCSeg.</description><author>Youquan Liu, Runnan Chen, Xin Li, Lingdong Kong, Yuchen Yang, Zhaoyang Xia, Yeqi Bai, Xinge Zhu, Yuexin Ma, Yikang Li, Yu Qiao, Yuenan Hou</author><pubDate>Mon, 11 Sep 2023 17:00:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05573v1</guid></item><item><title>ITI-GEN: Inclusive Text-to-Image Generation</title><link>http://arxiv.org/abs/2309.05569v1</link><description>Text-to-image generative models often reflect the biases of the trainingdata, leading to unequal representations of underrepresented groups. This studyinvestigates inclusive text-to-image generative models that generate imagesbased on human-written prompts and ensure the resulting images are uniformlydistributed across attributes of interest. Unfortunately, directly expressingthe desired attributes in the prompt often leads to sub-optimal results due tolinguistic ambiguity or model misrepresentation. Hence, this paper proposes adrastically different approach that adheres to the maxim that "a picture isworth a thousand words". We show that, for some attributes, images canrepresent concepts more expressively than text. For instance, categories ofskin tones are typically hard to specify by text but can be easily representedby example images. Building upon these insights, we propose a novel approach,ITI-GEN, that leverages readily available reference images for InclusiveText-to-Image GENeration. The key idea is learning a set of prompt embeddingsto generate images that can effectively represent all desired attributecategories. More importantly, ITI-GEN requires no model fine-tuning, making itcomputationally efficient to augment existing text-to-image models. Extensiveexperiments demonstrate that ITI-GEN largely improves over state-of-the-artmodels to generate inclusive images from a prompt. Project page:https://czhang0528.github.io/iti-gen.</description><author>Cheng Zhang, Xuanbai Chen, Siqi Chai, Chen Henry Wu, Dmitry Lagun, Thabo Beeler, Fernando De la Torre</author><pubDate>Mon, 11 Sep 2023 16:54:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05569v1</guid></item><item><title>Explainable AI by BAPC -- Before and After correction Parameter Comparison</title><link>http://arxiv.org/abs/2103.07155v2</link><description>A local surrogate for an AI-model correcting a simpler 'base' model isintroduced representing an analytical method to yield explanations ofAI-predictions. The approach is studied here in the context of the base modelbeing linear regression. The AI-model approximates the residual error of thelinear model and the explanations are formulated in terms of the change of theinterpretable base model's parameters. Criteria are formulated for the preciserelation between lost accuracy of the surrogate, the accuracy of the AI-model,and the surrogate fidelity. It is shown that, assuming a certain maximal amountof noise in the observed data, these criteria induce neighborhoods of theinstances to be explained which have an ideal size in terms of maximal accuracyand fidelity.</description><author>Florian Sobieczky, Manuela Geiß</author><pubDate>Mon, 11 Sep 2023 16:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2103.07155v2</guid></item><item><title>An Empirical Study of NetOps Capability of Pre-Trained Large Language Models</title><link>http://arxiv.org/abs/2309.05557v1</link><description>Large language models (LLMs) can respond to human language queries and haveshown powerful potential applications in network operations (NetOps). Thanks tothe large amount of commonsense knowledge inherent, LLMs achieve much betterinference accuracy than traditional models and emerge with strong abilities ingeneralization, reasoning, and code generation. These abilities may have acrucial boost to automated and intelligent NetOps. However, it remainsunder-explored how well LLMs perform in various NetOps tasks. In this work, wemake a systematic assessment of the capabilities, strengths, and limitations ofselected LLMs in the field of NetOps. The evaluation is conducted on acollection of 5,732 questions about NetOps, encompassing 26 publicly availablegeneral-domain LLMs, including ChatGPT, LLaMA, Falcon, etc. We also finetunesome of these LLMs with our collected NetOps corpus and evaluate the resultingmodels. The evaluation method follows the widely adopted benchmarks forgeneral-domain LLMs, combined with Chain-of-Thought Prompts andRetrieval-Augmented Generation. The results show that only GPT-4 achieves highaccuracy equivalent to passing the NetOps certification exam for humans, whileall the other LLMs have much lower accuracy. However, some open models likeLLaMA 2 still demonstrate significant potential. Furthermore, we evaluate theimpact of factors such as model parameters, prompt engineering, instructionfine-tuning etc. This work shall be treated as the initial effort to systematicevaluation of LLMs in NetOps, and a more rigorous study is required forproduction use. The evaluation code and dataset will be released to benefitfuture research.</description><author>Yukai Miao, Yu Bai, Li Chen, Dan Li, Haifeng Sun, Xizheng Wang, Ziqiu Luo, Dapeng Sun, Xiuting Xu</author><pubDate>Mon, 11 Sep 2023 16:45:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05557v1</guid></item><item><title>SciRE-Solver: Accelerating Diffusion Models Sampling by Score-integrand Solver with Recursive Difference</title><link>http://arxiv.org/abs/2308.07896v3</link><description>Diffusion models (DMs) have made significant progress in the fields of image,audio, and video generation. One downside of DMs is their slow iterativeprocess. Recent algorithms for fast sampling are designed from the perspectiveof differential equations. However, in higher-order algorithms based on Taylorexpansion, estimating the derivative of the score function becomes intractabledue to the complexity of large-scale, well-trained neural networks. Driven bythis motivation, in this work, we introduce the recursive difference (RD)method to calculate the derivative of the score function in the realm of DMs.Based on the RD method and the truncated Taylor expansion of score-integrand,we propose SciRE-Solver with the convergence order guarantee for acceleratingsampling of DMs. To further investigate the effectiveness of the RD method, wealso propose a variant named SciREI-Solver based on the RD method andexponential integrator. Our proposed sampling algorithms with RD method attainstate-of-the-art (SOTA) FIDs in comparison to existing training-free samplingalgorithms, across both discrete-time and continuous-time pre-trained DMs,under various number of score function evaluations (NFE). Remarkably,SciRE-Solver using a small NFEs demonstrates promising potential to surpass theFID achieved by some pre-trained models in their original papers using no fewerthan $1000$ NFEs. For example, we reach SOTA value of $2.40$ FID with $100$ NFEfor continuous-time DM and of $3.15$ FID with $84$ NFE for discrete-time DM onCIFAR-10, as well as of $2.17$ (2.02) FID with $18$ (50) NFE for discrete-timeDM on CelebA 64$\times$64.</description><author>Shigui Li, Wei Chen, Delu Zeng</author><pubDate>Mon, 11 Sep 2023 16:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07896v3</guid></item><item><title>Can Deep Neural Networks Predict Data Correlations from Column Names?</title><link>http://arxiv.org/abs/2107.04553v2</link><description>Recent publications suggest using natural language analysis on databaseschema elements to guide tuning and profiling efforts. The underlyinghypothesis is that state-of-the-art language processing methods, so-calledlanguage models, are able to extract information on data properties from schematext. This paper examines that hypothesis in the context of data correlationanalysis: is it possible to find column pairs with correlated data by analyzingtheir names via language models? First, the paper introduces a novel benchmarkfor data correlation analysis, created by analyzing thousands of Kaggle datasets (and available for download). Second, it uses that data to study theability of language models to predict correlation, based on column names. Theanalysis covers different language models, various correlation metrics, and amultitude of accuracy metrics. It pinpoints factors that contribute tosuccessful predictions, such as the length of column names as well as the ratioof words. Finally, \rev{the study analyzes the impact of column types onprediction performance.} The results show that schema text can be a usefulsource of information and inform future research efforts, targeted atNLP-enhanced database tuning and data profiling.</description><author>Immanuel Trummer</author><pubDate>Mon, 11 Sep 2023 16:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.04553v2</guid></item><item><title>OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data</title><link>http://arxiv.org/abs/2309.05551v1</link><description>The inexorable growth of online shopping and e-commerce demands scalable androbust machine learning-based solutions to accommodate customer requirements.In the context of automatic tagging classification and multimodal retrieval,prior works either defined a low generalizable supervised learning approach ormore reusable CLIP-based techniques while, however, training on closed sourcedata. In this work, we propose OpenFashionCLIP, a vision-and-languagecontrastive learning method that only adopts open-source fashion data stemmingfrom diverse domains, and characterized by varying degrees of specificity. Ourapproach is extensively validated across several tasks and benchmarks, andexperimental results highlight a significant out-of-domain generalizationcapability and consistent improvements over state-of-the-art methods both interms of accuracy and recall. Source code and trained models are publiclyavailable at: https://github.com/aimagelab/open-fashion-clip.</description><author>Giuseppe Cartella, Alberto Baldrati, Davide Morelli, Marcella Cornia, Marco Bertini, Rita Cucchiara</author><pubDate>Mon, 11 Sep 2023 16:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05551v1</guid></item><item><title>Model Based Residual Policy Learning with Applications to Antenna Control</title><link>http://arxiv.org/abs/2211.08796v3</link><description>Non-differentiable controllers and rule-based policies are widely used forcontrolling real systems such as telecommunication networks and robots.Specifically, parameters of mobile network base station antennas can bedynamically configured by these policies to improve users coverage and qualityof service. Motivated by the antenna tilt control problem, we introduceModel-Based Residual Policy Learning (MBRPL), a practical reinforcementlearning (RL) method. MBRPL enhances existing policies through a model-basedapproach, leading to improved sample efficiency and a decreased number ofinteractions with the actual environment when compared to off-the-shelf RLmethods.To the best of our knowledge, this is the first paper that examines amodel-based approach for antenna control. Experimental results reveal that ourmethod delivers strong initial performance while improving sample efficiencyover previous RL methods, which is one step towards deploying these algorithmsin real networks.</description><author>Viktor Eriksson Möllerstedt, Alessio Russo, Maxime Bouton</author><pubDate>Mon, 11 Sep 2023 16:34:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08796v3</guid></item><item><title>Distance-Aware eXplanation Based Learning</title><link>http://arxiv.org/abs/2309.05548v1</link><description>eXplanation Based Learning (XBL) is an interactive learning approach thatprovides a transparent method of training deep learning models by interactingwith their explanations. XBL augments loss functions to penalize a model basedon deviation of its explanations from user annotation of image features. Theliterature on XBL mostly depends on the intersection of visual modelexplanations and image feature annotations. We present a method to add adistance-aware explanation loss to categorical losses that trains a learner tofocus on important regions of a training dataset. Distance is an appropriateapproach for calculating explanation loss since visual model explanations suchas Gradient-weighted Class Activation Mapping (Grad-CAMs) are not strictlybounded as annotations and their intersections may not provide completeinformation on the deviation of a model's focus from relevant image regions. Inaddition to assessing our model using existing metrics, we propose aninterpretability metric for evaluating visual feature-attribution based modelexplanations that is more informative of the model's performance than existingmetrics. We demonstrate performance of our proposed method on three imageclassification tasks.</description><author>Misgina Tsighe Hagos, Niamh Belton, Kathleen M. Curran, Brian Mac Namee</author><pubDate>Mon, 11 Sep 2023 16:33:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05548v1</guid></item><item><title>Physics-Informed Neural Networks for Prognostics and Health Management of Lithium-Ion Batteries</title><link>http://arxiv.org/abs/2301.00776v2</link><description>For Prognostics and Health Management (PHM) of Lithium-ion (Li-ion)batteries, many models have been established to characterize their degradationprocess. The existing empirical or physical models can reveal importantinformation regarding the degradation dynamics. However, there are no generaland flexible methods to fuse the information represented by those models.Physics-Informed Neural Network (PINN) is an efficient tool to fuse empiricalor physical dynamic models with data-driven models. To take full advantage ofvarious information sources, we propose a model fusion scheme based on PINN. Itis implemented by developing a semi-empirical semi-physical PartialDifferential Equation (PDE) to model the degradation dynamics of Li-ionbatteries. When there is little prior knowledge about the dynamics, we leveragethe data-driven Deep Hidden Physics Model (DeepHPM) to discover the underlyinggoverning dynamic models. The uncovered dynamics information is then fused withthat mined by the surrogate neural network in the PINN framework. Moreover, anuncertainty-based adaptive weighting method is employed to balance the multiplelearning tasks when training the PINN. The proposed methods are verified on apublic dataset of Li-ion Phosphate (LFP)/graphite batteries.</description><author>Pengfei Wen, Zhi-Sheng Ye, Yong Li, Shaowei Chen, Pu Xie, Shuai Zhao</author><pubDate>Mon, 11 Sep 2023 16:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.00776v2</guid></item><item><title>Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications</title><link>http://arxiv.org/abs/2309.05542v1</link><description>Language model applications are becoming increasingly popular and complex,often including features like tool usage and retrieval augmentation. However,existing frameworks for such applications are often opinionated, deciding fordevelopers how their prompts ought to be formatted and imposing limitations oncustomizability and reproducibility. To solve this we present Kani: alightweight, flexible, and model-agnostic open-source framework for buildinglanguage model applications. Kani helps developers implement a variety ofcomplex features by supporting the core building blocks of chat interaction:model interfacing, chat management, and robust function calling. All Kani corefunctions are easily overridable and well documented to empower developers tocustomize functionality for their own needs. Kani thus serves as a useful toolfor researchers, hobbyists, and industry professionals alike to acceleratetheir development while retaining interoperability and fine-grained control.</description><author>Andrew Zhu, Liam Dugan, Alyssa Hwang, Chris Callison-Burch</author><pubDate>Mon, 11 Sep 2023 16:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05542v1</guid></item><item><title>Dynamic Y-KD: A Hybrid Approach to Continual Instance Segmentation</title><link>http://arxiv.org/abs/2303.06015v3</link><description>Despite the success of deep learning models on instance segmentation, currentmethods still suffer from catastrophic forgetting in continual learningscenarios. In this paper, our contributions for continual instance segmentationare threefold. First, we propose the Y-knowledge distillation (Y-KD), atechnique that shares a common feature extractor between the teacher andstudent networks. As the teacher is also updated with new data in Y-KD, theincreased plasticity results in new modules that are specialized on newclasses. Second, our Y-KD approach is supported by a dynamic architecturemethod that trains task-specific modules with a unique instance segmentationhead, thereby significantly reducing forgetting. Third, we complete ourapproach by leveraging checkpoint averaging as a simple method to manuallybalance the trade-off between performance on the various sets of classes, thusincreasing control over the model's behavior without any additional cost. Thesecontributions are united in our model that we name the Dynamic Y-KD network. We perform extensive experiments on several single-step and multi-stepsincremental learning scenarios, and we show that our approach outperformsprevious methods both on past and new classes. For instance, compared to recentwork, our method obtains +2.1% mAP on old classes in 15-1, +7.6% mAP on newclasses in 19-1 and reaches 91.5% of the mAP obtained by joint-training on allclasses in 15-5.</description><author>Mathieu Pagé-Fortin, Brahim Chaib-draa</author><pubDate>Mon, 11 Sep 2023 16:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06015v3</guid></item><item><title>PAI-Diffusion: Constructing and Serving a Family of Open Chinese Diffusion Models for Text-to-image Synthesis on the Cloud</title><link>http://arxiv.org/abs/2309.05534v1</link><description>Text-to-image synthesis for the Chinese language poses unique challenges dueto its large vocabulary size, and intricate character relationships. Whileexisting diffusion models have shown promise in generating images from textualdescriptions, they often neglect domain-specific contexts and lack robustnessin handling the Chinese language. This paper introduces PAI-Diffusion, acomprehensive framework that addresses these limitations. PAI-Diffusionincorporates both general and domain-specific Chinese diffusion models,enabling the generation of contextually relevant images. It explores thepotential of using LoRA and ControlNet for fine-grained image style transferand image editing, empowering users with enhanced control over imagegeneration. Moreover, PAI-Diffusion seamlessly integrates with Alibaba Cloud'sMachine Learning Platform for AI, providing accessible and scalable solutions.All the Chinese diffusion model checkpoints, LoRAs, and ControlNets, includingdomain-specific ones, are publicly available. A user-friendly Chinese WebUI andthe diffusers-api elastic inference toolkit, also open-sourced, furtherfacilitate the easy deployment of PAI-Diffusion models in various environments,making it a valuable resource for Chinese text-to-image synthesis.</description><author>Chengyu Wang, Zhongjie Duan, Bingyan Liu, Xinyi Zou, Cen Chen, Kui Jia, Jun Huang</author><pubDate>Mon, 11 Sep 2023 16:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05534v1</guid></item><item><title>On the meaning of uncertainty for ethical AI: philosophy and practice</title><link>http://arxiv.org/abs/2309.05529v1</link><description>Whether and how data scientists, statisticians and modellers should beaccountable for the AI systems they develop remains a controversial and highlydebated topic, especially given the complexity of AI systems and thedifficulties in comparing and synthesising competing claims arising from theirdeployment for data analysis. This paper proposes to address this issue bydecreasing the opacity and heightening the accountability of decision makingusing AI systems, through the explicit acknowledgement of the statisticalfoundations that underpin their development and the ways in which these dictatehow their results should be interpreted and acted upon by users. In turn, thisenhances (1) the responsiveness of the models to feedback, (2) the quality andmeaning of uncertainty on their outputs and (3) their transparency toevaluation. To exemplify this approach, we extend Posterior Belief Assessmentto offer a route to belief ownership from complex and competing AI structures.We argue that this is a significant way to bring ethical considerations intomathematical reasoning, and to implement ethical AI in statistical practice. Wedemonstrate these ideas within the context of competing models used to advisethe UK government on the spread of the Omicron variant of COVID-19 duringDecember 2021.</description><author>Cassandra Bird, Daniel Williamson, Sabina Leonelli</author><pubDate>Mon, 11 Sep 2023 16:13:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05529v1</guid></item><item><title>On the detection of Out-Of-Distribution samples in Multiple Instance Learning</title><link>http://arxiv.org/abs/2309.05528v1</link><description>The deployment of machine learning solutions in real-world scenarios ofteninvolves addressing the challenge of out-of-distribution (OOD) detection. Whilesignificant efforts have been devoted to OOD detection in classical supervisedsettings, the context of weakly supervised learning, particularly the MultipleInstance Learning (MIL) framework, remains under-explored. In this study, wetackle this challenge by adapting post-hoc OOD detection methods to the MILsetting while introducing a novel benchmark specifically designed to assess OODdetection performance in weakly supervised scenarios. Extensive experimentsbased on diverse public datasets do not reveal a single method with a clearadvantage over the others. Although DICE emerges as the best-performing methodoverall, it exhibits significant shortcomings on some datasets, emphasizing thecomplexity of this under-explored and challenging topic. Our findings shedlight on the complex nature of OOD detection under the MIL framework,emphasizing the importance of developing novel, robust, and reliable methodsthat can generalize effectively in a weakly supervised context. The code forthe paper is available here: https://github.com/loic-lb/OOD_MIL.</description><author>Loïc Le Bescond, Maria Vakalopoulou, Stergios Christodoulidis, Fabrice André, Hugues Talbot</author><pubDate>Mon, 11 Sep 2023 16:12:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05528v1</guid></item><item><title>ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation</title><link>http://arxiv.org/abs/2309.05527v1</link><description>Domain shifts such as sensor type changes and geographical situationvariations are prevalent in Autonomous Driving (AD), which poses a challengesince AD model relying on the previous-domain knowledge can be hardly directlydeployed to a new domain without additional costs. In this paper, we provide anew perspective and approach of alleviating the domain shifts, by proposing aReconstruction-Simulation-Perception (ReSimAD) scheme. Specifically, theimplicit reconstruction process is based on the knowledge from the previous olddomain, aiming to convert the domain-related knowledge into domain-invariantrepresentations, \textit{e.g.}, 3D scene-level meshes. Besides, the pointclouds simulation process of multiple new domains is conditioned on the abovereconstructed 3D meshes, where the target-domain-like simulation samples can beobtained, thus reducing the cost of collecting and annotating new-domain datafor the subsequent perception process. For experiments, we consider differentcross-domain situations such as Waymo-to-KITTI, Waymo-to-nuScenes,Waymo-to-ONCE, \textit{etc}, to verify the \textbf{zero-shot} target-domainperception using ReSimAD. Results demonstrate that our method is beneficial toboost the domain generalization ability, even promising for 3D pre-training.</description><author>Bo Zhang, Xinyu Cai, Jiakang Yuan, Donglin Yang, Jianfei Guo, Renqiu Xia, Botian Shi, Min Dou, Tao Chen, Si Liu, Junchi Yan, Yu Qiao</author><pubDate>Mon, 11 Sep 2023 16:11:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05527v1</guid></item><item><title>Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis</title><link>http://arxiv.org/abs/2309.05525v1</link><description>Integrating native AI support into the network architecture is an essentialobjective of 6G. Federated Learning (FL) emerges as a potential paradigm,facilitating decentralized AI model training across a diverse range of devicesunder the coordination of a central server. However, several challenges hinderits wide application in the 6G context, such as malicious attacks and privacysnooping on local model updates, and centralization pitfalls. This workproposes a trusted architecture for supporting FL, which utilizes DistributedLedger Technology (DLT) and Graph Neural Network (GNN), including three keyfeatures. First, a pre-processing layer employing homomorphic encryption isincorporated to securely aggregate local models, preserving the privacy ofindividual models. Second, given the distributed nature and graph structurebetween clients and nodes in the pre-processing layer, GNN is leveraged toidentify abnormal local models, enhancing system security. Third, DLT isutilized to decentralize the system by selecting one of the candidates toperform the central server's functions. Additionally, DLT ensures reliable datamanagement by recording data exchanges in an immutable and transparent ledger.The feasibility of the novel architecture is validated through simulations,demonstrating improved performance in anomalous model detection and globalmodel accuracy compared to relevant baselines.</description><author>Wenxuan Ye, Chendi Qian, Xueli An, Xueqiang Yan, Georg Carle</author><pubDate>Mon, 11 Sep 2023 16:10:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05525v1</guid></item><item><title>Re-formalization of Individual Fairness</title><link>http://arxiv.org/abs/2309.05521v1</link><description>The notion of individual fairness is a formalization of an ethical principle,"Treating like cases alike," which has been argued such as by Aristotle. In afairness-aware machine learning context, Dwork et al. firstly formalized thenotion. In their formalization, a similar pair of data in an unfair spaceshould be mapped to similar positions in a fair space. We propose tore-formalize individual fairness by the statistical independence conditioned byindividuals. This re-formalization has the following merits. First, ourformalization is compatible with that of Dwork et al. Second, our formalizationenables to combine individual fairness with the fairness notion, equalized oddsor sufficiency, as well as statistical parity. Third, though theirformalization implicitly assumes a pre-process approach for making fairprediction, our formalization is applicable to an in-process or post-processapproach.</description><author>Toshihiro Kamishima</author><pubDate>Mon, 11 Sep 2023 16:04:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05521v1</guid></item><item><title>NExT-GPT: Any-to-Any Multimodal LLM</title><link>http://arxiv.org/abs/2309.05519v1</link><description>While recently Multimodal Large Language Models (MM-LLMs) have made excitingstrides, they mostly fall prey to the limitation of only input-side multimodalunderstanding, without the ability to produce content in multiple modalities.As we humans always perceive the world and communicate with people throughvarious modalities, developing any-to-any MM-LLMs capable of accepting anddelivering content in any modality becomes essential to human-level AI. To fillthe gap, we present an end-to-end general-purpose any-to-any MM-LLM system,NExT-GPT. We connect an LLM with multimodal adaptors and different diffusiondecoders, enabling NExT-GPT to perceive inputs and generate outputs inarbitrary combinations of text, images, videos, and audio. By leveraging theexisting well-trained highly-performing encoders and decoders, NExT-GPT istuned with only a small amount of parameter (1%) of certain projection layers,which not only benefits low-cost training and also facilitates convenientexpansion to more potential modalities. Moreover, we introduce amodality-switching instruction tuning (MosIT) and manually curate ahigh-quality dataset for MosIT, based on which NExT-GPT is empowered withcomplex cross-modal semantic understanding and content generation. Overall, ourresearch showcases the promising possibility of building an AI agent capable ofmodeling universal modalities, paving the way for more human-like AI researchin the community.</description><author>Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua</author><pubDate>Mon, 11 Sep 2023 16:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05519v1</guid></item><item><title>Class-Incremental Learning of Plant and Disease Detection: Growing Branches with Knowledge Distillation</title><link>http://arxiv.org/abs/2304.06619v2</link><description>This paper investigates the problem of class-incremental object detection foragricultural applications where a model needs to learn new plant species anddiseases incrementally without forgetting the previously learned ones. We adapttwo public datasets to include new categories over time, simulating a morerealistic and dynamic scenario. We then compare three class-incrementallearning methods that leverage different forms of knowledge distillation tomitigate catastrophic forgetting. Our experiments show that all three methodssuffer from catastrophic forgetting, but the Dynamic Y-KD approach, whichadditionally uses a dynamic architecture that grows new branches to learn newtasks, outperforms ILOD and Faster-ILOD in most settings both on new and oldclasses. These results highlight the challenges and opportunities of continual objectdetection for agricultural applications. In particular, we hypothesize that thelarge intra-class and small inter-class variability that is typical of plantimages exacerbate the difficulty of learning new categories without interferingwith previous knowledge. We publicly release our code to encourage future work.</description><author>Mathieu Pagé Fortin</author><pubDate>Mon, 11 Sep 2023 16:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06619v2</guid></item><item><title>Stream-based Active Learning by Exploiting Temporal Properties in Perception with Temporal Predicted Loss</title><link>http://arxiv.org/abs/2309.05517v1</link><description>Active learning (AL) reduces the amount of labeled data needed to train amachine learning model by intelligently choosing which instances to label.Classic pool-based AL requires all data to be present in a datacenter, whichcan be challenging with the increasing amounts of data needed in deep learning.However, AL on mobile devices and robots, like autonomous cars, can filter thedata from perception sensor streams before reaching the datacenter. Weexploited the temporal properties for such image streams in our work andproposed the novel temporal predicted loss (TPL) method. To evaluate thestream-based setting properly, we introduced the GTA V streets and the A2D2streets dataset and made both publicly available. Our experiments showed thatour approach significantly improves the diversity of the selection while beingan uncertainty-based method. As pool-based approaches are more common inperception applications, we derived a concept for comparing pool-based andstream-based AL, where TPL out-performed state-of-the-art pool- or stream-basedapproaches for different models. TPL demonstrated a gain of 2.5 precept points(pp) less required data while being significantly faster than pool-basedmethods.</description><author>Sebastian Schmidt, Stephan Günnemann</author><pubDate>Mon, 11 Sep 2023 16:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05517v1</guid></item><item><title>Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</title><link>http://arxiv.org/abs/2309.05516v1</link><description>Large Language Models (LLMs) have proven their exceptional capabilities inperforming language-related tasks. However, their deployment poses significantchallenges due to their considerable memory and storage requirements. Inresponse to this issue, weight-only quantization, particularly 3 and 4-bitweight-only quantization, has emerged as one of the most viable solutions. Asthe number of bits decreases, the quantization grid broadens, thus emphasizingthe importance of up and down rounding. While previous studies havedemonstrated that fine-tuning up and down rounding with the addition ofperturbations can enhance accuracy in some scenarios, our study is driven bythe precise and limited boundary of these perturbations, where only thethreshold for altering the rounding value is of significance. Consequently, wepropose a concise and highly effective approach for optimizing the weightrounding task. Our method, named SignRound, involves lightweight block-wisetuning using signed gradient descent, enabling us to achieve outstandingresults within 400 steps. SignRound outperforms the established baseline ofrounding-to-nearest (RTN) and competes impressively against recent methods,without introducing additional inference overhead. The source code will bepublicly available at https://github.com/intel/neural-compressor soon.</description><author>Wenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao Lv</author><pubDate>Mon, 11 Sep 2023 15:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05516v1</guid></item><item><title>Several fitness functions and entanglement gates in quantum kernel generation</title><link>http://arxiv.org/abs/2309.03307v2</link><description>Quantum machine learning (QML) represents a promising frontier in the realmof quantum technologies. In this pursuit of quantum advantage, the quantumkernel method for support vector machine has emerged as a powerful approach.Entanglement, a fundamental concept in quantum mechanics, assumes a centralrole in quantum computing. In this paper, we study the necessities ofentanglement gates in the quantum kernel methods. We present several fitnessfunctions for a multi-objective genetic algorithm that simultaneously maximizesclassification accuracy while minimizing both the local and non-local gatecosts of the quantum feature map's circuit. We conduct comparisons withclassical classifiers to gain insights into the benefits of employingentanglement gates. Surprisingly, our experiments reveal that the optimalconfiguration of quantum circuits for the quantum kernel method incorporates aproportional number of non-local gates for entanglement, contrary to previousliterature where non-local gates were largely suppressed. Furthermore, we demonstrate that the separability indexes of data can beeffectively leveraged to determine the number of non-local gates required forthe quantum support vector machine's feature maps. This insight cansignificantly aid in selecting appropriate parameters, such as the entanglementparameter, in various quantum programming packages like https://qiskit.org/based on data analysis. Our findings offer valuable guidance for enhancing theefficiency and accuracy of quantum machine learning algorithm</description><author>Haiyan Wang</author><pubDate>Mon, 11 Sep 2023 15:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03307v2</guid></item><item><title>A Co-design Study for Multi-Stakeholder Job Recommender System Explanations</title><link>http://arxiv.org/abs/2309.05507v1</link><description>Recent legislation proposals have significantly increased the demand foreXplainable Artificial Intelligence (XAI) in many businesses, especially inso-called `high-risk' domains, such as recruitment. Within recruitment, AI hasbecome commonplace, mainly in the form of job recommender systems (JRSs), whichtry to match candidates to vacancies, and vice versa. However, common XAItechniques often fall short in this domain due to the different levels andtypes of expertise of the individuals involved, making explanations difficultto generalize. To determine the explanation preferences of the differentstakeholder types - candidates, recruiters, and companies - we created andvalidated a semi-structured interview guide. Using grounded theory, westructurally analyzed the results of these interviews and found that differentstakeholder types indeed have strongly differing explanation preferences.Candidates indicated a preference for brief, textual explanations that allowthem to quickly judge potential matches. On the other hand, hiring managerspreferred visual graph-based explanations that provide a more technical andcomprehensive overview at a glance. Recruiters found more exhaustive textualexplanations preferable, as those provided them with more talking points toconvince both parties of the match. Based on these findings, we describeguidelines on how to design an explanation interface that fulfills therequirements of all three stakeholder types. Furthermore, we provide thevalidated interview guide, which can assist future research in determining theexplanation preferences of different stakeholder types.</description><author>Roan Schellingerhout, Francesco Barile, Nava Tintarev</author><pubDate>Mon, 11 Sep 2023 15:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05507v1</guid></item><item><title>Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning</title><link>http://arxiv.org/abs/2309.05505v1</link><description>Repeated parameter sharing in federated learning causes significantinformation leakage about private data, thus defeating its main purpose: dataprivacy. Mitigating the risk of this information leakage, using state of theart differentially private algorithms, also does not come for free. Randomizedmechanisms can prevent convergence of models on learning even the usefulrepresentation functions, especially if there is more disagreement betweenlocal models on the classification functions (due to data heterogeneity). Inthis paper, we consider a representation federated learning objective thatencourages various parties to collaboratively refine the consensus part of themodel, with differential privacy guarantees, while separately allowingsufficient freedom for local personalization (without releasing it). We provethat in the linear representation setting, while the objective is non-convex,our proposed new algorithm \DPFEDREP\ converges to a ball centered around the\emph{global optimal} solution at a linear rate, and the radius of the ball isproportional to the reciprocal of the privacy budget. With this novel utilityanalysis, we improve the SOTA utility-privacy trade-off for this problem by afactor of $\sqrt{d}$, where $d$ is the input dimension. We empirically evaluateour method with the image classification task on CIFAR10, CIFAR100, and EMNIST,and observe a significant performance improvement over the prior work under thesame small privacy budget. The code can be found in this link:https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning.</description><author>Zebang Shen, Jiayuan Ye, Anmin Kang, Hamed Hassani, Reza Shokri</author><pubDate>Mon, 11 Sep 2023 15:46:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05505v1</guid></item><item><title>Long-Range Transformer Architectures for Document Understanding</title><link>http://arxiv.org/abs/2309.05503v1</link><description>Since their release, Transformers have revolutionized many fields fromNatural Language Understanding to Computer Vision. Document Understanding (DU)was not left behind with first Transformer based models for DU dating from late2019. However, the computational complexity of the self-attention operationlimits their capabilities to small sequences. In this paper we explore multiplestrategies to apply Transformer based models to long multi-page documents. Weintroduce 2 new multi-modal (text + layout) long-range models for DU. They arebased on efficient implementations of Transformers for long sequences.Long-range models can process whole documents at once effectively and are lessimpaired by the document's length. We compare them to LayoutLM, a classicalTransformer adapted for DU and pre-trained on millions of documents. We furtherpropose 2D relative attention bias to guide self-attention towards relevanttokens without harming model efficiency. We observe improvements on multi-pagebusiness documents on Information Retrieval for a small performance cost onsmaller sequences. Relative 2D attention revealed to be effective on dense textfor both normal and long-range models.</description><author>Thibault Douzon, Stefan Duffner, Christophe Garcia, Jérémy Espinas</author><pubDate>Mon, 11 Sep 2023 15:45:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05503v1</guid></item><item><title>Black-Box Analysis: GPTs Across Time in Legal Textual Entailment Task</title><link>http://arxiv.org/abs/2309.05501v1</link><description>The evolution of Generative Pre-trained Transformer (GPT) models has led tosignificant advancements in various natural language processing applications,particularly in legal textual entailment. We present an analysis of GPT-3.5(ChatGPT) and GPT-4 performances on COLIEE Task 4 dataset, a prominentbenchmark in this domain. The study encompasses data from Heisei 18 (2006) toReiwa 3 (2021), exploring the models' abilities to discern entailmentrelationships within Japanese statute law across different periods. Ourpreliminary experimental results unveil intriguing insights into the models'strengths and weaknesses in handling legal textual entailment tasks, as well asthe patterns observed in model performance. In the context of proprietarymodels with undisclosed architectures and weights, black-box analysis becomescrucial for evaluating their capabilities. We discuss the influence of trainingdata distribution and the implications on the models' generalizability. Thisanalysis serves as a foundation for future research, aiming to optimizeGPT-based models and enable their successful adoption in legal informationextraction and entailment applications.</description><author>Ha-Thanh Nguyen, Randy Goebel, Francesca Toni, Kostas Stathis, Ken Satoh</author><pubDate>Mon, 11 Sep 2023 15:43:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05501v1</guid></item><item><title>NeCo@ALQAC 2023: Legal Domain Knowledge Acquisition for Low-Resource Languages through Data Enrichment</title><link>http://arxiv.org/abs/2309.05500v1</link><description>In recent years, natural language processing has gained significantpopularity in various sectors, including the legal domain. This paper presentsNeCo Team's solutions to the Vietnamese text processing tasks provided in theAutomated Legal Question Answering Competition 2023 (ALQAC 2023), focusing onlegal domain knowledge acquisition for low-resource languages through dataenrichment. Our methods for the legal document retrieval task employ acombination of similarity ranking and deep learning models, while for thesecond task, which requires extracting an answer from a relevant legal articlein response to a question, we propose a range of adaptive techniques to handledifferent question types. Our approaches achieve outstanding results on bothtasks of the competition, demonstrating the potential benefits andeffectiveness of question answering systems in the legal field, particularlyfor low-resource languages.</description><author>Hai-Long Nguyen, Dieu-Quynh Nguyen, Hoang-Trung Nguyen, Thu-Trang Pham, Huu-Dong Nguyen, Thach-Anh Nguyen, Thi-Hai-Yen Vuong, Ha-Thanh Nguyen</author><pubDate>Mon, 11 Sep 2023 15:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05500v1</guid></item><item><title>Zero-Shot Co-salient Object Detection Framework</title><link>http://arxiv.org/abs/2309.05499v1</link><description>Co-salient Object Detection (CoSOD) endeavors to replicate the human visualsystem's capacity to recognize common and salient objects within a collectionof images. Despite recent advancements in deep learning models, these modelsstill rely on training with well-annotated CoSOD datasets. The exploration oftraining-free zero-shot CoSOD frameworks has been limited. In this paper,taking inspiration from the zero-shot transfer capabilities of foundationalcomputer vision models, we introduce the first zero-shot CoSOD framework thatharnesses these models without any training process. To achieve this, weintroduce two novel components in our proposed framework: the group promptgeneration (GPG) module and the co-saliency map generation (CMP) module. Weevaluate the framework's performance on widely-used datasets and observeimpressive results. Our approach surpasses existing unsupervised methods andeven outperforms fully supervised methods developed before 2020, whileremaining competitive with some fully supervised methods developed before 2022.</description><author>Haoke Xiao, Lv Tang, Bo Li, Zhiming Luo, Shaozi Li</author><pubDate>Mon, 11 Sep 2023 15:42:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05499v1</guid></item><item><title>Personality Detection and Analysis using Twitter Data</title><link>http://arxiv.org/abs/2309.05497v1</link><description>Personality types are important in various fields as they hold relevantinformation about the characteristics of a human being in an explainableformat. They are often good predictors of a person's behaviors in a particularenvironment and have applications ranging from candidate selection to marketingand mental health. Recently automatic detection of personality traits fromtexts has gained significant attention in computational linguistics. Mostpersonality detection and analysis methods have focused on small datasetsmaking their experimental observations often limited. To bridge this gap, wefocus on collecting and releasing the largest automatically curated dataset forthe research community which has 152 million tweets and 56 thousand data pointsfor the Myers-Briggs personality type (MBTI) prediction task. We perform aseries of extensive qualitative and quantitative studies on our dataset toanalyze the data patterns in a better way and infer conclusions. We show howour intriguing analysis results often follow natural intuition. We also performa series of ablation studies to show how the baselines perform for our dataset.</description><author>Abhilash Datta, Souvic Chakraborty, Animesh Mukherjee</author><pubDate>Mon, 11 Sep 2023 15:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05497v1</guid></item><item><title>Adaptive Top-K in SGD for Communication-Efficient Distributed Learning</title><link>http://arxiv.org/abs/2210.13532v2</link><description>Distributed stochastic gradient descent (SGD) with gradient compression hasbecome a popular communication-efficient solution for accelerating distributedlearning. One commonly used method for gradient compression is Top-Ksparsification, which sparsifies the gradients by a fixed degree during modeltraining. However, there has been a lack of an adaptive approach to adjust thesparsification degree to maximize the potential of the model's performance ortraining speed. This paper proposes a novel adaptive Top-K in SGD frameworkthat enables an adaptive degree of sparsification for each gradient descentstep to optimize the convergence performance by balancing the trade-off betweencommunication cost and convergence error. Firstly, an upper bound ofconvergence error is derived for the adaptive sparsification scheme and theloss function. Secondly, an algorithm is designed to minimize the convergenceerror under the communication cost constraints. Finally, numerical results onthe MNIST and CIFAR-10 datasets demonstrate that the proposed adaptive Top-Kalgorithm in SGD achieves a significantly better convergence rate compared tostate-of-the-art methods, even after considering error compensation.</description><author>Mengzhe Ruan, Guangfeng Yan, Yuanzhang Xiao, Linqi Song, Weitao Xu</author><pubDate>Mon, 11 Sep 2023 15:37:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.13532v2</guid></item><item><title>CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts</title><link>http://arxiv.org/abs/2309.05494v1</link><description>Social media platforms play an essential role in crisis communication, butanalyzing crisis-related social media texts is challenging due to theirinformal nature. Transformer-based pre-trained models like BERT and RoBERTahave shown success in various NLP tasks, but they are not tailored forcrisis-related texts. Furthermore, general-purpose sentence encoders are usedto generate sentence embeddings, regardless of the textual complexities incrisis-related texts. Advances in applications like text classification,semantic search, and clustering contribute to effective processing ofcrisis-related texts, which is essential for emergency responders to gain acomprehensive view of a crisis event, whether historical or real-time. Toaddress these gaps in crisis informatics literature, this study introducesCrisisTransformers, an ensemble of pre-trained language models and sentenceencoders trained on an extensive corpus of over 15 billion word tokens fromtweets associated with more than 30 crisis events, including disease outbreaks,natural disasters, conflicts, and other critical incidents. We evaluateexisting models and CrisisTransformers on 18 crisis-specific public datasets.Our pre-trained models outperform strong baselines across all datasets inclassification tasks, and our best-performing sentence encoder improves thestate-of-the-art by 17.43% in sentence encoding tasks. Additionally, weinvestigate the impact of model initialization on convergence and evaluate thesignificance of domain-specific models in generating semantically meaningfulsentence embeddings. All models are publicly released(https://huggingface.co/crisistransformers), with the anticipation that theywill serve as a robust baseline for tasks involving the analysis ofcrisis-related social media texts.</description><author>Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera</author><pubDate>Mon, 11 Sep 2023 15:36:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05494v1</guid></item><item><title>Multi-scale, Data-driven and Anatomically Constrained Deep Learning Image Registration for Adult and Fetal Echocardiography</title><link>http://arxiv.org/abs/2309.00831v2</link><description>Temporal echocardiography image registration is a basis for clinicalquantifications such as cardiac motion estimation, myocardial strainassessments, and stroke volume quantifications. In past studies, deep learningimage registration (DLIR) has shown promising results and is consistentlyaccurate and precise, requiring less computational time. We propose that agreater focus on the warped moving image's anatomic plausibility and imagequality can support robust DLIR performance. Further, past implementations havefocused on adult echocardiography, and there is an absence of DLIRimplementations for fetal echocardiography. We propose a framework thatcombines three strategies for DLIR in both fetal and adult echo: (1) ananatomic shape-encoded loss to preserve physiological myocardial and leftventricular anatomical topologies in warped images; (2) a data-driven loss thatis trained adversarially to preserve good image texture features in warpedimages; and (3) a multi-scale training scheme of a data-driven and anatomicallyconstrained algorithm to improve accuracy. Our tests show that good anatomicaltopology and image textures are strongly linked to shape-encoded anddata-driven adversarial losses. They improve different aspects of registrationperformance in a non-overlapping way, justifying their combination. Despitefundamental distinctions between adult and fetal echo images, we show thatthese strategies can provide excellent registration results in both adult andfetal echocardiography using the publicly available CAMUS adult echo datasetand our private multi-demographic fetal echo dataset. Our approach outperformstraditional non-DL gold standard registration approaches, including OpticalFlow and Elastix. Registration improvements could be translated to moreaccurate and precise clinical quantification of cardiac ejection fraction,demonstrating a potential for translation.</description><author>Md. Kamrul Hasan, Haobo Zhu, Guang Yang, Choon Hwai Yap</author><pubDate>Mon, 11 Sep 2023 15:34:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00831v2</guid></item><item><title>Learning Semantic Segmentation with Query Points Supervision on Aerial Images</title><link>http://arxiv.org/abs/2309.05490v1</link><description>Semantic segmentation is crucial in remote sensing, where high-resolutionsatellite images are segmented into meaningful regions. Recent advancements indeep learning have significantly improved satellite image segmentation.However, most of these methods are typically trained in fully supervisedsettings that require high-quality pixel-level annotations, which are expensiveand time-consuming to obtain. In this work, we present a weakly supervisedlearning algorithm to train semantic segmentation algorithms that only rely onquery point annotations instead of full mask labels. Our proposed approachperforms accurate semantic segmentation and improves efficiency bysignificantly reducing the cost and time required for manual annotation.Specifically, we generate superpixels and extend the query point labels intothose superpixels that group similar meaningful semantics. Then, we trainsemantic segmentation models, supervised with images partially labeled with thesuperpixels pseudo-labels. We benchmark our weakly supervised training approachon an aerial image dataset and different semantic segmentation architectures,showing that we can reach competitive performance compared to fully supervisedtraining while reducing the annotation effort.</description><author>Santiago Rivier, Carlos Hinojosa, Silvio Giancola, Bernard Ghanem</author><pubDate>Mon, 11 Sep 2023 15:32:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05490v1</guid></item><item><title>Leveraging Reviews: Learning to Price with Buyer and Seller Uncertainty</title><link>http://arxiv.org/abs/2302.09700v2</link><description>In online marketplaces, customers have access to hundreds of reviews for asingle product. Buyers often use reviews from other customers that share theirtype -- such as height for clothing, skin type for skincare products, andlocation for outdoor furniture -- to estimate their values, which they may notknow a priori. Customers with few relevant reviews may hesitate to make apurchase except at a low price, so for the seller, there is a tension betweensetting high prices and ensuring that there are enough reviews so that buyerscan confidently estimate their values. Simultaneously, sellers may use reviewsto gauge the demand for items they wish to sell. In this work, we study this pricing problem in an online setting where theseller interacts with a set of buyers of finitely many types, one by one, overa series of $T$ rounds. At each round, the seller first sets a price. Then abuyer arrives and examines the reviews of the previous buyers with the sametype, which reveal those buyers' ex-post values. Based on the reviews, thebuyer decides to purchase if they have good reason to believe that theirex-ante utility is positive. Crucially, the seller does not know the buyer'stype when setting the price, nor even the distribution over types. We provide ano-regret algorithm that the seller can use to obtain high revenue. When thereare $d$ types, after $T$ rounds, our algorithm achieves a problem-independent$\tilde O(T^{2/3}d^{1/3})$ regret bound. However, when the smallest probability$q_{\text{min}}$ that any given type appears is large, specifically when$q_{\text{min}} \in \Omega(d^{-2/3}T^{-1/3})$, then the same algorithm achievesa $\tilde O(T^{1/2}q_{\text{min}}^{-1/2})$ regret bound. We complement theseupper bounds with matching lower bounds in both regimes, showing that ouralgorithm is minimax optimal up to lower-order terms.</description><author>Wenshuo Guo, Nika Haghtalab, Kirthevasan Kandasamy, Ellen Vitercik</author><pubDate>Mon, 11 Sep 2023 15:19:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09700v2</guid></item><item><title>An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization</title><link>http://arxiv.org/abs/2307.04504v2</link><description>We study the complexity of producing $(\delta,\epsilon)$-stationary points ofLipschitz objectives which are possibly neither smooth nor convex, using onlynoisy function evaluations. Recent works proposed several stochastic zero-orderalgorithms that solve this task, all of which suffer from adimension-dependence of $\Omega(d^{3/2})$ where $d$ is the dimension of theproblem, which was conjectured to be optimal. We refute this conjecture byproviding a faster algorithm that has complexity$O(d\delta^{-1}\epsilon^{-3})$, which is optimal (up to numerical constants)with respect to $d$ and also optimal with respect to the accuracy parameters$\delta,\epsilon$, thus solving an open question due to Lin et al.(NeurIPS'22). Moreover, the convergence rate achieved by our algorithm is alsooptimal for smooth objectives, proving that in the nonconvex stochasticzero-order setting, nonsmooth optimization is as easy as smooth optimization.We provide algorithms that achieve the aforementioned convergence rate inexpectation as well as with high probability. Our analysis is based on a simpleyet powerful geometric lemma regarding the Goldstein-subdifferential set, whichallows utilizing recent advancements in first-order nonsmooth nonconvexoptimization.</description><author>Guy Kornowski, Ohad Shamir</author><pubDate>Mon, 11 Sep 2023 15:18:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04504v2</guid></item><item><title>Learning Objective-Specific Active Learning Strategies with Attentive Neural Processes</title><link>http://arxiv.org/abs/2309.05477v1</link><description>Pool-based active learning (AL) is a promising technology for increasingdata-efficiency of machine learning models. However, surveys show thatperformance of recent AL methods is very sensitive to the choice of dataset andtraining setting, making them unsuitable for general application. In order totackle this problem, the field Learning Active Learning (LAL) suggests to learnthe active learning strategy itself, allowing it to adapt to the given setting.In this work, we propose a novel LAL method for classification that exploitssymmetry and independence properties of the active learning problem with anAttentive Conditional Neural Process model. Our approach is based on learningfrom a myopic oracle, which gives our model the ability to adapt tonon-standard objectives, such as those that do not equally weight the error onall data points. We experimentally verify that our Neural Process modeloutperforms a variety of baselines in these settings. Finally, our experimentsshow that our model exhibits a tendency towards improved stability to changingdatasets. However, performance is sensitive to choice of classifier and morework is necessary to reduce the performance the gap with the myopic oracle andto improve scalability. We present our work as a proof-of-concept for LAL onnonstandard objectives and hope our analysis and modelling considerationsinspire future LAL work.</description><author>Tim Bakker, Herke van Hoof, Max Welling</author><pubDate>Mon, 11 Sep 2023 15:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05477v1</guid></item><item><title>Zero-shot Learning with Minimum Instruction to Extract Social Determinants and Family History from Clinical Notes using GPT Model</title><link>http://arxiv.org/abs/2309.05475v1</link><description>Demographics, Social determinants of health, and family history documented inthe unstructured text within the electronic health records are increasinglybeing studied to understand how this information can be utilized with thestructured data to improve healthcare outcomes. After the GPT models werereleased, many studies have applied GPT models to extract this information fromthe narrative clinical notes. Different from the existing work, our researchfocuses on investigating the zero-shot learning on extracting this informationtogether by providing minimum information to the GPT model. We utilizede-identified real-world clinical notes annotated for demographics, varioussocial determinants, and family history information. Given that the GPT modelmight provide text different from the text in the original data, we explore twosets of evaluation metrics, including the traditional NER evaluation metricsand semantic similarity evaluation metrics, to completely understand theperformance. Our results show that the GPT-3.5 method achieved an average of0.975 F1 on demographics extraction, 0.615 F1 on social determinantsextraction, and 0.722 F1 on family history extraction. We believe these resultscan be further improved through model fine-tuning or few-shots learning.Through the case studies, we also identified the limitations of the GPT models,which need to be addressed in future research.</description><author>Neel Jitesh Bhate, Ansh Mittal, Zhe He, Xiao Luo</author><pubDate>Mon, 11 Sep 2023 15:16:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05475v1</guid></item><item><title>Machine learning the dimension of a Fano variety</title><link>http://arxiv.org/abs/2309.05473v1</link><description>Fano varieties are basic building blocks in geometry - they are `atomicpieces' of mathematical shapes. Recent progress in the classification of Fanovarieties involves analysing an invariant called the quantum period. This is asequence of integers which gives a numerical fingerprint for a Fano variety. Itis conjectured that a Fano variety is uniquely determined by its quantumperiod. If this is true, one should be able to recover geometric properties ofa Fano variety directly from its quantum period. We apply machine learning tothe question: does the quantum period of X know the dimension of X? Note thatthere is as yet no theoretical understanding of this. We show that a simplefeed-forward neural network can determine the dimension of X with 98% accuracy.Building on this, we establish rigorous asymptotics for the quantum periods ofa class of Fano varieties. These asymptotics determine the dimension of X fromits quantum period. Our results demonstrate that machine learning can pick outstructure from complex mathematical data in situations where we lacktheoretical understanding. They also give positive evidence for the conjecturethat the quantum period of a Fano variety determines that variety.</description><author>Tom Coates, Alexander M. Kasprzyk, Sara Veneziale</author><pubDate>Mon, 11 Sep 2023 15:13:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05473v1</guid></item><item><title>LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech</title><link>http://arxiv.org/abs/2309.05472v1</link><description>Self-supervised learning (SSL) is at the origin of unprecedented improvementsin many different domains including computer vision and natural languageprocessing. Speech processing drastically benefitted from SSL as most of thecurrent domain-related tasks are now being approached with pre-trained models.This work introduces LeBenchmark 2.0 an open-source framework for assessing andbuilding SSL-equipped French speech technologies. It includes documented,large-scale and heterogeneous corpora with up to 14,000 hours of heterogeneousspeech, ten pre-trained SSL wav2vec 2.0 models containing from 26 million toone billion learnable parameters shared with the community, and an evaluationprotocol made of six downstream tasks to complement existing benchmarks.LeBenchmark 2.0 also presents unique perspectives on pre-trained SSL models forspeech with the investigation of frozen versus fine-tuned downstream models,task-agnostic versus task-specific pre-trained models as well as a discussionon the carbon footprint of large-scale model training.</description><author>Titouan Parcollet, Ha Nguyen, Solene Evain, Marcely Zanon Boito, Adrien Pupier, Salima Mdhaffar, Hang Le, Sina Alisamir, Natalia Tomashenko, Marco Dinarelli, Shucong Zhang, Alexandre Allauzen, Maximin Coavoux, Yannick Esteve, Mickael Rouvier, Jerome Goulian, Benjamin Lecouteux, Francois Portet, Solange Rossato, Fabien Ringeval, Didier Schwab, Laurent Besacier</author><pubDate>Mon, 11 Sep 2023 15:13:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05472v1</guid></item><item><title>Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks</title><link>http://arxiv.org/abs/2309.01838v2</link><description>Model stealing attacks have become a serious concern for deep learningmodels, where an attacker can steal a trained model by querying its black-boxAPI. This can lead to intellectual property theft and other security andprivacy risks. The current state-of-the-art defenses against model stealingattacks suggest adding perturbations to the prediction probabilities. However,they suffer from heavy computations and make impracticable assumptions aboutthe adversary. They often require the training of auxiliary models. This can betime-consuming and resource-intensive which hinders the deployment of thesedefenses in real-world applications. In this paper, we propose a simple yeteffective and efficient defense alternative. We introduce a heuristic approachto perturb the output probabilities. The proposed defense can be easilyintegrated into models without additional training. We show that our defense iseffective in defending against three state-of-the-art stealing attacks. Weevaluate our approach on large and quantized (i.e., compressed) ConvolutionalNeural Networks (CNNs) trained on several vision datasets. Our techniqueoutperforms the state-of-the-art defenses with a $\times37$ faster inferencelatency without requiring any additional model and with a low impact on themodel's performance. We validate that our defense is also effective forquantized CNNs targeting edge devices.</description><author>Kacem Khaled, Mouna Dhaouadi, Felipe Gohring de Magalhães, Gabriela Nicolescu</author><pubDate>Mon, 11 Sep 2023 15:09:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01838v2</guid></item><item><title>On Reducing Undesirable Behavior in Deep Reinforcement Learning Models</title><link>http://arxiv.org/abs/2309.02869v2</link><description>Deep reinforcement learning (DRL) has proven extremely useful in a largevariety of application domains. However, even successful DRL-based software canexhibit highly undesirable behavior. This is due to DRL training being based onmaximizing a reward function, which typically captures general trends butcannot precisely capture, or rule out, certain behaviors of the system. In thispaper, we propose a novel framework aimed at drastically reducing theundesirable behavior of DRL-based software, while maintaining its excellentperformance. In addition, our framework can assist in providing engineers witha comprehensible characterization of such undesirable behavior. Under the hood,our approach is based on extracting decision tree classifiers from erroneousstate-action pairs, and then integrating these trees into the DRL trainingloop, penalizing the system whenever it performs an error. We provide aproof-of-concept implementation of our approach, and use it to evaluate thetechnique on three significant case studies. We find that our approach canextend existing frameworks in a straightforward manner, and incurs only aslight overhead in training time. Further, it incurs only a very slight hit toperformance, or even in some cases - improves it, while significantly reducingthe frequency of undesirable behavior.</description><author>Ophir M. Carmel, Guy Katz</author><pubDate>Mon, 11 Sep 2023 15:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02869v2</guid></item><item><title>From Probabilistic Programming to Complexity-based Programming</title><link>http://arxiv.org/abs/2307.15453v2</link><description>The paper presents the main characteristics and a preliminary implementationof a novel computational framework named CompLog. Inspired by probabilisticprogramming systems like ProbLog, CompLog builds upon the inferentialmechanisms proposed by Simplicity Theory, relying on the computation of twoKolmogorov complexities (here implemented as min-path searches via ASPprograms) rather than probabilistic inference. The proposed system enablesusers to compute ex-post and ex-ante measures of unexpectedness of a certainsituation, mapping respectively to posterior and prior subjectiveprobabilities. The computation is based on the specification of world andmental models by means of causal and descriptive relations between predicatesweighted by complexity. The paper illustrates a few examples of application:generating relevant descriptions, and providing alternative approaches todisjunction and to negation.</description><author>Giovanni Sileno, Jean-Louis Dessalles</author><pubDate>Mon, 11 Sep 2023 15:06:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15453v2</guid></item><item><title>International Governance of Civilian AI: A Jurisdictional Certification Approach</title><link>http://arxiv.org/abs/2308.15514v2</link><description>This report describes trade-offs in the design of international governancearrangements for civilian artificial intelligence (AI) and presents oneapproach in detail. This approach represents the extension of a standards,licensing, and liability regime to the global level. We propose that statesestablish an International AI Organization (IAIO) to certify statejurisdictions (not firms or AI projects) for compliance with internationaloversight standards. States can give force to these international standards byadopting regulations prohibiting the import of goods whose supply chains embodyAI from non-IAIO-certified jurisdictions. This borrows attributes from modelsof existing international organizations, such as the International CivilianAviation Organization (ICAO), the International Maritime Organization (IMO),and the Financial Action Task Force (FATF). States can also adopt multilateralcontrols on the export of AI product inputs, such as specialized hardware, tonon-certified jurisdictions. Indeed, both the import and export standards couldbe required for certification. As international actors reach consensus on risksof and minimum standards for advanced AI, a jurisdictional certification regimecould mitigate a broad range of potential harms, including threats to publicsafety.</description><author>Robert Trager, Ben Harack, Anka Reuel, Allison Carnegie, Lennart Heim, Lewis Ho, Sarah Kreps, Ranjit Lall, Owen Larter, Seán Ó hÉigeartaigh, Simon Staffell, José Jaime Villalobos</author><pubDate>Mon, 11 Sep 2023 15:03:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15514v2</guid></item><item><title>Textbooks Are All You Need II: phi-1.5 technical report</title><link>http://arxiv.org/abs/2309.05463v1</link><description>We continue the investigation into the power of smaller Transformer-basedlanguage models as initiated by \textbf{TinyStories} -- a 10 million parametermodel that can produce coherent English -- and the follow-up work on\textbf{phi-1}, a 1.3 billion parameter model with Python coding performanceclose to the state-of-the-art. The latter work proposed to use existing LargeLanguage Models (LLMs) to generate ``textbook quality" data as a way to enhancethe learning process compared to traditional web data. We follow the``Textbooks Are All You Need" approach, focusing this time on common sensereasoning in natural language, and create a new 1.3 billion parameter modelnamed \textbf{phi-1.5}, with performance on natural language tasks comparableto models 5x larger, and surpassing most non-frontier LLMs on more complexreasoning tasks such as grade-school mathematics and basic coding. Moregenerally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs,both good -- such as the ability to ``think step by step" or perform somerudimentary in-context learning -- and bad, including hallucinations and thepotential for toxic and biased generations -- encouragingly though, we areseeing improvement on that front thanks to the absence of web data. Weopen-source \textbf{phi-1.5} to promote further research on these urgenttopics.</description><author>Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat Lee</author><pubDate>Mon, 11 Sep 2023 15:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05463v1</guid></item><item><title>Are Deep Neural Networks SMARTer than Second Graders?</title><link>http://arxiv.org/abs/2212.09993v6</link><description>Recent times have witnessed an increasing number of applications of deepneural networks towards solving tasks that require superior cognitiveabilities, e.g., playing Go, generating art, ChatGPT, etc. Such a dramaticprogress raises the question: how generalizable are neural networks in solvingproblems that demand broad skills? To answer this question, we propose SMART: aSimple Multimodal Algorithmic Reasoning Task and the associated SMART-101dataset, for evaluating the abstraction, deduction, and generalizationabilities of neural networks in solving visuo-linguistic puzzles designedspecifically for children in the 6--8 age group. Our dataset consists of 101unique puzzles; each puzzle comprises a picture and a question, and theirsolution needs a mix of several elementary skills, including arithmetic,algebra, and spatial reasoning, among others. To scale our dataset towardstraining deep neural networks, we programmatically generate entirely newinstances for each puzzle, while retaining their solution algorithm. Tobenchmark performances on SMART-101, we propose a vision and languagemeta-learning model using varied state-of-the-art backbones. Our experimentsreveal that while powerful deep models offer reasonable performances on puzzlesin a supervised setting, they are not better than random accuracy when analyzedfor generalization. We also evaluate the recent ChatGPT and other largelanguage models on a subset of SMART-101 and find that while these models showconvincing reasoning abilities, the answers are often incorrect.</description><author>Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Kevin A. Smith, Joshua B. Tenenbaum</author><pubDate>Mon, 11 Sep 2023 14:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09993v6</guid></item><item><title>Unveiling the Sentinels: Assessing AI Performance in Cybersecurity Peer Review</title><link>http://arxiv.org/abs/2309.05457v1</link><description>Peer review is the method employed by the scientific community for evaluatingresearch advancements. In the field of cybersecurity, the practice ofdouble-blind peer review is the de-facto standard. This paper touches on theholy grail of peer reviewing and aims to shed light on the performance of AI inreviewing for academic security conferences. Specifically, we investigate thepredictability of reviewing outcomes by comparing the results obtained fromhuman reviewers and machine-learning models. To facilitate our study, weconstruct a comprehensive dataset by collecting thousands of papers fromrenowned computer science conferences and the arXiv preprint website. Based onthe collected data, we evaluate the prediction capabilities of ChatGPT and atwo-stage classification approach based on the Doc2Vec model with variousclassifiers. Our experimental evaluation of review outcome prediction using theDoc2Vec-based approach performs significantly better than the ChatGPT andachieves an accuracy of over 90%. While analyzing the experimental results, weidentify the potential advantages and limitations of the tested ML models. Weexplore areas within the paper-reviewing process that can benefit fromautomated support approaches, while also recognizing the irreplaceable role ofhuman intellect in certain aspects that cannot be matched by state-of-the-artAI techniques.</description><author>Liang Niu, Nian Xue, Christina Pöpper</author><pubDate>Mon, 11 Sep 2023 14:51:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05457v1</guid></item><item><title>Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation</title><link>http://arxiv.org/abs/2309.05455v1</link><description>This paper describes a system developed for the GENEA (Generation andEvaluation of Non-verbal Behaviour for Embodied Agents) Challenge 2023. Oursolution builds on an existing diffusion-based motion synthesis model. Wepropose a contrastive speech and motion pretraining (CSMP) module, which learnsa joint embedding for speech and gesture with the aim to learn a semanticcoupling between these modalities. The output of the CSMP module is used as aconditioning signal in the diffusion-based gesture synthesis model in order toachieve semantically-aware co-speech gesture generation. Our entry achievedhighest human-likeness and highest speech appropriateness rating among thesubmitted entries. This indicates that our system is a promising approach toachieve human-like co-speech gestures in agents that carry semantic meaning.</description><author>Anna Deichler, Shivam Mehta, Simon Alexanderson, Jonas Beskow</author><pubDate>Mon, 11 Sep 2023 14:51:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05455v1</guid></item><item><title>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</title><link>http://arxiv.org/abs/2309.05454v1</link><description>Readability metrics and standards such as Flesch Kincaid Grade Level (FKGL)and the Common European Framework of Reference for Languages (CEFR) exist toguide teachers and educators to properly assess the complexity of educationalmaterials before administering them for classroom use. In this study, we selecta diverse set of open and closed-source instruction-tuned language models andinvestigate their performances in writing story completions and simplifyingnarratives$-$tasks that teachers perform$-$using standard-guided promptscontrolling text readability. Our extensive findings provide empirical proof ofhow globally recognized models like ChatGPT may be considered less effectiveand may require more refined prompts for these generative tasks compared toother open-sourced models such as BLOOMZ and FlanT5$-$which have shownpromising results.</description><author>Joseph Marvin Imperial, Harish Tayyar Madabushi</author><pubDate>Mon, 11 Sep 2023 14:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05454v1</guid></item><item><title>Evaluating the Deductive Competence of Large Language Models</title><link>http://arxiv.org/abs/2309.05452v1</link><description>The development of highly fluent large language models (LLMs) has promptedincreased interest in assessing their reasoning and problem-solvingcapabilities. We investigate whether several LLMs can solve a classic type ofdeductive reasoning problem from the cognitive science literature. The testedLLMs have limited abilities to solve these problems in their conventional form.We performed follow up experiments to investigate if changes to thepresentation format and content improve model performance. We do findperformance differences between conditions; however, they do not improveoverall performance. Moreover, we find that performance interacts withpresentation format and content in unexpected ways that differ from humanperformance. Overall, our results suggest that LLMs have unique reasoningbiases that are only partially predicted from human reasoning performance.</description><author>S. M. Seals, Valerie L. Shalin</author><pubDate>Mon, 11 Sep 2023 14:47:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05452v1</guid></item><item><title>Dual-view Curricular Optimal Transport for Cross-lingual Cross-modal Retrieval</title><link>http://arxiv.org/abs/2309.05451v1</link><description>Current research on cross-modal retrieval is mostly English-oriented, as theavailability of a large number of English-oriented human-labeledvision-language corpora. In order to break the limit of non-English labeleddata, cross-lingual cross-modal retrieval (CCR) has attracted increasingattention. Most CCR methods construct pseudo-parallel vision-language corporavia Machine Translation (MT) to achieve cross-lingual transfer. However, thetranslated sentences from MT are generally imperfect in describing thecorresponding visual contents. Improperly assuming the pseudo-parallel data arecorrectly correlated will make the networks overfit to the noisycorrespondence. Therefore, we propose Dual-view Curricular Optimal Transport(DCOT) to learn with noisy correspondence in CCR. In particular, we quantifythe confidence of the sample pair correlation with optimal transport theoryfrom both the cross-lingual and cross-modal views, and design dual-viewcurriculum learning to dynamically model the transportation costs according tothe learning stage of the two views. Extensive experiments are conducted on twomultilingual image-text datasets and one video-text dataset, and the resultsdemonstrate the effectiveness and robustness of the proposed method. Besides,our proposed method also shows a good expansibility to cross-lingual image-textbaselines and a decent generalization on out-of-domain data.</description><author>Yabing Wang, Shuhui Wang, Hao Luo, Jianfeng Dong, Fan Wang, Meng Han, Xun Wang, Meng Wang</author><pubDate>Mon, 11 Sep 2023 14:44:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05451v1</guid></item><item><title>Panoptic Vision-Language Feature Fields</title><link>http://arxiv.org/abs/2309.05448v1</link><description>Recently, methods have been proposed for 3D open-vocabulary semanticsegmentation. Such methods are able to segment scenes into arbitrary classesgiven at run-time using their text description. In this paper, we propose toour knowledge the first algorithm for open-vocabulary panoptic segmentation,simultaneously performing both semantic and instance segmentation. Ouralgorithm, Panoptic Vision-Language Feature Fields (PVLFF) learns a featurefield of the scene, jointly learning vision-language features and hierarchicalinstance features through a contrastive loss function from 2D instance segmentproposals on input frames. Our method achieves comparable performance againstthe state-of-the-art close-set 3D panoptic systems on the HyperSim, ScanNet andReplica dataset and outperforms current 3D open-vocabulary systems in terms ofsemantic segmentation. We additionally ablate our method to demonstrate theeffectiveness of our model architecture. Our code will be available athttps://github.com/ethz-asl/autolabel.</description><author>Haoran Chen, Kenneth Blomqvist, Francesco Milano, Roland Siegwart</author><pubDate>Mon, 11 Sep 2023 14:41:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05448v1</guid></item><item><title>TeGit: Generating High-Quality Instruction-Tuning Data with Text-Grounded Task Design</title><link>http://arxiv.org/abs/2309.05447v1</link><description>High-quality instruction-tuning data is critical to improving LLMcapabilities. Existing data collection methods are limited by unrealisticmanual labeling costs or by the hallucination of relying solely on LLMgeneration. To address the problems, this paper presents a scalable method toautomatically collect high-quality instructional adaptation data by traininglanguage models to automatically design tasks based on human-written texts.Intuitively, human-written text helps to help the model attenuate illusionsduring the generation of tasks. Unlike instruction back-translation-basedmethods that directly take the given text as a response, we require the modelto generate the \textit{instruction}, \textit{input}, and \textit{output}simultaneously to filter the noise. The results of the automated and manualevaluation experiments demonstrate the quality of our dataset.</description><author>Yongrui Chen, Haiyun Jiang, Xinting Huang, Shuming Shi, Guilin Qi</author><pubDate>Mon, 11 Sep 2023 14:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05447v1</guid></item><item><title>A Localization-to-Segmentation Framework for Automatic Tumor Segmentation in Whole-Body PET/CT Images</title><link>http://arxiv.org/abs/2309.05446v1</link><description>Fluorodeoxyglucose (FDG) positron emission tomography(PET) combined withcomputed tomography (CT) is considered the primary solution for detecting somecancers, such as lung cancer and melanoma. Automatic segmentation of tumors inPET/CT images can help reduce doctors' workload, thereby improving diagnosticquality. However, precise tumor segmentation is challenging due to the smallsize of many tumors and the similarity of high-uptake normal areas to the tumorregions. To address these issues, this paper proposes alocalization-to-segmentation framework (L2SNet) for precise tumor segmentation.L2SNet first localizes the possible lesions in the lesion localization phaseand then uses the location cues to shape the segmentation results in the lesionsegmentation phase. To further improve the segmentation performance of L2SNet,we design an adaptive threshold scheme that takes the segmentation results ofthe two phases into consideration. The experiments with the MICCAI 2023Automated Lesion Segmentation in Whole-Body FDG-PET/CT challenge dataset showthat our method achieved a competitive result and was ranked in the top 7methods on the preliminary test set. Our work is available at:https://github.com/MedCAI/L2SNet.</description><author>Linghan Cai, Jianhao Huang, Zihang Zhu, Jinpeng Lu, Yongbing Zhang</author><pubDate>Mon, 11 Sep 2023 14:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05446v1</guid></item><item><title>Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning</title><link>http://arxiv.org/abs/2309.05444v1</link><description>The Mixture of Experts (MoE) is a widely known neural architecture where anensemble of specialized sub-models optimizes overall performance with aconstant computational cost. However, conventional MoEs pose challenges atscale due to the need to store all experts in memory. In this paper, we pushMoE to the limit. We propose extremely parameter-efficient MoE by uniquelycombining MoE architecture with lightweight experts.Our MoE architectureoutperforms standard parameter-efficient fine-tuning (PEFT) methods and is onpar with full fine-tuning by only updating the lightweight experts -- less than1% of an 11B parameters model. Furthermore, our method generalizes to unseentasks as it does not depend on any prior task knowledge. Our researchunderscores the versatility of the mixture of experts architecture, showcasingits ability to deliver robust performance even when subjected to rigorousparameter constraints. Our code used in all the experiments is publiclyavailable here: https://github.com/for-ai/parameter-efficient-moe.</description><author>Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermiş, Acyr Locatelli, Sara Hooker</author><pubDate>Mon, 11 Sep 2023 14:31:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05444v1</guid></item><item><title>Towards Content-based Pixel Retrieval in Revisited Oxford and Paris</title><link>http://arxiv.org/abs/2309.05438v1</link><description>This paper introduces the first two pixel retrieval benchmarks. Pixelretrieval is segmented instance retrieval. Like semantic segmentation extendsclassification to the pixel level, pixel retrieval is an extension of imageretrieval and offers information about which pixels are related to the queryobject. In addition to retrieving images for the given query, it helps usersquickly identify the query object in true positive images and exclude falsepositive images by denoting the correlated pixels. Our user study results showpixel-level annotation can significantly improve the user experience. Compared with semantic and instance segmentation, pixel retrieval requires afine-grained recognition capability for variable-granularity targets. To thisend, we propose pixel retrieval benchmarks named PROxford and PRParis, whichare based on the widely used image retrieval datasets, ROxford and RParis.Three professional annotators label 5,942 images with two rounds ofdouble-checking and refinement. Furthermore, we conduct extensive experimentsand analysis on the SOTA methods in image search, image matching, detection,segmentation, and dense matching using our pixel retrieval benchmarks. Resultsshow that the pixel retrieval task is challenging to these approaches anddistinctive from existing problems, suggesting that further research canadvance the content-based pixel-retrieval and thus user search experience. Thedatasets can be downloaded from\href{https://github.com/anguoyuan/Pixel_retrieval-Segmented_instance_retrieval}{thislink}.</description><author>Guoyuan An, Woo Jae Kim, Saelyne Yang, Rong Li, Yuchi Huo, Sung-Eui Yoon</author><pubDate>Mon, 11 Sep 2023 14:21:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05438v1</guid></item><item><title>Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields</title><link>http://arxiv.org/abs/2308.11974v2</link><description>Text-driven localized editing of 3D objects is particularly difficult aslocally mixing the original 3D object with the intended new object and styleeffects without distorting the object's form is not a straightforward process.To address this issue, we propose a novel NeRF-based model, Blending-NeRF,which consists of two NeRF networks: pretrained NeRF and editable NeRF.Additionally, we introduce new blending operations that allow Blending-NeRF toproperly edit target regions which are localized by text. By using a pretrainedvision-language aligned model, CLIP, we guide Blending-NeRF to add new objectswith varying colors and densities, modify textures, and remove parts of theoriginal object. Our extensive experiments demonstrate that Blending-NeRFproduces naturally and locally edited 3D objects from various text prompts. Ourproject page is available at https://seokhunchoi.github.io/Blending-NeRF/</description><author>Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee, Taehyeong Kim</author><pubDate>Mon, 11 Sep 2023 14:18:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11974v2</guid></item><item><title>Quantized Fourier and Polynomial Features for more Expressive Tensor Network Models</title><link>http://arxiv.org/abs/2309.05436v1</link><description>In the context of kernel machines, polynomial and Fourier features arecommonly used to provide a nonlinear extension to linear models by mapping thedata to a higher-dimensional space. Unless one considers the dual formulationof the learning problem, which renders exact large-scale learning unfeasible,the exponential increase of model parameters in the dimensionality of the datacaused by their tensor-product structure prohibits to tackle high-dimensionalproblems. One of the possible approaches to circumvent this exponential scalingis to exploit the tensor structure present in the features by constraining themodel weights to be an underparametrized tensor network. In this paper wequantize, i.e. further tensorize, polynomial and Fourier features. Based onthis feature quantization we propose to quantize the associated model weights,yielding quantized models. We show that, for the same number of modelparameters, the resulting quantized models have a higher bound on theVC-dimension as opposed to their non-quantized counterparts, at no additionalcomputational cost while learning from identical features. We verifyexperimentally how this additional tensorization regularizes the learningproblem by prioritizing the most salient features in the data and how itprovides models with increased generalization capabilities. We finallybenchmark our approach on large regression task, achieving state-of-the-artresults on a laptop computer.</description><author>Frederiek Wesel, Kim Batselier</author><pubDate>Mon, 11 Sep 2023 14:18:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05436v1</guid></item><item><title>A parameterised model for link prediction using node centrality and similarity measure based on graph embedding</title><link>http://arxiv.org/abs/2309.05434v1</link><description>Link prediction is a key aspect of graph machine learning, with applicationsas diverse as disease prediction, social network recommendations, and drugdiscovery. It involves predicting new links that may form between networknodes. Despite the clear importance of link prediction, existing models havesignificant shortcomings. Graph Convolutional Networks, for instance, have beenproven to be highly efficient for link prediction on a variety of datasets.However, they encounter severe limitations when applied to short-path networksand ego networks, resulting in poor performance. This presents a criticalproblem space that this work aims to address. In this paper, we present theNode Centrality and Similarity Based Parameterised Model (NCSM), a novel methodfor link prediction tasks. NCSM uniquely integrates node centrality andsimilarity measures as edge features in a customised Graph Neural Network (GNN)layer, effectively leveraging the topological information of large networks.This model represents the first parameterised GNN-based link prediction modelthat considers topological information. The proposed model was evaluated onfive benchmark graph datasets, each comprising thousands of nodes and edges.Experimental results highlight NCSM's superiority over existingstate-of-the-art models like Graph Convolutional Networks and Variational GraphAutoencoder, as it outperforms them across various metrics and datasets. Thisexceptional performance can be attributed to NCSM's innovative integration ofnode centrality, similarity measures, and its efficient use of topologicalinformation.</description><author>Haohui Lu, Shahadat Uddin</author><pubDate>Mon, 11 Sep 2023 14:13:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05434v1</guid></item><item><title>Neuromorphic Auditory Perception by Neural Spiketrum</title><link>http://arxiv.org/abs/2309.05430v1</link><description>Neuromorphic computing holds the promise to achieve the energy efficiency androbust learning performance of biological neural systems. To realize thepromised brain-like intelligence, it needs to solve the challenges of theneuromorphic hardware architecture design of biological neural substrate andthe hardware amicable algorithms with spike-based encoding and learning. Herewe introduce a neural spike coding model termed spiketrum, to characterize andtransform the time-varying analog signals, typically auditory signals, intocomputationally efficient spatiotemporal spike patterns. It minimizes theinformation loss occurring at the analog-to-spike transformation and possessesinformational robustness to neural fluctuations and spike losses. The modelprovides a sparse and efficient coding scheme with precisely controllable spikerate that facilitates training of spiking neural networks in various auditoryperception tasks. We further investigate the algorithm-hardware co-designsthrough a neuromorphic cochlear prototype which demonstrates that our approachcan provide a systematic solution for spike-based artificial intelligence byfully exploiting its advantages with spike-based computation.</description><author>Huajin Tang, Pengjie Gu, Jayawan Wijekoon, MHD Anas Alsakkal, Ziming Wang, Jiangrong Shen, Rui Yan</author><pubDate>Mon, 11 Sep 2023 14:06:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05430v1</guid></item><item><title>Improving Information Extraction on Business Documents with Specific Pre-Training Tasks</title><link>http://arxiv.org/abs/2309.05429v1</link><description>Transformer-based Language Models are widely used in Natural LanguageProcessing related tasks. Thanks to their pre-training, they have beensuccessfully adapted to Information Extraction in business documents. However,most pre-training tasks proposed in the literature for business documents aretoo generic and not sufficient to learn more complex structures. In this paper,we use LayoutLM, a language model pre-trained on a collection of businessdocuments, and introduce two new pre-training tasks that further improve itscapacity to extract relevant information. The first is aimed at betterunderstanding the complex layout of documents, and the second focuses onnumeric values and their order of magnitude. These tasks force the model tolearn better-contextualized representations of the scanned documents. Wefurther introduce a new post-processing algorithm to decode BIESO tags inInformation Extraction that performs better with complex entities. Our methodsignificantly improves extraction performance on both public (from 93.88 to95.50 F1 score) and private (from 84.35 to 84.84 F1 score) datasets composed ofexpense receipts, invoices, and purchase orders.</description><author>Thibault Douzon, Stefan Duffner, Christophe Garcia, Jérémy Espinas</author><pubDate>Mon, 11 Sep 2023 14:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05429v1</guid></item><item><title>Quantum Ridgelet Transform: Winning Lottery Ticket of Neural Networks with Quantum Computation</title><link>http://arxiv.org/abs/2301.11936v2</link><description>A significant challenge in the field of quantum machine learning (QML) is toestablish applications of quantum computation to accelerate common tasks inmachine learning such as those for neural networks. Ridgelet transform has beena fundamental mathematical tool in the theoretical studies of neural networks,but the practical applicability of ridgelet transform to conducting learningtasks was limited since its numerical implementation by conventional classicalcomputation requires an exponential runtime $\exp(O(D))$ as data dimension $D$increases. To address this problem, we develop a quantum ridgelet transform(QRT), which implements the ridgelet transform of a quantum state within alinear runtime $O(D)$ of quantum computation. As an application, we also showthat one can use QRT as a fundamental subroutine for QML to efficiently find asparse trainable subnetwork of large shallow wide neural networks withoutconducting large-scale optimization of the original network. This applicationdiscovers an efficient way in this regime to demonstrate the lottery tickethypothesis on finding such a sparse trainable neural network. These resultsopen an avenue of QML for accelerating learning tasks with commonly usedclassical neural networks.</description><author>Hayata Yamasaki, Sathyawageeswar Subramanian, Satoshi Hayakawa, Sho Sonoda</author><pubDate>Mon, 11 Sep 2023 14:00:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.11936v2</guid></item><item><title>Reliable Joint Segmentation of Retinal Edema Lesions in OCT Images</title><link>http://arxiv.org/abs/2212.00330v3</link><description>Focusing on the complicated pathological features, such as blurredboundaries, severe scale differences between symptoms, background noiseinterference, etc., in the task of retinal edema lesions joint segmentationfrom OCT images and enabling the segmentation results more reliable. In thispaper, we propose a novel reliable multi-scale wavelet-enhanced transformernetwork, which can provide accurate segmentation results with reliabilityassessment. Specifically, aiming at improving the model's ability to learn thecomplex pathological features of retinal edema lesions in OCT images, wedevelop a novel segmentation backbone that integrates a wavelet-enhancedfeature extractor network and a multi-scale transformer module of our newlydesigned. Meanwhile, to make the segmentation results more reliable, a noveluncertainty segmentation head based on the subjective logical evidential theoryis introduced to generate the final segmentation results with a correspondingoverall uncertainty evaluation score map. We conduct comprehensive experimentson the public database of AI-Challenge 2018 for retinal edema lesionssegmentation, and the results show that our proposed method achieves bettersegmentation accuracy with a high degree of reliability as compared to otherstate-of-the-art segmentation approaches. The code will be released on:https://github.com/LooKing9218/ReliableRESeg.</description><author>Meng Wang, Kai Yu, Chun-Mei Feng, Ke Zou, Yanyu Xu, Qingquan Meng, Rick Siow Mong Goh, Yong Liu, Xinxing Xu, Huazhu Fu</author><pubDate>Mon, 11 Sep 2023 13:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00330v3</guid></item><item><title>No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function</title><link>http://arxiv.org/abs/2309.03224v2</link><description>Large language models (LLMs) demonstrate impressive language understandingand contextual learning abilities, making them suitable for natural languageprocessing (NLP) tasks and complex mathematical reasoning. However, whenapplied to mathematical reasoning tasks, LLMs often struggle to generatecorrect reasoning steps and answers despite having high probabilities for thesolutions. To overcome this limitation and enhance the mathematical reasoningcapabilities of fine-tuned LLMs without additional fine-tuning steps, wepropose a method that incorporates Monte Carlo Tree Search (MCTS) and alightweight energy function to rank decision steps and enable immediatereaction and precise reasoning. Specifically, we re-formulate the fine-tunedLLMs into a Residual-based Energy Model (Residual-EBM) and employ noisecontrastive estimation to estimate the energy function's parameters. We thenutilize MCTS with the energy function as a path verifier to search the outputspace and evaluate the reasoning path. Through extensive experiments on twomathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate theexceptional capabilities of our method, which significantly improves the pass@1metric of the fine-tuned model without requiring additional fine-tuning orreinforcement learning with human feedback alignment.</description><author>Haotian Xu</author><pubDate>Mon, 11 Sep 2023 13:50:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.03224v2</guid></item><item><title>Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of SSWP</title><link>http://arxiv.org/abs/2309.05423v1</link><description>In the realm of expressive Text-to-Speech (TTS), explicit prosodic boundariessignificantly advance the naturalness and controllability of synthesizedspeech. While human prosody annotation contributes a lot to the performance, itis a labor-intensive and time-consuming process, often resulting ininconsistent outcomes. Despite the availability of extensive supervised data,the current benchmark model still faces performance setbacks. To address thisissue, a two-stage automatic annotation pipeline is novelly proposed in thispaper. Specifically, in the first stage, we propose contrastive text-speechpretraining of Speech-Silence and Word-Punctuation (SSWP) pairs. Thepretraining procedure hammers at enhancing the prosodic space extracted fromjoint text-speech space. In the second stage, we build a multi-modal prosodyannotator, which consists of pretrained encoders, a straightforward yeteffective text-speech feature fusion scheme, and a sequence classifier.Extensive experiments conclusively demonstrate that our proposed method excelsat automatically generating prosody annotation and achieves state-of-the-art(SOTA) performance. Furthermore, our novel model has exhibited remarkableresilience when tested with varying amounts of data.</description><author>Jinzuomu Zhong, Yang Li, Hui Huang, Jie Liu, Zhiba Su, Jing Guo, Benlai Tang, Fengjie Zhu</author><pubDate>Mon, 11 Sep 2023 13:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05423v1</guid></item></channel></rss>