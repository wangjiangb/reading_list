<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 12 Nov 2024 01:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Recycled Attention: Efficient inference for long-context language models</title><link>http://arxiv.org/abs/2411.05787v1</link><description>Generating long sequences of tokens given a long-context input imposes aheavy computational burden for large language models (LLMs). One of thecomputational bottleneck comes from computing attention over a long sequence ofinput at each generation step. In this paper, we propose Recycled Attention, aninference-time method which alternates between full context attention andattention over a subset of input tokens. When performing partial attention, werecycle the attention pattern of a previous token that has performed fullattention and attend only to the top K most attended tokens, reducing the costof data movement and attention computation. Compared to previously proposedinference-time acceleration method which attends only to local context ortokens with high accumulative attention scores, our approach flexibly choosestokens that are relevant to the current decoding step. We evaluate our methodson RULER, a suite of tasks designed to comprehensively evaluate long-contextabilities, and long-context language modeling tasks. Applying our method tooff-the-shelf LLMs achieves comparable speedup to baselines which only considerlocal context while improving the performance by 2x. We further explore twoideas to improve performance-efficiency trade-offs: (1) dynamically decide whento perform recycled or full attention step based on the query similarities and(2) continued pre-training the model with Recycled Attention.</description><author>Fangyuan Xu, Tanya Goyal, Eunsol Choi</author><pubDate>Fri, 08 Nov 2024 18:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05787v1</guid></item><item><title>Is ChatGPT Transforming Academics' Writing Style?</title><link>http://arxiv.org/abs/2404.08627v2</link><description>Based on one million arXiv papers submitted from May 2018 to January 2024, weassess the textual density of ChatGPT's writing style in their abstractsthrough a statistical analysis of word frequency changes. Our model iscalibrated and validated on a mixture of real abstracts and ChatGPT-modifiedabstracts (simulated data) after a careful noise analysis. The words used forestimation are not fixed but adaptive, including those with decreasingfrequency. We find that large language models (LLMs), represented by ChatGPT,are having an increasing impact on arXiv abstracts, especially in the field ofcomputer science, where the fraction of LLM-style abstracts is estimated to beapproximately 35%, if we take the responses of GPT-3.5 to one simple prompt,"revise the following sentences", as a baseline. We conclude with an analysisof both positive and negative aspects of the penetration of LLMs intoacademics' writing style.</description><author>Mingmeng Geng, Roberto Trotta</author><pubDate>Fri, 08 Nov 2024 18:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.08627v2</guid></item><item><title>Logits of API-Protected LLMs Leak Proprietary Information</title><link>http://arxiv.org/abs/2403.09539v3</link><description>Large language model (LLM) providers often hide the architectural details andparameters of their proprietary models by restricting public access to alimited API. In this work we show that, with only a conservative assumptionabout the model architecture, it is possible to learn a surprisingly largeamount of non-public information about an API-protected LLM from a relativelysmall number of API queries (e.g., costing under $1000 USD for OpenAI'sgpt-3.5-turbo). Our findings are centered on one key observation: most modernLLMs suffer from a softmax bottleneck, which restricts the model outputs to alinear subspace of the full output space. We exploit this fact to unlockseveral capabilities, including (but not limited to) obtaining cheapfull-vocabulary outputs, auditing for specific types of model updates,identifying the source LLM given a single full LLM output, and even efficientlydiscovering the LLM's hidden size. Our empirical investigations show theeffectiveness of our methods, which allow us to estimate the embedding size ofOpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLMproviders can guard against these attacks, as well as how these capabilitiescan be viewed as a feature (rather than a bug) by allowing for greatertransparency and accountability.</description><author>Matthew Finlayson, Xiang Ren, Swabha Swayamdipta</author><pubDate>Fri, 08 Nov 2024 18:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09539v3</guid></item><item><title>ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles</title><link>http://arxiv.org/abs/2411.05783v1</link><description>Deaf and hard-of-hearing (DHH) students face significant barriers inaccessing science, technology, engineering, and mathematics (STEM) education,notably due to the scarcity of STEM resources in signed languages. To helpaddress this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipediaarticles on STEM topics in English, interpreted into over 300 hours of AmericanSign Language (ASL). ASL STEM Wiki is the first continuous signing datasetfocused on STEM, facilitating the development of AI resources for STEMeducation in ASL. We identify several use cases of ASL STEM Wiki withhuman-centered applications. For example, because this dataset highlights thefrequent use of fingerspelling for technical concepts, which inhibits DHHstudents' ability to learn, we develop models to identify fingerspelled words-- which can later be used to query for appropriate ASL signs to suggest tointerpreters.</description><author>Kayo Yin, Chinmay Singh, Fyodor O. Minakov, Vanessa Milan, Hal Daum√© III, Cyril Zhang, Alex X. Lu, Danielle Bragg</author><pubDate>Fri, 08 Nov 2024 18:50:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05783v1</guid></item><item><title>Using Language Models to Disambiguate Lexical Choices in Translation</title><link>http://arxiv.org/abs/2411.05781v1</link><description>In translation, a concept represented by a single word in a source languagecan have multiple variations in a target language. The task of lexicalselection requires using context to identify which variation is mostappropriate for a source text. We work with native speakers of nine languagesto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingualconcept variation when translating from English. We evaluate recent LLMs andneural machine translation systems on DTAiLS, with the best-performing model,GPT-4, achieving from 67 to 85% accuracy across languages. Finally, we uselanguage models to generate English rules describing target-language conceptvariations. Providing weaker models with high-quality lexical rules improvesaccuracy substantially, in some cases reaching or outperforming GPT-4.</description><author>Josh Barua, Sanjay Subramanian, Kayo Yin, Alane Suhr</author><pubDate>Fri, 08 Nov 2024 18:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05781v1</guid></item><item><title>GazeSearch: Radiology Findings Search Benchmark</title><link>http://arxiv.org/abs/2411.05780v1</link><description>Medical eye-tracking data is an important information source forunderstanding how radiologists visually interpret medical images. Thisinformation not only improves the accuracy of deep learning models for X-rayanalysis but also their interpretability, enhancing transparency indecision-making. However, the current eye-tracking data is dispersed,unprocessed, and ambiguous, making it difficult to derive meaningful insights.Therefore, there is a need to create a new dataset with more focus andpurposeful eyetracking data, improving its utility for diagnostic applications.In this work, we propose a refinement method inspired by the target-presentvisual search challenge: there is a specific finding and fixations are guidedto locate it. After refining the existing eye-tracking datasets, we transformthem into a curated visual search dataset, called GazeSearch, specifically forradiology findings, where each fixation sequence is purposefully aligned to thetask of locating a particular finding. Subsequently, we introduce a scan pathprediction baseline, called ChestSearch, specifically tailored to GazeSearch.Finally, we employ the newly introduced GazeSearch as a benchmark to evaluatethe performance of current state-of-the-art methods, offering a comprehensiveassessment for visual search in the medical imaging domain.</description><author>Trong Thang Pham, Tien-Phat Nguyen, Yuki Ikebe, Akash Awasthi, Zhigang Deng, Carol C. Wu, Hien Nguyen, Ngan Le</author><pubDate>Fri, 08 Nov 2024 18:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05780v1</guid></item><item><title>Curriculum Learning for Few-Shot Domain Adaptation in CT-based Airway Tree Segmentation</title><link>http://arxiv.org/abs/2411.05779v1</link><description>Despite advances with deep learning (DL), automated airway segmentation fromchest CT scans continues to face challenges in segmentation quality andgeneralization across cohorts. To address these, we propose integratingCurriculum Learning (CL) into airway segmentation networks, distributing thetraining set into batches according to ad-hoc complexity scores derived from CTscans and corresponding ground-truth tree features. We specifically investigatefew-shot domain adaptation, targeting scenarios where manual annotation of afull fine-tuning dataset is prohibitively expensive. Results are reported ontwo large open-cohorts (ATM22 and AIIB23) with high performance using CL forfull training (Source domain) and few-shot fine-tuning (Target domain), butwith also some insights on potential detrimental effects if using a classicBootstrapping scoring function or if not using proper scan sequencing.</description><author>Maxime Jacovella, Ali Keshavarzi, Elsa Angelini</author><pubDate>Fri, 08 Nov 2024 18:46:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05779v1</guid></item><item><title>LLMs as Method Actors: A Model for Prompt Engineering and Architecture</title><link>http://arxiv.org/abs/2411.05778v1</link><description>We introduce "Method Actors" as a mental model for guiding LLM promptengineering and prompt architecture. Under this mental model, LLMs should bethought of as actors; prompts as scripts and cues; and LLM responses asperformances. We apply this mental model to the task of improving LLMperformance at playing Connections, a New York Times word puzzle game thatprior research identified as a challenging benchmark for evaluating LLMreasoning. Our experiments with GPT-4o show that a "Method Actors" approach cansignificantly improve LLM performance over both a vanilla and "Chain ofThoughts" approach. A vanilla approach solves 27% of Connections puzzles in ourdataset and a "Chain of Thoughts" approach solves 41% of puzzles, whereas ourstrongest "Method Actor" approach solves 86% of puzzles. We also test OpenAI'snewest model designed specifically for complex reasoning tasks, o1-preview.When asked to solve a puzzle all at once, o1-preview solves 79% of Connectionspuzzles in our dataset, and when allowed to build puzzle solutions one guess ata time over multiple API calls, o1-preview solves 100% of the puzzles.Incorporating a "Method Actor" prompt architecture increases the percentage ofpuzzles that o1-preview solves perfectly from 76% to 87%.</description><author>Colin Doyle</author><pubDate>Fri, 08 Nov 2024 18:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05778v1</guid></item><item><title>Quantitative Assessment of Intersectional Empathetic Bias and Understanding</title><link>http://arxiv.org/abs/2411.05777v1</link><description>A growing amount of literature critiques the current operationalizations ofempathy based on loose definitions of the construct. Such definitionsnegatively affect dataset quality, model robustness, and evaluationreliability. We propose an empathy evaluation framework that operationalizesempathy close to its psychological origins. The framework measures the variancein responses of LLMs to prompts using existing metrics for empathy andemotional valence. The variance is introduced through the controlled generationof the prompts by varying social biases affecting context understanding, thusimpacting empathetic understanding. The control over generation ensures hightheoretical validity of the constructs in the prompt dataset. Also, it makeshigh-quality translation, especially into languages that currently havelittle-to-no way of evaluating empathy or bias, such as the Slavonic family,more manageable. Using chosen LLMs and various prompt types, we demonstrate theempathy evaluation with the framework, including multiple-choice answers andfree generation. The variance in our initial evaluation sample is small and wewere unable to measure convincing differences between the empatheticunderstanding in contexts given by different social groups. However, theresults are promising because the models showed significant alterations theirreasoning chains needed to capture the relatively subtle changes in theprompts. This provides the basis for future research into the construction ofthe evaluation sample and statistical methods for measuring the results.</description><author>Vojtech Formanek, Ondrej Sotolar</author><pubDate>Fri, 08 Nov 2024 18:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05777v1</guid></item><item><title>Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?</title><link>http://arxiv.org/abs/2411.05775v1</link><description>Political misinformation poses significant challenges to democraticprocesses, shaping public opinion and trust in media. Manual fact-checkingmethods face issues of scalability and annotator bias, while machine learningmodels require large, costly labelled datasets. This study investigates the useof state-of-the-art large language models (LLMs) as reliable annotators fordetecting political factuality in news articles. Using open-source LLMs, wecreate a politically diverse dataset, labelled for bias through LLM-generatedannotations. These annotations are validated by human experts and furtherevaluated by LLM-based judges to assess the accuracy and reliability of theannotations. Our approach offers a scalable and robust alternative totraditional fact-checking, enhancing transparency and public trust in media.</description><author>Veronica Chatrath, Marcelo Lotif, Shaina Raza</author><pubDate>Fri, 08 Nov 2024 18:36:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05775v1</guid></item><item><title>Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs</title><link>http://arxiv.org/abs/2404.04264v4</link><description>Despite the superb performance in many tasks, large language models (LLMs)bear the risk of generating hallucination or even wrong answers when confrontedwith tasks that demand the accuracy of knowledge. The issue becomes even morenoticeable when addressing logic queries that require multiple logic reasoningsteps. On the other hand, knowledge graph (KG) based question answering methodsare capable of accurately identifying the correct answers with the help ofknowledge graph, yet its accuracy could quickly deteriorate when the knowledgegraph itself is sparse and incomplete. It remains a critical challenge on howto integrate knowledge graph reasoning with LLMs in a mutually beneficial wayso as to mitigate both the hallucination problem of LLMs as well as theincompleteness issue of knowledge graphs. In this paper, we propose'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMswith knowledge graph based logic query reasoning. LGOT seamlessly combinesknowledge graph reasoning and LLMs, effectively breaking down complex logicqueries into easy to answer subquestions. Through the utilization of bothknowledge graph reasoning and LLMs, it successfully derives answers for eachsubquestion. By aggregating these results and selecting the highest qualitycandidate answers for each step, LGOT achieves accurate results to complexquestions. Our experimental findings demonstrate substantial performanceenhancements, with up to 20% improvement over ChatGPT.</description><author>Lihui Liu, Zihao Wang, Ruizhong Qiu, Yikun Ban, Eunice Chan, Yangqiu Song, Jingrui He, Hanghang Tong</author><pubDate>Fri, 08 Nov 2024 18:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.04264v4</guid></item><item><title>Sketched Equivariant Imaging Regularization and Deep Internal Learning for Inverse Problems</title><link>http://arxiv.org/abs/2411.05771v1</link><description>Equivariant Imaging (EI) regularization has become the de-facto technique forunsupervised training of deep imaging networks, without any need ofground-truth data. Observing that the EI-based unsupervised training paradigmcurrently has significant computational redundancy leading to inefficiency inhigh-dimensional applications, we propose a sketched EI regularization whichleverages the randomized sketching techniques for acceleration. We then extendour sketched EI regularization to develop an accelerated deep internal learningframework -- Sketched Equivariant Deep Image Prior (Sk.EI-DIP), which can beefficiently applied for single-image and task-adapted reconstruction. Ournumerical study on X-ray CT image reconstruction tasks demonstrate that ourapproach can achieve order-of-magnitude computational acceleration overstandard EI-based counterpart in single-input setting, and network adaptationat test time.</description><author>Guixian Xu, Jinglai Li, Junqi Tang</author><pubDate>Fri, 08 Nov 2024 18:33:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05771v1</guid></item><item><title>SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</title><link>http://arxiv.org/abs/2411.05007v2</link><description>Diffusion models have been proven highly effective at generating high-qualityimages. However, as these models grow larger, they require significantly morememory and suffer from higher latency, posing substantial challenges fordeployment. In this work, we aim to accelerate diffusion models by quantizingtheir weights and activations to 4 bits. At such an aggressive level, bothweights and activations are highly sensitive, where conventional post-trainingquantization methods for large language models like smoothing becomeinsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bitquantization paradigm. Different from smoothing which redistributes outliersbetween weights and activations, our approach absorbs these outliers using alow-rank branch. We first consolidate the outliers by shifting them fromactivations to weights, then employ a high-precision low-rank branch to take inthe weight outliers with Singular Value Decomposition (SVD). This process easesthe quantization on both sides. However, na\"{\i}vely running the low-rankbranch independently incurs significant overhead due to extra data movement ofactivations, negating the quantization speedup. To address this, we co-designan inference engine Nunchaku that fuses the kernels of the low-rank branch intothose of the low-bit branch to cut off redundant memory access. It can alsoseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need forre-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1validate the effectiveness of SVDQuant in preserving image quality. We reducethe memory usage for the 12B FLUX.1 models by 3.5$\times$, achieving3.0$\times$ speedup over the 4-bit weight-only quantized baseline on the 16GBlaptop 4090 GPU, paving the way for more interactive applications on PCs. Ourquantization library and inference engine are open-sourced.</description><author>Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han</author><pubDate>Fri, 08 Nov 2024 18:32:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05007v2</guid></item><item><title>FinDVer: Explainable Claim Verification over Long and Hybrid-Content Financial Documents</title><link>http://arxiv.org/abs/2411.05764v1</link><description>We introduce FinDVer, a comprehensive benchmark specifically designed toevaluate the explainable claim verification capabilities of LLMs in the contextof understanding and analyzing long, hybrid-content financial documents.FinDVer contains 2,400 expert-annotated examples, divided into three subsets:information extraction, numerical reasoning, and knowledge-intensive reasoning,each addressing common scenarios encountered in real-world financial contexts.We assess a broad spectrum of LLMs under long-context and RAG settings. Ourresults show that even the current best-performing system, GPT-4o, still lagsbehind human experts. We further provide in-depth analysis on long-context andRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offeringinsights to drive future advancements. We believe that FinDVer can serve as avaluable benchmark for evaluating LLMs in claim verification over complex,expert-domain documents.</description><author>Yilun Zhao, Yitao Long, Yuru Jiang, Chengye Wang, Weiyuan Chen, Hongjun Liu, Yiming Zhang, Xiangru Tang, Chen Zhao, Arman Cohan</author><pubDate>Fri, 08 Nov 2024 18:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05764v1</guid></item><item><title>Multi-hop Evidence Pursuit Meets the Web: Team Papelo at FEVER 2024</title><link>http://arxiv.org/abs/2411.05762v1</link><description>Separating disinformation from fact on the web has long challenged both thesearch and the reasoning powers of humans. We show that the reasoning power oflarge language models (LLMs) and the retrieval power of modern search enginescan be combined to automate this process and explainably verify claims. Weintegrate LLMs and search under a multi-hop evidence pursuit strategy. Thisstrategy generates an initial question based on an input claim using a sequenceto sequence model, searches and formulates an answer to the question, anditeratively generates follow-up questions to pursue the evidence that ismissing using an LLM. We demonstrate our system on the FEVER 2024 (AVeriTeC)shared task. Compared to a strategy of generating all the questions at once,our method obtains .045 higher label accuracy and .155 higher AVeriTeC score(evaluating the adequacy of the evidence). Through ablations, we show theimportance of various design choices, such as the question generation method,medium-sized context, reasoning with one document at a time, adding metadata,paraphrasing, reducing the problem to two classes, and reconsidering the finalverdict. Our submitted system achieves .510 AVeriTeC score on the dev set and.477 AVeriTeC score on the test set.</description><author>Christopher Malon</author><pubDate>Fri, 08 Nov 2024 18:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05762v1</guid></item><item><title>Physics-Aware Combinatorial Assembly Sequence Planning using Data-free Action Masking</title><link>http://arxiv.org/abs/2408.10162v2</link><description>Combinatorial assembly uses standardized unit primitives to build objectsthat satisfy user specifications. This paper studies assembly sequence planning(ASP) for physical combinatorial assembly. Given the shape of the desiredobject, the goal is to find a sequence of actions for placing unit primitivesto build the target object. In particular, we aim to ensure the plannedassembly sequence is physically executable. However, ASP for combinatorialassembly is particularly challenging due to its combinatorial nature. Toaddress the challenge, we employ deep reinforcement learning to learn aconstruction policy for placing unit primitives sequentially to build thedesired object. Specifically, we design an online physics-aware action maskthat filters out invalid actions, which effectively guides policy learning andensures violation-free deployment. In the end, we apply the proposed method toLego assembly with more than 250 3D structures. The experiment resultsdemonstrate that the proposed method plans physically valid assembly sequencesto build all structures, achieving a $100\%$ success rate, whereas the bestcomparable baseline fails more than $40$ structures. Our implementation isavailable at\url{https://github.com/intelligent-control-lab/PhysicsAwareCombinatorialASP}.</description><author>Ruixuan Liu, Alan Chen, Weiye Zhao, Changliu Liu</author><pubDate>Fri, 08 Nov 2024 18:22:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10162v2</guid></item><item><title>Meta-models for transfer learning in source localisation</title><link>http://arxiv.org/abs/2305.08657v2</link><description>In practice, non-destructive testing (NDT) procedures tend to considerexperiments (and their respective models) as distinct, conducted in isolationand associated with independent data. In contrast, this work looks to capturethe interdependencies between acoustic emission (AE) experiments (asmeta-models) and then use the resulting functions to predict the modelhyperparameters for previously unobserved systems. We utilise a Bayesianmultilevel approach (similar to deep Gaussian Processes) where a higher levelmeta-model captures the inter-task relationships. Our key contribution is howknowledge of the experimental campaign can be encoded between tasks as well aswithin tasks. We present an example of AE time-of-arrival mapping for sourcelocalisation, to illustrate how multilevel models naturally lend themselves torepresenting aggregate systems in engineering. We constrain the meta-modelbased on domain knowledge, then use the inter-task functions for transferlearning, predicting hyperparameters for models of previously unobservedexperiments (for a specific design).</description><author>Lawrence A. Bull, Matthew R. Jones, Elizabeth J. Cross, Andrew Duncan, Mark Girolami</author><pubDate>Fri, 08 Nov 2024 18:18:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08657v2</guid></item><item><title>Tract-RLFormer: A Tract-Specific RL policy based Decoder-only Transformer Network</title><link>http://arxiv.org/abs/2411.05757v1</link><description>Fiber tractography is a cornerstone of neuroimaging, enabling the detailedmapping of the brain's white matter pathways through diffusion MRI. This iscrucial for understanding brain connectivity and function, making it a valuabletool in neurological applications. Despite its importance, tractography faceschallenges due to its complexity and susceptibility to false positives,misrepresenting vital pathways. To address these issues, recent strategies haveshifted towards deep learning, utilizing supervised learning, which depends onprecise ground truth, or reinforcement learning, which operates without it. Inthis work, we propose Tract-RLFormer, a network utilizing both supervised andreinforcement learning, in a two-stage policy refinement process that markedlyimproves the accuracy and generalizability across various data-sets. Byemploying a tract-specific approach, our network directly delineates the tractsof interest, bypassing the traditional segmentation process. Through rigorousvalidation on datasets such as TractoInferno, HCP, and ISMRM-2015, ourmethodology demonstrates a leap forward in tractography, showcasing its abilityto accurately map the brain's white matter tracts.</description><author>Ankita Joshi, Ashutosh Sharma, Anoushkrit Goel, Ranjeet Ranjan Jha, Chirag Ahuja, Arnav Bhavsar, Aditya Nigam</author><pubDate>Fri, 08 Nov 2024 18:18:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05757v1</guid></item><item><title>Optimization without Retraction on the Random Generalized Stiefel Manifold</title><link>http://arxiv.org/abs/2405.01702v3</link><description>Optimization over the set of matrices $X$ that satisfy $X^\top B X = I_p$,referred to as the generalized Stiefel manifold, appears in many applicationsinvolving sampled covariance matrices such as the canonical correlationanalysis (CCA), independent component analysis (ICA), and the generalizedeigenvalue problem (GEVP). Solving these problems is typically done byiterative methods that require a fully formed $B$. We propose a cheapstochastic iterative method that solves the optimization problem while havingaccess only to random estimates of $B$. Our method does not enforce theconstraint in every iteration; instead, it produces iterations that converge tocritical points on the generalized Stiefel manifold defined in expectation. Themethod has lower per-iteration cost, requires only matrix multiplications, andhas the same convergence rates as its Riemannian optimization counterparts thatrequire the full matrix $B$. Experiments demonstrate its effectiveness invarious machine learning applications involving generalized orthogonalityconstraints, including CCA, ICA, and the GEVP.</description><author>Simon Vary, Pierre Ablin, Bin Gao, P. -A. Absil</author><pubDate>Fri, 08 Nov 2024 18:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01702v3</guid></item><item><title>End-to-End Navigation with Vision Language Models: Transforming Spatial Reasoning into Question-Answering</title><link>http://arxiv.org/abs/2411.05755v1</link><description>We present VLMnav, an embodied framework to transform a Vision-Language Model(VLM) into an end-to-end navigation policy. In contrast to prior work, we donot rely on a separation between perception, planning, and control; instead, weuse a VLM to directly select actions in one step. Surprisingly, we find that aVLM can be used as an end-to-end policy zero-shot, i.e., without anyfine-tuning or exposure to navigation data. This makes our approach open-endedand generalizable to any downstream navigation task. We run an extensive studyto evaluate the performance of our approach in comparison to baseline promptingmethods. In addition, we perform a design analysis to understand the mostimpactful design decisions. Visual examples and code for our project can befound at https://jirl-upenn.github.io/VLMnav/</description><author>Dylan Goetting, Himanshu Gaurav Singh, Antonio Loquercio</author><pubDate>Fri, 08 Nov 2024 18:16:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05755v1</guid></item><item><title>FisherMask: Enhancing Neural Network Labeling Efficiency in Image Classification Using Fisher Information</title><link>http://arxiv.org/abs/2411.05752v1</link><description>Deep learning (DL) models are popular across various domains due to theirremarkable performance and efficiency. However, their effectiveness reliesheavily on large amounts of labeled data, which are often time-consuming andlabor-intensive to generate manually. To overcome this challenge, it isessential to develop strategies that reduce reliance on extensive labeled datawhile preserving model performance. In this paper, we propose FisherMask, aFisher information-based active learning (AL) approach that identifies keynetwork parameters by masking them based on their Fisher information values.FisherMask enhances batch AL by using Fisher information to select the mostcritical parameters, allowing the identification of the most impactful samplesduring AL training. Moreover, Fisher information possesses favorablestatistical properties, offering valuable insights into model behavior andproviding a better understanding of the performance characteristics within theAL pipeline. Our extensive experiments demonstrate that FisherMasksignificantly outperforms state-of-the-art methods on diverse datasets,including CIFAR-10 and FashionMNIST, especially under imbalanced settings.These improvements lead to substantial gains in labeling efficiency. Henceserving as an effective tool to measure the sensitivity of model parameters todata samples. Our code is available on\url{https://github.com/sgchr273/FisherMask}.</description><author>Shreen Gul, Mohamed Elmahallawy, Sanjay Madria, Ardhendu Tripathy</author><pubDate>Fri, 08 Nov 2024 18:10:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05752v1</guid></item><item><title>On Differentially Private String Distances</title><link>http://arxiv.org/abs/2411.05750v1</link><description>Given a database of bit strings $A_1,\ldots,A_m\in \{0,1\}^n$, a fundamentaldata structure task is to estimate the distances between a given query $B\in\{0,1\}^n$ with all the strings in the database. In addition, one might furtherwant to ensure the integrity of the database by releasing these distancestatistics in a secure manner. In this work, we propose differentially private(DP) data structures for this type of tasks, with a focus on Hamming and editdistance. On top of the strong privacy guarantees, our data structures are alsotime- and space-efficient. In particular, our data structure is $\epsilon$-DPagainst any sequence of queries of arbitrary length, and for any query $B$ suchthat the maximum distance to any string in the database is at most $k$, weoutput $m$ distance estimates. Moreover, - For Hamming distance, our data structure answers any query in $\widetildeO(mk+n)$ time and each estimate deviates from the true distance by at most$\widetilde O(k/e^{\epsilon/\log k})$; - For edit distance, our data structure answers any query in $\widetildeO(mk^2+n)$ time and each estimate deviates from the true distance by at most$\widetilde O(k/e^{\epsilon/(\log k \log n)})$. For moderate $k$, both data structures support sublinear query operations. Weobtain these results via a novel adaptation of the randomized responsetechnique as a bit flipping procedure, applied to the sketched strings.</description><author>Jerry Yao-Chieh Hu, Erzhi Liu, Han Liu, Zhao Song, Lichen Zhang</author><pubDate>Fri, 08 Nov 2024 18:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05750v1</guid></item><item><title>Multi-Dimensional Reconfigurable, Physically Composable Hybrid Diffractive Optical Neural Network</title><link>http://arxiv.org/abs/2411.05748v1</link><description>Diffractive optical neural networks (DONNs), leveraging free-space light wavepropagation for ultra-parallel, high-efficiency computing, have emerged aspromising artificial intelligence (AI) accelerators. However, their inherentlack of reconfigurability due to fixed optical structures post-fabricationhinders practical deployment in the face of dynamic AI workloads and evolvingapplications. To overcome this challenge, we introduce, for the first time, amulti-dimensional reconfigurable hybrid diffractive ONN system (MDR-HDONN), aphysically composable architecture that unlocks a new degree of freedom andunprecedented versatility in DONNs. By leveraging full-system learnability,MDR-HDONN repurposes fixed fabricated optical hardware, achieving exponentiallyexpanded functionality and superior task adaptability through thedifferentiable learning of system variables. Furthermore, MDR-HDONN adopts ahybrid optical/photonic design, combining the reconfigurability of integratedphotonics with the ultra-parallelism of free-space diffractive systems.Extensive evaluations demonstrate that MDR-HDONN has digital-comparableaccuracy on various task adaptations with 74x faster speed and 194x lowerenergy. Compared to prior DONNs, MDR-HDONN shows exponentially largerfunctional space with 5x faster training speed, paving the way for a newparadigm of versatile, composable, hybrid optical/photonic AI computing. Wewill open-source our codes.</description><author>Ziang Yin, Yu Yao, Jeff Zhang, Jiaqi Gu</author><pubDate>Fri, 08 Nov 2024 18:08:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05748v1</guid></item><item><title>WavShadow: Wavelet Based Shadow Segmentation and Removal</title><link>http://arxiv.org/abs/2411.05747v1</link><description>Shadow removal and segmentation remain challenging tasks in computer vision,particularly in complex real-world scenarios. This study presents a novelapproach that enhances the ShadowFormer model by incorporating MaskedAutoencoder (MAE) priors and Fast Fourier Convolution (FFC) blocks, leading tosignificantly faster convergence and improved performance. We introduce keyinnovations: (1) integration of MAE priors trained on Places2 dataset forbetter context understanding, (2) adoption of Haar wavelet features forenhanced edge detection and multi-scale analysis, and (3) implementation of amodified SAM Adapter for robust shadow segmentation. Extensive experiments onthe challenging DESOBA dataset demonstrate that our approach achievesstate-of-the-art results, with notable improvements in both convergence speedand shadow removal quality.</description><author>Shreyans Jain, Aadya Arora, Viraj Vekaria, Karan Gandhi</author><pubDate>Fri, 08 Nov 2024 18:08:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05747v1</guid></item><item><title>Continuous-Time Analysis of Adaptive Optimization and Normalization</title><link>http://arxiv.org/abs/2411.05746v1</link><description>Adaptive optimization algorithms, particularly Adam and its variant AdamW,are fundamental components of modern deep learning. However, their trainingdynamics lack comprehensive theoretical understanding, with limited insightinto why common practices - such as specific hyperparameter choices andnormalization layers - contribute to successful generalization. This workpresents a continuous-time formulation of Adam and AdamW, facilitating atractable analysis of training dynamics that can shed light on such practicalquestions. We theoretically derive a stable region for Adam's hyperparameters$(\beta, \gamma)$ that ensures bounded updates, empirically verifying thesepredictions by observing unstable exponential growth of parameter updatesoutside this region. Furthermore, we theoretically justify the success ofnormalization layers by uncovering an implicit meta-adaptive effect ofscale-invariant architectural components. This insight leads to an explicitoptimizer, $2$-Adam, which we generalize to $k$-Adam - an optimizer thatapplies an adaptive normalization procedure $k$ times, encompassing Adam(corresponding to $k=1$) and Adam with a normalization layer (corresponding to$k=2$). Overall, our continuous-time formulation of Adam facilitates aprincipled analysis, offering deeper understanding of optimal hyperparameterchoices and architectural decisions in modern deep learning.</description><author>Rhys Gould, Hidenori Tanaka</author><pubDate>Fri, 08 Nov 2024 18:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05746v1</guid></item><item><title>Free Record-Level Privacy Risk Evaluation Through Artifact-Based Methods</title><link>http://arxiv.org/abs/2411.05743v1</link><description>Membership inference attacks (MIAs) are widely used to empirically assess theprivacy risks of samples used to train a target machine learning model.State-of-the-art methods however require training hundreds of shadow models,with the same size and architecture of the target model, solely to evaluate theprivacy risk. While one might be able to afford this for small models, the costoften becomes prohibitive for medium and large models. We here instead propose a novel approach to identify the at-risk samplesusing only artifacts available during training, with little to no additionalcomputational overhead. Our method analyzes individual per-sample loss tracesand uses them to identify the vulnerable data samples. We demonstrate theeffectiveness of our artifact-based approach through experiments on the CIFAR10dataset, showing high precision in identifying vulnerable samples as determinedby a SOTA shadow model-based MIA (LiRA). Impressively, our method reaches thesame precision as another SOTA MIA when measured against LiRA, despite it beingorders of magnitude cheaper. We then show LT-IQR to outperform alternative lossaggregation methods, perform ablation studies on hyperparameters, and validatethe robustness of our method to the target metric. Finally, we study theevolution of the vulnerability score distribution throughout training as ametric for model-level risk assessment.</description><author>Joseph Pollock, Igor Shilov, Euodia Dodd, Yves-Alexandre de Montjoye</author><pubDate>Fri, 08 Nov 2024 18:04:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05743v1</guid></item><item><title>Topology-aware Reinforcement Feature Space Reconstruction for Graph Data</title><link>http://arxiv.org/abs/2411.05742v1</link><description>Feature space is an environment where data points are vectorized to representthe original dataset. Reconstructing a good feature space is essential toaugment the AI power of data, improve model generalization, and increase theavailability of downstream ML models. Existing literature, such as featuretransformation and feature selection, is labor-intensive (e.g., heavy relianceon empirical experience) and mostly designed for tabular data. Moreover, thesemethods regard data samples as independent, which ignores the uniquetopological structure when applied to graph data, thus resulting in asuboptimal reconstruction feature space. Can we consider the topologicalinformation to automatically reconstruct feature space for graph data withoutheavy experiential knowledge? To fill this gap, we leverage topology-awarereinforcement learning to automate and optimize feature space reconstructionfor graph data. Our approach combines the extraction of core subgraphs tocapture essential structural information with a graph neural network (GNN) toencode topological features and reduce computing complexity. Then we introducethree reinforcement agents within a hierarchical structure to systematicallygenerate meaningful features through an iterative process, effectivelyreconstructing the feature space. This framework provides a principled solutionfor attributed graph feature space reconstruction. The extensive experimentsdemonstrate the effectiveness and efficiency of including topologicalawareness.</description><author>Wangyang Ying, Haoyue Bai, Kunpeng Liu, Yanjie Fu</author><pubDate>Fri, 08 Nov 2024 18:01:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05742v1</guid></item><item><title>Fairness-Aware Estimation of Graphical Models</title><link>http://arxiv.org/abs/2408.17396v2</link><description>This paper examines the issue of fairness in the estimation of graphicalmodels (GMs), particularly Gaussian, Covariance, and Ising models. These modelsplay a vital role in understanding complex relationships in high-dimensionaldata. However, standard GMs can result in biased outcomes, especially when theunderlying data involves sensitive characteristics or protected groups. Toaddress this, we introduce a comprehensive framework designed to reduce bias inthe estimation of GMs related to protected attributes. Our approach involvesthe integration of the pairwise graph disparity error and a tailored lossfunction into a nonsmooth multi-objective optimization problem, striving toachieve fairness across different sensitive groups while maintaining theeffectiveness of the GMs. Experimental evaluations on synthetic and real-worlddatasets demonstrate that our framework effectively mitigates bias withoutundermining GMs' performance.</description><author>Zhuoping Zhou, Davoud Ataee Tarzanagh, Bojian Hou, Qi Long, Li Shen</author><pubDate>Fri, 08 Nov 2024 18:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.17396v2</guid></item><item><title>StoX-Net: Stochastic Processing of Partial Sums for Efficient In-Memory Computing DNN Accelerators</title><link>http://arxiv.org/abs/2407.12378v2</link><description>Crossbar-based in-memory computing (IMC) has emerged as a promising platformfor hardware acceleration of deep neural networks (DNNs). However, the energyand latency of IMC systems are dominated by the large overhead of theperipheral analog-to-digital converters (ADCs). To address such ADC bottleneck,here we propose to implement stochastic processing of array-level partial sums(PS) for efficient IMC. Leveraging the probabilistic switching of spin-orbittorque magnetic tunnel junctions, the proposed PS processing eliminates thecostly ADC, achieving significant improvement in energy and area efficiency. Tomitigate accuracy loss, we develop PS-quantization-aware training that enablesbackward propagation across stochastic PS. Furthermore, a novel scheme with aninhomogeneous sampling length of the stochastic conversion is proposed. Whenrunning ResNet20 on the CIFAR-10 dataset, our architecture-to-algorithmco-design demonstrates up to 16x, 8x, and 10x improvement in energy, latency,and area, respectively, compared to IMC with standard ADC. Our optimized designconfiguration using stochastic PS achieved 130x (24x) improvement inEnergy-Delay-Product compared to IMC with full precision ADC (sparse low-bitADC), while maintaining near-software accuracy at various benchmarkclassification tasks.</description><author>Ethan G Rogers, Sohan Salahuddin Mugdho, Kshemal Kshemendra Gupte, Cheng Wang</author><pubDate>Fri, 08 Nov 2024 17:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12378v2</guid></item><item><title>StdGEN: Semantic-Decomposed 3D Character Generation from Single Images</title><link>http://arxiv.org/abs/2411.05738v1</link><description>We present StdGEN, an innovative pipeline for generating semanticallydecomposed high-quality 3D characters from single images, enabling broadapplications in virtual reality, gaming, and filmmaking, etc. Unlike previousmethods which struggle with limited decomposability, unsatisfactory quality,and long optimization times, StdGEN features decomposability, effectiveness andefficiency; i.e., it generates intricately detailed 3D characters withseparated semantic components such as the body, clothes, and hair, in threeminutes. At the core of StdGEN is our proposed Semantic-aware LargeReconstruction Model (S-LRM), a transformer-based generalizable model thatjointly reconstructs geometry, color and semantics from multi-view images in afeed-forward manner. A differentiable multi-layer semantic surface extractionscheme is introduced to acquire meshes from hybrid implicit fieldsreconstructed by our S-LRM. Additionally, a specialized efficient multi-viewdiffusion model and an iterative multi-layer surface refinement module areintegrated into the pipeline to facilitate high-quality, decomposable 3Dcharacter generation. Extensive experiments demonstrate our state-of-the-artperformance in 3D anime character generation, surpassing existing baselines bya significant margin in geometry, texture and decomposability. StdGEN offersready-to-use semantic-decomposed 3D characters and enables flexiblecustomization for a wide range of applications. Project page:https://stdgen.github.io</description><author>Yuze He, Yanning Zhou, Wang Zhao, Zhongkai Wu, Kaiwen Xiao, Wei Yang, Yong-Jin Liu, Xiao Han</author><pubDate>Fri, 08 Nov 2024 17:54:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05738v1</guid></item><item><title>Learning Delays Through Gradients and Structure: Emergence of Spatiotemporal Patterns in Spiking Neural Networks</title><link>http://arxiv.org/abs/2407.18917v2</link><description>We present a Spiking Neural Network (SNN) model that incorporates learnablesynaptic delays through two approaches: per-synapse delay learning via DilatedConvolutions with Learnable Spacings (DCLS) and a dynamic pruning strategy thatalso serves as a form of delay learning. In the latter approach, the networkdynamically selects and prunes connections, optimizing the delays in sparseconnectivity settings. We evaluate both approaches on the Raw Heidelberg Digitskeyword spotting benchmark using Backpropagation Through Time with surrogategradients. Our analysis of the spatio-temporal structure of synaptic interactionsreveals that, after training, excitation and inhibition group together in spaceand time. Notably, the dynamic pruning approach, which employs DEEP R forconnection removal and RigL for reconnection, not only preserves thesespatio-temporal patterns but outperforms per-synapse delay learning in sparsenetworks. Our results demonstrate the potential of combining delay learning withdynamic pruning to develop efficient SNN models for temporal data processing.Moreover, the preservation of spatio-temporal dynamics throughout pruning andrewiring highlights the robustness of these features, providing a solidfoundation for future neuromorphic computing applications.</description><author>Bal√°zs M√©sz√°ros, James Knight, Thomas Nowotny</author><pubDate>Fri, 08 Nov 2024 17:52:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18917v2</guid></item><item><title>log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling</title><link>http://arxiv.org/abs/2411.03320v2</link><description>Accurate prediction of chemical reaction yields is crucial for optimizingorganic synthesis, potentially reducing time and resources spent onexperimentation. With the rise of artificial intelligence (AI), there isgrowing interest in leveraging AI-based methods to accelerate yield predictionswithout conducting in vitro experiments. We present log-RRIM, an innovativegraph transformer-based framework designed for predicting chemical reactionyields. Our approach implements a unique local-to-global reactionrepresentation learning strategy. This approach initially captures detailedmolecule-level information and then models and aggregates intermolecularinteractions, ensuring that the impact of varying-sizes molecular fragments onyield is accurately accounted for. Another key feature of log-RRIM is itsintegration of a cross-attention mechanism that focuses on the interplaybetween reagents and reaction centers. This design reflects a fundamentalprinciple in chemical reactions: the crucial role of reagents in influencingbond-breaking and formation processes, which ultimately affect reaction yields.log-RRIM outperforms existing methods in our experiments, especially for mediumto high-yielding reactions, proving its reliability as a predictor. Itsadvanced modeling of reactant-reagent interactions and sensitivity to smallmolecular fragments make it a valuable tool for reaction planning andoptimization in chemical synthesis. The data and codes of log-RRIM areaccessible through https://github.com/ninglab/Yield_log_RRIM.</description><author>Xiao Hu, Ziqi Chen, Bo Peng, Daniel Adu-Ampratwum, Xia Ning</author><pubDate>Fri, 08 Nov 2024 17:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03320v2</guid></item><item><title>Aioli: A Unified Optimization Framework for Language Model Data Mixing</title><link>http://arxiv.org/abs/2411.05735v1</link><description>Language model performance depends on identifying the optimal mixture of datagroups to train on (e.g., law, code, math). Prior work has proposed a diverseset of methods to efficiently learn mixture proportions, ranging from fittingregression models over training runs to dynamically updating proportionsthroughout training. Surprisingly, we find that no existing method consistentlyoutperforms a simple stratified sampling baseline in terms of average testperplexity per group. In this paper, we study the cause of this inconsistencyby unifying existing methods into a standard optimization framework. We showthat all methods set proportions to minimize total loss, subject to amethod-specific mixing law -- an assumption on how loss is a function ofmixture proportions. We find that existing parameterizations of mixing laws canexpress the true loss-proportion relationship empirically, but the methodsthemselves often set the mixing law parameters inaccurately, resulting in poorand inconsistent performance. Finally, we leverage the insights from ourframework to derive a new online method named Aioli, which directly estimatesthe mixing law parameters throughout training and uses them to dynamicallyadjust proportions. Empirically, Aioli outperforms stratified sampling on 6 outof 6 datasets by an average of 0.28 test perplexity points, whereas existingmethods fail to consistently beat stratified sampling, doing up to 6.9 pointsworse. Moreover, in a practical setting where proportions are learned onshorter runs due to computational constraints, Aioli can dynamically adjustthese proportions over the full training run, consistently improvingperformance over existing methods by up to 12.01 test perplexity points.</description><author>Mayee F. Chen, Michael Y. Hu, Nicholas Lourie, Kyunghyun Cho, Christopher R√©</author><pubDate>Fri, 08 Nov 2024 17:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05735v1</guid></item><item><title>xAI-Drop: Don't Use What You Cannot Explain</title><link>http://arxiv.org/abs/2407.20067v2</link><description>Graph Neural Networks (GNNs) have emerged as the predominant paradigm forlearning from graph-structured data, offering a wide range of applications fromsocial network analysis to bioinformatics. Despite their versatility, GNNs facechallenges such as lack of generalization and poor interpretability, whichhinder their wider adoption and reliability in critical applications. Droppinghas emerged as an effective paradigm for improving the generalizationcapabilities of GNNs. However, existing approaches often rely on random orheuristic-based selection criteria, lacking a principled method to identify andexclude nodes that contribute to noise and over-complexity in the model. Inthis work, we argue that explainability should be a key indicator of a model'squality throughout its training phase. To this end, we introduce xAI-Drop, anovel topological-level dropping regularizer that leverages explainability topinpoint noisy network elements to be excluded from the GNN propagationmechanism. An empirical evaluation on diverse real-world datasets demonstratesthat our method outperforms current state-of-the-art dropping approaches inaccuracy, and improves explanation quality.</description><author>Vincenzo Marco De Luca, Antonio Longa, Andrea Passerini, Pietro Li√≤</author><pubDate>Fri, 08 Nov 2024 17:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20067v2</guid></item><item><title>Poze: Sports Technique Feedback under Data Constraints</title><link>http://arxiv.org/abs/2411.05734v1</link><description>Access to expert coaching is essential for developing technique in sports,yet economic barriers often place it out of reach for many enthusiasts. Tobridge this gap, we introduce Poze, an innovative video processing frameworkthat provides feedback on human motion, emulating the insights of aprofessional coach. Poze combines pose estimation with sequence comparison andis optimized to function effectively with minimal data. Poze surpassesstate-of-the-art vision-language models in video question-answering frameworks,achieving 70% and 196% increase in accuracy over GPT4V and LLaVAv1.6 7b,respectively.</description><author>Agamdeep Singh, Sujit PB, Mayank Vatsa</author><pubDate>Fri, 08 Nov 2024 17:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05734v1</guid></item><item><title>Differential Privacy Under Class Imbalance: Methods and Empirical Insights</title><link>http://arxiv.org/abs/2411.05733v1</link><description>Imbalanced learning occurs in classification settings where the distributionof class-labels is highly skewed in the training data, such as when predictingrare diseases or in fraud detection. This class imbalance presents asignificant algorithmic challenge, which can be further exacerbated whenprivacy-preserving techniques such as differential privacy are applied toprotect sensitive training data. Our work formalizes these challenges andprovides a number of algorithmic solutions. We consider DP variants ofpre-processing methods that privately augment the original dataset to reducethe class imbalance; these include oversampling, SMOTE, and private syntheticdata generation. We also consider DP variants of in-processing techniques,which adjust the learning algorithm to account for the imbalance; these includemodel bagging, class-weighted empirical risk minimization and class-weighteddeep learning. For each method, we either adapt an existing imbalanced learningtechnique to the private setting or demonstrate its incompatibility withdifferential privacy. Finally, we empirically evaluate these privacy-preservingimbalanced learning methods under various data and distributional settings. Wefind that private synthetic data methods perform well as a data pre-processingstep, while class-weighted ERMs are an alternative in higher-dimensionalsettings where private synthetic data suffers from the curse of dimensionality.</description><author>Lucas Rosenblatt, Yuliia Lut, Eitan Turok, Marco Avella-Medina, Rachel Cummings</author><pubDate>Fri, 08 Nov 2024 17:46:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05733v1</guid></item><item><title>PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for View-Adaptive Rendering</title><link>http://arxiv.org/abs/2411.05731v1</link><description>Recent advances in structured 3D Gaussians for view-adaptive rendering,particularly through methods like Scaffold-GS, have demonstrated promisingresults in neural scene representation. However, existing approaches still facechallenges in perceptual consistency and precise view-dependent effects. Wepresent PEP-GS, a novel framework that enhances structured 3D Gaussians throughthree key innovations: (1) a Local-Enhanced Multi-head Self-Attention (LEMSA)mechanism that replaces spherical harmonics for more accurate view-dependentcolor decoding, and (2) Kolmogorov-Arnold Networks (KAN) that optimize Gaussianopacity and covariance functions for enhanced interpretability and splattingprecision. (3) a Neural Laplacian Pyramid Decomposition (NLPD) that improvesperceptual similarity across views. Our comprehensive evaluation acrossmultiple datasets indicates that, compared to the current state-of-the-artmethods, these improvements are particularly evident in challenging scenariossuch as view-dependent effects, specular reflections, fine-scale details andfalse geometry generation.</description><author>Junxi Jin, Xiulai Li, Haiping Huang, Lianjun Liu, Yujie Sun</author><pubDate>Fri, 08 Nov 2024 17:42:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05731v1</guid></item><item><title>Learning Subsystem Dynamics in Nonlinear Systems via Port-Hamiltonian Neural Networks</title><link>http://arxiv.org/abs/2411.05730v1</link><description>Port-Hamiltonian neural networks (pHNNs) are emerging as a powerful modelingtool that integrates physical laws with deep learning techniques. While mostresearch has focused on modeling the entire dynamics of interconnected systems,the potential for identifying and modeling individual subsystems whileoperating as part of a larger system has been overlooked. This study addressesthis gap by introducing a novel method for using pHNNs to identify suchsubsystems based solely on input-output measurements. By utilizing the inherentcompositional property of the port-Hamiltonian systems, we developed analgorithm that learns the dynamics of individual subsystems, without requiringdirect access to their internal states. On top of that, by choosing an outputerror (OE) model structure, we have been able to handle measurement noiseeffectively. The effectiveness of the proposed approach is demonstrated throughtests on interconnected systems, including multi-physics scenarios,demonstrating its potential for identifying subsystem dynamics and facilitatingtheir integration into new interconnected models.</description><author>G. J. E. van Otterdijk, S. Moradi, S. Weiland, R. T√≥th, N. O. Jaensson, M. Schoukens</author><pubDate>Fri, 08 Nov 2024 17:41:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05730v1</guid></item><item><title>Graph-Dictionary Signal Model for Sparse Representations of Multivariate Data</title><link>http://arxiv.org/abs/2411.05729v1</link><description>Representing and exploiting multivariate signals require capturing complexrelations between variables. We define a novel Graph-Dictionary signal model,where a finite set of graphs characterizes relationships in data distributionthrough a weighted sum of their Laplacians. We propose a framework to infer thegraph dictionary representation from observed data, along with a bilineargeneralization of the primal-dual splitting algorithm to solve the learningproblem. Our new formulation allows to include a priori knowledge on signalproperties, as well as on underlying graphs and their coefficients. We show thecapability of our method to reconstruct graphs from signals in multiplesynthetic settings, where our model outperforms previous baselines. Then, weexploit graph-dictionary representations in a motor imagery decoding task onbrain activity data, where we classify imagined motion better than standardmethods relying on many more features.</description><author>William Cappelletti, Pascal Frossard</author><pubDate>Fri, 08 Nov 2024 17:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05729v1</guid></item><item><title>Counterfactual Fairness by Combining Factual and Counterfactual Predictions</title><link>http://arxiv.org/abs/2409.01977v2</link><description>In high-stake domains such as healthcare and hiring, the role of machinelearning (ML) in decision-making raises significant fairness concerns. Thiswork focuses on Counterfactual Fairness (CF), which posits that an ML model'soutcome on any individual should remain unchanged if they had belonged to adifferent demographic group. Previous works have proposed methods thatguarantee CF. Notwithstanding, their effects on the model's predictiveperformance remains largely unclear. To fill in this gap, we provide atheoretical study on the inherent trade-off between CF and predictiveperformance in a model-agnostic manner. We first propose a simple but effectivemethod to cast an optimal but potentially unfair predictor into a fair onewithout losing the optimality. By analyzing its excess risk in order to achieveCF, we quantify this inherent trade-off. Further analysis on our method'sperformance with access to only incomplete causal knowledge is also conducted.Built upon it, we propose a performant algorithm that can be applied in suchscenarios. Experiments on both synthetic and semi-synthetic datasetsdemonstrate the validity of our analysis and methods.</description><author>Zeyu Zhou, Tianci Liu, Ruqi Bai, Jing Gao, Murat Kocaoglu, David I. Inouye</author><pubDate>Fri, 08 Nov 2024 17:40:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.01977v2</guid></item><item><title>Learning Human-like Representations to Enable Learning Human Values</title><link>http://arxiv.org/abs/2312.14106v3</link><description>How can we build AI systems that can learn any set of individual human valuesboth quickly and safely, avoiding causing harm or violating societal standardsfor acceptable behavior during the learning process? We explore the effects ofrepresentational alignment between humans and AI agents on learning humanvalues. Making AI systems learn human-like representations of the world hasmany known benefits, including improving generalization, robustness to domainshifts, and few-shot learning performance. We demonstrate that this kind ofrepresentational alignment can also support safely learning and exploring humanvalues in the context of personalization. We begin with a theoreticalprediction, show that it applies to learning human morality judgments, thenshow that our results generalize to ten different aspects of human values --including ethics, honesty, and fairness -- training AI agents on each set ofvalues in a multi-armed bandit setting, where rewards reflect human valuejudgments over the chosen action. Using a set of textual action descriptions,we collect value judgments from humans, as well as similarity judgments fromboth humans and multiple language models, and demonstrate that representationalalignment enables both safe exploration and improved generalization whenlearning human values.</description><author>Andrea Wynn, Ilia Sucholutsky, Thomas L. Griffiths</author><pubDate>Fri, 08 Nov 2024 17:33:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14106v3</guid></item><item><title>HPE-CogVLM: Advancing Vision Language Models with a Head Pose Grounding Task</title><link>http://arxiv.org/abs/2406.01914v2</link><description>Head pose estimation (HPE) requires a sophisticated understanding of 3Dspatial relationships to generate precise yaw, pitch, and roll angles. PreviousHPE models, primarily CNN-based, rely on cropped close-up human head images asinputs and often lack robustness in real-world scenario. Vision Language Models(VLMs) can analyze entire images while focusing on specific objects throughtheir attention mechanisms. In this paper, we propose a novel framework toimprove the HPE accuracy by leveraging the object detection groundingcapability of a VLM, referred to as CogVLM. We empirically find that directlyLoRA fine-tuning of this VLM for the HPE task fails to achieve desirable HPEaccuracy, while some model merging methods can improve accuracy but frequentlyproduce blended invalid response formats, struggling to handle both objectdetection and HPE tasks simultaneously. To integrate HPE capability into CogVLMeffectively, we develop a novel LoRA layer-based model merging method. Thismerging approach applies a high cosine similarity threshold and awinner-takes-all layer selection strategy, aligning attention to the HPE taskwhile preserving original object detection knowledge. It successfully resolvesissues with blended invalid response formats and improves accuracy. Resultsshow that our HPE-CogVLM achieves a 31.5\% reduction in Mean Absolute Errorover the current state-of-the-art CNN model, 6DRepNet, in cross-datasetevaluation. Furthermore, HPE-CogVLM outperforms both directly LoRA fine-tunedand task arithmetic-based merged VLMs across all HPE metrics.</description><author>Yu Tian, Tianqi Shao, Tsukasa Demizu, Xuyang Wu, Hsin-Tai Wu</author><pubDate>Fri, 08 Nov 2024 17:33:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01914v2</guid></item><item><title>TropNNC: Structured Neural Network Compression Using Tropical Geometry</title><link>http://arxiv.org/abs/2409.03945v2</link><description>We present TropNNC, a framework for compressing neural networks with linearand convolutional layers and ReLU activations. TropNNC is a structuredcompression framework based on a geometrical approach to machine/deep learning,using tropical geometry and extending the work of Misiakos et al. (2022). Weuse the Hausdorff distance of zonotopes in its standard continuous form toachieve a tighter approximation bound for tropical polynomials compared toprevious work. This enhancement leads to the development of an effectivecompression algorithm that achieves superior functional approximations ofneural networks. Our method is significantly easier to implement compared toother frameworks, and does not depend on the availability of training datasamples. We validate our framework through extensive empirical evaluations onthe MNIST, CIFAR, and ImageNet datasets. Our results demonstrate that TropNNCachieves performance on par with state-of-the-art methods like ThiNet (evensurpassing it in compressing linear layers) and CUP. To the best of ourknowledge, it is the first method that achieves this using tropical geometry.</description><author>Konstantinos Fotopoulos, Petros Maragos, Panagiotis Misiakos</author><pubDate>Fri, 08 Nov 2024 17:29:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03945v2</guid></item><item><title>Lung tumor segmentation in MRI mice scans using 3D nnU-Net with minimum annotations</title><link>http://arxiv.org/abs/2411.00922v2</link><description>In drug discovery, accurate lung tumor segmentation is an important step forassessing tumor size and its progression using \textit{in-vivo} imaging such asMRI. While deep learning models have been developed to automate this process,the focus has predominantly been on human subjects, neglecting the pivotal roleof animal models in pre-clinical drug development. In this work, we focus onoptimizing lung tumor segmentation in mice. First, we demonstrate that thennU-Net model outperforms the U-Net, U-Net3+, and DeepMeta models. Mostimportantly, we achieve better results with nnU-Net 3D models than 2D models,indicating the importance of spatial context for segmentation tasks in MRI micescans. This study demonstrates the importance of 3D input over 2D input imagesfor lung tumor segmentation in MRI scans. Finally, we outperform the priorstate-of-the-art approach that involves the combined segmentation of lungs andtumors within the lungs. Our work achieves comparable results using only lungtumor annotations requiring fewer annotations, saving time and annotationefforts. This work(https://anonymous.4open.science/r/lung-tumour-mice-mri-64BB) is an importantstep in automating pre-clinical animal studies to quantify the efficacy ofexperimental drugs, particularly in assessing tumor changes.</description><author>Piotr Kaniewski, Fariba Yousefi, Yeman Brhane Hagos, Talha Qaiser, Nikolay Burlutskiy</author><pubDate>Fri, 08 Nov 2024 17:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00922v2</guid></item><item><title>A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics</title><link>http://arxiv.org/abs/2411.05718v1</link><description>Machine learning methods have a groundbreaking impact in many applicationdomains, but their application on real robotic platforms is still limited.Despite the many challenges associated with combining machine learningtechnology with robotics, robot learning remains one of the most promisingdirections for enhancing the capabilities of robots. When deployinglearning-based approaches on real robots, extra effort is required to addressthe challenges posed by various real-world factors. To investigate the keyfactors influencing real-world deployment and to encourage original solutionsfrom different researchers, we organized the Robot Air Hockey Challenge at theNeurIPS 2023 conference. We selected the air hockey task as a benchmark,encompassing low-level robotics problems and high-level tactics. Different fromother machine learning-centric benchmarks, participants need to tacklepractical challenges in robotics, such as the sim-to-real gap, low-levelcontrol issues, safety problems, real-time requirements, and the limitedavailability of real-world data. Furthermore, we focus on a dynamicenvironment, removing the typical assumption of quasi-static motions of otherreal-world benchmarks. The competition's results show that solutions combininglearning-based approaches with prior knowledge outperform those relying solelyon data when real-world deployment is challenging. Our ablation study revealswhich real-world factors may be overlooked when building a learning-basedsolution. The successful real-world air hockey deployment of best-performingagents sets the foundation for future competitions and follow-up researchdirections.</description><author>Puze Liu, Jonas G√ºnster, Niklas Funk, Simon Gr√∂ger, Dong Chen, Haitham Bou-Ammar, Julius Jankowski, Ante Mariƒá, Sylvain Calinon, Andrej Orsula, Miguel Olivares-Mendez, Hongyi Zhou, Rudolf Lioutikov, Gerhard Neumann, Amarildo Likmeta Amirhossein Zhalehmehrabi, Thomas Bonenfant, Marcello Restelli, Davide Tateo, Ziyuan Liu, Jan Peters</author><pubDate>Fri, 08 Nov 2024 17:20:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05718v1</guid></item><item><title>Towards Scalable and Stable Parallelization of Nonlinear RNNs</title><link>http://arxiv.org/abs/2407.19115v2</link><description>Conventional nonlinear RNNs are not naturally parallelizable across thesequence length, unlike transformers and linear RNNs. Lim et. al. (2024)therefore tackle parallelized evaluation of nonlinear RNNs, posing it as afixed point problem solved with Newton's method. By deriving and applying aparallelized form of Newton's method, they achieve large speedups oversequential evaluation. However, their approach inherits cubic computationalcomplexity and numerical instability. We tackle these weaknesses. To reduce thecomputational complexity, we apply quasi-Newton approximations and show theyconverge comparably, use less memory, and are faster, compared to full-Newton.To stabilize Newton's method, we leverage a connection between Newton's methoddamped with trust regions and Kalman smoothing. This connection allows us tostabilize the iteration, per the trust region, and use efficient parallelizedKalman algorithms to retain performance. We compare these methods empiricallyand highlight use cases where each algorithm excels.</description><author>Xavier Gonzalez, Andrew Warrington, Jimmy T. H. Smith, Scott W. Linderman</author><pubDate>Fri, 08 Nov 2024 17:20:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19115v2</guid></item><item><title>Multimodal Structure-Aware Quantum Data Processing</title><link>http://arxiv.org/abs/2411.04242v2</link><description>While large language models (LLMs) have advanced the field of naturallanguage processing (NLP), their "black box" nature obscures theirdecision-making processes. To address this, researchers developed structuredapproaches using higher order tensors. These are able to model linguisticrelations, but stall when training on classical computers due to theirexcessive size. Tensors are natural inhabitants of quantum systems and trainingon quantum computers provides a solution by translating text to variationalquantum circuits. In this paper, we develop MultiQ-NLP: a framework forstructure-aware data processing with multimodal text+image data. Here,"structure" refers to syntactic and grammatical relationships in language, aswell as the hierarchical organization of visual elements in images. We enrichthe translation with new types and type homomorphisms and develop novelarchitectures to represent structure. When tested on a main stream imageclassification task (SVO Probes), our best model showed a par performance withthe state of the art classical models; moreover the best model was fullystructured.</description><author>Hala Hawashin, Mehrnoosh Sadrzadeh</author><pubDate>Fri, 08 Nov 2024 17:16:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04242v2</guid></item><item><title>On the Role of Noise in AudioVisual Integration: Evidence from Artificial Neural Networks that Exhibit the McGurk Effect</title><link>http://arxiv.org/abs/2411.05715v1</link><description>Humans are able to fuse information from both auditory and visual modalitiesto help with understanding speech. This is frequently demonstrated through anphenomenon known as the McGurk Effect, during which a listener is presentedwith incongruent auditory and visual speech that fuse together into the perceptof an illusory intermediate phoneme. Building on a recent framework thatproposes how to address developmental 'why' questions using artificial neuralnetworks, we evaluated a set of recent artificial neural networks trained onaudiovisual speech by testing them with audiovisually incongruent wordsdesigned to elicit the McGurk effect. We compared networks trained on cleanspeech to those trained on noisy speech, and discovered that training withnoisy speech led to an increase in both visual responses and McGurk responsesacross all models. Furthermore, we observed that systematically increasing thelevel of auditory noise during ANN training also increased the amount ofaudiovisual integration up to a point, but at extreme noise levels, thisintegration failed to develop. These results suggest that excessive noiseexposure during critical periods of audiovisual learning may negativelyinfluence the development of audiovisual speech integration. This work alsodemonstrates that the McGurk effect reliably emerges untrained from thebehaviour of both supervised and unsupervised networks. This supports thenotion that artificial neural networks might be useful models for certainaspects of perception and cognition.</description><author>Lukas Grasse, Matthew S. Tata</author><pubDate>Fri, 08 Nov 2024 17:16:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05715v1</guid></item><item><title>STARS: Sensor-agnostic Transformer Architecture for Remote Sensing</title><link>http://arxiv.org/abs/2411.05714v1</link><description>We present a sensor-agnostic spectral transformer as the basis for spectralfoundation models. To that end, we introduce a Universal SpectralRepresentation (USR) that leverages sensor meta-data, such as sensing kernelspecifications and sensing wavelengths, to encode spectra obtained from anyspectral instrument into a common representation, such that a single model caningest data from any sensor. Furthermore, we develop a methodology forpre-training such models in a self-supervised manner using a novel randomsensor-augmentation and reconstruction pipeline to learn spectral featuresindependent of the sensing paradigm. We demonstrate that our architecture canlearn sensor independent spectral features that generalize effectively tosensors not seen during training. This work sets the stage for trainingfoundation models that can both leverage and be effective for the growingdiversity of spectral data.</description><author>Ethan King, Jaime Rodriguez, Diego Llanes, Timothy Doster, Tegan Emerson, James Koch</author><pubDate>Fri, 08 Nov 2024 17:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05714v1</guid></item><item><title>Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream</title><link>http://arxiv.org/abs/2411.05712v1</link><description>When trained on large-scale object classification datasets, certainartificial neural network models begin to approximate core object recognition(COR) behaviors and neural response patterns in the primate visual ventralstream (VVS). While recent machine learning advances suggest that scaling modelsize, dataset size, and compute resources improve task performance, the impactof scaling on brain alignment remains unclear. In this study, we explorescaling laws for modeling the primate VVS by systematically evaluating over 600models trained under controlled conditions on benchmarks spanning V1, V2, V4,IT and COR behaviors. We observe that while behavioral alignment continues toscale with larger models, neural alignment saturates. This observation remainstrue across model architectures and training datasets, even though models withstronger inductive bias and datasets with higher-quality images are morecompute-efficient. Increased scaling is especially beneficial for higher-levelvisual areas, where small models trained on few samples exhibit only pooralignment. Finally, we develop a scaling recipe, indicating that a greaterproportion of compute should be allocated to data samples over model size. Ourresults suggest that while scaling alone might suffice for alignment with humancore object recognition behavior, it will not yield improved models of thebrain's visual ventral stream with current architectures and datasets,highlighting the need for novel strategies in building brain-like models.</description><author>Abdulkadir Gokce, Martin Schrimpf</author><pubDate>Fri, 08 Nov 2024 17:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05712v1</guid></item><item><title>Sample and Computationally Efficient Robust Learning of Gaussian Single-Index Models</title><link>http://arxiv.org/abs/2411.05708v1</link><description>A single-index model (SIM) is a function of the form$\sigma(\mathbf{w}^{\ast} \cdot \mathbf{x})$, where $\sigma: \mathbb{R} \to\mathbb{R}$ is a known link function and $\mathbf{w}^{\ast}$ is a hidden unitvector. We study the task of learning SIMs in the agnostic (a.k.a. adversariallabel noise) model with respect to the $L^2_2$-loss under the Gaussiandistribution. Our main result is a sample and computationally efficientagnostic proper learner that attains $L^2_2$-error of$O(\mathrm{OPT})+\epsilon$, where $\mathrm{OPT}$ is the optimal loss. Thesample complexity of our algorithm is $\tilde{O}(d^{\lceilk^{\ast}/2\rceil}+d/\epsilon)$, where $k^{\ast}$ is the information-exponent of$\sigma$ corresponding to the degree of its first non-zero Hermite coefficient.This sample bound nearly matches known CSQ lower bounds, even in the realizablesetting. Prior algorithmic work in this setting had focused on learning in therealizable case or in the presence of semi-random noise. Prior computationallyefficient robust learners required significantly stronger assumptions on thelink function.</description><author>Puqian Wang, Nikos Zarifis, Ilias Diakonikolas, Jelena Diakonikolas</author><pubDate>Fri, 08 Nov 2024 17:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05708v1</guid></item><item><title>From CNN to ConvRNN: Adapting Visualization Techniques for Time-Series Anomaly Detection</title><link>http://arxiv.org/abs/2411.04707v2</link><description>Nowadays, neural networks are commonly used to solve various problems.Unfortunately, despite their effectiveness, they are often perceived as blackboxes capable of providing answers without explaining their decisions, whichraises numerous ethical and legal concerns. Fortunately, the field ofexplainability helps users understand these results. This aspect of machinelearning allows users to grasp the decision-making process of a model andverify the relevance of its outcomes. In this article, we focus on the learningprocess carried out by a ``time distributed`` convRNN, which performs anomalydetection from video data.</description><author>Fabien Poirier</author><pubDate>Fri, 08 Nov 2024 17:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04707v2</guid></item><item><title>Inferring stochastic low-rank recurrent neural networks from neural data</title><link>http://arxiv.org/abs/2406.16749v3</link><description>A central aim in computational neuroscience is to relate the activity oflarge populations of neurons to an underlying dynamical system. Models of theseneural dynamics should ideally be both interpretable and fit the observed datawell. Low-rank recurrent neural networks (RNNs) exhibit such interpretabilityby having tractable dynamics. However, it is unclear how to best fit low-rankRNNs to data consisting of noisy observations of an underlying stochasticsystem. Here, we propose to fit stochastic low-rank RNNs with variationalsequential Monte Carlo methods. We validate our method on several datasetsconsisting of both continuous and spiking neural data, where we obtain lowerdimensional latent dynamics than current state of the art methods.Additionally, for low-rank models with piecewise linear nonlinearities, we showhow to efficiently identify all fixed points in polynomial rather thanexponential cost in the number of units, making analysis of the inferreddynamics tractable for large RNNs. Our method both elucidates the dynamicalsystems underlying experimental recordings and provides a generative modelwhose trajectories match observed variability.</description><author>Matthijs Pals, A Erdem Saƒütekin, Felix Pei, Manuel Gloeckler, Jakob H Macke</author><pubDate>Fri, 08 Nov 2024 17:07:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16749v3</guid></item><item><title>Image2Text2Image: A Novel Framework for Label-Free Evaluation of Image-to-Text Generation with Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2411.05706v1</link><description>Evaluating the quality of automatically generated image descriptions is acomplex task that requires metrics capturing various dimensions, such asgrammaticality, coverage, accuracy, and truthfulness. Although human evaluationprovides valuable insights, its cost and time-consuming nature poselimitations. Existing automated metrics like BLEU, ROUGE, METEOR, and CIDErattempt to fill this gap, but they often exhibit weak correlations with humanjudgment. To address this challenge, we propose a novel evaluation frameworkcalled Image2Text2Image, which leverages diffusion models, such as StableDiffusion or DALL-E, for text-to-image generation. In the Image2Text2Imageframework, an input image is first processed by a selected image captioningmodel, chosen for evaluation, to generate a textual description. Using thisgenerated description, a diffusion model then creates a new image. By comparingfeatures extracted from the original and generated images, we measure theirsimilarity using a designated similarity metric. A high similarity scoresuggests that the model has produced a faithful textual description, while alow score highlights discrepancies, revealing potential weaknesses in themodel's performance. Notably, our framework does not rely on human-annotatedreference captions, making it a valuable tool for assessing image captioningmodels. Extensive experiments and human evaluations validate the efficacy ofour proposed Image2Text2Image evaluation framework. The code and dataset willbe published to support further research in the community.</description><author>Jia-Hong Huang, Hongyi Zhu, Yixian Shen, Stevan Rudinac, Evangelos Kanoulas</author><pubDate>Fri, 08 Nov 2024 17:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05706v1</guid></item><item><title>Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift</title><link>http://arxiv.org/abs/2302.10160v3</link><description>We develop and analyze a principled approach to kernel ridge regression undercovariate shift. The goal is to learn a regression function with small meansquared error over a target distribution, based on unlabeled data from thereand labeled data that may have a different feature distribution. We propose tosplit the labeled data into two subsets, and conduct kernel ridge regression onthem separately to obtain a collection of candidate models and an imputationmodel. We use the latter to fill the missing labels and then select the bestcandidate accordingly. Our non-asymptotic excess risk bounds demonstrate thatour estimator adapts effectively to both the structure of the targetdistribution and the covariate shift. This adaptation is quantified through anotion of effective sample size that reflects the value of labeled source datafor the target regression task. Our estimator achieves the minimax optimalerror rate up to a polylogarithmic factor, and we find that using pseudo-labelsfor model selection does not significantly hinder performance.</description><author>Kaizheng Wang</author><pubDate>Fri, 08 Nov 2024 17:05:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.10160v3</guid></item><item><title>Triple Component Matrix Factorization: Untangling Global, Local, and Noisy Components</title><link>http://arxiv.org/abs/2404.07955v2</link><description>In this work, we study the problem of common and unique feature extractionfrom noisy data. When we have N observation matrices from N different andassociated sources corrupted by sparse and potentially gross noise, can werecover the common and unique components from these noisy observations? This isa challenging task as the number of parameters to estimate is approximatelythrice the number of observations. Despite the difficulty, we propose anintuitive alternating minimization algorithm called triple component matrixfactorization (TCMF) to recover the three components exactly. TCMF isdistinguished from existing works in literature thanks to two salient features.First, TCMF is a principled method to separate the three components given noisyobservations provably. Second, the bulk of the computation in TCMF can bedistributed. On the technical side, we formulate the problem as a constrainednonconvex nonsmooth optimization problem. Despite the intricate nature of theproblem, we provide a Taylor series characterization of its solution by solvingthe corresponding Karush-Kuhn-Tucker conditions. Using this characterization,we can show that the alternating minimization algorithm makes significantprogress at each iteration and converges into the ground truth at a linearrate. Numerical experiments in video segmentation and anomaly detectionhighlight the superior feature extraction abilities of TCMF.</description><author>Naichen Shi, Salar Fattahi, Raed Al Kontar</author><pubDate>Fri, 08 Nov 2024 17:04:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07955v2</guid></item><item><title>Image inpainting enhancement by replacing the original mask with a self-attended region from the input image</title><link>http://arxiv.org/abs/2411.05705v1</link><description>Image inpainting, the process of restoring missing or corrupted regions of animage by reconstructing pixel information, has recently seen considerableadvancements through deep learning-based approaches. In this paper, weintroduce a novel deep learning-based pre-processing methodology for imageinpainting utilizing the Vision Transformer (ViT). Our approach involvesreplacing masked pixel values with those generated by the ViT, leveragingdiverse visual patches within the attention matrix to capture discriminativespatial features. To the best of our knowledge, this is the first instance ofsuch a pre-processing model being proposed for image inpainting tasks.Furthermore, we show that our methodology can be effectively applied using thepre-trained ViT model with pre-defined patch size. To evaluate thegeneralization capability of the proposed methodology, we provide experimentalresults comparing our approach with four standard models across four publicdatasets, demonstrating the efficacy of our pre-processing technique inenhancing inpainting performance.</description><author>Kourosh Kiani, Razieh Rastgoo, Alireza Chaji, Sergio Escalera</author><pubDate>Fri, 08 Nov 2024 17:04:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05705v1</guid></item><item><title>Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset</title><link>http://arxiv.org/abs/2403.17632v3</link><description>The escalating challenges of traffic congestion and environmental degradationunderscore the critical importance of embracing E-Mobility solutions in urbanspaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,play a pivotal role in this transition, offering sustainable alternatives forurban commuters. However, the energy consumption patterns for these tools are acritical aspect that impacts their effectiveness in real-world scenarios and isessential for trip planning and boosting user confidence in using these. Tothis effect, recent studies have utilised physical models customised forspecific mobility tools and conditions, but these models struggle withgeneralization and effectiveness in real-world scenarios due to a notableabsence of open datasets for thorough model evaluation and verification. Tofill this gap, our work presents an open dataset, collected in Dublin, Ireland,specifically designed for energy modelling research related to E-Scooters andE-Bikes. Furthermore, we provide a comprehensive analysis of energy consumptionmodelling based on the dataset using a set of representative machine learningalgorithms and compare their performance against the contemporary mathematicalmodels as a baseline. Our results demonstrate a notable advantage fordata-driven models in comparison to the corresponding mathematical models forestimating energy consumption. Specifically, data-driven models outperformphysical models in accuracy by up to 83.83% for E-Bikes and 82.16% forE-Scooters based on an in-depth analysis of the dataset under certainassumptions.</description><author>Yue Ding, Sen Yan, Maqsood Hussain Shah, Hongyuan Fang, Ji Li, Mingming Liu</author><pubDate>Fri, 08 Nov 2024 17:01:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17632v3</guid></item><item><title>Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification</title><link>http://arxiv.org/abs/2411.05698v1</link><description>Convolutional Neural Networks (CNNs) have seen significant performanceimprovements in recent years. However, due to their size and complexity, theyfunction as black-boxes, leading to transparency concerns. State-of-the-artsaliency methods generate local explanations that highlight the area in theinput image where a class is identified but cannot explain how a concept ofinterest contributes to the prediction, which is essential for bias mitigation.On the other hand, concept-based methods, such as TCAV (Testing with ConceptActivation Vectors), provide insights into how sensitive is the network to aconcept, but cannot compute its attribution in a specific prediction nor showits location within the input image. This paper introduces a novel post-hocexplainability framework, Visual-TCAV, which aims to bridge the gap betweenthese methods by providing both local and global explanations for CNN-basedimage classification. Visual-TCAV uses Concept Activation Vectors (CAVs) togenerate saliency maps that show where concepts are recognized by the network.Moreover, it can estimate the attribution of these concepts to the output ofany class using a generalization of Integrated Gradients. This framework isevaluated on popular CNN architectures, with its validity further confirmed viaexperiments where ground truth for explanations is known, and a comparison withTCAV. Our code will be made available soon.</description><author>Antonio De Santis, Riccardo Campi, Matteo Bianchi, Marco Brambilla</author><pubDate>Fri, 08 Nov 2024 16:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05698v1</guid></item><item><title>IPMN Risk Assessment under Federated Learning Paradigm</title><link>http://arxiv.org/abs/2411.05697v1</link><description>Accurate classification of Intraductal Papillary Mucinous Neoplasms (IPMN) isessential for identifying high-risk cases that require timely intervention. Inthis study, we develop a federated learning framework for multi-center IPMNclassification utilizing a comprehensive pancreas MRI dataset. This datasetincludes 653 T1-weighted and 656 T2-weighted MRI images, accompanied bycorresponding IPMN risk scores from 7 leading medical institutions, making itthe largest and most diverse dataset for IPMN classification to date. We assessthe performance of DenseNet-121 in both centralized and federated settings fortraining on distributed data. Our results demonstrate that the federatedlearning approach achieves high classification accuracy comparable tocentralized learning while ensuring data privacy across institutions. This workmarks a significant advancement in collaborative IPMN classification,facilitating secure and high-accuracy model training across multiple centers.</description><author>Hongyi Pan, Ziliang Hong, Gorkem Durak, Elif Keles, Halil Ertugrul Aktas, Yavuz Taktak, Alpay Medetalibeyoglu, Zheyuan Zhang, Yury Velichko, Concetto Spampinato, Ivo Schoots, Marco J. Bruno, Pallavi Tiwari, Candice Bolan, Tamas Gonda, Frank Miller, Rajesh N. Keswani, Michael B. Wallace, Ziyue Xu, Ulas Bagci</author><pubDate>Fri, 08 Nov 2024 16:52:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05697v1</guid></item><item><title>Language Models can Infer Action Semantics for Symbolic Planners from Environment Feedback</title><link>http://arxiv.org/abs/2406.02791v2</link><description>Symbolic planners can discover a sequence of actions from initial to goalstates given expert-defined, domain-specific logical action semantics. LargeLanguage Models (LLMs) can directly generate such sequences, but limitations inreasoning and state-tracking often result in plans that are insufficient orunexecutable. We propose Predicting Semantics of Actions with Language Models(PSALM), which automatically learns action semantics by leveraging thestrengths of both symbolic planners and LLMs. PSALM repeatedly proposes andexecutes plans, using the LLM to partially generate plans and to inferdomain-specific action semantics based on execution outcomes. PSALM maintains abelief over possible action semantics that is iteratively updated until a goalstate is reached. Experiments on 7 environments show that when learning justfrom one goal, PSALM boosts plan success rate from 36.4% (on Claude-3.5) to100%, and explores the environment more efficiently than prior work to inferground truth domain action semantics.</description><author>Wang Zhu, Ishika Singh, Robin Jia, Jesse Thomason</author><pubDate>Fri, 08 Nov 2024 16:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02791v2</guid></item><item><title>FALCON: Feedback-driven Adaptive Long/short-term memory reinforced Coding Optimization system</title><link>http://arxiv.org/abs/2410.21349v2</link><description>Recently, large language models (LLMs) have achieved significant progress inautomated code generation. Despite their strong instruction-followingcapabilities, these models frequently struggled to align with user intent incoding scenarios. In particular, they were hampered by datasets that lackeddiversity and failed to address specialized tasks or edge cases. Furthermore,challenges in supervised fine-tuning (SFT) and reinforcement learning fromhuman feedback (RLHF) led to failures in generating precise,human-intent-aligned code. To tackle these challenges and improve the codegeneration performance for automated programming systems, we proposeFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization(i.e., FALCON). FALCON is structured into two hierarchical levels. From theglobal level, long-term memory improves code quality by retaining and applyinglearned knowledge. At the local level, short-term memory allows for theincorporation of immediate feedback from compilers and AI systems.Additionally, we introduce meta-reinforcement learning with feedback rewards tosolve the global-local bi-level optimization problem and enhance the model'sadaptability across diverse code generation tasks. Extensive experimentsdemonstrate that our technique achieves state-of-the-art performance, leadingother reinforcement learning methods by more than 4.5 percentage points on theMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. Theopen-sourced code is publicly available at https://github.com/titurte/FALCON.</description><author>Zeyuan Li, Yangfan He, Lewei He, Jianhui Wang, Tianyu Shi, Bin Lei, Yuchen Li, Qiuwu Chen</author><pubDate>Fri, 08 Nov 2024 16:50:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21349v2</guid></item><item><title>YOSO: You-Only-Sample-Once via Compressed Sensing for Graph Neural Network Training</title><link>http://arxiv.org/abs/2411.05693v1</link><description>Graph neural networks (GNNs) have become essential tools for analyzingnon-Euclidean data across various domains. During training stage, samplingplays an important role in reducing latency by limiting the number of nodesprocessed, particularly in large-scale applications. However, as the demand forbetter prediction performance grows, existing sampling algorithms becomeincreasingly complex, leading to significant overhead. To mitigate this, wepropose YOSO (You-Only-Sample-Once), an algorithm designed to achieve efficienttraining while preserving prediction accuracy. YOSO introduces a compressedsensing (CS)-based sampling and reconstruction framework, where nodes aresampled once at input layer, followed by a lossless reconstruction at theoutput layer per epoch. By integrating the reconstruction process with the lossfunction of specific learning tasks, YOSO not only avoids costly computationsin traditional compressed sensing (CS) methods, such as orthonormal basiscalculations, but also ensures high-probability accuracy retention whichequivalent to full node participation. Experimental results on nodeclassification and link prediction demonstrate the effectiveness and efficiencyof YOSO, reducing GNN training by an average of 75\% compared tostate-of-the-art methods, while maintaining accuracy on par with top-performingbaselines.</description><author>Yi Li, Zhichun Guo, Guanpeng Li, Bingzhe Li</author><pubDate>Fri, 08 Nov 2024 16:47:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05693v1</guid></item><item><title>Autoregressive Adaptive Hypergraph Transformer for Skeleton-based Activity Recognition</title><link>http://arxiv.org/abs/2411.05692v1</link><description>Extracting multiscale contextual information and higher-order correlationsamong skeleton sequences using Graph Convolutional Networks (GCNs) alone isinadequate for effective action classification. Hypergraph convolutionaddresses the above issues but cannot harness the long-range dependencies.Transformer proves to be effective in capturing these dependencies and makingcomplex contextual features accessible. We propose an Autoregressive AdaptiveHyperGraph Transformer (AutoregAd-HGformer) model for in-phase (autoregressiveand discrete) and out-phase (adaptive) hypergraph generation. The vectorquantized in-phase hypergraph equipped with powerful autoregressive learnedpriors produces a more robust and informative representation suitable forhyperedge formation. The out-phase hypergraph generator provides amodel-agnostic hyperedge learning technique to align the attributes with inputskeleton embedding. The hybrid (supervised and unsupervised) learning inAutoregAd-HGformer explores the action-dependent feature along spatial,temporal, and channel dimensions. The extensive experimental results andablation study indicate the superiority of our model over state-of-the-arthypergraph architectures on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.</description><author>Abhisek Ray, Ayush Raj, Maheshkumar H. Kolekar</author><pubDate>Fri, 08 Nov 2024 16:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05692v1</guid></item><item><title>Boulder2Vec: Modeling Climber Performances in Professional Bouldering Competitions</title><link>http://arxiv.org/abs/2411.02343v2</link><description>Using data from professional bouldering competitions from 2008 to 2022, wetrain a logistic regression to predict climber results and measure climberskill. However, this approach is limited, as a single numeric coefficient perclimber cannot adequately capture the intricacies of climbers' varyingstrengths and weaknesses in different boulder problems. For example, someclimbers might prefer more static, technical routes while other climbers mayspecialize in powerful, dynamic problems. To this end, we apply Probabilistic Matrix Factorization (PMF), a frameworkcommonly used in recommender systems, to represent the unique characteristicsof climbers and problems with latent, multi-dimensional vectors. In thisframework, a climber's performance on a given problem is predicted by takingthe dot product of the corresponding climber vector and problem vectors. PMFeffectively handles sparse datasets, such as our dataset where only a subset ofclimbers attempt each particular problem, by extrapolating patterns fromsimilar climbers. We contrast the empirical performance of PMF to the logistic regressionapproach and investigate the multivariate representations produced by PMF togain insights into climber characteristics. Our results show that themultivariate PMF representations improve predictive performance of professionalbouldering competitions by capturing both the overall strength of climbers andtheir specialized skill sets. We provide our code open-source athttps://github.com/baronet2/boulder2vec.</description><author>Ethan Baron, Victor Hau, Zeke Weng</author><pubDate>Fri, 08 Nov 2024 16:44:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02343v2</guid></item><item><title>Mathematical Formalized Problem Solving and Theorem Proving in Different Fields in Lean 4</title><link>http://arxiv.org/abs/2409.05977v3</link><description>Formalizing mathematical proofs using computerized verification languageslike Lean 4 has the potential to significantly impact the field of mathematics,it offers prominent capabilities for advancing mathematical reasoning. However,existing efforts are largely limited to creating formalized versions of proofsfrom extensive online mathematical corpora, struggling to keep pace with therapidly evolving nature of mathematics. To bridge the gap between traditionaland computerized proof techniques, this paper explores the use of LargeLanguage Models (LLMs) to generate formal proof steps and complete formalizedproofs. By converting natural language (NL) mathematical proofs into formalizedversions, this work introduces the basic structure and tactics of the Lean 4language. The goal is to determine how AI can be leveraged to assist themathematical formalization process and improve its performance. Severalexamples are provided that demonstrate solving problems using both traditionaland Lean 4-based approaches. Ultimately, this paper presents an explanation ofthe foundations of Lean 4 and comparative analyses of the mathematicalformalization process using traditional and AI-augmented techniques. Thefindings indicate that AI- powered tools have significant potential toaccelerate and enhance the formalization of mathematical proofs, paving the wayfor more efficient and reliable theorem-proving for AI for Math in the future.</description><author>Xichen Tang</author><pubDate>Fri, 08 Nov 2024 16:42:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05977v3</guid></item><item><title>Asterisk*: Keep it Simple</title><link>http://arxiv.org/abs/2411.05691v1</link><description>This paper describes Asterisk, a compact GPT-based model for generating textembeddings. The model uses a minimalist architecture with two layers, twoattention heads, and 256 embedding dimensions. By applying knowledgedistillation from larger pretrained models, we explore the trade-offs betweenmodel size and performance while minimizing computational and memoryrequirements. The model is primarily evaluated and optimized for classificationtasks, with experimental results showing its moderate performance in zero-shotclassification across various downstream applications. With additionalconfiguration, the model performance can approach or even surpass that oflarger architectures on specific classification tasks.</description><author>Andrew Semenov</author><pubDate>Fri, 08 Nov 2024 16:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05691v1</guid></item><item><title>LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property Prediction</title><link>http://arxiv.org/abs/2411.00177v2</link><description>Large language models (LLMs) are increasingly being used in materialsscience. However, little attention has been given to benchmarking andstandardized evaluation for LLM-based materials property prediction, whichhinders progress. We present LLM4Mat-Bench, the largest benchmark to date forevaluating the performance of LLMs in predicting the properties of crystallinematerials. LLM4Mat-Bench contains about 1.9M crystal structures in total,collected from 10 publicly available materials data sources, and 45 distinctproperties. LLM4Mat-Bench features different input modalities: crystalcomposition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1Btokens in total for each modality, respectively. We use LLM4Mat-Bench tofine-tune models with different sizes, including LLM-Prop and MatBERT, andprovide zero-shot and few-shot prompts to evaluate the property predictioncapabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. Theresults highlight the challenges of general-purpose LLMs in materials scienceand the need for task-specific predictive models and task-specificinstruction-tuned LLMs in materials property prediction.</description><author>Andre Niyongabo Rubungo, Kangming Li, Jason Hattrick-Simpers, Adji Bousso Dieng</author><pubDate>Fri, 08 Nov 2024 16:42:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00177v2</guid></item><item><title>Data-Driven Distributed Common Operational Picture from Heterogeneous Platforms using Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2411.05683v1</link><description>The integration of unmanned platforms equipped with advanced sensors promisesto enhance situational awareness and mitigate the "fog of war" in militaryoperations. However, managing the vast influx of data from these platformsposes a significant challenge for Command and Control (C2) systems. This studypresents a novel multi-agent learning framework to address this challenge. Ourmethod enables autonomous and secure communication between agents and humans,which in turn enables real-time formation of an interpretable CommonOperational Picture (COP). Each agent encodes its perceptions and actions intocompact vectors, which are then transmitted, received and decoded to form a COPencompassing the current state of all agents (friendly and enemy) on thebattlefield. Using Deep Reinforcement Learning (DRL), we jointly train COPmodels and agent's action selection policies. We demonstrate resilience todegraded conditions such as denied GPS and disrupted communications.Experimental validation is performed in the Starcraft-2 simulation environmentto evaluate the precision of the COPs and robustness of policies. We reportless than 5% error in COPs and policies resilient to various adversarialconditions. In summary, our contributions include a method for autonomous COPformation, increased resilience through distributed prediction, and jointtraining of COP models and multi-agent RL policies. This research advancesadaptive and resilient C2, facilitating effective control of heterogeneousunmanned platforms.</description><author>Indranil Sur, Aswin Raghavan, Abrar Rahman, James Z Hare, Daniel Cassenti, Carl Busart</author><pubDate>Fri, 08 Nov 2024 16:31:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05683v1</guid></item><item><title>Eigen Attention: Attention in Low-Rank Space for KV Cache Compression</title><link>http://arxiv.org/abs/2408.05646v2</link><description>Large language models (LLMs) represent a groundbreaking advancement in thedomain of natural language processing due to their impressive reasoningabilities. Recently, there has been considerable interest in increasing thecontext lengths for these models to enhance their applicability to complextasks. However, at long context lengths and large batch sizes, the key-value(KV) cache, which stores the attention keys and values, emerges as the newbottleneck in memory usage during inference. To address this, we propose EigenAttention, which performs the attention operation in a low-rank space, therebyreducing the KV cache memory overhead. Our proposed approach is orthogonal toexisting KV cache compression techniques and can be used synergistically withthem. Through extensive experiments over OPT, MPT, and Llama model families, wedemonstrate that Eigen Attention results in up to 40% reduction in KV cachesizes and up to 60% reduction in attention operation latency with minimal dropin performance. Code is available athttps://github.com/UtkarshSaxena1/EigenAttn.</description><author>Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy</author><pubDate>Fri, 08 Nov 2024 16:29:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05646v2</guid></item><item><title>Tell What You Hear From What You See -- Video to Audio Generation Through Text</title><link>http://arxiv.org/abs/2411.05679v1</link><description>The content of visual and audio scenes is multi-faceted such that a video canbe paired with various audio and vice-versa. Thereby, in video-to-audiogeneration task, it is imperative to introduce steering approaches forcontrolling the generated audio. While Video-to-Audio generation is awell-established generative task, existing methods lack such controllability.In this work, we propose VATT, a multi-modal generative framework that takes avideo and an optional text prompt as input, and generates audio and optionaltextual description of the audio. Such a framework has two advantages: i)Video-to-Audio generation process can be refined and controlled via text whichcomplements the context of visual information, and ii) The model can suggestwhat audio to generate for the video by generating audio captions. VATTconsists of two key modules: VATT Converter, a LLM that is fine-tuned forinstructions and includes a projection layer that maps video features to theLLM vector space; and VATT Audio, a transformer that generates audio tokensfrom visual frames and from optional text prompt using iterative paralleldecoding. The audio tokens are converted to a waveform by pretrained neuralcodec. Experiments show that when VATT is compared to existing video-to-audiogeneration methods in objective metrics, it achieves competitive performancewhen the audio caption is not provided. When the audio caption is provided as aprompt, VATT achieves even more refined performance (lowest KLD score of 1.41).Furthermore, subjective studies show that VATT Audio has been chosen aspreferred generated audio than audio generated by existing methods. VATTenables controllable video-to-audio generation through text as well assuggesting text prompts for videos through audio captions, unlocking novelapplications such as text-guided video-to-audio generation and video-to-audiocaptioning.</description><author>Xiulong Liu, Kun Su, Eli Shlizerman</author><pubDate>Fri, 08 Nov 2024 16:29:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05679v1</guid></item><item><title>Improving Molecular Graph Generation with Flow Matching and Optimal Transport</title><link>http://arxiv.org/abs/2411.05676v1</link><description>Generating molecular graphs is crucial in drug design and discovery butremains challenging due to the complex interdependencies between nodes andedges. While diffusion models have demonstrated their potentiality in moleculargraph design, they often suffer from unstable training and inefficientsampling. To enhance generation performance and training stability, we proposeGGFlow, a discrete flow matching generative model incorporating optimaltransport for molecular graphs and it incorporates an edge-augmented graphtransformer to enable the direct communications among chemical bounds.Additionally, GGFlow introduces a novel goal-guided generation framework tocontrol the generative trajectory of our model, aiming to design novelmolecular structures with the desired properties. GGFlow demonstrates superiorperformance on both unconditional and conditional molecule generation tasks,outperforming existing baselines and underscoring its effectiveness andpotential for wider application.</description><author>Xiaoyang Hou, Tian Zhu, Milong Ren, Dongbo Bu, Xin Gao, Chunming Zhang, Shiwei Sun</author><pubDate>Fri, 08 Nov 2024 16:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05676v1</guid></item><item><title>Contextual Document Embeddings</title><link>http://arxiv.org/abs/2410.02525v4</link><description>Dense document embeddings are central to neural retrieval. The dominantparadigm is to train and construct embeddings by running encoders directly onindividual documents. In this work, we argue that these embeddings, whileeffective, are implicitly out-of-context for targeted use cases of retrieval,and that a contextualized document embedding should take into account both thedocument and neighboring documents in context - analogous to contextualizedword embeddings. We propose two complementary methods for contextualizeddocument embeddings: first, an alternative contrastive learning objective thatexplicitly incorporates the document neighbors into the intra-batch contextualloss; second, a new contextual architecture that explicitly encodes neighbordocument information into the encoded representation. Results show that bothmethods achieve better performance than biencoders in several settings, withdifferences especially pronounced out-of-domain. We achieve state-of-the-artresults on the MTEB benchmark with no hard negative mining, score distillation,dataset-specific instructions, intra-GPU example-sharing, or extremely largebatch sizes. Our method can be applied to improve performance on anycontrastive learning dataset and any biencoder.</description><author>John X. Morris, Alexander M. Rush</author><pubDate>Fri, 08 Nov 2024 16:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.02525v4</guid></item><item><title>DiffBatt: A Diffusion Model for Battery Degradation Prediction and Synthesis</title><link>http://arxiv.org/abs/2410.23893v3</link><description>Battery degradation remains a critical challenge in the pursuit of greentechnologies and sustainable energy solutions. Despite significant researchefforts, predicting battery capacity loss accurately remains a formidable taskdue to its complex nature, influenced by both aging and cycling behaviors. Toaddress this challenge, we introduce a novel general-purpose model for batterydegradation prediction and synthesis, DiffBatt. Leveraging an innovativecombination of conditional and unconditional diffusion models withclassifier-free guidance and transformer architecture, DiffBatt achieves highexpressivity and scalability. DiffBatt operates as a probabilistic model tocapture uncertainty in aging behaviors and a generative model to simulatebattery degradation. The performance of the model excels in prediction taskswhile also enabling the generation of synthetic degradation curves,facilitating enhanced model training by data augmentation. In the remaininguseful life prediction task, DiffBatt provides accurate results with a meanRMSE of 196 cycles across all datasets, outperforming all other models anddemonstrating superior generalizability. This work represents an important steptowards developing foundational models for battery degradation.</description><author>Hamidreza Eivazi, Andr√© Hebenbrock, Raphael Ginster, Steffen Bl√∂meke, Stefan Wittek, Christoph Herrmann, Thomas S. Spengler, Thomas Turek, Andreas Rausch</author><pubDate>Fri, 08 Nov 2024 16:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23893v3</guid></item><item><title>Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms</title><link>http://arxiv.org/abs/2310.20598v3</link><description>We introduce and study online conversion with switching costs, a family ofonline problems that capture emerging problems at the intersection of energyand sustainability. In this problem, an online player attempts to purchase(alternatively, sell) fractional shares of an asset during a fixed time horizonwith length $T$. At each time step, a cost function (alternatively, pricefunction) is revealed, and the player must irrevocably decide an amount ofasset to convert. The player also incurs a switching cost whenever theirdecision changes in consecutive time steps, i.e., when they increase ordecrease their purchasing amount. We introduce competitive (robust)threshold-based algorithms for both the minimization and maximization variantsof this problem, and show they are optimal among deterministic onlinealgorithms. We then propose learning-augmented algorithms that take advantageof untrusted black-box advice (such as predictions from a machine learningmodel) to achieve significantly better average-case performance withoutsacrificing worst-case competitive guarantees. Finally, we empirically evaluateour proposed algorithms using a carbon-aware EV charging case study, showingthat our algorithms substantially improve on baseline methods for this problem.</description><author>Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</author><pubDate>Fri, 08 Nov 2024 16:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20598v3</guid></item><item><title>DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection</title><link>http://arxiv.org/abs/2402.17176v2</link><description>Model-X knockoff has garnered significant attention among various featureselection methods due to its guarantees for controlling the false discoveryrate (FDR). Since its introduction in parametric design, knockoff techniqueshave evolved to handle arbitrary data distributions using deep learning-basedgenerative models. However, we have observed limitations in the currentimplementations of the deep Model-X knockoff framework. Notably, the "swapproperty" that knockoffs require often faces challenges at the sample level,resulting in diminished selection power. To address these issues, we develop"Deep Dependency Regularized Knockoff (DeepDRK)," a distribution-free deeplearning method that effectively balances FDR and power. In DeepDRK, weintroduce a novel formulation of the knockoff model as a learning problem undermulti-source adversarial attacks. By employing an innovative perturbationtechnique, we achieve lower FDR and higher power. Our model outperformsexisting benchmarks across synthetic, semi-synthetic, and real-world datasets,particularly when sample sizes are small and data distributions arenon-Gaussian.</description><author>Hongyu Shen, Yici Yan, Zhizhen Zhao</author><pubDate>Fri, 08 Nov 2024 16:09:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17176v2</guid></item><item><title>Gymnasium: A Standard Interface for Reinforcement Learning Environments</title><link>http://arxiv.org/abs/2407.17032v3</link><description>Reinforcement Learning (RL) is a continuously growing field that has thepotential to revolutionize many areas of artificial intelligence. However,despite its promise, RL research is often hindered by the lack ofstandardization in environment and algorithm implementations. This makes itdifficult for researchers to compare and build upon each other's work, slowingdown progress in the field. Gymnasium is an open-source library that provides astandard API for RL environments, aiming to tackle this issue. Gymnasium's mainfeature is a set of abstractions that allow for wide interoperability betweenenvironments and training algorithms, making it easier for researchers todevelop and test RL algorithms. In addition, Gymnasium provides a collection ofeasy-to-use environments, tools for easily customizing environments, and toolsto ensure the reproducibility and robustness of RL research. Through thisunified framework, Gymnasium significantly streamlines the process ofdeveloping and testing RL algorithms, enabling researchers to focus more oninnovation and less on implementation details. By providing a standardizedplatform for RL research, Gymnasium helps to drive forward the field ofreinforcement learning and unlock its full potential. Gymnasium is availableonline at https://github.com/Farama-Foundation/Gymnasium</description><author>Mark Towers, Ariel Kwiatkowski, Jordan Terry, John U. Balis, Gianluca De Cola, Tristan Deleu, Manuel Goul√£o, Andreas Kallinteris, Markus Krimmel, Arjun KG, Rodrigo Perez-Vicente, Andrea Pierr√©, Sander Schulhoff, Jun Jet Tai, Hannah Tan, Omar G. Younis</author><pubDate>Fri, 08 Nov 2024 16:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17032v3</guid></item><item><title>Unmasking the Limits of Large Language Models: A Systematic Evaluation of Masked Text Processing Ability through MskQA and MskCal</title><link>http://arxiv.org/abs/2411.05665v1</link><description>This paper sheds light on the limitations of Large Language Models (LLMs) byrigorously evaluating their ability to process masked text. We introduce twonovel tasks: MskQA, measuring reasoning on masked question-answering datasetslike RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmeticproblems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit someresilience to masked text, their performance is highly contingent on maskingrates and semantic cues. Specifically, "solid masking," where semantic cluesare entirely absent, leads to a significant performance drop compared to"partial lifting," where some semantic information is retained, indicatingLLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistentlyoutperforms 4o-mini, particularly in MskCal, demonstrating a greater ability tohandle numerical reasoning with masked text. This underscores the crucial roleof semantic cues in the reasoning process of LLMs. Our study illuminates theinterplay between background knowledge and reasoning ability in masked textprocessing, paving the way for a deeper understanding of LLM capabilities andlimitations, and highlighting the need for more robust evaluation methods toaccurately assess their true comprehension abilities.</description><author>Fuka Matsuzaki, Haru-Tada Sato</author><pubDate>Fri, 08 Nov 2024 16:07:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05665v1</guid></item><item><title>GPTKB: Building Very Large Knowledge Bases from Language Models</title><link>http://arxiv.org/abs/2411.04920v2</link><description>General-domain knowledge bases (KB), in particular the "big three" --Wikidata, Yago and DBpedia -- are the backbone of many intelligentapplications. While these three have seen steady development, comprehensive KBconstruction at large has seen few fresh attempts. In this work, we propose tobuild a large general-domain KB entirely from a large language model (LLM). Wedemonstrate the feasibility of large-scale KB construction from LLMs, whilehighlighting specific challenges arising around entity recognition, entity andproperty canonicalization, and taxonomy construction. As a prototype, we useGPT-4o-mini to construct GPTKB, which contains 105 million triples for morethan 2.9 million entities, at a cost 100x less than previous KBC projects. Ourwork is a landmark for two fields: For NLP, for the first time, it provides\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For theSemantic Web, it shows novel ways forward for the long-standing challenge ofgeneral-domain KB construction. GPTKB is accessible at http://gptkb.org.</description><author>Yujia Hu, Shrestha Ghosh, Tuan-Phong Nguyen, Simon Razniewski</author><pubDate>Fri, 08 Nov 2024 16:06:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04920v2</guid></item><item><title>Online-LoRA: Task-free Online Continual Learning via Low Rank Adaptation</title><link>http://arxiv.org/abs/2411.05663v1</link><description>Catastrophic forgetting is a significant challenge in online continuallearning (OCL), especially for non-stationary data streams that do not havewell-defined task boundaries. This challenge is exacerbated by the memoryconstraints and privacy concerns inherent in rehearsal buffers. To tacklecatastrophic forgetting, in this paper, we introduce Online-LoRA, a novelframework for task-free OCL. Online-LoRA allows to finetune pre-trained VisionTransformer (ViT) models in real-time to address the limitations of rehearsalbuffers and leverage pre-trained models' performance benefits. As the maincontribution, our approach features a novel online weight regularizationstrategy to identify and consolidate important model parameters. Moreover,Online-LoRA leverages the training dynamics of loss values to enable theautomatic recognition of the data distribution shifts. Extensive experimentsacross many task-free OCL scenarios and benchmark datasets (includingCIFAR-100, ImageNet-R, ImageNet-S, CUB-200 and CORe50) demonstrate thatOnline-LoRA can be robustly adapted to various ViT architectures, whileachieving better performance compared to SOTA methods. Our code will bepublicly available at:https://github.com/Christina200/Online-LoRA-official.git.</description><author>Xiwen Wei, Guihong Li, Radu Marculescu</author><pubDate>Fri, 08 Nov 2024 16:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05663v1</guid></item><item><title>Multi-armed Bandits with Missing Outcome</title><link>http://arxiv.org/abs/2411.05661v1</link><description>While significant progress has been made in designing algorithms thatminimize regret in online decision-making, real-world scenarios often introduceadditional complexities, perhaps the most challenging of which is missingoutcomes. Overlooking this aspect or simply assuming random missingnessinvariably leads to biased estimates of the rewards and may result in linearregret. Despite the practical relevance of this challenge, no rigorousmethodology currently exists for systematically handling missingness,especially when the missingness mechanism is not random. In this paper, weaddress this gap in the context of multi-armed bandits (MAB) with missingoutcomes by analyzing the impact of different missingness mechanisms onachievable regret bounds. We introduce algorithms that account for missingnessunder both missing at random (MAR) and missing not at random (MNAR) models.Through both analytical and simulation studies, we demonstrate the drasticimprovements in decision-making by accounting for missingness in thesesettings.</description><author>Ilia Mahrooghi, Mahshad Moradi, Sina Akbari, Negar Kiyavash</author><pubDate>Fri, 08 Nov 2024 16:02:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05661v1</guid></item><item><title>Adaptive Refinement Protocols for Distributed Distribution Estimation under $\ell^p$-Losses</title><link>http://arxiv.org/abs/2410.06884v2</link><description>Consider the communication-constrained estimation of discrete distributionsunder $\ell^p$ losses, where each distributed terminal holds multipleindependent samples and uses limited number of bits to describe the samples. Weobtain the minimax optimal rates of the problem in most parameter regimes. Anelbow effect of the optimal rates at $p=2$ is clearly identified. To show theoptimal rates, we first design estimation protocols to achieve them. The keyingredient of these protocols is to introduce adaptive refinement mechanisms,which first generate rough estimate by partial information and then establishrefined estimate in subsequent steps guided by the rough estimate. Theprotocols leverage successive refinement, sample compression, thresholding andrandom hashing methods to achieve the optimal rates in different parameterregimes. The optimality of the protocols is shown by deriving compatibleminimax lower bounds.</description><author>Deheng Yuan, Tao Guo, Zhongyi Huang</author><pubDate>Fri, 08 Nov 2024 16:02:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.06884v2</guid></item><item><title>Response Theory via Generative Score Modeling</title><link>http://arxiv.org/abs/2402.01029v3</link><description>We introduce an approach for analyzing the responses of dynamical systems toexternal perturbations that combines score-based generative modeling with theGeneralized Fluctuation-Dissipation Theorem (GFDT). The methodology enablesaccurate estimation of system responses, including those with non-Gaussianstatistics. We numerically validate our approach using time-series data fromthree different stochastic partial differential equations of increasingcomplexity: an Ornstein-Uhlenbeck process with spatially correlated noise, amodified stochastic Allen-Cahn equation, and the 2D Navier-Stokes equations. Wedemonstrate the improved accuracy of the methodology over conventional methodsand discuss its potential as a versatile tool for predicting the statisticalbehavior of complex dynamical systems.</description><author>Ludovico Theo Giorgini, Katherine Deck, Tobias Bischoff, Andre Souza</author><pubDate>Fri, 08 Nov 2024 15:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01029v3</guid></item><item><title>Aggregating distribution forecasts from deep ensembles</title><link>http://arxiv.org/abs/2204.02291v2</link><description>The importance of accurately quantifying forecast uncertainty has motivatedmuch recent research on probabilistic forecasting. In particular, a variety ofdeep learning approaches has been proposed, with forecast distributionsobtained as output of neural networks. These neural network-based methods areoften used in the form of an ensemble, e.g., based on multiple model runs fromdifferent random initializations or more sophisticated ensembling strategiessuch as dropout, resulting in a collection of forecast distributions that needto be aggregated into a final probabilistic prediction. With the aim ofconsolidating findings from the machine learning literature on ensemble methodsand the statistical literature on forecast combination, we address the questionof how to aggregate distribution forecasts based on such `deep ensembles'.Using theoretical arguments and a comprehensive analysis on twelve benchmarkdata sets, we systematically compare probability- and quantile-basedaggregation methods for three neural network-based approaches with differentforecast distribution types as output. Our results show that combining forecastdistributions from deep ensembles can substantially improve the predictiveperformance. We propose a general quantile aggregation framework for deepensembles that allows for corrections of systematic deficiencies and performswell in a variety of settings, often superior compared to a linear combinationof the forecast densities. Finally, we investigate the effects of the ensemblesize and derive recommendations of aggregating distribution forecasts from deepensembles in practice.</description><author>Benedikt Schulz, Lutz K√∂hler, Sebastian Lerch</author><pubDate>Fri, 08 Nov 2024 15:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.02291v2</guid></item><item><title>Using Time-Aware Graph Neural Networks to Predict Temporal Centralities in Dynamic Graphs</title><link>http://arxiv.org/abs/2310.15865v2</link><description>Node centralities play a pivotal role in network science, social networkanalysis, and recommender systems. In temporal data, static path-basedcentralities like closeness or betweenness can give misleading results aboutthe true importance of nodes in a temporal graph. To address this issue,temporal generalizations of betweenness and closeness have been defined thatare based on the shortest time-respecting paths between pairs of nodes.However, a major issue of those generalizations is that the calculation of suchpaths is computationally expensive. Addressing this issue, we study theapplication of De Bruijn Graph Neural Networks (DBGNN), a time-aware graphneural network architecture, to predict temporal path-based centralities intime series data. We experimentally evaluate our approach in 13 temporal graphsfrom biological and social systems and show that it considerably improves theprediction of betweenness and closeness centrality compared to (i) a staticGraph Convolutional Neural Network, (ii) an efficient sampling-basedapproximation technique for temporal betweenness, and (iii) twostate-of-the-art time-aware graph learning techniques for dynamic graphs.</description><author>Franziska Heeg, Ingo Scholtes</author><pubDate>Fri, 08 Nov 2024 15:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15865v2</guid></item><item><title>xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics</title><link>http://arxiv.org/abs/2406.14553v2</link><description>State-of-the-art trainable machine translation evaluation metrics like xCOMETachieve high correlation with human judgment but rely on large encoders (up to10.7B parameters), making them computationally expensive and inaccessible toresearchers with limited resources. To address this issue, we investigatewhether the knowledge stored in these large encoders can be compressed whilemaintaining quality. We employ distillation, quantization, and pruningtechniques to create efficient xCOMET alternatives and introduce a novel datacollection pipeline for efficient black-box distillation. Our experiments showthat, using quantization, xCOMET can be compressed up to three times with noquality degradation. Additionally, through distillation, we create an278M-sized xCOMET-lite metric, which has only 2.6% of xCOMET-XXL parameters,but retains 92.1% of its quality. Besides, it surpasses strong small-scalemetrics like COMET-22 and BLEURT-20 on the WMT22 metrics challenge dataset by6.4%, despite using 50% fewer parameters. All code, dataset, and models areavailable online at https://github.com/NL2G/xCOMET-lite.</description><author>Daniil Larionov, Mikhail Seleznyov, Vasiliy Viskov, Alexander Panchenko, Steffen Eger</author><pubDate>Fri, 08 Nov 2024 15:50:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14553v2</guid></item><item><title>The influence of persona and conversational task on social interactions with a LLM-controlled embodied conversational agent</title><link>http://arxiv.org/abs/2411.05653v1</link><description>Large Language Models (LLMs) have demonstrated remarkable capabilities inconversational tasks. Embodying an LLM as a virtual human allows users toengage in face-to-face social interactions in Virtual Reality. However, theinfluence of person- and task-related factors in social interactions withLLM-controlled agents remains unclear. In this study, forty-six participantsinteracted with a virtual agent whose persona was manipulated as extravert orintrovert in three different conversational tasks (small talk, knowledge test,convincing). Social-evaluation, emotional experience, and realism were assessedusing ratings. Interactive engagement was measured by quantifying participants'words and conversational turns. Finally, we measured participants' willingnessto ask the agent for help during the knowledge test. Our findings show that theextraverted agent was more positively evaluated, elicited a more pleasantexperience and greater engagement, and was assessed as more realistic comparedto the introverted agent. Whereas persona did not affect the tendency to askfor help, participants were generally more confident in the answer when theyhad help of the LLM. Variation of personality traits of LLM-controlled embodiedvirtual agents, therefore, affects social-emotional processing and behavior invirtual interactions. Embodied virtual agents allow the presentation ofnaturalistic social encounters in a virtual environment.</description><author>Leon O. H. Kroczek, Alexander May, Selina Hettenkofer, Andreas Ruider, Bernd Ludwig, Andreas M√ºhlberger</author><pubDate>Fri, 08 Nov 2024 15:49:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05653v1</guid></item><item><title>Enhancing Model Fairness and Accuracy with Similarity Networks: A Methodological Approach</title><link>http://arxiv.org/abs/2411.05648v1</link><description>In this paper, we propose an innovative approach to thoroughly exploredataset features that introduce bias in downstream machine-learning tasks.Depending on the data format, we use different techniques to map instances intoa similarity feature space. Our method's ability to adjust the resolution ofpairwise similarity provides clear insights into the relationship between thedataset classification complexity and model fairness. Experimental resultsconfirm the promising applicability of the similarity network in promoting fairmodels. Moreover, leveraging our methodology not only seems promising inproviding a fair downstream task such as classification, it also performs wellin imputation and augmentation of the dataset satisfying the fairness criteriasuch as demographic parity and imbalanced classes.</description><author>Samira Maghool, Paolo Ceravolo</author><pubDate>Fri, 08 Nov 2024 15:43:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05648v1</guid></item><item><title>Evaluating Large Language Model Capability in Vietnamese Fact-Checking Data Generation</title><link>http://arxiv.org/abs/2411.05641v1</link><description>Large Language Models (LLMs), with gradually improving reading comprehensionand reasoning capabilities, are being applied to a range of complex languagetasks, including the automatic generation of language data for variouspurposes. However, research on applying LLMs for automatic data generation inlow-resource languages like Vietnamese is still underdeveloped and lackscomprehensive evaluation. In this paper, we explore the use of LLMs forautomatic data generation for the Vietnamese fact-checking task, which facessignificant data limitations. Specifically, we focus on fact-checking datawhere claims are synthesized from multiple evidence sentences to assess theinformation synthesis capabilities of LLMs. We develop an automatic dataconstruction process using simple prompt techniques on LLMs and explore severalmethods to improve the quality of the generated data. To evaluate the qualityof the data generated by LLMs, we conduct both manual quality assessments andperformance evaluations using language models. Experimental results and manualevaluations illustrate that while the quality of the generated data hassignificantly improved through fine-tuning techniques, LLMs still cannot matchthe data quality produced by humans.</description><author>Long Truong To, Hung Tuan Le, Dat Van-Thanh Nguyen, Manh Trong Nguyen, Tri Thien Nguyen, Tin Van Huynh, Kiet Van Nguyen</author><pubDate>Fri, 08 Nov 2024 15:35:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05641v1</guid></item><item><title>Assessing Open-Source Large Language Models on Argumentation Mining Subtasks</title><link>http://arxiv.org/abs/2411.05639v1</link><description>We explore the capability of four open-sourcelarge language models (LLMs) inargumentation mining (AM). We conduct experiments on three different corpora;persuasive essays(PE), argumentative microtexts (AMT) Part 1 and Part 2, basedon two argumentation mining sub-tasks: (i) argumentative discourse unitsclassifications (ADUC), and (ii) argumentative relation classification (ARC).This work aims to assess the argumentation capability of open-source LLMs,including Mistral 7B, Mixtral8x7B, LlamA2 7B and LlamA3 8B in both, zero-shotand few-shot scenarios. Our analysis contributes to further assessingcomputational argumentation with open-source LLMs in future research efforts.</description><author>Mohammad Yeghaneh Abkenar, Weixing Wang, Hendrik Graupner, Manfred Stede</author><pubDate>Fri, 08 Nov 2024 15:34:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05639v1</guid></item><item><title>Impact of Fake News on Social Media Towards Public Users of Different Age Groups</title><link>http://arxiv.org/abs/2411.05638v1</link><description>This study examines how fake news affects social media users across a rangeof age groups and how machine learning (ML) and artificial intelligence (AI)can help reduce the spread of false information. The paper evaluates variousmachine learning models for their efficacy in identifying and categorizing fakenews and examines current trends in the spread of fake news, including deepfaketechnology. The study assesses four models using a Kaggle dataset: RandomForest, Support Vector Machine (SVM), Neural Networks, and Logistic Regression.The results show that SVM and neural networks perform better than other models,with accuracies of 93.29% and 93.69%, respectively. The study also emphasiseshow people in the elder age group diminished capacity for critical analysis ofnews content makes them more susceptible to disinformation. Natural languageprocessing (NLP) and deep learning approaches have the potential to improve theaccuracy of false news detection. Biases in AI and ML models and difficultiesin identifying information generated by AI continue to be major problems inspite of the developments. The study recommends that datasets be expanded toencompass a wider range of languages and that detection algorithms becontinuously improved to keep up with the latest advancements in disinformationtactics. In order to combat fake news and promote an informed and resilientsociety, this study emphasizes the value of cooperative efforts between AIresearchers, social media platforms, and governments.</description><author>Kahlil bin Abdul Hakim, Sathishkumar Veerappampalayam Easwaramoorthy</author><pubDate>Fri, 08 Nov 2024 15:32:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05638v1</guid></item><item><title>Video RWKV:Video Action Recognition Based RWKV</title><link>http://arxiv.org/abs/2411.05636v1</link><description>To address the challenges of high computational costs and long-distancedependencies in exist ing video understanding methods, such as CNNs andTransformers, this work introduces RWKV to the video domain in a novel way. Wepropose a LSTM CrossRWKV (LCR) framework, designed for spatiotemporalrepresentation learning to tackle the video understanding task. Specifically,the proposed linear complexity LCR incorporates a novel Cross RWKV gate tofacilitate interaction be tween current frame edge information and pastfeatures, enhancing the focus on the subject through edge features and globallyaggregating inter-frame features over time. LCR stores long-term mem ory forvideo processing through an enhanced LSTM recurrent execution mechanism. Byleveraging the Cross RWKV gate and recurrent execution, LCR effectivelycaptures both spatial and temporal features. Additionally, the edge informationserves as a forgetting gate for LSTM, guiding long-term memory management.Tubemasking strategy reduces redundant information in food and reducesoverfitting.These advantages enable LSTM CrossRWKV to set a new benchmark invideo under standing, offering a scalable and efficient solution forcomprehensive video analysis. All code and models are publicly available.</description><author>Zhuowen Yin, Chengru Li, Xingbo Dong</author><pubDate>Fri, 08 Nov 2024 15:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05636v1</guid></item><item><title>SynDroneVision: A Synthetic Dataset for Image-Based Drone Detection</title><link>http://arxiv.org/abs/2411.05633v1</link><description>Developing robust drone detection systems is often constrained by the limitedavailability of large-scale annotated training data and the high costsassociated with real-world data collection. However, leveraging synthetic datagenerated via game engine-based simulations provides a promising andcost-effective solution to overcome this issue. Therefore, we presentSynDroneVision, a synthetic dataset specifically designed for RGB-based dronedetection in surveillance applications. Featuring diverse backgrounds, lightingconditions, and drone models, SynDroneVision offers a comprehensive trainingfoundation for deep learning algorithms. To evaluate the dataset'seffectiveness, we perform a comparative analysis across a selection of recentYOLO detection models. Our findings demonstrate that SynDroneVision is avaluable resource for real-world data enrichment, achieving notableenhancements in model performance and robustness, while significantly reducingthe time and costs of real-world data acquisition. SynDroneVision will bepublicly released upon paper acceptance.</description><author>Tamara R. Lenhard, Andreas Weinmann, Kai Franke, Tobias Koch</author><pubDate>Fri, 08 Nov 2024 15:22:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05633v1</guid></item><item><title>Enhancing Robot Navigation Policies with Task-Specific Uncertainty Management</title><link>http://arxiv.org/abs/2410.15178v2</link><description>Robots performing navigation tasks in complex environments face significantchallenges due to uncertainty in state estimation. Effectively managing thisuncertainty is crucial, but the optimal approach varies depending on thespecific details of the task: different tasks require varying levels ofprecision in different regions of the environment. For instance, a robotnavigating a crowded space might need precise localization near obstacles butcan operate effectively with less precise state estimates in open areas. Thisvarying need for certainty in different parts of the environment, depending onthe task, calls for policies that can adapt their uncertainty managementstrategies based on task-specific requirements. In this paper, we present aframework for integrating task-specific uncertainty requirements directly intonavigation policies. We introduce Task-Specific Uncertainty Map (TSUM), whichrepresents acceptable levels of state estimation uncertainty across differentregions of the operating environment for a given task. Using TSUM, we proposeGeneralized Uncertainty Integration for Decision-Making and Execution (GUIDE),a policy conditioning framework that incorporates these uncertaintyrequirements into the robot's decision-making process. We find thatconditioning policies on TSUMs provides an effective way to expresstask-specific uncertainty requirements and enables the robot to reason aboutthe context-dependent value of certainty. We show how integrating GUIDE intoreinforcement learning frameworks allows the agent to learn navigation policieswithout the need for explicit reward engineering to balance task completion anduncertainty management. We evaluate GUIDE on a variety of real-world navigationtasks and find that it demonstrates significant improvements in task completionrates compared to baselines. Evaluation videos can be found athttps://guided-agents.github.io.</description><author>Gokul Puthumanaillam, Paulo Padrao, Jose Fuentes, Leonardo Bobadilla, Melkior Ornik</author><pubDate>Fri, 08 Nov 2024 15:22:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15178v2</guid></item><item><title>Physics-constrained coupled neural differential equations for one dimensional blood flow modeling</title><link>http://arxiv.org/abs/2411.05631v1</link><description>Computational cardiovascular flow modeling plays a crucial role inunderstanding blood flow dynamics. While 3D models provide acute details, theyare computationally expensive, especially with fluid-structure interaction(FSI) simulations. 1D models offer a computationally efficient alternative, bysimplifying the 3D Navier-Stokes equations through axisymmetric flow assumptionand cross-sectional averaging. However, traditional 1D models based on finiteelement methods (FEM) often lack accuracy compared to 3D averaged solutions.This study introduces a novel physics-constrained machine learning techniquethat enhances the accuracy of 1D blood flow models while maintainingcomputational efficiency. Our approach, utilizing a physics-constrained coupledneural differential equation (PCNDE) framework, demonstrates superiorperformance compared to conventional FEM-based 1D models across a wide range ofinlet boundary condition waveforms and stenosis blockage ratios. A keyinnovation lies in the spatial formulation of the momentum conservationequation, departing from the traditional temporal approach and capitalizing onthe inherent temporal periodicity of blood flow. This spatial neuraldifferential equation formulation switches space and time and overcomes issuesrelated to coupling stability and smoothness, while simplifying boundarycondition implementation. The model accurately captures flow rate, area, andpressure variations for unseen waveforms and geometries. We evaluate themodel's robustness to input noise and explore the loss landscapes associatedwith the inclusion of different physics terms. This advanced 1D modelingtechnique offers promising potential for rapid cardiovascular simulations,achieving computational efficiency and accuracy. By combining the strengths ofphysics-based and data-driven modeling, this approach enables fast and accuratecardiovascular simulations.</description><author>Hunor Csala, Arvind Mohan, Daniel Livescu, Amirhossein Arzani</author><pubDate>Fri, 08 Nov 2024 15:22:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05631v1</guid></item><item><title>Cross-validating causal discovery via Leave-One-Variable-Out</title><link>http://arxiv.org/abs/2411.05625v1</link><description>We propose a new approach to falsify causal discovery algorithms withoutground truth, which is based on testing the causal model on a pair of variablesthat has been dropped when learning the causal model. To this end, we use the"Leave-One-Variable-Out (LOVO)" prediction where $Y$ is inferred from $X$without any joint observations of $X$ and $Y$, given only training data from$X,Z_1,\dots,Z_k$ and from $Z_1,\dots,Z_k,Y$. We demonstrate that causal modelson the two subsets, in the form of Acyclic Directed Mixed Graphs (ADMGs), oftenentail conclusions on the dependencies between $X$ and $Y$, enabling this typeof prediction. The prediction error can then be estimated since the jointdistribution $P(X, Y)$ is assumed to be available, and $X$ and $Y$ have onlybeen omitted for the purpose of falsification. After presenting this graphicalmethod, which is applicable to general causal discovery algorithms, weillustrate how to construct a LOVO predictor tailored towards algorithmsrelying on specific a priori assumptions, such as linear additive noise models.Simulations indicate that the LOVO prediction error is indeed correlated withthe accuracy of the causal outputs, affirming the method's effectiveness.</description><author>Daniela Schkoda, Philipp Faller, Patrick Bl√∂baum, Dominik Janzing</author><pubDate>Fri, 08 Nov 2024 15:15:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05625v1</guid></item><item><title>Benchmarking Ultra-High-Definition Image Reflection Removal</title><link>http://arxiv.org/abs/2308.00265v2</link><description>Deep learning based methods have achieved significant success in the task ofsingle image reflection removal (SIRR). However, the majority of these methodsare focused on High-Definition/Standard-Definition (HD/SD) images, whileignoring higher resolution images such as Ultra-High-Definition (UHD) images.With the increasing prevalence of UHD images captured by modern devices, inthis paper, we aim to address the problem of UHD SIRR. Specifically, we firstsynthesize two large-scale UHD datasets, UHDRR4K and UHDRR8K. The UHDRR4Kdataset consists of $2,999$ and $168$ quadruplets of images for training andtesting respectively, and the UHDRR8K dataset contains $1,014$ and $105$quadruplets. To the best of our knowledge, these two datasets are the firstlargest-scale UHD datasets for SIRR. Then, we conduct a comprehensiveevaluation of six state-of-the-art SIRR methods using the proposed datasets.Based on the results, we provide detailed discussions regarding the strengthsand limitations of these methods when applied to UHD images. Finally, wepresent a transformer-based architecture named RRFormer for reflection removal.RRFormer comprises three modules, namely the Prepossessing Embedding Module,Self-attention Feature Extraction Module, and Multi-scale Spatial FeatureExtraction Module. These modules extract hypercolumn features, global andpartial attention features, and multi-scale spatial features, respectively. Toensure effective training, we utilize three terms in our loss function: pixelloss, feature loss, and adversarial loss. We demonstrate through experimentalresults that RRFormer achieves state-of-the-art performance on both the non-UHDdataset and our proposed UHDRR datasets. The code and datasets are publiclyavailable athttps://github.com/Liar-zzy/Benchmarking-Ultra-High-Definition-Single-Image-Reflection-Removal.</description><author>Zhenyuan Zhang, Zhenbo Song, Kaihao Zhang, Zhaoxin Fan, Jianfeng Lu</author><pubDate>Fri, 08 Nov 2024 15:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00265v2</guid></item><item><title>WHALE: Towards Generalizable and Scalable World Models for Embodied Decision-making</title><link>http://arxiv.org/abs/2411.05619v1</link><description>World models play a crucial role in decision-making within embodiedenvironments, enabling cost-free explorations that would otherwise be expensivein the real world. To facilitate effective decision-making, world models mustbe equipped with strong generalizability to support faithful imagination inout-of-distribution (OOD) regions and provide reliable uncertainty estimationto assess the credibility of the simulated experiences, both of which presentsignificant challenges for prior scalable approaches. This paper introducesWHALE, a framework for learning generalizable world models, consisting of twokey techniques: behavior-conditioning and retracing-rollout.Behavior-conditioning addresses the policy distribution shift, one of theprimary sources of the world model generalization error, whileretracing-rollout enables efficient uncertainty estimation without thenecessity of model ensembles. These techniques are universal and can becombined with any neural network architecture for world model learning.Incorporating these two techniques, we present Whale-ST, a scalablespatial-temporal transformer-based world model with enhanced generalizability.We demonstrate the superiority of Whale-ST in simulation tasks by evaluatingboth value estimation accuracy and video generation fidelity. Additionally, weexamine the effectiveness of our uncertainty estimation technique, whichenhances model-based policy optimization in fully offline scenarios.Furthermore, we propose Whale-X, a 414M parameter world model trained on 970Ktrajectories from Open X-Embodiment datasets. We show that Whale-X exhibitspromising scalability and strong generalizability in real-world manipulationscenarios using minimal demonstrations.</description><author>Zhilong Zhang, Ruifeng Chen, Junyin Ye, Yihao Sun, Pengyuan Wang, Jingcheng Pang, Kaiyuan Li, Tianshuo Liu, Haoxin Lin, Yang Yu, Zhi-Hua Zhou</author><pubDate>Fri, 08 Nov 2024 15:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.05619v1</guid></item><item><title>Loop Neural Networks for Parameter Sharing</title><link>http://arxiv.org/abs/2409.14199v3</link><description>The success of large-scale language models like GPT can be attributed totheir ability to efficiently predict the next token in a sequence. However,these models rely on constant computational effort regardless of the complexityof the token they are predicting, lacking the capacity for iterativerefinement. In this paper, we introduce a novel Loop Neural Network, whichachieves better performance by utilizing longer computational time withoutincreasing the model size. Our approach revisits the input multiple times,refining the prediction by iteratively looping over a subset of the model withresidual connections. We demonstrate the effectiveness of this method throughexperiments comparing versions of GPT-2 with our loop models, showing improvedperformance in language modeling tasks while maintaining similar parametercounts. Importantly, these improvements are achieved without the need for extratraining data.</description><author>Kei-Sing Ng, Qingchen Wang</author><pubDate>Fri, 08 Nov 2024 15:00:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.14199v3</guid></item><item><title>Enhancing Vision-Language Few-Shot Adaptation with Negative Learning</title><link>http://arxiv.org/abs/2403.12964v2</link><description>Large-scale pre-trained Vision-Language Models (VLMs) have exhibitedimpressive zero-shot performance and transferability, allowing them to adapt todownstream tasks in a data-efficient manner. However, when only a few labeledsamples are available, adapting VLMs to distinguish subtle differences betweensimilar classes in specific downstream tasks remains challenging. In this work,we propose a Simple yet effective Negative Learning approach, SimNL, to moreefficiently exploit the task-specific knowledge from few-shot labeled samples.Unlike previous methods that focus on identifying a set of representativepositive features defining "what is a {CLASS}", SimNL discovers a complementaryset of negative features that define "what is not a {CLASS}", providingadditional insights that supplement the positive features to enhancetask-specific recognition capability. Further, we identify that currentadaptation approaches are particularly vulnerable to potential noise in thefew-shot sample set. To mitigate this issue, we introduce a plug-and-playfew-shot instance reweighting technique to suppress noisy outliers and amplifyclean samples for more stable adaptation. Our extensive experimental resultsacross 15 datasets validate that the proposed SimNL outperforms existingstate-of-the-art methods on both few-shot learning and domain generalizationtasks while achieving competitive computational efficiency. Code is availableat https://github.com/zhangce01/SimNL.</description><author>Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie</author><pubDate>Fri, 08 Nov 2024 14:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12964v2</guid></item></channel></rss>