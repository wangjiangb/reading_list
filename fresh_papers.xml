<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 11 Dec 2024 13:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Video Motion Transfer with Diffusion Transformers</title><link>http://arxiv.org/abs/2412.07776v1</link><description>We propose DiTFlow, a method for transferring the motion of a reference videoto a newly synthesized one, designed specifically for Diffusion Transformers(DiT). We first process the reference video with a pre-trained DiT to analyzecross-frame attention maps and extract a patch-wise motion signal called theAttention Motion Flow (AMF). We guide the latent denoising process in anoptimization-based, training-free, manner by optimizing latents with our AMFloss to generate videos reproducing the motion of the reference one. We alsoapply our optimization strategy to transformer positional embeddings, grantingus a boost in zero-shot motion transfer capabilities. We evaluate DiTFlowagainst recently published methods, outperforming all across multiple metricsand human evaluation.</description><author>Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, Fabio Pizzati</author><pubDate>Tue, 10 Dec 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07776v1</guid></item><item><title>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets</title><link>http://arxiv.org/abs/2412.07775v1</link><description>While one commonly trains large diffusion models by collecting datasets ontarget downstream tasks, it is often desired to align and finetune pretraineddiffusion models on some reward functions that are either designed by expertsor learned from small-scale datasets. Existing methods for finetuning diffusionmodels typically suffer from lack of diversity in generated samples, lack ofprior preservation, and/or slow convergence in finetuning. Inspired by recentsuccesses in generative flow networks (GFlowNets), a class of probabilisticmodels that sample with the unnormalized density of a reward function, wepropose a novel GFlowNet method dubbed Nabla-GFlowNet (abbreviated as$\nabla$-GFlowNet), the first GFlowNet method that leverages the rich signal inreward gradients, together with an objective called $\nabla$-DB plus itsvariant residual $\nabla$-DB designed for prior-preserving diffusion alignment.We show that our proposed method achieves fast yet diversity- andprior-preserving alignment of Stable Diffusion, a large-scale text-conditionedimage diffusion model, on different realistic reward functions.</description><author>Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang</author><pubDate>Tue, 10 Dec 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07775v1</guid></item><item><title>UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</title><link>http://arxiv.org/abs/2412.07774v1</link><description>We introduce UniReal, a unified framework designed to address various imagegeneration and editing tasks. Existing solutions often vary by tasks, yet sharefundamental principles: preserving consistency between inputs and outputs whilecapturing visual variations. Inspired by recent video generation models thateffectively balance consistency and variation across frames, we propose aunifying approach that treats image-level tasks as discontinuous videogeneration. Specifically, we treat varying numbers of input and output imagesas frames, enabling seamless support for tasks such as image generation,editing, customization, composition, etc. Although designed for image-leveltasks, we leverage videos as a scalable source for universal supervision.UniReal learns world dynamics from large-scale videos, demonstrating advancedcapability in handling shadows, reflections, pose variation, and objectinteraction, while also exhibiting emergent capability for novel applications.</description><author>Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, Hui Ding, Zhe Lin, Hengshuang Zhao</author><pubDate>Tue, 10 Dec 2024 18:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07774v1</guid></item><item><title>From Slow Bidirectional to Fast Causal Video Generators</title><link>http://arxiv.org/abs/2412.07772v1</link><description>Current video diffusion models achieve impressive generation quality butstruggle in interactive applications due to bidirectional attentiondependencies. The generation of a single frame requires the model to processthe entire sequence, including the future. We address this limitation byadapting a pretrained bidirectional diffusion transformer to a causaltransformer that generates frames on-the-fly. To further reduce latency, weextend distribution matching distillation (DMD) to videos, distilling 50-stepdiffusion model into a 4-step generator. To enable stable and high-qualitydistillation, we introduce a student initialization scheme based on teacher'sODE trajectories, as well as an asymmetric distillation strategy thatsupervises a causal student model with a bidirectional teacher. This approacheffectively mitigates error accumulation in autoregressive generation, allowinglong-duration video synthesis despite training on short clips. Our modelsupports fast streaming generation of high quality videos at 9.4 FPS on asingle GPU thanks to KV caching. Our approach also enables streamingvideo-to-video translation, image-to-video, and dynamic prompting in azero-shot manner. We will release the code based on an open-source model in thefuture.</description><author>Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang</author><pubDate>Tue, 10 Dec 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07772v1</guid></item><item><title>Mobile-TeleVision: Predictive Motion Priors for Humanoid Whole-Body Control</title><link>http://arxiv.org/abs/2412.07773v1</link><description>Humanoid robots require both robust lower-body locomotion and preciseupper-body manipulation. While recent Reinforcement Learning (RL) approachesprovide whole-body loco-manipulation policies, they lack precise manipulationwith high DoF arms. In this paper, we propose decoupling upper-body controlfrom locomotion, using inverse kinematics (IK) and motion retargeting forprecise manipulation, while RL focuses on robust lower-body locomotion. Weintroduce PMP (Predictive Motion Priors), trained with Conditional VariationalAutoencoder (CVAE) to effectively represent upper-body motions. The locomotionpolicy is trained conditioned on this upper-body motion representation,ensuring that the system remains robust with both manipulation and locomotion.We show that CVAE features are crucial for stability and robustness, andsignificantly outperforms RL-based whole-body control in precise manipulation.With precise upper-body motion and robust lower-body locomotion control,operators can remotely control the humanoid to walk around and exploredifferent environments, while performing diverse manipulation tasks.</description><author>Chenhao Lu, Xuxin Cheng, Jialong Li, Shiqi Yang, Mazeyu Ji, Chengjing Yuan, Ge Yang, Sha Yi, Xiaolong Wang</author><pubDate>Tue, 10 Dec 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07773v1</guid></item><item><title>PETALface: Parameter Efficient Transfer Learning for Low-resolution Face Recognition</title><link>http://arxiv.org/abs/2412.07771v1</link><description>Pre-training on large-scale datasets and utilizing margin-based lossfunctions have been highly successful in training models for high-resolutionface recognition. However, these models struggle with low-resolution facedatasets, in which the faces lack the facial attributes necessary fordistinguishing different faces. Full fine-tuning on low-resolution datasets, anaive method for adapting the model, yields inferior performance due tocatastrophic forgetting of pre-trained knowledge. Additionally the domaindifference between high-resolution (HR) gallery images and low-resolution (LR)probe images in low resolution datasets leads to poor convergence for a singlemodel to adapt to both gallery and probe after fine-tuning. To this end, wepropose PETALface, a Parameter-Efficient Transfer Learning approach forlow-resolution face recognition. Through PETALface, we attempt to solve boththe aforementioned problems. (1) We solve catastrophic forgetting by leveragingthe power of parameter efficient fine-tuning(PEFT). (2) We introduce twolow-rank adaptation modules to the backbone, with weights adjusted based on theinput image quality to account for the difference in quality for the galleryand probe images. To the best of our knowledge, PETALface is the first workleveraging the powers of PEFT for low resolution face recognition. Extensiveexperiments demonstrate that the proposed method outperforms full fine-tuningon low-resolution datasets while preserving performance on high-resolution andmixed-quality datasets, all while using only 0.48% of the parameters. Code:https://kartik-3004.github.io/PETALface/</description><author>Kartik Narayan, Nithin Gopalakrishnan Nair, Jennifer Xu, Rama Chellappa, Vishal M. Patel</author><pubDate>Tue, 10 Dec 2024 18:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07771v1</guid></item><item><title>From an Image to a Scene: Learning to Imagine the World from a Million 360 Videos</title><link>http://arxiv.org/abs/2412.07770v1</link><description>Three-dimensional (3D) understanding of objects and scenes play a key role inhumans' ability to interact with the world and has been an active area ofresearch in computer vision, graphics, and robotics. Large scale synthetic andobject-centric 3D datasets have shown to be effective in training models thathave 3D understanding of objects. However, applying a similar approach toreal-world objects and scenes is difficult due to a lack of large-scale data.Videos are a potential source for real-world 3D data, but finding diverse yetcorresponding views of the same content has shown to be difficult at scale.Furthermore, standard videos come with fixed viewpoints, determined at the timeof capture. This restricts the ability to access scenes from a variety of morediverse and potentially useful perspectives. We argue that large scale 360videos can address these limitations to provide: scalable corresponding framesfrom diverse views. In this paper, we introduce 360-1M, a 360 video dataset,and a process for efficiently finding corresponding frames from diverseviewpoints at scale. We train our diffusion-based model, Odin, on 360-1M.Empowered by the largest real-world, multi-view dataset to date, Odin is ableto freely generate novel views of real-world scenes. Unlike previous methods,Odin can move the camera through the environment, enabling the model to inferthe geometry and layout of the scene. Additionally, we show improvedperformance on standard novel view synthesis and 3D reconstruction benchmarks.</description><author>Matthew Wallingford, Anand Bhattad, Aditya Kusupati, Vivek Ramanujan, Matt Deitke, Sham Kakade, Aniruddha Kembhavi, Roozbeh Mottaghi, Wei-Chiu Ma, Ali Farhadi</author><pubDate>Tue, 10 Dec 2024 18:59:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07770v1</guid></item><item><title>BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities</title><link>http://arxiv.org/abs/2412.07769v1</link><description>This paper introduces BiMediX2, a bilingual (Arabic-English) Bio-MedicalEXpert Large Multimodal Model (LMM) with a unified architecture that integratestext and visual modalities, enabling advanced image understanding and medicalapplications. BiMediX2 leverages the Llama3.1 architecture and integrates textand visual capabilities to facilitate seamless interactions in both English andArabic, supporting text-based inputs and multi-turn conversations involvingmedical images. The model is trained on an extensive bilingual healthcaredataset consisting of 1.6M samples of diverse medical interactions for bothtext and image modalities, mixed in Arabic and English. We also propose thefirst bilingual GPT-4o based medical LMM benchmark named BiMed-MBench. BiMediX2is benchmarked on both text-based and image-based tasks, achievingstate-of-the-art performance across several medical benchmarks. It outperformsrecent state-of-the-art models in medical LLM evaluation benchmarks. Our modelalso sets a new benchmark in multimodal medical evaluations with over 9%improvement in English and over 20% in Arabic evaluations. Additionally, itsurpasses GPT-4 by around 9% in UPHILL factual accuracy evaluations and excelsin various medical Visual Question Answering, Report Generation, and ReportSummarization tasks. The project page including source code and the trainedmodel, is available at https://github.com/mbzuai-oryx/BiMediX2.</description><author>Sahal Shaji Mullappilly, Mohammed Irfan Kurpath, Sara Pieri, Saeed Yahya Alseiari, Shanavas Cholakkal, Khaled Aldahmani, Fahad Khan, Rao Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal</author><pubDate>Tue, 10 Dec 2024 18:59:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07769v1</guid></item><item><title>Test-time Correction with Human Feedback: An Online 3D Detection System via Visual Prompting</title><link>http://arxiv.org/abs/2412.07768v1</link><description>This paper introduces Test-time Correction (TTC) system, a novel online 3Ddetection system designated for online correction of test-time errors via humanfeedback, to guarantee the safety of deployed autonomous driving systems.Unlike well-studied offline 3D detectors frozen at inference, TTC explores thecapability of instant online error rectification. By leveraging user feedbackwith interactive prompts at a frame, e.g., a simple click or draw of boxes, TTCcould immediately update the corresponding detection results for futurestreaming inputs, even though the model is deployed with fixed parameters. Thisenables autonomous driving systems to adapt to new scenarios immediately anddecrease deployment risks reliably without additional expensive training. Toachieve such TTC system, we equip existing 3D detectors with Online Adapter(OA) module, a prompt-driven query generator for online correction. At the coreof OA module are visual prompts, images of missed object-of-interest forguiding the corresponding detection and subsequent tracking. Those visualprompts, belonging to missed objects through online inference, are maintainedby the visual prompt buffer for continuous error correction in subsequentframes. By doing so, TTC consistently detects online missed objects andimmediately lowers driving risks. It achieves reliable, versatile, and adaptivedriving autonomy. Extensive experiments demonstrate significant gain on instanterror rectification over pre-trained 3D detectors, even in challengingscenarios with limited labels, zero-shot detection, and adverse conditions. Wehope this work would inspire the community to investigate online rectificationsystems for autonomous driving post-deployment. Code would be publicly shared.</description><author>Zetong Yang, Hanxue Zhang, Yanan Sun, Li Chen, Fei Xia, Fatma Guney, Hongyang Li</author><pubDate>Tue, 10 Dec 2024 18:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07768v1</guid></item><item><title>Learning Visual Generative Priors without Text</title><link>http://arxiv.org/abs/2412.07767v1</link><description>Although text-to-image (T2I) models have recently thrived as visualgenerative priors, their reliance on high-quality text-image pairs makesscaling up expensive. We argue that grasping the cross-modality alignment isnot a necessity for a sound visual generative prior, whose focus should be ontexture modeling. Such a philosophy inspires us to study image-to-image (I2I)generation, where models can learn from in-the-wild images in a self-supervisedmanner. We first develop a pure vision-based training framework, Lumos, andconfirm the feasibility and the scalability of learning I2I models. We thenfind that, as an upstream task of T2I, our I2I model serves as a morefoundational visual prior and achieves on-par or better performance thanexisting T2I models using only 1/10 text-image pairs for fine-tuning. Wefurther demonstrate the superiority of I2I priors over T2I priors on sometext-irrelevant visual generative tasks, like image-to-3D and image-to-video.</description><author>Shuailei Ma, Kecheng Zheng, Ying Wei, Wei Wu, Fan Lu, Yifei Zhang, Chen-wei Xie, Jiapeng Zhu, Yujun Shen</author><pubDate>Tue, 10 Dec 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07767v1</guid></item><item><title>Make-A-Texture: Fast Shape-Aware Texture Generation in 3 Seconds</title><link>http://arxiv.org/abs/2412.07766v1</link><description>We present Make-A-Texture, a new framework that efficiently synthesizeshigh-resolution texture maps from textual prompts for given 3D geometries. Ourapproach progressively generates textures that are consistent across multipleviewpoints with a depth-aware inpainting diffusion model, in an optimizedsequence of viewpoints determined by an automatic view selection algorithm. A significant feature of our method is its remarkable efficiency, achieving afull texture generation within an end-to-end runtime of just 3.07 seconds on asingle NVIDIA H100 GPU, significantly outperforming existing methods. Such anacceleration is achieved by optimizations in the diffusion model and aspecialized backprojection method. Moreover, our method reduces the artifactsin the backprojection phase, by selectively masking out non-frontal faces, andinternal faces of open-surfaced objects. Experimental results demonstrate that Make-A-Texture matches or exceeds thequality of other state-of-the-art methods. Our work significantly improves theapplicability and practicality of texture generation models for real-world 3Dcontent creation, including interactive creation and text-guided textureediting.</description><author>Xiaoyu Xiang, Liat Sless Gorelik, Yuchen Fan, Omri Armstrong, Forrest Iandola, Yilei Li, Ita Lifshitz, Rakesh Ranjan</author><pubDate>Tue, 10 Dec 2024 18:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07766v1</guid></item><item><title>Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences</title><link>http://arxiv.org/abs/2412.07763v1</link><description>To build effective therapeutics, biologists iteratively mutate antibodysequences to improve binding and stability. Proposed mutations can be informedby previous measurements or by learning from large antibody databases topredict only typical antibodies. Unfortunately, the space of typical antibodiesis enormous to search, and experiments often fail to find suitable antibodieson a budget. We introduce Clone-informed Bayesian Optimization (CloneBO), aBayesian optimization procedure that efficiently optimizes antibodies in thelab by teaching a generative model how our immune system optimizes antibodies.Our immune system makes antibodies by iteratively evolving specific portions oftheir sequences to bind their target strongly and stably, resulting in a set ofrelated, evolving sequences known as a clonal family. We train a large languagemodel, CloneLM, on hundreds of thousands of clonal families and use it todesign sequences with mutations that are most likely to optimize an antibodywithin the human immune system. We propose to guide our designs to fit previousmeasurements with a twisted sequential Monte Carlo procedure. We show thatCloneBO optimizes antibodies substantially more efficiently than previousmethods in realistic in silico experiments and designs stronger and more stablebinders in in vitro wet lab experiments.</description><author>Alan Nawzad Amin, Nate Gruver, Yilun Kuang, Lily Li, Hunter Elliott, Calvin McCarter, Aniruddh Raghu, Peyton Greenside, Andrew Gordon Wilson</author><pubDate>Tue, 10 Dec 2024 18:57:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07763v1</guid></item><item><title>Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data</title><link>http://arxiv.org/abs/2412.07762v1</link><description>The modern paradigm in machine learning involves pre-training on diversedata, followed by task-specific fine-tuning. In reinforcement learning (RL),this translates to learning via offline RL on a diverse historical dataset,followed by rapid online RL fine-tuning using interaction data. Most RLfine-tuning methods require continued training on offline data for stabilityand performance. However, this is undesirable because training on diverseoffline data is slow and expensive for large datasets, and in principle, alsolimit the performance improvement possible because of constraints or pessimismon offline data. In this paper, we show that retaining offline data isunnecessary as long as we use a properly-designed online RL approach forfine-tuning offline RL initializations. To build this approach, we start byanalyzing the role of retaining offline data in online fine-tuning. We findthat continued training on offline data is mostly useful for preventing asudden divergence in the value function at the onset of fine-tuning, caused bya distribution mismatch between the offline data and online rollouts. Thisdivergence typically results in unlearning and forgetting the benefits ofoffline pre-training. Our approach, Warm-start RL (WSRL), mitigates thecatastrophic forgetting of pre-trained initializations using a very simpleidea. WSRL employs a warmup phase that seeds the online RL run with a verysmall number of rollouts from the pre-trained policy to do fast online RL. Thedata collected during warmup helps ``recalibrate'' the offline Q-function tothe online distribution, allowing us to completely discard offline data withoutdestabilizing the online RL fine-tuning. We show that WSRL is able to fine-tunewithout retaining any offline data, and is able to learn faster and attainshigher performance than existing algorithms irrespective of whether they retainoffline data or not.</description><author>Zhiyuan Zhou, Andy Peng, Qiyang Li, Sergey Levine, Aviral Kumar</author><pubDate>Tue, 10 Dec 2024 18:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07762v1</guid></item><item><title>Repurposing Pre-trained Video Diffusion Models for Event-based Video Interpolation</title><link>http://arxiv.org/abs/2412.07761v1</link><description>Video Frame Interpolation aims to recover realistic missing frames betweenobserved frames, generating a high-frame-rate video from a low-frame-ratevideo. However, without additional guidance, the large motion between framesmakes this problem ill-posed. Event-based Video Frame Interpolation (EVFI)addresses this challenge by using sparse, high-temporal-resolution eventmeasurements as motion guidance. This guidance allows EVFI methods tosignificantly outperform frame-only methods. However, to date, EVFI methodshave relied on a limited set of paired event-frame training data, severelylimiting their performance and generalization capabilities. In this work, weovercome the limited data challenge by adapting pre-trained video diffusionmodels trained on internet-scale datasets to EVFI. We experimentally validateour approach on real-world EVFI datasets, including a new one that weintroduce. Our method outperforms existing methods and generalizes acrosscameras far better than existing approaches.</description><author>Jingxi Chen, Brandon Y. Feng, Haoming Cai, Tianfu Wang, Levi Burner, Dehao Yuan, Cornelia Fermuller, Christopher A. Metzler, Yiannis Aloimonos</author><pubDate>Tue, 10 Dec 2024 18:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07761v1</guid></item><item><title>SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints</title><link>http://arxiv.org/abs/2412.07760v1</link><description>Recent advancements in video diffusion models have shown exceptionalabilities in simulating real-world dynamics and maintaining 3D consistency.This progress inspires us to investigate the potential of these models toensure dynamic consistency across various viewpoints, a highly desirablefeature for applications such as virtual filming. Unlike existing methodsfocused on multi-view generation of single objects for 4D reconstruction, ourinterest lies in generating open-world videos from arbitrary viewpoints,incorporating 6 DoF camera poses. To achieve this, we propose a plug-and-playmodule that enhances a pre-trained text-to-video model for multi-camera videogeneration, ensuring consistent content across different viewpoints.Specifically, we introduce a multi-view synchronization module to maintainappearance and geometry consistency across these viewpoints. Given the scarcityof high-quality training data, we design a hybrid training scheme thatleverages multi-camera images and monocular videos to supplement UnrealEngine-rendered multi-camera videos. Furthermore, our method enables intriguingextensions, such as re-rendering a video from novel viewpoints. We also releasea multi-view synchronized video dataset, named SynCamVideo-Dataset. Projectpage: https://jianhongbai.github.io/SynCamMaster/.</description><author>Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, Di Zhang</author><pubDate>Tue, 10 Dec 2024 18:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07760v1</guid></item><item><title>3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation</title><link>http://arxiv.org/abs/2412.07759v1</link><description>This paper aims to manipulate multi-entity 3D motions in video generation.Previous methods on controllable video generation primarily leverage 2D controlsignals to manipulate object motions and have achieved remarkable synthesisresults. However, 2D control signals are inherently limited in expressing the3D nature of object motions. To overcome this problem, we introduce3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3Dspace, given user-desired 6DoF pose (location and rotation) sequences ofentities. At the core of our approach is a plug-and-play 3D-motion groundedobject injector that fuses multiple input entities with their respective 3Dtrajectories through a gated self-attention mechanism. In addition, we exploitan injector architecture to preserve the video diffusion prior, which iscrucial for generalization ability. To mitigate video quality degradation, weintroduce a domain adaptor during training and employ an annealed samplingstrategy during inference. To address the lack of suitable training data, weconstruct a 360-Motion Dataset, which first correlates collected 3D human andanimal assets with GPT-generated trajectory and then captures their motion with12 evenly-surround cameras on diverse 3D UE platforms. Extensive experimentsshow that 3DTrajMaster sets a new state-of-the-art in both accuracy andgeneralization for controlling multi-entity 3D motions. Project page:http://fuxiao0719.github.io/projects/3dtrajmaster</description><author>Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, Dahua Lin</author><pubDate>Tue, 10 Dec 2024 18:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07759v1</guid></item><item><title>XRZoo: A Large-Scale and Versatile Dataset of Extended Reality (XR) Applications</title><link>http://arxiv.org/abs/2412.06759v2</link><description>The rapid advancement of Extended Reality (XR, encompassing AR, MR, and VR)and spatial computing technologies forms a foundational layer for the emergingMetaverse, enabling innovative applications across healthcare, education,manufacturing, and entertainment. However, research in this area is oftenlimited by the lack of large, representative, and highquality applicationdatasets that can support empirical studies and the development of newapproaches benefiting XR software processes. In this paper, we introduce XRZoo,a comprehensive and curated dataset of XR applications designed to bridge thisgap. XRZoo contains 12,528 free XR applications, spanning nine app stores,across all XR techniques (i.e., AR, MR, and VR) and use cases, with detailedmetadata on key aspects such as application descriptions, applicationcategories, release dates, user review numbers, and hardware specifications,etc. By making XRZoo publicly available, we aim to foster reproducible XRsoftware engineering and security research, enable cross-disciplinaryinvestigations, and also support the development of advanced XR systems byproviding examples to developers. Our dataset serves as a valuable resource forresearchers and practitioners interested in improving the scalability,usability, and effectiveness of XR applications. XRZoo will be released andactively maintained.</description><author>Shuqing Li, Chenran Zhang, Cuiyun Gao, Michael R. Lyu</author><pubDate>Tue, 10 Dec 2024 18:54:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06759v2</guid></item><item><title>SAT: Spatial Aptitude Training for Multimodal Language Models</title><link>http://arxiv.org/abs/2412.07755v1</link><description>Spatial perception is a fundamental component of intelligence. While manystudies highlight that large multimodal language models (MLMs) struggle toreason about space, they only test for static spatial reasoning, such ascategorizing the relative positions of objects. Meanwhile, real-worlddeployment requires dynamic capabilities like perspective-taking and egocentricaction recognition. As a roadmap to improving spatial intelligence, weintroduce SAT, Spatial Aptitude Training, which goes beyond static relativeobject position questions to the more dynamic tasks. SAT contains 218Kquestion-answer pairs for 22K synthetic scenes across a training and testingset. Generated using a photo-realistic physics engine, our dataset can bearbitrarily scaled and easily extended to new actions, scenes, and 3D assets.We find that even MLMs that perform relatively well on static questionsstruggle to accurately answer dynamic spatial questions. Further, we show thatSAT instruction-tuning data improves not only dynamic spatial reasoning on SAT,but also zero-shot performance on existing real-image spatial benchmarks:$23\%$ on CVBench, $8\%$ on the harder BLINK benchmark, and $18\%$ on VSR. Wheninstruction-tuned on SAT, our 13B model matches larger proprietary MLMs likeGPT4-V and Gemini-3-1.0 in spatial reasoning. Our data/code is available athttp://arijitray1993.github.io/SAT/ .</description><author>Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, Kate Saenko</author><pubDate>Tue, 10 Dec 2024 18:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07755v1</guid></item><item><title>PortraitTalk: Towards Customizable One-Shot Audio-to-Talking Face Generation</title><link>http://arxiv.org/abs/2412.07754v1</link><description>Audio-driven talking face generation is a challenging task in digitalcommunication. Despite significant progress in the area, most existing methodsconcentrate on audio-lip synchronization, often overlooking aspects such asvisual quality, customization, and generalization that are crucial to producingrealistic talking faces. To address these limitations, we introduce a novel,customizable one-shot audio-driven talking face generation framework, namedPortraitTalk. Our proposed method utilizes a latent diffusion frameworkconsisting of two main components: IdentityNet and AnimateNet. IdentityNet isdesigned to preserve identity features consistently across the generated videoframes, while AnimateNet aims to enhance temporal coherence and motionconsistency. This framework also integrates an audio input with the referenceimages, thereby reducing the reliance on reference-style videos prevalent inexisting approaches. A key innovation of PortraitTalk is the incorporation oftext prompts through decoupled cross-attention mechanisms, which significantlyexpands creative control over the generated videos. Through extensiveexperiments, including a newly developed evaluation metric, our modeldemonstrates superior performance over the state-of-the-art methods, setting anew standard for the generation of customizable realistic talking facessuitable for real-world applications.</description><author>Fatemeh Nazarieh, Zhenhua Feng, Diptesh Kanojia, Muhammad Awais, Josef Kittler</author><pubDate>Tue, 10 Dec 2024 18:51:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07754v1</guid></item><item><title>Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites</title><link>http://arxiv.org/abs/2410.19643v3</link><description>Machine learning (ML) models benefit from large datasets. Collecting data inbiomedical domains is costly and challenging, hence, combining datasets hasbecome a common practice. However, datasets obtained under different conditionscould present undesired site-specific variability. Data harmonization methodsaim to remove site-specific variance while retaining biologically relevantinformation. This study evaluates the effectiveness of popularly usedComBat-based methods for harmonizing data in scenarios where the class balanceis not equal across sites. We find that these methods struggle with dataleakage issues. To overcome this problem, we propose a novel approachPrettYharmonize, designed to harmonize data by pretending the target labels. Wevalidate our approach using controlled datasets designed to benchmark theutility of harmonization. Finally, using real-world MRI and clinical data, wecompare leakage-prone methods with PrettYharmonize and show that it achievescomparable performance while avoiding data leakage, particularly insite-target-dependence scenarios.</description><author>Nicolás Nieto, Simon B. Eickhoff, Christian Jung, Martin Reuter, Kersten Diers, Malte Kelm, Artur Lichtenberg, Federico Raimondo, Kaustubh R. Patil</author><pubDate>Tue, 10 Dec 2024 18:50:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19643v3</guid></item><item><title>FlashRNN: Optimizing Traditional RNNs on Modern Hardware</title><link>http://arxiv.org/abs/2412.07752v1</link><description>While Transformers and other sequence-parallelizable neural networkarchitectures seem like the current state of the art in sequence modeling, theyspecifically lack state-tracking capabilities. These are important fortime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,as well as modern variants like sLSTM do have these capabilities at the cost ofstrictly sequential processing. While this is often seen as a stronglimitation, we show how fast these networks can get with ourhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to theregister level on modern GPUs. We extend traditional RNNs with aparallelization variant that processes multiple RNNs of smaller hidden state inparallel, similar to the head-wise processing in Transformers. To enableflexibility on different GPU variants, we introduce a new optimizationframework for hardware-internal cache sizes, memory and compute handling. Itmodels the hardware in a setting using polyhedral-like constraints, includingthe notion of divisibility. This speeds up the solution process in ourConstrINT library for general integer constraint satisfaction problems (integerCSPs). We show that our kernels can achieve 50x speed-ups over a vanillaPyTorch implementation and allow 40x larger hidden sizes compared to our Tritonimplementation. Our open-source kernels and the optimization library arereleased here to boost research in the direction of state-tracking enabled RNNsand sequence modeling: \url{https://github.com/NX-AI/flashrnn}</description><author>Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter</author><pubDate>Tue, 10 Dec 2024 18:50:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07752v1</guid></item><item><title>On Motion Blur and Deblurring in Visual Place Recognition</title><link>http://arxiv.org/abs/2412.07751v1</link><description>Visual Place Recognition (VPR) in mobile robotics enables robots to localizethemselves by recognizing previously visited locations using visual data. Whilethe reliability of VPR methods has been extensively studied under conditionssuch as changes in illumination, season, weather and viewpoint, the impact ofmotion blur is relatively unexplored despite its relevance not only in rapidmotion scenarios but also in low-light conditions where longer exposure timesare necessary. Similarly, the role of image deblurring in enhancing VPRperformance under motion blur has received limited attention so far. This paperbridges these gaps by introducing a new benchmark designed to evaluate VPRperformance under the influence of motion blur and image deblurring. Thebenchmark includes three datasets that encompass a wide range of motion blurintensities, providing a comprehensive platform for analysis. Experimentalresults with several well-established VPR and image deblurring methods providenew insights into the effects of motion blur and the potential improvementsachieved through deblurring. Building on these findings, the paper proposesadaptive deblurring strategies for VPR, designed to effectively manage motionblur in dynamic, real-world scenarios.</description><author>Timur Ismagilov, Bruno Ferrarini, Michael Milford, Tan Viet Tuyen Nguyen, SD Ramchurn, Shoaib Ehsan</author><pubDate>Tue, 10 Dec 2024 18:49:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07751v1</guid></item><item><title>Multi-Shot Character Consistency for Text-to-Video Generation</title><link>http://arxiv.org/abs/2412.07750v1</link><description>Text-to-video models have made significant strides in generating short videoclips from textual descriptions. Yet, a significant challenge remains:generating several video shots of the same characters, preserving theiridentity without hurting video quality, dynamics, and responsiveness to textprompts. We present Video Storyboarding, a training-free method to enablepretrained text-to-video models to generate multiple shots with consistentcharacters, by sharing features between them. Our key insight is thatself-attention query features (Q) encode both motion and identity. This createsa hard-to-avoid trade-off between preserving character identity and makingvideos dynamic, when features are shared. To address this issue, we introduce anovel query injection strategy that balances identity preservation and naturalmotion retention. This approach improves upon naive consistency techniquesapplied to videos, which often struggle to maintain this delicate equilibrium.Our experiments demonstrate significant improvements in character consistencyacross scenes while maintaining high-quality motion and text alignment. Theseresults offer insights into critical stages of video generation and theinterplay of structure and motion in video diffusion models.</description><author>Yuval Atzmon, Rinon Gal, Yoad Tewel, Yoni Kasten, Gal Chechik</author><pubDate>Tue, 10 Dec 2024 18:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07750v1</guid></item><item><title>Predictive Modeling of Homeless Service Assignment: A Representation Learning Approach</title><link>http://arxiv.org/abs/2412.07747v1</link><description>In recent years, there has been growing interest in leveraging machinelearning for homeless service assignment. However, the categorical nature ofadministrative data recorded for homeless individuals hinders the developmentof accurate machine learning methods for this task. This work asserts thatderiving latent representations of such features, while at the same timeleveraging underlying relationships between instances is crucial inalgorithmically enhancing the existing assignment decision-making process. Ourproposed approach learns temporal and functional relationships between servicesfrom historical data, as well as unobserved but relevant relationships betweenindividuals to generate features that significantly improve the prediction ofthe next service assignment compared to the state-of-the-art.</description><author>Khandker Sadia Rahman, Charalampos Chelmis</author><pubDate>Tue, 10 Dec 2024 18:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07747v1</guid></item><item><title>Right on Time: Revising Time Series Models by Constraining their Explanations</title><link>http://arxiv.org/abs/2402.12921v4</link><description>The reliability of deep time series models is often compromised by theirtendency to rely on confounding factors, which may lead to incorrect outputs.Our newly recorded, naturally confounded dataset named P2S from a realmechanical production line emphasizes this. To avoid "Clever-Hans" moments intime series, i.e., to mitigate confounders, we introduce the method Right onTime (RioT). RioT enables, for the first time, interactions with modelexplanations across both the time and frequency domain. Feedback onexplanations in both domains is then used to constrain the model, steering itaway from the annotated confounding factors. The dual-domain interactionstrategy is crucial for effectively addressing confounders in time seriesdatasets. We empirically demonstrate that RioT can effectively guide modelsaway from the wrong reasons in P2S as well as popular time seriesclassification and forecasting datasets.</description><author>Maurice Kraus, David Steinmann, Antonia Wüst, Andre Kokozinski, Kristian Kersting</author><pubDate>Tue, 10 Dec 2024 18:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12921v4</guid></item><item><title>Beyond Retrieval: Generating Narratives in Conversational Recommender Systems</title><link>http://arxiv.org/abs/2410.16780v2</link><description>The recent advances in Large Language Model's generation and reasoningcapabilities present an opportunity to develop truly conversationalrecommendation systems. However, effectively integrating recommender systemknowledge into LLMs for natural language generation which is tailored towardsrecommendation tasks remains a challenge. This paper addresses this challengeby making two key contributions. First, we introduce a new dataset (REGEN) for natural language generationtasks in conversational recommendations. REGEN (Reviews Enhanced withGEnerative Narratives) extends the Amazon Product Reviews dataset with richuser narratives, including personalized explanations of product preferences,product endorsements for recommended items, and summaries of user purchasehistory. REGEN is made publicly available to facilitate further research.Furthermore, we establish benchmarks using well-known generative metrics, andperform an automated evaluation of the new dataset using a rater LLM. Second,the paper introduces a fusion architecture (CF model with an LLM) which servesas a baseline for REGEN. And to the best of our knowledge, represents the firstattempt to analyze the capabilities of LLMs in understanding recommendersignals and generating rich narratives. We demonstrate that LLMs caneffectively learn from simple fusion architectures utilizing interaction-basedCF embeddings, and this can be further enhanced using the metadata andpersonalization data associated with items. Our experiments show that combiningCF and content embeddings leads to improvements of 4-12% in key languagemetrics compared to using either type of embedding individually. We alsoprovide an analysis to interpret how CF and content embeddings contribute tothis new generative task.</description><author>Krishna Sayana, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert, James Pine, Hubert Pham, Ambarish Jash, Sukhdeep Sodhi</author><pubDate>Tue, 10 Dec 2024 18:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16780v2</guid></item><item><title>LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation Models</title><link>http://arxiv.org/abs/2412.07746v1</link><description>Emerging 3D geometric foundation models, such as DUSt3R, offer a promisingapproach for in-the-wild 3D vision tasks. However, due to the high-dimensionalnature of the problem space and scarcity of high-quality 3D data, thesepre-trained models still struggle to generalize to many challengingcircumstances, such as limited view overlap or low lighting. To address this,we propose LoRA3D, an efficient self-calibration pipeline to$\textit{specialize}$ the pre-trained models to target scenes using their ownmulti-view predictions. Taking sparse RGB images as input, we leverage robustoptimization techniques to refine multi-view predictions and align them into aglobal coordinate frame. In particular, we incorporate prediction confidenceinto the geometric optimization process, automatically re-weighting theconfidence to better reflect point estimation accuracy. We use the calibratedconfidence to generate high-quality pseudo labels for the calibrating views anduse low-rank adaptation (LoRA) to fine-tune the models on the pseudo-labeleddata. Our method does not require any external priors or manual labels. Itcompletes the self-calibration process on a $\textbf{single standard GPU withinjust 5 minutes}$. Each low-rank adapter requires only $\textbf{18MB}$ ofstorage. We evaluated our method on $\textbf{more than 160 scenes}$ from theReplica, TUM and Waymo Open datasets, achieving up to $\textbf{88% performanceimprovement}$ on 3D reconstruction, multi-view pose estimation and novel-viewrendering.</description><author>Ziqi Lu, Heng Yang, Danfei Xu, Boyi Li, Boris Ivanovic, Marco Pavone, Yue Wang</author><pubDate>Tue, 10 Dec 2024 18:45:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07746v1</guid></item><item><title>StyleMaster: Stylize Your Video with Artistic Generation and Translation</title><link>http://arxiv.org/abs/2412.07744v1</link><description>Style control has been popular in video generation models. Existing methodsoften generate videos far from the given style, cause content leakage, andstruggle to transfer one video to the desired style. Our first observation isthat the style extraction stage matters, whereas existing methods emphasizeglobal style but ignore local textures. In order to bring texture featureswhile preventing content leakage, we filter content-related patches whileretaining style ones based on prompt-patch similarity; for global styleextraction, we generate a paired style dataset through model illusion tofacilitate contrastive learning, which greatly enhances the absolute styleconsistency. Moreover, to fill in the image-to-video gap, we train alightweight motion adapter on still videos, which implicitly enhancesstylization extent, and enables our image-trained model to be seamlesslyapplied to videos. Benefited from these efforts, our approach, StyleMaster, notonly achieves significant improvement in both style resemblance and temporalcoherence, but also can easily generalize to video style transfer with a graytile ControlNet. Extensive experiments and visualizations demonstrate thatStyleMaster significantly outperforms competitors, effectively generatinghigh-quality stylized videos that align with textual content and closelyresemble the style of reference images. Our project page is athttps://zixuan-ye.github.io/stylemaster</description><author>Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, Wenhan Luo</author><pubDate>Tue, 10 Dec 2024 18:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07744v1</guid></item><item><title>Zero-Shot ATC Coding with Large Language Models for Clinical Assessments</title><link>http://arxiv.org/abs/2412.07743v1</link><description>Manual assignment of Anatomical Therapeutic Chemical (ATC) codes toprescription records is a significant bottleneck in healthcare research andoperations at Ontario Health and InterRAI Canada, requiring extensive experttime and effort. To automate this process while maintaining data privacy, wedevelop a practical approach using locally deployable large language models(LLMs). Inspired by recent advances in automatic International Classificationof Diseases (ICD) coding, our method frames ATC coding as a hierarchicalinformation extraction task, guiding LLMs through the ATC ontology level bylevel. We evaluate our approach using GPT-4o as an accuracy ceiling and focusdevelopment on open-source Llama models suitable for privacy-sensitivedeployment. Testing across Health Canada drug product data, the RABBITSbenchmark, and real clinical notes from Ontario Health, our method achieves 78%exact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigateknowledge grounding through drug definitions, finding modest improvements inaccuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama3.1 70B accuracy, suggesting that effective ATC coding is feasible with smallermodels. Our results demonstrate the feasibility of automatic ATC coding inprivacy-sensitive healthcare environments, providing a foundation for futuredeployments.</description><author>Zijian Chen, John-Michael Gamble, Micaela Jantzi, John P. Hirdes, Jimmy Lin</author><pubDate>Tue, 10 Dec 2024 18:43:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07743v1</guid></item><item><title>M3TR: Generalist HD Map Construction with Variable Map Priors</title><link>http://arxiv.org/abs/2411.10316v2</link><description>Autonomous vehicles require road information for their operation, usually inform of HD maps. Since offline maps eventually become outdated or may only bepartially available, online HD map construction methods have been proposed toinfer map information from live sensor data. A key issue remains how to exploitsuch partial or outdated map information as a prior. We introduce M3TR(Multi-Masking Map Transformer), a generalist approach for HD map constructionboth with and without map priors. We address shortcomings in ground truthgeneration for Argoverse 2 and nuScenes and propose the first realisticscenarios with semantically diverse map priors. Examining various querydesigns, we use an improved method for integrating prior map elements into a HDmap construction model, increasing performance by +4.3 mAP. Finally, we showthat training across all prior scenarios yields a single Generalist model,whose performance is on par with previous Expert models that can handle onlyone specific type of map prior. M3TR thus is the first model capable ofleveraging variable map priors, making it suitable for real-world deployment.Code is available at https://github.com/immel-f/m3tr</description><author>Fabian Immel, Richard Fehler, Frank Bieder, Jan-Hendrik Pauls, Christoph Stiller</author><pubDate>Tue, 10 Dec 2024 18:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.10316v2</guid></item><item><title>Image Retrieval with Intra-Sweep Representation Learning for Neck Ultrasound Scanning Guidance</title><link>http://arxiv.org/abs/2412.07741v1</link><description>Purpose: Intraoperative ultrasound (US) can enhance real-time visualizationin transoral robotic surgery. The surgeon creates a mental map with apre-operative scan. Then, a surgical assistant performs freehand US scanningduring the surgery while the surgeon operates at the remote surgical console.Communicating the target scanning plane in the surgeon's mental map isdifficult. Automatic image retrieval can help match intraoperative images topreoperative scans, guiding the assistant to adjust the US probe toward thetarget plane. Methods: We propose a self-supervised contrastive learningapproach to match intraoperative US views to a preoperative image database. Weintroduce a novel contrastive learning strategy that leverages intra-sweepsimilarity and US probe location to improve feature encoding. Additionally, ourmodel incorporates a flexible threshold to reject unsatisfactory matches.Results: Our method achieves 92.30% retrieval accuracy on simulated data andoutperforms state-of-the-art temporal-based contrastive learning approaches.Our ablation study demonstrates that using probe location in the optimizationgoal improves image representation, suggesting that semantic information can beextracted from probe location. We also present our approach on real patientdata to show the feasibility of the proposed US probe localization systemdespite tissue deformation from tongue retraction. Conclusion: Our contrastivelearning method, which utilizes intra-sweep similarity and US probe location,enhances US image representation learning. We also demonstrate the feasibilityof using our image retrieval method to provide neck US localization on realpatient US after tongue retraction.</description><author>Wanwen Chen, Adam Schmidt, Eitan Prisman, Septimiu E. Salcudean</author><pubDate>Tue, 10 Dec 2024 18:39:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07741v1</guid></item><item><title>GASP: Gaussian Avatars with Synthetic Priors</title><link>http://arxiv.org/abs/2412.07739v1</link><description>Gaussian Splatting has changed the game for real-time photo-realisticrendering. One of the most popular applications of Gaussian Splatting is tocreate animatable avatars, known as Gaussian Avatars. Recent works have pushedthe boundaries of quality and rendering efficiency but suffer from two mainlimitations. Either they require expensive multi-camera rigs to produce avatarswith free-view rendering, or they can be trained with a single camera but onlyrendered at high quality from this fixed viewpoint. An ideal model would betrained using a short monocular video or image from available hardware, such asa webcam, and rendered from any view. To this end, we propose GASP: GaussianAvatars with Synthetic Priors. To overcome the limitations of existingdatasets, we exploit the pixel-perfect nature of synthetic data to train aGaussian Avatar prior. By fitting this prior model to a single photo or videoand fine-tuning it, we get a high-quality Gaussian Avatar, which supports360$^\circ$ rendering. Our prior is only required for fitting, not inference,enabling real-time application. Through our method, we obtain high-quality,animatable Avatars from limited data which can be animated and rendered at70fps on commercial hardware. See our project page(https://microsoft.github.io/GASP/) for results.</description><author>Jack Saunders, Charlie Hewitt, Yanan Jian, Marek Kowalski, Tadas Baltrusaitis, Yiye Chen, Darren Cosker, Virginia Estellers, Nicholas Gyde, Vinay P. Namboodiri, Benjamin E Lundell</author><pubDate>Tue, 10 Dec 2024 18:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07739v1</guid></item><item><title>A tutorial on automatic differentiation with complex numbers</title><link>http://arxiv.org/abs/2409.06752v3</link><description>Automatic differentiation is everywhere, but there exists only minimaldocumentation of how it works in complex arithmetic beyond stating "derivativesin $\mathbb{C}^d$" $\cong$ "derivatives in $\mathbb{R}^{2d}$" and, at best,shallow references to Wirtinger calculus. Unfortunately, the equivalence$\mathbb{C}^d \cong \mathbb{R}^{2d}$ becomes insufficient as soon as we need toderive custom gradient rules, e.g., to avoid differentiating "through"expensive linear algebra functions or differential equation simulators. Tocombat such a lack of documentation, this article surveys forward- andreverse-mode automatic differentiation with complex numbers, covering topicssuch as Wirtinger derivatives, a modified chain rule, and different gradientconventions while explicitly avoiding holomorphicity and the Cauchy--Riemannequations (which would be far too restrictive). To be precise, we will derive,explain, and implement a complex version of Jacobian-vector and vector-Jacobianproducts almost entirely with linear algebra without relying on complexanalysis or differential geometry. This tutorial is a call to action, for usersand developers alike, to take complex values seriously when implementing customgradient propagation rules -- the manuscript explains how.</description><author>Nicholas Krämer</author><pubDate>Tue, 10 Dec 2024 18:34:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.06752v3</guid></item><item><title>Explainable machine learning for neoplasms diagnosis via electrocardiograms: an externally validated study</title><link>http://arxiv.org/abs/2412.07737v1</link><description>Background: Neoplasms remains a leading cause of mortality worldwide, withtimely diagnosis being crucial for improving patient outcomes. Currentdiagnostic methods are often invasive, costly, and inaccessible to manypopulations. Electrocardiogram (ECG) data, widely available and non-invasive,has the potential to serve as a tool for neoplasms diagnosis by usingphysiological changes in cardiovascular function associated with neoplasticprescences. Methods: This study explores the application of machine learning models toanalyze ECG features for the diagnosis of neoplasms. We developed a pipelineintegrating tree-based models with Shapley values for explainability. The modelwas trained and internally validated and externally validated on a secondlarge-scale independent external cohort to ensure robustness andgeneralizability. Findings: The results demonstrate that ECG data can effectively captureneoplasms-associated cardiovascular changes, achieving high performance in bothinternal testing and external validation cohorts. Shapley values identified keyECG features influencing model predictions, revealing established and novelcardiovascular markers linked to neoplastic conditions. This non-invasiveapproach provides a cost-effective and scalable alternative for the diagnosisof neoplasms, particularly in resource-limited settings. Similarly, useful forthe management of secondary cardiovascular effects given neoplasms therapies. Interpretation: This study highlights the feasibility of leveraging ECGsignals and machine learning to enhance neoplasms diagnostics. By offeringinterpretable insights into cardio-neoplasms interactions, this approachbridges existing gaps in non-invasive diagnostics and has implications forintegrating ECG-based tools into broader neoplasms diagnostic frameworks, aswell as neoplasms therapy management.</description><author>Juan Miguel Lopez Alcaraz, Wilhelm Haverkamp, Nils Strodthoff</author><pubDate>Tue, 10 Dec 2024 18:34:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07737v1</guid></item><item><title>SKIPNet: Spatial Attention Skip Connections for Enhanced Brain Tumor Classification</title><link>http://arxiv.org/abs/2412.07736v1</link><description>Early detection of brain tumors through magnetic resonance imaging (MRI) isessential for timely treatment, yet access to diagnostic facilities remainslimited in remote areas. Gliomas, the most common primary brain tumors, arisefrom the carcinogenesis of glial cells in the brain and spinal cord, withglioblastoma patients having a median survival time of less than 14 months. MRIserves as a non-invasive and effective method for tumor detection, but manualsegmentation of brain MRI scans has traditionally been a labor-intensive taskfor neuroradiologists. Recent advancements in computer-aided design (CAD),machine learning (ML), and deep learning (DL) offer promising solutions forautomating this process. This study proposes an automated deep learning modelfor brain tumor detection and classification using MRI data. The model,incorporating spatial attention, achieved 96.90% accuracy, enhancing theaggregation of contextual information for better pattern recognition.Experimental results demonstrate that the proposed approach outperformsbaseline models, highlighting its robustness and potential for advancingautomated MRI-based brain tumor analysis.</description><author>Khush Mendiratta, Shweta Singh, Pratik Chattopadhyay</author><pubDate>Tue, 10 Dec 2024 18:32:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07736v1</guid></item><item><title>An objective function for order preserving hierarchical clustering</title><link>http://arxiv.org/abs/2109.04266v4</link><description>We present a theory and an objective function for similarity-basedhierarchical clustering of probabilistic partial orders and directed acyclicgraphs (DAGs). Specifically, given elements $x \le y$ in the partial order, andtheir respective clusters $[x]$ and $[y]$, the theory yields an order relation$\le'$ on the clusters such that $[x]\le'[y]$. The theory provides a concisedefinition of order-preserving hierarchical clustering, and offers aclassification theorem identifying the order-preserving trees (dendrograms). Todetermine the optimal order-preserving trees, we develop an objective functionthat frames the problem as a bi-objective optimisation, aiming to satisfy boththe order relation and the similarity measure. We prove that the optimal treesunder the objective are both order-preserving and exhibit high-qualityhierarchical clustering. Since finding an optimal solution is NP-hard, weintroduce a polynomial-time approximation algorithm and demonstrate that themethod outperforms existing methods for order-preserving hierarchicalclustering by a significant margin.</description><author>Daniel Bakkelund</author><pubDate>Tue, 10 Dec 2024 18:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.04266v4</guid></item><item><title>The BrowserGym Ecosystem for Web Agent Research</title><link>http://arxiv.org/abs/2412.05467v2</link><description>The BrowserGym ecosystem addresses the growing need for efficient evaluationand benchmarking of web agents, particularly those leveraging automation andLarge Language Models (LLMs) for web interaction tasks. Many existingbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,making it challenging to achieve reliable comparisons and reproducible results.BrowserGym aims to solve this by providing a unified, gym-like environment withwell-defined observation and action spaces, facilitating standardizedevaluation across diverse benchmarks. Combined with AgentLab, a complementaryframework that aids in agent creation, testing, and analysis, BrowserGym offersflexibility for integrating new benchmarks while ensuring consistent evaluationand comprehensive experiment management. This standardized approach seeks toreduce the time and complexity of developing web agents, supporting morereliable comparisons and facilitating in-depth analysis of agent behaviors, andcould result in more adaptable, capable agents, ultimately acceleratinginnovation in LLM-driven automation. As a supporting evidence, we conduct thefirst large-scale, multi-benchmark web agent experiment and compare theperformance of 6 state-of-the-art LLMs across all benchmarks currentlyavailable in BrowserGym. Among other findings, our results highlight a largediscrepancy between OpenAI and Anthropic's latests models, withClaude-3.5-Sonnet leading the way on almost all benchmarks, except onvision-related tasks where GPT-4o is superior. Despite these advancements, ourresults emphasize that building robust and efficient web agents remains asignificant challenge, due to the inherent complexity of real-world webenvironments and the limitations of current models.</description><author>Thibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan Salakhutdinov, Nicolas Chapados, Alexandre Lacoste</author><pubDate>Tue, 10 Dec 2024 18:28:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05467v2</guid></item><item><title>STIV: Scalable Text and Image Conditioned Video Generation</title><link>http://arxiv.org/abs/2412.07730v1</link><description>The field of video generation has made remarkable advancements, yet thereremains a pressing need for a clear, systematic recipe that can guide thedevelopment of robust and scalable models. In this work, we present acomprehensive study that systematically explores the interplay of modelarchitectures, training recipes, and data curation strategies, culminating in asimple and scalable text-image-conditioned video generation method, named STIV.Our framework integrates image condition into a Diffusion Transformer (DiT)through frame replacement, while incorporating text conditioning via a jointimage-text conditional classifier-free guidance. This design enables STIV toperform both text-to-video (T2V) and text-image-to-video (TI2V) taskssimultaneously. Additionally, STIV can be easily extended to variousapplications, such as video prediction, frame interpolation, multi-viewgeneration, and long video generation, etc. With comprehensive ablation studieson T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simpledesign. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V,surpassing both leading open and closed-source models like CogVideoX-5B, Pika,Kling, and Gen-3. The same-sized model also achieves a state-of-the-art resultof 90.1 on VBench I2V task at 512 resolution. By providing a transparent andextensible recipe for building cutting-edge video generation models, we aim toempower future research and accelerate progress toward more versatile andreliable video generation solutions.</description><author>Zongyu Lin, Wei Liu, Chen Chen, Jiasen Lu, Wenze Hu, Tsu-Jui Fu, Jesse Allardice, Zhengfeng Lai, Liangchen Song, Bowen Zhang, Cha Chen, Yiran Fei, Yifan Jiang, Lezhi Li, Yizhou Sun, Kai-Wei Chang, Yinfei Yang</author><pubDate>Tue, 10 Dec 2024 18:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07730v1</guid></item><item><title>Solving the Poisson Equation with Dirichlet data by shallow ReLU$^α$-networks: A regularity and approximation perspective</title><link>http://arxiv.org/abs/2412.07728v1</link><description>For several classes of neural PDE solvers (Deep Ritz, PINNs, DeepONets), theability to approximate the solution or solution operator to a partialdifferential equation (PDE) hinges on the abilitiy of a neural network toapproximate the solution in the spatial variables. We analyze the capacity ofneural networks to approximate solutions to an elliptic PDE assuming that theboundary condition can be approximated efficiently. Our focus is on the Laplaceoperator with Dirichlet boundary condition on a half space and on neuralnetworks with a single hidden layer and an activation function that is a powerof the popular ReLU activation function.</description><author>Malhar Vaishampayan, Stephan Wojtowytsch</author><pubDate>Tue, 10 Dec 2024 18:24:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07728v1</guid></item><item><title>MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion</title><link>http://arxiv.org/abs/2409.12140v2</link><description>We introduce MoRAG, a novel multi-part fusion based retrieval-augmentedgeneration strategy for text-based human motion generation. The method enhancesmotion diffusion models by leveraging additional knowledge obtained through animproved motion retrieval process. By effectively prompting large languagemodels (LLMs), we address spelling errors and rephrasing issues in motionretrieval. Our approach utilizes a multi-part retrieval strategy to improve thegeneralizability of motion retrieval across the language space. We creatediverse samples through the spatial composition of the retrieved motions.Furthermore, by utilizing low-level, part-specific motion information, we canconstruct motion samples for unseen text descriptions. Our experimentsdemonstrate that our framework can serve as a plug-and-play module, improvingthe performance of motion diffusion models. Code, pretrained models and samplevideos are available at: https://motion-rag.github.io/</description><author>Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla</author><pubDate>Tue, 10 Dec 2024 18:24:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12140v2</guid></item><item><title>TorchSISSO: A PyTorch-Based Implementation of the Sure Independence Screening and Sparsifying Operator for Efficient and Interpretable Model Discovery</title><link>http://arxiv.org/abs/2410.01752v2</link><description>Symbolic regression (SR) is a powerful machine learning approach thatsearches for both the structure and parameters of algebraic models, offeringinterpretable and compact representations of complex data. Unlike traditionalregression methods, SR explores progressively complex feature spaces, which canuncover simple models that generalize well, even from small datasets. Among SRalgorithms, the Sure Independence Screening and Sparsifying Operator (SISSO)has proven particularly effective in the natural sciences, helping torediscover fundamental physical laws as well as discover new interpretableequations for materials property modeling. However, its widespread adoption hasbeen limited by performance inefficiencies and the challenges posed by itsFORTRAN-based implementation, especially in modern computing environments. Inthis work, we introduce TorchSISSO, a native Python implementation built in thePyTorch framework. TorchSISSO leverages GPU acceleration, easy integration, andextensibility, offering a significant speed-up and improved accuracy over theoriginal. We demonstrate that TorchSISSO matches or exceeds the performance ofthe original SISSO across a range of tasks, while dramatically reducingcomputational time and improving accessibility for broader scientificapplications.</description><author>Madhav Muthyala, Farshud Sorourifar, Joel A. Paulson</author><pubDate>Tue, 10 Dec 2024 18:21:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01752v2</guid></item><item><title>Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing</title><link>http://arxiv.org/abs/2404.12253v2</link><description>Despite the impressive capabilities of Large Language Models (LLMs) onvarious tasks, they still struggle with scenarios that involves complexreasoning and planning. Recent work proposed advanced prompting techniques andthe necessity of fine-tuning with high-quality data to augment LLMs' reasoningabilities. However, these approaches are inherently constrained by dataavailability and quality. In light of this, self-correction and self-learningemerge as viable solutions, employing strategies that allow LLMs to refinetheir outputs and learn from self-assessed rewards. Yet, the efficacy of LLMsin self-refining its response, particularly in complex reasoning and planningtask, remains dubious. In this paper, we introduce AlphaLLM for theself-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) withLLMs to establish a self-improving loop, thereby enhancing the capabilities ofLLMs without additional annotations. Drawing inspiration from the success ofAlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLMfor self-improvement, including data scarcity, the vastness search spaces oflanguage tasks, and the subjective nature of feedback in language tasks.AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approachtailored for language tasks, and a trio of critic models for precise feedback.Our experimental results in mathematical reasoning tasks demonstrate thatAlphaLLM significantly enhances the performance of LLMs without additionalannotations, showing the potential for self-improvement in LLMs.</description><author>Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, Dong Yu</author><pubDate>Tue, 10 Dec 2024 18:19:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12253v2</guid></item><item><title>Granite Guardian</title><link>http://arxiv.org/abs/2412.07724v1</link><description>We introduce the Granite Guardian models, a suite of safeguards designed toprovide risk detection for prompts and responses, enabling safe and responsibleuse in combination with any large language model (LLM). These models offercomprehensive coverage across multiple risk dimensions, including social bias,profanity, violence, sexual content, unethical behavior, jailbreaking, andhallucination-related risks such as context relevance, groundedness, and answerrelevance for retrieval-augmented generation (RAG). Trained on a unique datasetcombining human annotations from diverse sources and synthetic data, GraniteGuardian models address risks typically overlooked by traditional riskdetection models, such as jailbreaks and RAG-specific issues. With AUC scoresof 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarksrespectively, Granite Guardian is the most generalizable and competitive modelavailable in the space. Released as open-source, Granite Guardian aims topromote responsible AI development across the community. https://github.com/ibm-granite/granite-guardian</description><author>Inkit Padhi, Manish Nagireddy, Giandomenico Cornacchia, Subhajit Chaudhury, Tejaswini Pedapati, Pierre Dognin, Keerthiram Murugesan, Erik Miehling, Martín Santillán Cooper, Kieran Fraser, Giulio Zizzo, Muhammad Zaid Hameed, Mark Purcell, Michael Desmond, Qian Pan, Inge Vejsbjerg, Elizabeth M. Daly, Michael Hind, Werner Geyer, Ambrish Rawat, Kush R. Varshney, Prasanna Sattigeri</author><pubDate>Tue, 10 Dec 2024 18:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07724v1</guid></item><item><title>ObjCtrl-2.5D: Training-free Object Control with Camera Poses</title><link>http://arxiv.org/abs/2412.07721v1</link><description>This study aims to achieve more precise and versatile object control inimage-to-video (I2V) generation. Current methods typically represent thespatial movement of target objects with 2D trajectories, which often fail tocapture user intention and frequently produce unnatural results. To enhancecontrol, we present ObjCtrl-2.5D, a training-free object control approach thatuses a 3D trajectory, extended from a 2D trajectory with depth information, asa control signal. By modeling object movement as camera movement, ObjCtrl-2.5Drepresents the 3D trajectory as a sequence of camera poses, enabling objectmotion control using an existing camera motion control I2V generation model(CMC-I2V) without training. To adapt the CMC-I2V model originally designed forglobal motion control to handle local object motion, we introduce a module toisolate the target object from the background, enabling independent localcontrol. In addition, we devise an effective way to achieve more accurateobject control by sharing low-frequency warped latent within the object'sregion across frames. Extensive experiments demonstrate that ObjCtrl-2.5Dsignificantly improves object control accuracy compared to training-freemethods and offers more diverse control capabilities than training-basedapproaches using 2D trajectories, enabling complex effects like objectrotation. Code and results are available athttps://wzhouxiff.github.io/projects/ObjCtrl-2.5D/.</description><author>Zhouxia Wang, Yushi Lan, Shangchen Zhou, Chen Change Loy</author><pubDate>Tue, 10 Dec 2024 18:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07721v1</guid></item><item><title>Do graph neural network states contain graph properties?</title><link>http://arxiv.org/abs/2411.02168v2</link><description>Deep neural networks (DNNs) achieve state-of-the-art performance on manytasks, but this often requires increasingly larger model sizes, which in turnleads to more complex internal representations. Explainability techniques (XAI)have made remarkable progress in the interpretability of ML models. However,the non-relational nature of Graph neural networks (GNNs) make it difficult toreuse already existing XAI methods. While other works have focused oninstance-based explanation methods for GNNs, very few have investigatedmodel-based methods and, to our knowledge, none have tried to probe theembedding of the GNNs for well-known structural graph properties. In this paperwe present a model agnostic explainability pipeline for GNNs employingdiagnostic classifiers. This pipeline aims to probe and interpret the learnedrepresentations in GNNs across various architectures and datasets, refining ourunderstanding and trust in these models.</description><author>Tom Pelletreau-Duris, Ruud van Bakel, Michael Cochez</author><pubDate>Tue, 10 Dec 2024 18:14:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02168v2</guid></item><item><title>ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer</title><link>http://arxiv.org/abs/2412.07720v1</link><description>The recent surge of interest in comprehensive multimodal models hasnecessitated the unification of diverse modalities. However, the unificationsuffers from disparate methodologies. Continuous visual generation necessitatesthe full-sequence diffusion-based approach, despite its divergence from theautoregressive modeling in the text domain. We posit that autoregressivemodeling, i.e., predicting the future based on past deterministic experience,remains crucial in developing both a visual generation model and a potentialunified multimodal model. In this paper, we explore an interpolation betweenthe autoregressive modeling and full-parameters diffusion to model visualinformation. At its core, we present ACDiT, an Autoregressive blockwiseConditional Diffusion Transformer, where the block size of diffusion, i.e., thesize of autoregressive units, can be flexibly adjusted to interpolate betweentoken-wise autoregression and full-sequence diffusion. ACDiT is easy toimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) duringtraining. During inference, the process iterates between diffusion denoisingand autoregressive decoding that can make full use of KV-Cache. We verify theeffectiveness of ACDiT on image and video generation tasks. We also demonstratethat benefitted from autoregressive modeling, ACDiT can be seamlessly used invisual understanding tasks despite being trained on the diffusion objective.The analysis of the trade-off between autoregressive modeling and diffusiondemonstrates the potential of ACDiT to be used in long-horizon visualgeneration tasks. These strengths make it promising as the backbone of futureunified models.</description><author>Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, Maosong Sun</author><pubDate>Tue, 10 Dec 2024 18:13:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07720v1</guid></item><item><title>ASTD Patterns for Integrated Continuous Anomaly Detection In Data Logs</title><link>http://arxiv.org/abs/2411.07272v2</link><description>This paper investigates the use of the ASTD language for ensemble anomalydetection in data logs. It uses a sliding window technique for continuouslearning in data streams, coupled with updating learning models upon thecompletion of each window to maintain accurate detection and align with currentdata trends. It proposes ASTD patterns for combining learning models,especially in the context of unsupervised learning, which is commonly used fordata streams. To facilitate this, a new ASTD operator is proposed, theQuantified Flow, which enables the seamless combination of learning modelswhile ensuring that the specification remains concise. Our contribution is aspecification pattern, highlighting the capacity of ASTDs to abstract andmodularize anomaly detection systems. The ASTD language provides a uniqueapproach to develop data flow anomaly detection systems, grounded in thecombination of processes through the graphical representation of the languageoperators. This simplifies the design task for developers, who can focusprimarily on defining the functional operations that constitute the system.</description><author>Chaymae El Jabri, Marc Frappier, Pierre-Martin Tardif</author><pubDate>Tue, 10 Dec 2024 18:04:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.07272v2</guid></item><item><title>Benchmark for Evaluation and Analysis of Citation Recommendation Models</title><link>http://arxiv.org/abs/2412.07713v1</link><description>Citation recommendation systems have attracted much academic interest,resulting in many studies and implementations. These systems help authorsautomatically generate proper citations by suggesting relevant references basedon the text they have written. However, the methods used in citationrecommendation differ across various studies and implementations. Someapproaches focus on the overall content of papers, while others consider thecontext of the citation text. Additionally, the datasets used in these studiesinclude different aspects of papers, such as metadata, citation context, oreven the full text of the paper in various formats and structures. Thediversity in models, datasets, and evaluation metrics makes it challenging toassess and compare citation recommendation methods effectively. To address thisissue, a standardized dataset and evaluation metrics are needed to evaluatethese models consistently. Therefore, we propose developing a benchmarkspecifically designed to analyze and compare citation recommendation models.This benchmark will evaluate the performance of models on different features ofthe citation context and provide a comprehensive evaluation of the modelsacross all these tasks, presenting the results in a standardized way. Bycreating a benchmark with standardized evaluation metrics, researchers andpractitioners in the field of citation recommendation will have a commonplatform to assess and compare different models. This will enable meaningfulcomparisons and help identify promising approaches for further research anddevelopment in the field.</description><author>Puja Maharjan</author><pubDate>Tue, 10 Dec 2024 18:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07713v1</guid></item><item><title>Scalable Influence and Fact Tracing for Large Language Model Pretraining</title><link>http://arxiv.org/abs/2410.17413v2</link><description>Training data attribution (TDA) methods aim to attribute model outputs backto specific training examples, and the application of these methods to largelanguage model (LLM) outputs could significantly advance model transparency anddata curation. However, it has been challenging to date to apply these methodsto the full scale of LLM pretraining. In this paper, we refine existinggradient-based methods to work effectively at scale, allowing us to retrieveinfluential examples for an 8B-parameter language model from a pretrainingcorpus of over 160B tokens with no need for subsampling or pre-filtering. Ourmethod combines several techniques, including optimizer state correction, atask-specific Hessian approximation, and normalized encodings, which we find tobe critical for performance at scale. In quantitative evaluations on a facttracing task, our method performs best at identifying examples that influencemodel predictions, but classical, model-agnostic retrieval methods such as BM25still perform better at finding passages which explicitly contain relevantfacts. These results demonstrate a misalignment between factual *attribution*and causal *influence*. With increasing model size and training tokens, we findthat influence more closely aligns with factual attribution. Finally, weexamine different types of examples identified as influential by our method,finding that while many directly entail a particular fact, others support thesame output by reinforcing priors on relation types, common entities, andnames. We release our prompt set and model outputs, along with a web-basedvisualization tool to explore influential examples for factual predictions,commonsense reasoning, arithmetic, and open-ended generation for an8B-parameter LLM.</description><author>Tyler A. Chang, Dheeraj Rajagopal, Tolga Bolukbasi, Lucas Dixon, Ian Tenney</author><pubDate>Tue, 10 Dec 2024 17:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17413v2</guid></item><item><title>Why Fine-grained Labels in Pretraining Benefit Generalization?</title><link>http://arxiv.org/abs/2410.23129v2</link><description>Recent studies show that pretraining a deep neural network with fine-grainedlabeled data, followed by fine-tuning on coarse-labeled data for downstreamtasks, often yields better generalization than pretraining with coarse-labeleddata. While there is ample empirical evidence supporting this, the theoreticaljustification remains an open problem. This paper addresses this gap byintroducing a "hierarchical multi-view" structure to confine the input datadistribution. Under this framework, we prove that: 1) coarse-grainedpretraining only allows a neural network to learn the common features well,while 2) fine-grained pretraining helps the network learn the rare features inaddition to the common ones, leading to improved accuracy on hard downstreamtest samples.</description><author>Guan Zhe Hong, Yin Cui, Ariel Fuxman, Stanley Chan, Enming Luo</author><pubDate>Tue, 10 Dec 2024 17:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23129v2</guid></item><item><title>GEXIA: Granularity Expansion and Iterative Approximation for Scalable Multi-grained Video-language Learning</title><link>http://arxiv.org/abs/2412.07704v1</link><description>In various video-language learning tasks, the challenge of achievingcross-modality alignment with multi-grained data persists. We propose a methodto tackle this challenge from two crucial perspectives: data and modeling.Given the absence of a multi-grained video-text pretraining dataset, weintroduce a Granularity EXpansion (GEX) method with Integration and Compressionoperations to expand the granularity of a single-grained dataset. To bettermodel multi-grained data, we introduce an Iterative Approximation Module (IAM),which embeds multi-grained videos and texts into a unified, low-dimensionalsemantic space while preserving essential information for cross-modalalignment. Furthermore, GEXIA is highly scalable with no restrictions on thenumber of video-text granularities for alignment. We evaluate our work on threecategories of video tasks across seven benchmark datasets, showcasingstate-of-the-art or comparable performance. Remarkably, our model excels intasks involving long-form video understanding, even though the pretrainingdataset only contains short video clips.</description><author>Yicheng Wang, Zhikang Zhang, Jue Wang, David Fan, Zhenlin Xu, Linda Liu, Xiang Hao, Vimal Bhat, Xinyu Li</author><pubDate>Tue, 10 Dec 2024 17:50:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07704v1</guid></item><item><title>Neural Localizer Fields for Continuous 3D Human Pose and Shape Estimation</title><link>http://arxiv.org/abs/2407.07532v2</link><description>With the explosive growth of available training data, single-image 3D humanmodeling is ahead of a transition to a data-centric paradigm. A key tosuccessfully exploiting data scale is to design flexible models that can besupervised from various heterogeneous data sources produced by differentresearchers or vendors. To this end, we propose a simple yet powerful paradigmfor seamlessly unifying different human pose and shape-related tasks anddatasets. Our formulation is centered on the ability -- both at training andtest time -- to query any arbitrary point of the human volume, and obtain itsestimated location in 3D. We achieve this by learning a continuous neural fieldof body point localizer functions, each of which is a differently parameterized3D heatmap-based convolutional point localizer (detector). For generatingparametric output, we propose an efficient post-processing step for fittingSMPL-family body models to nonparametric joint and vertex predictions. Withthis approach, we can naturally exploit differently annotated data sourcesincluding mesh, 2D/3D skeleton and dense pose, without having to convertbetween them, and thereby train large-scale 3D human mesh and skeletonestimation models that considerably outperform the state-of-the-art on severalpublic benchmarks including 3DPW, EMDB, EHF, SSP-3D and AGORA.</description><author>István Sárándi, Gerard Pons-Moll</author><pubDate>Tue, 10 Dec 2024 17:47:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07532v2</guid></item><item><title>An Entailment Tree Generation Approach for Multimodal Multi-Hop Question Answering with Mixture-of-Experts and Iterative Feedback Mechanism</title><link>http://arxiv.org/abs/2412.05821v2</link><description>With the rise of large-scale language models (LLMs), it is currently popularand effective to convert multimodal information into text descriptions formultimodal multi-hop question answering. However, we argue that the currentmethods of multi-modal multi-hop question answering still mainly face twochallenges: 1) The retrieved evidence containing a large amount of redundantinformation, inevitably leads to a significant drop in performance due toirrelevant information misleading the prediction. 2) The reasoning processwithout interpretable reasoning steps makes the model difficult to discover thelogical errors for handling complex questions. To solve these problems, wepropose a unified LLMs-based approach but without heavily relying on them dueto the LLM's potential errors, and innovatively treat multimodal multi-hopquestion answering as a joint entailment tree generation and question answeringproblem. Specifically, we design a multi-task learning framework with a focuson facilitating common knowledge sharing across interpretability and predictiontasks while preventing task-specific errors from interfering with each othervia mixture of experts. Afterward, we design an iterative feedback mechanism tofurther enhance both tasks by feeding back the results of the joint training tothe LLM for regenerating entailment trees, aiming to iteratively refine thepotential answer. Notably, our method has won the first place in the officialleaderboard of WebQA (since April 10, 2024), and achieves competitive resultson MultimodalQA.</description><author>Qing Zhang, Haocheng Lv, Jie Liu, Zhiyun Chen, Jianyong Duan, Hao Wang, Li He, Mingying Xv</author><pubDate>Tue, 10 Dec 2024 17:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05821v2</guid></item><item><title>Quantum vs. Classical Machine Learning Algorithms for Software Defect Prediction: Challenges and Opportunities</title><link>http://arxiv.org/abs/2412.07698v1</link><description>Software defect prediction is a critical aspect of software qualityassurance, as it enables early identification and mitigation of defects,thereby reducing the cost and impact of software failures. Over the past fewyears, quantum computing has risen as an exciting technology capable oftransforming multiple domains; Quantum Machine Learning (QML) is one of them.QML algorithms harness the power of quantum computing to solve complex problemswith better efficiency and effectiveness than their classical counterparts.However, research into its application in software engineering to predictsoftware defects still needs to be explored. In this study, we worked to fillthe research gap by comparing the performance of three QML and five classicalmachine learning (CML) algorithms on the 20 software defect datasets. Ourinvestigation reports the comparative scenarios of QML vs. CML algorithms andidentifies the better-performing and consistent algorithms to predict softwaredefects. We also highlight the challenges and future directions of employingQML algorithms in real software defect datasets based on the experience wefaced while performing this investigation. The findings of this study can helppractitioners and researchers further progress in this research domain bymaking software systems reliable and bug-free.</description><author>Md Nadim, Mohammad Hassan, Ashis Kumar Mandal, Chanchal K. Roy</author><pubDate>Tue, 10 Dec 2024 17:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07698v1</guid></item><item><title>SimVS: Simulating World Inconsistencies for Robust View Synthesis</title><link>http://arxiv.org/abs/2412.07696v1</link><description>Novel-view synthesis techniques achieve impressive results for static scenesbut struggle when faced with the inconsistencies inherent to casual capturesettings: varying illumination, scene motion, and other unintended effects thatare difficult to model explicitly. We present an approach for leveraginggenerative video models to simulate the inconsistencies in the world that canoccur during capture. We use this process, along with existing multi-viewdatasets, to create synthetic data for training a multi-view harmonizationnetwork that is able to reconcile inconsistent observations into a consistent3D scene. We demonstrate that our world-simulation strategy significantlyoutperforms traditional augmentation methods in handling real-world scenevariations, thereby enabling highly accurate static 3D reconstructions in thepresence of a variety of challenging inconsistencies. Project page:https://alextrevithick.github.io/simvs</description><author>Alex Trevithick, Roni Paiss, Philipp Henzler, Dor Verbin, Rundi Wu, Hadi Alzayer, Ruiqi Gao, Ben Poole, Jonathan T. Barron, Aleksander Holynski, Ravi Ramamoorthi, Pratul P. Srinivasan</author><pubDate>Tue, 10 Dec 2024 17:35:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07696v1</guid></item><item><title>Leveraging Content and Context Cues for Low-Light Image Enhancement</title><link>http://arxiv.org/abs/2412.07693v1</link><description>Low-light conditions have an adverse impact on machine cognition, limitingthe performance of computer vision systems in real life. Since low-light datais limited and difficult to annotate, we focus on image processing to enhancelow-light images and improve the performance of any downstream task model,instead of fine-tuning each of the models which can be prohibitively expensive.We propose to improve the existing zero-reference low-light enhancement byleveraging the CLIP model to capture image prior and for semantic guidance.Specifically, we propose a data augmentation strategy to learn an image priorvia prompt learning, based on image sampling, to learn the image prior withoutany need for paired or unpaired normal-light data. Next, we propose a semanticguidance strategy that maximally takes advantage of existing low-lightannotation by introducing both content and context cues about the imagetraining patches. We experimentally show, in a qualitative study, that theproposed prior and semantic guidance help to improve the overall image contrastand hue, as well as improve background-foreground discrimination, resulting inreduced over-saturation and noise over-amplification, common in relatedzero-reference methods. As we target machine cognition, rather than rely onassuming the correlation between human perception and downstream taskperformance, we conduct and present an ablation study and comparison withrelated zero-reference methods in terms of task-based performance across manylow-light datasets, including image classification, object and face detection,showing the effectiveness of our proposed method.</description><author>Igor Morawski, Kai He, Shusil Dangi, Winston H. Hsu</author><pubDate>Tue, 10 Dec 2024 17:32:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07693v1</guid></item><item><title>DriveMM: All-in-One Large Multimodal Model for Autonomous Driving</title><link>http://arxiv.org/abs/2412.07689v1</link><description>Large Multimodal Models (LMMs) have demonstrated exceptional comprehensionand interpretation capabilities in Autonomous Driving (AD) by incorporatinglarge language models. Despite the advancements, current data-driven ADapproaches tend to concentrate on a single dataset and specific tasks,neglecting their overall capabilities and ability to generalize. To bridgethese gaps, we propose DriveMM, a general large multimodal model designed toprocess diverse data inputs, such as images and multi-view videos, whileperforming a broad spectrum of AD tasks, including perception, prediction, andplanning. Initially, the model undergoes curriculum pre-training to processvaried visual signals and perform basic visual comprehension and perceptiontasks. Subsequently, we augment and standardize various AD-related datasets tofine-tune the model, resulting in an all-in-one LMM for autonomous driving. Toassess the general capabilities and generalization ability, we conductevaluations on six public benchmarks and undertake zero-shot transfer on anunseen dataset, where DriveMM achieves state-of-the-art performance across alltasks. We hope DriveMM as a promising solution for future end-toend autonomousdriving applications in the real world.</description><author>Zhijian Huang, Chengjian Feng, Feng Yan, Baihui Xiao, Zequn Jie, Yujie Zhong, Xiaodan Liang, Lin Ma</author><pubDate>Tue, 10 Dec 2024 17:27:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07689v1</guid></item><item><title>Toon3D: Seeing Cartoons from New Perspectives</title><link>http://arxiv.org/abs/2405.10320v3</link><description>We recover the underlying 3D structure from images of cartoons and animedepicting the same scene. This is an interesting problem domain because imagesin creative media are often depicted without explicit geometric consistency forstorytelling and creative expression-they are only 3D in a qualitative sense.While humans can easily perceive the underlying 3D scene from these images,existing Structure-from-Motion (SfM) methods that assume 3D consistency failcatastrophically. We present Toon3D for reconstructing geometricallyinconsistent images. Our key insight is to deform the input images whilerecovering camera poses and scene geometry, effectively explaining awaygeometrical inconsistencies to achieve consistency. This process is guided bythe structure inferred from monocular depth predictions. We curate a datasetwith multi-view imagery from cartoons and anime that we annotate with reliablesparse correspondences using our user-friendly annotation tool. Our recoveredpoint clouds can be plugged into novel-view synthesis methods to experiencecartoons from viewpoints never drawn before. We evaluate against classical andrecent learning-based SfM methods, where Toon3D is able to obtain more reliablecamera poses and scene geometry.</description><author>Ethan Weber, Riley Peterlinz, Rohan Mathur, Frederik Warburg, Alexei A. Efros, Angjoo Kanazawa</author><pubDate>Tue, 10 Dec 2024 17:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10320v3</guid></item><item><title>Privacy-Preserving Customer Support: A Framework for Secure and Scalable Interactions</title><link>http://arxiv.org/abs/2412.07687v1</link><description>The growing reliance on artificial intelligence (AI) in customer support hassignificantly improved operational efficiency and user experience. However,traditional machine learning (ML) approaches, which require extensive localtraining on sensitive datasets, pose substantial privacy risks and compliancechallenges with regulations like the General Data Protection Regulation (GDPR)and California Consumer Privacy Act (CCPA). Existing privacy-preservingtechniques, such as anonymization, differential privacy, and federatedlearning, address some concerns but face limitations in utility, scalability,and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) ina zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminatesthe need for local training on sensitive data by utilizing pre-trained LLMs togenerate responses directly. The framework incorporates real-time dataanonymization to redact or mask sensitive information, retrieval-augmentedgeneration (RAG) for domain-specific query resolution, and robustpost-processing to ensure compliance with regulatory standards. Thiscombination reduces privacy risks, simplifies compliance, and enhancesscalability and operational efficiency. Empirical analysis demonstrates thatthe PP-ZSL framework provides accurate, privacy-compliant responses whilesignificantly lowering the costs and complexities of deploying AI-drivencustomer support systems. The study highlights potential applications acrossindustries, including financial services, healthcare, e-commerce, legalsupport, telecommunications, and government services. By addressing the dualchallenges of privacy and performance, this framework establishes a foundationfor secure, efficient, and regulatory-compliant AI applications in customerinteractions.</description><author>Anant Prakash Awasthi, Chandraketu Singh, Rakshit Varma, Sanchit Sharma</author><pubDate>Tue, 10 Dec 2024 17:20:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07687v1</guid></item><item><title>Optimizing Sensor Redundancy in Sequential Decision-Making Problems</title><link>http://arxiv.org/abs/2412.07686v1</link><description>Reinforcement Learning (RL) policies are designed to predict actions based oncurrent observations to maximize cumulative future rewards. In real-worldapplications (i.e., non-simulated environments), sensors are essential formeasuring the current state and providing the observations on which RL policiesrely to make decisions. A significant challenge in deploying RL policies inreal-world scenarios is handling sensor dropouts, which can result fromhardware malfunctions, physical damage, or environmental factors like dust on acamera lens. A common strategy to mitigate this issue is the use of backupsensors, though this comes with added costs. This paper explores theoptimization of backup sensor configurations to maximize expected returns whilekeeping costs below a specified threshold, C. Our approach uses a second-orderapproximation of expected returns and includes penalties for exceeding costconstraints. We then optimize this quadratic program using Tabu Search, ameta-heuristic algorithm. The approach is evaluated across eight OpenAI Gymenvironments and a custom Unity-based robotic environment (RobotArmGrasping).Empirical results demonstrate that our quadratic program effectivelyapproximates real expected returns, facilitating the identification of optimalsensor configurations.</description><author>Jonas Nüßlein, Maximilian Zorn, Fabian Ritz, Jonas Stein, Gerhard Stenzel, Julian Schönberger, Thomas Gabor, Claudia Linnhoff-Popien</author><pubDate>Tue, 10 Dec 2024 17:20:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07686v1</guid></item><item><title>The Pitfalls of Memorization: When Memorization Hurts Generalization</title><link>http://arxiv.org/abs/2412.07684v1</link><description>Neural networks often learn simple explanations that fit the majority of thedata while memorizing exceptions that deviate from these explanations.Thisbehavior leads to poor generalization when the learned explanations rely onspurious correlations. In this work, we formalize the interplay betweenmemorization and generalization, showing that spurious correlations wouldparticularly lead to poor generalization when are combined with memorization.Memorization can reduce training loss to zero, leaving no incentive to learnrobust, generalizable patterns. To address this, we propose memorization-awaretraining (MAT), which uses held-out predictions as a signal of memorization toshift a model's logits. MAT encourages learning robust patterns invariantacross distributions, improving generalization under distribution shifts.</description><author>Reza Bayat, Mohammad Pezeshki, Elvis Dohmatob, David Lopez-Paz, Pascal Vincent</author><pubDate>Tue, 10 Dec 2024 17:18:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07684v1</guid></item><item><title>TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation</title><link>http://arxiv.org/abs/2412.07682v1</link><description>The inference cost of Large Language Models (LLMs) is a significant challengedue to their computational demands, specially on tasks requiring long outputs.However, natural language often contains redundancy, which presents anopportunity for optimization. We have observed that LLMs can generate distilledlanguage-concise outputs that retain essential meaning, when promptedappropriately. We propose a framework for saving computational cost, in which ashorter distilled output from the LLM is reconstructed into a full narrative bya smaller model with lower inference costs. Our experiments show promisingresults, particularly in general knowledge domains with 20.58% saved tokens onaverage with tiny decrease in evaluation metrics, hinting that this approachcan effectively balance efficiency and accuracy in language processing tasks.</description><author>Alfredo Garrachón Ruiz, Tomás de la Rosa, Daniel Borrajo</author><pubDate>Tue, 10 Dec 2024 17:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07682v1</guid></item><item><title>Optimistic Query Routing in Clustering-based Approximate Maximum Inner Product Search</title><link>http://arxiv.org/abs/2405.12207v3</link><description>Clustering-based nearest neighbor search is an effective method in whichpoints are partitioned into geometric shards to form an index, with only a fewshards searched during query processing to find a set of top-$k$ vectors. Eventhough the search efficacy is heavily influenced by the algorithm thatidentifies the shards to probe, it has received little attention in theliterature. This work bridges that gap by studying routing in clustering-basedmaximum inner product search. We unpack existing routers and notice thesurprising contribution of optimism. We then take a page from the sequentialdecision making literature and formalize that insight following the principleof ``optimism in the face of uncertainty.'' In particular, we present aframework that incorporates the moments of the distribution of inner productswithin each shard to estimate the maximum inner product. We then present aninstance of our algorithm that uses only the first two moments to reach thesame accuracy as state-of-the-art routers such as ScaNN by probing up to $50\%$fewer points on benchmark datasets. Our algorithm is also space-efficient: wedesign a sketch of the second moment whose size is independent of the number ofpoints and requires $\mathcal{O}(1)$ vectors per shard.</description><author>Sebastian Bruch, Aditya Krishnan, Franco Maria Nardini</author><pubDate>Tue, 10 Dec 2024 17:06:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12207v3</guid></item><item><title>RADIO Amplified: Improved Baselines for Agglomerative Vision Foundation Models</title><link>http://arxiv.org/abs/2412.07679v1</link><description>Agglomerative models have recently emerged as a powerful approach to trainingvision foundation models, leveraging multi-teacher distillation from existingmodels such as CLIP, DINO, and SAM. This strategy enables the efficientcreation of robust models, combining the strengths of individual teachers whilesignificantly reducing computational and resource demands. In this paper, wethoroughly analyze state-of-the-art agglomerative models, identifying criticalchallenges including resolution mode shifts, teacher imbalance, idiosyncraticteacher artifacts, and an excessive number of output tokens. To address theseissues, we propose several novel solutions: multi-resolution training, mosaicaugmentation, and improved balancing of teacher loss functions. Specifically,in the context of Vision Language Models, we introduce a token compressiontechnique to maintain high-resolution information within a fixed token count.We release our top-performing models, available in multiple scales (-B, -L, -H,and -g), alongside inference code and pretrained weights.</description><author>Greg Heinrich, Mike Ranzinger, Hongxu, Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, Pavlo Molchanov</author><pubDate>Tue, 10 Dec 2024 17:06:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07679v1</guid></item><item><title>Can linguists better understand DNA?</title><link>http://arxiv.org/abs/2412.07678v1</link><description>Multilingual transfer ability, which reflects how well models fine-tuned onone source language can be applied to other languages, has been well studied inmultilingual pre-trained models. However, the existence of such capabilitytransfer between natural language and gene sequences/languages remainsunderexplored.This study addresses this gap by drawing inspiration from thesentence-pair classification task used for evaluating sentence similarity innatural language. We constructed two analogous tasks: DNA-pairclassification(DNA sequence similarity) and DNA-protein-pairclassification(gene coding determination). These tasks were designed tovalidate the transferability of capabilities from natural language to genesequences. Even a small-scale pre-trained model like GPT-2-small, which waspre-trained on English, achieved an accuracy of 78% on the DNA-pairclassification task after being fine-tuned on English sentence-pairclassification data(XTREME PAWS-X). While training a BERT model on multilingualtext, the precision reached 82%.On the more complex DNA-protein-pairclassification task, however, the model's output was barely distinguishablefrom random output.Experiments suggest that there may be a capability transferfrom natural language to genetic language, but further task testing is neededto confirm this.</description><author>Wang Liang</author><pubDate>Tue, 10 Dec 2024 17:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07678v1</guid></item><item><title>BATIS: Bootstrapping, Autonomous Testing, and Initialization System for Quantum Dot Devices</title><link>http://arxiv.org/abs/2412.07676v1</link><description>Semiconductor quantum dot (QD) devices have become central to advancements inspin-based quantum computing. As the complexity of QD devices grows, manualtuning becomes increasingly infeasible, necessitating robust and scalableautotuning solutions. Tuning large arrays of QD qubits depends on efficientchoices of automated protocols. Here, we introduce a bootstrapping, autonomoustesting, and initialization system (BATIS), an automated framework designed tostreamline QD device testing and initialization. BATIS navigateshigh-dimensional gate voltage spaces, automating essential steps such asleakage testing and gate characterization. The current channel formationprotocol follows a novel and scalable approach that requires a singlemeasurement regardless of the number of channels. Demonstrated at 1.3 K on aquad-QD Si/Si$_x$Ge$_{1-x}$ device, BATIS eliminates the need for deepcryogenic environments during initial device diagnostics, significantlyenhancing scalability and reducing setup times. By requiring minimal priorknowledge of the device architecture, BATIS represents a platform-agnosticsolution, adaptable to various QD systems, which bridges a critical gap in QDautotuning.</description><author>Tyler J. Kovach, Daniel Schug, M. A. Wolfe, E. R. MacQuarrie, Patrick J. Walsh, Jared Benson, Mark Friesen, M. A. Eriksson, Justyna P. Zwolak</author><pubDate>Tue, 10 Dec 2024 17:04:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07676v1</guid></item><item><title>FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2412.07674v1</link><description>Recent advances in text-to-image generation have enabled the creation ofhigh-quality images with diverse applications. However, accurately describingdesired visual attributes can be challenging, especially for non-experts in artand photography. An intuitive solution involves adopting favorable attributesfrom the source images. Current methods attempt to distill identity and stylefrom source images. However, "style" is a broad concept that includes texture,color, and artistic elements, but does not cover other important attributessuch as lighting and dynamics. Additionally, a simplified "style" adaptationprevents combining multiple attributes from different sources into onegenerated image. In this work, we formulate a more effective approach todecompose the aesthetics of a picture into specific visual attributes, allowingusers to apply characteristics such as lighting, texture, and dynamics fromdifferent images. To achieve this goal, we constructed the first fine-grainedvisual attributes dataset (FiVA) to the best of our knowledge. This FiVAdataset features a well-organized taxonomy for visual attributes and includesaround 1 M high-quality generated images with visual attribute annotations.Leveraging this dataset, we propose a fine-grained visual attribute adaptationframework (FiVA-Adapter), which decouples and adapts visual attributes from oneor more source images into a generated one. This approach enhancesuser-friendly customization, allowing users to selectively apply desiredattributes to create images that meet their unique preferences and specificcontent requirements.</description><author>Tong Wu, Yinghao Xu, Ryan Po, Mengchen Zhang, Guandao Yang, Jiaqi Wang, Ziwei Liu, Dahua Lin, Gordon Wetzstein</author><pubDate>Tue, 10 Dec 2024 17:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07674v1</guid></item><item><title>RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text Rewriting</title><link>http://arxiv.org/abs/2412.07675v1</link><description>Despite the widespread use of LLMs due to their superior performance invarious tasks, their high computational costs often lead potential users to optfor the pretraining-finetuning pipeline. However, biases prevalent in manuallyconstructed datasets can introduce spurious correlations between tokens andlabels, creating so-called shortcuts and hindering the generalizability offine-tuned models. Existing debiasing methods often rely on prior knowledge ofspecific dataset biases, which is challenging to acquire a priori. We proposeRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,and data-focused debiasing approach based on text rewriting for shortcutmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased textsegments by replacing them with heuristically selected alternatives in ashortcut space defined by token statistics and positional information. Thisprocess aims to align surface-level text features more closely with diverselabel distributions, thereby promoting the learning of genuine linguisticpatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on theFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.Additionally, RAZOR effectively mitigates specific known biases, reducingbias-related terms by x2 without requiring prior bias information, a resultthat is on par with SoTA models that leverage prior information. Our workprioritizes data manipulation over architectural modifications, emphasizing thepivotal role of data quality in enhancing model performance and fairness. Thisresearch contributes to developing more robust evaluation benchmarks fordebiasing methods by incorporating metrics for bias reduction and overall modelefficacy.</description><author>Shuo Yang, Bardh Prenkaj, Gjergji Kasneci</author><pubDate>Tue, 10 Dec 2024 17:02:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07675v1</guid></item><item><title>FlexLLM: Exploring LLM Customization for Moving Target Defense on Black-Box LLMs Against Jailbreak Attacks</title><link>http://arxiv.org/abs/2412.07672v1</link><description>Defense in large language models (LLMs) is crucial to counter the numerousattackers exploiting these systems to generate harmful content throughmanipulated prompts, known as jailbreak attacks. Although many defensestrategies have been proposed, they often require access to the model'sinternal structure or need additional training, which is impractical forservice providers using LLM APIs, such as OpenAI APIs or Claude APIs. In thispaper, we propose a moving target defense approach that alters decodinghyperparameters to enhance model robustness against various jailbreak attacks.Our approach does not require access to the model's internal structure andincurs no additional training costs. The proposed defense includes two keycomponents: (1) optimizing the decoding strategy by identifying and adjustingdecoding hyperparameters that influence token generation probabilities, and (2)transforming the decoding hyperparameters and model system prompts into dynamictargets, which are continuously altered during each runtime. By continuouslymodifying decoding strategies and prompts, the defense effectively mitigatesthe existing attacks. Our results demonstrate that our defense is the mosteffective against jailbreak attacks in three of the models tested when usingLLMs as black-box APIs. Moreover, our defense offers lower inference costs andmaintains comparable response quality, making it a potential layer ofprotection when used alongside other defense methods.</description><author>Bocheng Chen, Hanqing Guo, Qiben Yan</author><pubDate>Tue, 10 Dec 2024 17:02:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07672v1</guid></item><item><title>DeMem: Privacy-Enhanced Robust Adversarial Learning via De-Memorization</title><link>http://arxiv.org/abs/2412.05767v2</link><description>Adversarial robustness, the ability of a model to withstand manipulatedinputs that cause errors, is essential for ensuring the trustworthiness ofmachine learning models in real-world applications. However, previous studieshave shown that enhancing adversarial robustness through adversarial trainingincreases vulnerability to privacy attacks. While differential privacy canmitigate these attacks, it often compromises robustness against both naturaland adversarial samples. Our analysis reveals that differential privacydisproportionately impacts low-risk samples, causing an unintended performancedrop. To address this, we propose DeMem, which selectively targets high-risksamples, achieving a better balance between privacy protection and modelrobustness. DeMem is versatile and can be seamlessly integrated into variousadversarial training techniques. Extensive evaluations across multiple trainingmethods and datasets demonstrate that DeMem significantly reduces privacyleakage while maintaining robustness against both natural and adversarialsamples. These results confirm DeMem's effectiveness and broad applicability inenhancing privacy without compromising robustness.</description><author>Xiaoyu Luo, Qiongxiu Li</author><pubDate>Tue, 10 Dec 2024 16:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05767v2</guid></item><item><title>Proc-GS: Procedural Building Generation for City Assembly with 3D Gaussians</title><link>http://arxiv.org/abs/2412.07660v1</link><description>Buildings are primary components of cities, often featuring repeated elementssuch as windows and doors. Traditional 3D building asset creation islabor-intensive and requires specialized skills to develop design rules. Recentgenerative models for building creation often overlook these patterns, leadingto low visual fidelity and limited scalability. Drawing inspiration fromprocedural modeling techniques used in the gaming and visual effects industry,our method, Proc-GS, integrates procedural code into the 3D Gaussian Splatting(3D-GS) framework, leveraging their advantages in high-fidelity rendering andefficient asset management from both worlds. By manipulating procedural code,we can streamline this process and generate an infinite variety of buildings.This integration significantly reduces model size by utilizing sharedfoundational assets, enabling scalable generation with precise control overbuilding assembly. We showcase the potential for expansive cityscape generationwhile maintaining high rendering fidelity and precise control on both real andsynthetic cases.</description><author>Yixuan Li, Xingjian Ran, Linning Xu, Tao Lu, Mulin Yu, Zhenzhi Wang, Yuanbo Xiangli, Dahua Lin, Bo Dai</author><pubDate>Tue, 10 Dec 2024 16:45:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07660v1</guid></item><item><title>Analytical-Heuristic Modeling and Optimization for Low-Light Image Enhancement</title><link>http://arxiv.org/abs/2412.07659v1</link><description>Low-light image enhancement remains an open problem, and the new wave ofartificial intelligence is at the center of this problem. This work describesthe use of genetic algorithms for optimizing analytical models that can improvethe visualization of images with poor light. Genetic algorithms are part ofmetaheuristic approaches, which proved helpful in solving challengingoptimization tasks. We propose two analytical methods combined withoptimization reasoning to approach a solution to the physical and computationalaspects of transforming dark images into visible ones. The experimentsdemonstrate that the proposed approach ranks at the top among 26state-of-the-art algorithms in the LOL benchmark. The results show evidencethat a simple genetic algorithm combined with analytical reasoning can defeatthe current mainstream in a challenging computer vision task through controlledexperiments and objective comparisons. This work opens interesting new researchavenues for the swarm and evolutionary computation community and othersinterested in analytical and heuristic reasoning.</description><author>Axel Martinez, Emilio Hernandez, Matthieu Olague, Gustavo Olague</author><pubDate>Tue, 10 Dec 2024 16:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07659v1</guid></item><item><title>TraSCE: Trajectory Steering for Concept Erasure</title><link>http://arxiv.org/abs/2412.07658v1</link><description>Recent advancements in text-to-image diffusion models have brought them tothe public spotlight, becoming widely accessible and embraced by everydayusers. However, these models have been shown to generate harmful content suchas not-safe-for-work (NSFW) images. While approaches have been proposed toerase such abstract concepts from the models, jail-breaking techniques havesucceeded in bypassing such safety measures. In this paper, we propose TraSCE,an approach to guide the diffusion trajectory away from generating harmfulcontent. Our approach is based on negative prompting, but as we show in thispaper, conventional negative prompting is not a complete solution and caneasily be bypassed in some corner cases. To address this issue, we firstpropose a modification of conventional negative prompting. Furthermore, weintroduce a localized loss-based guidance that enhances the modified negativeprompting technique by steering the diffusion trajectory. We demonstrate thatour proposed method achieves state-of-the-art results on various benchmarks inremoving harmful content including ones proposed by red teams; and erasingartistic styles and objects. Our proposed approach does not require anytraining, weight modifications, or training data (both image or prompt), makingit easier for model owners to erase new concepts.</description><author>Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir Memon, Julian Togelius, Yuki Mitsufuji</author><pubDate>Tue, 10 Dec 2024 16:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07658v1</guid></item><item><title>Bayesian Data Augmentation and Training for Perception DNN in Autonomous Aerial Vehicles</title><link>http://arxiv.org/abs/2412.07655v1</link><description>Learning-based solutions have enabled incredible capabilities for autonomoussystems. Autonomous vehicles, both aerial and ground, rely on DNN for variousintegral tasks, including perception. The efficacy of supervised learningsolutions hinges on the quality of the training data. Discrepancies betweentraining data and operating conditions result in faults that can lead tocatastrophic incidents. However, collecting vast amounts of context-sensitivedata, with broad coverage of possible operating environments, is prohibitivelydifficult. Synthetic data generation techniques for DNN allow for the easyexploration of diverse scenarios. However, synthetic data generation solutionsfor aerial vehicles are still lacking. This work presents a data augmentation framework for aerial vehicle'sperception training, leveraging photorealistic simulation integrated withhigh-fidelity vehicle dynamics. Safe landing is a crucial challenge in thedevelopment of autonomous air taxis, therefore, landing maneuver is chosen asthe focus of this work. With repeated simulations of landing in varyingscenarios we assess the landing performance of the VTOL type UAV and gathervaluable data. The landing performance is used as the objective function tooptimize the DNN through retraining. Given the high computational cost of DNNretraining, we incorporated Bayesian Optimization in our framework thatsystematically explores the data augmentation parameter space to retrain thebest-performing models. The framework allowed us to identify high-performingdata augmentation parameters that are consistently effective across differentlanding scenarios. Utilizing the capabilities of this data augmentationframework, we obtained a robust perception model. The model consistentlyimproved the perception-based landing success rate by at least 20% underdifferent lighting and weather conditions.</description><author>Ashik E Rasul, Humaira Tasnim, Hyung-Jin Yoon, Ayoosh Bansal, Duo Wang, Naira Hovakimyan, Lui Sha, Petros Voulgaris</author><pubDate>Tue, 10 Dec 2024 16:41:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07655v1</guid></item><item><title>AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making</title><link>http://arxiv.org/abs/2411.03865v2</link><description>Traditional interactive environments limit agents' intelligence growth withfixed tasks. Recently, single-agent environments address this by generating newtasks based on agent actions, enhancing task diversity. We consider thedecision-making problem in multi-agent settings, where tasks are furtherinfluenced by social connections, affecting rewards and information access.However, existing multi-agent environments lack a combination of adaptivephysical surroundings and social connections, hindering the learning ofintelligent behaviors. To address this, we introduce AdaSociety, a customizablemulti-agent environment featuring expanding state and action spaces, alongsideexplicit and alterable social structures. As agents progress, the environmentadaptively generates new tasks with social structures for agents to undertake.In AdaSociety, we develop three mini-games showcasing distinct socialstructures and tasks. Initial results demonstrate that specific socialstructures can promote both individual and collective benefits, though currentreinforcement learning and LLM-based algorithms show limited effectiveness inleveraging social structures to enhance performance. Overall, AdaSociety servesas a valuable research platform for exploring intelligence in diverse physicaland social settings. The code is available athttps://github.com/bigai-ai/AdaSociety.</description><author>Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Xiaoxi Wang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng</author><pubDate>Tue, 10 Dec 2024 16:41:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03865v2</guid></item><item><title>Relaxed Equivariant Graph Neural Networks</title><link>http://arxiv.org/abs/2407.20471v2</link><description>3D Euclidean symmetry equivariant neural networks have demonstrated notablesuccess in modeling complex physical systems. We introduce a framework forrelaxed $E(3)$ graph equivariant neural networks that can learn and representsymmetry breaking within continuous groups. Building on the existing e3nnframework, we propose the use of relaxed weights to allow for controlledsymmetry breaking. We show empirically that these relaxed weights learn thecorrect amount of symmetry breaking.</description><author>Elyssa Hofgard, Rui Wang, Robin Walters, Tess Smidt</author><pubDate>Tue, 10 Dec 2024 16:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20471v2</guid></item><item><title>Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space</title><link>http://arxiv.org/abs/2406.19370v3</link><description>Modern generative models demonstrate impressive capabilities, likely stemmingfrom an ability to identify and manipulate abstract concepts underlying theirtraining data. However, fundamental questions remain: what determines theconcepts a model learns, the order in which it learns them, and its ability tomanipulate those concepts? To address these questions, we propose analyzing amodel's learning dynamics via a framework we call the concept space, where eachaxis represents an independent concept underlying the data generating process.By characterizing learning dynamics in this space, we identify how the speed atwhich a concept is learned, and hence the order of concept learning, iscontrolled by properties of the data we term concept signal. Further, weobserve moments of sudden turns in the direction of a model's learning dynamicsin concept space. Surprisingly, these points precisely correspond to theemergence of hidden capabilities, i.e., where latent interventions show themodel possesses the capability to manipulate a concept, but these capabilitiescannot yet be elicited via naive input prompting. While our results focus onsynthetically defined toy datasets, we hypothesize a general claim on emergenceof hidden capabilities may hold: generative models possess latent capabilitiesthat emerge suddenly and consistently during training, though a model might notexhibit these capabilities under naive input prompting.</description><author>Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, Hidenori Tanaka</author><pubDate>Tue, 10 Dec 2024 16:40:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19370v3</guid></item><item><title>Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models</title><link>http://arxiv.org/abs/2410.10733v3</link><description>We present Deep Compression Autoencoder (DC-AE), a new family of autoencodermodels for accelerating high-resolution diffusion models. Existing autoencodermodels have demonstrated impressive results at a moderate spatial compressionratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy forhigh spatial compression ratios (e.g., 64x). We address this challenge byintroducing two key techniques: (1) Residual Autoencoding, where we design ourmodels to learn residuals based on the space-to-channel transformed features toalleviate the optimization difficulty of high spatial-compression autoencoders;(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phasestraining strategy for mitigating the generalization penalty of highspatial-compression autoencoders. With these designs, we improve theautoencoder's spatial compression ratio up to 128 while maintaining thereconstruction quality. Applying our DC-AE to latent diffusion models, weachieve significant speedup without accuracy drop. For example, on ImageNet512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedupon H100 GPU for UViT-H while achieving a better FID, compared with the widelyused SD-VAE-f8 autoencoder. Our code is available athttps://github.com/mit-han-lab/efficientvit.</description><author>Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han</author><pubDate>Tue, 10 Dec 2024 16:39:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10733v3</guid></item><item><title>EARN Fairness: Explaining, Asking, Reviewing, and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders</title><link>http://arxiv.org/abs/2407.11442v2</link><description>Numerous fairness metrics have been proposed and employed by artificialintelligence (AI) experts to quantitatively measure bias and define fairness inAI models. Recognizing the need to accommodate stakeholders' diverse fairnessunderstandings, efforts are underway to solicit their input. However, conveyingAI fairness metrics to stakeholders without AI expertise, capturing theirpersonal preferences, and seeking a collective consensus remain challenging andunderexplored. To bridge this gap, we propose a new framework, EARN Fairness,which facilitates collective metric decisions among stakeholders withoutrequiring AI expertise. The framework features an adaptable interactive systemand a stakeholder-centered EARN Fairness process to Explain fairness metrics,Ask stakeholders' personal metric preferences, Review metrics collectively, andNegotiate a consensus on metric selection. To gather empirical results, weapplied the framework to a credit rating scenario and conducted a user studyinvolving 18 decision subjects without AI knowledge. We identify their personalmetric preferences and their acceptable level of unfairness in individualsessions. Subsequently, we uncovered how they reached metric consensus in teamsessions. Our work shows that the EARN Fairness framework enables stakeholdersto express personal preferences and reach consensus, providing practicalguidance for implementing human-centered AI fairness in high-risk contexts.Through this approach, we aim to harmonize fairness expectations of diversestakeholders, fostering more equitable and inclusive AI fairness.</description><author>Lin Luo, Yuri Nakao, Mathieu Chollet, Hiroya Inakoshi, Simone Stumpf</author><pubDate>Tue, 10 Dec 2024 16:34:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11442v2</guid></item><item><title>Searching for Structure: Investigating Emergent Communication with Large Language Models</title><link>http://arxiv.org/abs/2412.07646v1</link><description>Human languages have evolved to be structured through repeated languagelearning and use. These processes introduce biases that operate during languageacquisition and shape linguistic systems toward communicative efficiency. Inthis paper, we investigate whether the same happens if artificial languages areoptimised for implicit biases of Large Language Models (LLMs). To this end, wesimulate a classical referential game in which LLMs learn and use artificiallanguages. Our results show that initially unstructured holistic languages areindeed shaped to have some structural properties that allow two LLM agents tocommunicate successfully. Similar to observations in human experiments,generational transmission increases the learnability of languages, but can atthe same time result in non-humanlike degenerate vocabularies. Taken together,this work extends experimental findings, shows that LLMs can be used as toolsin simulations of language evolution, and opens possibilities for futurehuman-machine experiments in this field.</description><author>Tom Kouwenhoven, Max Peeperkorn, Tessa Verhoef</author><pubDate>Tue, 10 Dec 2024 16:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07646v1</guid></item><item><title>Quantum anomaly detection in the latent space of proton collision events at the LHC</title><link>http://arxiv.org/abs/2301.10780v3</link><description>The ongoing quest to discover new phenomena at the LHC necessitates thecontinuous development of algorithms and technologies. Established approacheslike machine learning, along with emerging technologies such as quantumcomputing show promise in the enhancement of experimental capabilities. In thiswork, we propose a strategy for anomaly detection tasks at the LHC based onunsupervised quantum machine learning, and demonstrate its effectiveness inidentifying new phenomena. The designed quantum models, an unsupervised kernelmachine and two clustering algorithms, are trained to detect new-physics eventsusing a latent representation of LHC data, generated by an autoencoder designedto accommodate current quantum hardware limitations on problem size. Forkernel-based anomaly detection, we implement an instance of the model on aquantum computer, and we identify a regime where it significantly outperformsits classical counterparts. We show that the observed performance enhancementis related to the quantum resources utilised by the model.</description><author>Vasilis Belis, Kinga Anna Woźniak, Ema Puljak, Panagiotis Barkoutsos, Günther Dissertori, Michele Grossi, Maurizio Pierini, Florentin Reiter, Ivano Tavernelli, Sofia Vallecorsa</author><pubDate>Tue, 10 Dec 2024 16:31:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.10780v3</guid></item><item><title>AFD: Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement</title><link>http://arxiv.org/abs/2401.14707v2</link><description>Adversarial fine-tuning methods enhance adversarial robustness viafine-tuning the pre-trained model in an adversarial training manner. However,we identify that some specific latent features of adversarial samples areconfused by adversarial perturbation and lead to an unexpectedly increasing gapbetween features in the last hidden layer of natural and adversarial samples.To address this issue, we propose a disentanglement-based approach toexplicitly model and further remove the specific latent features. We introducea feature disentangler to separate out the specific latent features from thefeatures of the adversarial samples, thereby boosting robustness by eliminatingthe specific latent features. Besides, we align clean features in thepre-trained model with features of adversarial samples in the fine-tuned model,to benefit from the intrinsic features of natural samples. Empiricalevaluations on three benchmark datasets demonstrate that our approach surpassesexisting adversarial fine-tuning methods and adversarial training baselines.</description><author>Nuoyan Zhou, Dawei Zhou, Decheng Liu, Nannan Wang, Xinbo Gao</author><pubDate>Tue, 10 Dec 2024 16:28:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14707v2</guid></item><item><title>Symbolic Regression with a Learned Concept Library</title><link>http://arxiv.org/abs/2409.09359v3</link><description>We present a novel method for symbolic regression (SR), the task of searchingfor compact programmatic hypotheses that best explain a dataset. The problem iscommonly solved using genetic algorithms; we show that we can enhance suchmethods by inducing a library of abstract textual concepts. Our algorithm,called LaSR, uses zero-shot queries to a large language model (LLM) to discoverand evolve concepts occurring in known high-performing hypotheses. We discovernew hypotheses using a mix of standard evolutionary steps and LLM-guided steps(obtained through zero-shot LLM queries) conditioned on discovered concepts.Once discovered, hypotheses are used in a new round of concept abstraction andevolution. We validate LaSR on the Feynman equations, a popular SR benchmark,as well as a set of synthetic tasks. On these benchmarks, LaSR substantiallyoutperforms a variety of state-of-the-art SR approaches based on deep learningand evolutionary algorithms. Moreover, we show that LaSR can be used todiscover a novel and powerful scaling law for LLMs.</description><author>Arya Grayeli, Atharva Sehgal, Omar Costilla-Reyes, Miles Cranmer, Swarat Chaudhuri</author><pubDate>Tue, 10 Dec 2024 16:24:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09359v3</guid></item><item><title>Ballistic Convergence in Hit-and-Run Monte Carlo and a Coordinate-free Randomized Kaczmarz Algorithm</title><link>http://arxiv.org/abs/2412.07643v1</link><description>Hit-and-Run is a coordinate-free Gibbs sampler, yet the quantitativeadvantages of its coordinate-free property remain largely unexplored beyondempirical studies. In this paper, we prove sharp estimates for the Wassersteincontraction of Hit-and-Run in Gaussian target measures via coupling methods andconclude mixing time bounds. Our results uncover ballistic and superdiffusiveconvergence rates in certain settings. Furthermore, we extend these insights toa coordinate-free variant of the randomized Kaczmarz algorithm, an iterativemethod for linear systems, and demonstrate analogous convergence rates. Thesefindings offer new insights into the advantages and limitations ofcoordinate-free methods for both sampling and optimization.</description><author>Nawaf Bou-Rabee, Andreas Eberle, Stefan Oberdörster</author><pubDate>Tue, 10 Dec 2024 16:21:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07643v1</guid></item><item><title>Offline Multi-Agent Reinforcement Learning via In-Sample Sequential Policy Optimization</title><link>http://arxiv.org/abs/2412.07639v1</link><description>Offline Multi-Agent Reinforcement Learning (MARL) is an emerging field thataims to learn optimal multi-agent policies from pre-collected datasets.Compared to single-agent case, multi-agent setting involves a large jointstate-action space and coupled behaviors of multiple agents, which bring extracomplexity to offline policy optimization. In this work, we revisit theexisting offline MARL methods and show that in certain scenarios they can beproblematic, leading to uncoordinated behaviors and out-of-distribution (OOD)joint actions. To address these issues, we propose a new offline MARLalgorithm, named In-Sample Sequential Policy Optimization (InSPO). InSPOsequentially updates each agent's policy in an in-sample manner, which not onlyavoids selecting OOD joint actions but also carefully considers teammates'updated policies to enhance coordination. Additionally, by thoroughly exploringlow-probability actions in the behavior policy, InSPO can well address theissue of premature convergence to sub-optimal solutions. Theoretically, weprove InSPO guarantees monotonic policy improvement and converges to quantalresponse equilibrium (QRE). Experimental results demonstrate the effectivenessof our method compared to current state-of-the-art offline MARL methods.</description><author>Zongkai Liu, Qian Lin, Chao Yu, Xiawei Wu, Yile Liang, Donghui Li, Xuetao Ding</author><pubDate>Tue, 10 Dec 2024 16:19:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07639v1</guid></item><item><title>SurvBETA: Ensemble-Based Survival Models Using Beran Estimators and Several Attention Mechanisms</title><link>http://arxiv.org/abs/2412.07638v1</link><description>Many ensemble-based models have been proposed to solve machine learningproblems in the survival analysis framework, including random survival forests,the gradient boosting machine with weak survival models, ensembles of the Coxmodels. To extend the set of models, a new ensemble-based model called SurvBETA(the Survival Beran estimator Ensemble using Three Attention mechanisms) isproposed where the Beran estimator is used as a weak learner in the ensemble.The Beran estimator can be regarded as a kernel regression model taking intoaccount the relationship between instances. Outputs of weak learners in theform of conditional survival functions are aggregated with attention weightstaking into account the distance between the analyzed instance and prototypesof all bootstrap samples. The attention mechanism is used three times: forimplementation of the Beran estimators, for determining specific prototypes ofbootstrap samples and for aggregating the weak model predictions. The proposedmodel is presented in two forms: in a general form requiring to solve a complexoptimization problem for its training; in a simplified form by considering aspecial representation of the attention weights by means of the impreciseHuber's contamination model which leads to solving a simple optimizationproblem. Numerical experiments illustrate properties of the model on syntheticdata and compare the model with other survival models on real data. A codeimplementing the proposed model is publicly available.</description><author>Lev V. Utkin, Semen P. Khomets, Vlada A. Efremenko, Andrei V. Konstantinov</author><pubDate>Tue, 10 Dec 2024 16:17:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07638v1</guid></item><item><title>Sampling from Boltzmann densities with physics informed low-rank formats</title><link>http://arxiv.org/abs/2412.07637v1</link><description>Our method proposes the efficient generation of samples from an unnormalizedBoltzmann density by solving the underlying continuity equation in the low-ranktensor train (TT) format. It is based on the annealing path commonly used inMCMC literature, which is given by the linear interpolation in the space ofenergies. Inspired by Sequential Monte Carlo, we alternate betweendeterministic time steps from the TT representation of the flow field andstochastic steps, which include Langevin and resampling steps. These adjust therelative weights of the different modes of the target distribution and annealto the correct path distribution. We showcase the efficiency of our method onmultiple numerical examples.</description><author>Paul Hagemann, Janina Schütte, David Sommer, Martin Eigel, Gabriele Steidl</author><pubDate>Tue, 10 Dec 2024 16:17:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07637v1</guid></item><item><title>TrojanWhisper: Evaluating Pre-trained LLMs to Detect and Localize Hardware Trojans</title><link>http://arxiv.org/abs/2412.07636v1</link><description>Existing Hardware Trojans (HT) detection methods face several criticallimitations: logic testing struggles with scalability and coverage for largedesigns, side-channel analysis requires golden reference chips, and formalverification methods suffer from state-space explosion. The emergence of LargeLanguage Models (LLMs) offers a promising new direction for HT detection byleveraging their natural language understanding and reasoning capabilities. Forthe first time, this paper explores the potential of general-purpose LLMs indetecting various HTs inserted in Register Transfer Level (RTL) designs,including SRAM, AES, and UART modules. We propose a novel tool for this goalthat systematically assesses state-of-the-art LLMs (GPT-4o, Gemini 1.5 pro, andLlama 3.1) in detecting HTs without prior fine-tuning. To address potentialtraining data bias, the tool implements perturbation techniques, i.e., variablename obfuscation, and design restructuring, that make the cases moresophisticated for the used LLMs. Our experimental evaluation demonstratesperfect detection rates by GPT-4o and Gemini 1.5 pro in baseline scenarios(100%/100% precision/recall), with both models achieving better trigger linecoverage (TLC: 0.82-0.98) than payload line coverage (PLC: 0.32-0.46). Undercode perturbation, while Gemini 1.5 pro maintains perfect detection performance(100%/100%), GPT-4o (100%/85.7%) and Llama 3.1 (66.7%/85.7%) show somedegradation in detection rates, and all models experience decreased accuracy inlocalizing both triggers and payloads. This paper validates the potential ofLLM approaches for hardware security applications, highlighting areas forfuture improvement.</description><author>Md Omar Faruque, Peter Jamieson, Ahmad Patooghy, Abdel-Hameed A. Badawy</author><pubDate>Tue, 10 Dec 2024 16:16:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07636v1</guid></item><item><title>AFFSegNet: Adaptive Feature Fusion Segmentation Network for Microtumors and Multi-Organ Segmentation</title><link>http://arxiv.org/abs/2409.07779v3</link><description>Medical image segmentation, a crucial task in computer vision, facilitatesthe automated delineation of anatomical structures and pathologies, supportingclinicians in diagnosis, treatment planning, and disease monitoring. Notably,transformers employing shifted window-based self-attention have demonstratedexceptional performance. However, their reliance on local window attentionlimits the fusion of local and global contextual information, crucial forsegmenting microtumors and miniature organs. To address this limitation, wepropose the Adaptive Semantic Segmentation Network (ASSNet), a transformerarchitecture that effectively integrates local and global features for precisemedical image segmentation. ASSNet comprises a transformer-based U-shapedencoder-decoder network. The encoder utilizes shifted window self-attentionacross five resolutions to extract multi-scale features, which are thenpropagated to the decoder through skip connections. We introduce an augmentedmulti-layer perceptron within the encoder to explicitly model long-rangedependencies during feature extraction. Recognizing the constraints ofconventional symmetrical encoder-decoder designs, we propose an AdaptiveFeature Fusion (AFF) decoder to complement our encoder. This decoderincorporates three key components: the Long Range Dependencies (LRD) block, theMulti-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC)block. These components synergistically facilitate the effective fusion ofmulti-scale features extracted by the decoder while capturing long-rangedependencies and refining object boundaries. Comprehensive experiments ondiverse medical image segmentation tasks, including multi-organ, liver tumor,and bladder tumor segmentation, demonstrate that ASSNet achievesstate-of-the-art results. Code and models are available at:\url{https://github.com/lzeeorno/ASSNet}.</description><author>Fuchen Zheng, Xinyi Chen, Xuhang Chen, Haolun Li, Xiaojiao Guo, Weihuang Liu, Chi-Man Pun, Shoujun Zhou</author><pubDate>Tue, 10 Dec 2024 16:16:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07779v3</guid></item><item><title>ChocoLlama: Lessons Learned From Teaching Llamas Dutch</title><link>http://arxiv.org/abs/2412.07633v1</link><description>While Large Language Models (LLMs) have shown remarkable capabilities innatural language understanding and generation, their performance often lags inlower-resource, non-English languages due to biases in the training data. Inthis work, we explore strategies for adapting the primarily English LLMs(Llama-2 and Llama-3) to Dutch, a language spoken by 30 million peopleworldwide yet often underrepresented in LLM development. We collect 104GB ofDutch text ($32$B tokens) from various sources to first apply continuedpretraining using low-rank adaptation (LoRA), complemented with Dutchposttraining strategies provided by prior work. For Llama-2, we consider using(i) the tokenizer of the original model, and (ii) training a new,Dutch-specific tokenizer combined with embedding reinitialization. We evaluateour adapted models, ChocoLlama-2, both on standard benchmarks and a novel Dutchbenchmark, ChocoLlama-Bench. Our results demonstrate that LoRA can effectivelyscale for language adaptation, and that tokenizer modification with carefulweight reinitialization can improve performance. Notably, Llama-3 was releasedduring the course of this project and, upon evaluation, demonstrated superiorDutch capabilities compared to our Dutch-adapted versions of Llama-2. We henceapply the same adaptation technique to Llama-3, using its original tokenizer.While our adaptation methods enhanced Llama-2's Dutch capabilities, we foundlimited gains when applying the same techniques to Llama-3. This suggests thatfor ever improving, multilingual foundation models, language adaptationtechniques may benefit more from focusing on language-specific posttrainingrather than on continued pretraining. We hope this work contributes to thebroader understanding of adapting LLMs to lower-resource languages, and to thedevelopment of Dutch LLMs in particular.</description><author>Matthieu Meeus, Anthony Rathé, François Remy, Pieter Delobelle, Jens-Joris Decorte, Thomas Demeester</author><pubDate>Tue, 10 Dec 2024 16:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07633v1</guid></item><item><title>Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization</title><link>http://arxiv.org/abs/2306.06805v3</link><description>Feature visualization has gained substantial popularity, particularly afterthe influential work by Olah et al. in 2017, which established it as a crucialtool for explainability. However, its widespread adoption has been limited dueto a reliance on tricks to generate interpretable images, and correspondingchallenges in scaling it to deeper neural networks. Here, we describe MACO, asimple approach to address these shortcomings. The main idea is to generateimages by optimizing the phase spectrum while keeping the magnitude constant toensure that generated explanations lie in the space of natural images. Ourapproach yields significantly better results (both qualitatively andquantitatively) and unlocks efficient and interpretable feature visualizationsfor large state-of-the-art neural networks. We also show that our approachexhibits an attribution mechanism allowing us to augment feature visualizationswith spatial importance. We validate our method on a novel benchmark forcomparing feature visualization methods, and release its visualizations for allclasses of the ImageNet dataset on https://serre-lab.github.io/Lens/. Overall, our approach unlocks, for the first time, feature visualizations forlarge, state-of-the-art deep neural networks without resorting to anyparametric prior image model.</description><author>Thomas Fel, Thibaut Boissin, Victor Boutin, Agustin Picard, Paul Novello, Julien Colin, Drew Linsley, Tom Rousseau, Rémi Cadène, Lore Goetschalckx, Laurent Gardes, Thomas Serre</author><pubDate>Tue, 10 Dec 2024 16:12:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06805v3</guid></item><item><title>Piece of Table: A Divide-and-Conquer Approach for Selecting Sub-Tables in Table Question Answering</title><link>http://arxiv.org/abs/2412.07629v1</link><description>Applying language models (LMs) to tables is challenging due to the inherentstructural differences between two-dimensional tables and one-dimensional textfor which the LMs were originally designed. Furthermore, when applyinglinearized tables to LMs, the maximum token lengths often imposed inself-attention calculations make it difficult to comprehensively understand thecontext spread across large tables. To address these challenges, we presentPieTa (Piece of Table), a new framework for sub-table-based question answering(QA). PieTa operates through an iterative process of dividing tables intosmaller windows, using LMs to select relevant cells within each window, andmerging these cells into a sub-table. This multi-resolution approach capturesdependencies across multiple rows and columns while avoiding the limitationscaused by long context inputs. Instantiated as a simple iterative sub-tableunion algorithm, PieTa demonstrates improved performance over previoussub-table-based QA approaches.</description><author>Wonjin Lee, Kyumin Kim, Sungjae Lee, Jihun Lee, Kwang In KIm</author><pubDate>Tue, 10 Dec 2024 16:08:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07629v1</guid></item><item><title>Leveraging Large Language Models for Node Generation in Few-Shot Learning on Text-Attributed Graphs</title><link>http://arxiv.org/abs/2310.09872v2</link><description>Text-attributed graphs have recently garnered significant attention due totheir wide range of applications in web domains. Existing methodologies employword embedding models for acquiring text representations as node features,which are subsequently fed into Graph Neural Networks (GNNs) for training.Recently, the advent of Large Language Models (LLMs) has introduced theirpowerful capabilities in information retrieval and text generation, which cangreatly enhance the text attributes of graph data. Furthermore, the acquisitionand labeling of extensive datasets are both costly and time-consumingendeavors. Consequently, few-shot learning has emerged as a crucial problem inthe context of graph learning tasks. In order to tackle this challenge, wepropose a lightweight paradigm called LLM4NG, which adopts a plug-and-playapproach to empower text-attributed graphs through node generation using LLMs.Specifically, we utilize LLMs to extract semantic information from the labelsand generate samples that belong to these categories as exemplars.Subsequently, we employ an edge predictor to capture the structural informationinherent in the raw dataset and integrate the newly generated samples into theoriginal graph. This approach harnesses LLMs for enhancing class-levelinformation and seamlessly introduces labeled nodes and edges without modifyingthe raw dataset, thereby facilitating the node classification task in few-shotscenarios. Extensive experiments demonstrate the outstanding performance of ourproposed paradigm, particularly in low-shot scenarios. For instance, in the1-shot setting of the ogbn-arxiv dataset, LLM4NG achieves a 76% improvementover the baseline model.</description><author>Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang</author><pubDate>Tue, 10 Dec 2024 16:06:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09872v2</guid></item><item><title>OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations</title><link>http://arxiv.org/abs/2412.07626v1</link><description>Document content extraction is crucial in computer vision, especially formeeting the high-quality data needs of large language models (LLMs) andretrieval-augmented generation (RAG) technologies. However, current documentparsing methods suffer from significant limitations in terms of diversity andcomprehensive evaluation. To address these challenges, we introduceOmniDocBench, a novel multi-source benchmark designed to advance automateddocument content extraction. OmniDocBench includes a meticulously curated andannotated high-quality evaluation dataset comprising nine diverse documenttypes, such as academic papers, textbooks, slides, among others. Our benchmarkprovides a flexible and comprehensive evaluation framework with 19 layoutcategory labels and 14 attribute labels, enabling multi-level assessmentsacross entire datasets, individual modules, or specific data types. UsingOmniDocBench, we perform an exhaustive comparative analysis of existing modularpipelines and multimodal end-to-end methods, highlighting their limitations inhandling document diversity and ensuring fair evaluation. OmniDocBenchestablishes a robust, diverse, and fair evaluation standard for the documentcontent extraction field, offering crucial insights for future advancements andfostering the development of document parsing technologies. The codes anddataset is available in https://github.com/opendatalab/OmniDocBench.</description><author>Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu, Minghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang, Botian Shi, Zhongying Tu, Conghui He</author><pubDate>Tue, 10 Dec 2024 16:05:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07626v1</guid></item><item><title>Stable-Hair: Real-World Hair Transfer via Diffusion Model</title><link>http://arxiv.org/abs/2407.14078v2</link><description>Current hair transfer methods struggle to handle diverse and intricatehairstyles, limiting their applicability in real-world scenarios. In thispaper, we propose a novel diffusion-based hair transfer framework, named\textit{Stable-Hair}, which robustly transfers a wide range of real-worldhairstyles to user-provided faces for virtual hair try-on. To achieve thisgoal, our Stable-Hair framework is designed as a two-stage pipeline. In thefirst stage, we train a Bald Converter alongside stable diffusion to removehair from the user-provided face images, resulting in bald images. In thesecond stage, we specifically designed a Hair Extractor and a LatentIdentityNet to transfer the target hairstyle with highly detailed andhigh-fidelity to the bald image. The Hair Extractor is trained to encodereference images with the desired hairstyles, while the Latent IdentityNetensures consistency in identity and background. To minimize color deviationsbetween source images and transfer results, we introduce a novel LatentControlNet architecture, which functions as both the Bald Converter and LatentIdentityNet. After training on our curated triplet dataset, our methodaccurately transfers highly detailed and high-fidelity hairstyles to the sourceimages. Extensive experiments demonstrate that our approach achievesstate-of-the-art performance compared to existing hair transfer methods.Project page:\textcolor{red}{\url{https://xiaojiu-z.github.io/Stable-Hair.github.io/}}</description><author>Yuxuan Zhang, Qing Zhang, Yiren Song, Jichao Zhang, Hao Tang, Jiaming Liu</author><pubDate>Tue, 10 Dec 2024 16:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14078v2</guid></item><item><title>DRUM: Learning Demonstration Retriever for Large MUlti-modal Models</title><link>http://arxiv.org/abs/2412.07619v1</link><description>Recently, large language models (LLMs) have demonstrated impressivecapabilities in dealing with new tasks with the help of in-context learning(ICL). In the study of Large Vision-Language Models (LVLMs), when implementingICL, researchers usually adopts the naive strategies like fixed demonstrationsacross different samples, or selecting demonstrations directly via avisual-language embedding model. These methods does not guarantee theconfigured demonstrations fit the need of the LVLMs. To address this issue, wenow propose a novel framework, \underline{d}emonstration \underline{r}etrieverfor large m\underline{u}lti-modal \underline{m}odel (DRUM), which fine-tunesthe visual-language embedding model to better meet the LVLM's needs. First, wediscuss the retrieval strategies for a visual-language task, assuming anembedding model is given. And we propose to concate the image and textembeddings to enhance the retrieval performance. Second, we propose to re-rankthe demonstrations retrieved by the embedding model via the LVLM's feedbacks,and calculate a list-wise ranking loss for training the embedding model. Third,we propose an iterative demonstration mining strategy to improve the trainingof the embedding model. Through extensive experiments on 3 types ofvisual-language tasks, 7 benchmark datasets, our DRUM framework is proven to beeffective in boosting the LVLM's in-context learning performance via retrievingmore proper demonstrations.</description><author>Ellen Yi-Ge, Jiechao Gao, Wei Han, Wei Zhu</author><pubDate>Tue, 10 Dec 2024 15:56:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07619v1</guid></item><item><title>Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced Retrieval-Augmented Generation on Knowledge Graphs</title><link>http://arxiv.org/abs/2412.07618v1</link><description>Despite the superior performance of Large language models on many NLP tasks,they still face significant limitations in memorizing extensive worldknowledge. Recent studies have demonstrated that leveraging theRetrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphsthat encapsulate extensive factual data in a structured format, robustlyenhances the reasoning capabilities of LLMs. However, deploying such systems inreal-world scenarios presents challenges: the continuous evolution ofnon-stationary environments may lead to performance degradation and usersatisfaction requires a careful balance of performance and responsiveness. Toaddress these challenges, we introduce a Multi-objective Multi-Armed Banditenhanced RAG framework, supported by multiple retrieval methods with diversecapabilities under rich and evolving retrieval contexts in practice. Withinthis framework, each retrieval method is treated as a distinct ``arm''. Thesystem utilizes real-time user feedback to adapt to dynamic environments, byselecting the appropriate retrieval method based on input queries and thehistorical multi-objective performance of each arm. Extensive experimentsconducted on two benchmark KGQA datasets demonstrate that our methodsignificantly outperforms baseline methods in non-stationary settings whileachieving state-of-the-art performance in stationary environments. Code anddata are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git</description><author>Xiaqiang Tang, Jian Li, Nan Du, Sihong Xie</author><pubDate>Tue, 10 Dec 2024 15:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07618v1</guid></item><item><title>Swarm Behavior Cloning</title><link>http://arxiv.org/abs/2412.07617v1</link><description>In sequential decision-making environments, the primary approaches fortraining agents are Reinforcement Learning (RL) and Imitation Learning (IL).Unlike RL, which relies on modeling a reward function, IL leverages expertdemonstrations, where an expert policy $\pi_e$ (e.g., a human) provides thedesired behavior. Formally, a dataset $D$ of state-action pairs is provided: $D= {(s, a = \pi_e(s))}$. A common technique within IL is Behavior Cloning (BC),where a policy $\pi(s) = a$ is learned through supervised learning on $D$.Further improvements can be achieved by using an ensemble of $N$ individuallytrained BC policies, denoted as $E = {\pi_i(s)}{1 \leq i \leq N}$. Theensemble's action $a$ for a given state $s$ is the aggregated output of the $N$actions: $a = \frac{1}{N} \sum{i} \pi_i(s)$. This paper addresses the issue ofincreasing action differences -- the observation that discrepancies between the$N$ predicted actions grow in states that are underrepresented in the trainingdata. Large action differences can result in suboptimal aggregated actions. Toaddress this, we propose a method that fosters greater alignment among thepolicies while preserving the diversity of their computations. This approachreduces action differences and ensures that the ensemble retains its inherentstrengths, such as robustness and varied decision-making. We evaluate ourapproach across eight diverse environments, demonstrating a notable decrease inaction differences and significant improvements in overall performance, asmeasured by mean episode returns.</description><author>Jonas Nüßlein, Maximilian Zorn, Philipp Altmann, Claudia Linnhoff-Popien</author><pubDate>Tue, 10 Dec 2024 15:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07617v1</guid></item><item><title>PVP: Polar Representation Boost for 3D Semantic Occupancy Prediction</title><link>http://arxiv.org/abs/2412.07616v1</link><description>Recently, polar coordinate-based representations have shown promise for 3Dperceptual tasks. Compared to Cartesian methods, polar grids provide a viablealternative, offering better detail preservation in nearby spaces whilecovering larger areas. However, they face feature distortion due to non-uniformdivision. To address these issues, we introduce the Polar Voxel OccupancyPredictor (PVP), a novel 3D multi-modal predictor that operates in polarcoordinates. PVP features two key design elements to overcome distortion: aGlobal Represent Propagation (GRP) module that integrates global spatial datainto 3D volumes, and a Plane Decomposed Convolution (PD-Conv) that simplifies3D distortions into 2D convolutions. These innovations enable PVP to outperformexisting methods, achieving significant improvements in mIoU and IoU metrics onthe OpenOccupancy dataset.</description><author>Yujing Xue, Jiaxiang Liu, Jiawei Du, Joey Tianyi Zhou</author><pubDate>Tue, 10 Dec 2024 15:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07616v1</guid></item><item><title>ViewDelta: Text-Prompted Change Detection in Unaligned Images</title><link>http://arxiv.org/abs/2412.07612v1</link><description>Detecting changes between images is a fundamental problem in computer visionwith broad applications in situational awareness, infrastructure assessment,environment monitoring, and industrial automation. Existing supervised modelsare typically limited to detecting specific types of changes, necessitatingretraining for new tasks. To address these limitations with a single approach,we propose a novel change detection method that is the first to utilizeunaligned images and textual prompts to output a binary segmentation of changesrelevant to user-provided text. Our architecture not only enables flexibledetection across diverse change detection use cases, but also yieldsstate-of-the art performance on established benchmarks. Additionally, werelease an accompanying dataset comprising of 100,311 pairs of images with textprompts and the corresponding change detection labels. We demonstrate theeffectiveness of our method both quantitatively and qualitatively on datasetswith a wide variety of viewpoints in indoor, outdoor, street level, synthetic,and satellite images.</description><author>Subin Varghese, Joshua Gao, Vedhus Hoskere</author><pubDate>Tue, 10 Dec 2024 15:51:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07612v1</guid></item></channel></rss>