<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 17 Sep 2024 13:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval</title><link>http://arxiv.org/abs/2409.10516v1</link><description>Transformer-based large Language Models (LLMs) become increasingly importantin various domains. However, the quadratic time complexity of attentionoperation poses a significant challenge for scaling to longer contexts due tothe extremely high inference latency and GPU memory consumption for cachingkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-freeapproach to accelerate attention computation. To leverage the dynamic sparseproperty of attention, RetrievalAttention builds approximate nearest neighborsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the mostrelevant ones via vector search during generation. Due to theout-of-distribution (OOD) between query vectors and key vectors, off-the-shelfANNS indexes still need to scan O(N) (usually 30% of all keys) data foraccurate retrieval, which fails to exploit the high sparsity.RetrievalAttention first identifies the OOD challenge of ANNS-based attention,and addresses it via an attention-aware vector search algorithm that can adaptto queries and only access 1--3% of data, thus achieving a sub-linear timecomplexity. RetrievalAttention greatly reduces the inference cost oflong-context LLM with much lower GPU memory requirements while maintaining themodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory forserving 128K tokens in LLMs with 8B parameters, which is capable of generatingone token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).</description><author>Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu</author><pubDate>Mon, 16 Sep 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10516v1</guid></item><item><title>An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems</title><link>http://arxiv.org/abs/2409.10515v1</link><description>Dialog systems, such as voice assistants, are expected to engage with usersin complex, evolving conversations. Unfortunately, traditional automatic speechrecognition (ASR) systems deployed in such applications are usually trained torecognize each turn independently and lack the ability to adapt to theconversational context or incorporate user feedback. In this work, we introducea general framework for ASR in dialog systems that can go beyond learning fromsingle-turn utterances and learn over time how to adapt to both explicitsupervision and implicit user feedback present in multi-turn conversations. Weaccomplish that by leveraging advances in student-teacher learning andcontext-aware dialog processing, and designing contrastive self-supervisionapproaches with Ohm, a new online hard-negative mining approach. We show thatleveraging our new framework compared to traditional training leads to relativeWER reductions of close to 10% in real-world dialog systems, and up to 26% onpublic synthetic data.</description><author>Hitesh Tulsiani, David M. Chan, Shalini Ghosh, Garima Lalwani, Prabhat Pandey, Ankish Bansal, Sri Garimella, Ariya Rastrow, Björn Hoffmeister</author><pubDate>Mon, 16 Sep 2024 17:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10515v1</guid></item><item><title>Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</title><link>http://arxiv.org/abs/2407.21787v2</link><description>Scaling the amount of compute used to train language models has dramaticallyimproved their capabilities. However, when it comes to inference, we oftenlimit the amount of compute to only one attempt per problem. Here, we exploreinference compute as another axis for scaling by increasing the number ofgenerated samples. Across multiple tasks and models, we observe that coverage -the fraction of problems solved by any attempt - scales with the number ofsamples over four orders of magnitude. In domains like coding and formalproofs, where all answers can be automatically verified, these increases incoverage directly translate into improved performance. When we apply repeatedsampling to SWE-bench Lite, the fraction of issues solved withDeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250samples, outperforming the single-attempt state-of-the-art of 43% which usesmore capable frontier models. Moreover, using current API pricing, amplifyingthe cheaper DeepSeek model with five samples is more cost-effective and solvesmore issues than paying a premium for one sample from GPT-4o or Claude 3.5Sonnet. Interestingly, the relationship between coverage and the number ofsamples is often log-linear and can be modelled with an exponentiated powerlaw, suggesting the existence of inference-time scaling laws. Finally, we findthat identifying correct samples out of many generations remains an importantdirection for future research in domains without automatic verifiers. Whensolving math word problems from GSM8K and MATH, coverage with Llama-3 modelsgrows to over 95% with 10,000 samples. However, common methods to pick correctsolutions from a sample collection, such as majority voting or reward models,plateau beyond several hundred samples and fail to fully scale with the samplebudget.</description><author>Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, Azalia Mirhoseini</author><pubDate>Mon, 16 Sep 2024 17:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21787v2</guid></item><item><title>Decidability of Querying First-Order Theories via Countermodels of Finite Width</title><link>http://arxiv.org/abs/2304.06348v3</link><description>We propose a generic framework for establishing the decidability of a widerange of logical entailment problems (briefly called querying), based on theexistence of countermodels that are structurally simple, gauged by certaintypes of width measures (with treewidth and cliquewidth as popular examples).As an important special case of our framework, we identify logics exhibitingwidth-finite finitely universal model sets, warranting decidable entailment fora wide range of homomorphism-closed queries, subsuming a diverse set ofpractically relevant query languages. As a particularly powerful width measure,we propose to employ Blumensath's partitionwidth, which subsumes various othercommonly considered width measures and exhibits highly favorable computationaland structural properties. Focusing on the formalism of existential rules as apopular showcase, we explain how finite partitionwidth sets of rules subsumeother known abstract decidable classes but - leveraging existing notions ofstratification - also cover a wide range of new rulesets. We expose naturallimitations for fitting the class of finite unification sets into our pictureand suggest several options for remedy.</description><author>Thomas Feller, Tim S. Lyon, Piotr Ostropolski-Nalewaja, Sebastian Rudolph</author><pubDate>Mon, 16 Sep 2024 17:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06348v3</guid></item><item><title>Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination</title><link>http://arxiv.org/abs/2406.08818v2</link><description>We present a large-scale study of linguistic bias exhibited by ChatGPTcovering ten dialects of English (Standard American English, Standard BritishEnglish, and eight widely spoken non-"standard" varieties from around theworld). We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers ofeach variety and analyzed the responses via detailed linguistic featureannotation and native speaker evaluation. We find that the models default to"standard" varieties of English; based on evaluation by native speakers, wealso find that model responses to non-"standard" varieties consistently exhibita range of issues: stereotyping (19% worse than for "standard" varieties),demeaning content (25% worse), lack of comprehension (9% worse), andcondescending responses (15% worse). We also find that if these models areasked to imitate the writing style of prompts in non-"standard" varieties, theyproduce text that exhibits lower comprehension of the input and is especiallyprone to stereotyping. GPT-4 improves on GPT-3.5 in terms of comprehension,warmth, and friendliness, but also exhibits a marked increase in stereotyping(+18%). The results indicate that GPT-3.5 Turbo and GPT-4 can perpetuatelinguistic discrimination toward speakers of non-"standard" varieties.</description><author>Eve Fleisig, Genevieve Smith, Madeline Bossi, Ishita Rustagi, Xavier Yin, Dan Klein</author><pubDate>Mon, 16 Sep 2024 17:51:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08818v2</guid></item><item><title>Assumption-Lean and Data-Adaptive Post-Prediction Inference</title><link>http://arxiv.org/abs/2311.14220v4</link><description>A primary challenge facing modern scientific research is the limitedavailability of gold-standard data which can be costly, labor-intensive, orinvasive to obtain. With the rapid development of machine learning (ML),scientists can now employ ML algorithms to predict gold-standard outcomes withvariables that are easier to obtain. However, these predicted outcomes areoften used directly in subsequent statistical analyses, ignoring imprecisionand heterogeneity introduced by the prediction procedure. This will likelyresult in false positive findings and invalid scientific conclusions. In thiswork, we introduce PoSt-Prediction Adaptive inference (PSPA) that allows validand powerful inference based on ML-predicted data. Its "assumption-lean"property guarantees reliable statistical inference without assumptions on theML prediction. Its "data-adaptive" feature guarantees an efficiency gain overexisting methods, regardless of the accuracy of ML prediction. We demonstratethe statistical superiority and broad applicability of our method throughsimulations and real-data applications.</description><author>Jiacheng Miao, Xinran Miao, Yixuan Wu, Jiwei Zhao, Qiongshi Lu</author><pubDate>Mon, 16 Sep 2024 17:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14220v4</guid></item><item><title>DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction</title><link>http://arxiv.org/abs/2409.10504v1</link><description>Predicting high-dimensional or extreme multilabels, such as in medicalcoding, requires both accuracy and interpretability. Existing works often relyon local interpretability methods, failing to provide comprehensiveexplanations of the overall mechanism behind each label prediction within amultilabel set. We propose a mechanistic interpretability module calledDIctionary Label Attention (\method) that disentangles uninterpretable denseembeddings into a sparse embedding space, where each nonzero element (adictionary feature) represents a globally learned medical concept. Throughhuman evaluations, we show that our sparse embeddings are more humanunderstandable than its dense counterparts by at least 50 percent. Ourautomated dictionary feature identification pipeline, leveraging large languagemodels (LLMs), uncovers thousands of learned medical concepts by examining andsummarizing the highest activating tokens for each dictionary feature. Werepresent the relationships between dictionary features and medical codesthrough a sparse interpretable matrix, enhancing the mechanistic and globalunderstanding of the model's predictions while maintaining competitiveperformance and scalability without extensive human annotation.</description><author>John Wu, David Wu, Jimeng Sun</author><pubDate>Mon, 16 Sep 2024 17:45:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10504v1</guid></item><item><title>Manydepth2: Motion-Aware Self-Supervised Monocular Depth Estimation in Dynamic Scenes</title><link>http://arxiv.org/abs/2312.15268v2</link><description>Despite advancements in self-supervised monocular depth estimation,challenges persist in dynamic scenarios due to the dependence on assumptionsabout a static world. In this paper, we present Manydepth2, a Motion-GuidedCost Volume Depth Net, to achieve precise depth estimation for both dynamicobjects and static backgrounds, all while maintaining computational efficiency.To tackle the challenges posed by dynamic content, we incorporate optical flowand coarse monocular depth to create a novel static reference frame. This frameis then utilized to build a motion-guided cost volume in collaboration with thetarget frame. Additionally, to enhance the accuracy and resilience of thenetwork structure, we introduce an attention-based depth net architecture toeffectively integrate information from feature maps with varying resolutions.Compared to methods with similar computational costs, Manydepth2 achieves asignificant reduction of approximately five percent in root-mean-square errorfor self-supervised monocular depth estimation on the KITTI-2015 dataset. Thecode could be found: https://github.com/kaichen-z/Manydepth2</description><author>Kaichen Zhou, Jia-Wang Bian, Qian Xie, Jian-Qing Zheng, Niki Trigoni, Andrew Markham</author><pubDate>Mon, 16 Sep 2024 17:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15268v2</guid></item><item><title>Assessing biomedical knowledge robustness in large language models by query-efficient sampling attacks</title><link>http://arxiv.org/abs/2402.10527v2</link><description>The increasing depth of parametric domain knowledge in large language models(LLMs) is fueling their rapid deployment in real-world applications.Understanding model vulnerabilities in high-stakes and knowledge-intensivetasks is essential for quantifying the trustworthiness of model predictions andregulating their use. The recent discovery of named entities as adversarialexamples (i.e. adversarial entities) in natural language processing tasksraises questions about their potential impact on the knowledge robustness ofpre-trained and finetuned LLMs in high-stakes and specialized domains. Weexamined the use of type-consistent entity substitution as a template forcollecting adversarial entities for billion-parameter LLMs with biomedicalknowledge. To this end, we developed an embedding-space attack based onpowerscaled distance-weighted sampling to assess the robustness of theirbiomedical knowledge with a low query budget and controllable coverage. Ourmethod has favorable query efficiency and scaling over alternative approachesbased on random sampling and blackbox gradient-guided search, which wedemonstrated for adversarial distractor generation in biomedical questionanswering. Subsequent failure mode analysis uncovered two regimes ofadversarial entities on the attack surface with distinct characteristics and weshowed that entity substitution attacks can manipulate token-wise Shapley valueexplanations, which become deceptive in this setting. Our approach complementsstandard evaluations for high-capacity models and the results highlight thebrittleness of domain knowledge in LLMs.</description><author>R. Patrick Xian, Alex J. Lee, Satvik Lolla, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl</author><pubDate>Mon, 16 Sep 2024 17:44:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10527v2</guid></item><item><title>Boundary Attention: Learning curves, corners, junctions and grouping</title><link>http://arxiv.org/abs/2401.00935v3</link><description>We present a lightweight network that infers grouping and boundaries,including curves, corners and junctions. It operates in a bottom-up fashion,analogous to classical methods for sub-pixel edge localization andedge-linking, but with a higher-dimensional representation of local boundarystructure, and notions of local scale and spatial consistency that are learnedinstead of designed. Our network uses a mechanism that we call boundaryattention: a geometry-aware local attention operation that, when applieddensely and repeatedly, progressively refines a pixel-resolution field ofvariables that specify the boundary structure in every overlapping patch withinan image. Unlike many edge detectors that produce rasterized binary edge maps,our model provides a rich, unrasterized representation of the geometricstructure in every local region. We find that its intentional geometric biasallows it to be trained on simple synthetic shapes and then generalize toextracting boundaries from noisy low-light photographs.</description><author>Mia Gaia Polansky, Charles Herrmann, Junhwa Hur, Deqing Sun, Dor Verbin, Todd Zickler</author><pubDate>Mon, 16 Sep 2024 17:42:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00935v3</guid></item><item><title>Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles</title><link>http://arxiv.org/abs/2409.10502v1</link><description>Causal language modeling using the Transformer architecture has yieldedremarkable capabilities in Large Language Models (LLMs) over the last fewyears. However, the extent to which fundamental search and reasoningcapabilities emerged within LLMs remains a topic of ongoing debate. In thiswork, we study if causal language modeling can learn a complex task such assolving Sudoku puzzles. To solve a Sudoku, the model is first required tosearch over all empty cells of the puzzle to decide on a cell to fill and thenapply an appropriate strategy to fill the decided cell. Sometimes, theapplication of a strategy only results in thinning down the possible values ina cell rather than concluding the exact value of the cell. In such cases,multiple strategies are applied one after the other to fill a single cell. Weobserve that Transformer models trained on this synthetic task can indeed learnto solve Sudokus (our model solves $94.21\%$ of the puzzles fully correctly)when trained on a logical sequence of steps taken by a solver. We find thattraining Transformers with the logical sequence of steps is necessary andwithout such training, they fail to learn Sudoku. We also extend our analysisto Zebra puzzles (known as Einstein puzzles) and show that the model solves$92.04 \%$ of the puzzles fully correctly. In addition, we study the internalrepresentations of the trained Transformer and find that through linearprobing, we can decode information about the set of possible values in anygiven cell from them, pointing to the presence of a strong reasoning engineimplicit in the Transformer weights.</description><author>Kulin Shah, Nishanth Dikkala, Xin Wang, Rina Panigrahy</author><pubDate>Mon, 16 Sep 2024 17:42:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10502v1</guid></item><item><title>GMISeg: General Medical Image Segmentation without Re-Training</title><link>http://arxiv.org/abs/2311.12539v5</link><description>Deep learning models have become the dominant method for medical imagesegmentation. However, they often struggle to be generalisable to unknown tasksinvolving new anatomical structures, labels, or shapes. In these cases, themodel needs to be re-trained for the new tasks, posing a significant challengefor non-machine learning experts and requiring a considerable time investment.Here I developed a general model that can solve unknown medical imagesegmentation tasks without requiring additional training. Given an example setof images and visual prompts for defining new segmentation tasks, GMISeg(General Medical Image Segmentation) leverages a pre-trained image encoderbased on ViT and applies a low-rank fine-tuning strategy to the prompt encoderand mask decoder to fine-tune the model without in an efficient manner. Ievaluated the performance of the proposed method on medical image datasets withdifferent imaging modalities and anatomical structures. The proposed methodfacilitated the deployment of pre-trained AI models to new segmentation worksin a user-friendly way.</description><author>Jing Xu</author><pubDate>Mon, 16 Sep 2024 17:41:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12539v5</guid></item><item><title>Partial Distribution Matching via Partial Wasserstein Adversarial Networks</title><link>http://arxiv.org/abs/2409.10499v1</link><description>This paper studies the problem of distribution matching (DM), which is afundamental machine learning problem seeking to robustly align two probabilitydistributions. Our approach is established on a relaxed formulation, calledpartial distribution matching (PDM), which seeks to match a fraction of thedistributions instead of matching them completely. We theoretically derive theKantorovich-Rubinstein duality for the partial Wasserstain-1 (PW) discrepancy,and develop a partial Wasserstein adversarial network (PWAN) that efficientlyapproximates the PW discrepancy based on this dual form. Partial matching canthen be achieved by optimizing the network using gradient descent. Twopractical tasks, point set registration and partial domain adaptation areinvestigated, where the goals are to partially match distributions in 3D spaceand high-dimensional feature space respectively. The experiment results confirmthat the proposed PWAN effectively produces highly robust matching results,performing better or on par with the state-of-the-art methods.</description><author>Zi-Ming Wang, Nan Xue, Ling Lei, Rebecka Jörnsten, Gui-Song Xia</author><pubDate>Mon, 16 Sep 2024 17:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10499v1</guid></item><item><title>MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image Segmentation</title><link>http://arxiv.org/abs/2312.07128v5</link><description>Although transformer is preferred in natural language processing, somestudies has only been applied to the field of medical imaging in recent years.For its long-term dependency, the transformer is expected to contribute tounconventional convolution neural net conquer their inherent spatial inductionbias. The lately suggested transformer-based segmentation method only uses thetransformer as an auxiliary module to help encode the global context into aconvolutional representation. How to optimally integrate self-attention withconvolution has not been investigated in depth. To solve the problem, thispaper proposes MS-Twins (Multi-Scale Twins), which is a powerful segmentationmodel on account of the bond of self-attention and convolution. MS-Twins canbetter capture semantic and fine-grained information by combining differentscales and cascading features. Compared with the existing network structure,MS-Twins has made progress on the previous method based on the transformer oftwo in common use data sets, Synapse and ACDC. In particular, the performanceof MS-Twins on Synapse is 8% higher than SwinUNet. Even compared with nnUNet,the best entirely convoluted medical image segmentation network, theperformance of MS-Twins on Synapse and ACDC still has a bit advantage.</description><author>Jing Xu</author><pubDate>Mon, 16 Sep 2024 17:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07128v5</guid></item><item><title>Security Attacks on LLM-based Code Completion Tools</title><link>http://arxiv.org/abs/2408.11006v2</link><description>The rapid development of large language models (LLMs) has significantlyadvanced code completion capabilities, giving rise to a new generation ofLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, thesetools possess unique workflows, integrating multiple information sources asinput and prioritizing code suggestions over natural language interaction,which introduces distinct security challenges. Additionally, LCCTs often relyon proprietary code datasets for training, raising concerns about the potentialexposure of sensitive data. This paper exploits these distinct characteristicsof LCCTs to develop targeted attack methodologies on two critical securityrisks: jailbreaking and training data extraction attacks. Our experimentalresults expose significant vulnerabilities within LCCTs, including a 99.4%success rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rateon Amazon Q. Furthermore, We successfully extracted sensitive user data fromGitHub Copilot, including 54 real email addresses and 314 physical addressesassociated with GitHub usernames. Our study also demonstrates that thesecode-based attack methods are effective against general-purpose LLMs, such asthe GPT series, highlighting a broader security misalignment in the handling ofcode by modern LLMs. These findings underscore critical security challengesassociated with LCCTs and suggest essential directions for strengthening theirsecurity frameworks. The example code and attack samples from our research areprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.</description><author>Wen Cheng, Ke Sun, Xinyu Zhang, Wei Wang</author><pubDate>Mon, 16 Sep 2024 17:35:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.11006v2</guid></item><item><title>Model-independent variable selection via the rule-based variable priority</title><link>http://arxiv.org/abs/2409.09003v2</link><description>While achieving high prediction accuracy is a fundamental goal in machinelearning, an equally important task is finding a small number of features withhigh explanatory power. One popular selection technique is permutationimportance, which assesses a variable's impact by measuring the change inprediction error after permuting the variable. However, this can be problematicdue to the need to create artificial data, a problem shared by other methods aswell. Another problem is that variable selection methods can be limited bybeing model-specific. We introduce a new model-independent approach, VariablePriority (VarPro), which works by utilizing rules without the need to generateartificial data or evaluate prediction error. The method is relatively easy touse, requiring only the calculation of sample averages of simple statistics,and can be applied to many data settings, including regression, classification,and survival. We investigate the asymptotic properties of VarPro and show,among other things, that VarPro has a consistent filtering property for noisevariables. Empirical studies using synthetic and real-world data show themethod achieves a balanced performance and compares favorably to manystate-of-the-art procedures currently used for variable selection.</description><author>Min Lu, Hemant Ishwaran</author><pubDate>Mon, 16 Sep 2024 17:34:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.09003v2</guid></item><item><title>MusicLIME: Explainable Multimodal Music Understanding</title><link>http://arxiv.org/abs/2409.10496v1</link><description>Multimodal models are critical for music understanding tasks, as they capturethe complex interplay between audio and lyrics. However, as these models becomemore prevalent, the need for explainability grows-understanding how thesesystems make decisions is vital for ensuring fairness, reducing bias, andfostering trust. In this paper, we introduce MusicLIME, a model-agnosticfeature importance explanation method designed for multimodal music models.Unlike traditional unimodal methods, which analyze each modality separatelywithout considering the interaction between them, often leading to incompleteor misleading explanations, MusicLIME reveals how audio and lyrical featuresinteract and contribute to predictions, providing a holistic view of themodel's decision-making. Additionally, we enhance local explanations byaggregating them into global explanations, giving users a broader perspectiveof model behavior. Through this work, we contribute to improving theinterpretability of multimodal music models, empowering users to make informedchoices, and fostering more equitable, fair, and transparent musicunderstanding systems.</description><author>Theodoros Sotirou, Vassilis Lyberatos, Orfeas Menis Mastromichalakis, Giorgos Stamou</author><pubDate>Mon, 16 Sep 2024 17:28:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10496v1</guid></item><item><title>Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation</title><link>http://arxiv.org/abs/2409.10494v1</link><description>This paper presents a diffusion-based recommender system that incorporatesclassifier-free guidance. Most current recommender systems providerecommendations using conventional methods such as collaborative orcontent-based filtering. Diffusion is a new approach to generative AI thatimproves on previous generative AI approaches such as Variational Autoencoders(VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion ina recommender system that mirrors the sequence users take when browsing andrating items. Although a few current recommender systems incorporate diffusion,they do not incorporate classifier-free guidance, a new innovation in diffusionmodels as a whole. In this paper, we present a diffusion recommender systemthat augments the underlying recommender system model for improved performanceand also incorporates classifier-free guidance. Our findings show improvementsover state-of-the-art recommender systems for most metrics for severalrecommendation tasks on a variety of datasets. In particular, our approachdemonstrates the potential to provide better recommendations when data issparse.</description><author>Noah Buchanan, Susan Gauch, Quan Mai</author><pubDate>Mon, 16 Sep 2024 17:27:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10494v1</guid></item><item><title>Flash STU: Fast Spectral Transform Units</title><link>http://arxiv.org/abs/2409.10489v1</link><description>This paper describes an efficient, open source PyTorch implementation of theSpectral Transform Unit. We investigate sequence prediction tasks over severalmodalities including language, robotics, and simulated dynamical systems. Wefind that for the same parameter count, the STU and its variants outperform theTransformer as well as other leading state space models across variousmodalities.</description><author>Y. Isabel Liu, Windsor Nguyen, Yagiz Devre, Evan Dogariu, Anirudha Majumdar, Elad Hazan</author><pubDate>Mon, 16 Sep 2024 17:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10489v1</guid></item><item><title>Do Pre-trained Vision-Language Models Encode Object States?</title><link>http://arxiv.org/abs/2409.10488v1</link><description>For a vision-language model (VLM) to understand the physical world, such ascause and effect, a first step is to capture the temporal dynamics of thevisual world, for example how the physical states of objects evolve over time(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMspre-trained on web-scale data learn to encode object states, which can beextracted with zero-shot text prompts. We curate an object state recognitiondataset ChangeIt-Frames, and evaluate nine open-source VLMs, including modelstrained with contrastive and generative objectives. We observe that while thesestate-of-the-art vision-language models can reliably perform objectrecognition, they consistently fail to accurately distinguish the objects'physical states. Through extensive experiments, we identify three areas forimprovements for VLMs to better encode object states, namely the quality ofobject localization, the architecture to bind concepts to objects, and theobjective to learn discriminative visual and language encoders on objectstates. Data and code are released.</description><author>Kaleb Newman, Shijie Wang, Yuan Zang, David Heffren, Chen Sun</author><pubDate>Mon, 16 Sep 2024 17:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10488v1</guid></item><item><title>Schrodinger's Memory: Large Language Models</title><link>http://arxiv.org/abs/2409.10482v1</link><description>Memory is the foundation of LLMs' functionality, yet past research has lackedan in-depth exploration of their memory capabilities and underlying theory. Inthis paper, we apply UAT theory to explain the memory mechanism of LLMs andpropose a new approach for evaluating LLM performance by comparing the memorycapacities of different models. Through extensive experiments, we validate ourtheory and the memory abilities of LLMs. Finally, we compare the capabilitiesof the human brain and LLMs, highlighting both their similarities anddifferences in terms of working mechanisms.</description><author>Wei Wang, Qing Li</author><pubDate>Mon, 16 Sep 2024 17:18:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10482v1</guid></item><item><title>Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance</title><link>http://arxiv.org/abs/2409.10481v1</link><description>3D face reconstruction (3DFR) algorithms are based on specific assumptionstailored to distinct application scenarios. These assumptions limit their usewhen acquisition conditions, such as the subject's distance from the camera orthe camera's characteristics, are different than expected, as typically happensin video surveillance. Additionally, 3DFR algorithms follow various strategiesto address the reconstruction of a 3D shape from 2D data, such as statisticalmodel fitting, photometric stereo, or deep learning. In the present study, weexplore the application of three 3DFR algorithms representative of the SOTA,employing each one as the template set generator for a face verificationsystem. The scores provided by each system are combined by score-level fusion.We show that the complementarity induced by different 3DFR algorithms improvesperformance when tests are conducted at never-seen-before distances from thecamera and camera characteristics (cross-distance and cross-camera settings),thus encouraging further investigations on multiple 3DFR-based approaches.</description><author>Simone Maurizio La Cava, Sara Concas, Ruben Tolosana, Roberto Casula, Giulia Orrù, Martin Drahansky, Julian Fierrez, Gian Luca Marcialis</author><pubDate>Mon, 16 Sep 2024 17:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10481v1</guid></item><item><title>Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2406.14891v2</link><description>Multi-Hop Question Answering (MHQA) tasks present a significant challenge forlarge language models (LLMs) due to the intensive knowledge required. Currentsolutions, like Retrieval-Augmented Generation, typically retrieve potentialdocuments from an external corpus to read an answer. However, the performanceof this retrieve-then-read paradigm is constrained by the retriever and theinevitable noise in the retrieved documents. To mitigate these challenges, weintroduce a novel generate-then-ground (GenGround) framework, synergizing theparametric knowledge of LLMs and external documents to solve a multi-hopquestion. GenGround empowers LLMs to alternate two phases until the finalanswer is derived: (1) formulate a simpler, single-hop question and directlygenerate the answer; (2) ground the question-answer pair in retrieveddocuments, amending any wrong predictions in the answer. We also propose aninstructional grounding distillation method to generalize our method intosmaller models. Extensive experiments conducted on four datasets illustrate thesuperiority of our method.</description><author>Zhengliang Shi, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, Zhaochun Ren</author><pubDate>Mon, 16 Sep 2024 17:15:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14891v2</guid></item><item><title>SimInversion: A Simple Framework for Inversion-Based Text-to-Image Editing</title><link>http://arxiv.org/abs/2409.10476v1</link><description>Diffusion models demonstrate impressive image generation performance withtext guidance. Inspired by the learning process of diffusion, existing imagescan be edited according to text by DDIM inversion. However, the vanilla DDIMinversion is not optimized for classifier-free guidance and the accumulatederror will result in the undesired performance. While many algorithms aredeveloped to improve the framework of DDIM inversion for editing, in this work,we investigate the approximation error in DDIM inversion and propose todisentangle the guidance scale for the source and target branches to reduce theerror while keeping the original framework. Moreover, a better guidance scale(i.e., 0.5) than default settings can be derived theoretically. Experiments onPIE-Bench show that our proposal can improve the performance of DDIM inversiondramatically without sacrificing efficiency.</description><author>Qi Qian, Haiyang Xu, Ming Yan, Juhua Hu</author><pubDate>Mon, 16 Sep 2024 17:10:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10476v1</guid></item><item><title>MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion</title><link>http://arxiv.org/abs/2409.10473v1</link><description>Self-supervised learning has proved effective for skeleton-based human actionunderstanding. However, previous works either rely on contrastive learning thatsuffers false negative problems or are based on reconstruction that learns toomuch unessential low-level clues, leading to limited representations fordownstream tasks. Recently, great advances have been made in generativelearning, which is naturally a challenging yet meaningful pretext task to modelthe general underlying data distributions. However, the representation learningcapacity of generative models is under-explored, especially for the skeletonswith spacial sparsity and temporal redundancy. To this end, we propose MaskedConditional Diffusion (MacDiff) as a unified framework for human skeletonmodeling. For the first time, we leverage diffusion models as effectiveskeleton representation learners. Specifically, we train a diffusion decoderconditioned on the representations extracted by a semantic encoder. Randommasking is applied to encoder inputs to introduce a information bottleneck andremove redundancy of skeletons. Furthermore, we theoretically demonstrate thatour generative objective involves the contrastive learning objective whichaligns the masked and noisy views. Meanwhile, it also enforces therepresentation to complement for the noisy view, leading to bettergeneralization performance. MacDiff achieves state-of-the-art performance onrepresentation learning benchmarks while maintaining the competence forgenerative tasks. Moreover, we leverage the diffusion model for dataaugmentation, significantly enhancing the fine-tuning performance in scenarioswith scarce labeled data. Our project is available athttps://lehongwu.github.io/ECCV24MacDiff/.</description><author>Lehong Wu, Lilang Lin, Jiahang Zhang, Yiyang Ma, Jiaying Liu</author><pubDate>Mon, 16 Sep 2024 17:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10473v1</guid></item><item><title>Online Nonconvex Bilevel Optimization with Bregman Divergences</title><link>http://arxiv.org/abs/2409.10470v1</link><description>Bilevel optimization methods are increasingly relevant within machinelearning, especially for tasks such as hyperparameter optimization andmeta-learning. Compared to the offline setting, online bilevel optimization(OBO) offers a more dynamic framework by accommodating time-varying functionsand sequentially arriving data. This study addresses the onlinenonconvex-strongly convex bilevel optimization problem. In deterministicsettings, we introduce a novel online Bregman bilevel optimizer (OBBO) thatutilizes adaptive Bregman divergences. We demonstrate that OBBO enhances theknown sublinear rates for bilevel local regret through a novel hypergradienterror decomposition that adapts to the underlying geometry of the problem. Instochastic contexts, we introduce the first stochastic online bilevel optimizer(SOBBO), which employs a window averaging method for updating outer-levelvariables using a weighted average of recent stochastic approximations ofhypergradients. This approach not only achieves sublinear rates of bilevellocal regret but also serves as an effective variance reduction strategy,obviating the need for additional stochastic gradient samples at each timestep.Experiments on online hyperparameter optimization and online meta-learninghighlight the superior performance, efficiency, and adaptability of ourBregman-based algorithms compared to established online and offline bilevelbenchmarks.</description><author>Jason Bohne, David Rosenberg, Gary Kazantsev, Pawel Polak</author><pubDate>Mon, 16 Sep 2024 17:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10470v1</guid></item><item><title>Kolmogorov-Arnold Networks in Low-Data Regimes: A Comparative Study with Multilayer Perceptrons</title><link>http://arxiv.org/abs/2409.10463v1</link><description>Multilayer Perceptrons (MLPs) have long been a cornerstone in deep learning,known for their capacity to model complex relationships. Recently,Kolmogorov-Arnold Networks (KANs) have emerged as a compelling alternative,utilizing highly flexible learnable activation functions directly on networkedges, a departure from the neuron-centric approach of MLPs. However, KANssignificantly increase the number of learnable parameters, raising concernsabout their effectiveness in data-scarce environments. This paper presents acomprehensive comparative study of MLPs and KANs from both algorithmic andexperimental perspectives, with a focus on low-data regimes. We introduce aneffective technique for designing MLPs with unique, parameterized activationfunctions for each neuron, enabling a more balanced comparison with KANs. Usingempirical evaluations on simulated data and two real-world data sets frommedicine and engineering, we explore the trade-offs between model complexityand accuracy, with particular attention to the role of network depth. Ourfindings show that MLPs with individualized activation functions achievesignificantly higher predictive accuracy with only a modest increase inparameters, especially when the sample size is limited to around one hundred.For example, in a three-class classification problem within additivemanufacturing, MLPs achieve a median accuracy of 0.91, significantlyoutperforming KANs, which only reach a median accuracy of 0.53 with defaulthyperparameters. These results offer valuable insights into the impact ofactivation function selection in neural networks.</description><author>Farhad Pourkamali-Anaraki</author><pubDate>Mon, 16 Sep 2024 16:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10463v1</guid></item><item><title>PMT-MAE: Dual-Branch Self-Supervised Learning with Distillation for Efficient Point Cloud Classification</title><link>http://arxiv.org/abs/2409.02007v2</link><description>Advances in self-supervised learning are essential for enhancing featureextraction and understanding in point cloud processing. This paper introducesPMT-MAE (Point MLP-Transformer Masked Autoencoder), a novel self-supervisedlearning framework for point cloud classification. PMT-MAE features adual-branch architecture that integrates Transformer and MLP components tocapture rich features. The Transformer branch leverages global self-attentionfor intricate feature interactions, while the parallel MLP branch processestokens through shared fully connected layers, offering a complementary featuretransformation pathway. A fusion mechanism then combines these features,enhancing the model's capacity to learn comprehensive 3D representations.Guided by the sophisticated teacher model Point-M2AE, PMT-MAE employs adistillation strategy that includes feature distillation during pre-trainingand logit distillation during fine-tuning, ensuring effective knowledgetransfer. On the ModelNet40 classification task, achieving an accuracy of93.6\% without employing voting strategy, PMT-MAE surpasses the baselinePoint-MAE (93.2\%) and the teacher Point-M2AE (93.4\%), underscoring itsability to learn discriminative 3D point cloud representations. Additionally,this framework demonstrates high efficiency, requiring only 40 epochs for bothpre-training and fine-tuning. PMT-MAE's effectiveness and efficiency render itwell-suited for scenarios with limited computational resources, positioning itas a promising solution for practical point cloud analysis.</description><author>Qiang Zheng, Chao Zhang, Jian Sun</author><pubDate>Mon, 16 Sep 2024 16:51:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02007v2</guid></item><item><title>PointMT: Efficient Point Cloud Analysis with Hybrid MLP-Transformer Architecture</title><link>http://arxiv.org/abs/2408.05508v2</link><description>In recent years, point cloud analysis methods based on the Transformerarchitecture have made significant progress, particularly in the context ofmultimedia applications such as 3D modeling, virtual reality, and autonomoussystems. However, the high computational resource demands of the Transformerarchitecture hinder its scalability, real-time processing capabilities, anddeployment on mobile devices and other platforms with limited computationalresources. This limitation remains a significant obstacle to its practicalapplication in scenarios requiring on-device intelligence and multimediaprocessing. To address this challenge, we propose an efficient point cloudanalysis architecture, \textbf{Point} \textbf{M}LP-\textbf{T}ransformer(PointMT). This study tackles the quadratic complexity of the self-attentionmechanism by introducing a linear complexity local attention mechanism foreffective feature aggregation. Additionally, to counter the Transformer's focuson token differences while neglecting channel differences, we introduce aparameter-free channel temperature adaptation mechanism that adaptively adjuststhe attention weight distribution in each channel, enhancing the precision offeature aggregation. To improve the Transformer's slow convergence speed due tothe limited scale of point cloud datasets, we propose an MLP-Transformer hybridmodule, which significantly enhances the model's convergence speed.Furthermore, to boost the feature representation capability of point tokens, werefine the classification head, enabling point tokens to directly participatein prediction. Experimental results on multiple evaluation benchmarksdemonstrate that PointMT achieves performance comparable to state-of-the-artmethods while maintaining an optimal balance between performance and accuracy.</description><author>Qiang Zheng, Chao Zhang, Jian Sun</author><pubDate>Mon, 16 Sep 2024 16:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05508v2</guid></item><item><title>Can GPT-3.5 Generate and Code Discharge Summaries?</title><link>http://arxiv.org/abs/2401.13512v2</link><description>Objective: To investigate GPT-3.5 in generating and coding medical documentswith ICD-10 codes for data augmentation on low-resources labels. Materials and Methods: Employing GPT-3.5 we generated and coded 9,606discharge summaries based on lists of ICD-10 code descriptions of patients withinfrequent (generation) codes within the MIMIC-IV dataset. Combined with thebaseline training set, this formed an augmented training set. Neural codingmodels were trained on baseline and augmented data and evaluated on a MIMIC-IVtest set. We report micro- and macro-F1 scores on the full codeset, generationcodes, and their families. Weak Hierarchical Confusion Matrices were employedto determine within-family and outside-of-family coding errors in the lattercodesets. The coding performance of GPT-3.5 was evaluated both on prompt-guidedself-generated data and real MIMIC-IV data. Clinical professionals evaluatedthe clinical acceptability of the generated documents. Results: Augmentation slightly hinders the overall performance of the modelsbut improves performance for the generation candidate codes and their families,including one unseen in the baseline training data. Augmented models displaylower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by theprompted descriptions, but performs poorly on real data. Evaluators note thecorrectness of generated concepts while suffering in variety, supportinginformation, and narrative. Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding.Augmentation positively affects generation code families but mainly benefitscodes with existing examples. Augmentation reduces out-of-family errors.Discharge summaries generated by GPT-3.5 state prompted concepts correctly butlack variety, and authenticity in narratives. They are unsuitable for clinicalpractice.</description><author>Matúš Falis, Aryo Pradipta Gema, Hang Dong, Luke Daines, Siddharth Basetti, Michael Holder, Rose S Penfold, Alexandra Birch, Beatrice Alex</author><pubDate>Mon, 16 Sep 2024 16:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13512v2</guid></item><item><title>Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting</title><link>http://arxiv.org/abs/2211.15856v5</link><description>Producing high-quality forecasts of key climate variables, such astemperature and precipitation, on subseasonal time scales has long been a gapin operational forecasting. This study explores an application of machinelearning (ML) models as post-processing tools for subseasonal forecasting.Lagged numerical ensemble forecasts (i.e., an ensemble where the members havedifferent initialization dates) and observational data, including relativehumidity, pressure at sea level, and geopotential height, are incorporated intovarious ML methods to predict monthly average precipitation and two-metertemperature two weeks in advance for the continental United States. Forregression, quantile regression, and tercile classification tasks, we considerusing linear models, random forests, convolutional neural networks, and stackedmodels (a multi-model approach based on the prediction of the individual MLmodels). Unlike previous ML approaches that often use ensemble mean alone, weleverage information embedded in the ensemble forecasts to enhance predictionaccuracy. Additionally, we investigate extreme event predictions that arecrucial for planning and mitigation efforts. Considering ensemble members as acollection of spatial forecasts, we explore different approaches to usingspatial information. Trade-offs between different approaches may be mitigatedwith model stacking. Our proposed models outperform standard baselines such asclimatological forecasts and ensemble means. In addition, we investigatefeature importance, trade-offs between using the full ensemble or only theensemble mean, and different modes of accounting for spatial variability.</description><author>Elena Orlova, Haokun Liu, Raphael Rossellini, Benjamin A. Cash, Rebecca Willett</author><pubDate>Mon, 16 Sep 2024 16:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15856v5</guid></item><item><title>Signed Graph Autoencoder for Explainable and Polarization-Aware Network Embeddings</title><link>http://arxiv.org/abs/2409.10452v1</link><description>Autoencoders based on Graph Neural Networks (GNNs) have garnered significantattention in recent years for their ability to extract informative latentrepresentations, characterizing the structure of complex topologies, such asgraphs. Despite the prevalence of Graph Autoencoders, there has been limitedfocus on developing and evaluating explainable neural-based graph generativemodels specifically designed for signed networks. To address this gap, wepropose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAEextracts node-level representations that express node memberships over distinctextreme profiles, referred to as archetypes, within the network. This isachieved by projecting the graph onto a learned polytope, which governs itspolarization. The framework employs a recently proposed likelihood foranalyzing signed networks based on the Skellam distribution, combined withrelational archetypal analysis and GNNs. Our experimental evaluationdemonstrates the SGAAEs' capability to successfully infer node memberships overthe different underlying latent structures while extracting competingcommunities formed through the participation of the opposing views in thenetwork. Additionally, we introduce the 2-level network polarization problemand show how SGAAE is able to characterize such a setting. The proposed modelachieves high performance in different tasks of signed link prediction acrossfour real-world datasets, outperforming several baseline models.</description><author>Nikolaos Nakis, Chrysoula Kosma, Giannis Nikolentzos, Michalis Chatzianastasis, Iakovos Evdaimon, Michalis Vazirgiannis</author><pubDate>Mon, 16 Sep 2024 16:40:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10452v1</guid></item><item><title>Scalable Distributed Algorithms for Size-Constrained Submodular Maximization in the MapReduce and Adaptive Complexity Models</title><link>http://arxiv.org/abs/2206.09563v6</link><description>Distributed maximization of a submodular function in the MapReduce (MR) modelhas received much attention, culminating in two frameworks that allow acentralized algorithm to be run in the MR setting without loss ofapproximation, as long as the centralized algorithm satisfies a certainconsistency property -- which had previously only been known to be satisfied bythe standard greedy and continous greedy algorithms. A separate line of workhas studied parallelizability of submodular maximization in the adaptivecomplexity model, where each thread may have access to the entire ground set.For the size-constrained maximization of a monotone and submodular function, weshow that several sublinearly adaptive (highly parallelizable) algorithmssatisfy the consistency property required to work in the MR setting, whichyields practical, parallelizable and distributed algorithms. Separately, wedevelop the first distributed algorithm with linear query complexity for thisproblem. Finally, we provide a method to increase the maximum cardinalityconstraint for MR algorithms at the cost of additional MR rounds.</description><author>Yixin Chen, Tonmoy Dey, Alan Kuhnle</author><pubDate>Mon, 16 Sep 2024 16:39:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.09563v6</guid></item><item><title>Local Methods with Adaptivity via Scaling</title><link>http://arxiv.org/abs/2406.00846v3</link><description>The rapid development of machine learning and deep learning has introducedincreasingly complex optimization challenges that must be addressed. Indeed,training modern, advanced models has become difficult to implement withoutleveraging multiple computing nodes in a distributed environment. Distributedoptimization is also fundamental to emerging fields such as federated learning.Specifically, there is a need to organize the training process to minimize thetime lost due to communication. A widely used and extensively researchedtechnique to mitigate the communication bottleneck involves performing localtraining before communication. This approach is the focus of our paper.Concurrently, adaptive methods that incorporate scaling, notably led by Adam,have gained significant popularity in recent years. Therefore, this paper aimsto merge the local training technique with the adaptive approach to developefficient distributed learning methods. We consider the classical Local SGDmethod and enhance it with a scaling feature. A crucial aspect is that thescaling is described generically, allowing us to analyze various approaches,including Adam, RMSProp, and OASIS, in a unified manner. In addition totheoretical analysis, we validate the performance of our methods in practice bytraining a neural network.</description><author>Savelii Chezhegov, Sergey Skorik, Nikolas Khachaturov, Danil Shalagin, Aram Avetisyan, Martin Takáč, Yaroslav Kholodov, Aleksandr Beznosikov</author><pubDate>Mon, 16 Sep 2024 16:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.00846v3</guid></item><item><title>Deep-Wide Learning Assistance for Insect Pest Classification</title><link>http://arxiv.org/abs/2409.10445v1</link><description>Accurate insect pest recognition plays a critical role in agriculture. It isa challenging problem due to the intricate characteristics of insects. In thispaper, we present DeWi, novel learning assistance for insect pestclassification. With a one-stage and alternating training strategy, DeWisimultaneously improves several Convolutional Neural Networks in twoperspectives: discrimination (by optimizing a triplet margin loss in asupervised training manner) and generalization (via data augmentation). Fromthat, DeWi can learn discriminative and in-depth features of insect pests(deep) yet still generalize well to a large number of insect categories (wide).Experimental results show that DeWi achieves the highest performances on twoinsect pest classification benchmarks (76.44\% accuracy on the IP102 datasetand 99.79\% accuracy on the D0 dataset, respectively). In addition, extensiveevaluations and ablation studies are conducted to thoroughly investigate ourDeWi and demonstrate its superiority. Our source code is available athttps://github.com/toannguyen1904/DeWi.</description><author>Toan Nguyen, Huy Nguyen, Huy Ung, Hieu Ung, Binh Nguyen</author><pubDate>Mon, 16 Sep 2024 16:29:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10445v1</guid></item><item><title>Do Prompts Really Prompt? Exploring the Prompt Understanding Capability of Whisper</title><link>http://arxiv.org/abs/2406.05806v4</link><description>This research explores how the information of prompts interacts with thehigh-performing speech recognition model, Whisper. We compare its performanceswhen prompted by prompts with correct information and those corrupted withincorrect information. Our results unexpectedly show that Whisper may notunderstand the textual prompts in a human-expected way. Additionally, we findthat performance improvement is not guaranteed even with stronger adherence tothe topic information in textual prompts. It is also noted that English promptsgenerally outperform Mandarin ones on datasets of both languages, likely due todifferences in training data distributions for these languages despite themismatch with pre-training scenarios. Conversely, we discover that Whisperexhibits awareness of misleading information in language tokens by ignoringincorrect language tokens and focusing on the correct ones. In sum, We raiseinsightful questions about Whisper's prompt understanding and reveal itscounter-intuitive behaviors. We encourage further studies.</description><author>Chih-Kai Yang, Kuan-Po Huang, Hung-yi Lee</author><pubDate>Mon, 16 Sep 2024 16:26:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05806v4</guid></item><item><title>CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera</title><link>http://arxiv.org/abs/2409.10441v1</link><description>Camera-to-robot calibration is crucial for vision-based robot control andrequires effort to make it accurate. Recent advancements in markerless poseestimation methods have eliminated the need for time-consuming physical setupsfor camera-to-robot calibration. While the existing markerless pose estimationmethods have demonstrated impressive accuracy without the need for cumbersomesetups, they rely on the assumption that all the robot joints are visiblewithin the camera's field of view. However, in practice, robots usually move inand out of view, and some portion of the robot may stay out-of-frame during thewhole manipulation task due to real-world constraints, leading to a lack ofsufficient visual features and subsequent failure of these approaches. Toaddress this challenge and enhance the applicability to vision-based robotcontrol, we propose a novel framework capable of estimating the robot pose withpartially visible robot manipulators. Our approach leverages theVision-Language Models for fine-grained robot components detection, andintegrates it into a keypoint-based pose estimation network, which enables morerobust performance in varied operational conditions. The framework is evaluatedon both public robot datasets and self-collected partial-view datasets todemonstrate our robustness and generalizability. As a result, this method iseffective for robot pose estimation in a wider range of real-world manipulationscenarios.</description><author>Jingpei Lu, Zekai Liang, Tristin Xie, Florian Ritcher, Shan Lin, Sainan Liu, Michael C. Yip</author><pubDate>Mon, 16 Sep 2024 16:22:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10441v1</guid></item><item><title>Discrete Neural Algorithmic Reasoning</title><link>http://arxiv.org/abs/2402.11628v2</link><description>Neural algorithmic reasoning aims to capture computations with neuralnetworks via learning the models to imitate the execution of classicalgorithms. While common architectures are expressive enough to contain thecorrect model in the weights space, current neural reasoners are struggling togeneralize well on out-of-distribution data. On the other hand, classiccomputations are not affected by distributional shifts as they can be describedas transitions between discrete computational states. In this work, we proposeto force neural reasoners to maintain the execution trajectory as a combinationof finite predefined states. To achieve that, we separate discrete andcontinuous data flows and describe the interaction between them. Trained withsupervision on the algorithm's state transitions, such models are able toperfectly align with the original algorithm. To show this, we evaluate ourapproach on multiple algorithmic problems and get perfect test scores both insingle-task and multitask setups. Moreover, the proposed architectural choiceallows us to prove the correctness of the learned algorithms for any test~data.</description><author>Gleb Rodionov, Liudmila Prokhorenkova</author><pubDate>Mon, 16 Sep 2024 16:22:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11628v2</guid></item><item><title>Structure-preserving learning for multi-symplectic PDEs</title><link>http://arxiv.org/abs/2409.10432v1</link><description>This paper presents an energy-preserving machine learning method forinferring reduced-order models (ROMs) by exploiting the multi-symplectic formof partial differential equations (PDEs). The vast majority ofenergy-preserving reduced-order methods use symplectic Galerkin projection toconstruct reduced-order Hamiltonian models by projecting the full models onto asymplectic subspace. However, symplectic projection requires the existence offully discrete operators, and in many cases, such as black-box PDE solvers,these operators are inaccessible. In this work, we propose an energy-preservingmachine learning method that can infer the dynamics of the given PDE using dataonly, so that the proposed framework does not depend on the fully discreteoperators. In this context, the proposed method is non-intrusive. The proposedmethod is grey box in the sense that it requires only some basic knowledge ofthe multi-symplectic model at the partial differential equation level. We provethat the proposed method satisfies spatially discrete local energy conservationand preserves the multi-symplectic conservation laws. We test our method on thelinear wave equation, the Korteweg-de Vries equation, and theZakharov-Kuznetsov equation. We test the generalization of our learned modelsby testing them far outside the training time interval.</description><author>Süleyman Yıldız, Pawan Goyal, Peter Benner</author><pubDate>Mon, 16 Sep 2024 16:07:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10432v1</guid></item><item><title>VideoStudio: Generating Consistent-Content and Multi-Scene Videos</title><link>http://arxiv.org/abs/2401.01256v2</link><description>The recent innovations and breakthroughs in diffusion models havesignificantly expanded the possibilities of generating high-quality videos forthe given prompts. Most existing works tackle the single-scene scenario withonly one video event occurring in a single background. Extending to generatemulti-scene videos nevertheless is not trivial and necessitates to nicelymanage the logic in between while preserving the consistent visual appearanceof key content across video scenes. In this paper, we propose a novelframework, namely VideoStudio, for consistent-content and multi-scene videogeneration. Technically, VideoStudio leverages Large Language Models (LLM) toconvert the input prompt into comprehensive multi-scene script that benefitsfrom the logical knowledge learnt by LLM. The script for each scene includes aprompt describing the event, the foreground/background entities, as well ascamera movement. VideoStudio identifies the common entities throughout thescript and asks LLM to detail each entity. The resultant entity description isthen fed into a text-to-image model to generate a reference image for eachentity. Finally, VideoStudio outputs a multi-scene video by generating eachscene video via a diffusion process that takes the reference images, thedescriptive prompt of the event and camera movement into account. The diffusionmodel incorporates the reference images as the condition and alignment tostrengthen the content consistency of multi-scene videos. Extensive experimentsdemonstrate that VideoStudio outperforms the SOTA video generation models interms of visual quality, content consistency, and user preference. Source codeis available at \url{https://github.com/FuchenUSTC/VideoStudio}.</description><author>Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei</author><pubDate>Mon, 16 Sep 2024 16:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01256v2</guid></item><item><title>Meta-Whisper: Speech-Based Meta-ICL for ASR on Low-Resource Languages</title><link>http://arxiv.org/abs/2409.10429v1</link><description>This paper presents Meta-Whisper, a novel approach to improve automaticspeech recognition (ASR) for low-resource languages using the Whisper model. Byleveraging Meta In-Context Learning (Meta-ICL) and a k-Nearest Neighbors (KNN)algorithm for sample selection, Meta-Whisper enhances Whisper's ability torecognize speech in unfamiliar languages without extensive fine-tuning.Experiments on the ML-SUPERB dataset show that Meta-Whisper significantlyreduces the Character Error Rate (CER) for low-resource languages compared tothe original Whisper model. This method offers a promising solution fordeveloping more adaptable multilingual ASR systems, particularly for languageswith limited resources.</description><author>Ming-Hao Hsu, Kuan Po Huang, Hung-yi Lee</author><pubDate>Mon, 16 Sep 2024 16:04:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10429v1</guid></item><item><title>Towards Human-Like Driving: Active Inference in Autonomous Vehicle Control</title><link>http://arxiv.org/abs/2407.07684v2</link><description>This paper presents a novel approach to Autonomous Vehicle (AV) controlthrough the application of active inference, a theory derived from neurosciencethat conceptualizes the brain as a predictive machine. Traditional autonomousdriving systems rely heavily on Modular Pipelines, Imitation Learning, orReinforcement Learning, each with inherent limitations in adaptability,generalization, and computational efficiency. Active inference addresses thesechallenges by minimizing prediction error (termed "surprise") through a dynamicmodel that balances perception and action. Our method integrates activeinference with deep learning to manage lateral control in AVs, enabling them toperform lane following maneuvers within a simulated urban environment. Wedemonstrate that our model, despite its simplicity, effectively learns andgeneralizes from limited data without extensive retraining, significantlyreducing computational demands. The proposed approach not only enhances theadaptability and performance of AVs in dynamic scenarios but also alignsclosely with human-like driving behavior, leveraging a generative model topredict and adapt to environmental changes. Results from extensive experimentsin the CARLA simulator show promising outcomes, outperforming traditionalmethods in terms of adaptability and efficiency, thereby advancing thepotential of active inference in real-world autonomous driving applications.</description><author>Elahe Delavari, John Moore, Junho Hong, Jaerock Kwon</author><pubDate>Mon, 16 Sep 2024 16:02:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07684v2</guid></item><item><title>DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance</title><link>http://arxiv.org/abs/2312.03018v4</link><description>Image-to-video generation, which aims to generate a video starting from agiven reference image, has drawn great attention. Existing methods try toextend pre-trained text-guided image diffusion models to image-guided videogeneration models. Nevertheless, these methods often result in either lowfidelity or flickering over time due to their limitation to shallow imageguidance and poor temporal consistency. To tackle these problems, we propose ahigh-fidelity image-to-video generation method by devising a frame retentionbranch based on a pre-trained video diffusion model, named DreamVideo. Insteadof integrating the reference image into the diffusion process at a semanticlevel, our DreamVideo perceives the reference image via convolution layers andconcatenates the features with the noisy latents as model input. By this means,the details of the reference image can be preserved to the greatest extent. Inaddition, by incorporating double-condition classifier-free guidance, a singleimage can be directed to videos of different actions by providing varyingprompt texts. This has significant implications for controllable videogeneration and holds broad application prospects. We conduct comprehensiveexperiments on the public dataset, and both quantitative and qualitativeresults indicate that our method outperforms the state-of-the-art method.Especially for fidelity, our model has a powerful image retention ability anddelivers the best results in UCF101 compared to other image-to-video models toour best knowledge. Also, precise control can be achieved by giving differenttext prompts. Further details and comprehensive results of our model will bepresented in https://anonymous0769.github.io/DreamVideo/.</description><author>Cong Wang, Jiaxi Gu, Panwen Hu, Songcen Xu, Hang Xu, Xiaodan Liang</author><pubDate>Mon, 16 Sep 2024 16:02:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.03018v4</guid></item><item><title>EasyControl: Transfer ControlNet to Video Diffusion for Controllable Generation and Interpolation</title><link>http://arxiv.org/abs/2408.13005v2</link><description>Following the advancements in text-guided image generation technologyexemplified by Stable Diffusion, video generation is gaining increasedattention in the academic community. However, relying solely on text guidancefor video generation has serious limitations, as videos contain much richercontent than images, especially in terms of motion. This information can hardlybe adequately described with plain text. Fortunately, in computer vision,various visual representations can serve as additional control signals to guidegeneration. With the help of these signals, video generation can be controlledin finer detail, allowing for greater flexibility for different applications.Integrating various controls, however, is nontrivial. In this paper, we proposea universal framework called EasyControl. By propagating and injectingcondition features through condition adapters, our method enables users tocontrol video generation with a single condition map. With our framework,various conditions including raw pixels, depth, HED, etc., can be integratedinto different Unet-based pre-trained video diffusion models at a low practicalcost. We conduct comprehensive experiments on public datasets, and bothquantitative and qualitative results indicate that our method outperformsstate-of-the-art methods. EasyControl significantly improves various evaluationmetrics across multiple validation datasets compared to previous works.Specifically, for the sketch-to-video generation task, EasyControl achieves animprovement of 152.0 on FVD and 19.9 on IS, respectively, in UCF101 comparedwith VideoComposer. For fidelity, our model demonstrates powerful imageretention ability, resulting in high FVD and IS in UCF101 and MSR-VTT comparedto other image-to-video models.</description><author>Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</author><pubDate>Mon, 16 Sep 2024 15:56:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13005v2</guid></item><item><title>Learning Semi-Supervised Medical Image Segmentation from Spatial Registration</title><link>http://arxiv.org/abs/2409.10422v1</link><description>Semi-supervised medical image segmentation has shown promise in trainingmodels with limited labeled data and abundant unlabeled data. However,state-of-the-art methods ignore a potentially valuable source of unsupervisedsemantic information -- spatial registration transforms between image volumes.To address this, we propose CCT-R, a contrastive cross-teaching frameworkincorporating registration information. To leverage the semantic informationavailable in registrations between volume pairs, CCT-R incorporates twoproposed modules: Registration Supervision Loss (RSL) and Registration-EnhancedPositive Sampling (REPS). The RSL leverages segmentation knowledge derived fromtransforms between labeled and unlabeled volume pairs, providing an additionalsource of pseudo-labels. REPS enhances contrastive learning by identifyinganatomically-corresponding positives across volumes using registrationtransforms. Experimental results on two challenging medical segmentationbenchmarks demonstrate the effectiveness and superiority of CCT-R acrossvarious semi-supervised settings, with as few as one labeled case. Our code isavailable athttps://github.com/kathyliu579/ContrastiveCross-teachingWithRegistration.</description><author>Qianying Liu, Paul Henderson, Xiao Gu, Hang Dai, Fani Deligianni</author><pubDate>Mon, 16 Sep 2024 15:52:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10422v1</guid></item><item><title>Multidimensional Deconvolution with Profiling</title><link>http://arxiv.org/abs/2409.10421v1</link><description>In many experimental contexts, it is necessary to statistically remove theimpact of instrumental effects in order to physically interpret measurements.This task has been extensively studied in particle physics, where thedeconvolution task is called unfolding. A number of recent methods have shownhow to perform high-dimensional, unbinned unfolding using machine learning.However, one of the assumptions in all of these methods is that the detectorresponse is accurately modeled in the Monte Carlo simulation. In practice, thedetector response depends on a number of nuisance parameters that can beconstrained with data. We propose a new algorithm called Profile OmniFold(POF), which works in a similar iterative manner as the OmniFold (OF) algorithmwhile being able to simultaneously profile the nuisance parameters. Weillustrate the method with a Gaussian example as a proof of concepthighlighting its promising capabilities.</description><author>Huanbiao Zhu, Krish Desai, Mikael Kuusela, Vinicius Mikuni, Benjamin Nachman, Larry Wasserman</author><pubDate>Mon, 16 Sep 2024 15:52:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10421v1</guid></item><item><title>Efficient Point Cloud Classification via Offline Distillation Framework and Negative-Weight Self-Distillation Technique</title><link>http://arxiv.org/abs/2409.02020v2</link><description>The rapid advancement in point cloud processing technologies hassignificantly increased the demand for efficient and compact models thatachieve high-accuracy classification. Knowledge distillation has emerged as apotent model compression technique. However, traditional KD often requiresextensive computational resources for forward inference of large teachermodels, thereby reducing training efficiency for student models and increasingresource demands. To address these challenges, we introduce an innovativeoffline recording strategy that avoids the simultaneous loading of both teacherand student models, thereby reducing hardware demands. This approach feeds amultitude of augmented samples into the teacher model, recording both the dataaugmentation parameters and the corresponding logit outputs. By applyingshape-level augmentation operations such as random scaling and translation,while excluding point-level operations like random jittering, the size of therecords is significantly reduced. Additionally, to mitigate the issue of smallstudent model over-imitating the teacher model's outputs and converging tosuboptimal solutions, we incorporate a negative-weight self-distillationstrategy. Experimental results demonstrate that the proposed distillationstrategy enables the student model to achieve performance comparable tostate-of-the-art models while maintaining lower parameter count. This approachstrikes an optimal balance between performance and complexity. This studyhighlights the potential of our method to optimize knowledge distillation forpoint cloud classification tasks, particularly in resource-constrainedenvironments, providing a novel solution for efficient point cloud analysis.</description><author>Qiang Zheng, Chao Zhang, Jian Sun</author><pubDate>Mon, 16 Sep 2024 15:51:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02020v2</guid></item><item><title>HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models</title><link>http://arxiv.org/abs/2409.10419v1</link><description>Robots interacting with humans through natural language can unlock numerousapplications such as Referring Grasp Synthesis (RGS). Given a text query, RGSdetermines a stable grasp pose to manipulate the referred object in the robot'sworkspace. RGS comprises two steps: visual grounding and grasp pose estimation.Recent studies leverage powerful Vision-Language Models (VLMs) for visuallygrounding free-flowing natural language in real-world robotic execution.However, comparisons in complex, cluttered environments with multiple instancesof the same object are lacking. This paper introduces HiFi-CS, featuringhierarchical application of Featurewise Linear Modulation (FiLM) to fuse imageand text embeddings, enhancing visual grounding for complex attribute rich textqueries encountered in robotic grasping. Visual grounding associates an objectin 2D/3D space with natural language input and is studied in two scenarios:Closed and Open Vocabulary. HiFi-CS features a lightweight decoder combinedwith a frozen VLM and outperforms competitive baselines in closed vocabularysettings while being 100x smaller in size. Our model can effectively guideopen-set object detectors like GroundedSAM to enhance open-vocabularyperformance. We validate our approach through real-world RGS experiments usinga 7-DOF robotic arm, achieving 90.33\% visual grounding accuracy in 15 tabletopscenes. We include our codebase in the supplementary material.</description><author>Vineet Bhat, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami</author><pubDate>Mon, 16 Sep 2024 15:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10419v1</guid></item><item><title>Geometric Clustering for Hardware-Efficient Implementation of Chromatic Dispersion Compensation</title><link>http://arxiv.org/abs/2409.10416v1</link><description>Power efficiency remains a significant challenge in modern optical fibercommunication systems, driving efforts to reduce the computational complexityof digital signal processing, particularly in chromatic dispersion compensation(CDC) algorithms. While various strategies for complexity reduction have beenproposed, many lack the necessary hardware implementation to validate theirbenefits. This paper provides a theoretical analysis of the tap overlappingeffect in CDC filters for coherent receivers, introduces a novel Time-DomainClustered Equalizer (TDCE) technique based on this concept, and presents aField-Programmable Gate Array (FPGA) implementation for validation. Wedeveloped an innovative parallelization method for TDCE, implementing it inhardware for fiber lengths up to 640 km. A fair comparison with thestate-of-the-art frequency domain equalizer (FDE) under identical conditions isalso conducted. Our findings highlight that implementation strategies,including parallelization and memory management, are as crucial ascomputational complexity in determining hardware complexity and energyefficiency. The proposed TDCE hardware implementation achieves up to 70.7\%energy savings and 71.4\% multiplier usage savings compared to FDE, despite itshigher computational complexity.</description><author>Geraldo Gomes, Pedro Freire, Jaroslaw E. Prilepsky, Sergei K. Turitsyn</author><pubDate>Mon, 16 Sep 2024 15:48:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10416v1</guid></item><item><title>Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph</title><link>http://arxiv.org/abs/2406.07113v3</link><description>Locating objects described in natural language presents a significantchallenge for autonomous agents. Existing CLIP-based open-vocabulary methodssuccessfully perform 3D object grounding with simple (bare) queries, but cannotcope with ambiguous descriptions that demand an understanding of objectrelations. To tackle this problem, we propose a modular approach called BBQ(Beyond Bare Queries), which constructs 3D scene graph representation withmetric and semantic edges and utilizes a large language model as ahuman-to-agent interface through our deductive scene reasoning algorithm. BBQemploys robust DINO-powered associations to construct 3D object-centric map andan advanced raycasting algorithm with a 2D vision-language model to describethem as graph nodes. On the Replica and ScanNet datasets, we have demonstratedthat BBQ takes a leading place in open-vocabulary 3D semantic segmentationcompared to other zero-shot methods. Also, we show that leveraging spatialrelations is especially effective for scenes containing multiple entities ofthe same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks,our deductive approach demonstrates a significant improvement, enabling objectsgrounding by complex queries compared to other state-of-the-art methods. Thecombination of our design choices and software implementation has resulted insignificant data processing speed in experiments on the robot on-boardcomputer. This promising performance enables the application of our approach inintelligent robotics projects. We made the code publicly available athttps://linukc.github.io/BeyondBareQueries/.</description><author>Sergey Linok, Tatiana Zemskova, Svetlana Ladanova, Roman Titkov, Dmitry Yudin, Maxim Monastyrny, Aleksei Valenkov</author><pubDate>Mon, 16 Sep 2024 15:47:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07113v3</guid></item><item><title>Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices</title><link>http://arxiv.org/abs/2308.11295v2</link><description>Determining the degree of confidence of deep learning model in its predictionis an open problem in the field of natural language processing. Most of theclassical methods for uncertainty estimation are quite weak for textclassification models. We set the task of obtaining an uncertainty estimate forneural networks based on the Transformer architecture. A key feature of suchmo-dels is the attention mechanism, which supports the information flow betweenthe hidden representations of tokens in the neural network. We explore theformed relationships between internal representations using Topological DataAnalysis methods and utilize them to predict model's confidence. In this paper,we propose a method for uncertainty estimation based on the topologicalproperties of the attention mechanism and compare it with classical methods. Asa result, the proposed algorithm surpasses the existing methods in quality andopens up a new area of application of the attention mechanism, but requires theselection of topological features.</description><author>Elizaveta Kostenok, Daniil Cherniavskii, Alexey Zaytsev</author><pubDate>Mon, 16 Sep 2024 15:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11295v2</guid></item><item><title>Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes</title><link>http://arxiv.org/abs/2408.03539v3</link><description>Reinforcement learning (RL), particularly its combination with deep neuralnetworks referred to as deep RL (DRL), has shown tremendous promise across awide range of applications, suggesting its potential for enabling thedevelopment of sophisticated robotic behaviors. Robotics problems, however,pose fundamental difficulties for the application of RL, stemming from thecomplexity and cost of interacting with the physical world. This articleprovides a modern survey of DRL for robotics, with a particular focus onevaluating the real-world successes achieved with DRL in realizing several keyrobotic competencies. Our analysis aims to identify the key factors underlyingthose exciting successes, reveal underexplored areas, and provide an overallcharacterization of the status of DRL in robotics. We highlight severalimportant avenues for future work, emphasizing the need for stable andsample-efficient real-world RL paradigms, holistic approaches for discoveringand integrating various competencies to tackle complex long-horizon, open-worldtasks, and principled development and evaluation procedures. This survey isdesigned to offer insights for both RL practitioners and roboticists towardharnessing RL's power to create generally capable real-world robotic systems.</description><author>Chen Tang, Ben Abbatematteo, Jiaheng Hu, Rohan Chandra, Roberto Martín-Martín, Peter Stone</author><pubDate>Mon, 16 Sep 2024 15:41:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03539v3</guid></item><item><title>A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration</title><link>http://arxiv.org/abs/2409.10403v1</link><description>This paper proposes a knowledge-enhanced disease diagnosis method based on aprompt learning framework. The method retrieves structured knowledge fromexternal knowledge graphs related to clinical cases, encodes it, and injects itinto the prompt templates to enhance the language model's understanding andreasoning capabilities for the task.We conducted experiments on three publicdatasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that theproposed method significantly outperforms existing models across multipleevaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTCdataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.Additionally,ablation studies confirmed the critical role of the knowledgeinjection module,as the removal of this module resulted in a significant dropin F1 score. The experimental results demonstrate that the proposed method notonly effectively improves the accuracy of disease diagnosis but also enhancesthe interpretability of the predictions, providing more reliable support andevidence for clinical diagnosis.</description><author>Zhang Zheng</author><pubDate>Mon, 16 Sep 2024 15:34:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10403v1</guid></item><item><title>Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP</title><link>http://arxiv.org/abs/2406.17639v3</link><description>Contrastive Language--Image Pre-training (CLIP) has manifested remarkableimprovements in zero-shot classification and cross-modal vision-language tasks.Yet, from a geometrical point of view, the CLIP embedding space has been foundto have a pronounced modality gap. This gap renders the embedding space overlysparse and disconnected, with different modalities being densely distributed indistinct subregions of the hypersphere. In this work, we aim at answering threemain questions: 1. Does sharing the parameter space between the multi-modalencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apartthe uni-modal embeddings via intra-modality separation? 3. How do these gapreduction approaches affect the downstream performance? We design AlignCLIP, inorder to answer these questions and through extensive experiments, we show thatAlignCLIP achieves noticeable enhancements in the cross-modal alignment of theembeddings, and thereby, reduces the modality gap, while improving theperformance across several zero-shot and fine-tuning downstream evaluations.</description><author>Sedigheh Eslami, Gerard de Melo</author><pubDate>Mon, 16 Sep 2024 15:32:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17639v3</guid></item><item><title>MOST: MR reconstruction Optimization for multiple downStream Tasks via continual learning</title><link>http://arxiv.org/abs/2409.10394v1</link><description>Deep learning-based Magnetic Resonance (MR) reconstruction methods havefocused on generating high-quality images but they often overlook the impact ondownstream tasks (e.g., segmentation) that utilize the reconstructed images.Cascading separately trained reconstruction network and downstream task networkhas been shown to introduce performance degradation due to error propagationand domain gaps between training datasets. To mitigate this issue, downstreamtask-oriented reconstruction optimization has been proposed for a singledownstream task. Expanding this optimization to multi-task scenarios is notstraightforward. In this work, we extended this optimization to sequentiallyintroduced multiple downstream tasks and demonstrated that a single MRreconstruction network can be optimized for multiple downstream tasks bydeploying continual learning (MOST). MOST integrated techniques fromreplay-based continual learning and image-guided loss to overcome catastrophicforgetting. Comparative experiments demonstrated that MOST outperformed areconstruction network without finetuning, a reconstruction network withna\"ive finetuning, and conventional continual learning methods. Thisadvancement empowers the application of a single MR reconstruction network formultiple downstream tasks. The source code is available at:https://github.com/SNU-LIST/MOST</description><author>Hwihun Jeong, Se Young Chun, Jongho Lee</author><pubDate>Mon, 16 Sep 2024 15:31:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10394v1</guid></item><item><title>Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey</title><link>http://arxiv.org/abs/2310.07745v2</link><description>The rapid increase in the number of cyber-attacks in recent years raises theneed for principled methods for defending networks against malicious actors.Deep reinforcement learning (DRL) has emerged as a promising approach formitigating these attacks. However, while DRL has shown much potential for cyberdefence, numerous challenges must be overcome before DRL can be applied toautonomous cyber operations (ACO) at scale. Principled methods are required forenvironments that confront learners with very high-dimensional state spaces,large multi-discrete action spaces, and adversarial learning. Recent works havereported success in solving these problems individually. There have also beenimpressive engineering efforts towards solving all three for real-time strategygames. However, applying DRL to the full ACO problem remains an open challenge.Here, we survey the relevant DRL literature and conceptualize an idealisedACO-DRL agent. We provide: i.) A summary of the domain properties that definethe ACO problem; ii.) A comprehensive comparison of current ACO environmentsused for benchmarking DRL approaches; iii.) An overview of state-of-the-artapproaches for scaling DRL to domains that confront learners with the curse ofdimensionality, and; iv.) A survey and critique of current methods for limitingthe exploitability of agents within adversarial settings from the perspectiveof ACO. We conclude with open research questions that we hope will motivatefuture directions for researchers and practitioners working on ACO.</description><author>Gregory Palmer, Chris Parry, Daniel J. B. Harrold, Chris Willis</author><pubDate>Mon, 16 Sep 2024 15:28:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07745v2</guid></item><item><title>PointViG: A Lightweight GNN-based Model for Efficient Point Cloud Analysis</title><link>http://arxiv.org/abs/2407.00921v2</link><description>In the domain of point cloud analysis, despite the significant capabilitiesof Graph Neural Networks (GNNs) in managing complex 3D datasets, existingapproaches encounter challenges like high computational costs and scalabilityissues with extensive scenarios. These limitations restrict the practicaldeployment of GNNs, notably in resource-constrained environments. To addressthese issues, this study introduce &lt;b&gt;Point&lt;\b&gt; &lt;b&gt;Vi&lt;\b&gt;sion &lt;b&gt;G&lt;\b&gt;NN(PointViG), an efficient framework for point cloud analysis. PointViGincorporates a lightweight graph convolutional module to efficiently aggregatelocal features and mitigate over-smoothing. For large-scale point cloud scenes,we propose an adaptive dilated graph convolution technique that searches forsparse neighboring nodes within a dilated neighborhood based on semanticcorrelation, thereby expanding the receptive field and ensuring computationalefficiency. Experiments demonstrate that PointViG achieves performancecomparable to state-of-the-art models while balancing performance andcomplexity. On the ModelNet40 classification task, PointViG achieved 94.3%accuracy with 1.5M parameters. For the S3DIS segmentation task, it achieved anmIoU of 71.7% with 5.3M parameters. These results underscore the potential andefficiency of PointViG in point cloud analysis.</description><author>Qiang Zheng, Yafei Qi, Chen Wang, Chao Zhang, Jian Sun</author><pubDate>Mon, 16 Sep 2024 15:28:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00921v2</guid></item><item><title>TPFL: Tsetlin-Personalized Federated Learning with Confidence-Based Clustering</title><link>http://arxiv.org/abs/2409.10392v1</link><description>The world of Machine Learning (ML) has witnessed rapid changes in terms ofnew models and ways to process users data. The majority of work that has beendone is focused on Deep Learning (DL) based approaches. However, with theemergence of new algorithms such as the Tsetlin Machine (TM) algorithm, thereis growing interest in exploring alternative approaches that may offer uniqueadvantages in certain domains or applications. One of these domains isFederated Learning (FL), in which users privacy is of utmost importance. Due toits novelty, FL has seen a surge in the incorporation of personalizationtechniques to enhance model accuracy while maintaining user privacy underpersonalized conditions. In this work, we propose a novel approach dubbed TPFL:Tsetlin-Personalized Federated Learning, in which models are grouped intoclusters based on their confidence towards a specific class. In this way,clustering can benefit from two key advantages. Firstly, clients share onlywhat they are confident about, resulting in the elimination of wrongful weightaggregation among clients whose data for a specific class may have not beenenough during the training. This phenomenon is prevalent when the data arenon-Independent and Identically Distributed (non-IID). Secondly, by sharingonly weights towards a specific class, communication cost is substantiallyreduced, making TPLF efficient in terms of both accuracy and communicationcost. The results of TPFL demonstrated the highest accuracy on three differentdatasets; namely MNIST, FashionMNIST and FEMNIST.</description><author>Rasoul Jafari Gohari, Laya Aliahmadipour, Ezat Valipour</author><pubDate>Mon, 16 Sep 2024 15:27:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10392v1</guid></item><item><title>Prompt-and-Transfer: Dynamic Class-aware Enhancement for Few-shot Segmentation</title><link>http://arxiv.org/abs/2409.10389v1</link><description>For more efficient generalization to unseen domains (classes), most Few-shotSegmentation (FSS) would directly exploit pre-trained encoders and onlyfine-tune the decoder, especially in the current era of large models. However,such fixed feature encoders tend to be class-agnostic, inevitably activatingobjects that are irrelevant to the target class. In contrast, humans caneffortlessly focus on specific objects in the line of sight. This paper mimicsthe visual perception pattern of human beings and proposes a novel and powerfulprompt-driven scheme, called ``Prompt and Transfer" (PAT), which constructs adynamic class-aware prompting paradigm to tune the encoder for focusing on theinterested object (target class) in the current task. Three key points areelaborated to enhance the prompting: 1) Cross-modal linguistic information isintroduced to initialize prompts for each task. 2) Semantic Prompt Transfer(SPT) that precisely transfers the class-specific semantics within the imagesto prompts. 3) Part Mask Generator (PMG) that works in conjunction with SPT toadaptively generate different but complementary part prompts for differentindividuals. Surprisingly, PAT achieves competitive performance on 4 differenttasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remotesensing domains), Weak-label FSS, and Zero-shot Segmentation, setting newstate-of-the-arts on 11 benchmarks.</description><author>Hanbo Bi, Yingchao Feng, Wenhui Diao, Peijin Wang, Yongqiang Mao, Kun Fu, Hongqi Wang, Xian Sun</author><pubDate>Mon, 16 Sep 2024 15:24:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10389v1</guid></item><item><title>Revising the Structure of Recurrent Neural Networks to Eliminate Numerical Derivatives in Forming Physics Informed Loss Terms with Respect to Time</title><link>http://arxiv.org/abs/2409.10388v1</link><description>Solving unsteady partial differential equations (PDEs) using recurrent neuralnetworks (RNNs) typically requires numerical derivatives between each block ofthe RNN to form the physics informed loss function. However, this introducesthe complexities of numerical derivatives into the training process of thesemodels. In this study, we propose modifying the structure of the traditionalRNN to enable the prediction of each block over a time interval, making itpossible to calculate the derivative of the output with respect to time usingthe backpropagation algorithm. To achieve this, the time intervals of theseblocks are overlapped, defining a mutual loss function between them.Additionally, the employment of conditional hidden states enables us to achievea unique solution for each block. The forget factor is utilized to control theinfluence of the conditional hidden state on the prediction of the subsequentblock. This new model, termed the Mutual Interval RNN (MI-RNN), is applied tosolve three different benchmarks: the Burgers equation, unsteady heatconduction in an irregular domain, and the Green vortex problem. Our resultsdemonstrate that MI-RNN can find the exact solution more accurately compared toexisting RNN models. For instance, in the second problem, MI-RNN achieved oneorder of magnitude less relative error compared to the RNN model with numericalderivatives.</description><author>Mahyar Jahani-nasab, Mohamad Ali Bijarchi</author><pubDate>Mon, 16 Sep 2024 15:24:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10388v1</guid></item><item><title>Mamba-ST: State Space Model for Efficient Style Transfer</title><link>http://arxiv.org/abs/2409.10385v1</link><description>The goal of style transfer is, given a content image and a style source,generating a new image preserving the content but with the artisticrepresentation of the style source. Most of the state-of-the-art architecturesuse transformers or diffusion-based models to perform this task, despite theheavy computational burden that they require. In particular, transformers useself- and cross-attention layers which have large memory footprint, whilediffusion models require high inference time. To overcome the above, this paperexplores a novel design of Mamba, an emergent State-Space Model (SSM), calledMamba-ST, to perform style transfer. To do so, we adapt Mamba linear equationto simulate the behavior of cross-attention layers, which are able to combinetwo separate embeddings into a single output, but drastically reducing memoryusage and time complexity. We modified the Mamba's inner equations so to acceptinputs from, and combine, two separate data streams. To the best of ourknowledge, this is the first attempt to adapt the equations of SSMs to a visiontask like style transfer without requiring any other module likecross-attention or custom normalization layers. An extensive set of experimentsdemonstrates the superiority and efficiency of our method in performing styletransfer compared to transformers and diffusion models. Results show improvedquality in terms of both ArtFID and FID metrics. Code is available athttps://github.com/FilippoBotti/MambaST.</description><author>Filippo Botti, Alex Ergasti, Leonardo Rossi, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati</author><pubDate>Mon, 16 Sep 2024 15:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10385v1</guid></item><item><title>Instigating Cooperation among LLM Agents Using Adaptive Information Modulation</title><link>http://arxiv.org/abs/2409.10372v1</link><description>This paper introduces a novel framework combining LLM agents as proxies forhuman strategic behavior with reinforcement learning (RL) to engage theseagents in evolving strategic interactions within team environments. Ourapproach extends traditional agent-based simulations by using strategic LLMagents (SLA) and introducing dynamic and adaptive governance through apro-social promoting RL agent (PPA) that modulates information access acrossagents in a network, optimizing social welfare and promoting pro-socialbehavior. Through validation in iterative games, including the prisonerdilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.The PPA agent effectively learns to adjust information transparency, resultingin enhanced cooperation rates. This framework offers significant insights intoAI-mediated social dynamics, contributing to the deployment of AI in real-worldteam settings.</description><author>Qiliang Chen, Alireza, Ilami, Nunzio Lore, Babak Heydari</author><pubDate>Mon, 16 Sep 2024 15:15:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10372v1</guid></item><item><title>Learning Gentle Grasping from Human-Free Force Control Demonstration</title><link>http://arxiv.org/abs/2409.10371v1</link><description>Humans can steadily and gently grasp unfamiliar objects based on tactileperception. Robots still face challenges in achieving similar performance dueto the difficulty of learning accurate grasp-force predictions and forcecontrol strategies that can be generalized from limited data. In this article,we propose an approach for learning grasping from ideal force controldemonstrations, to achieve similar performance of human hands with limited datasize. Our approach utilizes objects with known contact characteristics toautomatically generate reference force curves without human demonstrations. Inaddition, we design the dual convolutional neural networks (Dual-CNN)architecture which incorporating a physics-based mechanics module for learningtarget grasping force predictions from demonstrations. The described method canbe effectively applied in vision-based tactile sensors and enables gentle andstable grasping of objects from the ground. The described prediction model andgrasping strategy were validated in offline evaluations and online experiments,and the accuracy and generalizability were demonstrated.</description><author>Mingxuan Li, Lunwei Zhang, Tiemin Li, Yao Jiang</author><pubDate>Mon, 16 Sep 2024 15:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10371v1</guid></item><item><title>Uncovering the Mechanism of Hepatotoxiciy of PFAS Targeting L-FABP Using GCN and Computational Modeling</title><link>http://arxiv.org/abs/2409.10370v1</link><description>Per- and polyfluoroalkyl substances (PFAS) are persistent environmentalpollutants with known toxicity and bioaccumulation issues. Their widespreadindustrial use and resistance to degradation have led to global environmentalcontamination and significant health concerns. While a minority of PFAS havebeen extensively studied, the toxicity of many PFAS remains poorly understooddue to limited direct toxicological data. This study advances the predictivemodeling of PFAS toxicity by combining semi-supervised graph convolutionalnetworks (GCNs) with molecular descriptors and fingerprints. We propose a novelapproach to enhance the prediction of PFAS binding affinities by isolatingmolecular fingerprints to construct graphs where then descriptors are set asthe node features. This approach specifically captures the structural,physicochemical, and topological features of PFAS without overfitting due to anabundance of features. Unsupervised clustering then identifies representativecompounds for detailed binding studies. Our results provide a more accurateability to estimate PFAS hepatotoxicity to provide guidance in chemicaldiscovery of new PFAS and the development of new safety regulations.</description><author>Lucas Jividen, Tibo Duran, Xi-Zhi Niu, Jun Bai</author><pubDate>Mon, 16 Sep 2024 15:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10370v1</guid></item><item><title>Robust image representations with counterfactual contrastive learning</title><link>http://arxiv.org/abs/2409.10365v1</link><description>Contrastive pretraining can substantially increase model generalisation anddownstream performance. However, the quality of the learned representations ishighly dependent on the data augmentation strategy applied to generate positivepairs. Positive contrastive pairs should preserve semantic meaning whilediscarding unwanted variations related to the data acquisition domain.Traditional contrastive pipelines attempt to simulate domain shifts throughpre-defined generic image transformations. However, these do not always mimicrealistic and relevant domain variations for medical imaging such as scannerdifferences. To tackle this issue, we herein introduce counterfactualcontrastive learning, a novel framework leveraging recent advances in causalimage synthesis to create contrastive positive pairs that faithfully capturerelevant domain variations. Our method, evaluated across five datasetsencompassing both chest radiography and mammography data, for two establishedcontrastive objectives (SimCLR and DINO-v2), outperforms standard contrastivelearning in terms of robustness to acquisition shift. Notably, counterfactualcontrastive learning achieves superior downstream performance on bothin-distribution and on external datasets, especially for images acquired withscanners under-represented in the training set. Further experiments show thatthe proposed framework extends beyond acquisition shifts, with models trainedwith counterfactual contrastive learning substantially improving subgroupperformance across biological sex.</description><author>Mélanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker</author><pubDate>Mon, 16 Sep 2024 15:11:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10365v1</guid></item><item><title>Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning</title><link>http://arxiv.org/abs/2409.10362v1</link><description>We present a novel frequency-based Self-Supervised Learning (SSL) approachthat significantly enhances its efficacy for pre-training. Prior work in thisdirection masks out pre-defined frequencies in the input image and employs areconstruction loss to pre-train the model. While achieving promising results,such an implementation has two fundamental limitations as identified in ourpaper. First, using pre-defined frequencies overlooks the variability of imagefrequency responses. Second, pre-trained with frequency-filtered images, theresulting model needs relatively more data to adapt to naturally looking imagesduring fine-tuning. To address these drawbacks, we propose FOurier transformcompression with seLf-Knowledge distillation (FOLK), integrating two dedicatedideas. First, inspired by image compression, we adaptively select themasked-out frequencies based on image frequency responses, creating moresuitable SSL tasks for pre-training. Second, we employ a two-branch frameworkempowered by knowledge distillation, enabling the model to take both thefiltered and original images as input, largely reducing the burden ofdownstream tasks. Our experimental results demonstrate the effectiveness ofFOLK in achieving competitive performance to many state-of-the-art SSL methodsacross various downstream tasks, including image classification, few-shotlearning, and semantic segmentation.</description><author>Amin Karimi Monsefi, Mengxi Zhou, Nastaran Karimi Monsefi, Ser-Nam Lim, Wei-Lun Chao, Rajiv Ramnath</author><pubDate>Mon, 16 Sep 2024 15:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10362v1</guid></item><item><title>2D or not 2D: How Does the Dimensionality of Gesture Representation Affect 3D Co-Speech Gesture Generation?</title><link>http://arxiv.org/abs/2409.10357v1</link><description>Co-speech gestures are fundamental for communication. The advent of recentdeep learning techniques has facilitated the creation of lifelike, synchronousco-speech gestures for Embodied Conversational Agents. "In-the-wild" datasets,aggregating video content from platforms like YouTube via human pose detectiontechnologies, provide a feasible solution by offering 2D skeletal sequencesaligned with speech. Concurrent developments in lifting models enable theconversion of these 2D sequences into 3D gesture databases. However, it isimportant to note that the 3D poses estimated from the 2D extracted poses are,in essence, approximations of the ground-truth, which remains in the 2D domain.This distinction raises questions about the impact of gesture representationdimensionality on the quality of generated motions - a topic that, to ourknowledge, remains largely unexplored. Our study examines the effect of usingeither 2D or 3D joint coordinates as training data on the performance ofspeech-to-gesture deep generative models. We employ a lifting model forconverting generated 2D pose sequences into 3D and assess how gestures createddirectly in 3D stack up against those initially generated in 2D and thenconverted to 3D. We perform an objective evaluation using widely used metricsin the gesture generation field as well as a user study to qualitativelyevaluate the different approaches.</description><author>Téo Guichoux, Laure Soulier, Nicolas Obin, Catherine Pelachaud</author><pubDate>Mon, 16 Sep 2024 15:06:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10357v1</guid></item><item><title>Taming Diffusion Models for Image Restoration: A Review</title><link>http://arxiv.org/abs/2409.10353v1</link><description>Diffusion models have achieved remarkable progress in generative modelling,particularly in enhancing image quality to conform to human preferences.Recently, these models have also been applied to low-level computer vision forphoto-realistic image restoration (IR) in tasks such as image denoising,deblurring, dehazing, etc. In this review paper, we introduce key constructionsin diffusion models and survey contemporary techniques that make use ofdiffusion models in solving general IR tasks. Furthermore, we point out themain challenges and limitations of existing diffusion-based IR frameworks andprovide potential directions for future work.</description><author>Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön</author><pubDate>Mon, 16 Sep 2024 15:04:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10353v1</guid></item><item><title>Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene Graph for Robot Navigation</title><link>http://arxiv.org/abs/2409.10350v1</link><description>Current open-vocabulary scene graph generation algorithms highly rely on both3D scene point cloud data and posed RGB-D images and thus have limitedapplications in scenarios where RGB-D images or camera poses are not readilyavailable. To solve this problem, we propose Point2Graph, a novel end-to-endpoint cloud-based 3D open-vocabulary scene graph generation framework in whichthe requirement of posed RGB-D image series is eliminated. This hierarchicalframework contains room and object detection/segmentation and open-vocabularyclassification. For the room layer, we leverage the advantage of merging thegeometry-based border detection algorithm with the learning-based regiondetection to segment rooms and create a "Snap-Lookup" framework foropen-vocabulary room classification. In addition, we create an end-to-endpipeline for the object layer to detect and classify 3D objects based solely on3D point cloud data. Our evaluation results show that our framework canoutperform the current state-of-the-art (SOTA) open-vocabulary object and roomsegmentation and classification algorithm on widely used real-scene datasets.</description><author>Yifan Xu, Ziming Luo, Qianwei Wang, Vineet Kamat, Carol Menassa</author><pubDate>Mon, 16 Sep 2024 15:01:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10350v1</guid></item><item><title>Quantifying and Learning Static vs. Dynamic Information in Deep Spatiotemporal Networks</title><link>http://arxiv.org/abs/2211.01783v2</link><description>There is limited understanding of the information captured by deepspatiotemporal models in their intermediate representations. For example, whileevidence suggests that action recognition algorithms are heavily influenced byvisual appearance in single frames, no quantitative methodology exists forevaluating such static bias in the latent representation compared to biastoward dynamics. We tackle this challenge by proposing an approach forquantifying the static and dynamic biases of any spatiotemporal model, andapply our approach to three tasks, action recognition, automatic video objectsegmentation (AVOS) and video instance segmentation (VIS). Our key findingsare: (i) Most examined models are biased toward static information. (ii) Somedatasets that are assumed to be biased toward dynamics are actually biasedtoward static information. (iii) Individual channels in an architecture can bebiased toward static, dynamic or a combination of the two. (iv) Most modelsconverge to their culminating biases in the first half of training. We thenexplore how these biases affect performance on dynamically biased datasets. Foraction recognition, we propose StaticDropout, a semantically guided dropoutthat debiases a model from static information toward dynamics. For AVOS, wedesign a better combination of fusion and cross connection layers compared withprevious architectures.</description><author>Matthew Kowal, Mennatullah Siam, Md Amirul Islam, Neil D. B. Bruce, Richard P. Wildes, Konstantinos G. Derpanis</author><pubDate>Mon, 16 Sep 2024 15:00:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.01783v2</guid></item><item><title>WinoPron: Revisiting English Winogender Schemas for Consistency, Coverage, and Grammatical Case</title><link>http://arxiv.org/abs/2409.05653v2</link><description>While measuring bias and robustness in coreference resolution are importantgoals, such measurements are only as good as the tools we use to measure themwith. Winogender schemas (Rudinger et al., 2018) are an influential datasetproposed to evaluate gender bias in coreference resolution, but a closer lookreveals issues with the data that compromise its use for reliable evaluation,including treating different pronominal forms as equivalent, violations oftemplate constraints, and typographical errors. We identify these issues andfix them, contributing a new dataset: WinoPron. Our changes affect performancewith state-of-the-art supervised coreference resolution systems as well as allmodel sizes of the language model FLAN-T5, with F1 dropping on average 10percentage points. We also propose a new method to evaluate pronominal bias incoreference resolution that goes beyond the binary. With this method and ournew dataset which is balanced for grammatical case, we empirically demonstratethat bias characteristics vary not just across pronoun sets, but also acrosssurface forms of those sets.</description><author>Vagrant Gautam, Julius Steuer, Eileen Bingert, Ray Johns, Anne Lauscher, Dietrich Klakow</author><pubDate>Mon, 16 Sep 2024 14:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05653v2</guid></item><item><title>Towards Supervised Performance on Speaker Verification with Self-Supervised Learning by Leveraging Large-Scale ASR Models</title><link>http://arxiv.org/abs/2406.02285v2</link><description>Recent advancements in Self-Supervised Learning (SSL) have shown promisingresults in Speaker Verification (SV). However, narrowing the performance gapwith supervised systems remains an ongoing challenge. Several studies haveobserved that speech representations from large-scale ASR models containvaluable speaker information. This work explores the limitations of fine-tuningthese models for SV using an SSL contrastive objective in an end-to-endapproach. Then, we propose a framework to learn speaker representations in anSSL context by fine-tuning a pre-trained WavLM with a supervised loss usingpseudo-labels. Initial pseudo-labels are derived from an SSL DINO-based modeland are iteratively refined by clustering the model embeddings. Our methodachieves 0.99% EER on VoxCeleb1-O, establishing the new state-of-the-art onself-supervised SV. As this performance is close to our supervised baseline of0.94% EER, this contribution is a step towards supervised performance on SVwith SSL.</description><author>Victor Miara, Theo Lepage, Reda Dehak</author><pubDate>Mon, 16 Sep 2024 14:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02285v2</guid></item><item><title>Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation</title><link>http://arxiv.org/abs/2409.10343v1</link><description>Implicit feedback, often used to build recommender systems, unavoidablyconfronts noise due to factors such as misclicks and position bias. Previousstudies have attempted to alleviate this by identifying noisy samples based ontheir diverged patterns, such as higher loss values, and mitigating the noisethrough sample dropping or reweighting. Despite the progress, we observeexisting approaches struggle to distinguish hard samples and noise samples, asthey often exhibit similar patterns, thereby limiting their effectiveness indenoising recommendations. To address this challenge, we propose a LargeLanguage Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,we construct an LLM-based scorer to evaluate the semantic consistency of itemswith the user preference, which is quantified based on summarized historicaluser interactions. The resulting scores are used to assess the hardness ofsamples for the pointwise or pairwise training objectives. To ensureefficiency, we introduce a variance-based sample pruning strategy to filterpotential hard samples before scoring. Besides, we propose an iterativepreference update module designed to continuously refine summarized userpreference, which may be biased due to false-positive user-item interactions.Extensive experiments on three real-world datasets and four backbonerecommenders demonstrate the effectiveness of our approach.</description><author>Tianrui Song, Wenshuo Chao, Hao Liu</author><pubDate>Mon, 16 Sep 2024 14:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10343v1</guid></item><item><title>Detecting Sexism in German Online Newspaper Comments with Open-Source Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks 1 and 2, Closed Track)</title><link>http://arxiv.org/abs/2409.10341v1</link><description>Sexism in online media comments is a pervasive challenge that often manifestssubtly, complicating moderation efforts as interpretations of what constitutessexism can vary among individuals. We study monolingual and multilingualopen-source text embeddings to reliably detect sexism and misogyny inGerman-language online comments from an Austrian newspaper. We observedclassifiers trained on text embeddings to mimic closely the individualjudgements of human annotators. Our method showed robust performance in theGermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1score of 0.597 (4th place, as reported on Codabench). It also accuratelypredicted the distribution of human annotations in GerMS-Detect Subtask 2, withan average Jensen-Shannon distance of 0.301 (2nd place). The computationalefficiency of our approach suggests potential for scalable applications acrossvarious languages and linguistic contexts.</description><author>Florian Bremm, Patrick Gustav Blaneck, Tobias Bornheim, Niklas Grieger, Stephan Bialonski</author><pubDate>Mon, 16 Sep 2024 14:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10341v1</guid></item><item><title>Hyperedge Modeling in Hypergraph Neural Networks by using Densest Overlapping Subgraphs</title><link>http://arxiv.org/abs/2409.10340v1</link><description>Hypergraphs tackle the limitations of traditional graphs by introducing {\emhyperedges}. While graph edges connect only two nodes, hyperedges connect anarbitrary number of nodes along their edges. Also, the underlyingmessage-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in theform of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer andmore complex structural information than traditional Graph Neural Networks(GNNs). More recently, the idea of overlapping subgraphs has emerged. Thesesubgraphs can capture more information about subgroups of vertices withoutlimiting one vertex belonging to just one group, allowing vertices to belong tomultiple groups or subgraphs. In addition, one of the most important problemsin graph clustering is to find densest overlapping subgraphs (DOS). In thispaper, we propose a solution to the DOS problem via Agglomerative GreedyEnumeration (DOSAGE) algorithm as a novel approach to enhance the process ofgenerating the densest overlapping subgraphs and, hence, a robust constructionof the hypergraphs. Experiments on standard benchmarks show that the DOSAGEalgorithm significantly outperforms the HGNNs and six other methods on the nodeclassification task.</description><author>Mehrad Soltani, Luis Rueda</author><pubDate>Mon, 16 Sep 2024 14:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10340v1</guid></item><item><title>My part is bigger than yours -- assessment within a group of peers</title><link>http://arxiv.org/abs/2407.01843v2</link><description>A project (e.g., writing a collaborative research paper) is often a groupeffort. At the end, each contributor identifies their contribution, oftenverbally. The reward, however, is very frequently financial. It leads to thequestion of what (percentage) share in the creation of the paper is due toindividual authors. Different authors may have various opinions on the matter;even worse, their opinions may have different relevance. In this paper, wepresent simple models that allow aggregation of experts' views, linking thepriority of his preference directly to the assessment made by other experts. Inthis approach, the more significant the contribution of a given expert, thegreater the importance of his opinion. The presented method can be consideredan attempt to find consensus among peers involved in the same project. Hence,its applications may go beyond the proposed study example of writing ascientific paper.</description><author>Konrad Kułakowski, Jacek Szybowski</author><pubDate>Mon, 16 Sep 2024 14:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01843v2</guid></item><item><title>A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems</title><link>http://arxiv.org/abs/2403.10996v2</link><description>Multi-agent reinforcement learning (MARL) systems usually requiresignificantly long training times due to their inherent complexity.Furthermore, deploying them in the real world demands a feature-richenvironment along with multiple embodied agents, which may not be feasible dueto budget or space limitations, not to mention energy consumption and safetyissues. This work tries to address these pain points by presenting asustainable digital twin framework capable of accelerating MARL training byselectively scaling parallelized workloads on-demand, and transferring thetrained policies from simulation to reality using minimal hardware resources.The applicability of the proposed digital twin framework is highlighted throughtwo representative use cases, which cover cooperative as well as competitiveclasses of MARL problems. We study the effect of agent and environmentparallelization on training time and that of systematic domain randomization onzero-shot sim2real transfer across both the case studies. Results indicate upto 76.3% reduction in training time with the proposed parallelization schemeand as low as 2.9% sim2real gap using the suggested deployment method.</description><author>Chinmay Vilas Samak, Tanmay Vilas Samak, Venkat Krovi</author><pubDate>Mon, 16 Sep 2024 14:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10996v2</guid></item><item><title>Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents</title><link>http://arxiv.org/abs/2406.05870v2</link><description>Retrieval-augmented generation (RAG) systems respond to queries by retrievingrelevant documents from a knowledge database, then generating an answer byapplying an LLM to the retrieved documents. We demonstrate that RAG systemsthat operate on databases with untrusted content are vulnerable to a new classof denial-of-service attacks we call jamming. An adversary can add a single``blocker'' document to the database that will be retrieved in response to aspecific query and result in the RAG system not answering this query -ostensibly because it lacks the information or because the answer is unsafe. We describe and measure the efficacy of several methods for generatingblocker documents, including a new method based on black-box optimization. Thismethod (1) does not rely on instruction injection, (2) does not require theadversary to know the embedding or LLM used by the target RAG system, and (3)does not use an auxiliary LLM to generate blocker documents. We evaluate jamming attacks on several LLMs and embeddings and demonstratethat the existing safety metrics for LLMs do not capture their vulnerability tojamming. We then discuss defenses against blocker documents.</description><author>Avital Shafran, Roei Schuster, Vitaly Shmatikov</author><pubDate>Mon, 16 Sep 2024 14:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05870v2</guid></item><item><title>VAE-QWGAN: Improving Quantum GANs for High Resolution Image Generation</title><link>http://arxiv.org/abs/2409.10339v1</link><description>This paper presents a novel hybrid quantum generative model, the VAE-QWGAN,which combines the strengths of a classical Variational AutoEncoder (VAE) witha hybrid Quantum Wasserstein Generative Adversarial Network (QWGAN). TheVAE-QWGAN integrates the VAE decoder and QGAN generator into a single quantummodel with shared parameters, utilizing the VAE's encoder for latent vectorsampling during training. To generate new data from the trained model atinference, input latent vectors are sampled from a Gaussian Mixture Model(GMM), learnt on the training latent vectors. This, in turn, enhances thediversity and quality of generated images. We evaluate the model's performanceon MNIST/Fashion-MNIST datasets, and demonstrate improved quality and diversityof generated images compared to existing approaches.</description><author>Aaron Mark Thomas, Sharu Theresa Jose</author><pubDate>Mon, 16 Sep 2024 14:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10339v1</guid></item><item><title>The 20 questions game to distinguish large language models</title><link>http://arxiv.org/abs/2409.10338v1</link><description>In a parallel with the 20 questions game, we present a method to determinewhether two large language models (LLMs), placed in a black-box context, arethe same or not. The goal is to use a small set of (benign) binary questions,typically under 20. We formalize the problem and first establish a baselineusing a random selection of questions from known benchmark datasets, achievingan accuracy of nearly 100% within 20 questions. After showing optimal boundsfor this problem, we introduce two effective questioning heuristics able todiscriminate 22 LLMs by using half as many questions for the same task. Thesemethods offer significant advantages in terms of stealth and are thus ofinterest to auditors or copyright owners facing suspicions of model leaks.</description><author>Gurvan Richardeau, Erwan Le Merrer, Camilla Penzo, Gilles Tredan</author><pubDate>Mon, 16 Sep 2024 14:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10338v1</guid></item><item><title>Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering</title><link>http://arxiv.org/abs/2409.10335v1</link><description>We propose two novel ideas (adoption of deferred rendering and mesh-basedrepresentation) to improve the quality of 3D Gaussian splatting (3DGS) basedinverse rendering. We first report a problem incurred by hidden Gaussians,where Gaussians beneath the surface adversely affect the pixel color in thevolume rendering adopted by the existing methods. In order to resolve theproblem, we propose applying deferred rendering and report new problemsincurred in a naive application of deferred rendering to the existing3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-basedinverse rendering under deferred rendering, we propose a novel two-steptraining approach which (1) exploits mesh extraction and utilizes a hybridmesh-3DGS representation and (2) applies novel regularization methods to betterexploit the mesh. Our experiments show that, under relighting, the proposedmethod offers significantly better rendering quality than the existing3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-basedinverse rendering method, it gives better rendering quality while offeringreal-time rendering.</description><author>Euntae Choi, Sungjoo Yoo</author><pubDate>Mon, 16 Sep 2024 14:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10335v1</guid></item><item><title>Enhancing Spatio-temporal Quantile Forecasting with Curriculum Learning: Lessons Learned</title><link>http://arxiv.org/abs/2406.12709v2</link><description>Training models on spatio-temporal (ST) data poses an open problem due to thecomplicated and diverse nature of the data itself, and it is challenging toensure the model's performance directly trained on the original ST data. Whilelimiting the variety of training data can make training easier, it can alsolead to a lack of knowledge and information for the model, resulting in adecrease in performance. To address this challenge, we presented an innovativeparadigm that incorporates three separate forms of curriculum learningspecifically targeting from spatial, temporal, and quantile perspectives.Furthermore, our framework incorporates a stacking fusion module to combinediverse information from three types of curriculum learning, resulting in astrong and thorough learning process. We demonstrated the effectiveness of thisframework with extensive empirical evaluations, highlighting its betterperformance in addressing complex ST challenges. We provided thorough ablationstudies to investigate the effectiveness of our curriculum and to explain howit contributes to the improvement of learning efficiency on ST data.</description><author>Du Yin, Jinliang Deng, Shuang Ao, Zechen Li, Hao Xue, Arian Prabowo, Renhe Jiang, Xuan Song, Flora Salim</author><pubDate>Mon, 16 Sep 2024 14:44:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12709v2</guid></item><item><title>Research and Design of a Financial Intelligent Risk Control Platform Based on Big Data Analysis and Deep Machine Learning</title><link>http://arxiv.org/abs/2409.10331v1</link><description>In the financial field of the United States, the application of big datatechnology has become one of the important means for financial institutions toenhance competitiveness and reduce risks. The core objective of this article isto explore how to fully utilize big data technology to achieve completeintegration of internal and external data of financial institutions, and createan efficient and reliable platform for big data collection, storage, andanalysis. With the continuous expansion and innovation of financial business,traditional risk management models are no longer able to meet the increasinglycomplex market demands. This article adopts big data mining and real-timestreaming data processing technology to monitor, analyze, and alert variousbusiness data. Through statistical analysis of historical data and precisemining of customer transaction behavior and relationships, potential risks canbe more accurately identified and timely responses can be made. This articledesigns and implements a financial big data intelligent risk control platform.This platform not only achieves effective integration, storage, and analysis ofinternal and external data of financial institutions, but also intelligentlydisplays customer characteristics and their related relationships, as well asintelligent supervision of various risk information</description><author>Shuochen Bi, Yufan Lian, Ziyue Wang</author><pubDate>Mon, 16 Sep 2024 14:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10331v1</guid></item><item><title>DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving</title><link>http://arxiv.org/abs/2409.10330v1</link><description>Recent advancements in autonomous driving have seen a paradigm shift towardsend-to-end learning paradigms, which map sensory inputs directly to drivingactions, thereby enhancing the robustness and adaptability of autonomousvehicles. However, these models often sacrifice interpretability, posingsignificant challenges to trust, safety, and regulatory compliance. To addressthese issues, we introduce DRIVE -- Dependable Robust Interpretable VisionaryEnsemble Framework in Autonomous Driving, a comprehensive framework designed toimprove the dependability and stability of explanations in end-to-endunsupervised autonomous driving models. Our work specifically targets theinherent instability problems observed in the Driving through the ConceptGridlock (DCG) model, which undermine the trustworthiness of its explanationsand decision-making processes. We define four key attributes of DRIVE:consistent interpretability, stable interpretability, consistent output, andstable output. These attributes collectively ensure that explanations remainreliable and robust across different scenarios and perturbations. Throughextensive empirical evaluations, we demonstrate the effectiveness of ourframework in enhancing the stability and dependability of explanations, therebyaddressing the limitations of current models. Our contributions include anin-depth analysis of the dependability issues within the DCG model, a rigorousdefinition of DRIVE with its fundamental properties, a framework to implementDRIVE, and novel metrics for evaluating the dependability of concept-basedexplainable autonomous driving models. These advancements lay the groundworkfor the development of more reliable and trusted autonomous driving systems,paving the way for their broader acceptance and deployment in real-worldapplications.</description><author>Songning Lai, Tianlang Xue, Hongru Xiao, Lijie Hu, Jiemin Wu, Ninghui Feng, Runwei Guan, Haicheng Liao, Zhenning Li, Yutao Yue</author><pubDate>Mon, 16 Sep 2024 14:40:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10330v1</guid></item><item><title>Enhancing Next Destination Prediction: A Novel Long Short-Term Memory Neural Network Approach Using Real-World Airline Data</title><link>http://arxiv.org/abs/2401.12830v2</link><description>In the modern transportation industry, accurate prediction of travelers' nextdestinations brings multiple benefits to companies, such as customersatisfaction and targeted marketing. This study focuses on developing a precisemodel that captures the sequential patterns and dependencies in travel data,enabling accurate predictions of individual travelers' future destinations. Toachieve this, a novel model architecture with a sliding window approach basedon Long Short-Term Memory (LSTM) is proposed for destination prediction in thetransportation industry. The experimental results highlight satisfactoryperformance and high scores achieved by the proposed model across differentdata sizes and performance metrics. This research contributes to advancingdestination prediction methods, empowering companies to deliver personalizedrecommendations and optimize customer experiences in the dynamic travellandscape.</description><author>Salih Salihoglu, Gulser Koksal, Orhan Abar</author><pubDate>Mon, 16 Sep 2024 14:40:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12830v2</guid></item><item><title>InfoDisent: Explainability of Image Classification Models by Information Disentanglement</title><link>http://arxiv.org/abs/2409.10329v1</link><description>Understanding the decisions made by image classification networks is acritical area of research in deep learning. This task is traditionally dividedinto two distinct approaches: post-hoc methods and intrinsic methods. Post-hocmethods, such as GradCam, aim to interpret the decisions of pre-trained modelsby identifying regions of the image where the network focuses its attention.However, these methods provide only a high-level overview, making it difficultto fully understand the network's decision-making process. Conversely,intrinsic methods, like prototypical parts models, offer a more detailedunderstanding of network predictions but are constrained by specificarchitectures, training methods, and datasets. In this paper, we introduce InfoDisent, a hybrid model that combines theadvantages of both approaches. By utilizing an information bottleneck,InfoDisent disentangles the information in the final layer of a pre-traineddeep network, enabling the breakdown of classification decisions into basic,understandable atomic components. Unlike standard prototypical partsapproaches, InfoDisent can interpret the decisions of pre-trainedclassification networks and be used for making classification decisions,similar to intrinsic models. We validate the effectiveness of InfoDisent onbenchmark datasets such as ImageNet, CUB-200-2011, Stanford Cars, and StanfordDogs for both convolutional and transformer backbones.</description><author>Łukasz Struski, Jacek Tabor</author><pubDate>Mon, 16 Sep 2024 14:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10329v1</guid></item><item><title>Fuse4Seg: Image-Level Fusion Based Multi-Modality Medical Image Segmentation</title><link>http://arxiv.org/abs/2409.10328v1</link><description>Although multi-modality medical image segmentation holds significantpotential for enhancing the diagnosis and understanding of complex diseases byintegrating diverse imaging modalities, existing methods predominantly rely onfeature-level fusion strategies. We argue the current feature-level fusionstrategy is prone to semantic inconsistencies and misalignments across variousimaging modalities because it merges features at intermediate layers in aneural network without evaluative control. To mitigate this, we introduce anovel image-level fusion based multi-modality medical image segmentationmethod, Fuse4Seg, which is a bi-level learning framework designed to model theintertwined dependencies between medical image segmentation and medical imagefusion. The image-level fusion process is seamlessly employed to guide andenhance the segmentation results through a layered optimization approach.Besides, the knowledge gained from the segmentation module can effectivelyenhance the fusion module. This ensures that the resultant fused image is acoherent representation that accurately amalgamates information from allmodalities. Moreover, we construct a BraTS-Fuse benchmark based on BraTSdataset, which includes 2040 paired original images, multi-modal fusion images,and ground truth. This benchmark not only serves image-level medicalsegmentation but is also the largest dataset for medical image fusion to date.Extensive experiments on several public datasets and our benchmark demonstratethe superiority of our approach over prior state-of-the-art (SOTA)methodologies.</description><author>Yuchen Guo, Weifeng Su</author><pubDate>Mon, 16 Sep 2024 14:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10328v1</guid></item><item><title>Baking Relightable NeRF for Real-time Direct/Indirect Illumination Rendering</title><link>http://arxiv.org/abs/2409.10327v1</link><description>Relighting, which synthesizes a novel view under a given lighting condition(unseen in training time), is a must feature for immersive photo-realisticexperience. However, real-time relighting is challenging due to highcomputation cost of the rendering equation which requires shape and materialdecomposition and visibility test to model shadow. Additionally, for indirectillumination, additional computation of rendering equation on each secondarysurface point (where reflection occurs) is required rendering real-timerelighting challenging. We propose a novel method that executes a CNN rendererto compute primary surface points and rendering parameters, required for directillumination. We also present a lightweight hash grid-based renderer, forindirect illumination, which is recursively executed to perform the secondaryray tracing process. Both renderers are trained in a distillation from apre-trained teacher model and provide real-time physically-based renderingunder unseen lighting condition at a negligible loss of rendering quality.</description><author>Euntae Choi, Vincent Carpentier, Seunghun Shin, Sungjoo Yoo</author><pubDate>Mon, 16 Sep 2024 14:38:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10327v1</guid></item><item><title>On the Hardness of Meaningful Local Guarantees in Nonsmooth Nonconvex Optimization</title><link>http://arxiv.org/abs/2409.10323v1</link><description>We study the oracle complexity of nonsmooth nonconvex optimization, with thealgorithm assumed to have access only to local function information. It hasbeen shown by Davis, Drusvyatskiy, and Jiang (2023) that for nonsmoothLipschitz functions satisfying certain regularity and strictness conditions,perturbed gradient descent converges to local minimizers asymptotically.Motivated by this result and by other recent algorithmic advances in nonconvexnonsmooth optimization concerning Goldstein stationarity, we consider thequestion of obtaining a non-asymptotic rate of convergence to local minima forthis problem class. We provide the following negative answer to this question: Local algorithmsacting on regular Lipschitz functions cannot, in the worst case, providemeaningful local guarantees in terms of function value in sub-exponential time,even when all near-stationary points are global minima. This sharply contrastswith the smooth setting, for which it is well-known that standard gradientmethods can do so in a dimension-independent rate. Our result complements therich body of work in the theoretical computer science literature that providehardness results conditional on conjectures such as $\mathsf{P}\neq\mathsf{NP}$or cryptographic assumptions, in that ours holds unconditional of any suchassumptions.</description><author>Guy Kornowski, Swati Padmanabhan, Ohad Shamir</author><pubDate>Mon, 16 Sep 2024 14:35:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10323v1</guid></item><item><title>SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation</title><link>http://arxiv.org/abs/2409.10320v1</link><description>Verification and validation of autonomous driving (AD) systems and componentsis of increasing importance, as such technology increases in real-worldprevalence. Safety-critical scenario generation is a key approach to robustifyAD policies through closed-loop training. However, existing approaches forscenario generation rely on simplistic objectives, resulting inoverly-aggressive or non-reactive adversarial behaviors. To generate diverseadversarial yet realistic scenarios, we propose SEAL, a scenario perturbationapproach which leverages learned scoring functions and adversarial, human-likeskills. SEAL-perturbed scenarios are more realistic than SOTA baselines,leading to improved ego task success across real-world, in-distribution, andout-of-distribution scenarios, of more than 20%. To facilitate future research,we release our code and tools: https://github.com/cmubig/SEAL</description><author>Benjamin Stoler, Ingrid Navarro, Jonathan Francis, Jean Oh</author><pubDate>Mon, 16 Sep 2024 14:33:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10320v1</guid></item><item><title>Probabilistic energy forecasting through quantile regression in reproducing kernel Hilbert spaces</title><link>http://arxiv.org/abs/2408.04405v3</link><description>Accurate energy demand forecasting is crucial for sustainable and resilientenergy development. To meet the Net Zero Representative Concentration Pathways(RCP) $4.5$ scenario in the DACH countries, increased renewable energyproduction, energy storage, and reduced commercial building consumption areneeded. This scenario's success depends on hydroelectric capacity and climaticfactors. Informed decisions require quantifying uncertainty in forecasts. Thisstudy explores a non-parametric method based on \emph{reproducing kernelHilbert spaces (RKHS)}, known as kernel quantile regression, for energyprediction. Our experiments demonstrate its reliability and sharpness, and webenchmark it against state-of-the-art methods in load and price forecasting forthe DACH region. We offer our implementation in conjunction with additionalscripts to ensure the reproducibility of our research.</description><author>Luca Pernigo, Rohan Sen, Davide Baroli</author><pubDate>Mon, 16 Sep 2024 14:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04405v3</guid></item><item><title>Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data</title><link>http://arxiv.org/abs/2310.10461v3</link><description>Anomaly detection is the task of identifying abnormal samples in largeunlabeled datasets. While the advent of foundation models has produced powerfulzero-shot anomaly detection methods, their deployment in practice is oftenhindered by the absence of labeled validation data -- without it, theirdetection performance cannot be evaluated reliably. In this work, we proposeSWSA (Selection With Synthetic Anomalies): a general-purpose framework toselect image-based anomaly detectors without labeled validation data. Insteadof collecting labeled validation data, we generate synthetic anomalies withoutany training or fine-tuning, using only a small support set of normal images.Our synthetic anomalies are used to create detection tasks that compose avalidation framework for model selection. In an empirical study, we evaluateSWSA with three types of synthetic anomalies and on two selection tasks: modelselection of image-based anomaly detectors and prompt selection for CLIP-basedanomaly detection. SWSA often selects models and prompts that match selectionsmade with a ground-truth validation set, outperforming baseline selectionstrategies.</description><author>Clement Fung, Chen Qiu, Aodong Li, Maja Rudolph</author><pubDate>Mon, 16 Sep 2024 14:24:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10461v3</guid></item><item><title>Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction</title><link>http://arxiv.org/abs/2403.10586v2</link><description>Notorious for its 70-80% recurrence rate, Non-muscle-invasive Bladder Cancer(NMIBC) imposes a significant human burden and is one of the costliest cancersto manage. Current tools for predicting NMIBC recurrence rely on scoringsystems that often overestimate risk and have poor accuracy. This is whereMachine learning (ML)-based techniques have emerged as a promising approach forpredicting NMIBC recurrence by leveraging molecular and clinical data. Thiscomprehensive review paper critically analyses ML-based frameworks forpredicting NMIBC recurrence, focusing on their statistical robustness andalgorithmic efficacy. We meticulously examine the strengths and weaknesses ofeach study, by focusing on various prediction tasks, data modalities, and MLmodels, highlighting their remarkable performance alongside inherentlimitations. A diverse array of ML algorithms that leverage multimodal dataspanning radiomics, clinical, histopathological, and genomic data, exhibitsignificant promise in accurately predicting NMIBC recurrence. However, thepath to widespread adoption faces challenges concerning the generalisabilityand interpretability of models, emphasising the need for collaborative efforts,robust datasets, and the incorporation of cost-effectiveness. Our detailedcategorisation and in-depth analysis illuminate the nuances, complexities, andcontexts that influence real-world advancement and adoption of these AI-basedtechniques. This rigorous analysis equips researchers with a deeperunderstanding of the intricacies of the ML algorithms employed. Researchers canuse these insights to refine approaches, address limitations, and boostgeneralisability of their ML models, ultimately leading to reduced healthcarecosts and improved patient outcomes.</description><author>Saram Abbas, Rishad Shafik, Naeem Soomro, Rakesh Heer, Kabita Adhikari</author><pubDate>Mon, 16 Sep 2024 14:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10586v2</guid></item><item><title>Know your limits! Optimize the robot's behavior through self-awareness</title><link>http://arxiv.org/abs/2409.10308v1</link><description>As humanoid robots transition from labs to real-world environments, it isessential to democratize robot control for non-expert users. Recent human-robotimitation algorithms focus on following a reference human motion with highprecision, but they are susceptible to the quality of the reference motion andrequire the human operator to simplify its movements to match the robot'scapabilities. Instead, we consider that the robot should understand and adaptthe reference motion to its own abilities, facilitating the operator's task.For that, we introduce a deep-learning model that anticipates the robot'sperformance when imitating a given reference. Then, our system can generatemultiple references given a high-level task command, assign a score to each ofthem, and select the best reference to achieve the desired robot behavior. OurSelf-AWare model (SAW) ranks potential robot behaviors based on variouscriteria, such as fall likelihood, adherence to the reference motion, andsmoothness. We integrate advanced motion generation, robot control, and SAW inone unique system, ensuring optimal robot behavior for any task command. Forinstance, SAW can anticipate falls with 99.29% accuracy. For more informationcheck our project page: https://evm7.github.io/Self-AWare</description><author>Esteve Valls Mascaro, Dongheui Lee</author><pubDate>Mon, 16 Sep 2024 14:14:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10308v1</guid></item><item><title>Parameterized Approximation for Robust Clustering in Discrete Geometric Spaces</title><link>http://arxiv.org/abs/2305.07316v2</link><description>We consider the well-studied Robust $(k, z)$-Clustering problem, whichgeneralizes the classic $k$-Median, $k$-Means, and $k$-Center problems. Given aconstant $z\ge 1$, the input to Robust $(k, z)$-Clustering is a set $P$ of $n$weighted points in a metric space $(M,\delta)$ and a positive integer $k$.Further, each point belongs to one (or more) of the $m$ many different groups$S_1,S_2,\ldots,S_m$. Our goal is to find a set $X$ of $k$ centers such that$\max_{i \in [m]} \sum_{p \in S_i} w(p) \delta(p,X)^z$ is minimized. This problem arises in the domains of robust optimization [Anthony, Goyal,Gupta, Nagarajan, Math. Oper. Res. 2010] and in algorithmic fairness. Forpolynomial time computation, an approximation factor of $O(\log m/\log\log m)$is known [Makarychev, Vakilian, COLT $2021$], which is tight under a plausiblecomplexity assumption even in the line metrics. For FPT time, there is a$(3^z+\epsilon)$-approximation algorithm, which is tight under GAP-ETH [Goyal,Jaiswal, Inf. Proc. Letters, 2023]. Motivated by the tight lower bounds for general discrete metrics, we focus on\emph{geometric} spaces such as the (discrete) high-dimensional Euclideansetting and metrics of low doubling dimension, which play an important role indata analysis applications. First, for a universal constant $\eta_0 &gt;0.0006$,we devise a $3^z(1-\eta_{0})$-factor FPT approximation algorithm for discretehigh-dimensional Euclidean spaces thereby bypassing the lower bound for generalmetrics. We complement this result by showing that even the special case of$k$-Center in dimension $\Theta(\log n)$ is $(\sqrt{3/2}- o(1))$-hard toapproximate for FPT algorithms. Finally, we complete the FPT approximationlandscape by designing an FPT $(1+\epsilon)$-approximation scheme (EPAS) forthe metric of sub-logarithmic doubling dimension.</description><author>Fateme Abbasi, Sandip Banerjee, Jarosław Byrka, Parinya Chalermsook, Ameet Gadekar, Kamyar Khodamoradi, Dániel Marx, Roohani Sharma, Joachim Spoerhase</author><pubDate>Mon, 16 Sep 2024 14:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07316v2</guid></item><item><title>How to do impactful research in artificial intelligence for chemistry and materials science</title><link>http://arxiv.org/abs/2409.10304v1</link><description>Machine learning has been pervasively touching many fields of science.Chemistry and materials science are no exception. While machine learning hasbeen making a great impact, it is still not reaching its full potential ormaturity. In this perspective, we first outline current applications across adiversity of problems in chemistry. Then, we discuss how machine learningresearchers view and approach problems in the field. Finally, we provide ourconsiderations for maximizing impact when researching machine learning forchemistry.</description><author>Austin Cheng, Cher Tian Ser, Marta Skreta, Andrés Guzmán-Cordero, Luca Thiede, Andreas Burger, Abdulrahman Aldossary, Shi Xuan Leong, Sergio Pablo-García, Felix Strieth-Kalthoff, Alán Aspuru-Guzik</author><pubDate>Mon, 16 Sep 2024 14:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10304v1</guid></item><item><title>On Synthetic Texture Datasets: Challenges, Creation, and Curation</title><link>http://arxiv.org/abs/2409.10297v1</link><description>The influence of textures on machine learning models has been an ongoinginvestigation, specifically in texture bias/learning, interpretability, androbustness. However, due to the lack of large and diverse texture dataavailable, the findings in these works have been limited, as more comprehensiveevaluations have not been feasible. Image generative models are able to providedata creation at scale, but utilizing these models for texture synthesis hasbeen unexplored and poses additional challenges both in creating accuratetexture images and validating those images. In this work, we introduce anextensible methodology and corresponding new dataset for generatinghigh-quality, diverse texture images capable of supporting a broad set oftexture-based tasks. Our pipeline consists of: (1) developing prompts from arange of descriptors to serve as input to text-to-image models, (2) adoptingand adapting Stable Diffusion pipelines to generate and filter thecorresponding images, and (3) further filtering down to the highest qualityimages. Through this, we create the Prompted Textures Dataset (PTD), a datasetof 362,880 texture images that span 56 textures. During the process ofgenerating images, we find that NSFW safety filters in image generationpipelines are highly sensitive to texture (and flag up to 60\% of our textureimages), uncovering a potential bias in these models and presenting uniquechallenges when working with texture data. Through both standard metrics and ahuman evaluation, we find that our dataset is high quality and diverse.</description><author>Blaine Hoak, Patrick McDaniel</author><pubDate>Mon, 16 Sep 2024 14:02:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10297v1</guid></item><item><title>MGSA: Multi-granularity Graph Structure Attention for Knowledge Graph-to-Text Generation</title><link>http://arxiv.org/abs/2409.10294v1</link><description>The Knowledge Graph-to-Text Generation task aims to convert structuredknowledge graphs into coherent and human-readable natural language text. Recentefforts in this field have focused on enhancing pre-trained language models(PLMs) by incorporating graph structure information to capture the intricatestructure details of knowledge graphs. However, most of these approaches tendto capture only single-granularity structure information, concentrating eitheron the relationships between entities within the original graph or on therelationships between words within the same entity or across differententities. This narrow focus results in a significant limitation: models thatconcentrate solely on entity-level structure fail to capture the nuancedsemantic relationships between words, while those that focus only on word-levelstructure overlook the broader relationships between original entire entities.To overcome these limitations, this paper introduces the Multi-granularityGraph Structure Attention (MGSA), which is based on PLMs. The encoder of themodel architecture features an entity-level structure encoding module, aword-level structure encoding module, and an aggregation module thatsynthesizes information from both structure. This multi-granularity structureencoding approach allows the model to simultaneously capture both entity-leveland word-level structure information, providing a more comprehensiveunderstanding of the knowledge graph's structure information, therebysignificantly improving the quality of the generated text. We conductedextensive evaluations of the MGSA model using two widely recognized KG-to-TextGeneration benchmark datasets, WebNLG and EventNarrative, where it consistentlyoutperformed models that rely solely on single-granularity structureinformation, demonstrating the effectiveness of our approach.</description><author>Shanshan Wang, Chun Zhang, Ning Zhang</author><pubDate>Mon, 16 Sep 2024 14:01:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10294v1</guid></item><item><title>SPAC: Sampling-based Progressive Attribute Compression for Dense Point Clouds</title><link>http://arxiv.org/abs/2409.10293v1</link><description>We propose an end-to-end attribute compression method for dense point clouds.The proposed method combines a frequency sampling module, an adaptive scalefeature extraction module with geometry assistance, and a global hyperpriorentropy model. The frequency sampling module uses a Hamming window and the FastFourier Transform to extract high-frequency components of the point cloud. Thedifference between the original point cloud and the sampled point cloud isdivided into multiple sub-point clouds. These sub-point clouds are thenpartitioned using an octree, providing a structured input for featureextraction. The feature extraction module integrates adaptive convolutionallayers and uses offset-attention to capture both local and global features.Then, a geometry-assisted attribute feature refinement module is used to refinethe extracted attribute features. Finally, a global hyperprior model isintroduced for entropy encoding. This model propagates hyperprior parametersfrom the deepest (base) layer to the other layers, further enhancing theencoding efficiency. At the decoder, a mirrored network is used toprogressively restore features and reconstruct the color attribute throughtransposed convolutional layers. The proposed method encodes base layerinformation at a low bitrate and progressively adds enhancement layerinformation to improve reconstruction accuracy. Compared to the latest G-PCCtest model (TMC13v23) under the MPEG common test conditions (CTCs), theproposed method achieved an average Bjontegaard delta bitrate reduction of24.58% for the Y component (21.23% for YUV combined) on the MPEG Category Soliddataset and 22.48% for the Y component (17.19% for YUV combined) on the MPEGCategory Dense dataset. This is the first instance of a learning-based codecoutperforming the G-PCC standard on these datasets under the MPEG CTCs.</description><author>Xiaolong Mao, Hui Yuan, Tian Guo, Shiqi Jiang, Raouf Hamzaoui, Sam Kwong</author><pubDate>Mon, 16 Sep 2024 13:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10293v1</guid></item><item><title>Anatomical Positional Embeddings</title><link>http://arxiv.org/abs/2409.10291v1</link><description>We propose a self-supervised model producing 3D anatomical positionalembeddings (APE) of individual medical image voxels. APE encodes voxels'anatomical closeness, i.e., voxels of the same organ or nearby organs alwayshave closer positional embeddings than the voxels of more distant body parts.In contrast to the existing models of anatomical positional embeddings, ourmethod is able to efficiently produce a map of voxel-wise embeddings for awhole volumetric input image, which makes it an optimal choice for differentdownstream applications. We train our APE model on 8400 publicly available CTimages of abdomen and chest regions. We demonstrate its superior performancecompared with the existing models on anatomical landmark retrieval andweakly-supervised few-shot localization of 13 abdominal organs. As a practicalapplication, we show how to cheaply train APE to crop raw CT images todifferent anatomical regions of interest with 0.99 recall, while reducing theimage volume by 10-100 times. The code and the pre-trained APE model areavailable at https://github.com/mishgon/ape .</description><author>Mikhail Goncharov, Valentin Samokhin, Eugenia Soboleva, Roman Sokolov, Boris Shirokikh, Mikhail Belyaev, Anvar Kurmukov, Ivan Oseledets</author><pubDate>Mon, 16 Sep 2024 13:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10291v1</guid></item></channel></rss>