<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 11 Mar 2024 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos</title><link>http://arxiv.org/abs/2403.05535v1</link><description>We introduce LaGTran, a novel framework that utilizes readily available oreasily acquired text descriptions to guide robust transfer of discriminativeknowledge from labeled source to unlabeled target data with domain shifts.While unsupervised adaptation methods have been established to address thisproblem, they show limitations in handling challenging domain shifts due totheir exclusive operation within the pixel-space. Motivated by our observationthat semantically richer text modality has more favorable transfer properties,we devise a transfer mechanism to use a source-trained text-classifier togenerate predictions on the target text descriptions, and utilize thesepredictions as supervision for the corresponding images. Our approach driven bylanguage guidance is surprisingly easy and simple, yet significantlyoutperforms all prior approaches on challenging datasets like GeoNet andDomainNet, validating its extreme effectiveness. To further extend the scope ofour study beyond images, we introduce a new benchmark to study ego-exo transferin videos and find that our language-aided LaGTran yields significant gains inthis highly challenging and non-trivial transfer setting. Code, models, andproposed datasets are publicly available athttps://tarun005.github.io/lagtran/.</description><author>Tarun Kalluri, Bodhisattwa Prasad Majumder, Manmohan Chandraker</author><pubDate>Fri, 08 Mar 2024 18:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05535v1</guid></item><item><title>Bayesian Preference Elicitation with Language Models</title><link>http://arxiv.org/abs/2403.05534v1</link><description>Aligning AI systems to users' interests requires understanding andincorporating humans' complex values and preferences. Recently, language models(LMs) have been used to gather information about the preferences of humanusers. This preference data can be used to fine-tune or guide other LMs and/orAI systems. However, LMs have been shown to struggle with crucial aspects ofpreference learning: quantifying uncertainty, modeling human mental states, andasking informative questions. These challenges have been addressed in otherareas of machine learning, such as Bayesian Optimal Experimental Design (BOED),which focus on designing informative queries within a well-defined featurespace. But these methods, in turn, are difficult to scale and apply toreal-world problems where simply identifying the relevant features can bedifficult. We introduce OPEN (Optimal Preference Elicitation with Naturallanguage) a framework that uses BOED to guide the choice of informativequestions and an LM to extract features and translate abstract BOED queriesinto natural language questions. By combining the flexibility of LMs with therigor of BOED, OPEN can optimize the informativity of queries while remainingadaptable to real-world domains. In user studies, we find that OPEN outperformsexisting LM- and BOED-based methods for preference elicitation.</description><author>Kunal Handa, Yarin Gal, Ellie Pavlick, Noah Goodman, Jacob Andreas, Alex Tamkin, Belinda Z. Li</author><pubDate>Fri, 08 Mar 2024 18:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05534v1</guid></item><item><title>Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets</title><link>http://arxiv.org/abs/2403.05532v1</link><description>We introduce Tune without Validation (Twin), a pipeline for tuning learningrate and weight decay without validation sets. We leverage a recent theoreticalframework concerning learning phases in hypothesis space to devise a heuristicthat predicts what hyper-parameter (HP) combinations yield bettergeneralization. Twin performs a grid search of trials according to anearly-/non-early-stopping scheduler and then segments the region that providesthe best results in terms of training loss. Among these trials, the weight normstrongly correlates with predicting generalization. To assess the effectivenessof Twin, we run extensive experiments on 20 image classification datasets andtrain several families of deep networks, including convolutional, transformer,and feed-forward models. We demonstrate proper HP selection when training fromscratch and fine-tuning, emphasizing small-sample scenarios.</description><author>Lorenzo Brigato, Stavroula Mougiakakou</author><pubDate>Fri, 08 Mar 2024 18:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05532v1</guid></item><item><title>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title><link>http://arxiv.org/abs/2403.05530v1</link><description>In this report, we present the latest model of the Gemini family, Gemini 1.5Pro, a highly compute-efficient multimodal mixture-of-experts model capable ofrecalling and reasoning over fine-grained information from millions of tokensof context, including multiple long documents and hours of video and audio.Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasksacross modalities, improves the state-of-the-art in long-document QA,long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra'sstate-of-the-art performance across a broad set of benchmarks. Studying thelimits of Gemini 1.5 Pro's long-context ability, we find continued improvementin next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10Mtokens, a generational leap over existing models such as Claude 2.1 (200k) andGPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of largelanguage models at the frontier; when given a grammar manual for Kalamang, alanguage with fewer than 200 speakers worldwide, the model learns to translateEnglish to Kalamang at a similar level to a person who learned from the samecontent.</description><author>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezene</author><pubDate>Fri, 08 Mar 2024 18:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05530v1</guid></item><item><title>Improving Probability-based Prompt Selection Through Unified Evaluation and Analysis</title><link>http://arxiv.org/abs/2305.14877v2</link><description>Previous works in prompt engineering for large language models haveintroduced different gradient-free probability-based prompt selection methodsthat aim to choose the optimal prompt among the candidates for a given task buthave failed to provide a comprehensive and fair comparison between each other.In this paper, we propose a unified framework to interpret and evaluate theexisting probability-based prompt selection methods by performing extensiveexperiments on 13 common and diverse NLP tasks. We find that each of theexisting methods can be interpreted as some variant of the method thatmaximizes mutual information between the input and the predicted output (MI).Utilizing this finding, we develop several other combinatorial variants of MIand increase the effectiveness of the oracle prompt selection method from87.79% to 94.98%, measured as the ratio of the performance of the selectedprompt to that of the optimal oracle prompt. Furthermore, considering that allthe methods rely on the output probability distribution of the model that mightbe biased, we propose a novel calibration method called Calibration byMarginalization (CBM) that is orthogonal to the existing methods and helpsincrease the prompt selection effectiveness of the best method to 96.85%,achieving 99.44% of the oracle prompt F1 without calibration.</description><author>Sohee Yang, Jonghyeon Kim, Joel Jang, Seonghyeon Ye, Hyunji Lee, Minjoon Seo</author><pubDate>Fri, 08 Mar 2024 18:51:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14877v2</guid></item><item><title>The Computational Complexity of Learning Gaussian Single-Index Models</title><link>http://arxiv.org/abs/2403.05529v1</link><description>Single-Index Models are high-dimensional regression problems with plantedstructure, whereby labels depend on an unknown one-dimensional projection ofthe input via a generic, non-linear, and potentially non-deterministictransformation. As such, they encompass a broad class of statistical inferencetasks, and provide a rich template to study statistical and computationaltrade-offs in the high-dimensional regime. While the information-theoretic sample complexity to recover the hiddendirection is linear in the dimension $d$, we show that computationallyefficient algorithms, both within the Statistical Query (SQ) and the Low-DegreePolynomial (LDP) framework, necessarily require $\Omega(d^{k^\star/2})$samples, where $k^\star$ is a "generative" exponent associated with the modelthat we explicitly characterize. Moreover, we show that this sample complexityis also sufficient, by establishing matching upper bounds using a partial-tracealgorithm. Therefore, our results provide evidence of a sharpcomputational-to-statistical gap (under both the SQ and LDP class) whenever$k^\star&gt;2$. To complete the study, we provide examples of smooth and Lipschitzdeterministic target functions with arbitrarily large generative exponents$k^\star$.</description><author>Alex Damian, Loucas Pillaud-Vivien, Jason D. Lee, Joan Bruna</author><pubDate>Fri, 08 Mar 2024 18:50:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05529v1</guid></item><item><title>GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM</title><link>http://arxiv.org/abs/2403.05527v1</link><description>Key-value (KV) caching has become the de-facto to accelerate generation speedfor large language models (LLMs) inference. However, the growing cache demandwith increasing sequence length has transformed LLM inference to be a memorybound problem, significantly constraining the system throughput. Existingmethods rely on dropping unimportant tokens or quantizing all entriesuniformly. Such methods, however, often incur high approximation errors torepresent the compressed matrices. The autoregressive decoding process furthercompounds the error of each step, resulting in critical deviation in modelgeneration and deterioration of performance. To tackle this challenge, wepropose GEAR, an efficient KV cache compression framework that achievesnear-lossless high-ratio compression. GEAR first applies quantization tomajority of entries of similar magnitudes to ultra-low precision. It thenemploys a low rank matrix to approximate the quantization error, and a sparsematrix to remedy individual errors from outlier entries. By adeptly integratingthree techniques, GEAR is able to fully exploit their synergistic potentials.Our experiments demonstrate that compared to alternatives, GEAR achievesnear-lossless 4-bit KV cache compression with up to 2.38x throughputimprovement, while reducing peak-memory size up to 2.29x. Our code is publiclyavailable at https://github.com/HaoKang-Timmy/GEAR.</description><author>Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao</author><pubDate>Fri, 08 Mar 2024 18:48:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05527v1</guid></item><item><title>DeepSeek-VL: Towards Real-World Vision-Language Understanding</title><link>http://arxiv.org/abs/2403.05525v1</link><description>We present DeepSeek-VL, an open-source Vision-Language (VL) Model designedfor real-world vision and language understanding applications. Our approach isstructured around three key dimensions: We strive to ensure our data is diverse, scalable, and extensively coversreal-world scenarios including web screenshots, PDFs, OCR, charts, andknowledge-based content, aiming for a comprehensive representation of practicalcontexts. Further, we create a use case taxonomy from real user scenarios andconstruct an instruction tuning dataset accordingly. The fine-tuning with thisdataset substantially improves the model's user experience in practicalapplications. Considering efficiency and the demands of most real-worldscenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficientlyprocesses high-resolution images (1024 x 1024), while maintaining a relativelylow computational overhead. This design choice ensures the model's ability tocapture critical semantic and detailed information across various visual tasks.We posit that a proficient Vision-Language Model should, foremost, possessstrong language abilities. To ensure the preservation of LLM capabilitiesduring pretraining, we investigate an effective VL pretraining strategy byintegrating LLM training from the beginning and carefully managing thecompetitive dynamics observed between vision and language modalities. The DeepSeek-VL family (both 1.3B and 7B models) showcases superior userexperiences as a vision-language chatbot in real-world applications, achievingstate-of-the-art or competitive performance across a wide range ofvisual-language benchmarks at the same model size while maintaining robustperformance on language-centric benchmarks. We have made both 1.3B and 7Bmodels publicly accessible to foster innovations based on this foundationmodel.</description><author>Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan</author><pubDate>Fri, 08 Mar 2024 18:46:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05525v1</guid></item><item><title>Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapola</title><link>http://arxiv.org/abs/2403.05523v1</link><description>Out-of-distribution (OOD) generalization is a favorable yet challengingproperty for deep neural networks. The core challenges lie in the limitedavailability of source domains that help models learn an invariantrepresentation from the spurious features. Various domain augmentation havebeen proposed but largely rely on interpolating existing domains and frequentlyface difficulties in creating truly "novel" domains. Humans, on the other hand,can easily extrapolate novel domains, thus, an intriguing question arises: Howcan neural networks extrapolate like humans and achieve OOD generalization? We introduce a novel approach to domain extrapolation that leveragesreasoning ability and the extensive knowledge encapsulated within largelanguage models (LLMs) to synthesize entirely new domains. Starting with theclass of interest, we query the LLMs to extract relevant knowledge for thesenovel domains. We then bridge the gap between the text-centric knowledgederived from LLMs and the pixel input space of the model using text-to-imagegeneration techniques. By augmenting the training set of domain generalizationdatasets with high-fidelity, photo-realistic images of these new domains, weachieve significant improvements over all existing methods, as demonstrated inboth single and multi-domain generalization across various benchmarks. With the ability to extrapolate any domains for any class, our method has thepotential to learn a generalized model for any task without any data. Toillustrate, we put forth a much more difficult setting termed, data-free domaingeneralization, that aims to learn a generalized model in the absence of anycollected data. Our empirical findings support the above argument and ourmethods exhibit commendable performance in this setting, even surpassing thesupervised setting by approximately 1-2\% on datasets such as VLCS.</description><author>Yijiang Li, Sucheng Ren, Weipeng Deng, Yuzhi Xu, Ying Gao, Edith Ngai, Haohan Wang</author><pubDate>Fri, 08 Mar 2024 18:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05523v1</guid></item><item><title>Probabilistic Image-Driven Traffic Modeling via Remote Sensing</title><link>http://arxiv.org/abs/2403.05521v1</link><description>This work addresses the task of modeling spatiotemporal traffic patternsdirectly from overhead imagery, which we refer to as image-driven trafficmodeling. We extend this line of work and introduce a multi-modal, multi-tasktransformer-based segmentation architecture that can be used to create densecity-scale traffic models. Our approach includes a geo-temporal positionalencoding module for integrating geo-temporal context and a probabilisticobjective function for estimating traffic speeds that naturally models temporalvariations. We evaluate our method extensively using the Dynamic Traffic Speeds(DTS) benchmark dataset and significantly improve the state-of-the-art.Finally, we introduce the DTS++ dataset to support mobility-related locationadaptation experiments.</description><author>Scott Workman, Armin Hadzic</author><pubDate>Fri, 08 Mar 2024 18:43:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05521v1</guid></item><item><title>Authorship Attribution in Bangla Literature (AABL) via Transfer Learning using ULMFiT</title><link>http://arxiv.org/abs/2403.05519v1</link><description>Authorship Attribution is the task of creating an appropriatecharacterization of text that captures the authors' writing style to identifythe original author of a given piece of text. With increased anonymity on theinternet, this task has become increasingly crucial in various security andplagiarism detection fields. Despite significant advancements in otherlanguages such as English, Spanish, and Chinese, Bangla lacks comprehensiveresearch in this field due to its complex linguistic feature and sentencestructure. Moreover, existing systems are not scalable when the number ofauthor increases, and the performance drops for small number of samples perauthor. In this paper, we propose the use of Average-Stochastic GradientDescent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and aneffective transfer learning approach that addresses the problem of complexlinguistic features extraction and scalability for authorship attribution inBangla Literature (AABL). We analyze the effect of different tokenization, suchas word, sub-word, and character level tokenization, and demonstrate theeffectiveness of these tokenizations in the proposed model. Moreover, weintroduce the publicly available Bangla Authorship Attribution Dataset of 16authors (BAAD16) containing 17,966 sample texts and 13.4+ million words tosolve the standard dataset scarcity problem and release six variations ofpre-trained language models for use in any Bangla NLP downstream task. Forevaluation, we used our developed BAAD16 dataset as well as other publiclyavailable datasets. Empirically, our proposed model outperformedstate-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.Furthermore, we showed that the proposed system scales much better even with anincreasing number of authors, and performance remains steady despite fewtraining samples.</description><author>Aisha Khatun, Anisur Rahman, Md Saiful Islam, Hemayet Ahmed Chowdhury, Ayesha Tasnim</author><pubDate>Fri, 08 Mar 2024 18:42:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05519v1</guid></item><item><title>MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction</title><link>http://arxiv.org/abs/2402.03762v5</link><description>Monocular SLAM has received a lot of attention due to its simple RGB inputsand the lifting of complex sensor constraints. However, existing monocular SLAMsystems are designed for bounded scenes, restricting the applicability of SLAMsystems. To address this limitation, we propose MoD-SLAM, the first monocularNeRF-based dense mapping method that allows 3D reconstruction in real-time inunbounded scenes. Specifically, we introduce a Gaussian-based unbounded scenerepresentation approach to solve the challenge of mapping scenes withoutboundaries. This strategy is essential to extend the SLAM application.Moreover, a depth estimation module in the front-end is designed to extractaccurate priori depth values to supervise mapping and tracking processes. Byintroducing a robust depth loss term into the tracking process, our SLAM systemachieves more precise pose estimation in large-scale scenes. Our experiments ontwo standard datasets show that MoD-SLAM achieves competitive performance,improving the accuracy of the 3D reconstruction and localization by up to 30%and 15% respectively compared with existing state-of-the-art monocular SLAMsystems.</description><author>Heng Zhou, Zhetao Guo, Shuhong Liu, Lechen Zhang, Qihao Wang, Yuxiang Ren, Mingrui Li</author><pubDate>Fri, 08 Mar 2024 18:42:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03762v5</guid></item><item><title>Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought</title><link>http://arxiv.org/abs/2403.05518v1</link><description>While chain-of-thought prompting (CoT) has the potential to improve theexplainability of language model reasoning, it can systematically misrepresentthe factors influencing models' behavior--for example, rationalizing answers inline with a user's opinion without mentioning this bias. To mitigate thisbiased reasoning problem, we introduce bias-augmented consistency training(BCT), an unsupervised fine-tuning scheme that trains models to give consistentreasoning across prompts with and without biasing features. We construct asuite testing nine forms of biased reasoning on seven question-answering tasks,and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate ofbiased reasoning by 86% on held-out tasks. Moreover, this model generalizes toother forms of bias, reducing biased reasoning on held-out biases by an averageof 37%. As BCT generalizes to held-out biases and does not require gold labels,this method may hold promise for reducing biased reasoning from as-of-yetunknown biases and on tasks where supervision for ground truth reasoning isunavailable.</description><author>James Chua, Edward Rees, Hunar Batra, Samuel R. Bowman, Julian Michael, Ethan Perez, Miles Turpin</author><pubDate>Fri, 08 Mar 2024 18:41:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05518v1</guid></item><item><title>Settling the Sample Complexity of Model-Based Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2204.05275v4</link><description>This paper is concerned with offline reinforcement learning (RL), whichlearns using pre-collected data without further exploration. Effective offlineRL would be able to accommodate distribution shift and limited data coverage.However, prior algorithms or analyses either suffer from suboptimal samplecomplexities or incur high burn-in cost to reach sample optimality, thus posingan impediment to efficient offline RL in sample-starved applications. We demonstrate that the model-based (or "plug-in") approach achievesminimax-optimal sample complexity without burn-in cost for tabular Markovdecision processes (MDPs). Concretely, consider a finite-horizon (resp.$\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$(resp. effective horizon $\frac{1}{1-\gamma}$), and suppose the distributionshift of data is reflected by some single-policy clipped concentrabilitycoefficient $C^{\star}_{\text{clipped}}$. We prove that model-based offline RLyields $\varepsilon$-accuracy with a sample complexity of \[ \begin{cases}\frac{H^{4}SC_{\text{clipped}}^{\star}}{\varepsilon^{2}} &amp;(\text{finite-horizon MDPs})\frac{SC_{\text{clipped}}^{\star}}{(1-\gamma)^{3}\varepsilon^{2}} &amp;(\text{infinite-horizon MDPs}) \end{cases} \] up to log factor, which isminimax optimal for the entire $\varepsilon$-range. The proposed algorithms are"pessimistic" variants of value iteration with Bernstein-style penalties, anddo not require sophisticated variance reduction. Our analysis framework isestablished upon delicate leave-one-out decoupling arguments in conjunctionwith careful self-bounding techniques tailored to MDPs.</description><author>Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, Yuting Wei</author><pubDate>Fri, 08 Mar 2024 18:40:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.05275v4</guid></item><item><title>Pix2Gif: Motion-Guided Diffusion for GIF Generation</title><link>http://arxiv.org/abs/2403.04634v2</link><description>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)generation. We tackle this problem differently by formulating the task as animage translation problem steered by text and motion magnitude prompts, asshown in teaser fig. To ensure that the model adheres to motion guidance, wepropose a new motion-guided warping module to spatially transform the featuresof the source image conditioned on the two types of prompts. Furthermore, weintroduce a perceptual loss to ensure the transformed feature map remainswithin the same space as the target image, ensuring content consistency andcoherence. In preparation for the model training, we meticulously curated databy extracting coherent image frames from the TGIF video-caption dataset, whichprovides rich information about the temporal changes of subjects. Afterpretraining, we apply our model in a zero-shot manner to a number of videodatasets. Extensive qualitative and quantitative experiments demonstrate theeffectiveness of our model -- it not only captures the semantic prompt fromtext but also the spatial ones from motion guidance. We train all our modelsusing a single node of 16xV100 GPUs. Code, dataset and models are made publicat: https://hiteshk03.github.io/Pix2Gif/.</description><author>Hitesh Kandala, Jianfeng Gao, Jianwei Yang</author><pubDate>Fri, 08 Mar 2024 18:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04634v2</guid></item><item><title>Intriguing Properties of Input-dependent Randomized Smoothing</title><link>http://arxiv.org/abs/2110.05365v3</link><description>Randomized smoothing is currently considered the state-of-the-art method toobtain certifiably robust classifiers. Despite its remarkable performance, themethod is associated with various serious problems such as "certified accuracywaterfalls", certification vs.\ accuracy trade-off, or even fairness issues.Input-dependent smoothing approaches have been proposed with intention ofovercoming these flaws. However, we demonstrate that these methods lack formalguarantees and so the resulting certificates are not justified. We show that ingeneral, the input-dependent smoothing suffers from the curse ofdimensionality, forcing the variance function to have low semi-elasticity. Onthe other hand, we provide a theoretical and practical framework that enablesthe usage of input-dependent smoothing even in the presence of the curse ofdimensionality, under strict restrictions. We present one concrete design ofthe smoothing variance function and test it on CIFAR10 and MNIST. Our designmitigates some of the problems of classical smoothing and is formallyunderlined, yet further improvement of the design is still necessary.</description><author>Peter Súkeník, Aleksei Kuvshinov, Stephan Günnemann</author><pubDate>Fri, 08 Mar 2024 18:10:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.05365v3</guid></item><item><title>RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization</title><link>http://arxiv.org/abs/2401.14280v2</link><description>This study addresses the challenge of extending Large Language Models (LLMs)to non-English languages using non-Roman scripts. We propose an approach thatutilizes the romanized form of text as an interface for LLMs, hypothesizingthat its frequent informal use and shared tokens with English enhancecross-lingual alignment. Our approach involves the continual pretraining of anEnglish LLM like Llama 2 on romanized text of non-English, non-Roman scriptlanguages, followed by instruction tuning on romanized data. The resultsindicate that romanized text not only reduces token fertility by 2x-4x but alsomatches or outperforms native script representation across various NLU, NLG,and MT tasks. Moreover, the embeddings computed on romanized text exhibitcloser alignment with their English translations than those from the nativescript. Our approach presents a promising direction for leveraging the power ofEnglish LLMs in languages traditionally underrepresented in NLP.</description><author>Jaavid Aktar Husain, Raj Dabre, Aswanth Kumar, Jay Gala, Thanmay Jayakumar, Ratish Puduppully, Anoop Kunchukuttan</author><pubDate>Fri, 08 Mar 2024 18:04:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14280v2</guid></item><item><title>To Err Is Human, but Llamas Can Learn It Too</title><link>http://arxiv.org/abs/2403.05493v1</link><description>This study explores enhancing grammatical error correction (GEC) throughartificial error generation (AEG) using language models (LMs). Specifically, wefine-tune Llama 2-based LMs for error generation and find that this approachyields synthetic errors akin to human errors. Next, we train GEC Llama modelswith the help of these artificial errors and outperform previousstate-of-the-art error correction models, with gains ranging between 0.8 and 6F0.5 points across all tested languages (German, Ukrainian, and Estonian).Moreover, we demonstrate that generating errors by fine-tuning smallersequence-to-sequence models and prompting large commercial LMs (GPT-3.5 andGPT-4) also results in synthetic errors beneficially affecting error generationmodels.</description><author>Agnes Luhtaru, Taido Purason, Martin Vainikko, Maksym Del, Mark Fishel</author><pubDate>Fri, 08 Mar 2024 18:04:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05493v1</guid></item><item><title>On marginal feature attributions of tree-based models</title><link>http://arxiv.org/abs/2302.08434v3</link><description>Due to their power and ease of use, tree-based machine learning models, suchas random forests and gradient-boosted tree ensembles, have become verypopular. To interpret them, local feature attributions based on marginalexpectations, e.g. marginal (interventional) Shapley, Owen or Banzhaf values,may be employed. Such methods are true to the model and implementationinvariant, i.e. dependent only on the input-output function of the model. Wecontrast this with the popular TreeSHAP algorithm by presenting two(statistically similar) decision trees that compute the exact same function forwhich the "path-dependent" TreeSHAP yields different rankings of features,whereas the marginal Shapley values coincide. Furthermore, we discuss how theinternal structure of tree-based models may be leveraged to help with computingtheir marginal feature attributions according to a linear game value. Oneimportant observation is that these are simple (piecewise-constant) functionswith respect to a certain grid partition of the input space determined by thetrained model. Another crucial observation, showcased by experiments withXGBoost, LightGBM and CatBoost libraries, is that only a portion of allfeatures appears in a tree from the ensemble. Thus, the complexity of computingmarginal Shapley (or Owen or Banzhaf) feature attributions may be reduced. Thisremains valid for a broader class of game values which we shall axiomaticallycharacterize. A prime example is the case of CatBoost models where the treesare oblivious (symmetric) and the number of features in each of them is nolarger than the depth. We exploit the symmetry to derive an explicit formula,with improved complexity and only in terms of the internal model parameters,for marginal Shapley (and Banzhaf and Owen) values of CatBoost models. Thisresults in a fast, accurate algorithm for estimating these featureattributions.</description><author>Khashayar Filom, Alexey Miroshnikov, Konstandinos Kotsiopoulos, Arjun Ravi Kannan</author><pubDate>Fri, 08 Mar 2024 18:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08434v3</guid></item><item><title>Poly-View Contrastive Learning</title><link>http://arxiv.org/abs/2403.05490v1</link><description>Contrastive learning typically matches pairs of related views among a numberof unrelated negative views. Views can be generated (e.g. by augmentations) orbe observed. We investigate matching when there are more than two related viewswhich we call poly-view tasks, and derive new representation learningobjectives using information maximization and sufficient statistics. We showthat with unlimited computation, one should maximize the number of relatedviews, and with a fixed compute budget, it is beneficial to decrease the numberof unique samples whilst increasing the number of views of those samples. Inparticular, poly-view contrastive models trained for 128 epochs with batch size256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k,challenging the belief that contrastive models require large batch sizes andmany training epochs.</description><author>Amitis Shidani, Devon Hjelm, Jason Ramapuram, Russ Webb, Eeshan Gunesh Dhekane, Dan Busbridge</author><pubDate>Fri, 08 Mar 2024 17:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05490v1</guid></item><item><title>JointMotion: Joint Self-supervision for Joint Motion Prediction</title><link>http://arxiv.org/abs/2403.05489v1</link><description>We present JointMotion, a self-supervised learning method for joint motionprediction in autonomous driving. Our method includes a scene-level objectiveconnecting motion and environments, and an instance-level objective to refinelearned representations. Our evaluations show that these objectives arecomplementary and outperform recent contrastive and autoencoding methods aspre-training for joint motion prediction. Furthermore, JointMotion adapts toall common types of environment representations used for motion prediction(i.e., agent-centric, scene-centric, and pairwise relative), and enableseffective transfer learning between the Waymo Open Motion and the Argoverse 2Forecasting datasets. Notably, our method improves the joint final displacementerror of Wayformer, Scene Transformer, and HPTR by 3%, 7%, and 11%,respectively.</description><author>Royden Wagner, Ömer Şahin Taş, Marvin Klemp, Carlos Fernandez</author><pubDate>Fri, 08 Mar 2024 17:54:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05489v1</guid></item><item><title>FFSTC: Fongbe to French Speech Translation Corpus</title><link>http://arxiv.org/abs/2403.05488v1</link><description>In this paper, we introduce the Fongbe to French Speech Translation Corpus(FFSTC) for the first time. This corpus encompasses approximately 31 hours ofcollected Fongbe language content, featuring both French transcriptions andcorresponding Fongbe voice recordings. FFSTC represents a comprehensive datasetcompiled through various collection methods and the efforts of dedicatedindividuals. Furthermore, we conduct baseline experiments using Fairseq'stransformer_s and conformer models to evaluate data quality and validity. Ourresults indicate a score of 8.96 for the transformer_s model and 8.14 for theconformer model, establishing a baseline for the FFSTC corpus.</description><author>D. Fortune Kponou, Frejus A. A. Laleye, Eugene C. Ezin</author><pubDate>Fri, 08 Mar 2024 17:53:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05488v1</guid></item><item><title>Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach</title><link>http://arxiv.org/abs/2402.17987v2</link><description>Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs)involves transmitting Electromagnetic Waves (EMWs) and performing target typerecognition on the received radar echo, crucial for defense and aerospaceapplications. Previous studies highlighted the advantages of multistatic radarconfigurations over monostatic ones in RATR. However, fusion methods inmultistatic radar configurations often suboptimally combine classificationvectors from individual radars probabilistically. To address this, we propose afully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) toaggregate classification probability vectors from multiple radars. OBF, basedon expected 0-1 loss, updates a Recursive Bayesian Classification (RBC)posterior distribution for target UAV type, conditioned on historicalobservations across multiple time steps. We evaluate the approach usingsimulated random walk trajectories for seven drones, correlating target aspectangles to Radar Cross Section (RCS) measurements in an anechoic chamber.Comparing against single radar Automated Target Recognition (ATR) systems andsuboptimal fusion methods, our empirical results demonstrate that the OBFmethod integrated with RBC significantly enhances classification accuracycompared to other fusion methods and single radar configurations.</description><author>Michael Potter, Murat Akcakaya, Marius Necsoiu, Gunar Schirner, Deniz Erdogmus, Tales Imbiriba</author><pubDate>Fri, 08 Mar 2024 17:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17987v2</guid></item><item><title>Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting</title><link>http://arxiv.org/abs/2211.15856v3</link><description>Producing high-quality forecasts of key climate variables, such astemperature and precipitation, on subseasonal time scales has long been a gapin operational forecasting. This study explores an application of machinelearning (ML) models as post-processing tools for subseasonal forecasting.Lagged numerical ensemble forecasts (i.e., an ensemble where the members havedifferent initial dates) and observational data, including relative humidity,pressure at sea level, and geopotential height, are incorporated into variousML methods to predict monthly average precipitation and two-meter temperaturetwo weeks in advance for the continental United States. Regression, quantileregression, and tercile classification tasks using linear models, randomforests, convolutional neural networks, and stacked models (a multi-modelapproach based on the prediction of the individual ML models) are considered.Unlike previous ML approaches that often use ensemble mean alone, we leverageinformation embedded in the ensemble forecasts to enhance prediction accuracy.Additionally, we investigate extreme event predictions that are crucial forplanning and mitigation efforts. Considering ensemble members as a collectionof spatial forecasts, we explore different approaches to address spatialvariability. Trade-offs between different approaches may be mitigated withmodel stacking. Our proposed models outperform standard baselines such asclimatological forecasts and ensemble means. This paper further includes aninvestigation of feature importance, trade-offs between using the full ensembleor only the ensemble mean, and different modes of accounting for spatialvariability.</description><author>Elena Orlova, Haokun Liu, Raphael Rossellini, Benjamin Cash, Rebecca Willett</author><pubDate>Fri, 08 Mar 2024 17:45:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15856v3</guid></item><item><title>Will GPT-4 Run DOOM?</title><link>http://arxiv.org/abs/2403.05468v1</link><description>We show that GPT-4's reasoning and planning capabilities extend to the 1993first-person shooter Doom. This large language model (LLM) is able to run andplay the game with only a few instructions, plus a textualdescription--generated by the model itself from screenshots--about the state ofthe game being observed. We find that GPT-4 can play the game to a passabledegree: it is able to manipulate doors, combat enemies, and perform pathing.More complex prompting strategies involving multiple model calls provide betterresults. While further work is required to enable the LLM to play the game aswell as its classical, reinforcement learning-based counterparts, we note thatGPT-4 required no training, leaning instead on its own reasoning andobservational capabilities. We hope our work pushes the boundaries onintelligent, LLM-based agents in video games. We conclude by discussing theethical implications of our work.</description><author>Adrian de Wynter</author><pubDate>Fri, 08 Mar 2024 17:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05468v1</guid></item><item><title>Grasping Trajectory Optimization with Point Clouds</title><link>http://arxiv.org/abs/2403.05466v1</link><description>We introduce a new trajectory optimization method for robotic grasping basedon a point-cloud representation of robots and task spaces. In our method,robots are represented by 3D points on their link surfaces. The task space of arobot is represented by a point cloud that can be obtained from depth sensors.Using the point-cloud representation, goal reaching in grasping can beformulated as point matching, while collision avoidance can be efficientlyachieved by querying the signed distance values of the robot points in thesigned distance field of the scene points. Consequently, a constrainednon-linear optimization problem is formulated to solve the joint motion andgrasp planning problem. The advantage of our method is that the point-cloudrepresentation is general to be used with any robot in any environment. Wedemonstrate the effectiveness of our method by conducting experiments on atabletop scene and a shelf scene for grasping with a Fetch mobile manipulatorand a Franka Panda arm.</description><author>Yu Xiang, Sai Haneesh Allu, Rohith Peddi, Tyler Summers, Vibhav Gogate</author><pubDate>Fri, 08 Mar 2024 17:29:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05466v1</guid></item><item><title>Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference</title><link>http://arxiv.org/abs/2403.05465v1</link><description>Traditional Deep Neural Network (DNN) quantization methods using integer,fixed-point, or floating-point data types struggle to capture diverse DNNparameter distributions at low precision, and often require large siliconoverhead and intensive quantization-aware training. In this study, we introduceLogarithmic Posits (LP), an adaptive, hardware-friendly data type inspired byposits that dynamically adapts to DNN weight/activation distributions byparameterizing LP bit fields. We also develop a novel genetic-algorithm basedframework, LP Quantization (LPQ), to find optimal layer-wise LP parameterswhile reducing representational divergence between quantized and full-precisionmodels through a novel global-local contrastive objective. Additionally, wedesign a unified mixed-precision LP accelerator (LPA) architecture comprisingof processing elements (PEs) incorporating LP in the computational datapath.Our algorithm-hardware co-design demonstrates on average &lt;1% drop in top-1accuracy across various CNN and ViT models. It also achieves ~ 2x improvementsin performance per unit area and 2.2x gains in energy efficiency compared tostate-of-the-art quantization accelerators using different data types.</description><author>Akshat Ramachandran, Zishen Wan, Geonhwa Jeong, John Gustafson, Tushar Krishna</author><pubDate>Fri, 08 Mar 2024 17:28:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05465v1</guid></item><item><title>ContriMix: Scalable stain color augmentation for domain generalization without domain labels in digital pathology</title><link>http://arxiv.org/abs/2306.04527v4</link><description>Differences in staining and imaging procedures can cause significant colorvariations in histopathology images, leading to poor generalization whendeploying deep-learning models trained from a different data source. Variouscolor augmentation methods have been proposed to generate synthetic imagesduring training to make models more robust, eliminating the need for stainnormalization during test time. Many color augmentation methods leverage domainlabels to generate synthetic images. This approach causes three significantchallenges to scaling such a model. Firstly, incorporating data from a newdomain into deep-learning models trained on existing domain labels is notstraightforward. Secondly, dependency on domain labels prevents the use ofpathology images without domain labels to improve model performance. Finally,implementation of these methods becomes complicated when multiple domain labels(e.g., patient identification, medical center, etc) are associated with asingle image. We introduce ContriMix, a novel domain label free stain coloraugmentation method based on DRIT++, a style-transfer method. Contrimixleverages sample stain color variation within a training minibatch and randommixing to extract content and attribute information from pathology images. Thisinformation can be used by a trained ContriMix model to create synthetic imagesto improve the performance of existing classifiers. ContriMix outperformscompeting methods on the Camelyon17-WILDS dataset. Its performance isconsistent across different slides in the test set while being robust to thecolor variation from rare substances in pathology images. We make our code andtrained ContriMix models available for research use. The code for ContriMix canbe found at https://gitlab.com/huutan86/contrimix</description><author>Tan H. Nguyen, Dinkar Juyal, Jin Li, Aaditya Prakash, Shima Nofallah, Chintan Shah, Sai Chowdary Gullapally, Limin Yu, Michael Griffin, Anand Sampat, John Abel, Justin Lee, Amaro Taylor-Weiner</author><pubDate>Fri, 08 Mar 2024 17:28:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04527v4</guid></item><item><title>Can LLMs Follow Simple Rules?</title><link>http://arxiv.org/abs/2311.04235v3</link><description>As Large Language Models (LLMs) are deployed with increasing real-worldresponsibilities, it is important to be able to specify and constrain thebehavior of these systems in a reliable manner. Model developers may wish toset explicit rules for the model, such as "do not generate abusive content",but these may be circumvented by jailbreaking techniques. Existing evaluationsof adversarial attacks and defenses on LLMs generally require either expensivemanual review or unreliable heuristic checks. To address this issue, we proposeRule-following Language Evaluation Scenarios (RuLES), a programmatic frameworkfor measuring rule-following ability in LLMs. RuLES consists of 14 simple textscenarios in which the model is instructed to obey various rules whileinteracting with the user. Each scenario has a programmatic evaluation functionto determine whether the model has broken any rules in a conversation. Ourevaluations of proprietary and open models show that almost all current modelsstruggle to follow scenario rules, even on straightforward test cases. We alsodemonstrate that simple optimization attacks suffice to significantly increasefailure rates on test cases. We conclude by exploring two potential avenues forimprovement: test-time steering and supervised fine-tuning.</description><author>Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Basel Alomair, Dan Hendrycks, David Wagner</author><pubDate>Fri, 08 Mar 2024 17:04:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04235v3</guid></item><item><title>What makes an image realistic?</title><link>http://arxiv.org/abs/2403.04493v2</link><description>The last decade has seen tremendous progress in our ability to generaterealistic-looking data, be it images, text, audio, or video. Here, we discussthe closely related problem of quantifying realism, that is, designingfunctions that can reliably tell realistic data from unrealistic data. Thisproblem turns out to be significantly harder to solve and remains poorlyunderstood, despite its prevalence in machine learning and recent breakthroughsin generative AI. Drawing on insights from algorithmic information theory, wediscuss why this problem is challenging, why a good generative model alone isinsufficient to solve it, and what a good solution would look like. Inparticular, we introduce the notion of a universal critic, which unlikeadversarial critics does not require adversarial training. While universalcritics are not immediately practical, they can serve both as a North Star forguiding practical implementations and as a tool for analyzing existing attemptsto capture realism.</description><author>Lucas Theis</author><pubDate>Fri, 08 Mar 2024 17:02:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04493v2</guid></item><item><title>The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy</title><link>http://arxiv.org/abs/2403.05452v1</link><description>Radio-interferometric (RI) imaging entails solving high-resolutionhigh-dynamic range inverse problems from large data volumes. Recent imagereconstruction techniques grounded in optimization theory have demonstratedremarkable capability for imaging precision, well beyond CLEAN's capability.These range from advanced proximal algorithms propelled by handcraftedregularization operators, such as the SARA family, to hybrid plug-and-play(PnP) algorithms propelled by learned regularization denoisers, such as AIRI.Optimization and PnP structures are however highly iterative, which hinderstheir ability to handle the extreme data sizes expected from futureinstruments. To address this scalability challenge, we introduce a novel deeplearning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamicrange imaging'. R2D2's reconstruction is formed as a series of residual images,iteratively estimated as outputs of Deep Neural Networks (DNNs) taking theprevious iteration's image estimate and associated data residual as inputs. Itthus takes a hybrid structure between a PnP algorithm and a learned version ofthe matching pursuit algorithm that underpins CLEAN. We present a comprehensivestudy of our approach, featuring its multiple incarnations distinguished bytheir DNN architectures. We provide a detailed description of its trainingprocess, targeting a telescope-specific approach. R2D2's capability to deliverhigh precision is demonstrated in simulation, across a variety of image andobservation settings using the Very Large Array (VLA). Its reconstruction speedis also demonstrated: with only few iterations required to clean data residualsat dynamic ranges up to 105, R2D2 opens the door to fast precision imaging.R2D2 codes are available in the BASPLib library on GitHub.</description><author>Amir Aghabiglou, Chung San Chu, Arwa Dabbech, Yves Wiaux</author><pubDate>Fri, 08 Mar 2024 16:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05452v1</guid></item><item><title>Attention-guided Feature Distillation for Semantic Segmentation</title><link>http://arxiv.org/abs/2403.05451v1</link><description>In contrast to existing complex methodologies commonly employed fordistilling knowledge from a teacher to a student, the pro-posed methodshowcases the efficacy of a simple yet powerful method for utilizing refinedfeature maps to transfer attention. The proposed method has proven to beeffective in distilling rich information, outperforming existing methods insemantic segmentation as a dense prediction task. The proposed Attention-guidedFeature Distillation (AttnFD) method, em-ploys the Convolutional BlockAttention Module (CBAM), which refines feature maps by taking into account bothchannel-specific and spatial information content. By only using the MeanSquared Error (MSE) loss function between the refined feature maps of theteacher and the student,AttnFD demonstrates outstanding performance in semanticsegmentation, achieving state-of-the-art results in terms of mean Intersectionover Union (mIoU) on the PascalVoc 2012 and Cityscapes datasets. The Code isavailable at https://github.com/AmirMansurian/AttnFD.</description><author>Amir M. Mansourian, Arya Jalali, Rozhan Ahmadi, Shohreh Kasaei</author><pubDate>Fri, 08 Mar 2024 16:57:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05451v1</guid></item><item><title>G3DR: Generative 3D Reconstruction in ImageNet</title><link>http://arxiv.org/abs/2403.00939v2</link><description>We introduce a novel 3D generative method, Generative 3D Reconstruction(G3DR) in ImageNet, capable of generating diverse and high-quality 3D objectsfrom single images, addressing the limitations of existing methods. At theheart of our framework is a novel depth regularization technique that enablesthe generation of scenes with high-geometric fidelity. G3DR also leverages apretrained language-vision model, such as CLIP, to enable reconstruction innovel views and improve the visual realism of generations. Additionally, G3DRdesigns a simple but effective sampling procedure to further improve thequality of generations. G3DR offers diverse and efficient 3D asset generationbased on class or text conditioning. Despite its simplicity, G3DR is able tobeat state-of-theart methods, improving over them by up to 22% in perceptualmetrics and 90% in geometry scores, while needing only half of the trainingtime. Code is available at https://github.com/preddy5/G3DR</description><author>Pradyumna Reddy, Ismail Elezi, Jiankang Deng</author><pubDate>Fri, 08 Mar 2024 16:55:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00939v2</guid></item><item><title>An Improved Algorithm for Learning Drifting Discrete Distributions</title><link>http://arxiv.org/abs/2403.05446v1</link><description>We present a new adaptive algorithm for learning discrete distributions underdistribution drift. In this setting, we observe a sequence of independentsamples from a discrete distribution that is changing over time, and the goalis to estimate the current distribution. Since we have access to only a singlesample for each time step, a good estimation requires a careful choice of thenumber of past samples to use. To use more samples, we must resort to samplesfurther in the past, and we incur a drift error due to the bias introduced bythe change in distribution. On the other hand, if we use a small number of pastsamples, we incur a large statistical error as the estimation has a highvariance. We present a novel adaptive algorithm that can solve this trade-offwithout any prior knowledge of the drift. Unlike previous adaptive results, ouralgorithm characterizes the statistical error using data-dependent bounds. Thistechnicality enables us to overcome the limitations of the previous work thatrequire a fixed finite support whose size is known in advance and that cannotchange over time. Additionally, we can obtain tighter bounds depending on thecomplexity of the drifting distribution, and also consider distributions withinfinite support.</description><author>Alessio Mazzetto</author><pubDate>Fri, 08 Mar 2024 16:54:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05446v1</guid></item><item><title>Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices</title><link>http://arxiv.org/abs/2403.05441v1</link><description>We present a first study of Bayesian forecasting of electricity prices tradedon the German continuous intraday market which fully incorporates parameteruncertainty. Our target variable is the IDFull price index, forecasts are givenin terms of posterior predictive distributions. For validation we use theexceedingly volatile electricity prices of 2022, which have hardly been thesubject of forecasting studies before. As a benchmark model, we use allavailable intraday transactions at the time of forecast creation to compute acurrent value for the IDFull. According to the weak-form efficiency hypothesis,it would not be possible to significantly improve this benchmark built fromlast price information. We do, however, observe statistically significantimprovement in terms of both point measures and probability scores. Finally, wechallenge the declared gold standard of using LASSO for feature selection inelectricity price forecasting by presenting strong statistical evidence thatOrthogonal Matching Pursuit (OMP) leads to better forecasting performance.</description><author>Daniel Nickelsen, Gernot Müller</author><pubDate>Fri, 08 Mar 2024 16:51:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05441v1</guid></item><item><title>Is Cosine-Similarity of Embeddings Really About Similarity?</title><link>http://arxiv.org/abs/2403.05440v1</link><description>Cosine-similarity is the cosine of the angle between two vectors, orequivalently the dot product between their normalizations. A popularapplication is to quantify semantic similarity between high-dimensional objectsby applying cosine-similarity to a learned low-dimensional feature embedding.This can work better but sometimes also worse than the unnormalized dot-productbetween embedded vectors in practice. To gain insight into this empiricalobservation, we study embeddings derived from regularized linear models, whereclosed-form solutions facilitate analytical insights. We derive analyticallyhow cosine-similarity can yield arbitrary and therefore meaningless`similarities.' For some linear models the similarities are not even unique,while for others they are implicitly controlled by the regularization. Wediscuss implications beyond linear models: a combination of differentregularizations are employed when learning deep models; these have implicit andunintended effects when taking cosine-similarities of the resulting embeddings,rendering results opaque and possibly arbitrary. Based on these insights, wecaution against blindly using cosine-similarity and outline alternatives.</description><author>Harald Steck, Chaitanya Ekanadham, Nathan Kallus</author><pubDate>Fri, 08 Mar 2024 16:48:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05440v1</guid></item><item><title>Spectrally-Corrected and Regularized Linear Discriminant Analysis for Spiked Covariance Model</title><link>http://arxiv.org/abs/2210.03859v3</link><description>This paper proposes an improved linear discriminant analysis calledspectrally-corrected and regularized LDA (SRLDA). This method integrates thedesign ideas of the sample spectrally-corrected covariance matrix and theregularized discriminant analysis. With the support of a large-dimensionalrandom matrix analysis framework, it is proved that SRLDA has a linearclassification global optimal solution under the spiked model assumption.According to simulation data analysis, the SRLDA classifier performs betterthan RLDA and ILDA and is closer to the theoretical classifier. Experiments ondifferent data sets show that the SRLDA algorithm performs better inclassification and dimensionality reduction than currently used tools.</description><author>Hua Li, Wenya Luo, Zhidong Bai, Huanchao Zhou, Zhangni Pu</author><pubDate>Fri, 08 Mar 2024 16:48:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03859v3</guid></item><item><title>Blind Source Separation of Single-Channel Mixtures via Multi-Encoder Autoencoders</title><link>http://arxiv.org/abs/2309.07138v3</link><description>The task of blind source separation (BSS) involves separating sources from amixture without prior knowledge of the sources or the mixing system.Single-channel mixtures and non-linear mixtures are a particularly challengingproblem in BSS. In this paper, we propose a novel method for addressing BSSwith single-channel non-linear mixtures by leveraging the natural featuresubspace specialization ability of multi-encoder autoencoders. During thetraining phase, our method unmixes the input into the separate encoding spacesof the multi-encoder network and then remixes these representations within thedecoder for a reconstruction of the input. Then to perform source inference, weintroduce a novel encoding masking technique whereby masking out all but one ofthe encodings enables the decoder to estimate a source signal. To this end, wealso introduce a sparse mixing loss that encourages sparse remixing of sourceencodings throughout the decoder and a so-called zero reconstruction loss onthe decoder for coherent source estimations. To analyze and evaluate ourmethod, we conduct experiments on a toy dataset, designed to demonstrate thisproperty of feature subspace specialization, and with real-world biosignalrecordings from a polysomnography sleep study for extracting respiration fromelectrocardiogram and photoplethysmography signals.</description><author>Matthew B. Webster, Joonnyong Lee</author><pubDate>Fri, 08 Mar 2024 16:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07138v3</guid></item><item><title>VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2403.05438v1</link><description>Text-to-image diffusion models (T2I) have demonstrated unprecedentedcapabilities in creating realistic and aesthetic images. On the contrary,text-to-video diffusion models (T2V) still lag far behind in frame quality andtext alignment, owing to insufficient quality and quantity of training videos.In this paper, we introduce VideoElevator, a training-free and plug-and-playmethod, which elevates the performance of T2V using superior capabilities ofT2I. Different from conventional T2V sampling (i.e., temporal and spatialmodeling), VideoElevator explicitly decomposes each sampling step into temporalmotion refining and spatial quality elevating. Specifically, temporal motionrefining uses encapsulated T2V to enhance temporal consistency, followed byinverting to the noise distribution required by T2I. Then, spatial qualityelevating harnesses inflated T2I to directly predict less noisy latent, addingmore photo-realistic details. We have conducted experiments in extensiveprompts under the combination of various T2V and T2I. The results show thatVideoElevator not only improves the performance of T2V baselines withfoundational T2I, but also facilitates stylistic video synthesis withpersonalized T2I. Our code is available athttps://github.com/YBYBZhang/VideoElevator.</description><author>Yabo Zhang, Yuxiang Wei, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji, Wangmeng Zuo</author><pubDate>Fri, 08 Mar 2024 16:44:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05438v1</guid></item><item><title>Cooperative data-driven modeling</title><link>http://arxiv.org/abs/2211.12971v2</link><description>Data-driven modeling in mechanics is evolving rapidly based on recent machinelearning advances, especially on artificial neural networks. As the fieldmatures, new data and models created by different groups become available,opening possibilities for cooperative modeling. However, artificial neuralnetworks suffer from catastrophic forgetting, i.e. they forget how to performan old task when trained on a new one. This hinders cooperation becauseadapting an existing model for a new task affects the performance on a previoustask trained by someone else. The authors developed a continual learning methodthat addresses this issue, applying it here for the first time to solidmechanics. In particular, the method is applied to recurrent neural networks topredict history-dependent plasticity behavior, although it can be used on anyother architecture (feedforward, convolutional, etc.) and to predict otherphenomena. This work intends to spawn future developments on continual learningthat will foster cooperative strategies among the mechanics community to solveincreasingly challenging problems. We show that the chosen continual learningstrategy can sequentially learn several constitutive laws without forgettingthem, using less data to achieve the same error as standard (non-cooperative)training of one law per model.</description><author>Aleksandr Dekhovich, O. Taylan Turan, Jiaxiang Yi, Miguel A. Bessa</author><pubDate>Fri, 08 Mar 2024 16:40:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.12971v2</guid></item><item><title>Dendrogram of mixing measures: Hierarchical clustering and model selection for finite mixture models</title><link>http://arxiv.org/abs/2403.01684v2</link><description>We present a new way to summarize and select mixture models via thehierarchical clustering tree (dendrogram) constructed from an overfitted latentmixing measure. Our proposed method bridges agglomerative hierarchicalclustering and mixture modeling. The dendrogram's construction is derived fromthe theory of convergence of the mixing measures, and as a result, we can bothconsistently select the true number of mixing components and obtain thepointwise optimal convergence rate for parameter estimation from the tree, evenwhen the model parameters are only weakly identifiable. In theory, itexplicates the choice of the optimal number of clusters in hierarchicalclustering. In practice, the dendrogram reveals more information on thehierarchy of subpopulations compared to traditional ways of summarizing mixturemodels. Several simulation studies are carried out to support our theory. Wealso illustrate the methodology with an application to single-cell RNA sequenceanalysis.</description><author>Dat Do, Linh Do, Scott A. McKinley, Jonathan Terhorst, XuanLong Nguyen</author><pubDate>Fri, 08 Mar 2024 16:39:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01684v2</guid></item><item><title>OmniCount: Multi-label Object Counting with Semantic-Geometric Priors</title><link>http://arxiv.org/abs/2403.05435v1</link><description>Object counting is pivotal for understanding the composition of scenes.Previously, this task was dominated by class-specific methods, which havegradually evolved into more adaptable class-agnostic strategies. However, thesestrategies come with their own set of limitations, such as the need for manualexemplar input and multiple passes for multiple categories, resulting insignificant inefficiencies. This paper introduces a new, more practicalapproach enabling simultaneous counting of multiple object categories using anopen vocabulary framework. Our solution, OmniCount, stands out by usingsemantic and geometric insights from pre-trained models to count multiplecategories of objects as specified by users, all without additional training.OmniCount distinguishes itself by generating precise object masks andleveraging point prompts via the Segment Anything Model for efficient counting.To evaluate OmniCount, we created the OmniCount-191 benchmark, afirst-of-its-kind dataset with multi-label object counts, including points,bounding boxes, and VQA annotations. Our comprehensive evaluation inOmniCount-191, alongside other leading benchmarks, demonstrates OmniCount'sexceptional performance, significantly outpacing existing solutions andheralding a new era in object counting technology.</description><author>Anindya Mondal, Sauradip Nag, Xiatian Zhu, Anjan Dutta</author><pubDate>Fri, 08 Mar 2024 16:38:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05435v1</guid></item><item><title>Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs</title><link>http://arxiv.org/abs/2403.05434v1</link><description>Large Language Models (LLMs) exhibit impressive zero/few-shot inference andgeneration quality for high-resource languages(HRLs). A few of them have beentrained in low-resource languages (LRLs) and give decent performance. Owing tothe prohibitive costs of training LLMs, they are usually used as a networkservice, with the client charged by the count of input and output tokens. Thenumber of tokens strongly depends on the script and language, as well as theLLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage,because the well-known LLMs produce more tokens for LRLs than HRLs. This isbecause most currently popular LLMs are optimized for HRL vocabularies. Ourobjective is to level the playing field: reduce the cost of processing LRLs incontemporary LLMs while ensuring that predictive and generative qualities arenot compromised. As means to reduce the number of tokens processed by the LLM,we consider code-mixing, translation, and transliteration of LRLs to HRLs. Weperform an extensive study using the IndicXTREME dataset, covering 15 Indianlanguages, while using GPT-4 (one of the costliest LLM services released sofar) as a commercial LLM. We observe and analyze interesting patterns involvingtoken count, cost,and quality across a multitude of languages and tasks. Weshow that choosing the best policy to interact with the LLM can reduce cost by90% while giving better or comparable performance, compared to communicatingwith the LLM in the original LRL.</description><author>Arijit Nag, Animesh Mukherjee, Niloy Ganguly, Soumen Chakrabarti</author><pubDate>Fri, 08 Mar 2024 16:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05434v1</guid></item><item><title>Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation</title><link>http://arxiv.org/abs/2403.05433v1</link><description>Precision medicine, such as patient-adaptive treatments utilizing medicalimages, poses new challenges for image segmentation algorithms due to (1) thelarge variability across different patients and (2) the limited availability ofannotated data for each patient. In this work, we propose a data-efficientsegmentation method to address these challenges, namely Part-aware PersonalizedSegment Anything Model (P^2SAM). Without any model fine-tuning, P^2SAM enablesseamless adaptation to any new patients relying only on one-shotpatient-specific data. We introduce a novel part-aware prompt mechanism toselect multiple-point prompts based on part-level features of the one-shotdata. To further promote the robustness of the selected prompt, we propose aretrieval approach to handle outlier prompts. Extensive experiments demonstratethat P^2SAM improves the performance by +8.0% and +2.0% mean Dice score withintwo patient-specific segmentation settings, and exhibits impressive generalityacross different application domains, e.g., +6.4% mIoU on the PerSeg benchmark.Code will be released upon acceptance.</description><author>Chenhui Zhao, Liyue Shen</author><pubDate>Fri, 08 Mar 2024 16:34:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05433v1</guid></item><item><title>Automated Efficient Estimation using Monte Carlo Efficient Influence Functions</title><link>http://arxiv.org/abs/2403.00158v2</link><description>Many practical problems involve estimating low dimensional statisticalquantities with high-dimensional models and datasets. Several approachesaddress these estimation tasks based on the theory of influence functions, suchas debiased/double ML or targeted minimum loss estimation. This paperintroduces \textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fullyautomated technique for approximating efficient influence functions thatintegrates seamlessly with existing differentiable probabilistic programmingsystems. MC-EIF automates efficient statistical estimation for a broad class ofmodels and target functionals that would previously require rigorous customanalysis. We prove that MC-EIF is consistent, and that estimators using MC-EIFachieve optimal $\sqrt{N}$ convergence rates. We show empirically thatestimators using MC-EIF are at parity with estimators using analytic EIFs.Finally, we demonstrate a novel capstone example using MC-EIF for optimalportfolio selection.</description><author>Raj Agrawal, Sam Witty, Andy Zane, Eli Bingham</author><pubDate>Fri, 08 Mar 2024 16:26:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00158v2</guid></item><item><title>An Adaptive Dimension Reduction Estimation Method for High-dimensional Bayesian Optimization</title><link>http://arxiv.org/abs/2403.05425v1</link><description>Bayesian optimization (BO) has shown impressive results in a variety ofapplications within low-to-moderate dimensional Euclidean spaces. However,extending BO to high-dimensional settings remains a significant challenge. Weaddress this challenge by proposing a two-step optimization framework.Initially, we identify the effective dimension reduction (EDR) subspace for theobjective function using the minimum average variance estimation (MAVE) method.Subsequently, we construct a Gaussian process model within this EDR subspaceand optimize it using the expected improvement criterion. Our algorithm offersthe flexibility to operate these steps either concurrently or in sequence. Inthe sequential approach, we meticulously balance the exploration-exploitationtrade-off by distributing the sampling budget between subspace estimation andfunction optimization, and the convergence rate of our algorithm inhigh-dimensional contexts has been established. Numerical experiments validatethe efficacy of our method in challenging scenarios.</description><author>Shouri Hu, Jiawei Li, Zhibo Cai</author><pubDate>Fri, 08 Mar 2024 16:21:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05425v1</guid></item><item><title>EVD4UAV: An Altitude-Sensitive Benchmark to Evade Vehicle Detection in UAV</title><link>http://arxiv.org/abs/2403.05422v1</link><description>Vehicle detection in Unmanned Aerial Vehicle (UAV) captured images has wideapplications in aerial photography and remote sensing. There are many publicbenchmark datasets proposed for the vehicle detection and tracking in UAVimages. Recent studies show that adding an adversarial patch on objects canfool the well-trained deep neural networks based object detectors, posingsecurity concerns to the downstream tasks. However, the current public UAVdatasets might ignore the diverse altitudes, vehicle attributes, fine-grainedinstance-level annotation in mostly side view with blurred vehicle roof, sonone of them is good to study the adversarial patch based vehicle detectionattack problem. In this paper, we propose a new dataset named EVD4UAV as analtitude-sensitive benchmark to evade vehicle detection in UAV with 6,284images and 90,886 fine-grained annotated vehicles. The EVD4UAV dataset hasdiverse altitudes (50m, 70m, 90m), vehicle attributes (color, type),fine-grained annotation (horizontal and rotated bounding boxes, instance-levelmask) in top view with clear vehicle roof. One white-box and two black-boxpatch based attack methods are implemented to attack three classic deep neuralnetworks based object detectors on EVD4UAV. The experimental results show thatthese representative attack methods could not achieve the robustaltitude-insensitive attack performance.</description><author>Huiming Sun, Jiacheng Guo, Zibo Meng, Tianyun Zhang, Jianwu Fang, Yuewei Lin, Hongkai Yu</author><pubDate>Fri, 08 Mar 2024 16:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05422v1</guid></item><item><title>Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery</title><link>http://arxiv.org/abs/2403.05419v1</link><description>Recent advances in unsupervised learning have demonstrated the ability oflarge vision models to achieve promising results on downstream tasks bypre-training on large amount of unlabelled data. Such pre-training techniqueshave also been explored recently in the remote sensing domain due to theavailability of large amount of unlabelled data. Different from standardnatural image datasets, remote sensing data is acquired from various sensortechnologies and exhibit diverse range of scale variations as well asmodalities. Existing satellite image pre-training methods either ignore thescale information present in the remote sensing imagery or restrict themselvesto use only a single type of data modality. In this paper, we re-visittransformers pre-training and leverage multi-scale information that iseffectively utilized with multiple modalities. Our proposed approach, namedSatMAE++, performs multi-scale pre-training and utilizes convolution basedupsampling blocks to reconstruct the image at higher scales making itextensible to include more scales. Compared to existing works, the proposedSatMAE++ with multi-scale pre-training is equally effective for both optical aswell as multi-spectral imagery. Extensive experiments on six datasets revealthe merits of proposed contributions, leading to state-of-the-art performanceon all datasets. SatMAE++ achieves mean average precision (mAP) gain of 2.5\%for multi-label classification task on BigEarthNet dataset. Our code andpre-trained models are available at \url{https://github.com/techmn/satmae_pp}.</description><author>Mubashir Noman, Muzammal Naseer, Hisham Cholakkal, Rao Muhammad Anwar, Salman Khan, Fahad Shahbaz Khan</author><pubDate>Fri, 08 Mar 2024 16:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05419v1</guid></item><item><title>SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised Learning for Robust Infrared Small Target Detection</title><link>http://arxiv.org/abs/2403.05416v1</link><description>Single-frame infrared small target (SIRST) detection aims to recognize smalltargets from clutter backgrounds. Recently, convolutional neural networks haveachieved significant advantages in general object detection. With thedevelopment of Transformer, the scale of SIRST models is constantly increasing.Due to the limited training samples, performance has not been improvedaccordingly. The quality, quantity, and diversity of the infrared dataset arecritical to the detection of small targets. To highlight this issue, we proposea negative sample augmentation method in this paper. Specifically, a negativeaugmentation approach is proposed to generate massive negatives forself-supervised learning. Firstly, we perform a sequential noise modelingtechnology to generate realistic infrared data. Secondly, we fuse the extractednoise with the original data to facilitate diversity and fidelity in thegenerated data. Lastly, we proposed a negative augmentation strategy to enrichdiversity as well as maintain semantic invariance. The proposed algorithmproduces a synthetic SIRST-5K dataset, which contains massive pseudo-data andcorresponding labels. With a rich diversity of infrared small target data, ouralgorithm significantly improves the model performance and convergence speed.Compared with other state-of-the-art (SOTA) methods, our method achievesoutstanding performance in terms of probability of detection (Pd), false-alarmrate (Fa), and intersection over union (IoU).</description><author>Yahao Lu, Yupei Lin, Han Wu, Xiaoyu Xian, Yukai Shi, Liang Lin</author><pubDate>Fri, 08 Mar 2024 16:14:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05416v1</guid></item><item><title>TetraSphere: A Neural Descriptor for O(3)-Invariant Point Cloud Analysis</title><link>http://arxiv.org/abs/2211.14456v5</link><description>In many practical applications, 3D point cloud analysis requires rotationinvariance. In this paper, we present a learnable descriptor invariant under 3Drotations and reflections, i.e., the O(3) actions, utilizing the recentlyintroduced steerable 3D spherical neurons and vector neurons. Specifically, wepropose an embedding of the 3D spherical neurons into 4D vector neurons, whichleverages end-to-end training of the model. In our approach, we performTetraTransform--an equivariant embedding of the 3D input into 4D, constructedfrom the steerable neurons--and extract deeper O(3)-equivariant features usingvector neurons. This integration of the TetraTransform into the VN-DGCNNframework, termed TetraSphere, negligibly increases the number of parameters byless than 0.0002%. TetraSphere sets a new state-of-the-art performanceclassifying randomly rotated real-world object scans of the challenging subsetsof ScanObjectNN. Additionally, TetraSphere outperforms all equivariant methodson randomly rotated synthetic data: classifying objects from ModelNet40 andsegmenting parts of the ShapeNet shapes. Thus, our results reveal the practicalvalue of steerable 3D spherical neurons for learning in 3D Euclidean space. Thecode is available at \url{https://github.com/pavlo-melnyk/tetrasphere}.</description><author>Pavlo Melnyk, Andreas Robinson, Michael Felsberg, Mårten Wadenbäck</author><pubDate>Fri, 08 Mar 2024 16:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14456v5</guid></item><item><title>FedFMS: Exploring Federated Foundation Models for Medical Image Segmentation</title><link>http://arxiv.org/abs/2403.05408v1</link><description>Medical image segmentation is crucial for clinical diagnosis. TheSegmentation Anything Model (SAM) serves as a powerful foundation model forvisual segmentation and can be adapted for medical image segmentation. However,medical imaging data typically contain privacy-sensitive information, making itchallenging to train foundation models with centralized storage and sharing. Todate, there are few foundation models tailored for medical image deploymentwithin the federated learning framework, and the segmentation performance, aswell as the efficiency of communication and training, remain unexplored. Inresponse to these issues, we developed Federated Foundation models for Medicalimage Segmentation (FedFMS), which includes the Federated SAM (FedSAM) and acommunication and training-efficient Federated SAM with Medical SAM Adapter(FedMSA). Comprehensive experiments on diverse datasets are conducted toinvestigate the performance disparities between centralized training andfederated learning across various configurations of FedFMS. The experimentsrevealed that FedFMS could achieve performance comparable to models trained viacentralized training methods while maintaining privacy. Furthermore, FedMSAdemonstrated the potential to enhance communication and training efficiency.Our model implementation codes are available athttps://github.com/LIU-YUXI/FedFMS.</description><author>Yuxi Liu, Guibo Luo, Yuesheng Zhu</author><pubDate>Fri, 08 Mar 2024 16:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05408v1</guid></item><item><title>Algorithmic Identification of Essential Exogenous Nodes for Causal Sufficiency in Brain Networks</title><link>http://arxiv.org/abs/2403.05407v1</link><description>In the investigation of any causal mechanisms, such as the brain's causalnetworks, the assumption of causal sufficiency plays a critical role. Notably,neglecting this assumption can result in significant errors, a fact that isoften disregarded in the causal analysis of brain networks. In this study, wepropose an algorithmic identification approach for determining essentialexogenous nodes that satisfy the critical need for causal sufficiency to adhereto it in such inquiries. Our approach consists of three main steps: First, bycapturing the essence of the Peter-Clark (PC) algorithm, we conductindependence tests for pairs of regions within a network, as well as for thesame pairs conditioned on nodes from other networks. Next, we distinguishcandidate confounders by analyzing the differences between the conditional andunconditional results, using the Kolmogorov-Smirnov test. Subsequently, weutilize Non-Factorized identifiable Variational Autoencoders (NF-iVAE) alongwith the Correlation Coefficient index (CCI) metric to identify the confoundingvariables within these candidate nodes. Applying our method to the HumanConnectome Projects (HCP) movie-watching task data, we demonstrate that whileinteractions exist between dorsal and ventral regions, only dorsal regionsserve as confounders for the visual networks, and vice versa. These findingsalign consistently with those resulting from the neuroscientific perspective.Finally, we show the reliability of our results by testing 30 independent runsfor NF-iVAE initialization.</description><author>Abdolmahdi Bagheri, Mahdi Dehshiri, Babak Nadjar Araabi, Alireza Akhondi Asl</author><pubDate>Fri, 08 Mar 2024 16:05:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05407v1</guid></item><item><title>Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting</title><link>http://arxiv.org/abs/2403.05406v1</link><description>The forecasting of Multivariate Time Series (MTS) has long been an importantbut challenging task. Due to the non-stationary problem across long-distancetime steps, previous studies primarily adopt stationarization method toattenuate the non-stationary problem of the original series for betterpredictability. However, existing methods always adopt the stationarizedseries, which ignores the inherent non-stationarity, and has difficulty inmodeling MTS with complex distributions due to the lack of stochasticity. Totackle these problems, we first develop a powerful hierarchical probabilisticgenerative module to consider the non-stationarity and stochasticcharacteristics within MTS, and then combine it with transformer for awell-defined variational generative dynamic model named Hierarchical Timeseries Variational Transformer (HTV-Trans), which recovers the intrinsicnon-stationary information into temporal dependencies. Being a powerfulprobabilistic model, HTV-Trans is utilized to learn expressive representationsof MTS and applied to forecasting tasks. Extensive experiments on diversedatasets show the efficiency of HTV-Trans on MTS forecasting tasks</description><author>Muyao Wang, Wenchao Chen, Bo Chen</author><pubDate>Fri, 08 Mar 2024 16:04:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05406v1</guid></item><item><title>DualBEV: CNN is All You Need in View Transformation</title><link>http://arxiv.org/abs/2403.05402v1</link><description>Camera-based Bird's-Eye-View (BEV) perception often struggles betweenadopting 3D-to-2D or 2D-to-3D view transformation (VT). The 3D-to-2D VTtypically employs resource intensive Transformer to establish robustcorrespondences between 3D and 2D feature, while the 2D-to-3D VT utilizes theLift-Splat-Shoot (LSS) pipeline for real-time application, potentially missingdistant information. To address these limitations, we propose DualBEV, aunified framework that utilizes a shared CNN-based feature transformationincorporating three probabilistic measurements for both strategies. Byconsidering dual-view correspondences in one-stage, DualBEV effectively bridgesthe gap between these strategies, harnessing their individual strengths. Ourmethod achieves state-of-the-art performance without Transformer, deliveringcomparable efficiency to the LSS approach, with 55.2% mAP and 63.4% NDS on thenuScenes test set. Code will be released athttps://github.com/PeidongLi/DualBEV.</description><author>Peidong Li, Wancheng Shen, Qihao Huang, Dixiao Cui</author><pubDate>Fri, 08 Mar 2024 15:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05402v1</guid></item><item><title>HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction</title><link>http://arxiv.org/abs/2403.05396v1</link><description>Histopathology serves as the gold standard in cancer diagnosis, with clinicalreports being vital in interpreting and understanding this process, guidingcancer treatment and patient care. The automation of histopathology reportgeneration with deep learning stands to significantly enhance clinicalefficiency and lessen the labor-intensive, time-consuming burden onpathologists in report writing. In pursuit of this advancement, we introduceHistGen, a multiple instance learning-empowered framework for histopathologyreport generation together with the first benchmark dataset for evaluation.Inspired by diagnostic and report-writing workflows, HistGen features twodelicately designed modules, aiming to boost report generation by aligningwhole slide images (WSIs) and diagnostic reports from local and globalgranularity. To achieve this, a local-global hierarchical encoder is developedfor efficient visual feature aggregation from a region-to-slide perspective.Meanwhile, a cross-modal context module is proposed to explicitly facilitatealignment and interaction between distinct modalities, effectively bridging thegap between the extensive visual sequences of WSIs and corresponding highlysummarized reports. Experimental results on WSI report generation show theproposed model outperforms state-of-the-art (SOTA) models by a large margin.Moreover, the results of fine-tuning our model on cancer subtyping and survivalanalysis tasks further demonstrate superior performance compared to SOTAmethods, showcasing strong transfer learning capability. Dataset, modelweights, and source code are available inhttps://github.com/dddavid4real/HistGen.</description><author>Zhengrui Guo, Jiabo Ma, Yingxue Xu, Yihui Wang, Liansheng Wang, Hao Chen</author><pubDate>Fri, 08 Mar 2024 15:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05396v1</guid></item><item><title>Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems trained with Gradient Descent</title><link>http://arxiv.org/abs/2403.05395v1</link><description>Advanced machine learning methods, and more prominently neural networks, havebecome standard to solve inverse problems over the last years. However, thetheoretical recovery guarantees of such methods are still scarce and difficultto achieve. Only recently did unsupervised methods such as Deep Image Prior(DIP) get equipped with convergence and recovery guarantees for generic lossfunctions when trained through gradient flow with an appropriateinitialization. In this paper, we extend these results by proving that theseguarantees hold true when using gradient descent with an appropriately chosenstep-size/learning rate. We also show that the discretization only affects theoverparametrization bound for a two-layer DIP network by a constant and thusthat the different guarantees found for the gradient flow will hold forgradient descent.</description><author>Nathan Buskulic, Jalal Fadili, Yvain Quéau</author><pubDate>Fri, 08 Mar 2024 15:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05395v1</guid></item><item><title>A Deep Learning Method for Classification of Biophilic Artworks</title><link>http://arxiv.org/abs/2403.05394v1</link><description>Biophilia is an innate love for living things and nature itself that has beenassociated with a positive impact on mental health and well-being. This studyexplores the application of deep learning methods for the classification ofBiophilic artwork, in order to learn and explain the different Biophiliccharacteristics present in a visual representation of a painting. Using theconcept of Biophilia that postulates the deep connection of human beings withnature, we use an artificially intelligent algorithm to recognise the differentpatterns underlying the Biophilic features in an artwork. Our proposed methoduses a lower-dimensional representation of an image and a decoder model toextract salient features of the image of each Biophilic trait, such as plants,water bodies, seasons, animals, etc., based on learnt factors such as shape,texture, and illumination. The proposed classification model is capable ofextracting Biophilic artwork that not only helps artists, collectors, andresearchers studying to interpret and exploit the effects of mental well-beingon exposure to nature-inspired visual aesthetics but also enables a methodicalexploration of the study of Biophilia and Biophilic artwork for aestheticpreferences. Using the proposed algorithms, we have also created a gallery ofBiophilic collections comprising famous artworks from different European andAmerican art galleries, which will soon be published on the Vieunite@ onlinecommunity.</description><author>Purna Kar, Jordan J. Bird, Yangang Xing, Alexander Sumich, Andrew Knight, Ahmad Lotfi, Benedict Carpenter van Barthold</author><pubDate>Fri, 08 Mar 2024 15:45:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05394v1</guid></item><item><title>Multi-View Causal Representation Learning with Partial Observability</title><link>http://arxiv.org/abs/2311.04056v2</link><description>We present a unified framework for studying the identifiability ofrepresentations learned from simultaneously observed views, such as differentdata modalities. We allow a partially observed setting in which each viewconstitutes a nonlinear mixture of a subset of underlying latent variables,which can be causally related. We prove that the information shared across allsubsets of any number of views can be learned up to a smooth bijection usingcontrastive learning and a single encoder per view. We also provide graphicalcriteria indicating which latent variables can be identified through a simpleset of rules, which we refer to as identifiability algebra. Our generalframework and theoretical results unify and extend several previous works onmulti-view nonlinear ICA, disentanglement, and causal representation learning.We experimentally validate our claims on numerical, image, and multi-modal datasets. Further, we demonstrate that the performance of prior methods isrecovered in different special cases of our setup. Overall, we find that accessto multiple partial views enables us to identify a more fine-grainedrepresentation, under the generally milder assumption of partial observability.</description><author>Dingling Yao, Danru Xu, Sébastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg Martius, Julius von Kügelgen, Francesco Locatello</author><pubDate>Fri, 08 Mar 2024 15:43:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04056v2</guid></item><item><title>Machine Learning Security against Data Poisoning: Are We There Yet?</title><link>http://arxiv.org/abs/2204.05986v3</link><description>The recent success of machine learning (ML) has been fueled by the increasingavailability of computing power and large amounts of data in many differentapplications. However, the trustworthiness of the resulting models can becompromised when such data is maliciously manipulated to mislead the learningprocess. In this article, we first review poisoning attacks that compromise thetraining data used to learn ML models, including attacks that aim to reduce theoverall performance, manipulate the predictions on specific test samples, andeven implant backdoors in the model. We then discuss how to mitigate theseattacks using basic security principles, or by deploying ML-oriented defensivemechanisms. We conclude our article by formulating some relevant openchallenges which are hindering the development of testing methods andbenchmarks suitable for assessing and improving the trustworthiness of MLmodels against data poisoning attacks</description><author>Antonio Emanuele Cinà, Kathrin Grosse, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo</author><pubDate>Fri, 08 Mar 2024 15:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.05986v3</guid></item><item><title>EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain</title><link>http://arxiv.org/abs/2401.16822v3</link><description>Multi-modal large language models (MLLMs) have demonstrated remarkablesuccess in vision and visual-language tasks within the natural image domain.Owing to the significant diversities between the natural and remote sensing(RS) images, the development of MLLMs in the RS domain is still in the infantstage. To fill the gap, a pioneer MLLM named EarthGPT integrating variousmulti-sensor RS interpretation tasks uniformly is proposed in this paper foruniversal RS image comprehension. In EarthGPT, three key techniques aredeveloped including a visual-enhanced perception mechanism, a cross-modalmutual comprehension approach, and a unified instruction tuning method formulti-sensor multi-task in the RS domain. More importantly, a dataset namedMMRS-1M featuring large-scale multi-sensor multi-modal RS instruction-followingis constructed, comprising over 1M image-text pairs based on 34 existingdiverse RS datasets and including multi-sensor images such as optical,synthetic aperture radar (SAR), and infrared. The MMRS-1M dataset addresses thedrawback of MLLMs on RS expert knowledge and stimulates the development ofMLLMs in the RS domain. Extensive experiments are conducted, demonstrating theEarthGPT's superior performance in various RS visual interpretation taskscompared with the other specialist models and MLLMs, proving the effectivenessof the proposed EarthGPT and offering a versatile paradigm for open-setreasoning tasks.</description><author>Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, Xuerui Mao</author><pubDate>Fri, 08 Mar 2024 15:36:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16822v3</guid></item><item><title>Generalized Correspondence Matching via Flexible Hierarchical Refinement and Patch Descriptor Distillation</title><link>http://arxiv.org/abs/2403.05388v1</link><description>Correspondence matching plays a crucial role in numerous roboticsapplications. In comparison to conventional hand-crafted methods and recentdata-driven approaches, there is significant interest in plug-and-playalgorithms that make full use of pre-trained backbone networks for multi-scalefeature extraction and leverage hierarchical refinement strategies to generatematched correspondences. The primary focus of this paper is to address thelimitations of deep feature matching (DFM), a state-of-the-art (SoTA)plug-and-play correspondence matching approach. First, we eliminate thepre-defined threshold employed in the hierarchical refinement process of DFM byleveraging a more flexible nearest neighbor search strategy, thereby preventingthe exclusion of repetitive yet valid matches during the early stages. Oursecond technical contribution is the integration of a patch descriptor, whichextends the applicability of DFM to accommodate a wide range of backbonenetworks pre-trained across diverse computer vision tasks, including imageclassification, semantic segmentation, and stereo matching. Taking into accountthe practical applicability of our method in real-world robotics applications,we also propose a novel patch descriptor distillation strategy to furtherreduce the computational complexity of correspondence matching. Extensiveexperiments conducted on three public datasets demonstrate the superiorperformance of our proposed method. Specifically, it achieves an overallperformance in terms of mean matching accuracy of 0.68, 0.92, and 0.95 withrespect to the tolerances of 1, 3, and 5 pixels, respectively, on the HPatchesdataset, outperforming all other SoTA algorithms. Our source code, demo video,and supplement are publicly available at mias.group/GCM.</description><author>Yu Han, Ziwei Long, Yanting Zhang, Jin Wu, Zhijun Fang, Rui Fan</author><pubDate>Fri, 08 Mar 2024 15:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05388v1</guid></item><item><title>Switching the Loss Reduces the Cost in Batch Reinforcement Learning</title><link>http://arxiv.org/abs/2403.05385v1</link><description>We propose training fitted Q-iteration with log-loss (FQI-LOG) for batchreinforcement learning (RL). We show that the number of samples needed to learna near-optimal policy with FQI-LOG scales with the accumulated cost of theoptimal policy, which is zero in problems where acting optimally achieves thegoal and incurs no cost. In doing so, we provide a general framework forproving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimalachievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG usesfewer samples than FQI trained with squared loss on problems where the optimalpolicy reliably achieves the goal.</description><author>Alex Ayoub, Kaiwen Wang, Vincent Liu, Samuel Robertson, James McInerney, Dawen Liang, Nathan Kallus, Csaba Szepesvári</author><pubDate>Fri, 08 Mar 2024 15:30:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05385v1</guid></item><item><title>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</title><link>http://arxiv.org/abs/2309.12307v3</link><description>We present LongLoRA, an efficient fine-tuning approach that extends thecontext sizes of pre-trained large language models (LLMs), with limitedcomputation cost. Typically, training LLMs with long context sizes iscomputationally expensive, requiring extensive training hours and GPUresources. For example, training on the context length of 8192 needs 16xcomputational costs in self-attention layers as that of 2048. In this paper, wespeed up the context extension of LLMs in two aspects. On the one hand,although dense global attention is needed during inference, fine-tuning themodel can be effectively and efficiently done by sparse local attention. Theproposed shifted sparse attention effectively enables context extension,leading to non-trivial computation saving with similar performance tofine-tuning with vanilla attention. Particularly, it can be implemented withonly two lines of code in training, while being optional in inference. On theother hand, we revisit the parameter-efficient fine-tuning regime for contextexpansion. Notably, we find that LoRA for context extension works well underthe premise of trainable embedding and normalization. LongLoRA combines thisimproved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results onvarious tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7Bfrom 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine.LongLoRA extends models' context while retaining their original architectures,and is compatible with most existing techniques, like Flash-Attention2. Inaddition, we further conduct supervised fine-tuning with LongLoRA and our longinstruction-following LongAlpaca dataset.</description><author>Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia</author><pubDate>Fri, 08 Mar 2024 15:26:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12307v3</guid></item><item><title>A Data Augmentation Pipeline to Generate Synthetic Labeled Datasets of 3D Echocardiography Images using a GAN</title><link>http://arxiv.org/abs/2403.05384v1</link><description>Due to privacy issues and limited amount of publicly available labeleddatasets in the domain of medical imaging, we propose an image generationpipeline to synthesize 3D echocardiographic images with corresponding groundtruth labels, to alleviate the need for data collection and for laborious anderror-prone human labeling of images for subsequent Deep Learning (DL) tasks.The proposed method utilizes detailed anatomical segmentations of the heart asground truth label sources. This initial dataset is combined with a seconddataset made up of real 3D echocardiographic images to train a GenerativeAdversarial Network (GAN) to synthesize realistic 3D cardiovascular Ultrasoundimages paired with ground truth labels. To generate the synthetic 3D dataset,the trained GAN uses high resolution anatomical models from Computed Tomography(CT) as input. A qualitative analysis of the synthesized images showed that themain structures of the heart are well delineated and closely follow the labelsobtained from the anatomical models. To assess the usability of these syntheticimages for DL tasks, segmentation algorithms were trained to delineate the leftventricle, left atrium, and myocardium. A quantitative analysis of the 3Dsegmentations given by the models trained with the synthetic images indicatedthe potential use of this GAN approach to generate 3D synthetic data, use thedata to train DL models for different clinical tasks, and therefore tackle theproblem of scarcity of 3D labeled echocardiography datasets.</description><author>Cristiana Tiago, Andrew Gilbert, Ahmed S. Beela, Svein Arne Aase, Sten Roar Snare, Jurica Sprem</author><pubDate>Fri, 08 Mar 2024 15:26:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05384v1</guid></item><item><title>Exploring Robust Features for Few-Shot Object Detection in Satellite Imagery</title><link>http://arxiv.org/abs/2403.05381v1</link><description>The goal of this paper is to perform object detection in satellite imagerywith only a few examples, thus enabling users to specify any object class withminimal annotation. To this end, we explore recent methods and ideas fromopen-vocabulary detection for the remote sensing domain. We develop a few-shotobject detector based on a traditional two-stage architecture, where theclassification block is replaced by a prototype-based classifier. A large-scalepre-trained model is used to build class-reference embeddings or prototypes,which are compared to region proposal contents for label prediction. Inaddition, we propose to fine-tune prototypes on available training images toboost performance and learn differences between similar classes, such asaircraft types. We perform extensive evaluations on two remote sensing datasetscontaining challenging and rare objects. Moreover, we study the performance ofboth visual and image-text features, namely DINOv2 and CLIP, including two CLIPmodels specifically tailored for remote sensing applications. Results indicatethat visual features are largely superior to vision-language models, as thelatter lack the necessary domain-specific vocabulary. Lastly, the developeddetector outperforms fully supervised and few-shot methods evaluated on theSIMD and DIOR datasets, despite minimal training parameters.</description><author>Xavier Bou, Gabriele Facciolo, Rafael Grompone von Gioi, Jean-Michel Morel, Thibaud Ehret</author><pubDate>Fri, 08 Mar 2024 15:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05381v1</guid></item><item><title>Spectrogram-Based Detection of Auto-Tuned Vocals in Music Recordings</title><link>http://arxiv.org/abs/2403.05380v1</link><description>In the domain of music production and audio processing, the implementation ofautomatic pitch correction of the singing voice, also known as Auto-Tune, hassignificantly transformed the landscape of vocal performance. While auto-tuningtechnology has offered musicians the ability to tune their vocal pitches andachieve a desired level of precision, its use has also sparked debatesregarding its impact on authenticity and artistic integrity. As a result,detecting and analyzing Auto-Tuned vocals in music recordings has becomeessential for music scholars, producers, and listeners. However, to the best ofour knowledge, no prior effort has been made in this direction. This studyintroduces a data-driven approach leveraging triplet networks for the detectionof Auto-Tuned songs, backed by the creation of a dataset composed of originaland Auto-Tuned audio clips. The experimental results demonstrate thesuperiority of the proposed method in both accuracy and robustness compared toRawnet2, an end-to-end model proposed for anti-spoofing and widely used forother audio forensic tasks.</description><author>Mahyar Gohari, Paolo Bestagini, Sergio Benini, Nicola Adami</author><pubDate>Fri, 08 Mar 2024 15:19:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05380v1</guid></item><item><title>A Composite Decomposition Method for Large-Scale Global Optimization</title><link>http://arxiv.org/abs/2403.01192v2</link><description>Cooperative co-evolution (CC) algorithms, based on the divide-and-conquerstrategy, have emerged as the predominant approach to solving large-scaleglobal optimization (LSGO) problems. The efficiency and accuracy of thegrouping stage significantly impact the performance of the optimizationprocess. While the general separability grouping (GSG) method has overcome thelimitation of previous differential grouping (DG) methods by enabling thedecomposition of non-additively separable functions, it suffers from highcomputational complexity. To address this challenge, this article proposes acomposite separability grouping (CSG) method, seamlessly integrating DG and GSGinto a problem decomposition framework to utilize the strengths of bothapproaches. CSG introduces a step-by-step decomposition framework thataccurately decomposes various problem types using fewer computationalresources. By sequentially identifying additively, multiplicatively andgenerally separable variables, CSG progressively groups non-separable variablesby recursively considering the interactions between each non-separable variableand the formed non-separable groups. Furthermore, to enhance the efficiency andaccuracy of CSG, we introduce two innovative methods: a multiplicativelyseparable variable detection method and a non-separable variable groupingmethod. These two methods are designed to effectively detect multiplicativelyseparable variables and efficiently group non-separable variables,respectively. Extensive experimental results demonstrate that CSG achieves moreaccurate variable grouping with lower computational complexity compared to GSGand state-of-the-art DG series designs.</description><author>Maojiang Tian, Minyang Chen, Wei Du, Yang Tang, Yaochu Jin, Gary G. Yen</author><pubDate>Fri, 08 Mar 2024 15:18:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01192v2</guid></item><item><title>Multi-Scale and Multi-Layer Contrastive Learning for Domain Generalization</title><link>http://arxiv.org/abs/2308.14418v3</link><description>During the past decade, deep neural networks have led to fast-paced progressand significant achievements in computer vision problems, for both academia andindustry. Yet despite their success, state-of-the-art image classificationapproaches fail to generalize well in previously unseen visual contexts, asrequired by many real-world applications. In this paper, we focus on thisdomain generalization (DG) problem and argue that the generalization ability ofdeep convolutional neural networks can be improved by taking advantage ofmulti-layer and multi-scaled representations of the network. We introduce aframework that aims at improving domain generalization of image classifiers bycombining both low-level and high-level features at multiple scales, enablingthe network to implicitly disentangle representations in its latent space andlearn domain-invariant attributes of the depicted objects. Additionally, tofurther facilitate robust representation learning, we propose a novel objectivefunction, inspired by contrastive learning, which aims at constraining theextracted representations to remain invariant under distribution shifts. Wedemonstrate the effectiveness of our method by evaluating on the domaingeneralization datasets of PACS, VLCS, Office-Home and NICO. Through extensiveexperimentation, we show that our model is able to surpass the performance ofprevious DG methods and consistently produce competitive and state-of-the-artresults in all datasets</description><author>Aristotelis Ballas, Christos Diou</author><pubDate>Fri, 08 Mar 2024 15:17:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14418v3</guid></item><item><title>Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia Classification</title><link>http://arxiv.org/abs/2403.05379v1</link><description>Automated disease diagnosis using medical image analysis relies on deeplearning, often requiring large labeled datasets for supervised model training.Diseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce andcostly annotations on a single-cell level. Multiple Instance Learning (MIL)addresses weakly labeled scenarios but necessitates powerful encoders typicallytrained with labeled data. In this study, we explore Self-Supervised Learning(SSL) as a pre-training approach for MIL-based AML subtype classification fromblood smears, removing the need for labeled data during encoder training. Weinvestigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, andcompare their performance against supervised pre-training. Our findings showthat SSL-pretrained encoders achieve comparable performance, showcasing thepotential of SSL in MIL. This breakthrough offers a cost-effective anddata-efficient solution, propelling the field of AI-based disease diagnosis.</description><author>Salome Kazeminia, Max Joosten, Dragan Bosnacki, Carsten Marr</author><pubDate>Fri, 08 Mar 2024 15:16:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05379v1</guid></item><item><title>CriticBench: Benchmarking LLMs for Critique-Correct Reasoning</title><link>http://arxiv.org/abs/2402.14809v2</link><description>The ability of Large Language Models (LLMs) to critique and refine theirreasoning is crucial for their application in evaluation, feedback provision,and self-improvement. This paper introduces CriticBench, a comprehensivebenchmark designed to assess LLMs' abilities to critique and rectify theirreasoning across a variety of tasks. CriticBench encompasses five reasoningdomains: mathematical, commonsense, symbolic, coding, and algorithmic. Itcompiles 15 datasets and incorporates responses from three LLM families.Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs ingeneration, critique, and correction reasoning, i.e., GQC reasoning. Ourfindings reveal: (1) a linear relationship in GQC capabilities, withcritique-focused training markedly enhancing performance; (2) a task-dependentvariation in correction effectiveness, with logic-oriented tasks being moreamenable to correction; (3) GQC knowledge inconsistencies that decrease asmodel size increases; and (4) an intriguing inter-model critiquing dynamic,where stronger models are better at critiquing weaker ones, while weaker modelscan surprisingly surpass stronger ones in their self-critique. We hope theseinsights into the nuanced critique-correct reasoning of LLMs will fosterfurther research in LLM critique and self-improvement.</description><author>Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang</author><pubDate>Fri, 08 Mar 2024 15:15:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14809v2</guid></item><item><title>Improved Convergence Rates of Windowed Anderson Acceleration for Symmetric Fixed-Point Iterations</title><link>http://arxiv.org/abs/2311.02490v2</link><description>This paper studies the commonly utilized windowed Anderson acceleration (AA)algorithm for fixed-point methods, $x^{(k+1)}=q(x^{(k)})$. It provides thefirst proof that when the operator $q$ is linear and symmetric the windowed AA,which uses a sliding window of prior iterates, improves the root-linearconvergence factor over the fixed-point iterations. When $q$ is nonlinear, yethas a symmetric Jacobian at a fixed point, a slightly modified AA algorithm isproved to have an analogous root-linear convergence factor improvement overfixed-point iterations. Simulations verify our observations. Furthermore,experiments with different data models demonstrate AA is significantly superiorto the standard fixed-point methods for Tyler's M-estimation.</description><author>Casey Garner, Gilad Lerman, Teng Zhang</author><pubDate>Fri, 08 Mar 2024 15:15:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02490v2</guid></item><item><title>Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations</title><link>http://arxiv.org/abs/2212.14411v4</link><description>Sequential testing, always-valid $p$-values, and confidence sequences promiseflexible statistical inference and on-the-fly decision making. However, unlikefixed-$n$ inference based on asymptotic normality, existing sequential testseither make parametric assumptions and end up under-covering/over-rejectingwhen these fail or use non-parametric but conservative concentrationinequalities and end up over-covering/under-rejecting. To circumvent theseissues, we sidestep exact at-least-$\alpha$ coverage and focus on asymptoticcalibration and asymptotic optimality. That is, we seek sequential tests whoseprobability of \emph{ever} rejecting a true hypothesis approaches $\alpha$ andwhose expected time to reject a false hypothesis approaches a lower bound onall such asymptotically calibrated tests, both "approaches" occurring under anappropriate limit. We permit observations to be both non-parametric anddependent and focus on testing whether the observations form a martingaledifference sequence. We propose the universal sequential probability ratio test(uSPRT), a slight modification to the normal-mixture sequential probabilityratio test, where we add a burn-in period and adjust thresholds accordingly. Weshow that even in this very general setting, the uSPRT is asymptoticallyoptimal under mild generic conditions. We apply the results to stabilizedestimating equations to test means, treatment effects, {\etc} Our results alsoprovide corresponding guarantees for the implied confidence sequences.Numerical simulations verify our guarantees and the benefits of the uSPRT overalternatives.</description><author>Aurelien Bibaut, Nathan Kallus, Michael Lindon</author><pubDate>Fri, 08 Mar 2024 15:04:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.14411v4</guid></item><item><title>Improved particle-flow event reconstruction with scalable neural networks for current and future particle detectors</title><link>http://arxiv.org/abs/2309.06782v5</link><description>Efficient and accurate algorithms are necessary to reconstruct particles inthe highly granular detectors anticipated at the High-Luminosity Large HadronCollider and the Future Circular Collider. We study scalable machine learningmodels for event reconstruction in electron-positron collisions based on a fulldetector simulation. Particle-flow reconstruction can be formulated as asupervised learning task using tracks and calorimeter clusters. We compare agraph neural network and kernel-based transformer and demonstrate that we canavoid quadratic operations while achieving realistic reconstruction. We showthat hyperparameter tuning significantly improves the performance of themodels. The best graph neural network model shows improvement in the jettransverse momentum resolution by up to 50% compared to the rule-basedalgorithm. The resulting model is portable across Nvidia, AMD and Habanahardware. Accurate and fast machine-learning based reconstruction cansignificantly improve future measurements at colliders.</description><author>Joosep Pata, Eric Wulff, Farouk Mokhtar, David Southwick, Mengke Zhang, Maria Girone, Javier Duarte</author><pubDate>Fri, 08 Mar 2024 15:01:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06782v5</guid></item><item><title>Frequency-Adaptive Dilated Convolution for Semantic Segmentation</title><link>http://arxiv.org/abs/2403.05369v1</link><description>Dilated convolution, which expands the receptive field by inserting gapsbetween its consecutive elements, is widely employed in computer vision. Inthis study, we propose three strategies to improve individual phases of dilatedconvolution from the view of spectrum analysis. Departing from the conventionalpractice of fixing a global dilation rate as a hyperparameter, we introduceFrequency-Adaptive Dilated Convolution (FADC), which dynamically adjustsdilation rates spatially based on local frequency components. Subsequently, wedesign two plug-in modules to directly enhance effective bandwidth andreceptive field size. The Adaptive Kernel (AdaKern) module decomposesconvolution weights into low-frequency and high-frequency components,dynamically adjusting the ratio between these components on a per-channelbasis. By increasing the high-frequency part of convolution weights, AdaKerncaptures more high-frequency components, thereby improving effective bandwidth.The Frequency Selection (FreqSelect) module optimally balances high- andlow-frequency components in feature representations through spatially variantreweighting. It suppresses high frequencies in the background to encourage FADCto learn a larger dilation, thereby increasing the receptive field for anexpanded scope. Extensive experiments on segmentation and object detectionconsistently validate the efficacy of our approach. The code is publiclyavailable at \url{https://github.com/Linwei-Chen/FADC}.</description><author>Linwei Chen, Lin Gu, Ying Fu</author><pubDate>Fri, 08 Mar 2024 15:00:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05369v1</guid></item><item><title>(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection</title><link>http://arxiv.org/abs/2401.14040v2</link><description>In the universe of Natural Language Processing, Transformer-based languagemodels like BERT and (Chat)GPT have emerged as lexical superheroes with greatpower to solve open research problems. In this paper, we specifically focus onthe temporal problem of semantic change, and evaluate their ability to solvetwo diachronic extensions of the Word-in-Context (WiC) task: TempoWiC andHistoWiC. In particular, we investigate the potential of a novel, off-the-shelftechnology like ChatGPT (and GPT) 3.5 compared to BERT, which represents afamily of models that currently stand as the state-of-the-art for modelingsemantic change. Our experiments represent the first attempt to assess the useof (Chat)GPT for studying semantic change. Our results indicate that ChatGPTperforms significantly worse than the foundational GPT version. Furthermore,our results demonstrate that (Chat)GPT achieves slightly lower performance thanBERT in detecting long-term changes but performs significantly worse indetecting short-term changes.</description><author>Francesco Periti, Haim Dubossarsky, Nina Tahmasebi</author><pubDate>Fri, 08 Mar 2024 15:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.14040v2</guid></item><item><title>Exploring the Links between the Fundamental Lemma and Kernel Regression</title><link>http://arxiv.org/abs/2403.05368v1</link><description>Generalizations and variations of the fundamental lemma by Willems et al. arean active topic of recent research. In this note, we explore and formalize thelinks between kernel regression and known nonlinear extensions of thefundamental lemma. Applying a transformation to the usual linear equation inHankel matrices, we arrive at an alternative implicit kernel representation ofthe system trajectories while keeping the requirements on persistency ofexcitation. We show that this representation is equivalent to the solution of aspecific kernel regression problem. We explore the possible structures of theunderlying kernel as well as the system classes to which they correspond.</description><author>Oleksii Molodchyk, Timm Faulwasser</author><pubDate>Fri, 08 Mar 2024 14:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05368v1</guid></item><item><title>RealCraft: Attention Control as A Tool for Zero-Shot Consistent Video Editing</title><link>http://arxiv.org/abs/2312.12635v3</link><description>Even though large-scale text-to-image generative models show promisingperformance in synthesizing high-quality images, applying these models directlyto image editing remains a significant challenge. This challenge is furtheramplified in video editing due to the additional dimension of time. This isespecially the case for editing real-world videos as it necessitatesmaintaining a stable structural layout across frames while executing localizededits without disrupting the existing content. In this paper, we proposeRealCraft, an attention-control-based method for zero-shot real-world videoediting. By swapping cross-attention for new feature injection and relaxingspatial-temporal attention of the editing object, we achieve localizedshape-wise edit along with enhanced temporal consistency. Our model directlyuses Stable Diffusion and operates without the need for additional information.We showcase the proposed zero-shot attention-control-based method across arange of videos, demonstrating shape-wise, time-consistent and parameter-freeediting in videos of up to 64 frames.</description><author>Shutong Jin, Ruiyu Wang, Florian T. Pokorny</author><pubDate>Fri, 08 Mar 2024 14:57:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12635v3</guid></item><item><title>The Impact of Quantization on the Robustness of Transformer-based Text Classifiers</title><link>http://arxiv.org/abs/2403.05365v1</link><description>Transformer-based models have made remarkable advancements in various NLPareas. Nevertheless, these models often exhibit vulnerabilities when confrontedwith adversarial attacks. In this paper, we explore the effect of quantizationon the robustness of Transformer-based models. Quantization usually involvesmapping a high-precision real number to a lower-precision value, aiming atreducing the size of the model at hand. To the best of our knowledge, this workis the first application of quantization on the robustness of NLP models. Inour experiments, we evaluate the impact of quantization on BERT and DistilBERTmodels in text classification using SST-2, Emotion, and MR datasets. We alsoevaluate the performance of these models against TextFooler, PWWS, and PSOadversarial attacks. Our findings show that quantization significantly improves(by an average of 18.68%) the adversarial accuracy of the models. Furthermore,we compare the effect of quantization versus that of the adversarial trainingapproach on robustness. Our experiments indicate that quantization increasesthe robustness of the model by 18.80% on average compared to adversarialtraining without imposing any extra computational overhead during training.Therefore, our results highlight the effectiveness of quantization in improvingthe robustness of NLP models.</description><author>Seyed Parsa Neshaei, Yasaman Boreshban, Gholamreza Ghassem-Sani, Seyed Abolghasem Mirroshandel</author><pubDate>Fri, 08 Mar 2024 14:55:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05365v1</guid></item><item><title>Distill n' Explain: explaining graph neural networks using simple surrogates</title><link>http://arxiv.org/abs/2303.10139v2</link><description>Explaining node predictions in graph neural networks (GNNs) often boils downto finding graph substructures that preserve predictions. Finding thesestructures usually implies back-propagating through the GNN, bonding thecomplexity (e.g., number of layers) of the GNN to the cost of explaining it.This naturally begs the question: Can we break this bond by explaining asimpler surrogate GNN? To answer the question, we propose Distill n' Explain(DnX). First, DnX learns a surrogate GNN via knowledge distillation. Then, DnXextracts node or edge-level explanations by solving a simple convex program. Wealso propose FastDnX, a faster version of DnX that leverages the lineardecomposition of our surrogate model. Experiments show that DnX and FastDnXoften outperform state-of-the-art GNN explainers while being orders ofmagnitude faster. Additionally, we support our empirical findings withtheoretical results linking the quality of the surrogate model (i.e.,distillation error) to the faithfulness of explanations.</description><author>Tamara Pereira, Erik Nascimento, Lucas E. Resck, Diego Mesquita, Amauri Souza</author><pubDate>Fri, 08 Mar 2024 14:51:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10139v2</guid></item><item><title>SplAgger: Split Aggregation for Meta-Reinforcement Learning</title><link>http://arxiv.org/abs/2403.03020v2</link><description>A core ambition of reinforcement learning (RL) is the creation of agentscapable of rapid learning in novel tasks. Meta-RL aims to achieve this bydirectly learning such agents. Black box methods do so by trainingoff-the-shelf sequence models end-to-end. By contrast, task inference methodsexplicitly infer a posterior distribution over the unknown task, typicallyusing distinct objectives and sequence models designed to enable taskinference. Recent work has shown that task inference methods are not necessaryfor strong performance. However, it remains unclear whether task inferencesequence models are beneficial even when task inference objectives are not. Inthis paper, we present strong evidence that task inference sequence models arestill beneficial. In particular, we investigate sequence models withpermutation invariant aggregation, which exploit the fact that, due to theMarkov property, the task posterior does not depend on the order of data. Weempirically confirm the advantage of permutation invariant sequence modelswithout the use of task inference objectives. However, we also find,surprisingly, that there are multiple conditions under which permutationvariance remains useful. Therefore, we propose SplAgger, which uses bothpermutation variant and invariant components to achieve the best of bothworlds, outperforming all baselines on continuous control and memoryenvironments.</description><author>Jacob Beck, Matthew Jackson, Risto Vuorio, Zheng Xiong, Shimon Whiteson</author><pubDate>Fri, 08 Mar 2024 14:51:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03020v2</guid></item><item><title>A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change</title><link>http://arxiv.org/abs/2402.12011v3</link><description>Contextualized embeddings are the preferred tool for modeling LexicalSemantic Change (LSC). Current evaluations typically focus on a specific taskknown as Graded Change Detection (GCD). However, performance comparison acrosswork are often misleading due to their reliance on diverse settings. In thispaper, we evaluate state-of-the-art models and approaches for GCD under equalconditions. We further break the LSC problem into Word-in-Context (WiC) andWord Sense Induction (WSI) tasks, and compare models across these differentlevels. Our evaluation is performed across different languages on eightavailable benchmarks for LSC, and shows that (i) APD outperforms otherapproaches for GCD; (ii) XL-LEXEME outperforms other contextualized models forWiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear needfor improving the modeling of word meanings, as well as focus on how, when, andwhy these meanings change, rather than solely focusing on the extent ofsemantic change.</description><author>Francesco Periti, Nina Tahmasebi</author><pubDate>Fri, 08 Mar 2024 14:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12011v3</guid></item><item><title>In-n-Out: Calibrating Graph Neural Networks for Link Prediction</title><link>http://arxiv.org/abs/2403.04605v2</link><description>Deep neural networks are notoriously miscalibrated, i.e., their outputs donot reflect the true probability of the event we aim to predict. While networksfor tabular or image data are usually overconfident, recent works have shownthat graph neural networks (GNNs) show the opposite behavior for node-levelclassification. But what happens when we are predicting links? We show that, inthis case, GNNs often exhibit a mixed behavior. More specifically, they may beoverconfident in negative predictions while being underconfident in positiveones. Based on this observation, we propose IN-N-OUT, the first-ever method tocalibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions:i) attributing true/false labels to an edge while respecting a GNNs predictionshould cause but small fluctuations in that edge's embedding; and, conversely,ii) if we label that same edge contradicting our GNN, embeddings should changemore substantially. An extensive experimental campaign shows that IN-N-OUTsignificantly improves the calibration of GNNs in link prediction, consistentlyoutperforming the baselines available -- which are not designed for thisspecific task.</description><author>Erik Nascimento, Diego Mesquita, Samuel Kaski, Amauri H Souza</author><pubDate>Fri, 08 Mar 2024 14:49:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04605v2</guid></item><item><title>Variational Inference of Parameters in Opinion Dynamics Models</title><link>http://arxiv.org/abs/2403.05358v1</link><description>Despite the frequent use of agent-based models (ABMs) for studying socialphenomena, parameter estimation remains a challenge, often relying on costlysimulation-based heuristics. This work uses variational inference to estimatethe parameters of an opinion dynamics ABM, by transforming the estimationproblem into an optimization task that can be solved directly. Our proposal relies on probabilistic generative ABMs (PGABMs): we start bysynthesizing a probabilistic generative model from the ABM rules. Then, wetransform the inference process into an optimization problem suitable forautomatic differentiation. In particular, we use the Gumbel-Softmaxreparameterization for categorical agent attributes and stochastic variationalinference for parameter estimation. Furthermore, we explore the trade-offs ofusing variational distributions with different complexity: normal distributionsand normalizing flows. We validate our method on a bounded confidence model with agent roles(leaders and followers). Our approach estimates both macroscopic (boundedconfidence intervals and backfire thresholds) and microscopic ($200$categorical, agent-level roles) more accurately than simulation-based and MCMCmethods. Consequently, our technique enables experts to tune and validate theirABMs against real-world observations, thus providing insights into humanbehavior in social systems via data-driven analysis.</description><author>Jacopo Lenti, Fabrizio Silvestri, Gianmarco De Francisci Morales</author><pubDate>Fri, 08 Mar 2024 14:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05358v1</guid></item><item><title>Approximate Multiagent Reinforcement Learning for On-Demand Urban Mobility Problem on a Large Map (extended version)</title><link>http://arxiv.org/abs/2311.01534v3</link><description>In this paper, we focus on the autonomous multiagent taxi routing problem fora large urban environment where the location and number of future ride requestsare unknown a-priori, but can be estimated by an empirical distribution. Recenttheory has shown that a rollout algorithm with a stable base policy produces anear-optimal stable policy. In the routing setting, a policy is stable if itsexecution keeps the number of outstanding requests uniformly bounded over time.Although, rollout-based approaches are well-suited for learning cooperativemultiagent policies with considerations for future demand, applying suchmethods to a large urban environment can be computationally expensive due tothe large number of taxis required for stability. In this paper, we aim toaddress the computational bottleneck of multiagent rollout by proposing anapproximate multiagent rollout-based two phase algorithm that reducescomputational costs, while still achieving a stable near-optimal policy. Ourapproach partitions the graph into sectors based on the predicted demand andthe maximum number of taxis that can run sequentially given the user'scomputational resources. The algorithm then applies instantaneous assignment(IA) for re-balancing taxis across sectors and a sector-wide multiagent rolloutalgorithm that is executed in parallel for each sector. We provide two maintheoretical results: 1) characterize the number of taxis $m$ that is sufficientfor IA to be stable; 2) derive a necessary condition on $m$ to maintainstability for IA as time goes to infinity. Our numerical results show that ourapproach achieves stability for an $m$ that satisfies the theoreticalconditions. We also empirically demonstrate that our proposed two phasealgorithm has equivalent performance to the one-at-a-time rollout over theentire map, but with significantly lower runtimes.</description><author>Daniel Garces, Sushmita Bhattacharya, Dimitri Bertsekas, Stephanie Gil</author><pubDate>Fri, 08 Mar 2024 14:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01534v3</guid></item><item><title>SaGE: Evaluating Moral Consistency in Large Language Models</title><link>http://arxiv.org/abs/2402.13709v2</link><description>Despite recent advancements showcasing the impressive capabilities of LargeLanguage Models (LLMs) in conversational systems, we show that evenstate-of-the-art LLMs are morally inconsistent in their generations,questioning their reliability (and trustworthiness in general). Prior works inLLM evaluation focus on developing ground-truth data to measure accuracy onspecific tasks. However, for moral scenarios that often lack universallyagreed-upon answers, consistency in model responses becomes crucial for theirreliability. To address this issue, we propose an information-theoretic measurecalled Semantic Graph Entropy (SaGE), grounded in the concept of "Rules ofThumb" (RoTs) to measure a model's moral consistency. RoTs are abstractprinciples learned by a model and can help explain their decision-makingstrategies effectively. To this extent, we construct the Moral ConsistencyCorpus (MCC), containing 50K moral questions, responses to them by LLMs, andthe RoTs that these models followed. Furthermore, to illustrate thegeneralizability of SaGE, we use it to investigate LLM consistency on twopopular datasets -- TruthfulQA and HellaSwag. Our results reveal thattask-accuracy and consistency are independent problems, and there is a direneed to investigate these issues further.</description><author>Vamshi Krishna Bonagiri, Sreeram Vennam, Priyanshul Govil, Ponnurangam Kumaraguru, Manas Gaur</author><pubDate>Fri, 08 Mar 2024 14:35:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13709v2</guid></item><item><title>Hybridized Convolutional Neural Networks and Long Short-Term Memory for Improved Alzheimer's Disease Diagnosis from MRI Scans</title><link>http://arxiv.org/abs/2403.05353v1</link><description>Brain-related diseases are more sensitive than other diseases due to severalfactors, including the complexity of surgical procedures, high costs, and otherchallenges. Alzheimer's disease is a common brain disorder that causes memoryloss and the shrinking of brain cells. Early detection is critical forproviding proper treatment to patients. However, identifying Alzheimer's at anearly stage using manual scanning of CT or MRI scans is challenging. Therefore,researchers have delved into the exploration of computer-aided systems,employing Machine Learning and Deep Learning methodologies, which entail thetraining of datasets to detect Alzheimer's disease. This study aims to presenta hybrid model that combines a CNN model's feature extraction capabilities withan LSTM model's detection capabilities. This study has applied the transferlearning called VGG16 in the hybrid model to extract features from MRI images.The LSTM detects features between the convolution layer and the fully connectedlayer. The output layer of the fully connected layer uses the softmax function.The training of the hybrid model involved utilizing the ADNI dataset. The trialfindings revealed that the model achieved a level of accuracy of 98.8%, asensitivity rate of 100%, and a specificity rate of 76%. The proposed hybridmodel outperforms its contemporary CNN counterparts, showcasing a superiorperformance.</description><author>Maleka Khatun, Md Manowarul Islam, Habibur Rahman Rifat, Md. Shamim Bin Shahid, Md. Alamin Talukder, Md Ashraf Uddin</author><pubDate>Fri, 08 Mar 2024 14:34:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05353v1</guid></item><item><title>Enhancing Plausibility Evaluation for Generated Designs with Denoising Autoencoder</title><link>http://arxiv.org/abs/2403.05352v1</link><description>A great interest has arisen in using Deep Generative Models (DGM) forgenerative design. When assessing the quality of the generated designs, humandesigners focus more on structural plausibility, e.g., no missing component,rather than visual artifacts, e.g., noises in the images. Meanwhile, commonlyused metrics such as Fr\'echet Inception Distance (FID) may not evaluateaccurately as they tend to penalize visual artifacts instead of structuralimplausibility. As such, FID might not be suitable to assess the performance ofDGMs for a generative design task. In this work, we propose to encode the inputdesigns with a simple Denoising Autoencoder (DAE) and measure the distributiondistance in the latent space thereof. We experimentally test our DAE-basedmetrics with FID and other state-of-the-art metrics on three data sets:compared to FID and some more recent works, e.g., FD$_\text{DINO-V2}$ andtopology distance, DAE-based metrics can effectively detect implausiblestructures and are more consistent with structural inspection by human experts.</description><author>Jiajie Fan, Amal Trigui, Thomas Bäck, Hao Wang</author><pubDate>Fri, 08 Mar 2024 14:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05352v1</guid></item><item><title>Multiple Instance Learning with random sampling for Whole Slide Image Classification</title><link>http://arxiv.org/abs/2403.05351v1</link><description>In computational pathology, random sampling of patches during training ofMultiple Instance Learning (MIL) methods is computationally efficient andserves as a regularization strategy. Despite its promising benefits, questionsconcerning performance trends for varying sample sizes and its influence onmodel interpretability remain. Addressing these, we reach an optimalperformance enhancement of 1.7% using thirty percent of patches on theCAMELYON16 dataset, and 3.7% with only eight samples on the TUPAC16 dataset. Wealso find interpretability effects are strongly dataset-dependent, withinterpretability impacted on CAMELYON16, while remaining unaffected on TUPAC16.This reinforces that both the performance and interpretability relationshipswith sampling are closely task-specific. End-to-end training with 1024 samplesreveals improvements across both datasets compared to pre-extracted features,further highlighting the potential of this efficient approach.</description><author>H. Keshvarikhojasteh, J. P. W. Pluim, M. Veta</author><pubDate>Fri, 08 Mar 2024 14:31:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05351v1</guid></item><item><title>Context-Based Multimodal Fusion</title><link>http://arxiv.org/abs/2403.04650v2</link><description>The fusion models, which effectively combine information from differentsources, are widely used in solving multimodal tasks. However, they havesignificant limitations related to aligning data distributions across differentmodalities. This challenge can lead to inconsistencies and difficulties inlearning robust representations. Alignment models, while specificallyaddressing this issue, often require training "from scratch" with largedatasets to achieve optimal results, which can be costly in terms of resourcesand time. To overcome these limitations, we propose an innovative model calledContext-Based Multimodal Fusion (CBMF), which combines both modality fusion anddata distribution alignment. In CBMF, each modality is represented by aspecific context vector, fused with the embedding of each modality. Thisenables the use of large pre-trained models that can be frozen, reducing thecomputational and training data requirements. Additionally, the network learnsto differentiate embeddings of different modalities through fusion with contextand aligns data distributions using a contrastive approach for self-supervisedlearning. Thus, CBMF offers an effective and economical solution for solvingcomplex multimodal tasks.</description><author>Bilal Faye, Hanane Azzag, Mustapha Lebbah, Djamel Bouchaffra</author><pubDate>Fri, 08 Mar 2024 14:29:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04650v2</guid></item><item><title>Language Generation from Brain Recordings</title><link>http://arxiv.org/abs/2311.09889v4</link><description>Generating human language through non-invasive brain-computer interfaces(BCIs) has the potential to unlock many applications, such as serving disabledpatients and improving communication. Currently, however, generating languagevia BCIs has been previously successful only within a classification setup forselecting pre-generated sentence continuation candidates with the most likelycortical semantic representation. Inspired by recent research that revealedassociations between the brain and the large computational language models, wepropose a generative language BCI that utilizes the capacity of a largelanguage model (LLM) jointly with a semantic brain decoder to directly generatelanguage from functional magnetic resonance imaging (fMRI) input. The proposedmodel can generate coherent language sequences aligned with the semanticcontent of visual or auditory language stimuli perceived, without priorknowledge of any pre-generated candidates. We compare the language generatedfrom the presented model with a random control, pre-generated languageselection approach, and a standard LLM, which generates common coherent textsolely based on the next word likelihood according to statistical languagetraining data. The proposed model is found to generate language that is morealigned with semantic stimulus in response to which brain input is sampled. Ourfindings demonstrate the potential and feasibility of employing BCIs in directlanguage generation.</description><author>Ziyi Ye, Qingyao Ai, Yiqun Liu, Maarten de Rijke, Min Zhang, Christina Lioma, Tuukka Ruotsalo</author><pubDate>Fri, 08 Mar 2024 14:23:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09889v4</guid></item><item><title>VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model</title><link>http://arxiv.org/abs/2403.05346v1</link><description>In the field of Class Incremental Object Detection (CIOD), creating modelsthat can continuously learn like humans is a major challenge. Pseudo-labelingmethods, although initially powerful, struggle with multi-scenario incrementallearning due to their tendency to forget past knowledge. To overcome this, weintroduce a new approach called Vision-Language Model assisted Pseudo-Labeling(VLM-PL). This technique uses Vision-Language Model (VLM) to verify thecorrectness of pseudo ground-truths (GTs) without requiring additional modeltraining. VLM-PL starts by deriving pseudo GTs from a pre-trained detector.Then, we generate custom queries for each pseudo GT using carefully designedprompt templates that combine image and text features. This allows the VLM toclassify the correctness through its responses. Furthermore, VLM-PL integratesrefined pseudo and real GTs from upcoming training, effectively combining newand old knowledge. Extensive experiments conducted on the Pascal VOC and MSCOCO datasets not only highlight VLM-PL's exceptional performance inmulti-scenario but also illuminate its effectiveness in dual-scenario byachieving state-of-the-art results in both.</description><author>Junsu Kim, Yunhoe Ku, Jihyeon Kim, Junuk Cha, Seungryul Baek</author><pubDate>Fri, 08 Mar 2024 14:23:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05346v1</guid></item><item><title>Federated Learning Method for Preserving Privacy in Face Recognition System</title><link>http://arxiv.org/abs/2403.05344v1</link><description>The state-of-the-art face recognition systems are typically trained on asingle computer, utilizing extensive image datasets collected from variousnumber of users. However, these datasets often contain sensitive personalinformation that users may hesitate to disclose. To address potential privacyconcerns, we explore the application of federated learning, both with andwithout secure aggregators, in the context of both supervised and unsupervisedface recognition systems. Federated learning facilitates the training of ashared model without necessitating the sharing of individual private data,achieving this by training models on decentralized edge devices housing thedata. In our proposed system, each edge device independently trains its ownmodel, which is subsequently transmitted either to a secure aggregator ordirectly to the central server. To introduce diverse data without the need fordata transmission, we employ generative adversarial networks to generateimposter data at the edge. Following this, the secure aggregator or centralserver combines these individual models to construct a global model, which isthen relayed back to the edge devices. Experimental findings based on theCelebA datasets reveal that employing federated learning in both supervised andunsupervised face recognition systems offers dual benefits. Firstly, itsafeguards privacy since the original data remains on the edge devices.Secondly, the experimental results demonstrate that the aggregated model yieldsnearly identical performance compared to the individual models, particularlywhen the federated model does not utilize a secure aggregator. Hence, ourresults shed light on the practical challenges associated withprivacy-preserving face image training, particularly in terms of the balancebetween privacy and accuracy.</description><author>Enoch Solomon, Abraham Woubie</author><pubDate>Fri, 08 Mar 2024 14:21:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05344v1</guid></item><item><title>Embedded Deployment of Semantic Segmentation in Medicine through Low-Resolution Inputs</title><link>http://arxiv.org/abs/2403.05340v1</link><description>When deploying neural networks in real-life situations, the size andcomputational effort are often the limiting factors. This is especially true inenvironments where big, expensive hardware is not affordable, like in embeddedmedical devices, where budgets are often tight. State-of-the-art proposedmultiple different lightweight solutions for such use cases, mostly by changingthe base model architecture, not taking the input and output resolution intoconsideration. In this paper, we propose our architecture that takes advantageof the fact that in hardware-limited environments, we often refrain from usingthe highest available input resolutions to guarantee a higher throughput.Although using lower-resolution input leads to a significant reduction incomputing and memory requirements, it may also incur reduced predictionquality. Our architecture addresses this problem by exploiting the fact that wecan still utilize high-resolution ground-truths in training. The proposed modelinputs lower-resolution images and high-resolution ground truths, which canimprove the prediction quality by 5.5% while adding less than 200 parameters tothe model. %reducing the frames per second only from 25 to 20. We conduct anextensive analysis to illustrate that our architecture enhances existingstate-of-the-art frameworks for lightweight semantic segmentation of cancer inMRI images. We also tested the deployment speed of state-of-the-art lightweightnetworks and our architecture on Nvidia's Jetson Nano to emulate deployment inresource-constrained embedded scenarios.</description><author>Erik Ostrowski, Muhammad Shafique</author><pubDate>Fri, 08 Mar 2024 14:17:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05340v1</guid></item><item><title>DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text</title><link>http://arxiv.org/abs/2310.12557v2</link><description>Spatial reasoning in text plays a crucial role in various real-worldapplications. Existing approaches for spatial reasoning typically infer spatialrelations from pure text, which overlooks the gap between natural language andsymbolic structures. Graph neural networks (GNNs) have showcased exceptionalproficiency in inducing and aggregating symbolic structures. However, classicalGNNs face challenges in handling multi-hop spatial reasoning due to theover-smoothing issue, i.e., the performance decreases substantially as thenumber of graph layers increases. To cope with these challenges, we propose anovel Depth-Wise Graph Neural Network (DepWiGNN). Specifically, we design anovel node memory scheme and aggregate the information over the depth dimensioninstead of the breadth dimension of the graph, which empowers the ability tocollect long dependencies without stacking multiple layers. Experimentalresults on two challenging multi-hop spatial reasoning datasets show thatDepWiGNN outperforms existing spatial reasoning methods. The comparisons withthe other three GNNs further demonstrate its superiority in capturing longdependency in the graph.</description><author>Shuaiyi Li, Yang Deng, Wai Lam</author><pubDate>Fri, 08 Mar 2024 14:16:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12557v2</guid></item><item><title>Explaining Pre-Trained Language Models with Attribution Scores: An Analysis in Low-Resource Settings</title><link>http://arxiv.org/abs/2403.05338v1</link><description>Attribution scores indicate the importance of different input parts and can,thus, explain model behaviour. Currently, prompt-based models are gainingpopularity, i.a., due to their easier adaptability in low-resource settings.However, the quality of attribution scores extracted from prompt-based modelshas not been investigated yet. In this work, we address this topic by analyzingattribution scores extracted from prompt-based models w.r.t. plausibility andfaithfulness and comparing them with attribution scores extracted fromfine-tuned models and large language models. In contrast to previous work, weintroduce training size as another dimension into the analysis. We find thatusing the prompting paradigm (with either encoder-based or decoder-basedmodels) yields more plausible explanations than fine-tuning the models inlow-resource settings and Shapley Value Sampling consistently outperformsattention and Integrated Gradients in terms of leading to more plausible andfaithful explanations.</description><author>Wei Zhou, Heike Adel, Hendrik Schuff, Ngoc Thang Vu</author><pubDate>Fri, 08 Mar 2024 14:14:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05338v1</guid></item><item><title>WatChat: Explaining perplexing programs by debugging mental models</title><link>http://arxiv.org/abs/2403.05334v1</link><description>Often, a good explanation for a program's unexpected behavior is a bug in theprogrammer's code. But sometimes, an even better explanation is a bug in theprogrammer's mental model of the language they are using. Instead of merelydebugging our current code ("giving the programmer a fish"), what if our toolscould directly debug our mental models ("teaching the programmer to fish")? Inthis paper, we apply ideas from computational cognitive science to do exactlythat. Given a perplexing program, we use program synthesis techniques toautomatically infer potential misconceptions that might cause the user to besurprised by the program's behavior. By analyzing these misconceptions, weprovide succinct, useful explanations of the program's behavior. Our methodscan even be inverted to synthesize pedagogical example programs for diagnosingand correcting misconceptions in students.</description><author>Kartik Chandra, Tzu-Mao Li, Rachit Nigam, Joshua Tenenbaum, Jonathan Ragan-Kelley</author><pubDate>Fri, 08 Mar 2024 14:10:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05334v1</guid></item><item><title>ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots</title><link>http://arxiv.org/abs/2310.10486v2</link><description>Learning a locomotion policy for quadruped robots has traditionally beenconstrained to a specific robot morphology, mass, and size. The learningprocess must usually be repeated for every new robot, where hyperparameters andreward function weights must be re-tuned to maximize performance for each newsystem. Alternatively, attempting to train a single policy to accommodatedifferent robot sizes, while maintaining the same degrees of freedom (DoF) andmorphology, requires either complex learning frameworks, or mass, inertia, anddimension randomization, which leads to prolonged training periods. In ourstudy, we show that drawing inspiration from animal motor control allows us toeffectively train a single locomotion policy capable of controlling a diverserange of quadruped robots. The robot differences encompass: a variable numberof DoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad massrange spanning from 2 kg to 200 kg, and nominal standing heights ranging from18 cm to 100 cm. Our policy modulates a representation of the Central PatternGenerator (CPG) in the spinal cord, effectively coordinating both frequenciesand amplitudes of the CPG to produce rhythmic output (Rhythm Generation), whichis then mapped to a Pattern Formation (PF) layer. Across different robots, theonly varying component is the PF layer, which adjusts the scaling parametersfor the stride height and length. Subsequently, we evaluate the sim-to-realtransfer by testing the single policy on both the Unitree Go1 and A1 robots.Remarkably, we observe robust performance, even when adding a 15 kg load,equivalent to 125% of the A1 robot's nominal mass.</description><author>Milad Shafiee, Guillaume Bellegarda, Auke Ijspeert</author><pubDate>Fri, 08 Mar 2024 14:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10486v2</guid></item><item><title>Consecutive Model Editing with Batch alongside HooK Layers</title><link>http://arxiv.org/abs/2403.05330v1</link><description>As the typical retraining paradigm is unacceptably time- andresource-consuming, researchers are turning to model editing in order to seekan effective, consecutive, and batch-supportive way to edit the model behaviordirectly. Despite all these practical expectations, existing model editingmethods fail to realize all of them. Furthermore, the memory demands for suchsuccession-supportive model editing approaches tend to be prohibitive,frequently necessitating an external memory that grows incrementally over time.To cope with these challenges, we propose COMEBA-HK, a model editing methodthat is both consecutive and batch-supportive. COMEBA-HK is memory-friendly asit only needs a small amount of it to store several hook layers with updatedweights. Experimental results demonstrate the superiority of our method overother batch-supportive model editing methods under both single-round andconsecutive batch editing scenarios. Extensive analyses of COMEBA-HK have beenconducted to verify the stability of our method over 1) the number ofconsecutive steps and 2) the number of editing instance.</description><author>Shuaiyi Li, Yang Deng, Deng Cai, Hongyuan Lu, Liang Chen, Wai Lam</author><pubDate>Fri, 08 Mar 2024 14:07:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05330v1</guid></item><item><title>OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy Prediction</title><link>http://arxiv.org/abs/2403.05329v1</link><description>3D occupancy prediction based on multi-sensor fusion, crucial for a reliableautonomous driving system, enables fine-grained understanding of 3D scenes.Previous fusion-based 3D occupancy predictions relied on depth estimation forprocessing 2D image features. However, depth estimation is an ill-posedproblem, hindering the accuracy and robustness of these methods. Furthermore,fine-grained occupancy prediction demands extensive computational resources. Weintroduce OccFusion, a multi-modal fusion method free from depth estimation,and a corresponding point cloud sampling algorithm for dense integration ofimage features. Building on this, we propose an active training method and anactive coarse to fine pipeline, enabling the model to adaptively learn morefrom complex samples and optimize predictions specifically for challengingareas such as small or overlapping objects. The active methods we propose canbe naturally extended to any occupancy prediction model. Experiments on theOpenOccupancy benchmark show our method surpasses existing state-of-the-art(SOTA) multi-modal methods in IoU across all categories. Additionally, ourmodel is more efficient during both the training and inference phases,requiring far fewer computational resources. Comprehensive ablation studiesdemonstrate the effectiveness of our proposed techniques.</description><author>Ji Zhang, Yiran Ding</author><pubDate>Fri, 08 Mar 2024 14:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05329v1</guid></item><item><title>DiffSF: Diffusion Models for Scene Flow Estimation</title><link>http://arxiv.org/abs/2403.05327v1</link><description>Scene flow estimation is an essential ingredient for a variety of real-worldapplications, especially for autonomous agents, such as self-driving cars androbots. While recent scene flow estimation approaches achieve a reasonableaccuracy, their applicability to real-world systems additionally benefits froma reliability measure. Aiming at improving accuracy while additionallyproviding an estimate for uncertainty, we propose DiffSF that combinestransformer-based scene flow estimation with denoising diffusion models. In thediffusion process, the ground truth scene flow vector field is graduallyperturbed by adding Gaussian noise. In the reverse process, starting fromrandomly sampled Gaussian noise, the scene flow vector field prediction isrecovered by conditioning on a source and a target point cloud. We show thatthe diffusion process greatly increases the robustness of predictions comparedto prior approaches resulting in state-of-the-art performance on standard sceneflow estimation benchmarks. Moreover, by sampling multiple times with differentinitial states, the denoising process predicts multiple hypotheses, whichenables measuring the output uncertainty, allowing our approach to detect amajority of the inaccurate predictions.</description><author>Yushan Zhang, Bastian Wandt, Maria Magnusson, Michael Felsberg</author><pubDate>Fri, 08 Mar 2024 14:06:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05327v1</guid></item></channel></rss>