<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 13 Nov 2023 06:00:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization</title><link>http://arxiv.org/abs/2311.06243v1</link><description>Large foundation models are becoming ubiquitous, but training them fromscratch is prohibitively expensive. Thus, efficiently adapting these powerfulmodels to downstream tasks is increasingly important. In this paper, we study aprincipled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstreamtask adaptation. Despite demonstrating good generalizability, OFT still uses afairly large number of trainable parameters due to the high dimensionality oforthogonal matrices. To address this, we start by examining OFT from aninformation transmission perspective, and then identify a few key desideratathat enable better parameter-efficiency. Inspired by how the Cooley-Tukey fastFourier transform algorithm enables efficient information transmission, wepropose an efficient orthogonal parameterization using butterfly structures. Weapply this parameterization to OFT, creating a novel parameter-efficientfinetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as aspecial case, BOFT introduces a generalized orthogonal finetuning framework.Finally, we conduct an extensive empirical study of adapting large visiontransformers, large language models, and text-to-image diffusion models tovarious downstream tasks in vision and language.</description><author>Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, Bernhard Sch√∂lkopf</author><pubDate>Fri, 10 Nov 2023 18:59:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06243v1</guid></item><item><title>Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</title><link>http://arxiv.org/abs/2311.06242v1</link><description>We introduce Florence-2, a novel vision foundation model with a unified,prompt-based representation for a variety of computer vision andvision-language tasks. While existing large vision models excel in transferlearning, they struggle to perform a diversity of tasks with simpleinstructions, a capability that implies handling the complexity of variousspatial hierarchy and semantic granularity. Florence-2 was designed to taketext-prompt as task instructions and generate desirable results in text forms,whether it be captioning, object detection, grounding or segmentation. Thismulti-task learning setup demands large-scale, high-quality annotated data. Tothis end, we co-developed FLD-5B that consists of 5.4 billion comprehensivevisual annotations on 126 million images, using an iterative strategy ofautomated image annotation and model refinement. We adopted asequence-to-sequence structure to train Florence-2 to perform versatile andcomprehensive vision tasks. Extensive evaluations on numerous tasksdemonstrated Florence-2 to be a strong vision foundation model contender withunprecedented zero-shot and fine-tuning capabilities.</description><author>Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan</author><pubDate>Fri, 10 Nov 2023 18:59:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06242v1</guid></item><item><title>Argumentation Element Annotation Modeling using XLNet</title><link>http://arxiv.org/abs/2311.06239v1</link><description>This study demonstrates the effectiveness of XLNet, a transformer-basedlanguage model, for annotating argumentative elements in persuasive essays.XLNet's architecture incorporates a recurrent mechanism that allows it to modellong-term dependencies in lengthy texts. Fine-tuned XLNet models were appliedto three datasets annotated with different schemes - a proprietary datasetusing the Annotations for Revisions and Reflections on Writing (ARROW) scheme,the PERSUADE corpus, and the Argument Annotated Essays (AAE) dataset. The XLNetmodels achieved strong performance across all datasets, even surpassing humanagreement levels in some cases. This shows XLNet capably handles diverseannotation schemes and lengthy essays. Comparisons between the model outputs ondifferent datasets also revealed insights into the relationships between theannotation tags. Overall, XLNet's strong performance on modeling argumentativestructures across diverse datasets highlights its suitability for providingautomated feedback on essay organization.</description><author>Christopher Ormerod, Amy Burkhardt, Mackenzie Young, Sue Lottridge</author><pubDate>Fri, 10 Nov 2023 18:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06239v1</guid></item><item><title>Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild</title><link>http://arxiv.org/abs/2311.06237v1</link><description>Engaging in the deliberate generation of abnormal outputs from large languagemodels (LLMs) by attacking them is a novel human activity. This paper presentsa thorough exposition of how and why people perform such attacks. Using aformal qualitative methodology, we interviewed dozens of practitioners from abroad range of backgrounds, all contributors to this novel work of attemptingto cause LLMs to fail. We relate and connect this activity between itspractitioners' motivations and goals; the strategies and techniques theydeploy; and the crucial role the community plays. As a result, this paperpresents a grounded theory of how and why people attack large language models:LLM red teaming in the wild.</description><author>Nanna Inie, Jonathan Stray, Leon Derczynski</author><pubDate>Fri, 10 Nov 2023 18:52:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06237v1</guid></item><item><title>EVORA: Deep Evidential Traversability Learning for Risk-Aware Off-Road Autonomy</title><link>http://arxiv.org/abs/2311.06234v1</link><description>Traversing terrain with good traction is crucial for achieving fast off-roadnavigation. Instead of manually designing costs based on terrain features,existing methods learn terrain properties directly from data viaself-supervision, but challenges remain to properly quantify and mitigate risksdue to uncertainties in learned models. This work efficiently quantifies bothaleatoric and epistemic uncertainties by learning discrete tractiondistributions and probability densities of the traction predictor's latentfeatures. Leveraging evidential deep learning, we parameterize Dirichletdistributions with the network outputs and propose a novel uncertainty-awaresquared Earth Mover's distance loss with a closed-form expression that improveslearning accuracy and navigation performance. The proposed risk-aware plannersimulates state trajectories with the worst-case expected traction to handlealeatoric uncertainty, and penalizes trajectories moving through terrain withhigh epistemic uncertainty. Our approach is extensively validated in simulationand on wheeled and quadruped robots, showing improved navigation performancecompared to methods that assume no slip, assume the expected traction, oroptimize for the worst-case expected cost.</description><author>Xiaoyi Cai, Siddharth Ancha, Lakshay Sharma, Philip R. Osteen, Bernadette Bucher, Stephen Phillips, Jiuguang Wang, Michael Everett, Nicholas Roy, Jonathan P. How</author><pubDate>Fri, 10 Nov 2023 18:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06234v1</guid></item><item><title>Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models</title><link>http://arxiv.org/abs/2311.06233v1</link><description>We propose the Data Contamination Quiz, a simple and effective approach todetect data contamination in large language models (LLMs) and estimate theamount of it. Specifically, we frame data contamination detection as a seriesof multiple-choice questions. We devise a quiz format wherein three perturbedversions of each dataset instance are created. These changes only includeword-level perturbations, replacing words with their contextual synonyms,ensuring both the semantic and sentence structure remain exactly the same asthe original instance. Together with the original instance, these perturbedversions constitute the choices in the quiz. Given that the only distinguishingsignal among these choices is the exact wording, an LLM, when tasked withidentifying the original instance from the choices, opts for the original if ithas memorized it in its pre-training phase--a trait intrinsic to LLMs. Adataset partition is then marked as contaminated if the LLM's performance onthe quiz surpasses what random chance suggests. Our evaluation spans sevendatasets and their respective splits (train and test/validation) on twostate-of-the-art LLMs: GPT-4 and GPT-3.5. While lacking access to thepre-training data, our results suggest that our approach not only enhances thedetection of data contamination but also provides an accurate estimation of itsextent, even when the contamination signal is weak.</description><author>Shahriar Golchin, Mihai Surdeanu</author><pubDate>Fri, 10 Nov 2023 18:48:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06233v1</guid></item><item><title>'Person' == Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion</title><link>http://arxiv.org/abs/2310.19981v2</link><description>We study stereotypes embedded within one of the most popular text-to-imagegenerators: Stable Diffusion. We examine what stereotypes of gender andnationality/continental identity does Stable Diffusion display in the absenceof such information i.e. what gender and nationality/continental identity isassigned to `a person', or to `a person from Asia'. Using vision-language modelCLIP's cosine similarity to compare images generated by CLIP-based StableDiffusion v2.1 verified by manual examination, we chronicle results from 136prompts (50 results/prompt) of front-facing images of persons from 6 differentcontinents, 27 nationalities and 3 genders. We observe how Stable Diffusionoutputs of `a person' without any additional gender/nationality informationcorrespond closest to images of men and least with persons of nonbinary gender,and to persons from Europe/North America over Africa/Asia, pointing towardsStable Diffusion having a concerning representation of personhood to be aEuropean/North American man. We also show continental stereotypes and resultantharms e.g. a person from Oceania is deemed to be Australian/New Zealander overPapua New Guinean, pointing to the erasure of Indigenous Oceanic peoples, whoform a majority over descendants of colonizers both in Papua New Guinea and inOceania overall. Finally, we unexpectedly observe a pattern ofoversexualization of women, specifically Latin American, Mexican, Indian andEgyptian women relative to other nationalities, measured through an NSFWdetector. This demonstrates how Stable Diffusion perpetuates Westernfetishization of women of color through objectification in media, which if leftunchecked will amplify this stereotypical representation. Image datasets aremade publicly available.</description><author>Sourojit Ghosh, Aylin Caliskan</author><pubDate>Fri, 10 Nov 2023 18:47:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19981v2</guid></item><item><title>Learning Human Action Recognition Representations Without Real Humans</title><link>http://arxiv.org/abs/2311.06231v1</link><description>Pre-training on massive video datasets has become essential to achieve highaction recognition performance on smaller downstream datasets. However, mostlarge-scale video datasets contain images of people and hence are accompaniedwith issues related to privacy, ethics, and data protection, often preventingthem from being publicly shared for reproducible research. Existing work hasattempted to alleviate these problems by blurring faces, downsampling videos,or training on synthetic data. On the other hand, analysis on thetransferability of privacy-preserving pre-trained models to downstream taskshas been limited. In this work, we study this problem by first asking thequestion: can we pre-train models for human action recognition with data thatdoes not include real humans? To this end, we present, for the first time, abenchmark that leverages real-world videos with humans removed and syntheticdata containing virtual humans to pre-train a model. We then evaluate thetransferability of the representation learned on this data to a diverse set ofdownstream action recognition benchmarks. Furthermore, we propose a novelpre-training strategy, called Privacy-Preserving MAE-Align, to effectivelycombine synthetic data and human-removed real data. Our approach outperformsprevious baselines by up to 5% and closes the performance gap between human andno-human action recognition representations on downstream tasks, for bothlinear probing and fine-tuning. Our benchmark, code, and models are availableat https://github.com/howardzh01/PPMA .</description><author>Howard Zhong, Samarth Mishra, Donghyun Kim, SouYoung Jin, Rameswar Panda, Hilde Kuehne, Leonid Karlinsky, Venkatesh Saligrama, Aude Oliva, Rogerio Feris</author><pubDate>Fri, 10 Nov 2023 18:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06231v1</guid></item><item><title>Chanakya: Learning Runtime Decisions for Adaptive Real-Time Perception</title><link>http://arxiv.org/abs/2106.05665v3</link><description>Real-time perception requires planned resource utilization. Computationalplanning in real-time perception is governed by two considerations -- accuracyand latency. There exist run-time decisions (e.g. choice of input resolution)that induce tradeoffs affecting performance on a given hardware, arising fromintrinsic (content, e.g. scene clutter) and extrinsic (system, e.g. resourcecontention) characteristics. Earlier runtime execution frameworks employed rule-based decision algorithmsand operated with a fixed algorithm latency budget to balance these concerns,which is sub-optimal and inflexible. We propose Chanakya, a learned approximateexecution framework that naturally derives from the streaming perceptionparadigm, to automatically learn decisions induced by these tradeoffs instead.Chanakya is trained via novel rewards balancing accuracy and latencyimplicitly, without approximating either objectives. Chanakya simultaneouslyconsiders intrinsic and extrinsic context, and predicts decisions in a flexiblemanner. Chanakya, designed with low overhead in mind, outperformsstate-of-the-art static and dynamic execution policies on public datasets onboth server GPUs and edge devices.</description><author>Anurag Ghosh, Vaibhav Balloli, Akshay Nambi, Aditya Singh, Tanuja Ganu</author><pubDate>Fri, 10 Nov 2023 18:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.05665v3</guid></item><item><title>Learning material synthesis-structure-property relationship by data fusion: Bayesian Co-regionalization N-Dimensional Piecewise Function Learning</title><link>http://arxiv.org/abs/2311.06228v1</link><description>Advanced materials are needed to further next-generation technologies such asquantum computing, carbon capture, and low-cost medical imaging. However,advanced materials discovery is confounded by two fundamental challenges: thechallenge of a high-dimensional, complex materials search space and thechallenge of combining knowledge, i.e., data fusion across instruments andlabs. To overcome the first challenge, researchers employ knowledge of theunderlying material synthesis-structure-property relationship, as a material'sstructure is often predictive of its functional property and vice versa. Forexample, optimal materials often occur along composition-phase boundaries orwithin specific phase regions. Additionally, knowledge of thesynthesis-structure-property relationship is fundamental to understandingunderlying physical mechanisms. However, quantifying thesynthesis-structure-property relationship requires overcoming the secondchallenge. Researchers must merge knowledge gathered across instruments,measurement modalities, and even laboratories. We present theSynthesis-structure-property relAtionship coreGionalized lEarner (SAGE)algorithm. A fully Bayesian algorithm that uses multimodal coregionalization tomerge knowledge across data sources to learn synthesis-structure-propertyrelationships.</description><author>A. Gilad Kusne, Austin McDannald, Brian DeCost</author><pubDate>Fri, 10 Nov 2023 18:34:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06228v1</guid></item><item><title>Does Differential Privacy Prevent Backdoor Attacks in Practice?</title><link>http://arxiv.org/abs/2311.06227v1</link><description>Differential Privacy (DP) was originally developed to protect privacy.However, it has recently been utilized to secure machine learning (ML) modelsfrom poisoning attacks, with DP-SGD receiving substantial attention.Nevertheless, a thorough investigation is required to assess the effectivenessof different DP techniques in preventing backdoor attacks in practice. In thispaper, we investigate the effectiveness of DP-SGD and, for the first time inliterature, examine PATE in the context of backdoor attacks. We also explorethe role of different components of DP algorithms in defending against backdoorattacks and will show that PATE is effective against these attacks due to thebagging structure of the teacher models it employs. Our experiments reveal thathyperparameters and the number of backdoors in the training dataset impact thesuccess of DP algorithms. Additionally, we propose Label-DP as a faster andmore accurate alternative to DP-SGD and PATE. We conclude that while Label-DPalgorithms generally offer weaker privacy protection, accurate hyper-parametertuning can make them more effective than DP methods in defending againstbackdoor attacks while maintaining model accuracy.</description><author>Fereshteh Razmi, Jian Lou, Li Xiong</author><pubDate>Fri, 10 Nov 2023 18:32:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06227v1</guid></item><item><title>Harnessing Synthetic Datasets: The Role of Shape Bias in Deep Neural Network Generalization</title><link>http://arxiv.org/abs/2311.06224v1</link><description>Recent advancements in deep learning have been primarily driven by the use oflarge models trained on increasingly vast datasets. While neural scaling lawshave emerged to predict network performance given a specific level ofcomputational resources, the growing demand for expansive datasets raisesconcerns. To address this, a new research direction has emerged, focusing onthe creation of synthetic data as a substitute. In this study, we investigatehow neural networks exhibit shape bias during training on synthetic datasets,serving as an indicator of the synthetic data quality. Specifically, ourfindings indicate three key points: (1) Shape bias varies across networkarchitectures and types of supervision, casting doubt on its reliability as apredictor for generalization and its ability to explain differences in modelrecognition compared to human capabilities. (2) Relying solely on shape bias toestimate generalization is unreliable, as it is entangled with diversity andnaturalism. (3) We propose a novel interpretation of shape bias as a tool forestimating the diversity of samples within a dataset. Our research aims toclarify the implications of using synthetic data and its associated shape biasin deep learning, addressing concerns regarding generalization and datasetquality.</description><author>Elior Benarous, Sotiris Anagnostidis, Luca Biggio, Thomas Hofmann</author><pubDate>Fri, 10 Nov 2023 18:25:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06224v1</guid></item><item><title>Diffusion Models for Earth Observation Use-cases: from cloud removal to urban change detection</title><link>http://arxiv.org/abs/2311.06222v1</link><description>The advancements in the state of the art of generative ArtificialIntelligence (AI) brought by diffusion models can be highly beneficial in novelcontexts involving Earth observation data. After introducing this new family ofgenerative models, this work proposes and analyses three use cases whichdemonstrate the potential of diffusion-based approaches for satellite imagedata. Namely, we tackle cloud removal and inpainting, dataset generation forchange-detection tasks, and urban replanning.</description><author>Fulvio Sanguigni, Mikolaj Czerkawski, Lorenzo Papa, Irene Amerini, Bertrand Le Saux</author><pubDate>Fri, 10 Nov 2023 18:24:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06222v1</guid></item><item><title>A Comparison of Lexicon-Based and ML-Based Sentiment Analysis: Are There Outlier Words?</title><link>http://arxiv.org/abs/2311.06221v1</link><description>Lexicon-based approaches to sentiment analysis of text are based on each wordor lexical entry having a pre-defined weight indicating its sentiment polarity.These are usually manually assigned but the accuracy of these when comparedagainst machine leaning based approaches to computing sentiment, are not known.It may be that there are lexical entries whose sentiment values cause alexicon-based approach to give results which are very different to a machinelearning approach. In this paper we compute sentiment for more than 150,000English language texts drawn from 4 domains using the Hedonometer, alexicon-based technique and Azure, a contemporary machine-learning basedapproach which is part of the Azure Cognitive Services family of APIs which iseasy to use. We model differences in sentiment scores between approaches fordocuments in each domain using a regression and analyse the independentvariables (Hedonometer lexical entries) as indicators of each word's importanceand contribution to the score differences. Our findings are that the importanceof a word depends on the domain and there are no standout lexical entries whichsystematically cause differences in sentiment scores.</description><author>Siddhant Jaydeep Mahajani, Shashank Srivastava, Alan F. Smeaton</author><pubDate>Fri, 10 Nov 2023 18:21:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06221v1</guid></item><item><title>Semantic-aware Video Representation for Few-shot Action Recognition</title><link>http://arxiv.org/abs/2311.06218v1</link><description>Recent work on action recognition leverages 3D features and textualinformation to achieve state-of-the-art performance. However, most of thecurrent few-shot action recognition methods still rely on 2D frame-levelrepresentations, often require additional components to model temporalrelations, and employ complex distance functions to achieve accurate alignmentof these representations. In addition, existing methods struggle to effectivelyintegrate textual semantics, some resorting to concatenation or addition oftextual and visual features, and some using text merely as an additionalsupervision without truly achieving feature fusion and information transferfrom different modalities. In this work, we propose a simple yet effectiveSemantic-Aware Few-Shot Action Recognition (SAFSAR) model to address theseissues. We show that directly leveraging a 3D feature extractor combined withan effective feature-fusion scheme, and a simple cosine similarity forclassification can yield better performance without the need of extracomponents for temporal modeling or complex distance functions. We introduce aninnovative scheme to encode the textual semantics into the video representationwhich adaptively fuses features from text and video, and encourages the visualencoder to extract more semantically consistent features. In this scheme,SAFSAR achieves alignment and fusion in a compact way. Experiments on fivechallenging few-shot action recognition benchmarks under various settingsdemonstrate that the proposed SAFSAR model significantly improves thestate-of-the-art performance.</description><author>Yutao Tang, Benjamin Bejar, Rene Vidal</author><pubDate>Fri, 10 Nov 2023 18:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06218v1</guid></item><item><title>MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things</title><link>http://arxiv.org/abs/2311.06217v1</link><description>The Internet of Things (IoT), the network integrating billions of smartphysical devices embedded with sensors, software, and communicationtechnologies for the purpose of connecting and exchanging data with otherdevices and systems, is a critical and rapidly expanding component of ourmodern world. The IoT ecosystem provides a rich source of real-world modalitiessuch as motion, thermal, geolocation, imaging, depth, sensors, video, and audiofor prediction tasks involving the pose, gaze, activities, and gestures ofhumans as well as the touch, contact, pose, 3D of physical objects. Machinelearning presents a rich opportunity to automatically process IoT data atscale, enabling efficient inference for impact in understanding humanwellbeing, controlling physical devices, and interconnecting smart cities. Todevelop machine learning technologies for IoT, this paper proposes MultiIoT,the most expansive IoT benchmark to date, encompassing over 1.15 millionsamples from 12 modalities and 8 tasks. MultiIoT introduces unique challengesinvolving (1) learning from many sensory modalities, (2) fine-grainedinteractions across long temporal ranges, and (3) extreme heterogeneity due tounique structure and noise topologies in real-world sensors. We also release aset of strong modeling baselines, spanning modality and task-specific methodsto multisensory and multitask models to encourage future research inmultisensory representation learning for IoT.</description><author>Shentong Mo, Paul Pu Liang, Russ Salakhutdinov, Louis-Philippe Morency</author><pubDate>Fri, 10 Nov 2023 18:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06217v1</guid></item><item><title>Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model</title><link>http://arxiv.org/abs/2311.06214v1</link><description>Text-to-3D with diffusion models have achieved remarkable progress in recentyears. However, existing methods either rely on score distillation-basedoptimization which suffer from slow inference, low diversity and Janusproblems, or are feed-forward methods that generate low quality results due tothe scarcity of 3D training data. In this paper, we propose Instant3D, a novelmethod that generates high-quality and diverse 3D assets from text prompts in afeed-forward manner. We adopt a two-stage paradigm, which first generates asparse set of four structured and consistent views from text in one shot with afine-tuned 2D text-to-image diffusion model, and then directly regresses theNeRF from the generated images with a novel transformer-based sparse-viewreconstructor. Through extensive experiments, we demonstrate that our methodcan generate high-quality, diverse and Janus-free 3D assets within 20 seconds,which is two order of magnitude faster than previous optimization-based methodsthat can take 1 to 10 hours. Our project webpage: https://jiahao.ai/instant3d/.</description><author>Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, Sai Bi</author><pubDate>Fri, 10 Nov 2023 18:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06214v1</guid></item><item><title>Differentiable VQ-VAE's for Robust White Matter Streamline Encodings</title><link>http://arxiv.org/abs/2311.06212v1</link><description>Given the complex geometry of white matter streamlines, Autoencoders havebeen proposed as a dimension-reduction tool to simplify the analysisstreamlines in a low-dimensional latent spaces. However, despite these recentsuccesses, the majority of encoder architectures only perform dimensionreduction on single streamlines as opposed to a full bundle of streamlines.This is a severe limitation of the encoder architecture that completelydisregards the global geometric structure of streamlines at the expense ofindividual fibers. Moreover, the latent space may not be well structured whichleads to doubt into their interpretability. In this paper we propose a novelDifferentiable Vector Quantized Variational Autoencoder, which are engineeredto ingest entire bundles of streamlines as single data-point and providesreliable trustworthy encodings that can then be later used to analyzestreamlines in the latent space. Comparisons with several state of the artAutoencoders demonstrate superior performance in both encoding and synthesis.</description><author>Andrew Lizarraga, Brandon Taraku, Edouardo Honig, Ying Nian Wu, Shantanu H. Joshi</author><pubDate>Fri, 10 Nov 2023 17:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06212v1</guid></item><item><title>ASSIST: Interactive Scene Nodes for Scalable and Realistic Indoor Simulation</title><link>http://arxiv.org/abs/2311.06211v1</link><description>We present ASSIST, an object-wise neural radiance field as a panopticrepresentation for compositional and realistic simulation. Central to ourapproach is a novel scene node data structure that stores the information ofeach object in a unified fashion, allowing online interaction in both intra-and cross-scene settings. By incorporating a differentiable neural networkalong with the associated bounding box and semantic features, the proposedstructure guarantees user-friendly interaction on independent objects to scaleup novel view simulation. Objects in the scene can be queried, added,duplicated, deleted, transformed, or swapped simply through mouse/keyboardcontrols or language instructions. Experiments demonstrate the efficacy of theproposed method, where scaled realistic simulation can be achieved throughinteractive editing and compositional rendering, with color images, depthimages, and panoptic segmentation masks generated in a 3D consistent manner.</description><author>Zhide Zhong, Jiakai Cao, Songen Gu, Sirui Xie, Weibo Gao, Liyi Luo, Zike Yan, Hao Zhao, Guyue Zhou</author><pubDate>Fri, 10 Nov 2023 17:56:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06211v1</guid></item><item><title>Optimal Cooperative Multiplayer Learning Bandits with Noisy Rewards and No Communication</title><link>http://arxiv.org/abs/2311.06210v1</link><description>We consider a cooperative multiplayer bandit learning problem where theplayers are only allowed to agree on a strategy beforehand, but cannotcommunicate during the learning process. In this problem, each playersimultaneously selects an action. Based on the actions selected by all players,the team of players receives a reward. The actions of all the players arecommonly observed. However, each player receives a noisy version of the rewardwhich cannot be shared with other players. Since players receive potentiallydifferent rewards, there is an asymmetry in the information used to selecttheir actions. In this paper, we provide an algorithm based on upper and lowerconfidence bounds that the players can use to select their optimal actionsdespite the asymmetry in the reward information. We show that this algorithmcan achieve logarithmic $O(\frac{\log T}{\Delta_{\bm{a}}})$ (gap-dependent)regret as well as $O(\sqrt{T\log T})$ (gap-independent) regret. This isasymptotically optimal in $T$. We also show that it performs empirically betterthan the current state of the art algorithm for this environment.</description><author>William Chang, Yuanhao Lu</author><pubDate>Fri, 10 Nov 2023 17:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06210v1</guid></item><item><title>Conceptual structure coheres in human cognition but not in large language models</title><link>http://arxiv.org/abs/2304.02754v2</link><description>Neural network models of language have long been used as a tool fordeveloping hypotheses about conceptual representation in the mind and brain.For many years, such use involved extracting vector-space representations ofwords and using distances among these to predict or understand human behaviorin various semantic tasks. Contemporary large language models (LLMs), however,make it possible to interrogate the latent structure of conceptualrepresentations using experimental methods nearly identical to those commonlyused with human participants. The current work utilizes three common techniquesborrowed from cognitive psychology to estimate and compare the structure ofconcepts in humans and a suite of LLMs. In humans, we show that conceptualstructure is robust to differences in culture, language, and method ofestimation. Structures estimated from LLM behavior, while individually fairlyconsistent with those estimated from human behavior, vary much more dependingupon the particular task used to generate responses--across tasks, estimates ofconceptual structure from the very same model cohere less with one another thando human structure estimates. These results highlight an important differencebetween contemporary LLMs and human cognition, with implications forunderstanding some fundamental limitations of contemporary machine language.</description><author>Siddharth Suresh, Kushin Mukherjee, Xizheng Yu, Wei-Chun Huang, Lisa Padua, Timothy T Rogers</author><pubDate>Fri, 10 Nov 2023 17:42:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02754v2</guid></item><item><title>BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection on Bangla Clickbait Dataset</title><link>http://arxiv.org/abs/2311.06204v1</link><description>Intentionally luring readers to click on a particular content by exploitingtheir curiosity defines a title as clickbait. Although several studies focusedon detecting clickbait titles in English articles, low resource language likeBangla has not been given adequate attention. To tackle clickbait titles inBangla, we have constructed the first Bangla clickbait detection datasetcontaining 15,056 labeled news articles and 65,406 unlabelled news articlesextracted from clickbait dense news sites. Each article has been labeled bythree expert linguists and includes an article's title, body, and othermetadata. By incorporating labeled and unlabelled data, we finetune apretrained Bangla transformer model in an adversarial fashion using SemiSupervised Generative Adversarial Networks (SS GANs). The proposed model actsas a good baseline for this dataset, outperforming traditional neural networkmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect thatthis dataset and the detailed analysis and comparison of these clickbaitdetection models will provide a fundamental basis for future research intodetecting clickbait titles in Bengali articles. We have released thecorresponding code and dataset.</description><author>Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque</author><pubDate>Fri, 10 Nov 2023 17:38:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06204v1</guid></item><item><title>Neural Control of Parametric Solutions for High-dimensional Evolution PDEs</title><link>http://arxiv.org/abs/2302.00045v2</link><description>We develop a novel computational framework to approximate solution operatorsof evolution partial differential equations (PDEs). By employing a generalnonlinear reduced-order model, such as a deep neural network, to approximatethe solution of a given PDE, we realize that the evolution of the modelparameter is a control problem in the parameter space. Based on thisobservation, we propose to approximate the solution operator of the PDE bylearning the control vector field in the parameter space. From any initialvalue, this control field can steer the parameter to generate a trajectory suchthat the corresponding reduced-order model solves the PDE. This allows forsubstantially reduced computational cost to solve the evolution PDE witharbitrary initial conditions. We also develop comprehensive error analysis forthe proposed method when solving a large class of semilinear parabolic PDEs.Numerical experiments on different high-dimensional evolution PDEs with variousinitial conditions demonstrate the promising results of the proposed method.</description><author>Nathan Gaby, Xiaojing Ye, Haomin Zhou</author><pubDate>Fri, 10 Nov 2023 17:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00045v2</guid></item><item><title>Greedy PIG: Adaptive Integrated Gradients</title><link>http://arxiv.org/abs/2311.06192v1</link><description>Deep learning has become the standard approach for most machine learningtasks. While its impact is undeniable, interpreting the predictions of deeplearning models from a human perspective remains a challenge. In contrast tomodel training, model interpretability is harder to quantify and pose as anexplicit optimization problem. Inspired by the AUC softmax information curve(AUC SIC) metric for evaluating feature attribution methods, we propose aunified discrete optimization framework for feature attribution and featureselection based on subset selection. This leads to a natural adaptivegeneralization of the path integrated gradients (PIG) method for featureattribution, which we call Greedy PIG. We demonstrate the success of Greedy PIGon a wide variety of tasks, including image feature attribution, graphcompression/explanation, and post-hoc feature selection on tabular data. Ourresults show that introducing adaptivity is a powerful and versatile method formaking attribution methods more powerful.</description><author>Kyriakos Axiotis, Sami Abu-al-haija, Lin Chen, Matthew Fahrbach, Gang Fu</author><pubDate>Fri, 10 Nov 2023 17:16:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06192v1</guid></item><item><title>Computationally-Efficient Neural Image Compression with Shallow Decoders</title><link>http://arxiv.org/abs/2304.06244v2</link><description>Neural image compression methods have seen increasingly strong performance inrecent years. However, they suffer orders of magnitude higher computationalcomplexity compared to traditional codecs, which hinders their real-worlddeployment. This paper takes a step forward towards closing this gap indecoding complexity by using a shallow or even linear decoding transformresembling that of JPEG. To compensate for the resulting drop in compressionperformance, we exploit the often asymmetrical computation budget betweenencoding and decoding, by adopting more powerful encoder networks and iterativeencoding. We theoretically formalize the intuition behind, and our experimentalresults establish a new frontier in the trade-off between rate-distortion anddecoding complexity for neural image compression. Specifically, we achieverate-distortion performance competitive with the established mean-scalehyperprior architecture of Minnen et al. (2018) at less than 50K decodingFLOPs/pixel, reducing the baseline's overall decoding complexity by 80%, orover 90% for the synthesis transform alone. Our code can be found athttps://github.com/mandt-lab/shallow-ntc.</description><author>Yibo Yang, Stephan Mandt</author><pubDate>Fri, 10 Nov 2023 17:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06244v2</guid></item><item><title>FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective</title><link>http://arxiv.org/abs/2311.06190v1</link><description>Multivariate time series (MTS) forecasting has shown great importance innumerous industries. Current state-of-the-art graph neural network (GNN)-basedforecasting methods usually require both graph networks (e.g., GCN) andtemporal networks (e.g., LSTM) to capture inter-series (spatial) dynamics andintra-series (temporal) dependencies, respectively. However, the uncertaincompatibility of the two networks puts an extra burden on handcrafted modeldesigns. Moreover, the separate spatial and temporal modeling naturallyviolates the unified spatiotemporal inter-dependencies in real world, whichlargely hinders the forecasting performance. To overcome these problems, weexplore an interesting direction of directly applying graph networks andrethink MTS forecasting from a pure graph perspective. We first define a noveldata structure, hypervariate graph, which regards each series value (regardlessof variates or timestamps) as a graph node, and represents sliding windows asspace-time fully-connected graphs. This perspective considers spatiotemporaldynamics unitedly and reformulates classic MTS forecasting into the predictionson hypervariate graphs. Then, we propose a novel architecture Fourier GraphNeural Network (FourierGNN) by stacking our proposed Fourier Graph Operator(FGO) to perform matrix multiplications in Fourier space. FourierGNNaccommodates adequate expressiveness and achieves much lower complexity, whichcan effectively and efficiently accomplish the forecasting. Besides, ourtheoretical analysis reveals FGO's equivalence to graph convolutions in thetime domain, which further verifies the validity of FourierGNN. Extensiveexperiments on seven datasets have demonstrated our superior performance withhigher efficiency and fewer parameters compared with state-of-the-art methods.</description><author>Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu, Pengyang Wang, Ning An, Longbing Cao, Zhendong Niu</author><pubDate>Fri, 10 Nov 2023 17:13:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06190v1</guid></item><item><title>Syntax-semantics interface: an algebraic model</title><link>http://arxiv.org/abs/2311.06189v1</link><description>We extend our formulation of Merge and Minimalism in terms of Hopf algebrasto an algebraic model of a syntactic-semantic interface. We show that methodsadopted in the formulation of renormalization (extraction of meaningfulphysical values) in theoretical physics are relevant to describe the extractionof meaning from syntactic expressions. We show how this formulation relates tocomputational models of semantics and we answer some recent controversies aboutimplications for generative linguistics of the current functioning of largelanguage models.</description><author>Matilde Marcolli, Robert C. Berwick, Noam Chomsky</author><pubDate>Fri, 10 Nov 2023 17:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06189v1</guid></item><item><title>An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in Breast Cancer</title><link>http://arxiv.org/abs/2311.06185v1</link><description>Tumour-infiltrating lymphocytes (TILs) are considered as a valuableprognostic markers in both triple-negative and human epidermal growth factorreceptor 2 (HER2) breast cancer. In this study, we introduce an innovative deeplearning pipeline based on the Efficient-UNet architecture to compute a TILsscore for breast cancer whole slide images. Our pipeline first segmentstumour-stroma regions and generates a tumour bulk mask. Subsequently, itdetects TILs within the tumour-associated stroma, generating a TILs score byclosely mirroring the pathologist's workflow. Our method exhibitsstate-of-the-art performance in segmenting tumour/stroma areas and TILsdetection, as demonstrated by internal cross-validation on the TiGER Challengetraining dataset and evaluation on the final leaderboards. Additionally, ourTILs score proves competitive in predicting survival outcomes within the samechallenge, underscoring the clinical relevance and potential of our automatedTILs scoring system as a breast cancer prognostic tool.</description><author>Adam J Shephard, Mostafa Jahanifar, Ruoyu Wang, Muhammad Dawood, Simon Graham, Kastytis Sidlauskas, Syed Ali Khurram, Nasir M Rajpoot, Shan E Ahmed Raza</author><pubDate>Fri, 10 Nov 2023 17:06:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06185v1</guid></item><item><title>Frequency-domain MLPs are More Effective Learners in Time Series Forecasting</title><link>http://arxiv.org/abs/2311.06184v1</link><description>Time series forecasting has played the key role in different industrial,including finance, traffic, energy, and healthcare domains. While existingliteratures have designed many sophisticated architectures based on RNNs, GNNs,or Transformers, another kind of approaches based on multi-layer perceptrons(MLPs) are proposed with simple structure, low complexity, and {superiorperformance}. However, most MLP-based forecasting methods suffer from thepoint-wise mappings and information bottleneck, which largely hinders theforecasting performance. To overcome this problem, we explore a novel directionof applying MLPs in the frequency domain for time series forecasting. Weinvestigate the learned patterns of frequency-domain MLPs and discover theirtwo inherent characteristic benefiting forecasting, (i) global view: frequencyspectrum makes MLPs own a complete view for signals and learn globaldependencies more easily, and (ii) energy compaction: frequency-domain MLPsconcentrate on smaller key part of frequency components with compact signalenergy. Then, we propose FreTS, a simple yet effective architecture built uponFrequency-domain MLPs for Time Series forecasting. FreTS mainly involves twostages, (i) Domain Conversion, that transforms time-domain signals into complexnumbers of frequency domain; (ii) Frequency Learning, that performs ourredesigned MLPs for the learning of real and imaginary part of frequencycomponents. The above stages operated on both inter-series and intra-seriesscales further contribute to channel-wise and time-wise dependency learning.Extensive experiments on 13 real-world benchmarks (including 7 benchmarks forshort-term forecasting and 6 benchmarks for long-term forecasting) demonstrateour consistent superiority over state-of-the-art methods.</description><author>Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Defu Lian, Ning An, Longbing Cao, Zhendong Niu</author><pubDate>Fri, 10 Nov 2023 17:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06184v1</guid></item><item><title>Hopfield-Enhanced Deep Neural Networks for Artifact-Resilient Brain State Decoding</title><link>http://arxiv.org/abs/2311.03421v3</link><description>The study of brain states, ranging from highly synchronous to asynchronousneuronal patterns like the sleep-wake cycle, is fundamental for assessing thebrain's spatiotemporal dynamics and their close connection to behavior.However, the development of new techniques to accurately identify them stillremains a challenge, as these are often compromised by the presence of noise,artifacts, and suboptimal recording quality. In this study, we propose atwo-stage computational framework combining Hopfield Networks for artifact datapreprocessing with Convolutional Neural Networks (CNNs) for classification ofbrain states in rat neural recordings under different levels of anesthesia. Toevaluate the robustness of our framework, we deliberately introduced noiseartifacts into the neural recordings. We evaluated our hybrid Hopfield-CNNpipeline by benchmarking it against two comparative models: a standalone CNNhandling the same noisy inputs, and another CNN trained and tested onartifact-free data. Performance across various levels of data compression andnoise intensities showed that our framework can effectively mitigate artifacts,allowing the model to reach parity with the clean-data CNN at lower noiselevels. Although this study mainly benefits small-scale experiments, thefindings highlight the necessity for advanced deep learning and HopfieldNetwork models to improve scalability and robustness in diverse real-worldsettings.</description><author>Arnau Marin-Llobet, Arnau Manasanch, Maria V. Sanchez-Vives</author><pubDate>Fri, 10 Nov 2023 16:52:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03421v3</guid></item><item><title>State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding</title><link>http://arxiv.org/abs/2309.12482v2</link><description>As more non-AI experts use complex AI systems for daily tasks, there has beenan increasing effort to develop methods that produce explanations of AIdecision making that are understandable by non-AI experts. Towards this effort,leveraging higher-level concepts and producing concept-based explanations havebecome a popular method. Most concept-based explanations have been developedfor classification techniques, and we posit that the few existing methods forsequential decision making are limited in scope. In this work, we firstcontribute a desiderata for defining concepts in sequential decision makingsettings. Additionally, inspired by the Protege Effect which states explainingknowledge often reinforces one's self-learning, we explore how concept-basedexplanations of an RL agent's decision making can in turn improve the agent'slearning rate, as well as improve end-user understanding of the agent'sdecision making. To this end, we contribute a unified framework,State2Explanation (S2E), that involves learning a joint embedding model betweenstate-action pairs and concept-based explanations, and leveraging such learnedmodel to both (1) inform reward shaping during an agent's training, and (2)provide explanations to end-users at deployment for improved task performance.Our experimental validations, in Connect 4 and Lunar Lander, demonstrate thesuccess of S2E in providing a dual-benefit, successfully informing rewardshaping and improving agent learning rate, as well as significantly improvingend user task performance at deployment time.</description><author>Devleena Das, Sonia Chernova, Been Kim</author><pubDate>Fri, 10 Nov 2023 16:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12482v2</guid></item><item><title>Automatic Report Generation for Histopathology images using pre-trained Vision Transformers</title><link>http://arxiv.org/abs/2311.06176v1</link><description>Deep learning for histopathology has been successfully used for diseaseclassification, image segmentation and more. However, combining image and textmodalities using current state-of-the-art methods has been a challenge due tothe high resolution of histopathology images. Automatic report generation forhistopathology images is one such challenge. In this work, we show that usingan existing pre-trained Vision Transformer in a two-step process of first usingit to encode 4096x4096 sized patches of the Whole Slide Image (WSI) and thenusing it as the encoder and an LSTM decoder for report generation, we can builda fairly performant and portable report generation mechanism that takes intoaccount the whole of the high resolution image, instead of just the patches. Weare also able to use representations from an existing powerful pre-trainedhierarchical vision transformer and show its usefulness in not just zero shotclassification but also for report generation.</description><author>Saurav Sengupta, Donald E. Brown</author><pubDate>Fri, 10 Nov 2023 16:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06176v1</guid></item><item><title>Search-Based Fairness Testing: An Overview</title><link>http://arxiv.org/abs/2311.06175v1</link><description>Artificial Intelligence (AI) has demonstrated remarkable capabilities indomains such as recruitment, finance, healthcare, and the judiciary. However,biases in AI systems raise ethical and societal concerns, emphasizing the needfor effective fairness testing methods. This paper reviews current research onfairness testing, particularly its application through search-based testing.Our analysis highlights progress and identifies areas of improvement inaddressing AI systems biases. Future research should focus on leveragingestablished search-based testing methodologies for fairness testing.</description><author>Hussaini Mamman, Shuib Basri, Abdullateef Oluwaqbemiga Balogun, Abdullahi Abubakar Imam, Ganesh Kumar, Luiz Fernando Capretz</author><pubDate>Fri, 10 Nov 2023 16:47:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06175v1</guid></item><item><title>DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor</title><link>http://arxiv.org/abs/2310.00564v2</link><description>With the remarkable progress that technology has made, the need forprocessing data near the sensors at the edge has increased dramatically. Theelectronic systems used in these applications must process data continuously,in real-time, and extract relevant information using the smallest possibleenergy budgets. A promising approach for implementing always-on processing ofsensory signals that supports on-demand, sparse, and edge-computing is to takeinspiration from biological nervous system. Following this approach, we presenta brain-inspired platform for prototyping real-time event-based Spiking NeuralNetworks (SNNs). The system proposed supports the direct emulation of dynamicand realistic neural processing phenomena such as short-term plasticity, NMDAgating, AMPA diffusion, homeostasis, spike frequency adaptation,conductance-based dendritic compartments and spike transmission delays. Theanalog circuits that implement such primitives are paired with a low latencyasynchronous digital circuits for routing and mapping events. This asynchronousinfrastructure enables the definition of different network architectures, andprovides direct event-based interfaces to convert and encode data fromevent-based and continuous-signal sensors. Here we describe the overall systemarchitecture, we characterize the mixed signal analog-digital circuits thatemulate neural dynamics, demonstrate their features with experimentalmeasurements, and present a low- and high-level software ecosystem that can beused for configuring the system. The flexibility to emulate differentbiologically plausible neural networks, and the chip's ability to monitor bothpopulation and single neuron signals in real-time, allow to develop andvalidate complex models of neural processing for both basic research andedge-computing applications.</description><author>Ole Richter, Chenxi Wu, Adrian M. Whatley, German K√∂stinger, Carsten Nielsen, Ning Qiao, Giacomo Indiveri</author><pubDate>Fri, 10 Nov 2023 16:46:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00564v2</guid></item><item><title>Dynamic Sparsity Is Channel-Level Sparsity Learner</title><link>http://arxiv.org/abs/2305.19454v2</link><description>Sparse training has received an upsurging interest in machine learning due toits tantalizing saving potential for the entire training process as well asinference. Dynamic sparse training (DST), as a leading sparse trainingapproach, can train deep neural networks at high sparsity from scratch to matchthe performance of their dense counterparts. However, most if not all DST priorarts demonstrate their effectiveness on unstructured sparsity with highlyirregular sparse patterns, which receives limited support in common hardware.This limitation hinders the usage of DST in practice. In this paper, we proposeChannel-aware dynamic sparse (Chase), which for the first time seamlesslytranslates the promise of unstructured dynamic sparsity to GPU-friendlychannel-level sparsity (not fine-grained N:M or group sparsity) during oneend-to-end training process, without any ad-hoc operations. The resulting smallsparse networks can be directly accelerated by commodity hardware, withoutusing any particularly sparsity-aware hardware accelerators. This appealingoutcome is partially motivated by a hidden phenomenon of dynamic sparsity:off-the-shelf unstructured DST implicitly involves biased parameterreallocation across channels, with a large fraction of channels (up to 60%)being sparser than others. By progressively identifying and removing thesechannels during training, our approach translates unstructured sparsity tochannel-wise sparsity. Our experimental results demonstrate that Chase achieves1.7 X inference throughput speedup on common GPU devices without compromisingaccuracy with ResNet-50 on ImageNet. We release our codes inhttps://github.com/luuyin/chase.</description><author>Lu Yin, Gen Li, Meng Fang, Li Shen, Tianjin Huang, Zhangyang Wang, Vlado Menkovski, Xiaolong Ma, Mykola Pechenizkiy, Shiwei Liu</author><pubDate>Fri, 10 Nov 2023 16:42:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19454v2</guid></item><item><title>Time Scale Network: A Shallow Neural Network For Time Series Data</title><link>http://arxiv.org/abs/2311.06170v1</link><description>Time series data is often composed of information at multiple time scales,particularly in biomedical data. While numerous deep learning strategies existto capture this information, many make networks larger, require more data, aremore demanding to compute, and are difficult to interpret. This limits theirusefulness in real-world applications facing even modest computational or dataconstraints and can further complicate their translation into practice. Wepresent a minimal, computationally efficient Time Scale Network combining thetranslation and dilation sequence used in discrete wavelet transforms withtraditional convolutional neural networks and back-propagation. The networksimultaneously learns features at many time scales for sequence classificationwith significantly reduced parameters and operations. We demonstrate advantagesin Atrial Dysfunction detection including: superior accuracy-per-parameter andaccuracy-per-operation, fast training and inference speeds, and visualizationand interpretation of learned patterns in atrial dysfunction detection on ECGsignals. We also demonstrate impressive performance in seizure prediction usingEEG signals. Our network isolated a few time scales that could be strategicallyselected to achieve 90.9% accuracy using only 1,133 active parameters andconsistently converged on pulsatile waveform shapes. This method does not reston any constraints or assumptions regarding signal content and could beleveraged in any area of time series analysis dealing with signals containingfeatures at many time scales.</description><author>Trevor Meyer, Camden Shultz, Najim Dehak, Laureano Moro-Velazquez, Pedro Irazoqui</author><pubDate>Fri, 10 Nov 2023 16:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06170v1</guid></item><item><title>Deep Fast Vision: A Python Library for Accelerated Deep Transfer Learning Vision Prototyping</title><link>http://arxiv.org/abs/2311.06169v1</link><description>Deep learning-based vision is characterized by intricate frameworks thatoften necessitate a profound understanding, presenting a barrier to newcomersand limiting broad adoption. With many researchers grappling with theconstraints of smaller datasets, there's a pronounced reliance on pre-trainedneural networks, especially for tasks such as image classification. Thisreliance is further intensified in niche imaging areas where obtaining vastdatasets is challenging. Despite the widespread use of transfer learning as aremedy to the small dataset dilemma, a conspicuous absence of tailored auto-MLsolutions persists. Addressing these challenges is "Deep Fast Vision", a pythonlibrary that streamlines the deep learning process. This tool offers auser-friendly experience, enabling results through a simple nested dictionarydefinition, helping to democratize deep learning for non-experts. Designed forsimplicity and scalability, Deep Fast Vision appears as a bridge, connectingthe complexities of existing deep learning frameworks with the needs of adiverse user base.</description><author>Fabi Prezja</author><pubDate>Fri, 10 Nov 2023 16:36:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06169v1</guid></item><item><title>Language Models can be Logical Solvers</title><link>http://arxiv.org/abs/2311.06158v1</link><description>Logical reasoning is a fundamental aspect of human intelligence and a keycomponent of tasks like problem-solving and decision-making. Recentadvancements have enabled Large Language Models (LLMs) to potentially exhibitreasoning capabilities, but complex logical reasoning remains a challenge. Thestate-of-the-art, solver-augmented language models, use LLMs to parse naturallanguage logical questions into symbolic representations first and then adoptexternal logical solvers to take in the symbolic representations and output theanswers. Despite their impressive performance, any parsing errors willinevitably result in the failure of the execution of the external logicalsolver and no answer to the logical questions. In this paper, we introduceLoGiPT, a novel language model that directly emulates the reasoning processesof logical solvers and bypasses the parsing errors by learning to strictadherence to solver syntax and grammar. LoGiPT is fine-tuned on a newlyconstructed instruction-tuning dataset derived from revealing and refining theinvisible reasoning process of deductive solvers. Experimental results on twopublic deductive reasoning datasets demonstrate that LoGiPT outperformsstate-of-the-art solver-augmented LMs and few-shot prompting methods oncompetitive LLMs like ChatGPT or GPT-4.</description><author>Jiazhan Feng, Ruochen Xu, Junheng Hao, Hiteshi Sharma, Yelong Shen, Dongyan Zhao, Weizhu Chen</author><pubDate>Fri, 10 Nov 2023 16:23:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06158v1</guid></item><item><title>MFT: Long-Term Tracking of Every Pixel</title><link>http://arxiv.org/abs/2305.12998v2</link><description>We propose MFT -- Multi-Flow dense Tracker -- a novel method for dense,pixel-level, long-term tracking. The approach exploits optical flows estimatednot only between consecutive frames, but also for pairs of frames atlogarithmically spaced intervals. It selects the most reliable sequence offlows on the basis of estimates of its geometric accuracy and the probabilityof occlusion, both provided by a pre-trained CNN. We show that MFT achievescompetitive performance on the TAP-Vid benchmark, outperforming baselines by asignificant margin, and tracking densely orders of magnitude faster than thestate-of-the-art point-tracking methods. The method is insensitive tomedium-length occlusions and it is robustified by estimating flow with respectto the reference frame, which reduces drift.</description><author>Michal Neoral, Jon√°≈° ≈†er√Ωch, Ji≈ô√≠ Matas</author><pubDate>Fri, 10 Nov 2023 16:21:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12998v2</guid></item><item><title>Challenging Common Assumptions in Multi-task Learning</title><link>http://arxiv.org/abs/2311.04698v2</link><description>While multi-task learning (MTL) has gained significant attention in recentyears, its underlying mechanisms remain poorly understood. Recent methods didnot yield consistent performance improvements over single task learning (STL)baselines, underscoring the importance of gaining more profound insights aboutchallenges specific to MTL. In our study, we challenge common assumptions inMTL in the context of STL: First, the choice of optimizer has only been mildlyinvestigated in MTL. We show the pivotal role of common STL tools such as theAdam optimizer in MTL. We deduce the effectiveness of Adam to its partialloss-scale invariance. Second, the notion of gradient conflicts has often beenphrased as a specific problem in MTL. We delve into the role of gradientconflicts in MTL and compare it to STL. For angular gradient alignment we findno evidence that this is a unique problem in MTL. We emphasize differences ingradient magnitude as the main distinguishing factor. Lastly, we compare thetransferability of features learned through MTL and STL on common imagecorruptions, and find no conclusive evidence that MTL leads to superiortransferability. Overall, we find surprising similarities between STL and MTLsuggesting to consider methods from both fields in a broader context.</description><author>Cathrin Elich, Lukas Kirchdorfer, Jan M. K√∂hler, Lukas Schott</author><pubDate>Fri, 10 Nov 2023 16:19:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04698v2</guid></item><item><title>Interpretable Graph Anomaly Detection using Gradient Attention Maps</title><link>http://arxiv.org/abs/2311.06153v1</link><description>Detecting unusual patterns in graph data is a crucial task in data mining.However, existing methods often face challenges in consistently achievingsatisfactory performance and lack interpretability, which hinders ourunderstanding of anomaly detection decisions. In this paper, we propose a novelapproach to graph anomaly detection that leverages the power ofinterpretability to enhance performance. Specifically, our method extracts anattention map derived from gradients of graph neural networks, which serves asa basis for scoring anomalies. In addition, we conduct theoretical analysisusing synthetic data to validate our method and gain insights into itsdecision-making process. To demonstrate the effectiveness of our method, weextensively evaluate our approach against state-of-the-art graph anomalydetection techniques. The results consistently demonstrate the superiorperformance of our method compared to the baselines.</description><author>Yifei Yang, Peng Wang, Xiaofan He, Dongmian Zou</author><pubDate>Fri, 10 Nov 2023 16:14:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06153v1</guid></item><item><title>An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits</title><link>http://arxiv.org/abs/2310.19025v2</link><description>We present an oracle-efficient relaxation for the adversarial contextualbandits problem, where the contexts are sequentially drawn i.i.d from a knowndistribution and the cost sequence is chosen by an online adversary. Ouralgorithm has a regret bound of$O(T^{\frac{2}{3}}(K\log(|\Pi|))^{\frac{1}{3}})$ and makes at most $O(K)$ callsper round to an offline optimization oracle, where $K$ denotes the number ofactions, $T$ denotes the number of rounds and $\Pi$ denotes the set ofpolicies. This is the first result to improve the prior best bound of$O((TK)^{\frac{2}{3}}(\log(|\Pi|))^{\frac{1}{3}})$ as obtained by Syrgkanis etal. at NeurIPS 2016, and the first to match the original bound of Langford andZhang at NeurIPS 2007 which was obtained for the stochastic case.</description><author>Kiarash Banihashem, MohammadTaghi Hajiaghayi, Suho Shin, Max Springer</author><pubDate>Fri, 10 Nov 2023 16:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19025v2</guid></item><item><title>Going beyond persistent homology using persistent homology</title><link>http://arxiv.org/abs/2311.06152v1</link><description>Representational limits of message-passing graph neural networks (MP-GNNs),e.g., in terms of the Weisfeiler-Leman (WL) test for isomorphism, are wellunderstood. Augmenting these graph models with topological features viapersistent homology (PH) has gained prominence, but identifying the class ofattributed graphs that PH can recognize remains open. We introduce a novelconcept of color-separating sets to provide a complete resolution to thisimportant problem. Specifically, we establish the necessary and sufficientconditions for distinguishing graphs based on the persistence of theirconnected components, obtained from filter functions on vertex and edge colors.Our constructions expose the limits of vertex- and edge-level PH, proving thatneither category subsumes the other. Leveraging these theoretical insights, wepropose RePHINE for learning topological features on graphs. RePHINEefficiently combines vertex- and edge-level PH, achieving a scheme that isprovably more powerful than both. Integrating RePHINE into MP-GNNs boosts theirexpressive power, resulting in gains over standard PH on several benchmarks forgraph classification.</description><author>Johanna Immonen, Amauri H. Souza, Vikas Garg</author><pubDate>Fri, 10 Nov 2023 16:12:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06152v1</guid></item><item><title>Dense Visual Odometry Using Genetic Algorithm</title><link>http://arxiv.org/abs/2311.06149v1</link><description>Our work aims to estimate the camera motion mounted on the head of a mobilerobot or a moving object from RGB-D images in a static scene. The problem ofmotion estimation is transformed into a nonlinear least squares function.Methods for solving such problems are iterative. Various classic methods gavean iterative solution by linearizing this function. We can also use themetaheuristic optimization method to solve this problem and improve results. Inthis paper, a new algorithm is developed for visual odometry using a sequenceof RGB-D images. This algorithm is based on a genetic algorithm. The proposediterative genetic algorithm searches using particles to estimate the optimalmotion and then compares it to the traditional methods. To evaluate our method,we use the root mean square error to compare it with the based energy methodand another metaheuristic method. We prove the efficiency of our innovativealgorithm on a large set of images.</description><author>Slimane Djema, Zoubir Abdeslem Benselama, Ramdane Hedjar, Krabi Abdallah</author><pubDate>Fri, 10 Nov 2023 16:09:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06149v1</guid></item><item><title>Incorporating sufficient physical information into artificial neural networks: a guaranteed improvement via physics-based Rao-Blackwellization</title><link>http://arxiv.org/abs/2311.06147v1</link><description>The concept of Rao-Blackwellization is employed to improve predictions ofartificial neural networks by physical information. The error norm and theproof of improvement are transferred from the original statistical concept to adeterministic one, using sufficient information on physics-based conditions.The proposed strategy is applied to material modeling and illustrated byexamples of the identification of a yield function, elasto-plastic steelsimulations, the identification of driving forces for quasi-brittle damage andrubber experiments. Sufficient physical information is employed, e.g., in theform of invariants, parameters of a minimization problem, dimensional analysis,isotropy and differentiability. It is proven how intuitive accretion ofinformation can yield improvement if it is physically sufficient, but also howinsufficient or superfluous information can cause impairment. Opportunities forthe improvement of artificial neural networks are explored in terms of thetraining data set, the networks' structure and output filters. Even crudeinitial predictions are remarkably improved by reducing noise, overfitting anddata requirements.</description><author>Gian-Luca Geuken, J√∂rn Mosler, Patrick Kurzeja</author><pubDate>Fri, 10 Nov 2023 16:05:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06147v1</guid></item><item><title>An Evaluation of Forensic Facial Recognition</title><link>http://arxiv.org/abs/2311.06145v1</link><description>Recent advances in machine learning and computer vision have led to reportedfacial recognition accuracies surpassing human performance. We question ifthese systems will translate to real-world forensic scenarios in which apotentially low-resolution, low-quality, partially-occluded image is comparedagainst a standard facial database. We describe the construction of alarge-scale synthetic facial dataset along with a controlled facial forensiclineup, the combination of which allows for a controlled evaluation of facialrecognition under a range of real-world conditions. Using this syntheticdataset, and a popular dataset of real faces, we evaluate the accuracy of twopopular neural-based recognition systems. We find that previously reported facerecognition accuracies of more than 95% drop to as low as 65% in this morechallenging forensic scenario.</description><author>Justin Norman, Shruti Agarwal, Hany Farid</author><pubDate>Fri, 10 Nov 2023 16:02:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06145v1</guid></item><item><title>Astrocytes as a mechanism for meta-plasticity and contextually-guided network function</title><link>http://arxiv.org/abs/2311.03508v2</link><description>Astrocytes are a ubiquitous and enigmatic type of non-neuronal cell and arefound in the brain of all vertebrates. While traditionally viewed as beingsupportive of neurons, it is increasingly recognized that astrocytes may play amore direct and active role in brain function and neural computation. Onaccount of their sensitivity to a host of physiological covariates and abilityto modulate neuronal activity and connectivity on slower time scales,astrocytes may be particularly well poised to modulate the dynamics of neuralcircuits in functionally salient ways. In the current paper, we seek to capturethese features via actionable abstractions within computational models ofneuron-astrocyte interaction. Specifically, we engage how nested feedback loopsof neuron-astrocyte interaction, acting over separated time-scales may endowastrocytes with the capability to enable learning in context-dependentsettings, where fluctuations in task parameters may occur much more slowly thanwithin-task requirements. We pose a general model of neuron-synapse-astrocyteinteraction and use formal analysis to characterize how astrocytic modulationmay constitute a form of meta-plasticity, altering the ways in which synapsesand neurons adapt as a function of time. We then embed this model in abandit-based reinforcement learning task environment, and show how the presenceof time-scale separated astrocytic modulation enables learning over multiplefluctuating contexts. Indeed, these networks learn far more reliably versusdynamically homogeneous networks and conventional non-network-based banditalgorithms. Our results indicate how the presence of neuron-astrocyteinteraction in the brain may benefit learning over different time-scales andthe conveyance of task-relevant contextual information onto circuit dynamics.</description><author>Lulu Gong, Fabio Pasqualetti, Thomas Papouin, ShiNung Ching</author><pubDate>Fri, 10 Nov 2023 15:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03508v2</guid></item><item><title>Federated Learning Across Decentralized and Unshared Archives for Remote Sensing Image Classification</title><link>http://arxiv.org/abs/2311.06141v1</link><description>Federated learning (FL) enables the collaboration of multiple deep learningmodels to learn from decentralized data archives (i.e., clients) withoutaccessing data on clients. Although FL offers ample opportunities in knowledgediscovery from distributed image archives, it is seldom considered in remotesensing (RS). In this paper, as a first time in RS, we present a comparativestudy of state-of-the-art FL algorithms. To this end, we initially provide asystematic review of the FL algorithms presented in the computer visioncommunity for image classification problems, and select severalstate-of-the-art FL algorithms based on their effectiveness with respect totraining data heterogeneity across clients (known as non-IID data). Afterpresenting an extensive overview of the selected algorithms, a theoreticalcomparison of the algorithms is conducted based on their: 1) local trainingcomplexity; 2) aggregation complexity; 3) learning efficiency; 4) communicationcost; and 5) scalability in terms of number of clients. As the classificationtask, we consider multi-label classification (MLC) problem since RS imagestypically consist of multiple classes, and thus can simultaneously beassociated with multi-labels. After the theoretical comparison, experimentalanalyses are presented to compare them under different decentralizationscenarios in terms of MLC performance. Based on our comprehensive analyses, wefinally derive a guideline for selecting suitable FL algorithms in RS. The codeof this work will be publicly available at https://git.tu-berlin.de/rsim/FL-RS.</description><author>Barƒ±≈ü B√ºy√ºkta≈ü, Gencer Sumbul, Beg√ºm Demir</author><pubDate>Fri, 10 Nov 2023 15:58:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06141v1</guid></item><item><title>Minimum norm interpolation by perceptra: Explicit regularization and implicit bias</title><link>http://arxiv.org/abs/2311.06138v1</link><description>We investigate how shallow ReLU networks interpolate between known regions.Our analysis shows that empirical risk minimizers converge to a minimum norminterpolant as the number of data points and parameters tends to infinity whena weight decay regularizer is penalized with a coefficient which vanishes at aprecise rate as the network width and the number of data points grow. With andwithout explicit regularization, we numerically study the implicit bias ofcommon optimization algorithms towards known minimum norm interpolants.</description><author>Jiyoung Park, Ian Pelakh, Stephan Wojtowytsch</author><pubDate>Fri, 10 Nov 2023 15:55:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06138v1</guid></item><item><title>MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</title><link>http://arxiv.org/abs/2311.06137v1</link><description>Self-supervised monocular depth estimation methods aim to be used in criticalapplications such as autonomous vehicles for environment analysis. Tocircumvent the potential imperfections of these approaches, a quantification ofthe prediction confidence is crucial to guide decision-making systems that relyon depth estimation. In this paper, we propose MonoProb, a new unsupervisedmonocular depth estimation method that returns an interpretable uncertainty,which means that the uncertainty reflects the expected error of the network inits depth predictions. We rethink the stereo or the structure-from-motionparadigms used to train unsupervised monocular depth models as a probabilisticproblem. Within a single forward pass inference, this model provides a depthprediction and a measure of its confidence, without increasing the inferencetime. We then improve the performance on depth and uncertainty with a novelself-distillation loss for which a student is supervised by a pseudo groundtruth that is a probability distribution on depth output by a teacher. Toquantify the performance of our models we design new metrics that, unliketraditional ones, measure the absolute performance of uncertainty predictions.Our experiments highlight enhancements achieved by our method on standard depthand uncertainty metrics as well as on our tailored metrics.https://github.com/CEA-LIST/MonoProb</description><author>Remi Marsal Florian Chabot, Angelique Loesch, William Grolleau, Hichem Sahbi</author><pubDate>Fri, 10 Nov 2023 15:55:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06137v1</guid></item><item><title>Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images</title><link>http://arxiv.org/abs/2311.06009v1</link><description>Optical Coherence Tomography Angiography (OCTA) is a promising tool fordetecting Alzheimer's disease (AD) by imaging the retinal microvasculature.Ophthalmologists commonly use region-based analysis, such as the ETDRS grid, tostudy OCTA image biomarkers and understand the correlation with AD. However,existing studies have used general deep computer vision methods, which presentchallenges in providing interpretable results and leveraging clinical priorknowledge. To address these challenges, we propose a novel deep-learningframework called Polar-Net. Our approach involves mapping OCTA images fromCartesian coordinates to polar coordinates, which allows for the use ofapproximate sector convolution and enables the implementation of the ETDRSgrid-based regional analysis method commonly used in clinical practice.Furthermore, Polar-Net incorporates clinical prior information of each sectorregion into the training process, which further enhances its performance.Additionally, our framework adapts to acquire the importance of thecorresponding retinal region, which helps researchers and clinicians understandthe model's decision-making process in detecting AD and assess its conformityto clinical observations. Through evaluations on private and public datasets,we have demonstrated that Polar-Net outperforms existing state-of-the-artmethods and provides more valuable pathological evidence for the associationbetween retinal vascular changes and AD. In addition, we also show that the twoinnovative modules introduced in our framework have a significant impact onimproving overall performance.</description><author>Shouyue Liu, Jinkui Hao, Yanwu Xu, Huazhu Fu, Xinyu Guo, Jiang Liu, Yalin Zheng, Yonghuai Liu, Jiong Zhang, Yitian Zhao</author><pubDate>Fri, 10 Nov 2023 11:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06009v1</guid></item><item><title>Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models</title><link>http://arxiv.org/abs/2305.01219v6</link><description>The prompt-based learning paradigm, which bridges the gap betweenpre-training and fine-tuning, achieves state-of-the-art performance on severalNLP tasks, particularly in few-shot settings. Despite being widely applied,prompt-based learning is vulnerable to backdoor attacks. Textual backdoorattacks are designed to introduce targeted vulnerabilities into models bypoisoning a subset of training samples through trigger injection and labelmodification. However, they suffer from flaws such as abnormal natural languageexpressions resulting from the trigger and incorrect labeling of poisonedsamples. In this study, we propose ProAttack, a novel and efficient method forperforming clean-label backdoor attacks based on the prompt, which uses theprompt itself as a trigger. Our method does not require external triggers andensures correct labeling of poisoned samples, improving the stealthy nature ofthe backdoor attack. With extensive experiments on rich-resource and few-shottext classification tasks, we empirically validate ProAttack's competitiveperformance in textual backdoor attacks. Notably, in the rich-resource setting,ProAttack achieves state-of-the-art attack success rates in the clean-labelbackdoor attack benchmark without external triggers.</description><author>Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, Jie Fu</author><pubDate>Fri, 10 Nov 2023 11:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01219v6</guid></item><item><title>Keystroke Verification Challenge (KVC): Biometric and Fairness Benchmark Evaluation</title><link>http://arxiv.org/abs/2311.06000v1</link><description>Analyzing keystroke dynamics (KD) for biometric verification has severaladvantages: it is among the most discriminative behavioral traits; keyboardsare among the most common human-computer interfaces, being the primary meansfor users to enter textual data; its acquisition does not require additionalhardware, and its processing is relatively lightweight; and it allows fortransparently recognizing subjects. However, the heterogeneity of experimentalprotocols and metrics, and the limited size of the databases adopted in theliterature impede direct comparisons between different systems, thusrepresenting an obstacle in the advancement of keystroke biometrics. Toalleviate this aspect, we present a new experimental framework to benchmarkKD-based biometric verification performance and fairness based on tweet-longsequences of variable transcript text from over 185,000 subjects, acquiredthrough desktop and mobile keyboards, extracted from the Aalto KeystrokeDatabases. The framework runs on CodaLab in the form of the KeystrokeVerification Challenge (KVC). Moreover, we also introduce a novel fairnessmetric, the Skewed Impostor Ratio (SIR), to capture inter- andintra-demographic group bias patterns in the verification scores. Wedemonstrate the usefulness of the proposed framework by employing twostate-of-the-art keystroke verification systems, TypeNet and TypeFormer, tocompare different sets of input features, achieving a less privacy-invasivesystem, by discarding the analysis of text content (ASCII codes of the keyspressed) in favor of extended features in the time domain. Our experiments showthat this approach allows to maintain satisfactory performance.</description><author>Giuseppe Stragapede, Ruben Vera-Rodriguez, Ruben Tolosana, Aythami Morales, Naser Damer, Julian Fierrez, Javier Ortega-Garcia</author><pubDate>Fri, 10 Nov 2023 11:23:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06000v1</guid></item><item><title>JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models</title><link>http://arxiv.org/abs/2311.05997v1</link><description>Achieving human-like planning and control with multimodal observations in anopen world is a key milestone for more functional generalist agents. Existingapproaches can handle certain long-horizon tasks in an open world. However,they still struggle when the number of open-world tasks could potentially beinfinite and lack the capability to progressively enhance task completion asgame time progresses. We introduce JARVIS-1, an open-world agent that canperceive multimodal input (visual observations and human instructions),generate sophisticated plans, and perform embodied control, all within thepopular yet challenging open-world Minecraft universe. Specifically, we developJARVIS-1 on top of pre-trained multimodal language models, which map visualobservations and textual instructions to plans. The plans will be ultimatelydispatched to the goal-conditioned controllers. We outfit JARVIS-1 with amultimodal memory, which facilitates planning using both pre-trained knowledgeand its actual game survival experiences. In our experiments, JARVIS-1 exhibitsnearly perfect performances across over 200 varying tasks from the MinecraftUniverse Benchmark, ranging from entry to intermediate levels. JARVIS-1 hasachieved a completion rate of 12.5% in the long-horizon diamond pickaxe task.This represents a significant increase up to 5 times compared to previousrecords. Furthermore, we show that JARVIS-1 is able to $\textit{self-improve}$following a life-long learning paradigm thanks to multimodal memory, sparking amore general intelligence and improved autonomy. The project page is availableat https://craftjarvis-jarvis1.github.io.</description><author>Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang</author><pubDate>Fri, 10 Nov 2023 11:17:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05997v1</guid></item><item><title>SANSformers: Self-Supervised Forecasting in Electronic Health Records with Attention-Free Models</title><link>http://arxiv.org/abs/2108.13672v4</link><description>Despite the proven effectiveness of Transformer neural networks acrossmultiple domains, their performance with Electronic Health Records (EHR) can benuanced. The unique, multidimensional sequential nature of EHR data cansometimes make even simple linear models with carefully engineered featuresmore competitive. Thus, the advantages of Transformers, such as efficienttransfer learning and improved scalability are not always fully exploited inEHR applications. Addressing these challenges, we introduce SANSformer, anattention-free sequential model designed with specific inductive biases tocater for the unique characteristics of EHR data. In this work, we aim to forecast the demand for healthcare services, bypredicting the number of patient visits to healthcare facilities. The challengeamplifies when dealing with divergent patient subgroups, like those with rarediseases, which are characterized by unique health trajectories and aretypically smaller in size. To address this, we employ a self-supervisedpretraining strategy, Generative Summary Pretraining (GSP), which predictsfuture summary statistics based on past health records of a patient. Our modelsare pretrained on a health registry of nearly one million patients, thenfine-tuned for specific subgroup prediction tasks, showcasing the potential tohandle the multifaceted nature of EHR data. In evaluation, SANSformer consistently surpasses robust EHR baselines, withour GSP pretraining method notably amplifying model performance, particularlywithin smaller patient subgroups. Our results illuminate the promisingpotential of tailored attention-free models and self-supervised pretraining inrefining healthcare utilization predictions across various patientdemographics.</description><author>Yogesh Kumar, Alexander Ilin, Henri Salo, Sangita Kulathinal, Maarit K. Leinonen, Pekka Marttinen</author><pubDate>Fri, 10 Nov 2023 11:11:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2108.13672v4</guid></item><item><title>Robust Adversarial Attacks Detection for Deep Learning based Relative Pose Estimation for Space Rendezvous</title><link>http://arxiv.org/abs/2311.05992v1</link><description>Research on developing deep learning techniques for autonomous spacecraftrelative navigation challenges is continuously growing in recent years.Adopting those techniques offers enhanced performance. However, such approachesalso introduce heightened apprehensions regarding the trustability and securityof such deep learning methods through their susceptibility to adversarialattacks. In this work, we propose a novel approach for adversarial attackdetection for deep neural network-based relative pose estimation schemes basedon the explainability concept. We develop for an orbital rendezvous scenario aninnovative relative pose estimation technique adopting our proposedConvolutional Neural Network (CNN), which takes an image from the chaser'sonboard camera and outputs accurately the target's relative position androtation. We perturb seamlessly the input images using adversarial attacks thatare generated by the Fast Gradient Sign Method (FGSM). The adversarial attackdetector is then built based on a Long Short Term Memory (LSTM) network whichtakes the explainability measure namely SHapley Value from the CNN-based poseestimator and flags the detection of adversarial attacks when acting.Simulation results show that the proposed adversarial attack detector achievesa detection accuracy of 99.21%. Both the deep relative pose estimator andadversarial attack detector are then tested on real data captured from ourlaboratory-designed setup. The experimental results from ourlaboratory-designed setup demonstrate that the proposed adversarial attackdetector achieves an average detection accuracy of 96.29%.</description><author>Ziwei Wang, Nabil Aouf, Jose Pizarro, Christophe Honvault</author><pubDate>Fri, 10 Nov 2023 11:07:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05992v1</guid></item><item><title>Efficient Semi-Supervised Federated Learning for Heterogeneous Participants</title><link>http://arxiv.org/abs/2307.15870v2</link><description>Federated Learning (FL) has emerged to allow multiple clients tocollaboratively train machine learning models on their private data. However,training and deploying large-scale models on resource-constrained clients ischallenging. Fortunately, Split Federated Learning (SFL) offers a feasiblesolution by alleviating the computation and/or communication burden on clients.However, existing SFL works often assume sufficient labeled data on clients,which is usually impractical. Besides, data non-IIDness across clients posesanother challenge to ensure efficient model training. To our best knowledge,the above two issues have not been simultaneously addressed in SFL. Herein, wepropose a novel Semi-SFL system, which incorporates clustering regularizationto perform SFL under the more practical scenario with unlabeled and non-IIDclient data. Moreover, our theoretical and experimental investigations intomodel convergence reveal that the inconsistent training processes on labeledand unlabeled data have an influence on the effectiveness of clusteringregularization. To this end, we develop a control algorithm for dynamicallyadjusting the global updating frequency, so as to mitigate the traininginconsistency and improve training performance. Extensive experiments onbenchmark models and datasets show that our system provides a 3.0x speed-up intraining time and reduces the communication cost by about 70.3% while reachingthe target accuracy, and achieves up to 5.1% improvement in accuracy undernon-IID scenarios compared to the state-of-the-art baselines.</description><author>Zhipeng Sun, Yang Xu, Hongli Xu, Zhiyuan Wang, Yunming Liao</author><pubDate>Fri, 10 Nov 2023 11:05:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15870v2</guid></item><item><title>Vision Big Bird: Random Sparsification for Full Attention</title><link>http://arxiv.org/abs/2311.05988v1</link><description>Recently, Transformers have shown promising performance in various visiontasks. However, the high costs of global self-attention remain challenging forTransformers, especially for high-resolution vision tasks. Inspired by one ofthe most successful transformers-based models for NLP: Big Bird, we propose anovel sparse attention mechanism for Vision Transformers (ViT). Specifically,we separate the heads into three groups, the first group used convolutionalneural network (CNN) to extract local features and provide positionalinformation for the model, the second group used Random Sampling Windows(RS-Win) for sparse self-attention calculation, and the third group reduces theresolution of the keys and values by average pooling for global attention.Based on these components, ViT maintains the sparsity of self-attention whilemaintaining the merits of Big Bird (i.e., the model is a universal approximatorof sequence functions and is Turing complete). Moreover, our results show thatthe positional encoding, a crucial component in ViTs, can be safely removed inour model. Experiments show that Vision Big Bird demonstrates competitiveperformance on common vision tasks.</description><author>Zhemin Zhang, Xun Gong</author><pubDate>Fri, 10 Nov 2023 11:00:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05988v1</guid></item><item><title>Wordification: A New Way of Teaching English Spelling Patterns</title><link>http://arxiv.org/abs/2309.12981v2</link><description>Literacy, or the ability to read and write, is a crucial indicator of successin life and greater society. It is estimated that 85% of people in juveniledelinquent systems cannot adequately read or write, that more than half ofthose with substance abuse issues have complications in reading or writing andthat two-thirds of those who do not complete high school lack proper literacyskills. Furthermore, young children who do not possess reading skills matchinggrade level by the fourth grade are approximately 80% likely to not catch up atall. Many may believe that in a developed country such as the United States,literacy fails to be an issue; however, this is a dangerous misunderstanding.Globally an estimated 1.19 trillion dollars are lost every year due to issuesin literacy; in the USA, the loss is an estimated 300 billion. To put it inmore shocking terms, one in five American adults still fail to comprehend basicsentences. Making matters worse, the only tools available now to correct a lackof reading and writing ability are found in expensive tutoring or otherprograms that oftentimes fail to be able to reach the required audience. Inthis paper, our team puts forward a new way of teaching English spelling andword recognitions to grade school students in the United States: Wordification.Wordification is a web application designed to teach English literacy usingprinciples of linguistics applied to the orthographic and phonologicalproperties of words in a manner not fully utilized previously in anycomputer-based teaching application.</description><author>Lexington Whalen, Nathan Bickel, Shash Comandur, Dalton Craven, Stanley Dubinsky, Homayoun Valafar</author><pubDate>Fri, 10 Nov 2023 10:49:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12981v2</guid></item><item><title>ASIF: Coupled Data Turns Unimodal Models to Multimodal Without Training</title><link>http://arxiv.org/abs/2210.01738v3</link><description>CLIP proved that aligning visual and language spaces is key to solving manyvision tasks without explicit training, but required to train image and textencoders from scratch on a huge dataset. LiT improved this by only training thetext encoder and using a pre-trained vision network. In this paper, we showthat a common space can be created without any training at all, usingsingle-domain encoders (trained with or without supervision) and a much smalleramount of image-text pairs. Furthermore, our model has unique properties. Mostnotably, deploying a new version with updated training samples can be done in amatter of seconds. Additionally, the representations in the common space areeasily interpretable as every dimension corresponds to the similarity of theinput to a unique image-text pair in the multimodal dataset. Experiments onstandard zero-shot visual benchmarks demonstrate the typical transfer abilityof image-text models. Overall, our method represents a simple yet surprisinglystrong baseline for foundation multimodal models, raising important questionson their data efficiency and on the role of retrieval in machine learning.</description><author>Antonio Norelli, Marco Fumero, Valentino Maiorca, Luca Moschella, Emanuele Rodol√†, Francesco Locatello</author><pubDate>Fri, 10 Nov 2023 10:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.01738v3</guid></item><item><title>Comparing Male Nyala and Male Kudu Classification using Transfer Learning with ResNet-50 and VGG-16</title><link>http://arxiv.org/abs/2311.05981v1</link><description>Reliable and efficient monitoring of wild animals is crucial to informmanagement and conservation decisions. The process of manually identifyingspecies of animals is time-consuming, monotonous, and expensive. Leveraging onadvances in deep learning and computer vision, we investigate in this paper theefficiency of pre-trained models, specifically the VGG-16 and ResNet-50 model,in identifying a male Kudu and a male Nyala in their natural habitats. Thesepre-trained models have proven to be efficient in animal identification ingeneral. Still, there is little research on animals like the Kudu and Nyala,who are usually well camouflaged and have similar features. The method oftransfer learning used in this paper is the fine-tuning method. The models areevaluated before and after fine-tuning. The experimental results achieved anaccuracy of 93.2\% and 97.7\% for the VGG-16 and ResNet-50 models,respectively, before fine-tuning and 97.7\% for both models after fine-tuning.Although these results are impressive, it should be noted that they were takenover a small sample size of 550 images split in half between the two classes;therefore, this might not cater to enough scenarios to get a full conclusion ofthe efficiency of the models. Therefore, there is room for more work in gettinga more extensive dataset and testing and extending to the female counterpartsof these species and the whole antelope species.</description><author>T. T Lemani, T. L. van Zyl</author><pubDate>Fri, 10 Nov 2023 10:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05981v1</guid></item><item><title>Physics-Informed Neural Networks for Time-Domain Simulations: Accuracy, Computational Cost, and Flexibility</title><link>http://arxiv.org/abs/2303.08994v2</link><description>The simulation of power system dynamics poses a computationally expensivetask. Considering the growing uncertainty of generation and demand patterns,thousands of scenarios need to be continuously assessed to ensure the safety ofpower systems. Physics-Informed Neural Networks (PINNs) have recently emergedas a promising solution for drastically accelerating computations of non-lineardynamical systems. This work investigates the applicability of these methodsfor power system dynamics, focusing on the dynamic response to loaddisturbances. Comparing the prediction of PINNs to the solution of conventionalsolvers, we find that PINNs can be 10 to 1000 times faster than conventionalsolvers. At the same time, we find them to be sufficiently accurate andnumerically stable even for large time steps. To facilitate a deeperunderstanding, this paper also present a new regularisation of Neural Network(NN) training by introducing a gradient-based term in the loss function. Theresulting NNs, which we call dtNNs, help us deliver a comprehensive analysisabout the strengths and weaknesses of the NN based approaches, howincorporating knowledge of the underlying physics affects NN performance, andhow this compares with conventional solvers for power system dynamics.</description><author>Jochen Stiasny, Spyros Chatzivasileiadis</author><pubDate>Fri, 10 Nov 2023 10:33:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08994v2</guid></item><item><title>Average degree of the essential variety</title><link>http://arxiv.org/abs/2212.01596v2</link><description>The essential variety is an algebraic subvariety of dimension $5$ in realprojective space $\mathbb R\mathrm P^{8}$ which encodes the relative pose oftwo calibrated pinhole cameras. The $5$-point algorithm in computer visioncomputes the real points in the intersection of the essential variety with alinear space of codimension $5$. The degree of the essential variety is $10$,so this intersection consists of 10 complex points in general. We compute the expected number of real intersection points when the linearspace is random. We focus on two probability distributions for linear spaces.The first distribution is invariant under the action of the orthogonal group$\mathrm{O}(9)$ acting on linear spaces in $\mathbb R\mathrm P^{8}$. In thiscase, the expected number of real intersection points is equal to $4$. Thesecond distribution is motivated from computer vision and is defined bychoosing 5 point correspondences in the image planes $\mathbb R\mathrmP^2\times \mathbb R\mathrm P^2$ uniformly at random. A Monte Carlo computationsuggests that with high probability the expected value lies in the interval$(3.95 - 0.05,\ 3.95 + 0.05)$.</description><author>Paul Breiding, Samantha Fairchild, Pierpaola Santarsiero, Elima Shehu</author><pubDate>Fri, 10 Nov 2023 10:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.01596v2</guid></item><item><title>Sum-max Submodular Bandits</title><link>http://arxiv.org/abs/2311.05975v1</link><description>Many online decision-making problems correspond to maximizing a sequence ofsubmodular functions. In this work, we introduce sum-max functions, a subclassof monotone submodular functions capturing several interesting problems,including best-of-$K$-bandits, combinatorial bandits, and the bandit versionson facility location, $M$-medians, and hitting sets. We show that all functionsin this class satisfy a key property that we call pseudo-concavity. This allowsus to prove $\big(1 - \frac{1}{e}\big)$-regret bounds for bandit feedback inthe nonstochastic setting of the order of $\sqrt{MKT}$ (ignoring log factors),where $T$ is the time horizon and $M$ is a cardinality constraint. This bound,attained by a simple and efficient algorithm, significantly improves on the$\widetilde{O}\big(T^{2/3}\big)$ regret bound for online monotone submodularmaximization with bandit feedback.</description><author>Stephen Pasteris, Alberto Rumi, Fabio Vitale, Nicol√≤ Cesa-Bianchi</author><pubDate>Fri, 10 Nov 2023 10:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05975v1</guid></item><item><title>(Un)likelihood Training for Interpretable Embedding</title><link>http://arxiv.org/abs/2207.00282v3</link><description>Cross-modal representation learning has become a new normal for bridging thesemantic gap between text and visual data. Learning modality agnosticrepresentations in a continuous latent space, however, is often treated as ablack-box data-driven training process. It is well-known that the effectivenessof representation learning depends heavily on the quality and scale of trainingdata. For video representation learning, having a complete set of labels thatannotate the full spectrum of video content for training is highly difficult ifnot impossible. These issues, black-box training and dataset bias, makerepresentation learning practically challenging to be deployed for videounderstanding due to unexplainable and unpredictable results. In this paper, wepropose two novel training objectives, likelihood and unlikelihood functions,to unroll semantics behind embeddings while addressing the label sparsityproblem in training. The likelihood training aims to interpret semantics ofembeddings beyond training labels, while the unlikelihood training leveragesprior knowledge for regularization to ensure semantically coherentinterpretation. With both training objectives, a new encoder-decoder network,which learns interpretable cross-modal representation, is proposed for ad-hocvideo search. Extensive experiments on TRECVid and MSR-VTT datasets show theproposed network outperforms several state-of-the-art retrieval models with astatistically significant performance margin.</description><author>Jiaxin Wu, Chong-Wah Ngo, Wing-Kwong Chan, Zhijian Hou</author><pubDate>Fri, 10 Nov 2023 10:18:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.00282v3</guid></item><item><title>Quantized Distillation: Optimizing Driver Activity Recognition Models for Resource-Constrained Environments</title><link>http://arxiv.org/abs/2311.05970v1</link><description>Deep learning-based models are at the forefront of most driver observationbenchmarks due to their remarkable accuracies but are also associated with highcomputational costs. This is challenging, as resources are often limited inreal-world driving scenarios. This paper introduces a lightweight framework forresource-efficient driver activity recognition. The framework enhances 3DMobileNet, a neural architecture optimized for speed in video classification,by incorporating knowledge distillation and model quantization to balance modelaccuracy and computational efficiency. Knowledge distillation helps maintainaccuracy while reducing the model size by leveraging soft labels from a largerteacher model (I3D), instead of relying solely on original ground truth data.Model quantization significantly lowers memory and computation demands by usinglower precision integers for model weights and activations. Extensive testingon a public dataset for in-vehicle monitoring during autonomous drivingdemonstrates that this new framework achieves a threefold reduction in modelsize and a 1.4-fold improvement in inference time, compared to an alreadyoptimized architecture. The code for this study is available athttps://github.com/calvintanama/qd-driver-activity-reco.</description><author>Calvin Tanama, Kunyu Peng, Zdravko Marinov, Rainer Stiefelhagen, Alina Roitberg</author><pubDate>Fri, 10 Nov 2023 10:07:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05970v1</guid></item><item><title>Plasma Surrogate Modelling using Fourier Neural Operators</title><link>http://arxiv.org/abs/2311.05967v1</link><description>Predicting plasma evolution within a Tokamak reactor is crucial to realizingthe goal of sustainable fusion. Capabilities in forecasting the spatio-temporalevolution of plasma rapidly and accurately allow us to quickly iterate overdesign and control strategies on current Tokamak devices and future reactors.Modelling plasma evolution using numerical solvers is often expensive,consuming many hours on supercomputers, and hence, we need alternativeinexpensive surrogate models. We demonstrate accurate predictions of plasmaevolution both in simulation and experimental domains using deep learning-basedsurrogate modelling tools, viz., Fourier Neural Operators (FNO). We show thatFNO has a speedup of six orders of magnitude over traditional solvers inpredicting the plasma dynamics simulated from magnetohydrodynamic models, whilemaintaining a high accuracy (MSE $\approx$ $10^{-5}$). Our modified version ofthe FNO is capable of solving multi-variable Partial Differential Equations(PDE), and can capture the dependence among the different variables in a singlemodel. FNOs can also predict plasma evolution on real-world experimental dataobserved by the cameras positioned within the MAST Tokamak, i.e., cameraslooking across the central solenoid and the divertor in the Tokamak. We showthat FNOs are able to accurately forecast the evolution of plasma and have thepotential to be deployed for real-time monitoring. We also illustrate theircapability in forecasting the plasma shape, the locations of interactions ofthe plasma with the central solenoid and the divertor for the full duration ofthe plasma shot within MAST. The FNO offers a viable alternative for surrogatemodelling as it is quick to train and infer, and requires fewer data points,while being able to do zero-shot super-resolution and getting high-fidelitysolutions.</description><author>Vignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, Daniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, Marc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team</author><pubDate>Fri, 10 Nov 2023 10:05:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05967v1</guid></item><item><title>Large Language Models are Zero Shot Hypothesis Proposers</title><link>http://arxiv.org/abs/2311.05965v1</link><description>Significant scientific discoveries have driven the progress of humancivilisation. The explosion of scientific literature and data has createdinformation barriers across disciplines that have slowed the pace of scientificdiscovery. Large Language Models (LLMs) hold a wealth of global andinterdisciplinary knowledge that promises to break down these informationbarriers and foster a new wave of scientific discovery. However, the potentialof LLMs for scientific discovery has not been formally explored. In this paper,we start from investigating whether LLMs can propose scientific hypotheses. Tothis end, we construct a dataset consist of background knowledge and hypothesispairs from biomedical literature. The dataset is divided into training, seen,and unseen test sets based on the publication date to control visibility. Wesubsequently evaluate the hypothesis generation capabilities of varioustop-tier instructed models in zero-shot, few-shot, and fine-tuning settings,including both closed and open-source LLMs. Additionally, we introduce anLLM-based multi-agent cooperative framework with different role designs andexternal tools to enhance the capabilities related to generating hypotheses. Wealso design four metrics through a comprehensive review to evaluate thegenerated hypotheses for both ChatGPT-based and human evaluations. Throughexperiments and analyses, we arrive at the following findings: 1) LLMssurprisingly generate untrained yet validated hypotheses from testingliterature. 2) Increasing uncertainty facilitates candidate generation,potentially enhancing zero-shot hypothesis generation capabilities. Thesefindings strongly support the potential of LLMs as catalysts for new scientificdiscoveries and guide further exploration.</description><author>Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, Bowen Zhou</author><pubDate>Fri, 10 Nov 2023 10:03:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05965v1</guid></item><item><title>Multiscale Neural Operators for Solving Time-Independent PDEs</title><link>http://arxiv.org/abs/2311.05964v1</link><description>Time-independent Partial Differential Equations (PDEs) on large meshes posesignificant challenges for data-driven neural PDE solvers. We introduce a novelgraph rewiring technique to tackle some of these challenges, such asaggregating information across scales and on irregular meshes. Our proposedapproach bridges distant nodes, enhancing the global interaction capabilitiesof GNNs. Our experiments on three datasets reveal that GNN-based methods setnew performance standards for time-independent PDEs on irregular meshes.Finally, we show that our graph rewiring strategy boosts the performance ofbaseline methods, achieving state-of-the-art results in one of the tasks.</description><author>Winfried Ripken, Lisa Coiffard, Felix Pieper, Sebastian Dziadzio</author><pubDate>Fri, 10 Nov 2023 10:02:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05964v1</guid></item><item><title>Learning with Exposure Constraints in Recommendation Systems</title><link>http://arxiv.org/abs/2302.01377v2</link><description>Recommendation systems are dynamic economic systems that balance the needs ofmultiple stakeholders. A recent line of work studies incentives from thecontent providers' point of view. Content providers, e.g., vloggers andbloggers, contribute fresh content and rely on user engagement to createrevenue and finance their operations. In this work, we propose a contextualmulti-armed bandit setting to model the dependency of content providers onexposure. In our model, the system receives a user context in every round andhas to select one of the arms. Every arm is a content provider who must receivea minimum number of pulls every fixed time period (e.g., a month) to remainviable in later rounds; otherwise, the arm departs and is no longer available.The system aims to maximize the users' (content consumers) welfare. To thatend, it should learn which arms are vital and ensure they remain viable bysubsidizing arm pulls if needed. We develop algorithms with sub-linear regret,as well as a lower bound that demonstrates that our algorithms are optimal upto logarithmic factors.</description><author>Omer Ben-Porat, Rotem Torkan</author><pubDate>Fri, 10 Nov 2023 09:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01377v2</guid></item><item><title>PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning</title><link>http://arxiv.org/abs/2310.09183v2</link><description>Classical federated learning (FL) enables training machine learning modelswithout sharing data for privacy preservation, but heterogeneous datacharacteristic degrades the performance of the localized model. Personalized FL(PFL) addresses this by synthesizing personalized models from a global modelvia training on local data. Such a global model may overlook the specificinformation that the clients have been sampled. In this paper, we propose anovel scheme to inject personalized prior knowledge into the global model ineach client, which attempts to mitigate the introduced incomplete informationproblem in PFL. At the heart of our proposed approach is a framework, the PFLwith Bregman Divergence (pFedBreD), decoupling the personalized prior from thelocal objective function regularized by Bregman divergence for greateradaptability in personalized scenarios. We also relax the mirror descent (RMD)to extract the prior explicitly to provide optional strategies. Additionally,our pFedBreD is backed up by a convergence analysis. Sufficient experimentsdemonstrate that our method reaches the state-of-the-art performances on 5datasets and outperforms other methods by up to 3.5% across 8 benchmarks.Extensive analyses verify the robustness and necessity of proposed designs.</description><author>Mingjia Shi, Yuhao Zhou, Kai Wang, Huaizheng Zhang, Shudong Huang, Qing Ye, Jiangcheng Lv</author><pubDate>Fri, 10 Nov 2023 09:53:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.09183v2</guid></item><item><title>Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles</title><link>http://arxiv.org/abs/2310.15952v3</link><description>While deep learning models have achieved remarkable success across a range ofmedical image analysis tasks, deployment of these models in real clinicalcontexts requires that they be robust to variability in the acquired images.While many methods apply predefined transformations to augment the trainingdata to enhance test-time robustness, these transformations may not ensure themodel's robustness to the diverse variability seen in patient images. In thispaper, we introduce a novel three-stage approach based on transformers coupledwith conditional diffusion models, with the goal of improving model robustnessto the kinds of imaging variability commonly encountered in practice withoutthe need for pre-determined data augmentation strategies. To this end, multipleimage encoders first learn hierarchical feature representations to builddiscriminative latent spaces. Next, a reverse diffusion process, guided by thelatent code, acts on an informative prior and proposes prediction candidates ina generative manner. Finally, several prediction candidates are aggregated in abi-level aggregation protocol to produce the final output. Through extensiveexperiments on medical imaging benchmark datasets, we show that our methodimproves upon state-of-the-art methods in terms of robustness and confidencecalibration. Additionally, we introduce a strategy to quantify the predictionuncertainty at the instance level, increasing their trustworthiness toclinicians using them in clinical practice.</description><author>Xing Shen, Hengguan Huang, Brennan Nichyporuk, Tal Arbel</author><pubDate>Fri, 10 Nov 2023 09:52:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.15952v3</guid></item><item><title>StrainTensorNet: Predicting crystal structure elastic properties using SE(3)-equivariant graph neural networks</title><link>http://arxiv.org/abs/2306.12818v2</link><description>Accurately predicting the elastic properties of crystalline solids is vitalfor computational materials science. However, traditional atomistic scale abinitio approaches are computationally intensive, especially for studyingcomplex materials with a large number of atoms in a unit cell. We introduce anovel data-driven approach to efficiently predict the elastic properties ofcrystal structures using SE(3)-equivariant graph neural networks (GNNs). Thisapproach yields important scalar elastic moduli with the accuracy comparable torecent data-driven studies. Importantly, our symmetry-aware GNNs model alsoenables the prediction of the strain energy density (SED) and the associatedelastic constants, the fundamental tensorial quantities that are significantlyinfluenced by a material's crystallographic group. The model consistentlydistinguishes independent elements of SED tensors, in accordance with thesymmetry of the crystal structures. Finally, our deep learning model possessesmeaningful latent features, offering an interpretable prediction of the elasticproperties.</description><author>Teerachote Pakornchote, Annop Ektarawong, Thiparat Chotibut</author><pubDate>Fri, 10 Nov 2023 09:49:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12818v2</guid></item><item><title>Hierarchical deep learning-based adaptive time-stepping scheme for multiscale simulations</title><link>http://arxiv.org/abs/2311.05961v1</link><description>Multiscale is a hallmark feature of complex nonlinear systems. While thesimulation using the classical numerical methods is restricted by the local\textit{Taylor} series constraints, the multiscale techniques are often limitedby finding heuristic closures. This study proposes a new method for simulatingmultiscale problems using deep neural networks. By leveraging the hierarchicallearning of neural network time steppers, the method adapts time steps toapproximate dynamical system flow maps across timescales. This approachachieves state-of-the-art performance in less computational time compared tofixed-step neural network solvers. The proposed method is demonstrated onseveral nonlinear dynamical systems, and source codes are provided forimplementation. This method has the potential to benefit multiscale analysis ofcomplex systems and encourage further investigation in this area.</description><author>Asif Hamid, Danish Rafiq, Shahkar Ahmad Nahvi, Mohammad Abid Bazaz</author><pubDate>Fri, 10 Nov 2023 09:47:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05961v1</guid></item><item><title>A Neural Height-Map Approach for the Binocular Photometric Stereo Problem</title><link>http://arxiv.org/abs/2311.05958v1</link><description>In this work we propose a novel, highly practical, binocular photometricstereo (PS) framework, which has same acquisition speed as single view PS,however significantly improves the quality of the estimated geometry. As in recent neural multi-view shape estimation frameworks such as NeRF,SIREN and inverse graphics approaches to multi-view photometric stereo (e.g.PS-NeRF) we formulate shape estimation task as learning of a differentiablesurface and texture representation by minimising surface normal discrepancy fornormals estimated from multiple varying light images for two views as well asdiscrepancy between rendered surface intensity and observed images. Our methoddiffers from typical multi-view shape estimation approaches in two key ways.First, our surface is represented not as a volume but as a neural heightmapwhere heights of points on a surface are computed by a deep neural network.Second, instead of predicting an average intensity as PS-NeRF or introducinglambertian material assumptions as Guo et al., we use a learnt BRDF and performnear-field per point intensity rendering. Our method achieves the state-of-the-art performance on the DiLiGenT-MVdataset adapted to binocular stereo setup as well as a new binocularphotometric stereo dataset - LUCES-ST.</description><author>Fotios Logothetis, Ignas Budvytis, Roberto Cipolla</author><pubDate>Fri, 10 Nov 2023 09:45:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05958v1</guid></item><item><title>Planning Landmark Based Goal Recognition Revisited: Does Using Initial State Landmarks Make Sense?</title><link>http://arxiv.org/abs/2306.15362v2</link><description>Goal recognition is an important problem in many application domains (e.g.,pervasive computing, intrusion detection, computer games, etc.). In manyapplication scenarios, it is important that goal recognition algorithms canrecognize goals of an observed agent as fast as possible. However, many earlyapproaches in the area of Plan Recognition As Planning, require quite largeamounts of computation time to calculate a solution. Mainly to address thisissue, recently, Pereira et al. developed an approach that is based on planninglandmarks and is much more computationally efficient than previous approaches.However, the approach, as proposed by Pereira et al., also uses triviallandmarks (i.e., facts that are part of the initial state and goal descriptionare landmarks by definition). In this paper, we show that it does not provideany benefit to use landmarks that are part of the initial state in a planninglandmark based goal recognition approach. The empirical results show thatomitting initial state landmarks for goal recognition improves goal recognitionperformance.</description><author>Nils Wilken, Lea Cohausz, Christian Bartelt, Heiner Stuckenschmidt</author><pubDate>Fri, 10 Nov 2023 09:44:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15362v2</guid></item><item><title>ID Embedding as Subtle Features of Content and Structure for Multimodal Recommendation</title><link>http://arxiv.org/abs/2311.05956v1</link><description>Multimodal recommendation aims to model user and item representationscomprehensively with the involvement of multimedia content for effectiverecommendations. Existing research has shown that it is beneficial forrecommendation performance to combine (user- and item-) ID embeddings withmultimodal salient features, indicating the value of IDs. However, there is alack of a thorough analysis of the ID embeddings in terms of feature semanticsin the literature. In this paper, we revisit the value of ID embeddings formultimodal recommendation and conduct a thorough study regarding its semantics,which we recognize as subtle features of content and structures. Then, wepropose a novel recommendation model by incorporating ID embeddings to enhancethe semantic features of both content and structures. Specifically, we putforward a hierarchical attention mechanism to incorporate ID embeddings inmodality fusing, coupled with contrastive learning, to enhance contentrepresentations. Meanwhile, we propose a lightweight graph convolutionalnetwork for each modality to amalgamate neighborhood and ID embeddings forimproving structural representations. Finally, the content and structurerepresentations are combined to form the ultimate item embedding forrecommendation. Extensive experiments on three real-world datasets (Baby,Sports, and Clothing) demonstrate the superiority of our method overstate-of-the-art multimodal recommendation methods and the effectiveness offine-grained ID embeddings.</description><author>Yuting Liu, Enneng Yang, Yizhou Dang, Guibing Guo, Qiang Liu, Yuliang Liang, Linying Jiang, Xingwei Wang</author><pubDate>Fri, 10 Nov 2023 09:41:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05956v1</guid></item><item><title>Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning</title><link>http://arxiv.org/abs/2305.13971v5</link><description>Despite their impressive performance, large language models (LMs) stillstruggle with reliably generating complex output structures when not finetunedto follow the required output format exactly. To address this issue,grammar-constrained decoding (GCD) can be used to control the generation ofLMs, guaranteeing that the output follows a given structure. Most existing GCDmethods are, however, limited to specific tasks, such as parsing or codegeneration. In this work, we demonstrate that formal grammars can describe theoutput space for a much wider range of tasks and argue that GCD can serve as aunified framework for structured NLP tasks in general. For increasedflexibility, we introduce input-dependent grammars, which allow the grammar todepend on the input and thus enable the generation of different outputstructures for different inputs. We then empirically demonstrate the power andflexibility of GCD-enhanced LMs on (1) information extraction, (2) entitydisambiguation, and (3) constituency parsing. Our results indicate thatgrammar-constrained LMs substantially outperform unconstrained LMs or even beattask-specific finetuned models. Grammar constraints thus hold great promise forharnessing off-the-shelf LMs for a wide range of structured NLP tasks,especially where training data is scarce or finetuning is expensive. Code anddata: https://github.com/epfl-dlab/GCD.</description><author>Saibo Geng, Martin Josifoski, Maxime Peyrard, Robert West</author><pubDate>Fri, 10 Nov 2023 09:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13971v5</guid></item><item><title>InfoEntropy Loss to Mitigate Bias of Learning Difficulties for Generative Language Models</title><link>http://arxiv.org/abs/2310.19531v3</link><description>Generative language models are usually pretrained on large text corpus viapredicting the next token (i.e., sub-word/word/phrase) given the previous ones.Recent works have demonstrated the impressive performance of large generativelanguage models on downstream tasks. However, existing generative languagemodels generally neglect an inherent challenge in text corpus during training,i.e., the imbalance between frequent tokens and infrequent ones. It can lead alanguage model to be dominated by common and easy-to-learn tokens, therebyoverlooking the infrequent and difficult-to-learn ones. To alleviate that, wepropose an Information Entropy Loss (InfoEntropy Loss) function. Duringtraining, it can dynamically assess the learning difficulty of a to-be-learnedtoken, according to the information entropy of the corresponding predictedprobability distribution over the vocabulary. Then it scales the training lossadaptively, trying to lead the model to focus more on the difficult-to-learntokens. On the Pile dataset, we train generative language models at differentscales of 468M, 1.2B, and 6.7B parameters. Experiments reveal that modelsincorporating the proposed InfoEntropy Loss can gain consistent performanceimprovement on downstream benchmarks.</description><author>Zhenpeng Su, Xing Wu, Xue Bai, Zijia Lin, Hui Chen, Guiguang Ding, Wei Zhou, Songlin Hu</author><pubDate>Fri, 10 Nov 2023 09:35:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19531v3</guid></item><item><title>Learning-Augmented Scheduling for Solar-Powered Electric Vehicle Charging</title><link>http://arxiv.org/abs/2311.05941v1</link><description>We tackle the complex challenge of scheduling the charging of electricvehicles (EVs) equipped with solar panels and batteries, particularly underout-of-distribution (OOD) conditions. Traditional scheduling approaches, suchas reinforcement learning (RL) and model predictive control (MPC), often failto provide satisfactory results when faced with OOD data, struggling to balancerobustness (worst-case performance) and consistency (near-optimal averageperformance). To address this gap, we introduce a novel learning-augmentedpolicy. This policy employs a dynamic robustness budget, which is adapted inreal-time based on the reinforcement learning policy's performance.Specifically, it leverages the temporal difference (TD) error, a measure of thelearning policy's prediction accuracy, to assess the trustworthiness of themachine-learned policy. This method allows for a more effective balance betweenconsistency and robustness in EV charging schedules, significantly enhancingadaptability and efficiency in real-world, unpredictable environments. Ourresults demonstrate that this approach markedly improves schedulingeffectiveness and reliability, particularly in OOD contexts, paving the way formore resilient and adaptive EV charging systems.</description><author>Tongxin Li</author><pubDate>Fri, 10 Nov 2023 08:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05941v1</guid></item><item><title>SCAAT: Improving Neural Network Interpretability via Saliency Constrained Adaptive Adversarial Training</title><link>http://arxiv.org/abs/2311.05143v2</link><description>Deep Neural Networks (DNNs) are expected to provide explanation for users tounderstand their black-box predictions. Saliency map is a common form ofexplanation illustrating the heatmap of feature attributions, but it suffersfrom noise in distinguishing important features. In this paper, we propose amodel-agnostic learning method called Saliency Constrained Adaptive AdversarialTraining (SCAAT) to improve the quality of such DNN interpretability. Byconstructing adversarial samples under the guidance of saliency map, SCAATeffectively eliminates most noise and makes saliency maps sparser and morefaithful without any modification to the model architecture. We apply SCAAT tomultiple DNNs and evaluate the quality of the generated saliency maps onvarious natural and pathological image datasets. Evaluations on differentdomains and metrics show that SCAAT significantly improves the interpretabilityof DNNs by providing more faithful saliency maps without sacrificing theirpredictive power.</description><author>Rui Xu, Wenkang Qin, Peixiang Huang, Hao Wang, Lin Luo</author><pubDate>Fri, 10 Nov 2023 08:53:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05143v2</guid></item><item><title>Genetic Algorithm enhanced by Deep Reinforcement Learning in parent selection mechanism and mutation : Minimizing makespan in permutation flow shop scheduling problems</title><link>http://arxiv.org/abs/2311.05937v1</link><description>This paper introduces a reinforcement learning (RL) approach to address thechallenges associated with configuring and optimizing genetic algorithms (GAs)for solving difficult combinatorial or non-linear problems. The proposed RL+GAmethod was specifically tested on the flow shop scheduling problem (FSP). Thehybrid algorithm incorporates neural networks (NN) and uses the off-policymethod Q-learning or the on-policy method Sarsa(0) to control two key geneticalgorithm (GA) operators: parent selection mechanism and mutation. At eachgeneration, the RL agent's action is determining the selection method, theprobability of the parent selection and the probability of the offspringmutation. This allows the RL agent to dynamically adjust the selection andmutation based on its learned policy. The results of the study highlight theeffectiveness of the RL+GA approach in improving the performance of theprimitive GA. They also demonstrate its ability to learn and adapt frompopulation diversity and solution improvements over time. This adaptabilityleads to improved scheduling solutions compared to static parameterconfigurations while maintaining population diversity throughout theevolutionary process.</description><author>Maissa Irmouli, Nourelhouda Benazzoug, Alaa Dania Adimi, Fatma Zohra Rezkellah, Imane Hamzaoui, Thanina Hamitouche</author><pubDate>Fri, 10 Nov 2023 08:51:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05937v1</guid></item><item><title>Neuro-Inspired Hierarchical Multimodal Learning</title><link>http://arxiv.org/abs/2309.15877v2</link><description>Integrating and processing information from various sources or modalities arecritical for obtaining a comprehensive and accurate perception of the realworld. Drawing inspiration from neuroscience, we develop theInformation-Theoretic Hierarchical Perception (ITHP) model, which utilizes theconcept of information bottleneck. Distinct from most traditional fusion modelsthat aim to incorporate all modalities as input, our model designates the primemodality as input, while the remaining modalities act as detectors in theinformation pathway. Our proposed perception model focuses on constructing aneffective and compact information flow by achieving a balance between theminimization of mutual information between the latent state and the input modalstate, and the maximization of mutual information between the latent states andthe remaining modal states. This approach leads to compact latent staterepresentations that retain relevant information while minimizing redundancy,thereby substantially enhancing the performance of downstream tasks.Experimental evaluations on both the MUStARD and CMU-MOSI datasets demonstratethat our model consistently distills crucial information in multimodal learningscenarios, outperforming state-of-the-art benchmarks.</description><author>Xiongye Xiao, Gengshuo Liu, Gaurav Gupta, Defu Cao, Shixuan Li, Yaxing Li, Tianqing Fang, Mingxi Cheng, Paul Bogdan</author><pubDate>Fri, 10 Nov 2023 08:51:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15877v2</guid></item><item><title>Aggregation Weighting of Federated Learning via Generalization Bound Estimation</title><link>http://arxiv.org/abs/2311.05936v1</link><description>Federated Learning (FL) typically aggregates client model parameters using aweighting approach determined by sample proportions. However, this naiveweighting method may lead to unfairness and degradation in model performancedue to statistical heterogeneity and the inclusion of noisy data among clients.Theoretically, distributional robustness analysis has shown that thegeneralization performance of a learning model with respect to any shifteddistribution is bounded. This motivates us to reconsider the weighting approachin federated learning. In this paper, we replace the aforementioned weightingmethod with a new strategy that considers the generalization bounds of eachlocal model. Specifically, we estimate the upper and lower bounds of thesecond-order origin moment of the shifted distribution for the current localmodel, and then use these bounds disagreements as the aggregation proportionsfor weightings in each communication round. Experiments demonstrate that theproposed weighting strategy significantly improves the performance of severalrepresentative FL algorithms on benchmark datasets.</description><author>Mingwei Xu, Xiaofeng Cao, Ivor W. Tsang, James T. Kwok</author><pubDate>Fri, 10 Nov 2023 08:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05936v1</guid></item><item><title>NExT-Chat: An LMM for Chat, Detection and Segmentation</title><link>http://arxiv.org/abs/2311.04498v2</link><description>The development of large language models (LLMs) has greatly advanced thefield of multimodal understanding, leading to the emergence of large multimodalmodels (LMMs). In order to enhance the level of visual comprehension, recentstudies have equipped LMMs with region-level understanding capabilities byrepresenting object bounding box coordinates as a series of text sequences(pixel2seq). In this paper, we introduce a novel paradigm for object locationmodeling called pixel2emb method, where we ask the LMM to output the locationembeddings and then decoded by different decoders. This paradigm allows fordifferent location formats (such as bounding boxes and masks) to be used inmultimodal conversations Furthermore, this kind of embedding based locationmodeling enables the utilization of existing practices in localization tasks,such as detection and segmentation. In scenarios with limited resources, ourpixel2emb demonstrates superior performance compared to existingstate-of-the-art (SOTA) approaches in both the location input and output tasksunder fair comparison. Leveraging the proposed pixel2emb method, we train anLMM named NExT-Chat and demonstrate its capability of handling multiple taskslike visual grounding, region caption, and grounded reasoning.</description><author>Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji, Tat-Seng Chua</author><pubDate>Fri, 10 Nov 2023 08:46:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04498v2</guid></item><item><title>Long-term Time Series Forecasting based on Decomposition and Neural Ordinary Differential Equations</title><link>http://arxiv.org/abs/2311.04522v2</link><description>Long-term time series forecasting (LTSF) is a challenging task that has beeninvestigated in various domains such as finance investment, health care,traffic, and weather forecasting. In recent years, Linear-based LTSF modelsshowed better performance, pointing out the problem of Transformer-basedapproaches causing temporal information loss. However, Linear-based approachhas also limitations that the model is too simple to comprehensively exploitthe characteristics of the dataset. To solve these limitations, we proposeLTSF-DNODE, which applies a model based on linear ordinary differentialequations (ODEs) and a time series decomposition method according to datastatistical characteristics. We show that LTSF-DNODE outperforms the baselineson various real-world datasets. In addition, for each dataset, we explore theimpacts of regularization in the neural ordinary differential equation (NODE)framework.</description><author>Seonkyu Lim, Jaehyeon Park, Seojin Kim, Hyowon Wi, Haksoo Lim, Jinsung Jeon, Jeongwhan Choi, Noseong Park</author><pubDate>Fri, 10 Nov 2023 08:45:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04522v2</guid></item><item><title>AutoGraph: Predicting Lane Graphs from Traffic Observations</title><link>http://arxiv.org/abs/2306.15410v3</link><description>Lane graph estimation is a long-standing problem in the context of autonomousdriving. Previous works aimed at solving this problem by relying onlarge-scale, hand-annotated lane graphs, introducing a data bottleneck fortraining models to solve this task. To overcome this limitation, we propose touse the motion patterns of traffic participants as lane graph annotations. Inour AutoGraph approach, we employ a pre-trained object tracker to collect thetracklets of traffic participants such as vehicles and trucks. Based on thelocation of these tracklets, we predict the successor lane graph from aninitial position using overhead RGB images only, not requiring any humansupervision. In a subsequent stage, we show how the individual successorpredictions can be aggregated into a consistent lane graph. We demonstrate theefficacy of our approach on the UrbanLaneGraph dataset and perform extensivequantitative and qualitative evaluations, indicating that AutoGraph is on parwith models trained on hand-annotated graph data. Model and dataset will bemade available at redacted-for-review.</description><author>Jannik Z√ºrn, Ingmar Posner, Wolfram Burgard</author><pubDate>Fri, 10 Nov 2023 08:44:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15410v3</guid></item><item><title>Attention-based Multi-task Learning for Base Editor Outcome Prediction</title><link>http://arxiv.org/abs/2310.02919v2</link><description>Human genetic diseases often arise from point mutations, emphasizing thecritical need for precise genome editing techniques. Among these, base editingstands out as it allows targeted alterations at the single nucleotide level.However, its clinical application is hindered by low editing efficiency andunintended mutations, necessitating extensive trial-and-error experimentationin the laboratory. To speed up this process, we present an attention-basedtwo-stage machine learning model that learns to predict the likelihood of allpossible editing outcomes for a given genomic target sequence. We furtherpropose a multi-task learning schema to jointly learn multiple base editors(i.e. variants) at once. Our model's predictions consistently demonstrated astrong correlation with the actual experimental results on multiple datasetsand base editor variants. These results provide further validation for themodels' capacity to enhance and accelerate the process of refining base editingdesigns.</description><author>Amina Mollaysa, Ahmed Allam, Michael Krauthammer</author><pubDate>Fri, 10 Nov 2023 08:38:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02919v2</guid></item><item><title>Anytime-Valid Confidence Sequences for Consistent Uncertainty Estimation in Early-Exit Neural Networks</title><link>http://arxiv.org/abs/2311.05931v1</link><description>Early-exit neural networks (EENNs) facilitate adaptive inference by producingpredictions at multiple stages of the forward pass. In safety-criticalapplications, these predictions are only meaningful when complemented withreliable uncertainty estimates. Yet, due to their sequential structure, anEENN's uncertainty estimates should also be consistent: labels that are deemedimprobable at one exit should not reappear within the confidence interval / setof later exits. We show that standard uncertainty quantification techniques,like Bayesian methods or conformal prediction, can lead to inconsistency acrossexits. We address this problem by applying anytime-valid confidence sequences(AVCSs) to the exits of EENNs. By design, AVCSs maintain consistency acrossexits. We examine the theoretical and practical challenges of applying AVCSs toEENNs and empirically validate our approach on both regression andclassification tasks.</description><author>Metod Jazbec, Patrick Forr√©, Stephan Mandt, Dan Zhang, Eric Nalisnick</author><pubDate>Fri, 10 Nov 2023 08:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05931v1</guid></item><item><title>TRansPose: Large-Scale Multispectral Dataset for Transparent Object</title><link>http://arxiv.org/abs/2307.05016v3</link><description>Transparent objects are encountered frequently in our daily lives, yetrecognizing them poses challenges for conventional vision sensors due to theirunique material properties, not being well perceived from RGB or depth cameras.Overcoming this limitation, thermal infrared cameras have emerged as asolution, offering improved visibility and shape information for transparentobjects. In this paper, we present TRansPose, the first large-scalemultispectral dataset that combines stereo RGB-D, thermal infrared (TIR)images, and object poses to promote transparent object research. The datasetincludes 99 transparent objects, encompassing 43 household items, 27 recyclabletrashes, 29 chemical laboratory equivalents, and 12 non-transparent objects. Itcomprises a vast collection of 333,819 images and 4,000,056 annotations,providing instance-level segmentation masks, ground-truth poses, and completeddepth information. The data was acquired using a FLIR A65 thermal infrared(TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Pandarobot manipulator. Spanning 87 sequences, TRansPose covers various challengingreal-life scenarios, including objects filled with water, diverse lightingconditions, heavy clutter, non-transparent or translucent containers, objectsin plastic bags, and multi-stacked objects. TRansPose dataset can be accessedfrom the following link: https://sites.google.com/view/transpose-dataset</description><author>Jeongyun Kim, Myung-Hwan Jeon, Sangwoo Jung, Wooseong Yang, Minwoo Jung, Jaeho Shin, Ayoung Kim</author><pubDate>Fri, 10 Nov 2023 08:33:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05016v3</guid></item><item><title>Efficient Segmentation with Texture in Ore Images Based on Box-supervised Approach</title><link>http://arxiv.org/abs/2311.05929v1</link><description>Image segmentation methods have been utilized to determine the particle sizedistribution of crushed ores. Due to the complex working environment,high-powered computing equipment is difficult to deploy. At the same time, theore distribution is stacked, and it is difficult to identify the completefeatures. To address this issue, an effective box-supervised technique withtexture features is provided for ore image segmentation that can identifycomplete and independent ores. Firstly, a ghost feature pyramid network(Ghost-FPN) is proposed to process the features obtained from the backbone toreduce redundant semantic information and computation generated by complexnetworks. Then, an optimized detection head is proposed to obtain the featureto maintain accuracy. Finally, Lab color space (Lab) and local binary patterns(LBP) texture features are combined to form a fusion feature similarity-basedloss function to improve accuracy while incurring no loss. Experiments on MSCOCO have shown that the proposed fusion features are also worth studying onother types of datasets. Extensive experimental results demonstrate theeffectiveness of the proposed method, which achieves over 50 frames per secondwith a small model size of 21.6 MB. Meanwhile, the method maintains a highlevel of accuracy compared with the state-of-the-art approaches on ore imagedataset. The source code is available at\url{https://github.com/MVME-HBUT/OREINST}.</description><author>Guodong Sun, Delong Huang, Yuting Peng, Le Cheng, Bo Wu, Yang Zhang</author><pubDate>Fri, 10 Nov 2023 08:28:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05929v1</guid></item><item><title>The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models</title><link>http://arxiv.org/abs/2311.05928v1</link><description>In this study, we present an investigation into the anisotropy dynamics andintrinsic dimension of embeddings in transformer architectures, focusing on thedichotomy between encoders and decoders. Our findings reveal that theanisotropy profile in transformer decoders exhibits a distinct bell-shapedcurve, with the highest anisotropy concentrations in the middle layers. Thispattern diverges from the more uniformly distributed anisotropy observed inencoders. In addition, we found that the intrinsic dimension of embeddingsincreases in the initial phases of training, indicating an expansion intohigher-dimensional space. Which is then followed by a compression phase towardsthe end of training with dimensionality decrease, suggesting a refinement intomore compact representations. Our results provide fresh insights to theunderstanding of encoders and decoders embedding properties.</description><author>Anton Razzhigaev, Matvey Mikhalchuk, Elizaveta Goncharova, Ivan Oseledets, Denis Dimitrov, Andrey Kuznetsov</author><pubDate>Fri, 10 Nov 2023 08:25:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05928v1</guid></item><item><title>Automated Sperm Assessment Framework and Neural Network Specialized for Sperm Video Recognition</title><link>http://arxiv.org/abs/2311.05927v1</link><description>Infertility is a global health problem, and an increasing number of couplesare seeking medical assistance to achieve reproduction, at least half of whichare caused by men. The success rate of assisted reproductive technologiesdepends on sperm assessment, in which experts determine whether sperm can beused for reproduction based on morphology and motility of sperm. Previous spermassessment studies with deep learning have used datasets comprising images thatinclude only sperm heads, which cannot consider motility and other morphologiesof sperm. Furthermore, the labels of the dataset are one-hot, which providesinsufficient support for experts, because assessment results are inconsistentbetween experts, and they have no absolute answer. Therefore, we constructedthe video dataset for sperm assessment whose videos include sperm head as wellas neck and tail, and its labels were annotated with soft-label. Furthermore,we proposed the sperm assessment framework and the neural network, RoSTFine,for sperm video recognition. Experimental results showed that RoSTFine couldimprove the sperm assessment performances compared to existing videorecognition models and focus strongly on important sperm parts (i.e., head andneck).</description><author>Takuro Fujii, Hayato Nakagawa, Teppei Takeshima, Yasushi Yumura, Tomoki Hamagami</author><pubDate>Fri, 10 Nov 2023 08:23:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05927v1</guid></item><item><title>Federated Learning with Manifold Regularization and Normalized Update Reaggregation</title><link>http://arxiv.org/abs/2311.05924v1</link><description>Federated Learning (FL) is an emerging collaborative machine learningframework where multiple clients train the global model without sharing theirown datasets. In FL, the model inconsistency caused by the local dataheterogeneity across clients results in the near-orthogonality of clientupdates, which leads to the global update norm reduction and slows down theconvergence. Most previous works focus on eliminating the difference ofparameters (or gradients) between the local and global models, which may failto reflect the model inconsistency due to the complex structure of the machinelearning model and the Euclidean space's limitation in meaningful geometricrepresentations. In this paper, we propose FedMRUR by adopting the manifoldmodel fusion scheme and a new global optimizer to alleviate the negativeimpacts. Concretely, FedMRUR adopts a hyperbolic graph manifold regularizerenforcing the representations of the data in the local and global models areclose to each other in a low-dimensional subspace. Because the machine learningmodel has the graph structure, the distance in hyperbolic space can reflect themodel bias better than the Euclidean distance. In this way, FedMRUR exploitsthe manifold structures of the representations to significantly reduce themodel inconsistency. FedMRUR also aggregates the client updates norms as theglobal update norm, which can appropriately enlarge each client's contributionto the global update, thereby mitigating the norm reduction introduced by thenear-orthogonality of client updates. Furthermore, we theoretically prove thatour algorithm can achieve a linear speedup property for non-convex settingunder partial client participation.Experiments demonstrate that FedMRUR canachieve a new state-of-the-art (SOTA) accuracy with less communication.</description><author>Xuming An, Li Shen, Han Hu, Yong Luo</author><pubDate>Fri, 10 Nov 2023 08:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05924v1</guid></item><item><title>Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction</title><link>http://arxiv.org/abs/2311.05922v1</link><description>Few-shot relation extraction involves identifying the type of relationshipbetween two specific entities within a text, using a limited number ofannotated samples. A variety of solutions to this problem have emerged byapplying meta-learning and neural graph techniques which typically necessitatea training process for adaptation. Recently, the strategy of in-contextlearning has been demonstrating notable results without the need of training.Few studies have already utilized in-context learning for zero-shot informationextraction. Unfortunately, the evidence for inference is either not consideredor implicitly modeled during the construction of chain-of-thought prompts. Inthis paper, we propose a novel approach for few-shot relation extraction usinglarge language models, named CoT-ER, chain-of-thought with explicit evidencereasoning. In particular, CoT-ER first induces large language models togenerate evidences using task-specific and concept-level knowledge. Then theseevidences are explicitly incorporated into chain-of-thought prompting forrelation extraction. Experimental results demonstrate that our CoT-ER approach(with 0% training data) achieves competitive performance compared to thefully-supervised (with 100% training data) state-of-the-art approach on theFewRel1.0 and FewRel2.0 datasets.</description><author>Xilai Ma, Jing Li, Min Zhang</author><pubDate>Fri, 10 Nov 2023 08:12:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05922v1</guid></item><item><title>Inter-object Discriminative Graph Modeling for Indoor Scene Recognition</title><link>http://arxiv.org/abs/2311.05919v1</link><description>Variable scene layouts and coexisting objects across scenes make indoor scenerecognition still a challenging task. Leveraging object information withinscenes to enhance the distinguishability of feature representations has emergedas a key approach in this domain. Currently, most object-assisted methods use aseparate branch to process object information, combining object and scenefeatures heuristically. However, few of them pay attention to interpretablyhandle the hidden discriminative knowledge within object information. In thispaper, we propose to leverage discriminative object knowledge to enhance scenefeature representations. Initially, we capture the object-scene discriminativerelationships from a probabilistic perspective, which are transformed into anInter-Object Discriminative Prototype (IODP). Given the abundant priorknowledge from IODP, we subsequently construct a Discriminative Graph Network(DGN), in which pixel-level scene features are defined as nodes and thediscriminative relationships between node features are encoded as edges. DGNaims to incorporate inter-object discriminative knowledge into the imagerepresentation through graph convolution. With the proposed IODP and DGN, weobtain state-of-the-art results on several widely used scene datasets,demonstrating the effectiveness of the proposed approach.</description><author>Chuanxin Song, Hanbo Wu, Xin Ma, Yibin Li</author><pubDate>Fri, 10 Nov 2023 08:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05919v1</guid></item><item><title>Fake Alignment: Are LLMs Really Aligned Well?</title><link>http://arxiv.org/abs/2311.05915v1</link><description>The growing awareness of safety concerns in large language models (LLMs) hassparked considerable interest in the evaluation of safety within currentresearch endeavors. This study investigates an interesting issue pertaining tothe evaluation of LLMs, namely the substantial discrepancy in performancebetween multiple-choice questions and open-ended questions. Inspired byresearch on jailbreak attack patterns, we argue this is caused by mismatchedgeneralization. That is, the LLM does not have a comprehensive understanding ofthe complex concept of safety. Instead, it only remembers what to answer foropen-ended safety questions, which makes it unable to solve other forms ofsafety tests. We refer to this phenomenon as fake alignment and construct acomparative benchmark to empirically verify its existence in LLMs. Such fakealignment renders previous evaluation protocols unreliable. To address this, weintroduce the FAEF framework and two novel metrics\textemdash Consistency Score(CS) and Consistent Safety Score (CSS), which jointly assess two complementaryforms of evaluation to quantify fake alignment and obtain corrected performanceestimates. Applying FAEF to 14 widely-used LLMs reveals several models withpurported safety are poorly aligned in practice. Our work highlights potentiallimitations in prevailing alignment methodologies.</description><author>Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang, Wenwei Zhang, Xingjun Ma, Yingchun Wang</author><pubDate>Fri, 10 Nov 2023 08:01:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.05915v1</guid></item><item><title>A Semi-Bayesian Nonparametric Estimator of the Maximum Mean Discrepancy Measure: Applications in Goodness-of-Fit Testing and Generative Adversarial Networks</title><link>http://arxiv.org/abs/2303.02637v2</link><description>A classic inferential statistical problem is the goodness-of-fit (GOF) test.Such a test can be challenging when the hypothesized parametric model has anintractable likelihood and its distributional form is not available. Bayesianmethods for GOF can be appealing due to their ability to incorporate expertknowledge through prior distributions. However, standard Bayesian methods for this test often require strongdistributional assumptions on the data and their relevant parameters. Toaddress this issue, we propose a semi-Bayesian nonparametric (semi-BNP)procedure in the context of the maximum mean discrepancy (MMD) measure that canbe applied to the GOF test. Our method introduces a novel Bayesian estimatorfor the MMD, enabling the development of a measure-based hypothesis test forintractable models. Through extensive experiments, we demonstrate that ourproposed test outperforms frequentist MMD-based methods by achieving a lowerfalse rejection and acceptance rate of the null hypothesis. Furthermore, weshowcase the versatility of our approach by embedding the proposed estimatorwithin a generative adversarial network (GAN) framework. It facilitates arobust BNP learning approach as another significant application of our method.With our BNP procedure, this new GAN approach can enhance sample diversity andimprove inferential accuracy compared to traditional techniques.</description><author>Forough Fazeli-Asl, Michael Minyi Zhang, Lizhen Lin</author><pubDate>Fri, 10 Nov 2023 07:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02637v2</guid></item><item><title>Parameterized Convex Minorant for Objective Function Approximation in Amortized Optimization</title><link>http://arxiv.org/abs/2310.02519v3</link><description>Parameterized convex minorant (PCM) method is proposed for the approximationof the objective function in amortized optimization. In the proposed method,the objective function approximator is expressed by the sum of a PCM and anonnegative gap function, where the objective function approximator is boundedfrom below by the PCM convex in the optimization variable. The proposedobjective function approximator is a universal approximator for continuousfunctions, and the global minimizer of the PCM attains the global minimum ofthe objective function approximator. Therefore, the global minimizer of theobjective function approximator can be obtained by a single convexoptimization. As a realization of the proposed method, extended parameterizedlog-sum-exp network is proposed by utilizing a parameterized log-sum-expnetwork as the PCM. Numerical simulation is performed for parameterizednon-convex objective function approximation and for learning-based nonlinearmodel predictive control to demonstrate the performance and characteristics ofthe proposed method. The simulation results support that the proposed methodcan be used to learn objective functions and to find a global minimizerreliably and quickly by using convex optimization algorithms.</description><author>Jinrae Kim, Youdan Kim</author><pubDate>Fri, 10 Nov 2023 07:49:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02519v3</guid></item><item><title>On the (In)security of Peer-to-Peer Decentralized Machine Learning</title><link>http://arxiv.org/abs/2205.08443v3</link><description>In this work, we carry out the first, in-depth, privacy analysis ofDecentralized Learning -- a collaborative machine learning framework aimed ataddressing the main limitations of federated learning. We introduce a suite ofnovel attacks for both passive and active decentralized adversaries. Wedemonstrate that, contrary to what is claimed by decentralized learningproposers, decentralized learning does not offer any security advantage overfederated learning. Rather, it increases the attack surface enabling any userin the system to perform privacy attacks such as gradient inversion, and evengain full control over honest users' local model. We also show that, given thestate of the art in protections, privacy-preserving configurations ofdecentralized learning require fully connected networks, losing any practicaladvantage over the federated setup and therefore completely defeating theobjective of the decentralized approach.</description><author>Dario Pasquini, Mathilde Raynal, Carmela Troncoso</author><pubDate>Fri, 10 Nov 2023 07:47:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.08443v3</guid></item></channel></rss>