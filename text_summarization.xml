<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivtext summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 16 Jul 2024 01:19:54 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MixSumm: Topic-based Data Augmentation using LLMs for Low-resource Extractive Text Summarization</title><link>http://arxiv.org/abs/2407.07341v1</link><description>Low-resource extractive text summarization is a vital but heavilyunderexplored area of research. Prior literature either focuses on abstractivetext summarization or prompts a large language model (LLM) like GPT-3 directlyto generate summaries. In this work, we propose MixSumm for low-resourceextractive text summarization. Specifically, MixSumm prompts an open-sourceLLM, LLaMA-3-70b, to generate documents that mix information from multipletopics as opposed to generating documents without mixup, and then trains asummarization model on the generated dataset. We use ROUGE scores and L-Eval, areference-free LLaMA-3-based evaluation method to measure the quality ofgenerated summaries. We conduct extensive experiments on a challenging textsummarization benchmark comprising the TweetSumm, WikiHow, and ArXiv/PubMeddatasets and show that our LLM-based data augmentation framework outperformsrecent prompt-based approaches for low-resource extractive summarization.Additionally, our results also demonstrate effective knowledge distillationfrom LLaMA-3-70b to a small BERT-based extractive summarizer.</description><author>Gaurav Sahu, Issam H. Laradji</author><pubDate>Wed, 10 Jul 2024 03:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07341v1</guid></item><item><title>Tamil Language Computing: the Present and the Future</title><link>http://arxiv.org/abs/2407.08618v1</link><description>This paper delves into the text processing aspects of Language Computing,which enables computers to understand, interpret, and generate human language.Focusing on tasks such as speech recognition, machine translation, sentimentanalysis, text summarization, and language modelling, language computingintegrates disciplines including linguistics, computer science, and cognitivepsychology to create meaningful human-computer interactions. Recentadvancements in deep learning have made computers more accessible and capableof independent learning and adaptation. In examining the landscape of languagecomputing, the paper emphasises foundational work like encoding, where Tamiltransitioned from ASCII to Unicode, enhancing digital communication. Itdiscusses the development of computational resources, including raw data,dictionaries, glossaries, annotated data, and computational grammars, necessaryfor effective language processing. The challenges of linguistic annotation, thecreation of treebanks, and the training of large language models are alsocovered, emphasising the need for high-quality, annotated data and advancedlanguage models. The paper underscores the importance of building practicalapplications for languages like Tamil to address everyday communication needs,highlighting gaps in current technology. It calls for increased researchcollaboration, digitization of historical texts, and fostering digital usage toensure the comprehensive development of Tamil language processing, ultimatelyenhancing global communication and access to digital services.</description><author>Kengatharaiyer Sarveswaran</author><pubDate>Thu, 11 Jul 2024 15:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08618v1</guid></item><item><title>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</title><link>http://arxiv.org/abs/2311.09184v2</link><description>While large language models (LLMs) can already achieve strong performance onstandard generic summarization benchmarks, their performance on more complexsummarization task settings is less studied. Therefore, we benchmark LLMs oninstruction controllable text summarization, where the model input consists ofboth a source article and a natural language requirement for desired summarycharacteristics. To this end, we curate an evaluation-only dataset for thistask setting and conduct human evaluations of five LLM-based systems to assesstheir instruction-following capabilities in controllable summarization. We thenbenchmark LLM-based automatic evaluation for this task with 4 differentevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our studyreveals that instruction controllable text summarization remains a challengingtask for LLMs, since (1) all LLMs evaluated still make factual and other typesof errors in their summaries; (2) no LLM-based evaluation methods can achieve astrong alignment with human annotators when judging the quality of candidatesummaries; (3) different LLMs show large performance gaps in summary generationand evaluation capabilities. We make our collected benchmark InstruSum publiclyavailable to facilitate future research in this direction.</description><author>Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan</author><pubDate>Fri, 12 Jul 2024 17:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09184v2</guid></item></channel></rss>