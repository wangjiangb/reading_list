<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivtext summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 19 Jul 2024 13:00:41 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MixSumm: Topic-based Data Augmentation using LLMs for Low-resource Extractive Text Summarization</title><link>http://arxiv.org/abs/2407.07341v1</link><description>Low-resource extractive text summarization is a vital but heavilyunderexplored area of research. Prior literature either focuses on abstractivetext summarization or prompts a large language model (LLM) like GPT-3 directlyto generate summaries. In this work, we propose MixSumm for low-resourceextractive text summarization. Specifically, MixSumm prompts an open-sourceLLM, LLaMA-3-70b, to generate documents that mix information from multipletopics as opposed to generating documents without mixup, and then trains asummarization model on the generated dataset. We use ROUGE scores and L-Eval, areference-free LLaMA-3-based evaluation method to measure the quality ofgenerated summaries. We conduct extensive experiments on a challenging textsummarization benchmark comprising the TweetSumm, WikiHow, and ArXiv/PubMeddatasets and show that our LLM-based data augmentation framework outperformsrecent prompt-based approaches for low-resource extractive summarization.Additionally, our results also demonstrate effective knowledge distillationfrom LLaMA-3-70b to a small BERT-based extractive summarizer.</description><author>Gaurav Sahu, Issam H. Laradji</author><pubDate>Wed, 10 Jul 2024 03:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07341v1</guid></item><item><title>IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization</title><link>http://arxiv.org/abs/2407.10486v1</link><description>Query-focused summarization (QFS) aims to produce summaries that answerparticular questions of interest, enabling greater user control andpersonalization. With the advent of large language models (LLMs), shows theirimpressive capability of textual understanding through large-scale pretraining,which implies the great potential of extractive snippet generation. In thispaper, we systematically investigated two indispensable characteristics thatthe LLMs-based QFS models should be harnessed, Lengthy Document Summarizationand Efficiently Fine-grained Query-LLM Alignment, respectively.Correspondingly, we propose two modules called Query-aware HyperExpert andQuery-focused Infini-attention to access the aforementioned characteristics.These innovations pave the way for broader application and accessibility in thefield of QFS technology. Extensive experiments conducted on existing QFSbenchmarks indicate the effectiveness and generalizability of the proposedapproach. Our code is publicly available athttps://github.com/DCDmllm/IDEAL_Summary.</description><author>Jie Cao, Dian Jiao, Qiang Yan, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</author><pubDate>Mon, 15 Jul 2024 07:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10486v1</guid></item><item><title>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</title><link>http://arxiv.org/abs/2311.09184v2</link><description>While large language models (LLMs) can already achieve strong performance onstandard generic summarization benchmarks, their performance on more complexsummarization task settings is less studied. Therefore, we benchmark LLMs oninstruction controllable text summarization, where the model input consists ofboth a source article and a natural language requirement for desired summarycharacteristics. To this end, we curate an evaluation-only dataset for thistask setting and conduct human evaluations of five LLM-based systems to assesstheir instruction-following capabilities in controllable summarization. We thenbenchmark LLM-based automatic evaluation for this task with 4 differentevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our studyreveals that instruction controllable text summarization remains a challengingtask for LLMs, since (1) all LLMs evaluated still make factual and other typesof errors in their summaries; (2) no LLM-based evaluation methods can achieve astrong alignment with human annotators when judging the quality of candidatesummaries; (3) different LLMs show large performance gaps in summary generationand evaluation capabilities. We make our collected benchmark InstruSum publiclyavailable to facilitate future research in this direction.</description><author>Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan</author><pubDate>Fri, 12 Jul 2024 17:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09184v2</guid></item><item><title>Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot Whole Slide Image Classification</title><link>http://arxiv.org/abs/2407.10814v1</link><description>Current multi-instance learning algorithms for pathology image analysis oftenrequire a substantial number of Whole Slide Images for effective training butexhibit suboptimal performance in scenarios with limited learning data. Inclinical settings, restricted access to pathology slides is inevitable due topatient privacy concerns and the prevalence of rare or emerging diseases. Theemergence of the Few-shot Weakly Supervised WSI Classification accommodates thesignificant challenge of the limited slide data and sparse slide-level labelsfor diagnosis. Prompt learning based on the pre-trained models (\eg, CLIP)appears to be a promising scheme for this setting; however, current research inthis area is limited, and existing algorithms often focus solely on patch-levelprompts or confine themselves to language prompts. This paper proposes amulti-instance prompt learning framework enhanced with pathology knowledge,\ie, integrating visual and textual prior knowledge into prompts at both patchand slide levels. The training process employs a combination of static andlearnable prompts, effectively guiding the activation of pre-trained models andfurther facilitating the diagnosis of key pathology patterns. LightweightMessenger (self-attention) and Summary (attention-pooling) layers areintroduced to model relationships between patches and slides within the samepatient data. Additionally, alignment-wise contrastive losses ensure thefeature-level alignment between visual and textual learnable prompts for bothpatches and slides. Our method demonstrates superior performance in threechallenging clinical tasks, significantly outperforming comparative few-shotmethods.</description><author>Linhao Qu, Dingkang Yang, Dan Huang, Qinhao Guo, Rongkui Luo, Shaoting Zhang, Xiaosong Wang</author><pubDate>Mon, 15 Jul 2024 15:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10814v1</guid></item><item><title>What's Wrong? Refining Meeting Summaries with LLM Feedback</title><link>http://arxiv.org/abs/2407.11919v1</link><description>Meeting summarization has become a critical task since digital encountershave become a common practice. Large language models (LLMs) show greatpotential in summarization, offering enhanced coherence and contextunderstanding compared to traditional methods. However, they still struggle tomaintain relevance and avoid hallucination. We introduce a multi-LLM correctionapproach for meeting summarization using a two-phase process that mimics thehuman review process: mistake identification and summary refinement. We releaseQMSum Mistake, a dataset of 200 automatically generated meeting summariesannotated by humans on nine error types, including structural, omission, andirrelevance errors. Our experiments show that these errors can be identifiedwith high accuracy by an LLM. We transform identified mistakes into actionablefeedback to improve the quality of a given summary measured by relevance,informativeness, conciseness, and coherence. This post-hoc refinementeffectively improves summary quality by leveraging multiple LLMs to validateoutput quality. Our multi-LLM approach for meeting summarization showspotential for similar complex text generation tasks requiring robustness,action planning, and discussion towards a goal.</description><author>Frederic Kirstein, Terry Ruas, Bela Gipp</author><pubDate>Tue, 16 Jul 2024 17:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11919v1</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v1</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell Espuny</author><pubDate>Tue, 16 Jul 2024 10:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v1</guid></item><item><title>Semantic-Aware Representation of Multi-Modal Data for Data Ingress: A Literature Review</title><link>http://arxiv.org/abs/2407.12438v1</link><description>Machine Learning (ML) is continuously permeating a growing amount ofapplication domains. Generative AI such as Large Language Models (LLMs) alsosees broad adoption to process multi-modal data such as text, images, audio,and video. While the trend is to use ever-larger datasets for training,managing this data efficiently has become a significant practical challenge inthe industry-double as much data is certainly not double as good. Rather theopposite is important since getting an understanding of the inherent qualityand diversity of the underlying data lakes is a growing challenge forapplication-specific ML as well as for fine-tuning foundation models.Furthermore, information retrieval (IR) from expanding data lakes iscomplicated by the temporal dimension inherent in time-series data which mustbe considered to determine its semantic value. This study focuses on thedifferent semantic-aware techniques to extract embeddings from mono-modal,multi-modal, and cross-modal data to enhance IR capabilities in a growing datalake. Articles were collected to summarize information about thestate-of-the-art techniques focusing on applications of embedding for threedifferent categories of data modalities.</description><author>Pierre Lamart, Yinan Yu, Christian Berger</author><pubDate>Wed, 17 Jul 2024 09:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12438v1</guid></item><item><title>Turning Generative Models Degenerate: The Power of Data Poisoning Attacks</title><link>http://arxiv.org/abs/2407.12281v2</link><description>The increasing use of large language models (LLMs) trained by third partiesraises significant security concerns. In particular, malicious actors canintroduce backdoors through poisoning attacks to generate undesirable outputs.While such attacks have been extensively studied in image domains andclassification tasks, they remain underexplored for natural language generation(NLG) tasks. To address this gap, we conduct an investigation of variouspoisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, aParameter Efficient Fine-Tuning (PEFT) method. We assess their effectivenessacross two generative tasks: text summarization and text completion; and wealso introduce new metrics to quantify the success and stealthiness of such NLGpoisoning attacks. Through our experiments, we find that the prefix-tuninghyperparameters and trigger designs are the most crucial factors to influenceattack success and stealthiness. Moreover, we demonstrate that existing populardefenses are ineffective against our poisoning attacks. Our study presents thefirst systematic approach to understanding poisoning attacks targeting NLGtasks during fine-tuning via PEFT across a wide range of triggers and attacksettings. We hope our findings will aid the AI security community in developingeffective defenses against such threats.</description><author>Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Farhan Ahmed, Ling Cai, Nathalie Baracaldo</author><pubDate>Thu, 18 Jul 2024 05:52:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12281v2</guid></item><item><title>On Learning to Summarize with Large Language Models as References</title><link>http://arxiv.org/abs/2305.14239v3</link><description>Recent studies have found that summaries generated by large language models(LLMs) are favored by human annotators over the original reference summaries incommonly used summarization datasets. Therefore, we study an LLM-as-referencelearning setting for smaller text summarization models to investigate whethertheir performance can be substantially improved. To this end, we use LLMs asboth oracle summary generators for standard supervised fine-tuning and oraclesummary evaluators for efficient contrastive learning that leverages the LLMs'supervision signals. We conduct comprehensive experiments with source newsarticles and find that (1) summarization models trained under theLLM-as-reference setting achieve significant performance improvement in bothLLM and human evaluations; (2) contrastive learning outperforms standardsupervised fine-tuning under both low and high resource settings. Ourexperimental results also enable a meta-analysis of LLMs' summary evaluationcapacities under a challenging setting, showing that LLMs are not well-alignedwith human evaluators. Particularly, our expert human evaluation revealsremaining nuanced performance gaps between LLMs and our fine-tuned models,which LLMs fail to capture. Thus, we call for further studies into both thepotential and challenges of using LLMs in summarization model development.</description><author>Yixin Liu, Kejian Shi, Katherine S He, Longtian Ye, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, Arman Cohan</author><pubDate>Thu, 18 Jul 2024 17:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14239v3</guid></item><item><title>PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like (PL) Tasks</title><link>http://arxiv.org/abs/2407.13597v1</link><description>Text summarization is a well-studied problem that deals with derivinginsights from unstructured text consumed by humans, and it has found extensivebusiness applications. However, many real-life tasks involve generating aseries of actions to achieve specific goals, such as workflows, recipes,dialogs, and travel plans. We refer to them as planning-like (PL) tasks notingthat the main commonality they share is control flow information. which may bepartially specified. Their structure presents an opportunity to create morepractical summaries to help users make quick decisions. We investigate thisobservation by introducing a novel plan summarization problem, presenting adataset, and providing a baseline method for generating PL summaries. Usingquantitative metrics and qualitative user studies to establish baselines, weevaluate the plan summaries from our method and large language models. Webelieve the novel problem and dataset can reinvigorate research insummarization, which some consider as a solved problem.</description><author>Vishal Pallagani, Biplav Srivastava, Nitin Gupta</author><pubDate>Thu, 18 Jul 2024 15:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13597v1</guid></item><item><title>Survey in Characterization of Semantic Change</title><link>http://arxiv.org/abs/2402.19088v3</link><description>Live languages continuously evolve to integrate the cultural change of humansocieties. This evolution manifests through neologisms (new words) or\textbf{semantic changes} of words (new meaning to existing words).Understanding the meaning of words is vital for interpreting texts coming fromdifferent cultures (regionalism or slang), domains (e.g., technical terms), orperiods. In computer science, these words are relevant to computationallinguistics algorithms such as translation, information retrieval, questionanswering, etc. Semantic changes can potentially impact the quality of theoutcomes of these algorithms. Therefore, it is important to understand andcharacterize these changes formally. The study of this impact is a recentproblem that has attracted the attention of the computational linguisticscommunity. Several approaches propose methods to detect semantic changes withgood precision, but more effort is needed to characterize how the meaning ofwords changes and to reason about how to reduce the impact of semantic change.This survey provides an understandable overview of existing approaches to the\textit{characterization of semantic changes} and also formally defines threeclasses of characterizations: if the meaning of a word becomes more general ornarrow (change in dimension) if the word is used in a more pejorative orpositive/ameliorated sense (change in orientation), and if there is a trend touse the word in a, for instance, metaphoric or metonymic context (change inrelation). We summarized the main aspects of the selected publications in atable and discussed the needs and trends in the research activities on semanticchange characterization.</description><author>Jader Martins Camboim de Sá, Marcos Da Silveira, Cédric Pruski</author><pubDate>Thu, 18 Jul 2024 12:28:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19088v3</guid></item><item><title>Multi-sentence Video Grounding for Long Video Generation</title><link>http://arxiv.org/abs/2407.13219v1</link><description>Video generation has witnessed great success recently, but their applicationin generating long videos still remains challenging due to the difficulty inmaintaining the temporal consistency of generated videos and the high memorycost during generation. To tackle the problems, in this paper, we propose abrave and new idea of Multi-sentence Video Grounding for Long Video Generation,connecting the massive video moment retrieval to the video generation task forthe first time, providing a new paradigm for long video generation. The methodof our work can be summarized as three steps: (i) We design sequential scenetext prompts as the queries for video grounding, utilizing the massive videomoment retrieval to search for video moment segments that meet the textrequirements in the video database. (ii) Based on the source frames ofretrieved video moment segments, we adopt video editing methods to create newvideo content while preserving the temporal consistency of the retrieved video.Since the editing can be conducted segment by segment, and even frame by frame,it largely reduces the memory cost. (iii) We also attempt video morphing andpersonalized generation methods to improve the subject consistency of longvideo generation, providing ablation experimental results for the subtasks oflong video generation. Our approach seamlessly extends the development inimage/video editing, video morphing and personalized generation, and videogrounding to the long video generation, offering effective solutions forgenerating long videos at low memory cost.</description><author>Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Wenwu Zhu</author><pubDate>Thu, 18 Jul 2024 07:05:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13219v1</guid></item><item><title>Tamil Language Computing: the Present and the Future</title><link>http://arxiv.org/abs/2407.08618v1</link><description>This paper delves into the text processing aspects of Language Computing,which enables computers to understand, interpret, and generate human language.Focusing on tasks such as speech recognition, machine translation, sentimentanalysis, text summarization, and language modelling, language computingintegrates disciplines including linguistics, computer science, and cognitivepsychology to create meaningful human-computer interactions. Recentadvancements in deep learning have made computers more accessible and capableof independent learning and adaptation. In examining the landscape of languagecomputing, the paper emphasises foundational work like encoding, where Tamiltransitioned from ASCII to Unicode, enhancing digital communication. Itdiscusses the development of computational resources, including raw data,dictionaries, glossaries, annotated data, and computational grammars, necessaryfor effective language processing. The challenges of linguistic annotation, thecreation of treebanks, and the training of large language models are alsocovered, emphasising the need for high-quality, annotated data and advancedlanguage models. The paper underscores the importance of building practicalapplications for languages like Tamil to address everyday communication needs,highlighting gaps in current technology. It calls for increased researchcollaboration, digitization of historical texts, and fostering digital usage toensure the comprehensive development of Tamil language processing, ultimatelyenhancing global communication and access to digital services.</description><author>Kengatharaiyer Sarveswaran</author><pubDate>Thu, 11 Jul 2024 15:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08618v1</guid></item></channel></rss>