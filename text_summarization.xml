<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivtext summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 26 Jul 2024 13:00:19 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching</title><link>http://arxiv.org/abs/2407.17349v1</link><description>With the introduction of large language models (LLMs), automatic mathreasoning has seen tremendous success. However, current methods primarily focuson providing solutions or using techniques like Chain-of-Thought to enhanceproblem-solving accuracy. In this paper, we focus on improving the capabilityof mathematics teaching via a Socratic teaching-based LLM(\texttt{SocraticLLM}), which guides learners toward profound thinking withclarity and self-discovery via conversation. We collect and release ahigh-quality mathematical teaching dataset, named \texttt{SocraticMATH}, whichprovides Socratic-style conversations of problems with extra knowledge. Also,we propose a knowledge-enhanced LLM as a strong baseline to generate reliableresponses with review, guidance/heuristic, rectification, and summarization.Experimental results show the great advantages of \texttt{SocraticLLM} bycomparing it with several strong generative models. The codes and datasets areavailable on \url{https://github.com/ECNU-ICALK/SocraticMath}.</description><author>Yuyang Ding, Hanglei Hu, Jie Zhou, Qin Chen, Bo Jiang, Liang He</author><pubDate>Wed, 24 Jul 2024 15:18:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17349v1</guid></item><item><title>MixSumm: Topic-based Data Augmentation using LLMs for Low-resource Extractive Text Summarization</title><link>http://arxiv.org/abs/2407.07341v1</link><description>Low-resource extractive text summarization is a vital but heavilyunderexplored area of research. Prior literature either focuses on abstractivetext summarization or prompts a large language model (LLM) like GPT-3 directlyto generate summaries. In this work, we propose MixSumm for low-resourceextractive text summarization. Specifically, MixSumm prompts an open-sourceLLM, LLaMA-3-70b, to generate documents that mix information from multipletopics as opposed to generating documents without mixup, and then trains asummarization model on the generated dataset. We use ROUGE scores and L-Eval, areference-free LLaMA-3-based evaluation method to measure the quality ofgenerated summaries. We conduct extensive experiments on a challenging textsummarization benchmark comprising the TweetSumm, WikiHow, and ArXiv/PubMeddatasets and show that our LLM-based data augmentation framework outperformsrecent prompt-based approaches for low-resource extractive summarization.Additionally, our results also demonstrate effective knowledge distillationfrom LLaMA-3-70b to a small BERT-based extractive summarizer.</description><author>Gaurav Sahu, Issam H. Laradji</author><pubDate>Wed, 10 Jul 2024 03:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07341v1</guid></item><item><title>IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization</title><link>http://arxiv.org/abs/2407.10486v1</link><description>Query-focused summarization (QFS) aims to produce summaries that answerparticular questions of interest, enabling greater user control andpersonalization. With the advent of large language models (LLMs), shows theirimpressive capability of textual understanding through large-scale pretraining,which implies the great potential of extractive snippet generation. In thispaper, we systematically investigated two indispensable characteristics thatthe LLMs-based QFS models should be harnessed, Lengthy Document Summarizationand Efficiently Fine-grained Query-LLM Alignment, respectively.Correspondingly, we propose two modules called Query-aware HyperExpert andQuery-focused Infini-attention to access the aforementioned characteristics.These innovations pave the way for broader application and accessibility in thefield of QFS technology. Extensive experiments conducted on existing QFSbenchmarks indicate the effectiveness and generalizability of the proposedapproach. Our code is publicly available athttps://github.com/DCDmllm/IDEAL_Summary.</description><author>Jie Cao, Dian Jiao, Qiang Yan, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</author><pubDate>Mon, 15 Jul 2024 07:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10486v1</guid></item><item><title>Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot Whole Slide Image Classification</title><link>http://arxiv.org/abs/2407.10814v1</link><description>Current multi-instance learning algorithms for pathology image analysis oftenrequire a substantial number of Whole Slide Images for effective training butexhibit suboptimal performance in scenarios with limited learning data. Inclinical settings, restricted access to pathology slides is inevitable due topatient privacy concerns and the prevalence of rare or emerging diseases. Theemergence of the Few-shot Weakly Supervised WSI Classification accommodates thesignificant challenge of the limited slide data and sparse slide-level labelsfor diagnosis. Prompt learning based on the pre-trained models (\eg, CLIP)appears to be a promising scheme for this setting; however, current research inthis area is limited, and existing algorithms often focus solely on patch-levelprompts or confine themselves to language prompts. This paper proposes amulti-instance prompt learning framework enhanced with pathology knowledge,\ie, integrating visual and textual prior knowledge into prompts at both patchand slide levels. The training process employs a combination of static andlearnable prompts, effectively guiding the activation of pre-trained models andfurther facilitating the diagnosis of key pathology patterns. LightweightMessenger (self-attention) and Summary (attention-pooling) layers areintroduced to model relationships between patches and slides within the samepatient data. Additionally, alignment-wise contrastive losses ensure thefeature-level alignment between visual and textual learnable prompts for bothpatches and slides. Our method demonstrates superior performance in threechallenging clinical tasks, significantly outperforming comparative few-shotmethods.</description><author>Linhao Qu, Dingkang Yang, Dan Huang, Qinhao Guo, Rongkui Luo, Shaoting Zhang, Xiaosong Wang</author><pubDate>Mon, 15 Jul 2024 15:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10814v1</guid></item><item><title>What's Wrong? Refining Meeting Summaries with LLM Feedback</title><link>http://arxiv.org/abs/2407.11919v1</link><description>Meeting summarization has become a critical task since digital encountershave become a common practice. Large language models (LLMs) show greatpotential in summarization, offering enhanced coherence and contextunderstanding compared to traditional methods. However, they still struggle tomaintain relevance and avoid hallucination. We introduce a multi-LLM correctionapproach for meeting summarization using a two-phase process that mimics thehuman review process: mistake identification and summary refinement. We releaseQMSum Mistake, a dataset of 200 automatically generated meeting summariesannotated by humans on nine error types, including structural, omission, andirrelevance errors. Our experiments show that these errors can be identifiedwith high accuracy by an LLM. We transform identified mistakes into actionablefeedback to improve the quality of a given summary measured by relevance,informativeness, conciseness, and coherence. This post-hoc refinementeffectively improves summary quality by leveraging multiple LLMs to validateoutput quality. Our multi-LLM approach for meeting summarization showspotential for similar complex text generation tasks requiring robustness,action planning, and discussion towards a goal.</description><author>Frederic Kirstein, Terry Ruas, Bela Gipp</author><pubDate>Tue, 16 Jul 2024 17:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11919v1</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v1</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell Espuny</author><pubDate>Tue, 16 Jul 2024 10:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v1</guid></item><item><title>Semantic-Aware Representation of Multi-Modal Data for Data Ingress: A Literature Review</title><link>http://arxiv.org/abs/2407.12438v1</link><description>Machine Learning (ML) is continuously permeating a growing amount ofapplication domains. Generative AI such as Large Language Models (LLMs) alsosees broad adoption to process multi-modal data such as text, images, audio,and video. While the trend is to use ever-larger datasets for training,managing this data efficiently has become a significant practical challenge inthe industry-double as much data is certainly not double as good. Rather theopposite is important since getting an understanding of the inherent qualityand diversity of the underlying data lakes is a growing challenge forapplication-specific ML as well as for fine-tuning foundation models.Furthermore, information retrieval (IR) from expanding data lakes iscomplicated by the temporal dimension inherent in time-series data which mustbe considered to determine its semantic value. This study focuses on thedifferent semantic-aware techniques to extract embeddings from mono-modal,multi-modal, and cross-modal data to enhance IR capabilities in a growing datalake. Articles were collected to summarize information about thestate-of-the-art techniques focusing on applications of embedding for threedifferent categories of data modalities.</description><author>Pierre Lamart, Yinan Yu, Christian Berger</author><pubDate>Wed, 17 Jul 2024 09:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12438v1</guid></item><item><title>Turning Generative Models Degenerate: The Power of Data Poisoning Attacks</title><link>http://arxiv.org/abs/2407.12281v2</link><description>The increasing use of large language models (LLMs) trained by third partiesraises significant security concerns. In particular, malicious actors canintroduce backdoors through poisoning attacks to generate undesirable outputs.While such attacks have been extensively studied in image domains andclassification tasks, they remain underexplored for natural language generation(NLG) tasks. To address this gap, we conduct an investigation of variouspoisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, aParameter Efficient Fine-Tuning (PEFT) method. We assess their effectivenessacross two generative tasks: text summarization and text completion; and wealso introduce new metrics to quantify the success and stealthiness of such NLGpoisoning attacks. Through our experiments, we find that the prefix-tuninghyperparameters and trigger designs are the most crucial factors to influenceattack success and stealthiness. Moreover, we demonstrate that existing populardefenses are ineffective against our poisoning attacks. Our study presents thefirst systematic approach to understanding poisoning attacks targeting NLGtasks during fine-tuning via PEFT across a wide range of triggers and attacksettings. We hope our findings will aid the AI security community in developingeffective defenses against such threats.</description><author>Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Farhan Ahmed, Ling Cai, Nathalie Baracaldo</author><pubDate>Thu, 18 Jul 2024 05:52:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12281v2</guid></item><item><title>On Learning to Summarize with Large Language Models as References</title><link>http://arxiv.org/abs/2305.14239v3</link><description>Recent studies have found that summaries generated by large language models(LLMs) are favored by human annotators over the original reference summaries incommonly used summarization datasets. Therefore, we study an LLM-as-referencelearning setting for smaller text summarization models to investigate whethertheir performance can be substantially improved. To this end, we use LLMs asboth oracle summary generators for standard supervised fine-tuning and oraclesummary evaluators for efficient contrastive learning that leverages the LLMs'supervision signals. We conduct comprehensive experiments with source newsarticles and find that (1) summarization models trained under theLLM-as-reference setting achieve significant performance improvement in bothLLM and human evaluations; (2) contrastive learning outperforms standardsupervised fine-tuning under both low and high resource settings. Ourexperimental results also enable a meta-analysis of LLMs' summary evaluationcapacities under a challenging setting, showing that LLMs are not well-alignedwith human evaluators. Particularly, our expert human evaluation revealsremaining nuanced performance gaps between LLMs and our fine-tuned models,which LLMs fail to capture. Thus, we call for further studies into both thepotential and challenges of using LLMs in summarization model development.</description><author>Yixin Liu, Kejian Shi, Katherine S He, Longtian Ye, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, Arman Cohan</author><pubDate>Thu, 18 Jul 2024 17:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14239v3</guid></item><item><title>PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like (PL) Tasks</title><link>http://arxiv.org/abs/2407.13597v1</link><description>Text summarization is a well-studied problem that deals with derivinginsights from unstructured text consumed by humans, and it has found extensivebusiness applications. However, many real-life tasks involve generating aseries of actions to achieve specific goals, such as workflows, recipes,dialogs, and travel plans. We refer to them as planning-like (PL) tasks notingthat the main commonality they share is control flow information. which may bepartially specified. Their structure presents an opportunity to create morepractical summaries to help users make quick decisions. We investigate thisobservation by introducing a novel plan summarization problem, presenting adataset, and providing a baseline method for generating PL summaries. Usingquantitative metrics and qualitative user studies to establish baselines, weevaluate the plan summaries from our method and large language models. Webelieve the novel problem and dataset can reinvigorate research insummarization, which some consider as a solved problem.</description><author>Vishal Pallagani, Biplav Srivastava, Nitin Gupta</author><pubDate>Thu, 18 Jul 2024 15:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13597v1</guid></item><item><title>Survey in Characterization of Semantic Change</title><link>http://arxiv.org/abs/2402.19088v3</link><description>Live languages continuously evolve to integrate the cultural change of humansocieties. This evolution manifests through neologisms (new words) or\textbf{semantic changes} of words (new meaning to existing words).Understanding the meaning of words is vital for interpreting texts coming fromdifferent cultures (regionalism or slang), domains (e.g., technical terms), orperiods. In computer science, these words are relevant to computationallinguistics algorithms such as translation, information retrieval, questionanswering, etc. Semantic changes can potentially impact the quality of theoutcomes of these algorithms. Therefore, it is important to understand andcharacterize these changes formally. The study of this impact is a recentproblem that has attracted the attention of the computational linguisticscommunity. Several approaches propose methods to detect semantic changes withgood precision, but more effort is needed to characterize how the meaning ofwords changes and to reason about how to reduce the impact of semantic change.This survey provides an understandable overview of existing approaches to the\textit{characterization of semantic changes} and also formally defines threeclasses of characterizations: if the meaning of a word becomes more general ornarrow (change in dimension) if the word is used in a more pejorative orpositive/ameliorated sense (change in orientation), and if there is a trend touse the word in a, for instance, metaphoric or metonymic context (change inrelation). We summarized the main aspects of the selected publications in atable and discussed the needs and trends in the research activities on semanticchange characterization.</description><author>Jader Martins Camboim de Sá, Marcos Da Silveira, Cédric Pruski</author><pubDate>Thu, 18 Jul 2024 12:28:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19088v3</guid></item><item><title>Multi-sentence Video Grounding for Long Video Generation</title><link>http://arxiv.org/abs/2407.13219v1</link><description>Video generation has witnessed great success recently, but their applicationin generating long videos still remains challenging due to the difficulty inmaintaining the temporal consistency of generated videos and the high memorycost during generation. To tackle the problems, in this paper, we propose abrave and new idea of Multi-sentence Video Grounding for Long Video Generation,connecting the massive video moment retrieval to the video generation task forthe first time, providing a new paradigm for long video generation. The methodof our work can be summarized as three steps: (i) We design sequential scenetext prompts as the queries for video grounding, utilizing the massive videomoment retrieval to search for video moment segments that meet the textrequirements in the video database. (ii) Based on the source frames ofretrieved video moment segments, we adopt video editing methods to create newvideo content while preserving the temporal consistency of the retrieved video.Since the editing can be conducted segment by segment, and even frame by frame,it largely reduces the memory cost. (iii) We also attempt video morphing andpersonalized generation methods to improve the subject consistency of longvideo generation, providing ablation experimental results for the subtasks oflong video generation. Our approach seamlessly extends the development inimage/video editing, video morphing and personalized generation, and videogrounding to the long video generation, offering effective solutions forgenerating long videos at low memory cost.</description><author>Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Wenwu Zhu</author><pubDate>Thu, 18 Jul 2024 07:05:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13219v1</guid></item><item><title>PassTSL: Modeling Human-Created Passwords through Two-Stage Learning</title><link>http://arxiv.org/abs/2407.14145v1</link><description>Textual passwords are still the most widely used user authenticationmechanism. Due to the close connections between textual passwords and naturallanguages, advanced technologies in natural language processing (NLP) andmachine learning (ML) could be used to model passwords for different purposessuch as studying human password-creation behaviors and developing more advancedpassword cracking methods for informing better defence mechanisms. In thispaper, we propose PassTSL (modeling human-created Passwords through Two-StageLearning), inspired by the popular pretraining-finetuning framework in NLP anddeep learning (DL). We report how different pretraining settings affectedPassTSL and proved its effectiveness by applying it to six large leakedpassword databases. Experimental results showed that it outperforms fivestate-of-the-art (SOTA) password cracking methods on password guessing by asignificant margin ranging from 4.11% to 64.69% at the maximum point. Based onPassTSL, we also implemented a password strength meter (PSM), and ourexperiments showed that it was able to estimate password strength moreaccurately, causing fewer unsafe errors (overestimating the password strength)than two other SOTA PSMs when they produce the same rate of safe errors(underestimating the password strength): a neural-network based method andzxcvbn. Furthermore, we explored multiple finetuning settings, and ourevaluations showed that, even a small amount of additional training data, e.g.,only 0.1% of the pretrained data, can lead to over 3% improvement in passwordguessing on average. We also proposed a heuristic approach to selectingfinetuning passwords based on JS (Jensen-Shannon) divergence and experimentalresults validated its usefulness. In summary, our contributions demonstrate thepotential and feasibility of applying advanced NLP and ML methods to passwordmodeling and cracking.</description><author>Yangde Wang, Haozhang Li, Weidong Qiu, Shujun Li, Peng Tang</author><pubDate>Fri, 19 Jul 2024 09:23:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14145v1</guid></item><item><title>AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description</title><link>http://arxiv.org/abs/2407.15850v1</link><description>Our objective is to generate Audio Descriptions (ADs) for both movies and TVseries in a training-free manner. We use the power of off-the-shelfVisual-Language Models (VLMs) and Large Language Models (LLMs), and developvisual and text prompting strategies for this task. Our contributions arethree-fold: (i) We demonstrate that a VLM can successfully name and refer tocharacters if directly prompted with character information through visualindications without requiring any fine-tuning; (ii) A two-stage process isdeveloped to generate ADs, with the first stage asking the VLM tocomprehensively describe the video, followed by a second stage utilising a LLMto summarise dense textual information into one succinct AD sentence; (iii) Anew dataset for TV audio description is formulated. Our approach, namedAutoAD-Zero, demonstrates outstanding performance (even competitive with somemodels fine-tuned on ground truth ADs) in AD generation for both movies and TVseries, achieving state-of-the-art CRITIC scores.</description><author>Junyu Xie, Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman</author><pubDate>Mon, 22 Jul 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15850v1</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v2</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell</author><pubDate>Mon, 22 Jul 2024 13:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v2</guid></item><item><title>AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations</title><link>http://arxiv.org/abs/2407.16010v1</link><description>For many use-cases, it is often important to explain the prediction of ablack-box model by identifying the most influential training data samples.Existing approaches lack customization for user intent and often provide ahomogeneous set of explanation samples, failing to reveal the model's reasoningfrom different angles. In this paper, we propose AIDE, an approach for providing antithetical (i.e.,contrastive), intent-based, diverse explanations for opaque and complex models.AIDE distinguishes three types of explainability intents: interpreting acorrect, investigating a wrong, and clarifying an ambiguous prediction. Foreach intent, AIDE selects an appropriate set of influential training samplesthat support or oppose the prediction either directly or by contrast. Toprovide a succinct summary, AIDE uses diversity-aware sampling to avoidredundancy and increase coverage of the training data. We demonstrate the effectiveness of AIDE on image and text classificationtasks, in three ways: quantitatively, assessing correctness and continuity;qualitatively, comparing anecdotal evidence from AIDE and other example-basedapproaches; and via a user study, evaluating multiple aspects of AIDE. Theresults show that AIDE addresses the limitations of existing methods andexhibits desirable traits for an explainability method.</description><author>Ikhtiyor Nematov, Dimitris Sacharidis, Tomer Sagi, Katja Hose</author><pubDate>Mon, 22 Jul 2024 19:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16010v1</guid></item><item><title>Speech Editing -- a Summary</title><link>http://arxiv.org/abs/2407.17172v1</link><description>With the rise of video production and social media, speech editing has becomecrucial for creators to address issues like mispronunciations, missing words,or stuttering in audio recordings. This paper explores text-based speechediting methods that modify audio via text transcripts without manual waveformediting. These approaches ensure edited audio is indistinguishable from theoriginal by altering the mel-spectrogram. Recent advancements, such ascontext-aware prosody correction and advanced attention mechanisms, haveimproved speech editing quality. This paper reviews state-of-the-art methods,compares key metrics, and examines widely used datasets. The aim is tohighlight ongoing issues and inspire further research and innovation in speechediting.</description><author>Tobias Kässmann, Yining Liu, Danni Liu</author><pubDate>Wed, 24 Jul 2024 11:22:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17172v1</guid></item><item><title>Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</title><link>http://arxiv.org/abs/2407.18003v1</link><description>Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,have revolutionized various industries with their advanced languagecomprehension. However, their efficiency is challenged by the Transformerarchitecture' s struggle with handling long texts. KV-Cache has emerged as apivotal solution to this issue, converting the time complexity of tokengeneration from quadratic to linear, albeit with increased GPU memory overheadproportional to conversation length. With the development of the LLM communityand academia, various KV-Cache compression methods have been proposed. In thisreview, we dissect the various properties of KV-Cache and elaborate on variousmethods currently used to optimize the KV-Cache space usage of LLMs. Thesemethods span the pre-training phase, deployment phase, and inference phase, andwe summarize the commonalities and differences among these methods.Additionally, we list some metrics for evaluating the long-text capabilities oflarge language models, from both efficiency and capability perspectives. Ourreview thus sheds light on the evolving landscape of LLM optimization, offeringinsights into future advancements in this dynamic field.</description><author>Shi Luohe, Zhang Hongyi, Yao Yao, Li Zuchao, Zhao Hai</author><pubDate>Thu, 25 Jul 2024 12:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18003v1</guid></item><item><title>An Iterative Approach to Topic Modelling</title><link>http://arxiv.org/abs/2407.17892v1</link><description>Topic modelling has become increasingly popular for summarizing text data,such as social media posts and articles. However, topic modelling is usuallycompleted in one shot. Assessing the quality of resulting topics ischallenging. No effective methods or measures have been developed for assessingthe results or for making further enhancements to the topics. In this research,we propose we propose to use an iterative process to perform topic modellingthat gives rise to a sense of completeness of the resulting topics when theprocess is complete. Using the BERTopic package, a popular method in topicmodelling, we demonstrate how the modelling process can be applied iterativelyto arrive at a set of topics that could not be further improved upon using oneof the three selected measures for clustering comparison as the decisioncriteria. This demonstration is conducted using a subset of the COVIDSenti-Adataset. The early success leads us to believe that further research using inusing this approach in conjunction with other topic modelling algorithms couldbe viable.</description><author>Albert Wong, Florence Wing Yau Cheng, Ashley Keung, Yamileth Hercules, Mary Alexandra Garcia, Yew-Wei Lim, Lien Pham</author><pubDate>Thu, 25 Jul 2024 09:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17892v1</guid></item><item><title>IgnitionInnovators at "Discharge Me!": Chain-of-Thought Instruction Finetuning Large Language Models for Discharge Summaries</title><link>http://arxiv.org/abs/2407.17636v1</link><description>This paper presents our proposed approach to the Discharge Me! shared task,collocated with the 23th Workshop on Biomedical Natural Language Processing(BioNLP). In this work, we develop an LLM-based framework for solving theDischarge Summary Documentation (DSD) task, i.e., generating the two criticaltarget sections `Brief Hospital Course' and `Discharge Instructions' in thedischarge summary. By streamlining the recent instruction-finetuning process onLLMs, we explore several prompting strategies for optimally adapting LLMs tospecific generation task of DSD. Experimental results show that providing aclear output structure, complimented by a set of comprehensiveChain-of-Thoughts (CoT) questions, effectively improves the model's reasoningcapability, and thereby, enhancing the structural correctness and faithfulnessof clinical information in the generated text. Source code is available at:https://github.com/antangrocket1312/Discharge_LLM</description><author>An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh</author><pubDate>Wed, 24 Jul 2024 21:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17636v1</guid></item><item><title>Tamil Language Computing: the Present and the Future</title><link>http://arxiv.org/abs/2407.08618v1</link><description>This paper delves into the text processing aspects of Language Computing,which enables computers to understand, interpret, and generate human language.Focusing on tasks such as speech recognition, machine translation, sentimentanalysis, text summarization, and language modelling, language computingintegrates disciplines including linguistics, computer science, and cognitivepsychology to create meaningful human-computer interactions. Recentadvancements in deep learning have made computers more accessible and capableof independent learning and adaptation. In examining the landscape of languagecomputing, the paper emphasises foundational work like encoding, where Tamiltransitioned from ASCII to Unicode, enhancing digital communication. Itdiscusses the development of computational resources, including raw data,dictionaries, glossaries, annotated data, and computational grammars, necessaryfor effective language processing. The challenges of linguistic annotation, thecreation of treebanks, and the training of large language models are alsocovered, emphasising the need for high-quality, annotated data and advancedlanguage models. The paper underscores the importance of building practicalapplications for languages like Tamil to address everyday communication needs,highlighting gaps in current technology. It calls for increased researchcollaboration, digitization of historical texts, and fostering digital usage toensure the comprehensive development of Tamil language processing, ultimatelyenhancing global communication and access to digital services.</description><author>Kengatharaiyer Sarveswaran</author><pubDate>Thu, 11 Jul 2024 15:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08618v1</guid></item><item><title>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</title><link>http://arxiv.org/abs/2311.09184v2</link><description>While large language models (LLMs) can already achieve strong performance onstandard generic summarization benchmarks, their performance on more complexsummarization task settings is less studied. Therefore, we benchmark LLMs oninstruction controllable text summarization, where the model input consists ofboth a source article and a natural language requirement for desired summarycharacteristics. To this end, we curate an evaluation-only dataset for thistask setting and conduct human evaluations of five LLM-based systems to assesstheir instruction-following capabilities in controllable summarization. We thenbenchmark LLM-based automatic evaluation for this task with 4 differentevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our studyreveals that instruction controllable text summarization remains a challengingtask for LLMs, since (1) all LLMs evaluated still make factual and other typesof errors in their summaries; (2) no LLM-based evaluation methods can achieve astrong alignment with human annotators when judging the quality of candidatesummaries; (3) different LLMs show large performance gaps in summary generationand evaluation capabilities. We make our collected benchmark InstruSum publiclyavailable to facilitate future research in this direction.</description><author>Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan</author><pubDate>Fri, 12 Jul 2024 17:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09184v2</guid></item></channel></rss>