<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivtext summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 22 Sep 2024 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>A BERT-Based Summarization approach for depression detection</title><link>http://arxiv.org/abs/2409.08483v1</link><description>Depression is a globally prevalent mental disorder with potentially severerepercussions if not addressed, especially in individuals with recurrentepisodes. Prior research has shown that early intervention has the potential tomitigate or alleviate symptoms of depression. However, implementing suchinterventions in a real-world setting may pose considerable challenges. Apromising strategy involves leveraging machine learning and artificialintelligence to autonomously detect depression indicators from diverse datasources. One of the most widely available and informative data sources is text,which can reveal a person's mood, thoughts, and feelings. In this context,virtual agents programmed to conduct interviews using clinically validatedquestionnaires, such as those found in the DAIC-WOZ dataset, offer a robustmeans for depression detection through linguistic analysis. UtilizingBERT-based models, which are powerful and versatile yet use fewer resourcesthan contemporary large language models, to convert text into numericalrepresentations significantly enhances the precision of depression diagnosis.These models adeptly capture complex semantic and syntactic nuances, improvingthe detection accuracy of depressive symptoms. Given the inherent limitationsof these models concerning text length, our study proposes text summarizationas a preprocessing technique to diminish the length and intricacies of inputtexts. Implementing this method within our uniquely developed framework forfeature extraction and classification yielded an F1-score of 0.67 on the testset surpassing all prior benchmarks and 0.81 on the validation set exceedingmost previous results on the DAIC-WOZ dataset. Furthermore, we have devised adepression lexicon to assess summary quality and relevance. This lexiconconstitutes a valuable asset for ongoing research in depression detection.</description><author>Hossein Salahshoor Gavalan, Mohmmad Naim Rastgoo, Bahareh Nakisa</author><pubDate>Fri, 13 Sep 2024 02:14:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08483v1</guid></item><item><title>Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching</title><link>http://arxiv.org/abs/2407.17349v1</link><description>With the introduction of large language models (LLMs), automatic mathreasoning has seen tremendous success. However, current methods primarily focuson providing solutions or using techniques like Chain-of-Thought to enhanceproblem-solving accuracy. In this paper, we focus on improving the capabilityof mathematics teaching via a Socratic teaching-based LLM(\texttt{SocraticLLM}), which guides learners toward profound thinking withclarity and self-discovery via conversation. We collect and release ahigh-quality mathematical teaching dataset, named \texttt{SocraticMATH}, whichprovides Socratic-style conversations of problems with extra knowledge. Also,we propose a knowledge-enhanced LLM as a strong baseline to generate reliableresponses with review, guidance/heuristic, rectification, and summarization.Experimental results show the great advantages of \texttt{SocraticLLM} bycomparing it with several strong generative models. The codes and datasets areavailable on \url{https://github.com/ECNU-ICALK/SocraticMath}.</description><author>Yuyang Ding, Hanglei Hu, Jie Zhou, Qin Chen, Bo Jiang, Liang He</author><pubDate>Wed, 24 Jul 2024 15:18:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17349v1</guid></item><item><title>Zero-shot Factual Consistency Evaluation Across Domains</title><link>http://arxiv.org/abs/2408.04114v1</link><description>This work addresses the challenge of factual consistency in text generationsystems. We unify the tasks of Natural Language Inference, SummarizationEvaluation, Factuality Verification and Factual Consistency Evaluation to trainmodels capable of evaluating the factual consistency of source-target pairsacross diverse domains. We rigorously evaluate these against eight baselines ona comprehensive benchmark suite comprising 22 datasets that span various tasks,domains, and document lengths. Results demonstrate that our method achievesstate-of-the-art performance on this heterogeneous benchmark while addressingefficiency concerns and attaining cross-domain generalization.</description><author>Raunak Agarwal</author><pubDate>Wed, 07 Aug 2024 22:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04114v1</guid></item><item><title>MixSumm: Topic-based Data Augmentation using LLMs for Low-resource Extractive Text Summarization</title><link>http://arxiv.org/abs/2407.07341v1</link><description>Low-resource extractive text summarization is a vital but heavilyunderexplored area of research. Prior literature either focuses on abstractivetext summarization or prompts a large language model (LLM) like GPT-3 directlyto generate summaries. In this work, we propose MixSumm for low-resourceextractive text summarization. Specifically, MixSumm prompts an open-sourceLLM, LLaMA-3-70b, to generate documents that mix information from multipletopics as opposed to generating documents without mixup, and then trains asummarization model on the generated dataset. We use ROUGE scores and L-Eval, areference-free LLaMA-3-based evaluation method to measure the quality ofgenerated summaries. We conduct extensive experiments on a challenging textsummarization benchmark comprising the TweetSumm, WikiHow, and ArXiv/PubMeddatasets and show that our LLM-based data augmentation framework outperformsrecent prompt-based approaches for low-resource extractive summarization.Additionally, our results also demonstrate effective knowledge distillationfrom LLaMA-3-70b to a small BERT-based extractive summarizer.</description><author>Gaurav Sahu, Issam H. Laradji</author><pubDate>Wed, 10 Jul 2024 03:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07341v1</guid></item><item><title>IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization</title><link>http://arxiv.org/abs/2407.10486v1</link><description>Query-focused summarization (QFS) aims to produce summaries that answerparticular questions of interest, enabling greater user control andpersonalization. With the advent of large language models (LLMs), shows theirimpressive capability of textual understanding through large-scale pretraining,which implies the great potential of extractive snippet generation. In thispaper, we systematically investigated two indispensable characteristics thatthe LLMs-based QFS models should be harnessed, Lengthy Document Summarizationand Efficiently Fine-grained Query-LLM Alignment, respectively.Correspondingly, we propose two modules called Query-aware HyperExpert andQuery-focused Infini-attention to access the aforementioned characteristics.These innovations pave the way for broader application and accessibility in thefield of QFS technology. Extensive experiments conducted on existing QFSbenchmarks indicate the effectiveness and generalizability of the proposedapproach. Our code is publicly available athttps://github.com/DCDmllm/IDEAL_Summary.</description><author>Jie Cao, Dian Jiao, Qiang Yan, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</author><pubDate>Mon, 15 Jul 2024 07:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10486v1</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v1</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell Espuny</author><pubDate>Tue, 16 Jul 2024 10:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v1</guid></item><item><title>Semantic-Aware Representation of Multi-Modal Data for Data Ingress: A Literature Review</title><link>http://arxiv.org/abs/2407.12438v1</link><description>Machine Learning (ML) is continuously permeating a growing amount ofapplication domains. Generative AI such as Large Language Models (LLMs) alsosees broad adoption to process multi-modal data such as text, images, audio,and video. While the trend is to use ever-larger datasets for training,managing this data efficiently has become a significant practical challenge inthe industry-double as much data is certainly not double as good. Rather theopposite is important since getting an understanding of the inherent qualityand diversity of the underlying data lakes is a growing challenge forapplication-specific ML as well as for fine-tuning foundation models.Furthermore, information retrieval (IR) from expanding data lakes iscomplicated by the temporal dimension inherent in time-series data which mustbe considered to determine its semantic value. This study focuses on thedifferent semantic-aware techniques to extract embeddings from mono-modal,multi-modal, and cross-modal data to enhance IR capabilities in a growing datalake. Articles were collected to summarize information about thestate-of-the-art techniques focusing on applications of embedding for threedifferent categories of data modalities.</description><author>Pierre Lamart, Yinan Yu, Christian Berger</author><pubDate>Wed, 17 Jul 2024 09:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12438v1</guid></item><item><title>Turning Generative Models Degenerate: The Power of Data Poisoning Attacks</title><link>http://arxiv.org/abs/2407.12281v2</link><description>The increasing use of large language models (LLMs) trained by third partiesraises significant security concerns. In particular, malicious actors canintroduce backdoors through poisoning attacks to generate undesirable outputs.While such attacks have been extensively studied in image domains andclassification tasks, they remain underexplored for natural language generation(NLG) tasks. To address this gap, we conduct an investigation of variouspoisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, aParameter Efficient Fine-Tuning (PEFT) method. We assess their effectivenessacross two generative tasks: text summarization and text completion; and wealso introduce new metrics to quantify the success and stealthiness of such NLGpoisoning attacks. Through our experiments, we find that the prefix-tuninghyperparameters and trigger designs are the most crucial factors to influenceattack success and stealthiness. Moreover, we demonstrate that existing populardefenses are ineffective against our poisoning attacks. Our study presents thefirst systematic approach to understanding poisoning attacks targeting NLGtasks during fine-tuning via PEFT across a wide range of triggers and attacksettings. We hope our findings will aid the AI security community in developingeffective defenses against such threats.</description><author>Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Farhan Ahmed, Ling Cai, Nathalie Baracaldo</author><pubDate>Thu, 18 Jul 2024 05:52:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12281v2</guid></item><item><title>On Learning to Summarize with Large Language Models as References</title><link>http://arxiv.org/abs/2305.14239v3</link><description>Recent studies have found that summaries generated by large language models(LLMs) are favored by human annotators over the original reference summaries incommonly used summarization datasets. Therefore, we study an LLM-as-referencelearning setting for smaller text summarization models to investigate whethertheir performance can be substantially improved. To this end, we use LLMs asboth oracle summary generators for standard supervised fine-tuning and oraclesummary evaluators for efficient contrastive learning that leverages the LLMs'supervision signals. We conduct comprehensive experiments with source newsarticles and find that (1) summarization models trained under theLLM-as-reference setting achieve significant performance improvement in bothLLM and human evaluations; (2) contrastive learning outperforms standardsupervised fine-tuning under both low and high resource settings. Ourexperimental results also enable a meta-analysis of LLMs' summary evaluationcapacities under a challenging setting, showing that LLMs are not well-alignedwith human evaluators. Particularly, our expert human evaluation revealsremaining nuanced performance gaps between LLMs and our fine-tuned models,which LLMs fail to capture. Thus, we call for further studies into both thepotential and challenges of using LLMs in summarization model development.</description><author>Yixin Liu, Kejian Shi, Katherine S He, Longtian Ye, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, Arman Cohan</author><pubDate>Thu, 18 Jul 2024 17:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14239v3</guid></item><item><title>PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like (PL) Tasks</title><link>http://arxiv.org/abs/2407.13597v1</link><description>Text summarization is a well-studied problem that deals with derivinginsights from unstructured text consumed by humans, and it has found extensivebusiness applications. However, many real-life tasks involve generating aseries of actions to achieve specific goals, such as workflows, recipes,dialogs, and travel plans. We refer to them as planning-like (PL) tasks notingthat the main commonality they share is control flow information. which may bepartially specified. Their structure presents an opportunity to create morepractical summaries to help users make quick decisions. We investigate thisobservation by introducing a novel plan summarization problem, presenting adataset, and providing a baseline method for generating PL summaries. Usingquantitative metrics and qualitative user studies to establish baselines, weevaluate the plan summaries from our method and large language models. Webelieve the novel problem and dataset can reinvigorate research insummarization, which some consider as a solved problem.</description><author>Vishal Pallagani, Biplav Srivastava, Nitin Gupta</author><pubDate>Thu, 18 Jul 2024 15:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13597v1</guid></item><item><title>Survey in Characterization of Semantic Change</title><link>http://arxiv.org/abs/2402.19088v3</link><description>Live languages continuously evolve to integrate the cultural change of humansocieties. This evolution manifests through neologisms (new words) or\textbf{semantic changes} of words (new meaning to existing words).Understanding the meaning of words is vital for interpreting texts coming fromdifferent cultures (regionalism or slang), domains (e.g., technical terms), orperiods. In computer science, these words are relevant to computationallinguistics algorithms such as translation, information retrieval, questionanswering, etc. Semantic changes can potentially impact the quality of theoutcomes of these algorithms. Therefore, it is important to understand andcharacterize these changes formally. The study of this impact is a recentproblem that has attracted the attention of the computational linguisticscommunity. Several approaches propose methods to detect semantic changes withgood precision, but more effort is needed to characterize how the meaning ofwords changes and to reason about how to reduce the impact of semantic change.This survey provides an understandable overview of existing approaches to the\textit{characterization of semantic changes} and also formally defines threeclasses of characterizations: if the meaning of a word becomes more general ornarrow (change in dimension) if the word is used in a more pejorative orpositive/ameliorated sense (change in orientation), and if there is a trend touse the word in a, for instance, metaphoric or metonymic context (change inrelation). We summarized the main aspects of the selected publications in atable and discussed the needs and trends in the research activities on semanticchange characterization.</description><author>Jader Martins Camboim de Sá, Marcos Da Silveira, Cédric Pruski</author><pubDate>Thu, 18 Jul 2024 12:28:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19088v3</guid></item><item><title>Multi-sentence Video Grounding for Long Video Generation</title><link>http://arxiv.org/abs/2407.13219v1</link><description>Video generation has witnessed great success recently, but their applicationin generating long videos still remains challenging due to the difficulty inmaintaining the temporal consistency of generated videos and the high memorycost during generation. To tackle the problems, in this paper, we propose abrave and new idea of Multi-sentence Video Grounding for Long Video Generation,connecting the massive video moment retrieval to the video generation task forthe first time, providing a new paradigm for long video generation. The methodof our work can be summarized as three steps: (i) We design sequential scenetext prompts as the queries for video grounding, utilizing the massive videomoment retrieval to search for video moment segments that meet the textrequirements in the video database. (ii) Based on the source frames ofretrieved video moment segments, we adopt video editing methods to create newvideo content while preserving the temporal consistency of the retrieved video.Since the editing can be conducted segment by segment, and even frame by frame,it largely reduces the memory cost. (iii) We also attempt video morphing andpersonalized generation methods to improve the subject consistency of longvideo generation, providing ablation experimental results for the subtasks oflong video generation. Our approach seamlessly extends the development inimage/video editing, video morphing and personalized generation, and videogrounding to the long video generation, offering effective solutions forgenerating long videos at low memory cost.</description><author>Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Wenwu Zhu</author><pubDate>Thu, 18 Jul 2024 07:05:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13219v1</guid></item><item><title>PassTSL: Modeling Human-Created Passwords through Two-Stage Learning</title><link>http://arxiv.org/abs/2407.14145v1</link><description>Textual passwords are still the most widely used user authenticationmechanism. Due to the close connections between textual passwords and naturallanguages, advanced technologies in natural language processing (NLP) andmachine learning (ML) could be used to model passwords for different purposessuch as studying human password-creation behaviors and developing more advancedpassword cracking methods for informing better defence mechanisms. In thispaper, we propose PassTSL (modeling human-created Passwords through Two-StageLearning), inspired by the popular pretraining-finetuning framework in NLP anddeep learning (DL). We report how different pretraining settings affectedPassTSL and proved its effectiveness by applying it to six large leakedpassword databases. Experimental results showed that it outperforms fivestate-of-the-art (SOTA) password cracking methods on password guessing by asignificant margin ranging from 4.11% to 64.69% at the maximum point. Based onPassTSL, we also implemented a password strength meter (PSM), and ourexperiments showed that it was able to estimate password strength moreaccurately, causing fewer unsafe errors (overestimating the password strength)than two other SOTA PSMs when they produce the same rate of safe errors(underestimating the password strength): a neural-network based method andzxcvbn. Furthermore, we explored multiple finetuning settings, and ourevaluations showed that, even a small amount of additional training data, e.g.,only 0.1% of the pretrained data, can lead to over 3% improvement in passwordguessing on average. We also proposed a heuristic approach to selectingfinetuning passwords based on JS (Jensen-Shannon) divergence and experimentalresults validated its usefulness. In summary, our contributions demonstrate thepotential and feasibility of applying advanced NLP and ML methods to passwordmodeling and cracking.</description><author>Yangde Wang, Haozhang Li, Weidong Qiu, Shujun Li, Peng Tang</author><pubDate>Fri, 19 Jul 2024 09:23:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14145v1</guid></item><item><title>AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description</title><link>http://arxiv.org/abs/2407.15850v1</link><description>Our objective is to generate Audio Descriptions (ADs) for both movies and TVseries in a training-free manner. We use the power of off-the-shelfVisual-Language Models (VLMs) and Large Language Models (LLMs), and developvisual and text prompting strategies for this task. Our contributions arethree-fold: (i) We demonstrate that a VLM can successfully name and refer tocharacters if directly prompted with character information through visualindications without requiring any fine-tuning; (ii) A two-stage process isdeveloped to generate ADs, with the first stage asking the VLM tocomprehensively describe the video, followed by a second stage utilising a LLMto summarise dense textual information into one succinct AD sentence; (iii) Anew dataset for TV audio description is formulated. Our approach, namedAutoAD-Zero, demonstrates outstanding performance (even competitive with somemodels fine-tuned on ground truth ADs) in AD generation for both movies and TVseries, achieving state-of-the-art CRITIC scores.</description><author>Junyu Xie, Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman</author><pubDate>Mon, 22 Jul 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15850v1</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v2</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell</author><pubDate>Mon, 22 Jul 2024 13:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v2</guid></item><item><title>AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations</title><link>http://arxiv.org/abs/2407.16010v1</link><description>For many use-cases, it is often important to explain the prediction of ablack-box model by identifying the most influential training data samples.Existing approaches lack customization for user intent and often provide ahomogeneous set of explanation samples, failing to reveal the model's reasoningfrom different angles. In this paper, we propose AIDE, an approach for providing antithetical (i.e.,contrastive), intent-based, diverse explanations for opaque and complex models.AIDE distinguishes three types of explainability intents: interpreting acorrect, investigating a wrong, and clarifying an ambiguous prediction. Foreach intent, AIDE selects an appropriate set of influential training samplesthat support or oppose the prediction either directly or by contrast. Toprovide a succinct summary, AIDE uses diversity-aware sampling to avoidredundancy and increase coverage of the training data. We demonstrate the effectiveness of AIDE on image and text classificationtasks, in three ways: quantitatively, assessing correctness and continuity;qualitatively, comparing anecdotal evidence from AIDE and other example-basedapproaches; and via a user study, evaluating multiple aspects of AIDE. Theresults show that AIDE addresses the limitations of existing methods andexhibits desirable traits for an explainability method.</description><author>Ikhtiyor Nematov, Dimitris Sacharidis, Tomer Sagi, Katja Hose</author><pubDate>Mon, 22 Jul 2024 19:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16010v1</guid></item><item><title>Speech Editing -- a Summary</title><link>http://arxiv.org/abs/2407.17172v1</link><description>With the rise of video production and social media, speech editing has becomecrucial for creators to address issues like mispronunciations, missing words,or stuttering in audio recordings. This paper explores text-based speechediting methods that modify audio via text transcripts without manual waveformediting. These approaches ensure edited audio is indistinguishable from theoriginal by altering the mel-spectrogram. Recent advancements, such ascontext-aware prosody correction and advanced attention mechanisms, haveimproved speech editing quality. This paper reviews state-of-the-art methods,compares key metrics, and examines widely used datasets. The aim is tohighlight ongoing issues and inspire further research and innovation in speechediting.</description><author>Tobias Kässmann, Yining Liu, Danni Liu</author><pubDate>Wed, 24 Jul 2024 11:22:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17172v1</guid></item><item><title>Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</title><link>http://arxiv.org/abs/2407.18003v1</link><description>Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,have revolutionized various industries with their advanced languagecomprehension. However, their efficiency is challenged by the Transformerarchitecture' s struggle with handling long texts. KV-Cache has emerged as apivotal solution to this issue, converting the time complexity of tokengeneration from quadratic to linear, albeit with increased GPU memory overheadproportional to conversation length. With the development of the LLM communityand academia, various KV-Cache compression methods have been proposed. In thisreview, we dissect the various properties of KV-Cache and elaborate on variousmethods currently used to optimize the KV-Cache space usage of LLMs. Thesemethods span the pre-training phase, deployment phase, and inference phase, andwe summarize the commonalities and differences among these methods.Additionally, we list some metrics for evaluating the long-text capabilities oflarge language models, from both efficiency and capability perspectives. Ourreview thus sheds light on the evolving landscape of LLM optimization, offeringinsights into future advancements in this dynamic field.</description><author>Shi Luohe, Zhang Hongyi, Yao Yao, Li Zuchao, Zhao Hai</author><pubDate>Thu, 25 Jul 2024 12:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18003v1</guid></item><item><title>An Iterative Approach to Topic Modelling</title><link>http://arxiv.org/abs/2407.17892v1</link><description>Topic modelling has become increasingly popular for summarizing text data,such as social media posts and articles. However, topic modelling is usuallycompleted in one shot. Assessing the quality of resulting topics ischallenging. No effective methods or measures have been developed for assessingthe results or for making further enhancements to the topics. In this research,we propose we propose to use an iterative process to perform topic modellingthat gives rise to a sense of completeness of the resulting topics when theprocess is complete. Using the BERTopic package, a popular method in topicmodelling, we demonstrate how the modelling process can be applied iterativelyto arrive at a set of topics that could not be further improved upon using oneof the three selected measures for clustering comparison as the decisioncriteria. This demonstration is conducted using a subset of the COVIDSenti-Adataset. The early success leads us to believe that further research using inusing this approach in conjunction with other topic modelling algorithms couldbe viable.</description><author>Albert Wong, Florence Wing Yau Cheng, Ashley Keung, Yamileth Hercules, Mary Alexandra Garcia, Yew-Wei Lim, Lien Pham</author><pubDate>Thu, 25 Jul 2024 09:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17892v1</guid></item><item><title>IgnitionInnovators at "Discharge Me!": Chain-of-Thought Instruction Finetuning Large Language Models for Discharge Summaries</title><link>http://arxiv.org/abs/2407.17636v1</link><description>This paper presents our proposed approach to the Discharge Me! shared task,collocated with the 23th Workshop on Biomedical Natural Language Processing(BioNLP). In this work, we develop an LLM-based framework for solving theDischarge Summary Documentation (DSD) task, i.e., generating the two criticaltarget sections `Brief Hospital Course' and `Discharge Instructions' in thedischarge summary. By streamlining the recent instruction-finetuning process onLLMs, we explore several prompting strategies for optimally adapting LLMs tospecific generation task of DSD. Experimental results show that providing aclear output structure, complimented by a set of comprehensiveChain-of-Thoughts (CoT) questions, effectively improves the model's reasoningcapability, and thereby, enhancing the structural correctness and faithfulnessof clinical information in the generated text. Source code is available at:https://github.com/antangrocket1312/Discharge_LLM</description><author>An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh</author><pubDate>Wed, 24 Jul 2024 21:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17636v1</guid></item><item><title>Assessing Brittleness of Image-Text Retrieval Benchmarks from Vision-Language Models Perspective</title><link>http://arxiv.org/abs/2407.15239v2</link><description>Image-text retrieval (ITR), an important task in information retrieval (IR),is driven by pretrained vision-language models (VLMs) that consistently achievestate-of-the-art performance. However, a significant challenge lies in thebrittleness of existing ITR benchmarks. In standard datasets for the task,captions often provide broad summaries of scenes, neglecting detailedinformation about specific concepts. Additionally, the current evaluation setupassumes simplistic binary matches between images and texts and focuses onintra-modality rather than cross-modal relationships, which can lead tomisinterpretations of model performance. Motivated by this gap, in this study,we focus on examining the brittleness of the ITR evaluation pipeline with afocus on concept granularity. We start by analyzing two common benchmarks,MS-COCO and Flickr30k, and compare them with their augmented versions,MS-COCO-FG and Flickr30k-FG, given a specified set of linguistic featurescapturing concept granularity. We discover that Flickr30k-FG and MS COCO-FGconsistently achieve higher scores across all the selected features. Toinvestigate the performance of VLMs on coarse and fine-grained datasets, weintroduce a taxonomy of perturbations. We apply these perturbations to theselected datasets. We evaluate four state-of-the-art models - ALIGN, AltCLIP,CLIP, and GroupViT - on the standard and fine-grained datasets under zero-shotconditions, with and without the applied perturbations. The results demonstratethat although perturbations generally degrade model performance, thefine-grained datasets exhibit a smaller performance drop than their standardcounterparts. Moreover, the relative performance drop across all setups isconsistent across all models and datasets, indicating that the issue lieswithin the benchmarks. We conclude the paper by providing an agenda forimproving ITR evaluation pipelines.</description><author>Mariya Hendriksen, Shuo Zhang, Ridho Reinanda, Mohamed Yahya, Edgar Meij, Maarten de Rijke</author><pubDate>Thu, 25 Jul 2024 19:52:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15239v2</guid></item><item><title>Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</title><link>http://arxiv.org/abs/2407.18003v2</link><description>Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,have revolutionized various industries with their advanced languagecomprehension. However, their efficiency is challenged by the Transformerarchitecture' s struggle with handling long texts. KV-Cache has emerged as apivotal solution to this issue, converting the time complexity of tokengeneration from quadratic to linear, albeit with increased GPU memory overheadproportional to conversation length. With the development of the LLM communityand academia, various KV-Cache compression methods have been proposed. In thisreview, we dissect the various properties of KV-Cache and elaborate on variousmethods currently used to optimize the KV-Cache space usage of LLMs. Thesemethods span the pre-training phase, deployment phase, and inference phase, andwe summarize the commonalities and differences among these methods.Additionally, we list some metrics for evaluating the long-text capabilities oflarge language models, from both efficiency and capability perspectives. Ourreview thus sheds light on the evolving landscape of LLM optimization, offeringinsights into future advancements in this dynamic field.</description><author>Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao</author><pubDate>Sun, 28 Jul 2024 14:42:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18003v2</guid></item><item><title>Synthesizing Scientific Summaries: An Extractive and Abstractive Approach</title><link>http://arxiv.org/abs/2407.19779v1</link><description>The availability of a vast array of research papers in any area of study,necessitates the need of automated summarisation systems that can present thekey research conducted and their corresponding findings. Scientific papersummarisation is a challenging task for various reasons including token lengthlimits in modern transformer models and corresponding memory and computerequirements for long text. A significant amount of work has been conducted inthis area, with approaches that modify the attention mechanisms of existingtransformer models and others that utilise discourse information to capturelong range dependencies in research papers. In this paper, we propose a hybridmethodology for research paper summarisation which incorporates an extractiveand abstractive approach. We use the extractive approach to capture the keyfindings of research, and pair it with the introduction of the paper whichcaptures the motivation for research. We use two models based on unsupervisedlearning for the extraction stage and two transformer language models,resulting in four combinations for our hybrid approach. The performances of themodels are evaluated on three metrics and we present our findings in thispaper. We find that using certain combinations of hyper parameters, it ispossible for automated summarisation systems to exceed the abstractiveness ofsummaries written by humans. Finally, we state our future scope of research inextending this methodology to summarisation of generalised long documents.</description><author>Grishma Sharma, Aditi Paretkar, Deepak Sharma</author><pubDate>Mon, 29 Jul 2024 08:21:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19779v1</guid></item><item><title>Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency</title><link>http://arxiv.org/abs/2407.21443v1</link><description>Despite large language models (LLMs) have demonstrated impressive performancein various tasks, they are still suffering from the factual inconsistencyproblem called hallucinations. For instance, LLMs occasionally generate contentthat diverges from source article, and prefer to extract information thatappears at the beginning and end of the context, especially in long documentsummarization. Inspired by these findings, we propose to improve thefaithfulness of LLMs in summarization by impelling them to process the entirearticle more fairly and faithfully. We present a novel summary generationstrategy, namely SliSum, which exploits the ideas of sliding windows andself-consistency. Specifically, SliSum divides the source article intooverlapping windows, and utilizes LLM to generate local summaries for thecontent in the windows. Finally, SliSum aggregates all local summaries usingclustering and majority voting algorithm to produce more faithful summary ofentire article. Extensive experiments demonstrate that SliSum significantlyimproves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 andGPT-3.5 in both short and long text summarization, while maintaining theirfluency and informativeness and without additional fine-tuning and resources.We further conduct qualitative and quantitative studies to investigate whySliSum works and impacts of hyperparameters in SliSum on performance.</description><author>Taiji Li, Zhi Li, Yin Zhang</author><pubDate>Wed, 31 Jul 2024 08:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21443v1</guid></item><item><title>Debiased Distribution Compression</title><link>http://arxiv.org/abs/2404.12290v3</link><description>Modern compression methods can summarize a target distribution $\mathbb{P}$more succinctly than i.i.d. sampling but require access to a low-bias inputsequence like a Markov chain converging quickly to $\mathbb{P}$. We introduce anew suite of compression methods suitable for compression with biased inputsequences. Given $n$ points targeting the wrong distribution and quadratictime, Stein kernel thinning (SKT) returns $\sqrt{n}$ equal-weighted points with$\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\mathbb{P}$. Forlarger-scale compression tasks, low-rank SKT achieves the same feat insub-quadratic time using an adaptive low-rank debiasing procedure that may beof independent interest. For downstream tasks that support simplex orconstant-preserving weights, Stein recombination and Stein Cholesky achieveeven greater parsimony, matching the guarantees of SKT with as few as$\text{poly-log}(n)$ weighted points. Underlying these advances are newguarantees for the quality of simplex-weighted coresets, the spectral decay ofkernel matrices, and the covering numbers of Stein kernel Hilbert spaces. Inour experiments, our techniques provide succinct and accurate posteriorsummaries while overcoming biases due to burn-in, approximate Markov chainMonte Carlo, and tempering.</description><author>Lingxiao Li, Raaz Dwivedi, Lester Mackey</author><pubDate>Wed, 31 Jul 2024 20:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12290v3</guid></item><item><title>Sentence-wise Speech Summarization: Task, Datasets, and End-to-End Modeling with LM Knowledge Distillation</title><link>http://arxiv.org/abs/2408.00205v1</link><description>This paper introduces a novel approach called sentence-wise speechsummarization (Sen-SSum), which generates text summaries from a spoken documentin a sentence-by-sentence manner. Sen-SSum combines the real-time processing ofautomatic speech recognition (ASR) with the conciseness of speechsummarization. To explore this approach, we present two datasets for Sen-SSum:Mega-SSum and CSJ-SSum. Using these datasets, our study evaluates two types ofTransformer-based models: 1) cascade models that combine ASR and strong textsummarization models, and 2) end-to-end (E2E) models that directly convertspeech into a text summary. While E2E models are appealing to developcompute-efficient models, they perform worse than cascade models. Therefore, wepropose knowledge distillation for E2E models using pseudo-summaries generatedby the cascade models. Our experiments show that this proposed knowledgedistillation effectively improves the performance of the E2E model on bothdatasets.</description><author>Kohei Matsuura, Takanori Ashihara, Takafumi Moriya, Masato Mimura, Takatomo Kano, Atsunori Ogawa, Marc Delcroix</author><pubDate>Thu, 01 Aug 2024 00:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00205v1</guid></item><item><title>From Attributes to Natural Language: A Survey and Foresight on Text-based Person Re-identification</title><link>http://arxiv.org/abs/2408.00096v1</link><description>Text-based person re-identification (Re-ID) is a challenging topic in thefield of complex multimodal analysis, its ultimate aim is to recognize specificpedestrians by scrutinizing attributes/natural language descriptions. Despitethe wide range of applicable areas such as security surveillance, videoretrieval, person tracking, and social media analytics, there is a notableabsence of comprehensive reviews dedicated to summarizing the text-based personRe-ID from a technical perspective. To address this gap, we propose tointroduce a taxonomy spanning Evaluation, Strategy, Architecture, andOptimization dimensions, providing a comprehensive survey of the text-basedperson Re-ID task. We start by laying the groundwork for text-based personRe-ID, elucidating fundamental concepts related to attribute/naturallanguage-based identification. Then a thorough examination of existingbenchmark datasets and metrics is presented. Subsequently, we further delveinto prevalent feature extraction strategies employed in text-based personRe-ID research, followed by a concise summary of common network architectureswithin the domain. Prevalent loss functions utilized for model optimization andmodality alignment in text-based person Re-ID are also scrutinized. Toconclude, we offer a concise summary of our findings, pinpointing challenges intext-based person Re-ID. In response to these challenges, we outline potentialavenues for future open-set text-based person Re-ID and present a baselinearchitecture for text-based pedestrian image generation-guidedre-identification(TBPGR).</description><author>Fanzhi Jiang, Su Yang, Mark W. Jones, Liumei Zhang</author><pubDate>Wed, 31 Jul 2024 18:16:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00096v1</guid></item><item><title>Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models</title><link>http://arxiv.org/abs/2408.01308v1</link><description>Learning token embeddings based on token co-occurrence statistics has proveneffective for both pre-training and fine-tuning in natural language processing.However, recent studies have pointed out the distribution of learned embeddingsdegenerates into anisotropy, and even pre-trained language models (PLMs) sufferfrom a loss of semantics-related information in embeddings for low-frequencytokens. This study first analyzes fine-tuning dynamics of a PLM, BART-large,and demonstrates its robustness against degeneration. On the basis of thisfinding, we propose DefinitionEMB, a method that utilizes definitions toconstruct isotropically distributed and semantics-related token embeddings forPLMs while maintaining original robustness during fine-tuning. Our experimentsdemonstrate the effectiveness of leveraging definitions from Wiktionary toconstruct such embeddings for RoBERTa-base and BART-large. Furthermore, theconstructed embeddings for low-frequency tokens improve the performance ofthese models across various GLUE and four text summarization datasets.</description><author>Ying Zhang, Dongyuan Li, Manabu Okumura</author><pubDate>Fri, 02 Aug 2024 15:00:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01308v1</guid></item><item><title>High-Throughput Phenotyping of Clinical Text Using Large Language Models</title><link>http://arxiv.org/abs/2408.01214v1</link><description>High-throughput phenotyping automates the mapping of patient signs tostandardized ontology concepts and is essential for precision medicine. Thisstudy evaluates the automation of phenotyping of clinical summaries from theOnline Mendelian Inheritance in Man (OMIM) database using large languagemodels. Due to their rich phenotype data, these summaries can be surrogates forphysician notes. We conduct a performance comparison of GPT-4 andGPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo inidentifying, categorizing, and normalizing signs, achieving concordance withmanual annotators comparable to inter-rater agreement. Despite some limitationsin sign normalization, the extensive pre-training of GPT-4 results in highperformance and generalizability across several phenotyping tasks whileobviating the need for manually annotated training data. Large language modelsare expected to be the dominant method for automating high-throughputphenotyping of clinical text.</description><author>Daniel B. Hier, S. Ilyas Munzir, Anne Stahlfeld, Tayo Obafemi-Ajayi, Michael D. Carrithers</author><pubDate>Fri, 02 Aug 2024 12:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01214v1</guid></item><item><title>Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs</title><link>http://arxiv.org/abs/2408.01008v1</link><description>In recent years, Large Language Models (LLMs) have demonstrated remarkablecapabilities across a wide range of natural language processing (NLP) tasks,such as question-answering, sentiment analysis, text summarization, and machinetranslation. However, the ever-growing complexity of LLMs demands immensecomputational resources, hindering the broader research and application ofthese models. To address this, various parameter-efficient fine-tuningstrategies, such as Low-Rank Approximation (LoRA) and Adapters, have beendeveloped. Despite their potential, these methods often face limitations incompressibility. Specifically, LoRA struggles to scale effectively with theincreasing number of trainable parameters in modern large scale LLMs.Additionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), whichutilizes tensor train decomposition, has not yet achieved the level ofcompression necessary for fine-tuning very large scale models with limitedresources. This paper introduces Tensor Train Low-Rank Approximation (TT-LoRA),a novel parameter-efficient fine-tuning (PEFT) approach that extends LoRETTAwith optimized tensor train (TT) decomposition integration. By eliminatingAdapters and traditional LoRA-based structures, TT-LoRA achieves greater modelcompression without compromising downstream task performance, along withreduced inference latency and computational overhead. We conduct an exhaustiveparameter search to establish benchmarks that highlight the trade-off betweenmodel compression and performance. Our results demonstrate significantcompression of LLMs while maintaining comparable performance to larger models,facilitating their deployment on resource-constraint platforms.</description><author>Afia Anjum, Maksim E. Eren, Ismael Boureima, Boian Alexandrov, Manish Bhattarai</author><pubDate>Fri, 02 Aug 2024 04:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01008v1</guid></item><item><title>Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs</title><link>http://arxiv.org/abs/2408.02571v1</link><description>The emoticons are symbolic representations that generally accompany thetextual content to visually enhance or summarize the true intention of awritten message. Although widely utilized in the realm of social media, thecore semantics of these emoticons have not been extensively explored based onmultiple modalities. Incorporating textual and visual information within asingle message develops an advanced way of conveying information. Hence, thisresearch aims to analyze the relationship among sentences, visuals, andemoticons. For an orderly exposition, this paper initially provides a detailedexamination of the various techniques for extracting multimodal features,emphasizing the pros and cons of each method. Through conducting acomprehensive examination of several multimodal algorithms, with specificemphasis on the fusion approaches, we have proposed a novel contrastivelearning based multimodal architecture. The proposed model employs the jointtraining of dual-branch encoder along with the contrastive learning toaccurately map text and images into a common latent space. Our key finding isthat by integrating the principle of contrastive learning with that of theother two branches yields superior results. The experimental resultsdemonstrate that our suggested methodology surpasses existing multimodalapproaches in terms of accuracy and robustness. The proposed model attained anaccuracy of 91% and an MCC-score of 90% while assessing emoticons using theMultimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidencethat deep features acquired by contrastive learning are more efficient,suggesting that the proposed fusion technique also possesses stronggeneralisation capabilities for recognising emoticons across several modes.</description><author>Ananya Pandey, Dinesh Kumar Vishwakarma</author><pubDate>Mon, 05 Aug 2024 15:45:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02571v1</guid></item><item><title>MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance</title><link>http://arxiv.org/abs/2408.01869v1</link><description>In the era of Large Language Models (LLMs), given their remarkable textunderstanding and generation abilities, there is an unprecedented opportunityto develop new, LLM-based methods for trustworthy medical knowledge synthesis,extraction and summarization. This paper focuses on the problem ofPharmacovigilance (PhV), where the significance and challenges lie inidentifying Adverse Drug Events (ADEs) from diverse text sources, such asmedical literature, clinical notes, and drug labels. Unfortunately, this taskis hindered by factors including variations in the terminologies of drugs andoutcomes, and ADE descriptions often being buried in large amounts of narrativetext. We present MALADE, the first effective collaborative multi-agent systempowered by LLM with Retrieval Augmented Generation for ADE extraction from druglabel data. This technique involves augmenting a query to an LLM with relevantinformation extracted from text resources, and instructing the LLM to compose aresponse consistent with the augmented data. MALADE is a general LLM-agnosticarchitecture, and its unique capabilities are: (1) leveraging a variety ofexternal sources, such as medical literature, drug labels, and FDA tools (e.g.,OpenFDA drug information API), (2) extracting drug-outcome association in astructured format along with the strength of the association, and (3) providingexplanations for established associations. Instantiated with GPT-4 Turbo orGPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an AreaUnder ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Ourimplementation leverages the Langroid multi-agent LLM framework and can befound at https://github.com/jihyechoi77/malade.</description><author>Jihye Choi, Nils Palumbo, Prasad Chalasani, Matthew M. Engelhard, Somesh Jha, Anivarya Kumar, David Page</author><pubDate>Sat, 03 Aug 2024 22:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01869v1</guid></item><item><title>Graph Unfolding and Sampling for Transitory Video Summarization via Gershgorin Disc Alignment</title><link>http://arxiv.org/abs/2408.01859v1</link><description>User-generated videos (UGVs) uploaded from mobile phones to social mediasites like YouTube and TikTok are short and non-repetitive. We summarize atransitory UGV into several keyframes in linear time via fast graph samplingbased on Gershgorin disc alignment (GDA). Specifically, we first model asequence of $N$ frames in a UGV as an $M$-hop path graph $\mathcal{G}^o$ for $M\ll N$, where the similarity between two frames within $M$ time instants isencoded as a positive edge based on feature similarity. Towards efficientsampling, we then "unfold" $\mathcal{G}^o$ to a $1$-hop path graph$\mathcal{G}$, specified by a generalized graph Laplacian matrix $\mathcal{L}$,via one of two graph unfolding procedures with provable performance bounds. Weshow that maximizing the smallest eigenvalue $\lambda_{\min}(\mathbf{B})$ of acoefficient matrix $\mathbf{B} = \textit{diag}\left(\mathbf{h}\right) + \mu\mathcal{L}$, where $\mathbf{h}$ is the binary keyframe selection vector, isequivalent to minimizing a worst-case signal reconstruction error. We maximizeinstead the Gershgorin circle theorem (GCT) lower bound$\lambda^-_{\min}(\mathbf{B})$ by choosing $\mathbf{h}$ via a new fast graphsampling algorithm that iteratively aligns left-ends of Gershgorin discs forall graph nodes (frames). Extensive experiments on multiple short videodatasets show that our algorithm achieves comparable or better videosummarization performance compared to state-of-the-art methods, at asubstantially reduced complexity.</description><author>Sadid Sahami, Gene Cheung, Chia-Wen Lin</author><pubDate>Sat, 03 Aug 2024 20:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01859v1</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v7</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Tue, 06 Aug 2024 14:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v7</guid></item><item><title>Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization</title><link>http://arxiv.org/abs/2408.03149v1</link><description>The rapid increase in multimedia data has spurred advancements in MultimodalSummarization with Multimodal Output (MSMO), which aims to produce a multimodalsummary that integrates both text and relevant images. The inherentheterogeneity of content within multimodal inputs and outputs presents asignificant challenge to the execution of MSMO. Traditional approachestypically adopt a holistic perspective on coarse image-text data or individualvisual objects, overlooking the essential connections between objects and theentities they represent. To integrate the fine-grained entity knowledge, wepropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,building on BART, utilizes dual multimodal encoders with shared weights toprocess text-image and entity-image information concurrently. A gatingmechanism then combines visual data for enhanced textual summary generation,while image selection is refined through knowledge distillation from apre-trained vision-language model. Extensive experiments on public MSMO datasetvalidate the superiority of the EGMS method, which also prove the necessity toincorporate entity information into MSMO problem.</description><author>Yanghai Zhang, Ye Liu, Shiwei Wu, Kai Zhang, Xukai Liu, Qi Liu, Enhong Chen</author><pubDate>Tue, 06 Aug 2024 12:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03149v1</guid></item><item><title>L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification &amp; Summarization</title><link>http://arxiv.org/abs/2408.03033v1</link><description>This article details our participation (L3iTC) in the FinLLM Challenge Task2024, focusing on two key areas: Task 1, financial text classification, andTask 2, financial text summarization. To address these challenges, wefine-tuned several large language models (LLMs) to optimize performance foreach task. Specifically, we used 4-bit quantization and LoRA to determine whichlayers of the LLMs should be trained at a lower precision. This approach notonly accelerated the fine-tuning process on the training data provided by theorganizers but also enabled us to run the models on low GPU memory. Ourfine-tuned models achieved third place for the financial classification taskwith an F1-score of 0.7543 and secured sixth place in the financialsummarization task on the official test datasets.</description><author>Elvys Linhares Pontes, Carlos-Emiliano González-Gallardo, Mohamed Benjannet, Caryn Qu, Antoine Doucet</author><pubDate>Tue, 06 Aug 2024 08:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03033v1</guid></item><item><title>ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval</title><link>http://arxiv.org/abs/2408.02978v1</link><description>E-commerce is increasingly multimedia-enriched, with products exhibited in abroad-domain manner as images, short videos, or live stream promotions. Aunified and vectorized cross-domain production representation is essential. Dueto large intra-product variance and high inter-product similarity in thebroad-domain scenario, a visual-only representation is inadequate. WhileAutomatic Speech Recognition (ASR) text derived from the short or live-streamvideos is readily accessible, how to de-noise the excessively noisy text formultimodal representation learning is mostly untouched. We propose ASR-enhancedMultimodal Product Representation Learning (AMPere). In order to extractproduct-specific information from the raw ASR text, AMPere uses aneasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,together with visual data, is then fed into a multi-branch network to generatecompact multimodal embeddings. Extensive experiments on a large-scaletri-domain dataset verify the effectiveness of AMPere in obtaining a unifiedmultimodal product representation that clearly improves cross-domain productretrieval.</description><author>Ruixiang Zhao, Jian Jia, Yan Li, Xuehan Bai, Quan Chen, Han Li, Peng Jiang, Xirong Li</author><pubDate>Tue, 06 Aug 2024 06:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02978v1</guid></item><item><title>A Framework for Fine-Tuning LLMs using Heterogeneous Feedback</title><link>http://arxiv.org/abs/2408.02861v1</link><description>Large language models (LLMs) have been applied to a wide range of tasks,including text summarization, web navigation, and chatbots. They havebenefitted from supervised fine-tuning (SFT) and reinforcement learning fromhuman feedback (RLHF) following an unsupervised pretraining. These datasets canbe difficult to collect, limited in scope, and vary in sample quality.Additionally, datasets can vary extensively in supervision format, fromnumerical to binary as well as multi-dimensional with many different values. Wepresent a framework for fine-tuning LLMs using heterogeneous feedback, whichhas two main components. First, we combine the heterogeneous feedback data intoa single supervision format, compatible with methods like SFT and RLHF. Next,given this unified feedback dataset, we extract a high-quality and diversesubset to obtain performance increases potentially exceeding the full dataset.We conduct extensive experiments to understand the effectiveness of thesetechniques for incorporating heterogeneous feedback, and demonstrateimprovements from using a high-quality and diverse subset of the data. We findthat our framework is able to improve models in multiple areas simultaneously,such as in instruction following and bias reduction.</description><author>Ryan Aponte, Ryan A. Rossi, Shunan Guo, Franck Dernoncourt, Tong Yu, Xiang Chen, Subrata Mitra, Nedim Lipka</author><pubDate>Mon, 05 Aug 2024 23:20:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02861v1</guid></item><item><title>'Finance Wizard' at the FinLLM Challenge Task: Financial Text Summarization</title><link>http://arxiv.org/abs/2408.03762v1</link><description>This paper presents our participation under the team name `Finance Wizard' inthe FinNLP-AgentScen 2024 shared task #2: Financial Text Summarization. Itdocuments our pipeline approach of fine-tuning a foundation model into atask-specific model for Financial Text Summarization. It involves (1) adaptingLlama3 8B, a foundation model, to the Finance domain via continuedpre-training, (2) multi-task instruction-tuning to further equip the model withmore finance-related capabilities, (3) finally fine-tuning the model into atask-specific `expert'. Our model, FinLlama3\_sum, yielded commendable results,securing the third position in its category with a ROUGE-1 score of 0.521.</description><author>Meisin Lee, Soon Lay-Ki</author><pubDate>Wed, 07 Aug 2024 13:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03762v1</guid></item><item><title>MMSummary: Multimodal Summary Generation for Fetal Ultrasound Video</title><link>http://arxiv.org/abs/2408.03761v1</link><description>We present the first automated multimodal summary generation system,MMSummary, for medical imaging video, particularly with a focus on fetalultrasound analysis. Imitating the examination process performed by a humansonographer, MMSummary is designed as a three-stage pipeline, progressing fromkeyframe detection to keyframe captioning and finally anatomy segmentation andmeasurement. In the keyframe detection stage, an innovative automated workflowis proposed to progressively select a concise set of keyframes, preservingsufficient video information without redundancy. Subsequently, we adapt a largelanguage model to generate meaningful captions for fetal ultrasound keyframesin the keyframe captioning stage. If a keyframe is captioned as fetal biometry,the segmentation and measurement stage estimates biometric parameters bysegmenting the region of interest according to the textual prior. The MMSummarysystem provides comprehensive summaries for fetal ultrasound examinations andbased on reported experiments is estimated to reduce scanning time byapproximately 31.5%, thereby suggesting the potential to enhance clinicalworkflow efficiency.</description><author>Xiaoqing Guo, Qianhui Men, J. Alison Noble</author><pubDate>Wed, 07 Aug 2024 13:30:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03761v1</guid></item><item><title>AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations</title><link>http://arxiv.org/abs/2407.16010v2</link><description>For many use-cases, it is often important to explain the prediction of ablack-box model by identifying the most influential training data samples.Existing approaches lack customization for user intent and often provide ahomogeneous set of explanation samples, failing to reveal the model's reasoningfrom different angles. In this paper, we propose AIDE, an approach for providing antithetical (i.e.,contrastive), intent-based, diverse explanations for opaque and complex models.AIDE distinguishes three types of explainability intents: interpreting acorrect, investigating a wrong, and clarifying an ambiguous prediction. Foreach intent, AIDE selects an appropriate set of influential training samplesthat support or oppose the prediction either directly or by contrast. Toprovide a succinct summary, AIDE uses diversity-aware sampling to avoidredundancy and increase coverage of the training data. We demonstrate the effectiveness of AIDE on image and text classificationtasks, in three ways: quantitatively, assessing correctness and continuity;qualitatively, comparing anecdotal evidence from AIDE and other example-basedapproaches; and via a user study, evaluating multiple aspects of AIDE. Theresults show that AIDE addresses the limitations of existing methods andexhibits desirable traits for an explainability method.</description><author>Ikhtiyor Nematov, Dimitris Sacharidis, Tomer Sagi, Katja Hose</author><pubDate>Thu, 08 Aug 2024 09:12:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16010v2</guid></item><item><title>A Review of Hybrid and Ensemble in Deep Learning for Natural Language Processing</title><link>http://arxiv.org/abs/2312.05589v2</link><description>This review presents a comprehensive exploration of hybrid and ensemble deeplearning models within Natural Language Processing (NLP), shedding light ontheir transformative potential across diverse tasks such as Sentiment Analysis,Named Entity Recognition, Machine Translation, Question Answering, TextClassification, Generation, Speech Recognition, Summarization, and LanguageModeling. The paper systematically introduces each task, delineates keyarchitectures from Recurrent Neural Networks (RNNs) to Transformer-based modelslike BERT, and evaluates their performance, challenges, and computationaldemands. The adaptability of ensemble techniques is emphasized, highlightingtheir capacity to enhance various NLP applications. Challenges inimplementation, including computational overhead, overfitting, and modelinterpretation complexities, are addressed alongside the trade-off betweeninterpretability and performance. Serving as a concise yet invaluable guide,this review synthesizes insights into tasks, architectures, and challenges,offering a holistic perspective for researchers and practitioners aiming toadvance language-driven applications through ensemble deep learning in NLP.</description><author>Jianguo Jia, Wen Liang, Youzhi Liang</author><pubDate>Thu, 08 Aug 2024 07:15:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05589v2</guid></item><item><title>Tell Me What's Next: Textual Foresight for Generic UI Representations</title><link>http://arxiv.org/abs/2406.07822v2</link><description>Mobile app user interfaces (UIs) are rich with action, text, structure, andimage content that can be utilized to learn generic UI representations fortasks like automating user commands, summarizing content, and evaluating theaccessibility of user interfaces. Prior work has learned strong visualrepresentations with local or global captioning losses, but fails to retainboth granularities. To combat this, we propose Textual Foresight, a novelpretraining objective for learning UI screen representations. Textual Foresightgenerates global text descriptions of future UI states given a current UI andlocal action taken. Our approach requires joint reasoning over elements andentire screens, resulting in improved UI features: on generation tasks, UIagents trained with Textual Foresight outperform state-of-the-art by 2% with28x fewer images. We train with our newly constructed mobile app dataset,OpenApp, which results in the first public dataset for app UI representationlearning. OpenApp enables new baselines, and we find Textual Foresight improvesaverage task performance over them by 5.7% while having access to 2x less data.</description><author>Andrea Burns, Kate Saenko, Bryan A. Plummer</author><pubDate>Thu, 08 Aug 2024 02:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07822v2</guid></item><item><title>Text Clustering with LLM Embeddings</title><link>http://arxiv.org/abs/2403.15112v4</link><description>Text clustering is an important method for organising the increasing volumeof digital content, aiding in the structuring and discovery of hidden patternsin uncategorised data. The effectiveness of text clustering largely depends onthe selection of textual embeddings and clustering algorithms. This studyargues that recent advancements in large language models (LLMs) have thepotential to enhance this task. The research investigates how different textualembeddings, particularly those utilised in LLMs, and various clusteringalgorithms influence the clustering of text datasets. A series of experimentswere conducted to evaluate the impact of embeddings on clustering results, therole of dimensionality reduction through summarisation, and the adjustment ofmodel size. The findings indicate that LLM embeddings are superior at capturingsubtleties in structured language. OpenAI's GPT-3.5 Turbo model yields betterresults in three out of five clustering metrics across most tested datasets.Most LLM embeddings show improvements in cluster purity and provide a moreinformative silhouette score, reflecting a refined structural understanding oftext data compared to traditional methods. Among the more lightweight models,BERT demonstrates leading performance. Additionally, it was observed thatincreasing model dimensionality and employing summarisation techniques do notconsistently enhance clustering efficiency, suggesting that these strategiesrequire careful consideration for practical application. These resultshighlight a complex balance between the need for refined text representationand computational feasibility in text clustering applications. This studyextends traditional text clustering frameworks by integrating embeddings fromLLMs, offering improved methodologies and suggesting new avenues for futureresearch in various types of textual analysis.</description><author>Alina Petukhova, João P. Matos-Carvalho, Nuno Fachada</author><pubDate>Fri, 09 Aug 2024 16:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15112v4</guid></item><item><title>Efficient automatic segmentation for multi-level pulmonary arteries: The PARSE challenge</title><link>http://arxiv.org/abs/2304.03708v2</link><description>Efficient automatic segmentation of multi-level (i.e. main and branch)pulmonary arteries (PA) in CTPA images plays a significant role in clinicalapplications. However, most existing methods concentrate only on main PA orbranch PA segmentation separately and ignore segmentation efficiency. Besides,there is no public large-scale dataset focused on PA segmentation, which makesit highly challenging to compare the different methods. To benchmarkmulti-level PA segmentation algorithms, we organized the first\textbf{P}ulmonary \textbf{AR}tery \textbf{SE}gmentation (PARSE) challenge. Onthe one hand, we focus on both the main PA and the branch PA segmentation. Onthe other hand, for better clinical application, we assign the same scoreweight to segmentation efficiency (mainly running time and GPU memoryconsumption during inference) while ensuring PA segmentation accuracy. Wepresent a summary of the top algorithms and offer some suggestions forefficient and accurate multi-level PA automatic segmentation. We provide thePARSE challenge as open-access for the community to benchmark future algorithmdevelopments at \url{https://parse2022.grand-challenge.org/Parse2022/}.</description><author>Gongning Luo, Kuanquan Wang, Jun Liu, Shuo Li, Xinjie Liang, Xiangyu Li, Shaowei Gan, Wei Wang, Suyu Dong, Wenyi Wang, Pengxin Yu, Enyou Liu, Hongrong Wei, Na Wang, Jia Guo, Huiqi Li, Zhao Zhang, Ziwei Zhao, Na Gao, Nan An, Ashkan Pakzad, Bojidar Rangelov, Jiaqi Dou, Song Tian, Zeyu Liu, Yi Wang, Ampatishan Sivalingam, Kumaradevan Punithakumar, Zhaowen Qiu, Xin Gao</author><pubDate>Fri, 09 Aug 2024 17:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03708v2</guid></item><item><title>Quantitative Information Extraction from Humanitarian Documents</title><link>http://arxiv.org/abs/2408.04941v1</link><description>Humanitarian action is accompanied by a mass of reports, summaries, news, andother documents. To guide its activities, important information must be quicklyextracted from such free-text resources. Quantities, such as the number ofpeople affected, amount of aid distributed, or the extent of infrastructuredamage, are central to emergency response and anticipatory action. In thiswork, we contribute an annotated dataset for the humanitarian domain for theextraction of such quantitative information, along side its important context,including units it refers to, any modifiers, and the relevant event. Further,we develop a custom Natural Language Processing pipeline to extract thequantities alongside their units, and evaluate it in comparison to baseline andrecent literature. The proposed model achieves a consistent improvement in theperformance, especially in the documents pertaining to the Dominican Republicand select African countries. We make the dataset and code available to theresearch community to continue the improvement of NLP tools for thehumanitarian domain.</description><author>Daniele Liberatore, Kyriaki Kalimeri, Derya Sever, Yelena Mejova</author><pubDate>Fri, 09 Aug 2024 08:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04941v1</guid></item><item><title>Exploration of Masked and Causal Language Modelling for Text Generation</title><link>http://arxiv.org/abs/2405.12630v2</link><description>Large Language Models (LLMs) have revolutionised the field of NaturalLanguage Processing (NLP) and have achieved state-of-the-art performance inpractically every task in this field. However, the prevalent approach used intext generation, Causal Language Modelling (CLM), which generates textsequentially from left to right, inherently limits the freedom of the model,which does not decide when and where each token is generated. In contrast,Masked Language Modelling (MLM), primarily used for language understandingtasks, can generate tokens anywhere in the text and any order. This paperconducts an extensive comparison of MLM and CLM approaches for text generationtasks. To do so, we pre-train several language models of comparable sizes onthree different datasets, namely 1) medical discharge summaries, 2) movie plotsynopses, and 3) authorship verification datasets. To assess the quality of thegenerations, we first employ quantitative metrics and then perform aqualitative human evaluation to analyse coherence and grammatical correctness.In addition, we evaluate the usefulness of the generated texts by using them inthree different downstream tasks: 1) Entity Recognition, 2) TextClassification, and 3) Authorship Verification. The results show that MLMconsistently outperforms CLM in text generation across all datasets, withhigher quantitative scores and better coherence in the generated text. Thestudy also finds \textit{no strong correlation} between the quality of thegenerated text and the performance of the models in the downstream tasks. Withthis study, we show that MLM for text generation has great potential for futureresearch and provides direction for future studies in this area.</description><author>Nicolo Micheletti, Samuel Belkadi, Lifeng Han, Goran Nenadic</author><pubDate>Fri, 09 Aug 2024 00:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12630v2</guid></item><item><title>Tamil Language Computing: the Present and the Future</title><link>http://arxiv.org/abs/2407.08618v2</link><description>This paper delves into the text processing aspects of Language Computing,which enables computers to understand, interpret, and generate human language.Focusing on tasks such as speech recognition, machine translation, sentimentanalysis, text summarization, and language modelling, language computingintegrates disciplines including linguistics, computer science, and cognitivepsychology to create meaningful human-computer interactions. Recentadvancements in deep learning have made computers more accessible and capableof independent learning and adaptation. In examining the landscape of languagecomputing, the paper emphasises foundational work like encoding, where Tamiltransitioned from ASCII to Unicode, enhancing digital communication. Itdiscusses the development of computational resources, including raw data,dictionaries, glossaries, annotated data, and computational grammars, necessaryfor effective language processing. The challenges of linguistic annotation, thecreation of treebanks, and the training of large language models are alsocovered, emphasising the need for high-quality, annotated data and advancedlanguage models. The paper underscores the importance of building practicalapplications for languages like Tamil to address everyday communication needs,highlighting gaps in current technology. It calls for increased researchcollaboration, digitization of historical texts, and fostering digital usage toensure the comprehensive development of Tamil language processing, ultimatelyenhancing global communication and access to digital services.</description><author>Kengatharaiyer Sarveswaran</author><pubDate>Mon, 12 Aug 2024 09:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08618v2</guid></item><item><title>Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges</title><link>http://arxiv.org/abs/2405.15604v2</link><description>Text generation has become more accessible than ever, and the increasinginterest in these systems, especially those using large language models, hasspurred an increasing number of related publications. We provide a systematicliterature review comprising 244 selected papers between 2017 and 2024. Thisreview categorizes works in text generation into five main tasks: open-endedtext generation, summarization, translation, paraphrasing, and questionanswering. For each task, we review their relevant characteristics, sub-tasks,and specific challenges (e.g., missing datasets for multi-documentsummarization, coherence in story generation, and complex reasoning forquestion answering). Additionally, we assess current approaches for evaluatingtext generation systems and ascertain problems with current metrics. Ourinvestigation shows nine prominent challenges common to all tasks and sub-tasksin recent text generation publications: bias, reasoning, hallucinations,misuse, privacy, interpretability, transparency, datasets, and computing. Weprovide a detailed analysis of these challenges, their potential solutions, andwhich gaps still require further engagement from the community. This systematicliterature review targets two main audiences: early career researchers innatural language processing looking for an overview of the field and promisingresearch directions, as well as experienced researchers seeking a detailed viewof tasks, evaluation methodologies, open challenges, and recent mitigationstrategies.</description><author>Jonas Becker, Jan Philip Wahle, Bela Gipp, Terry Ruas</author><pubDate>Mon, 12 Aug 2024 08:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15604v2</guid></item><item><title>Creating Arabic LLM Prompts at Scale</title><link>http://arxiv.org/abs/2408.05882v1</link><description>The debut of chatGPT and BARD has popularized instruction following textgeneration using LLMs, where a user can interrogate an LLM using naturallanguage requests and obtain natural language answers that matches theirrequests. Training LLMs to respond in this manner requires a large number ofworked out examples of user requests (aka prompts) with corresponding goldresponses. In this paper, we introduce two methods for creating such promptsfor Arabic cheaply and quickly. The first methods entails automaticallytranslating existing prompt datasets from English, such as PromptSource andSuper-NaturalInstructions, and then using machine translation qualityestimation to retain high quality translations only. The second method involvescreating natural language prompts on top of existing Arabic NLP datasets. Usingthese two methods we were able to create more than 67.4 million Arabic promptsthat cover a variety of tasks including summarization, headline generation,grammar checking, open/closed question answering, creative writing, etc. Weshow that fine tuning an open 7 billion parameter large language model, namelybase Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameterinstruction tuned model, namely Llama3 70B, in handling Arabic prompts.</description><author>Abdelrahman El-Sheikh, Ahmed Elmogtaba, Kareem Darwish, Muhammad Elmallah, Ashraf Elneima, Hassan Sawaf</author><pubDate>Mon, 12 Aug 2024 00:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05882v1</guid></item><item><title>AutoTemplate: A Simple Recipe for Lexically Constrained Text Generation</title><link>http://arxiv.org/abs/2211.08387v2</link><description>Lexically constrained text generation is one of the constrained textgeneration tasks, which aims to generate text that covers all the givenconstraint lexicons. While the existing approaches tackle this problem using alexically constrained beam search algorithm or dedicated model usingnon-autoregressive decoding, there is a trade-off between the generated textquality and the hard constraint satisfaction. We introduce AutoTemplate, asimple yet effective lexically constrained text generation framework dividedinto template generation and lexicalization tasks. The template generation isto generate the text with the placeholders, and lexicalization replaces theminto the constraint lexicons to perform lexically constrained text generation.We conducted the experiments on two tasks: keywords-to-sentence generations andentity-guided summarization. Experimental results show that the AutoTemplateoutperforms the competitive baselines on both tasks while satisfying the hardlexical constraints. The code is available athttps://github.com/megagonlabs/autotemplate</description><author>Hayate Iso</author><pubDate>Fri, 09 Aug 2024 23:34:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08387v2</guid></item><item><title>A Novel Computational and Modeling Foundation for Automatic Coherence Assessment</title><link>http://arxiv.org/abs/2310.00598v2</link><description>Coherence is an essential property of well-written texts, that refers to theway textual units relate to one another. In the era of generative AI, coherenceassessment is essential for many NLP tasks; summarization, generation,long-form question-answering, and more. However, in NLP {coherence} is anill-defined notion, not having a formal definition or evaluation metrics, thatwould allow for large-scale automatic and systematic coherence assessment. Tobridge this gap, in this work we employ the formal linguistic definition of\citet{Reinhart:1980} of what makes a discourse coherent, consisting of threeconditions -- {\em cohesion, consistency} and {\em relevance} -- and formalizethese conditions as respective computational tasks. We hypothesize that (i) amodel trained on all of these tasks will learn the features required forcoherence detection, and that (ii) a joint model for all tasks will exceed theperformance of models trained on each task individually. On two benchmarks forcoherence scoring rated by humans, one containing 500 automatically-generatedshort stories and another containing 4k real-world texts, our experimentsconfirm that jointly training on the proposed tasks leads to better performanceon each task compared with task-specific models, and to better performance onassessing coherence overall, compared with strong baselines. We conclude thatthe formal and computational setup of coherence as proposed here provides asolid foundation for advanced methods of large-scale automatic assessment ofcoherence.</description><author>Aviya Maimon, Reut Tsarfaty</author><pubDate>Tue, 13 Aug 2024 13:19:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00598v2</guid></item><item><title>Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</title><link>http://arxiv.org/abs/2407.18003v3</link><description>Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,have revolutionized various industries with their advanced languagecomprehension. However, their efficiency is challenged by the Transformerarchitecture' s struggle with handling long texts. KV-Cache has emerged as apivotal solution to this issue, converting the time complexity of tokengeneration from quadratic to linear, albeit with increased GPU memory overheadproportional to conversation length. With the development of the LLM communityand academia, various KV-Cache compression methods have been proposed. In thisreview, we dissect the various properties of KV-Cache and elaborate on variousmethods currently used to optimize the KV-Cache space usage of LLMs. Thesemethods span the pre-training phase, deployment phase, and inference phase, andwe summarize the commonalities and differences among these methods.Additionally, we list some metrics for evaluating the long-text capabilities oflarge language models, from both efficiency and capability perspectives. Ourreview thus sheds light on the evolving landscape of LLM optimization, offeringinsights into future advancements in this dynamic field.</description><author>Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao</author><pubDate>Tue, 13 Aug 2024 09:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18003v3</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v8</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Fri, 09 Aug 2024 14:48:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v8</guid></item><item><title>Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey</title><link>http://arxiv.org/abs/2408.07583v1</link><description>With significant advancements in Transformers LLMs, NLP has extended itsreach into many research fields due to its enhanced capabilities in textgeneration and user interaction. One field benefiting greatly from theseadvancements is cybersecurity. In cybersecurity, many parameters that need tobe protected and exchanged between senders and receivers are in the form oftext and tabular data, making NLP a valuable tool in enhancing the securitymeasures of communication protocols. This survey paper provides a comprehensiveanalysis of the utilization of Transformers and LLMs in cyber-threat detectionsystems. The methodology of paper selection and bibliometric analysis isoutlined to establish a rigorous framework for evaluating existing research.The fundamentals of Transformers are discussed, including backgroundinformation on various cyber-attacks and datasets commonly used in this field.The survey explores the application of Transformers in IDSs, focusing ondifferent architectures such as Attention-based models, LLMs like BERT and GPT,CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.Furthermore, it explores the diverse environments and applications whereTransformers and LLMs-based IDS have been implemented, including computernetworks, IoT devices, critical infrastructure protection, cloud computing,SDN, as well as in autonomous vehicles. The paper also addresses researchchallenges and future directions in this area, identifying key issues such asinterpretability, scalability, and adaptability to evolving threats, and more.Finally, the conclusion summarizes the findings and highlights the significanceof Transformers and LLMs in enhancing cyber-threat detection capabilities,while also outlining potential avenues for further research and development.</description><author>Hamza Kheddar</author><pubDate>Wed, 14 Aug 2024 14:28:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07583v1</guid></item><item><title>$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation</title><link>http://arxiv.org/abs/2402.19457v3</link><description>Assessing the quality of summarizers poses significant challenges. Inresponse, we propose a novel task-oriented evaluation approach that assessessummarizers based on their capacity to produce summaries that are useful fordownstream tasks, while preserving task outcomes. We theoretically establish adirect relationship between the resulting error probability of these tasks andthe mutual information between source texts and generated summaries. Weintroduce $\texttt{COSMIC}$ as a practical implementation of this metric,demonstrating its strong correlation with human judgment-based metrics and itseffectiveness in predicting downstream task performance. Comparative analysesagainst established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$highlight the competitive performance of $\texttt{COSMIC}$.</description><author>Maxime Darrin, Philippe Formont, Jackie Chi Kit Cheung, Pablo Piantanida</author><pubDate>Wed, 14 Aug 2024 14:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19457v3</guid></item><item><title>MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL</title><link>http://arxiv.org/abs/2408.07930v1</link><description>Recent In-Context Learning based methods have achieved remarkable success inText-to-SQL task. However, there is still a large gap between the performanceof these models and human performance on datasets with complex database schemaand difficult questions, such as BIRD. Besides, existing work has neglected tosupervise intermediate steps when solving questions iteratively with questiondecomposition methods, and the schema linking methods used in these works arevery rudimentary. To address these issues, we propose MAG-SQL, a multi-agentgenerative approach with soft schema linking and iterative Sub-SQL refinement.In our framework, an entity-based method with tables' summary is used to selectthe columns in database, and a novel targets-conditions decomposition method isintroduced to decompose those complex questions. Additionally, we build aiterative generating module which includes a Sub-SQL Generator and Sub-SQLRefiner, introducing external oversight for each step of generation. Through aseries of ablation studies, the effectiveness of each agent in our frameworkhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQLachieves an execution accuracy of 61.08\%, compared to the baseline accuracy of46.35\% for vanilla GPT-4 and the baseline accuracy of 57.56\% for MAC-SQL.Besides, our approach makes similar progress on Spider.</description><author>Wenxuan Xie, Gaochen Wu, Bowen Zhou</author><pubDate>Thu, 15 Aug 2024 04:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07930v1</guid></item><item><title>An Efficient and Explanatory Image and Text Clustering System with Multimodal Autoencoder Architecture</title><link>http://arxiv.org/abs/2408.07791v1</link><description>We demonstrate the efficiencies and explanatory abilities of extensions tothe common tools of Autoencoders and LLM interpreters, in the novel context ofcomparing different cultural approaches to the same international news event.We develop a new Convolutional-Recurrent Variational Autoencoder (CRVAE) modelthat extends the modalities of previous CVAE models, by using fully-connectedlatent layers to embed in parallel the CNN encodings of video frames, togetherwith the LSTM encodings of their related text derived from audio. Weincorporate the model within a larger system that includes frame-captionalignment, latent space vector clustering, and a novel LLM-based clusterinterpreter. We measure, tune, and apply this system to the task of summarizinga video into three to five thematic clusters, with each theme described by tenLLM-produced phrases. We apply this system to two news topics, COVID-19 and theWinter Olympics, and five other topics are in progress.</description><author>Tiancheng Shi, Yuanchen Wei, John R. Kender</author><pubDate>Wed, 14 Aug 2024 20:03:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07791v1</guid></item><item><title>MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL</title><link>http://arxiv.org/abs/2408.07930v2</link><description>Recent In-Context Learning based methods have achieved remarkable success inText-to-SQL task. However, there is still a large gap between the performanceof these models and human performance on datasets with complex database schemaand difficult questions, such as BIRD. Besides, existing work has neglected tosupervise intermediate steps when solving questions iteratively with questiondecomposition methods, and the schema linking methods used in these works arevery rudimentary. To address these issues, we propose MAG-SQL, a multi-agentgenerative approach with soft schema linking and iterative Sub-SQL refinement.In our framework, an entity-based method with tables' summary is used to selectthe columns in database, and a novel targets-conditions decomposition method isintroduced to decompose those complex questions. Additionally, we build aiterative generating module which includes a Sub-SQL Generator and Sub-SQLRefiner, introducing external oversight for each step of generation. Through aseries of ablation studies, the effectiveness of each agent in our frameworkhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQLachieves an execution accuracy of 61.08%, compared to the baseline accuracy of46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.Besides, our approach makes similar progress on Spider.</description><author>Wenxuan Xie, Gaochen Wu, Bowen Zhou</author><pubDate>Fri, 16 Aug 2024 02:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07930v2</guid></item><item><title>Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey</title><link>http://arxiv.org/abs/2406.08068v2</link><description>Compared to traditional sentiment analysis, which only considers text,multimodal sentiment analysis needs to consider emotional signals frommultimodal sources simultaneously and is therefore more consistent with the wayhow humans process sentiment in real-world scenarios. It involves processingemotional information from various sources such as natural language, images,videos, audio, physiological signals, etc. However, although other modalitiesalso contain diverse emotional cues, natural language usually contains richercontextual information and therefore always occupies a crucial position inmultimodal sentiment analysis. The emergence of ChatGPT has opened up immensepotential for applying large language models (LLMs) to text-centric multimodaltasks. However, it is still unclear how existing LLMs can adapt better totext-centric multimodal sentiment analysis tasks. This survey aims to (1)present a comprehensive review of recent research in text-centric multimodalsentiment analysis tasks, (2) examine the potential of LLMs for text-centricmultimodal sentiment analysis, outlining their approaches, advantages, andlimitations, (3) summarize the application scenarios of LLM-based multimodalsentiment analysis technology, and (4) explore the challenges and potentialresearch directions for multimodal sentiment analysis in the future.</description><author>Hao Yang, Yanyan Zhao, Yang Wu, Shilong Wang, Tian Zheng, Hongbo Zhang, Zongyang Ma, Wanxiang Che, Bing Qin</author><pubDate>Fri, 16 Aug 2024 10:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08068v2</guid></item><item><title>Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models</title><link>http://arxiv.org/abs/2408.10124v1</link><description>Molecular property prediction is a crucial foundation for drug discovery. Inrecent years, pre-trained deep learning models have been widely applied to thistask. Some approaches that incorporate prior biological domain knowledge intothe pre-training framework have achieved impressive results. However, thesemethods heavily rely on biochemical experts, and retrieving and summarizingvast amounts of domain knowledge literature is both time-consuming andexpensive. Large Language Models (LLMs) have demonstrated remarkableperformance in understanding and efficiently providing general knowledge.Nevertheless, they occasionally exhibit hallucinations and lack precision ingenerating domain-specific knowledge. Conversely, Domain-specific Small Models(DSMs) possess rich domain knowledge and can accurately calculate moleculardomain-related metrics. However, due to their limited model size and singularfunctionality, they lack the breadth of knowledge necessary for comprehensiverepresentation learning. To leverage the advantages of both approaches inmolecular property prediction, we propose a novel Molecular Graphrepresentation learning framework that integrates Large language models andDomain-specific small models (MolGraph-LarDo). Technically, we design atwo-stage prompt strategy where DSMs are introduced to calibrate the knowledgeprovided by LLMs, enhancing the accuracy of domain-specific information andthus enabling LLMs to generate more precise textual descriptions for molecularsamples. Subsequently, we employ a multi-modal alignment method to coordinatevarious modalities, including molecular graphs and their correspondingdescriptive texts, to guide the pre-training of molecular representations.Extensive experiments demonstrate the effectiveness of the proposed method.</description><author>Tianyu Zhang, Yuxiang Ren, Chengbin Hou, Hairong Lv, Xuegong Zhang</author><pubDate>Mon, 19 Aug 2024 16:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10124v1</guid></item><item><title>GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization</title><link>http://arxiv.org/abs/2408.10115v1</link><description>Pre-trained language models are increasingly being used in multi-documentsummarization tasks. However, these models need large-scale corpora forpre-training and are domain-dependent. Other non-neural unsupervisedsummarization approaches mostly rely on key sentence extraction, which can leadto information loss. To address these challenges, we propose a lightweight yeteffective unsupervised approach called GLIMMER: a Graph and LexIcal featuresbased unsupervised Multi-docuMEnt summaRization approach. It first constructs asentence graph from the source documents, then automatically identifiessemantic clusters by mining low-level features from raw texts, therebyimproving intra-cluster correlation and the fluency of generated sentences.Finally, it summarizes clusters into natural sentences. Experiments conductedon Multi-News, Multi-XScience and DUC-2004 demonstrate that our approachoutperforms existing unsupervised approaches. Furthermore, it surpassesstate-of-the-art pre-trained multi-document summarization models (e.g. PEGASUSand PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally,human evaluations indicate that summaries generated by GLIMMER achieve highreadability and informativeness scores. Our code is available athttps://github.com/Oswald1997/GLIMMER.</description><author>Ran Liu, Ming Liu, Min Yu, Jianguo Jiang, Gang Li, Dan Zhang, Jingyuan Li, Xiang Meng, Weiqing Huang</author><pubDate>Mon, 19 Aug 2024 16:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10115v1</guid></item><item><title>Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation</title><link>http://arxiv.org/abs/2408.09698v2</link><description>Recent advances in Large Language Models (LLMs) have demonstrated significantpotential in the field of Recommendation Systems (RSs). Most existing studieshave focused on converting user behavior logs into textual prompts andleveraging techniques such as prompt tuning to enable LLMs for recommendationtasks. Meanwhile, research interest has recently grown in multimodalrecommendation systems that integrate data from images, text, and other sourcesusing modality fusion techniques. This introduces new challenges to theexisting LLM-based recommendation paradigm which relies solely on text modalityinformation. Moreover, although Multimodal Large Language Models (MLLMs)capable of processing multi-modal inputs have emerged, how to equip MLLMs withmulti-modal recommendation capabilities remains largely unexplored. To thisend, in this paper, we propose the Multimodal Large Language Model-enhancedMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamicuser preference, we design a two-stage user preference summarization method.Specifically, we first utilize an MLLM-based item-summarizer to extract imagefeature given an item and convert the image into text. Then, we employ arecurrent user preference summarization generation paradigm to capture thedynamic changes in user preferences based on an LLM-based user-summarizer.Finally, to enable the MLLM for multi-modal recommendation task, we propose tofine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)techniques. Extensive evaluations across various datasets validate theeffectiveness of MLLM-MSR, showcasing its superior ability to capture and adaptto the evolving dynamics of user preferences.</description><author>Yuyang Ye, Zhi Zheng, Yishan Shen, Tianshu Wang, Hengruo Zhang, Peijun Zhu, Runlong Yu, Kai Zhang, Hui Xiong</author><pubDate>Tue, 20 Aug 2024 16:09:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09698v2</guid></item><item><title>Language-Guided Self-Supervised Video Summarization Using Text Semantic Matching Considering the Diversity of the Video</title><link>http://arxiv.org/abs/2405.08890v2</link><description>Current video summarization methods rely heavily on supervised computervision techniques, which demands time-consuming and subjective manualannotations. To overcome these limitations, we investigated self-supervisedvideo summarization. Inspired by the success of Large Language Models (LLMs),we explored the feasibility in transforming the video summarization task into aNatural Language Processing (NLP) task. By leveraging the advantages of LLMs incontext understanding, we aim to enhance the effectiveness of self-supervisedvideo summarization. Our method begins by generating captions for individualvideo frames, which are then synthesized into text summaries by LLMs.Subsequently, we measure semantic distance between the captions and the textsummary. Notably, we propose a novel loss function to optimize our modelaccording to the diversity of the video. Finally, the summarized video can begenerated by selecting the frames with captions similar to the text summary.Our method achieves state-of-the-art performance on the SumMe dataset in rankcorrelation coefficients. In addition, our method has a novel feature of beingable to achieve personalized summarization.</description><author>Tomoya Sugihara, Shuntaro Masuda, Ling Xiao, Toshihiko Yamasaki</author><pubDate>Tue, 20 Aug 2024 14:19:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08890v2</guid></item><item><title>Towards Efficient Large Language Models for Scientific Text: A Review</title><link>http://arxiv.org/abs/2408.10729v1</link><description>Large language models (LLMs) have ushered in a new era for processing complexinformation in various fields, including science. The increasing amount ofscientific literature allows these models to acquire and understand scientificknowledge effectively, thus improving their performance in a wide range oftasks. Due to the power of LLMs, they require extremely expensive computationalresources, intense amounts of data, and training time. Therefore, in recentyears, researchers have proposed various methodologies to make scientific LLMsmore affordable. The most well-known approaches align in two directions. It canbe either focusing on the size of the models or enhancing the quality of data.To date, a comprehensive review of these two families of methods has not yetbeen undertaken. In this paper, we (I) summarize the current advances in theemerging abilities of LLMs into more accessible AI solutions for science, and(II) investigate the challenges and opportunities of developing affordablesolutions for scientific domains using LLMs.</description><author>Huy Quoc To, Ming Liu, Guangyan Huang</author><pubDate>Tue, 20 Aug 2024 10:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10729v1</guid></item><item><title>One Law, Many Languages: Benchmarking Multilingual Legal Reasoning for Judicial Support</title><link>http://arxiv.org/abs/2306.09237v3</link><description>Recent strides in Large Language Models (LLMs) have saturated many NaturalLanguage Processing (NLP) benchmarks, emphasizing the need for more challengingones to properly assess LLM capabilities. However, domain-specific andmultilingual benchmarks are rare because they require in-depth expertise todevelop. Still, most public models are trained predominantly on Englishcorpora, while other languages remain understudied, particularly for practicaldomain-specific NLP tasks. In this work, we introduce a novel NLP benchmark forthe legal domain that challenges LLMs in five key dimensions: processing\emph{long documents} (up to 50K tokens), using \emph{domain-specificknowledge} (embodied in legal texts), \emph{multilingual} understanding(covering five languages), \emph{multitasking} (comprising legaldocument-to-document Information Retrieval, Court View Generation, LeadingDecision Summarization, Citation Extraction, and eight challenging TextClassification tasks) and \emph{reasoning} (comprising especially Court ViewGeneration, but also the Text Classification tasks). Our benchmark containsdiverse datasets from the Swiss legal system, allowing for a comprehensivestudy of the underlying non-English, inherently multilingual legal system.Despite the large size of our datasets (some with hundreds of thousands ofexamples), existing publicly available multilingual models struggle with mosttasks, even after extensive in-domain pre-training and fine-tuning. We publishall resources (benchmark suite, pre-trained models, code) under permissive openCC BY-SA licenses.</description><author>Ronja Stern, Vishvaksenan Rasiah, Veton Matoshi, Srinanda Brügger Bose, Matthias Stürmer, Ilias Chalkidis, Daniel E. Ho, Joel Niklaus</author><pubDate>Wed, 21 Aug 2024 10:36:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09237v3</guid></item><item><title>V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning</title><link>http://arxiv.org/abs/2404.12353v2</link><description>Video summarization aims to create short, accurate, and cohesive summaries oflonger videos. Despite the existence of various video summarization datasets, anotable limitation is their limited amount of source videos, which hampers theeffective training of advanced large vision-language models (VLMs).Additionally, most existing datasets are created for video-to-videosummarization, overlooking the contemporary need for multimodal video contentsummarization. Recent efforts have been made to expand from unimodal tomultimodal video summarization, categorizing the task into three sub-tasksbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), anda combination of video and text summarization (V2VT). However, the textualsummaries in previous multimodal datasets are inadequate. To address theseissues, we introduce Instruct-V2Xum, a cross-modal video summarization datasetfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from40 to 940 seconds and an average summarization ratio of 16.39%. Each videosummary in Instruct-V2Xum is paired with a textual summary that referencesspecific frame indexes, facilitating the generation of aligned video andtextual summaries. In addition, we propose a new video summarization frameworknamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is thefirst framework that unifies different video summarization tasks into one largelanguage model's (LLM) text decoder and achieves task-controllable videosummarization with temporal prompts and task instructions. Experiments showthat V2Xum-LLaMA outperforms strong baseline models on multiple videosummarization tasks. Furthermore, we propose an enhanced evaluation metric forV2V and V2VT summarization tasks.</description><author>Hang Hua, Yunlong Tang, Chenliang Xu, Jiebo Luo</author><pubDate>Tue, 20 Aug 2024 23:47:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12353v2</guid></item><item><title>Controllable Text Generation for Large Language Models: A Survey</title><link>http://arxiv.org/abs/2408.12599v1</link><description>In Natural Language Processing (NLP), Large Language Models (LLMs) havedemonstrated high text generation quality. However, in real-world applications,LLMs must meet increasingly complex requirements. Beyond avoiding misleading orinappropriate content, LLMs are also expected to cater to specific user needs,such as imitating particular writing styles or generating text with poeticrichness. These varied demands have driven the development of Controllable TextGeneration (CTG) techniques, which ensure that outputs adhere to predefinedcontrol conditions--such as safety, sentiment, thematic consistency, andlinguistic style--while maintaining high standards of helpfulness, fluency, anddiversity. This paper systematically reviews the latest advancements in CTG for LLMs,offering a comprehensive definition of its core concepts and clarifying therequirements for control conditions and text quality. We categorize CTG tasksinto two primary types: content control and attribute control. The key methodsare discussed, including model retraining, fine-tuning, reinforcement learning,prompt engineering, latent space manipulation, and decoding-time intervention.We analyze each method's characteristics, advantages, and limitations,providing nuanced insights for achieving generation control. Additionally, wereview CTG evaluation methods, summarize its applications across domains, andaddress key challenges in current research, including reduced fluency andpracticality. We also propose several appeals, such as placing greater emphasison real-world applications in future research. This paper aims to offervaluable guidance to researchers and developers in the field. Our referencelist and Chinese version are open-sourced athttps://github.com/IAAR-Shanghai/CTGSurvey.</description><author>Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao Song, Jiawei Yang, Simin Niu, Jie Hu, Dan Liu, Shunyu Yao, Feiyu Xiong, Zhiyu Li</author><pubDate>Thu, 22 Aug 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12599v1</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v9</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Thu, 22 Aug 2024 10:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v9</guid></item><item><title>Preference-Guided Reflective Sampling for Aligning Language Models</title><link>http://arxiv.org/abs/2408.12163v1</link><description>Large language models (LLMs) are aligned with human preferences byreinforcement learning from human feedback (RLHF). Effective data sampling iscrucial for RLHF, as it determines the efficiency of model training, ensuringthat models learn from the informative samples. To achieve better datageneration, we propose a new sampling method called Preference-GuidedReflective Sampling (PRS). PRS frames the response generation as anoptimization process to the explicitly specified user preference described innatural language. It employs a tree-based generation framework to enable anefficient sampling process, which guides the direction of generation throughpreference and better explores the sampling space with adaptiveself-refinement. Notably, PRS can align LLMs to diverse preferences. We studypreference-controlled text generation for instruction following andkeyword-focused document summarization. Our findings indicate that PRS, acrossdifferent LLM policies, generates training data with much higher rewards thanstrong baselines. PRS also excels in post-RL training.</description><author>Hai Ye, Hwee Tou Ng</author><pubDate>Thu, 22 Aug 2024 07:18:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12163v1</guid></item><item><title>Unlocking Attributes' Contribution to Successful Camouflage: A Combined Textual and VisualAnalysis Strategy</title><link>http://arxiv.org/abs/2408.12086v1</link><description>In the domain of Camouflaged Object Segmentation (COS), despite continuousimprovements in segmentation performance, the underlying mechanisms ofeffective camouflage remain poorly understood, akin to a black box. To addressthis gap, we present the first comprehensive study to examine the impact ofcamouflage attributes on the effectiveness of camouflage patterns, offering aquantitative framework for the evaluation of camouflage designs. To supportthis analysis, we have compiled the first dataset comprising descriptions ofcamouflaged objects and their attribute contributions, termed COD-Text AndX-attributions (COD-TAX). Moreover, drawing inspiration from the hierarchicalprocess by which humans process information: from high-level textualdescriptions of overarching scenarios, through mid-level summaries of localareas, to low-level pixel data for detailed analysis. We have developed arobust framework that combines textual and visual information for the task ofCOS, named Attribution CUe Modeling with Eye-fixation Network (ACUMEN). ACUMENdemonstrates superior performance, outperforming nine leading methods acrossthree widely-used datasets. We conclude by highlighting key insights derivedfrom the attributes identified in our study. Code:https://github.com/lyu-yx/ACUMEN.</description><author>Hong Zhang, Yixuan Lyu, Qian Yu, Hanyang Liu, Huimin Ma, Ding Yuan, Yifan Yang</author><pubDate>Thu, 22 Aug 2024 02:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12086v1</guid></item><item><title>Intelligent OPC Engineer Assistant for Semiconductor Manufacturing</title><link>http://arxiv.org/abs/2408.12775v1</link><description>Advancements in chip design and manufacturing have enabled the processing ofcomplex tasks such as deep learning and natural language processing, paving theway for the development of artificial general intelligence (AGI). AI, on theother hand, can be leveraged to innovate and streamline semiconductortechnology from planning and implementation to manufacturing. In this paper, wepresent \textit{Intelligent OPC Engineer Assistant}, an AI/LLM-poweredmethodology designed to solve the core manufacturing-aware optimization problemknown as optical proximity correction (OPC). The methodology involves areinforcement learning-based OPC recipe search and a customized multi-modalagent system for recipe summarization. Experiments demonstrate that ourmethodology can efficiently build OPC recipes on various chip designs withspecially handled design topologies, a task that typically requires thefull-time effort of OPC engineers with years of experience.</description><author>Guojin Chen, Haoyu Yang, Haoxing Ren, Bei Yu</author><pubDate>Fri, 23 Aug 2024 00:49:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12775v1</guid></item><item><title>LaMSUM: Creating Extractive Summaries of User Generated Content using LLMs</title><link>http://arxiv.org/abs/2406.15809v2</link><description>Large Language Models (LLMs) have demonstrated impressive performance acrossa wide range of NLP tasks, including summarization. LLMs inherently produceabstractive summaries by paraphrasing the original text, while the generationof extractive summaries - selecting specific subsets from the original text -remains largely unexplored. LLMs have a limited context window size,restricting the amount of data that can be processed at once. We tackle thischallenge by introducing LaMSUM, a novel multi-level framework designed togenerate extractive summaries from large collections of user-generated textusing LLMs. LaMSUM integrates summarization with different voting methods toachieve robust summaries. Extensive evaluation using four popular LLMs (Llama3, Mixtral, Gemini, GPT-4o) demonstrates that LaMSUM outperformsstate-of-the-art extractive summarization methods. Overall, this workrepresents one of the first attempts to achieve extractive summarization byleveraging the power of LLMs, and is likely to spark further interest withinthe research community.</description><author>Garima Chhikara, Anurag Sharma, V. Gurucharan, Kripabandhu Ghosh, Abhijnan Chakraborty</author><pubDate>Thu, 22 Aug 2024 19:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.15809v2</guid></item><item><title>MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues</title><link>http://arxiv.org/abs/2408.14418v1</link><description>Automatic Speech Recognition (ASR) systems are pivotal in transcribing speechinto text, yet the errors they introduce can significantly degrade theperformance of downstream tasks like summarization. This issue is particularlypronounced in clinical dialogue summarization, a low-resource domain wheresupervised data for fine-tuning is scarce, necessitating the use of ASR modelsas black-box solutions. Employing conventional data augmentation for enhancingthe noise robustness of summarization models is not feasible either due to theunavailability of sufficient medical dialogue audio recordings andcorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,an approach for generating synthetic samples for data augmentation using LargeLanguage Models (LLMs). Specifically, we leverage the in-context learningcapabilities of LLMs and instruct them to generate ASR-like errors based on afew available medical dialogue examples with audio recordings. Experimentalresults show that LLMs can effectively model ASR noise, and incorporating thisnoisy data into the training process significantly improves the robustness andaccuracy of medical dialogue summarization systems. This approach addresses thechallenges of noisy ASR outputs in critical applications, offering a robustsolution to enhance the reliability of clinical dialogue summarization.</description><author>Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler</author><pubDate>Mon, 26 Aug 2024 17:04:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14418v1</guid></item><item><title>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</title><link>http://arxiv.org/abs/2406.10833v2</link><description>In many scientific fields, large language models (LLMs) have revolutionizedthe way text and other modalities of data (e.g., molecules and proteins) arehandled, achieving superior performance in various applications and augmentingthe scientific discovery process. Nevertheless, previous surveys on scientificLLMs often concentrate on one or two fields or a single modality. In thispaper, we aim to provide a more holistic view of the research landscape byunveiling cross-field and cross-modal connections between scientific LLMsregarding their architectures and pre-training techniques. To this end, wecomprehensively survey over 250 scientific LLMs, discuss their commonalitiesand differences, as well as summarize pre-training datasets and evaluationtasks for each field and modality. Moreover, we investigate how LLMs have beendeployed to benefit scientific discovery. Resources related to this survey areavailable at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.</description><author>Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han</author><pubDate>Mon, 26 Aug 2024 08:47:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10833v2</guid></item><item><title>Bidirectional Awareness Induction in Autoregressive Seq2Seq Models</title><link>http://arxiv.org/abs/2408.13959v1</link><description>Autoregressive Sequence-To-Sequence models are the foundation of many DeepLearning achievements in major research fields such as Vision and NaturalLanguage Processing. Despite that, they still present significant limitations.For instance, when errors occur in the early steps of the prediction, the wholeoutput is severely affected. Such reliance on previously predicted tokens andthe inherent computational unfriendliness of sequential algorithms, motivatedresearchers to explore different architectures and methods in the search forbidirectional approaches. In this work, we introduce the BidirectionalAwareness Induction (BAI), a training method that leverages a subset ofelements in the network, the Pivots, to perform bidirectional learning withoutbreaking the autoregressive constraints. To showcase its flexibility, we applythe method to three architectures, the Transformer, ExpansionNet v2 and GPT,then perform experiments over three tasks. Experimental results showcase BAI'seffectiveness on all selected tasks and architectures. In particular, weobserved an increase of up to 2.4 CIDEr in Image-Captioning, 4.96 BLEU inNeural Machine Translation, and 1.16 ROUGE in Text Summarization compared tothe respective baselines. Notably, BAI not only has a positive impact on modelstrained from scratch but on pre-trained models as well. Such an aspect,combined with the absence of architectural requirements synergizes well withthe current trend of LLMs.</description><author>Jia Cheng Hu, Roberto Cavicchioli, Alessandro Capotondi</author><pubDate>Sun, 25 Aug 2024 23:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13959v1</guid></item><item><title>QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs</title><link>http://arxiv.org/abs/2405.05109v2</link><description>Table summarization is a crucial task aimed at condensing information fromtabular data into concise and comprehensible textual summaries. However,existing approaches often fall short of adequately meeting users' informationand quality requirements and tend to overlook the complexities of real-worldqueries. In this paper, we propose a novel method to address these limitationsby introducing query-focused multi-table summarization. Our approach, whichcomprises a table serialization module, a summarization controller, and a largelanguage model (LLM), utilizes textual queries and multiple tables to generatequery-dependent table summaries tailored to users' information needs. Tofacilitate research in this area, we present a comprehensive datasetspecifically tailored for this task, consisting of 4909 query-summary pairs,each associated with multiple tables. Through extensive experiments using ourcurated dataset, we demonstrate the effectiveness of our proposed methodcompared to baseline approaches. Our findings offer insights into thechallenges of complex table reasoning for precise summarization, contributingto the advancement of research in query-focused multi-table summarization.</description><author>Weijia Zhang, Vaishali Pal, Jia-Hong Huang, Evangelos Kanoulas, Maarten de Rijke</author><pubDate>Sun, 25 Aug 2024 17:22:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05109v2</guid></item><item><title>Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data</title><link>http://arxiv.org/abs/2408.13833v1</link><description>Large language models (LLMs) have shown potential in biomedical applications,leading to efforts to fine-tune them on domain-specific data. However, theeffectiveness of this approach remains unclear. This study evaluates theperformance of biomedically fine-tuned LLMs against their general-purposecounterparts on a variety of clinical tasks. We evaluated their performance onclinical case challenges from the New England Journal of Medicine (NEJM) andthe Journal of the American Medical Association (JAMA) and on several clinicaltasks (e.g., information extraction, document summarization, and clinicalcoding). Using benchmarks specifically chosen to be likely outside thefine-tuning datasets of biomedical models, we found that biomedical LLMs mostlyperform inferior to their general-purpose counterparts, especially on tasks notfocused on medical knowledge. While larger models showed similar performance oncase tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMAcases), smaller biomedical models showed more pronounced underperformance(e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases).Similar trends were observed across the CLUE (Clinical Language UnderstandingEvaluation) benchmark tasks, with general-purpose models often performingbetter on text generation, question answering, and coding tasks. Our resultssuggest that fine-tuning LLMs to biomedical data may not provide the expectedbenefits and may potentially lead to reduced performance, challengingprevailing assumptions about domain-specific adaptation of LLMs andhighlighting the need for more rigorous evaluation frameworks in healthcare AI.Alternative approaches, such as retrieval-augmented generation, may be moreeffective in enhancing the biomedical capabilities of LLMs without compromisingtheir general knowledge.</description><author>Felix J. Dorfner, Amin Dada, Felix Busch, Marcus R. Makowski, Tianyu Han, Daniel Truhn, Jens Kleesiek, Madhumita Sushil, Jacqueline Lammert, Lisa C. Adams, Keno K. Bressem</author><pubDate>Sun, 25 Aug 2024 13:36:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13833v1</guid></item><item><title>CodeRefine: A Pipeline for Enhancing LLM-Generated Code Implementations of Research Papers</title><link>http://arxiv.org/abs/2408.13366v1</link><description>This paper presents CodeRefine, a novel framework for automaticallytransforming research paper methodologies into functional code using LargeLanguage Models (LLMs). Our multi-step approach first extracts and summarizeskey text chunks from papers, analyzes their code relevance, and creates aknowledge graph using a predefined ontology. Code is then generated from thisstructured representation and enhanced through a proposed retrospectiveretrieval-augmented generation approach. CodeRefine addresses the challenge ofbridging theoretical research and practical implementation, offering a moreaccurate alternative to LLM zero-shot prompting. Evaluations on diversescientific papers demonstrate CodeRefine's ability to improve codeimplementation from the paper, potentially accelerating the adoption ofcutting-edge algorithms in real-world applications.</description><author>Ekaterina Trofimova, Emil Sataev, Abhijit Singh Jowhari</author><pubDate>Fri, 23 Aug 2024 20:51:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.13366v1</guid></item><item><title>Measuring text summarization factuality using atomic facts entailment metrics in the context of retrieval augmented generation</title><link>http://arxiv.org/abs/2408.15171v1</link><description>The use of large language models (LLMs) has significantly increased since theintroduction of ChatGPT in 2022, demonstrating their value across variousapplications. However, a major challenge for enterprise and commercial adoptionof LLMs is their tendency to generate inaccurate information, a phenomenonknown as "hallucination." This project proposes a method for estimating thefactuality of a summary generated by LLMs when compared to a source text. Ourapproach utilizes Naive Bayes classification to assess the accuracy of thecontent produced.</description><author>N. E. Kriman</author><pubDate>Tue, 27 Aug 2024 16:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15171v1</guid></item><item><title>A Survey of Large Language Models for European Languages</title><link>http://arxiv.org/abs/2408.15040v2</link><description>Large Language Models (LLMs) have gained significant attention due to theirhigh performance on a wide range of natural language tasks since the release ofChatGPT. The LLMs learn to understand and generate language by trainingbillions of model parameters on vast volumes of text data. Despite being arelatively new field, LLM research is rapidly advancing in various directions.In this paper, we present an overview of LLM families, including LLaMA, PaLM,GPT, and MoE, and the methods developed to create and enhance LLMs for officialEuropean Union (EU) languages. We provide a comprehensive summary of commonmonolingual and multilingual datasets used for pretraining large languagemodels.</description><author>Wazir Ali, Sampo Pyysalo</author><pubDate>Wed, 28 Aug 2024 03:56:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15040v2</guid></item><item><title>Intelligent OPC Engineer Assistant for Semiconductor Manufacturing</title><link>http://arxiv.org/abs/2408.12775v2</link><description>Advancements in chip design and manufacturing have enabled the processing ofcomplex tasks such as deep learning and natural language processing, paving theway for the development of artificial general intelligence (AGI). AI, on theother hand, can be leveraged to innovate and streamline semiconductortechnology from planning and implementation to manufacturing. In this paper, wepresent \textit{Intelligent OPC Engineer Assistant}, an AI/LLM-poweredmethodology designed to solve the core manufacturing-aware optimization problemknown as optical proximity correction (OPC). The methodology involves areinforcement learning-based OPC recipe search and a customized multi-modalagent system for recipe summarization. Experiments demonstrate that ourmethodology can efficiently build OPC recipes on various chip designs withspecially handled design topologies, a task that typically requires thefull-time effort of OPC engineers with years of experience.</description><author>Guojin Chen, Haoyu Yang, Bei Yu, Haoxing Ren</author><pubDate>Tue, 27 Aug 2024 06:02:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12775v2</guid></item><item><title>Personalized Video Summarization using Text-Based Queries and Conditional Modeling</title><link>http://arxiv.org/abs/2408.14743v1</link><description>The proliferation of video content on platforms like YouTube and Vimeopresents significant challenges in efficiently locating relevant information.Automatic video summarization aims to address this by extracting and presentingkey content in a condensed form. This thesis explores enhancing videosummarization by integrating text-based queries and conditional modeling totailor summaries to user needs. Traditional methods often produce fixedsummaries that may not align with individual requirements. To overcome this, wepropose a multi-modal deep learning approach that incorporates both textualqueries and visual information, fusing them at different levels of the modelarchitecture. Evaluation metrics such as accuracy and F1-score assess thequality of the generated summaries. The thesis also investigates improvingtext-based query representations using contextualized word embeddings andspecialized attention networks. This enhances the semantic understanding ofqueries, leading to better video summaries. To emulate human-likesummarization, which accounts for both visual coherence and abstract factorslike storyline consistency, we introduce a conditional modeling approach. Thismethod uses multiple random variables and joint distributions to capture keysummarization components, resulting in more human-like and explainablesummaries. Addressing data scarcity in fully supervised learning, the thesisproposes a segment-level pseudo-labeling approach. This self-supervised methodgenerates additional data, improving model performance even with limitedhuman-labeled datasets. In summary, this research aims to enhance automaticvideo summarization by incorporating text-based queries, improving queryrepresentations, introducing conditional modeling, and addressing datascarcity, thereby creating more effective and personalized video summaries.</description><author>Jia-Hong Huang</author><pubDate>Tue, 27 Aug 2024 02:43:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14743v1</guid></item><item><title>What Makes a Good Story and How Can We Measure It? A Comprehensive Survey of Story Evaluation</title><link>http://arxiv.org/abs/2408.14622v1</link><description>With the development of artificial intelligence, particularly the success ofLarge Language Models (LLMs), the quantity and quality of automaticallygenerated stories have significantly increased. This has led to the need forautomatic story evaluation to assess the generative capabilities of computingsystems and analyze the quality of both automatic-generated and human-writtenstories. Evaluating a story can be more challenging than other generationevaluation tasks. While tasks like machine translation primarily focus onassessing the aspects of fluency and accuracy, story evaluation demands complexadditional measures such as overall coherence, character development,interestingness, etc. This requires a thorough review of relevant research. Inthis survey, we first summarize existing storytelling tasks, includingtext-to-text, visual-to-text, and text-to-visual. We highlight their evaluationchallenges, identify various human criteria to measure stories, and presentexisting benchmark datasets. Then, we propose a taxonomy to organize evaluationmetrics that have been developed or can be adopted for story evaluation. Wealso provide descriptions of these metrics, along with the discussion of theirmerits and limitations. Later, we discuss the human-AI collaboration for storyevaluation and generation. Finally, we suggest potential future researchdirections, extending from story evaluation to general evaluations.</description><author>Dingyi Yang, Qin Jin</author><pubDate>Mon, 26 Aug 2024 20:35:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14622v1</guid></item><item><title>Towards Graph Prompt Learning: A Survey and Beyond</title><link>http://arxiv.org/abs/2408.14520v1</link><description>Large-scale "pre-train and prompt learning" paradigms have demonstratedremarkable adaptability, enabling broad applications across diverse domainssuch as question answering, image recognition, and multimodal retrieval. Thisapproach fully leverages the potential of large-scale pre-trained models,reducing downstream data requirements and computational costs while enhancingmodel applicability across various tasks. Graphs, as versatile data structuresthat capture relationships between entities, play pivotal roles in fields suchas social network analysis, recommender systems, and biological graphs. Despitethe success of pre-train and prompt learning paradigms in Natural LanguageProcessing (NLP) and Computer Vision (CV), their application in graph domainsremains nascent. In graph-structured data, not only do the node and edgefeatures often have disparate distributions, but the topological structuresalso differ significantly. This diversity in graph data can lead toincompatible patterns or gaps between pre-training and fine-tuning ondownstream graphs. We aim to bridge this gap by summarizing methods foralleviating these disparities. This includes exploring prompt designmethodologies, comparing related techniques, assessing application scenariosand datasets, and identifying unresolved problems and challenges. This surveycategorizes over 100 relevant works in this field, summarizing general designprinciples and the latest applications, including text-attributed graphs,molecules, proteins, and recommendation systems. Through this extensive review,we provide a foundational understanding of graph prompt learning, aiming toimpact not only the graph mining community but also the broader ArtificialGeneral Intelligence (AGI) community.</description><author>Qingqing Long, Yuchen Yan, Peiyan Zhang, Chen Fang, Wentao Cui, Zhiyuan Ning, Meng Xiao, Ning Cao, Xiao Luo, Lingjun Xu, Shiyue Jiang, Zheng Fang, Chong Chen, Xian-Sheng Hua, Yuanchun Zhou</author><pubDate>Mon, 26 Aug 2024 06:36:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14520v1</guid></item><item><title>HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus</title><link>http://arxiv.org/abs/2309.02731v3</link><description>ChatGPT has garnered significant interest due to its impressive performance;however, there is growing concern about its potential risks, particularly inthe detection of AI-generated content (AIGC), which is often challenging foruntrained individuals to identify. Current datasets used for detectingChatGPT-generated text primarily focus on question-answering tasks, oftenoverlooking tasks with semantic-invariant properties, such as summarization,translation, and paraphrasing. In this paper, we demonstrate that detectingmodel-generated text in semantic-invariant tasks is more challenging. Toaddress this gap, we introduce a more extensive and comprehensive dataset thatincorporates a wider range of tasks than previous work, including those withsemantic-invariant properties.</description><author>Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu</author><pubDate>Wed, 28 Aug 2024 15:40:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02731v3</guid></item><item><title>SITransformer: Shared Information-Guided Transformer for Extreme Multimodal Summarization</title><link>http://arxiv.org/abs/2408.15829v1</link><description>Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes anattractive summarization approach by integrating various types of informationto create extremely concise yet informative summaries for individualmodalities. Existing methods overlook the issue that multimodal data oftencontains more topic irrelevant information, which can mislead the model intoproducing inaccurate summaries especially for extremely short ones. In thispaper, we propose SITransformer, a \textbf{S}hared \textbf{I}nformation-guided\textbf{T}ransformer for extreme multimodal summarization. It has a sharedinformation guided pipeline which involves a cross-modal shared informationextractor and a cross-modal interaction module. The extractor formulatessemantically shared salient information from different modalities by devising anovel filtering process consisting of a differentiable top-k selector and ashared-information guided gating unit. As a result, the common, salient, andrelevant contents across modalities are identified. Next, a transformer withcross-modal attentions is developed for intra- and inter-modality learning withthe shared information guidance to produce the extreme summary. Comprehensiveexperiments demonstrate that SITransformer significantly enhances thesummarization quality for both video and text summaries for XMSMO. Our codewill be publicly available at https://github.com/SichengLeoLiu/MMAsia24-XMSMO.</description><author>Sicheng Liu, Lintao Wang, Xiaogan Zhu, Xuequan Lu, Zhiyong Wang, Kun Hu</author><pubDate>Wed, 28 Aug 2024 14:44:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15829v1</guid></item><item><title>Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization</title><link>http://arxiv.org/abs/2408.15801v1</link><description>In an era where digital text is proliferating at an unprecedented rate,efficient summarization tools are becoming indispensable. While Large LanguageModels (LLMs) have been successfully applied in various NLP tasks, their rolein extractive text summarization remains underexplored. This paper introducesEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractiveSummarization), a framework that leverages LLMs, specifically LLAMA2-7B andChatGLM2-6B, for extractive summarization of lengthy text documents. Instead ofabstractive methods, which often suffer from issues like factual inaccuraciesand hallucinations, EYEGLAXS focuses on extractive summarization to ensurefactual and grammatical integrity. Utilizing state-of-the-art techniques suchas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXSaddresses the computational and resource challenges typically associated withLLMs. The system sets new performance benchmarks on well-known datasets likePubMed and ArXiv. Furthermore, we extend our research through additionalanalyses that explore the adaptability of LLMs in handling different sequencelengths and their efficiency in training on smaller datasets. Thesecontributions not only set a new standard in the field but also open uppromising avenues for future research in extractive text summarization.</description><author>Léo Hemamou, Mehdi Debiane</author><pubDate>Wed, 28 Aug 2024 13:52:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15801v1</guid></item><item><title>μgat: Improving Single-Page Document Parsing by Providing Multi-Page Context</title><link>http://arxiv.org/abs/2408.15646v1</link><description>Regesta are catalogs of summaries of other documents and, in some cases, arethe only source of information about the content of such full-length documents.For this reason, they are of great interest to scholars in many social andhumanities fields. In this work, we focus on Regesta Pontificum Romanum, alarge collection of papal registers. Regesta are visually rich documents, wherethe layout is as important as the text content to convey the containedinformation through the structure, and are inherently multi-page documents.Among Digital Humanities techniques that can help scholars efficiently exploitregesta and other documental sources in the form of scanned documents, DocumentParsing has emerged as a task to process document images and convert them intomachine-readable structured representations, usually markup language. However,current models focus on scientific and business documents, and most of themconsider only single-paged documents. To overcome this limitation, in thiswork, we propose {\mu}gat, an extension of the recently proposed Documentparsing Nougat architecture, which can handle elements spanning over the singlepage limits. Specifically, we adapt Nougat to process a larger, multi-pagecontext, consisting of the previous and the following page, while parsing thecurrent page. Experimental results, both qualitative and quantitative,demonstrate the effectiveness of our proposed approach also in the case of thechallenging Regesta Pontificum Romanorum.</description><author>Fabio Quattrini, Carmine Zaccagnino, Silvia Cascianelli, Laura Righi, Rita Cucchiara</author><pubDate>Wed, 28 Aug 2024 09:01:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15646v1</guid></item><item><title>SITransformer: Shared Information-Guided Transformer for Extreme Multimodal Summarization</title><link>http://arxiv.org/abs/2408.15829v2</link><description>Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes anattractive summarization approach by integrating various types of informationto create extremely concise yet informative summaries for individualmodalities. Existing methods overlook the issue that multimodal data oftencontains more topic irrelevant information, which can mislead the model intoproducing inaccurate summaries especially for extremely short ones. In thispaper, we propose SITransformer, a Shared Information-guided Transformer forextreme multimodal summarization. It has a shared information guided pipelinewhich involves a cross-modal shared information extractor and a cross-modalinteraction module. The extractor formulates semantically shared salientinformation from different modalities by devising a novel filtering processconsisting of a differentiable top-k selector and a shared-information guidedgating unit. As a result, the common, salient, and relevant contents acrossmodalities are identified. Next, a transformer with cross-modal attentions isdeveloped for intra- and inter-modality learning with the shared informationguidance to produce the extreme summary. Comprehensive experiments demonstratethat SITransformer significantly enhances the summarization quality for bothvideo and text summaries for XMSMO. Our code will be publicly available athttps://github.com/SichengLeoLiu/MMAsia24-XMSMO.</description><author>Sicheng Liu, Lintao Wang, Xiaogan Zhu, Xuequan Lu, Zhiyong Wang, Kun Hu</author><pubDate>Thu, 29 Aug 2024 02:16:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.15829v2</guid></item><item><title>Reinforcement Learning without Human Feedback for Last Mile Fine-Tuning of Large Language Models</title><link>http://arxiv.org/abs/2408.16753v1</link><description>Reinforcement learning is used to align language models with human preferencesignals after first pre-training the model to predict the next token of textwithin a large corpus using likelihood maximization. Before being deployed in aspecific domain, models are often further fine-tuned on task specific data.Since human preferences are often unavailable for the last step, it isperformed using likelihood maximization as that is the typical default method.However, reinforcement learning has other advantages besides facilitatingalignment to a human derived reward function. For one, whereas likelihoodmaximization is a form of imitation learning in which the model is trained onwhat to do under ideal conditions, reinforcement learning is not limited todemonstrating actions just for optimally reached states and trains a model whatto do under a range of scenarios as it explores the policy space. In addition,it also trains a model what not to do, suppressing competitive but pooractions. This work develops a framework for last-mile fine-tuning usingreinforcement learning and tests whether it garners performance gains. Theexperiments center on abstractive summarization, but the framework is generaland broadly applicable. Use of the procedure produced significantly betterresults than likelihood maximization when comparing raw predictions. For thespecific data tested, the gap could be bridged by employing post-processing ofthe maximum likelihood outputs. Nonetheless, the framework offers a new avenuefor model optimization in situations where post-processing may be lessstraightforward or effective, and it can be extended to include more complexclasses of undesirable outputs to penalize and train against, such ashallucinations.</description><author>Alec Solway</author><pubDate>Thu, 29 Aug 2024 17:49:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16753v1</guid></item><item><title>VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation</title><link>http://arxiv.org/abs/2408.16730v1</link><description>A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) isthat while increasing the number of vision tokens generally enhances visualunderstanding, it also significantly raises memory and computational costs,especially in long-term, dense video frame streaming scenarios. Althoughlearnable approaches like Q-Former and Perceiver Resampler have been developedto reduce the vision token burden, they overlook the context causally modeledby LLMs (i.e., key-value cache), potentially leading to missed visual cues whenaddressing user queries. In this paper, we introduce a novel approach to reducevision compute by leveraging redundant vision tokens "skipping layers" ratherthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, isinspired by mixture-of-depths LLMs and addresses the challenge of numerousvision tokens in long-term or streaming video. Specifically, for eachtransformer layer, we learn to skip the computation for a high proportion(e.g., 80\%) of vision tokens, passing them directly to the next layer. Thisapproach significantly enhances model efficiency, achieving approximately\textasciitilde42\% time and \textasciitilde30\% memory savings for the entiretraining. Moreover, our method reduces the computation in the context and avoiddecreasing the vision tokens, thus preserving or even improving performancecompared to the vanilla model. We conduct extensive experiments to demonstratethe effectiveness of VideoLLM-MoD, showing its state-of-the-art results onmultiple benchmarks, including narration, forecasting, and summarization tasksin COIN, Ego4D, and Ego-Exo4D datasets.</description><author>Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, Mike Zheng Shou</author><pubDate>Thu, 29 Aug 2024 17:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16730v1</guid></item><item><title>SurveySum: A Dataset for Summarizing Multiple Scientific Articles into a Survey Section</title><link>http://arxiv.org/abs/2408.16444v1</link><description>Document summarization is a task to shorten texts into concise andinformative summaries. This paper introduces a novel dataset designed forsummarizing multiple scientific articles into a section of a survey. Ourcontributions are: (1) SurveySum, a new dataset addressing the gap indomain-specific summarization tools; (2) two specific pipelines to summarizescientific articles into a section of a survey; and (3) the evaluation of thesepipelines using multiple metrics to compare their performance. Our resultshighlight the importance of high-quality retrieval stages and the impact ofdifferent configurations on the quality of generated summaries.</description><author>Leandro Carísio Fernandes, Gustavo Bartz Guedes, Thiago Soares Laitz, Thales Sales Almeida, Rodrigo Nogueira, Roberto Lotufo, Jayr Pereira</author><pubDate>Thu, 29 Aug 2024 11:13:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16444v1</guid></item><item><title>Towards Graph Prompt Learning: A Survey and Beyond</title><link>http://arxiv.org/abs/2408.14520v2</link><description>Large-scale "pre-train and prompt learning" paradigms have demonstratedremarkable adaptability, enabling broad applications across diverse domainssuch as question answering, image recognition, and multimodal retrieval. Thisapproach fully leverages the potential of large-scale pre-trained models,reducing downstream data requirements and computational costs while enhancingmodel applicability across various tasks. Graphs, as versatile data structuresthat capture relationships between entities, play pivotal roles in fields suchas social network analysis, recommender systems, and biological graphs. Despitethe success of pre-train and prompt learning paradigms in Natural LanguageProcessing (NLP) and Computer Vision (CV), their application in graph domainsremains nascent. In graph-structured data, not only do the node and edgefeatures often have disparate distributions, but the topological structuresalso differ significantly. This diversity in graph data can lead toincompatible patterns or gaps between pre-training and fine-tuning ondownstream graphs. We aim to bridge this gap by summarizing methods foralleviating these disparities. This includes exploring prompt designmethodologies, comparing related techniques, assessing application scenariosand datasets, and identifying unresolved problems and challenges. This surveycategorizes over 100 relevant works in this field, summarizing general designprinciples and the latest applications, including text-attributed graphs,molecules, proteins, and recommendation systems. Through this extensive review,we provide a foundational understanding of graph prompt learning, aiming toimpact not only the graph mining community but also the broader ArtificialGeneral Intelligence (AGI) community.</description><author>Qingqing Long, Yuchen Yan, Peiyan Zhang, Chen Fang, Wentao Cui, Zhiyuan Ning, Meng Xiao, Ning Cao, Xiao Luo, Lingjun Xu, Shiyue Jiang, Zheng Fang, Chong Chen, Xian-Sheng Hua, Yuanchun Zhou</author><pubDate>Fri, 30 Aug 2024 01:26:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14520v2</guid></item><item><title>Advancing Chinese biomedical text mining with community challenges</title><link>http://arxiv.org/abs/2403.04261v2</link><description>Objective: This study aims to review the recent advances in communitychallenges for biomedical text mining in China. Methods: We collectedinformation of evaluation tasks released in community challenges of biomedicaltext mining, including task description, dataset description, data source, tasktype and related links. A systematic summary and comparative analysis wereconducted on various biomedical natural language processing tasks, such asnamed entity recognition, entity normalization, attribute extraction, relationextraction, event extraction, text classification, text similarity, knowledgegraph construction, question answering, text generation, and large languagemodel evaluation. Results: We identified 39 evaluation tasks from 6 communitychallenges that spanned from 2017 to 2023. Our analysis revealed the diverserange of evaluation task types and data sources in biomedical text mining. Weexplored the potential clinical applications of these community challenge tasksfrom a translational biomedical informatics perspective. We compared with theirEnglish counterparts, and discussed the contributions, limitations, lessons andguidelines of these community challenges, while highlighting future directionsin the era of large language models. Conclusion: Community challenge evaluationcompetitions have played a crucial role in promoting technology innovation andfostering interdisciplinary collaboration in the field of biomedical textmining. These challenges provide valuable platforms for researchers to developstate-of-the-art solutions.</description><author>Hui Zong, Rongrong Wu, Jiaxue Cha, Weizhe Feng, Erman Wu, Jiakun Li, Aibin Shao, Liang Tao, Zuofeng Li, Buzhou Tang, Bairong Shen</author><pubDate>Fri, 30 Aug 2024 02:47:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04261v2</guid></item><item><title>ECC Analyzer: Extract Trading Signal from Earnings Conference Calls using Large Language Model for Stock Performance Prediction</title><link>http://arxiv.org/abs/2404.18470v2</link><description>In the realm of financial analytics, leveraging unstructured data, such asearnings conference calls (ECCs), to forecast stock volatility is a criticalchallenge that has attracted both academics and investors. While previousstudies have used multimodal deep learning-based models to obtain a generalview of ECCs for volatility predicting, they often fail to capture detailed,complex information. Our research introduces a novel framework: \textbf{ECCAnalyzer}, which utilizes large language models (LLMs) to extract richer, morepredictive content from ECCs to aid the model's prediction performance. We usethe pre-trained large models to extract textual and audio features from ECCsand implement a hierarchical information extraction strategy to extract morefine-grained information. This strategy first extracts paragraph-level generalinformation by summarizing the text and then extracts fine-grained focussentences using Retrieval-Augmented Generation (RAG). These features are thenfused through multimodal feature fusion to perform volatility prediction.Experimental results demonstrate that our model outperforms traditionalanalytical benchmarks, confirming the effectiveness of advanced LLM techniquesin financial analysis.</description><author>Yupeng Cao, Zhi Chen, Qingyun Pei, Nathan Jinseok Lee, K. P. Subbalakshmi, Papa Momar Ndiaye</author><pubDate>Thu, 29 Aug 2024 23:13:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18470v2</guid></item><item><title>FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models</title><link>http://arxiv.org/abs/2406.16069v2</link><description>Large language models (LLMs) excel in generating coherent text, but theyoften struggle with context awareness, leading to inaccuracies in tasksrequiring faithful adherence to provided information. We introduce FastMem, anovel method designed to enhance instruction fine-tuned LLMs' context awarenessthrough fast memorization of the prompt. FastMem maximizes the likelihood ofthe prompt before inference by fine-tuning only the last Feed-Forward Network(FFN) module. This targeted approach ensures efficient optimization withoutoverfitting, significantly improving the model's ability to comprehend andaccurately follow the context. Our experiments demonstrate substantial gains inreading comprehension, text summarization and adherence to output structures.For instance, FastMem improves the accuracy of Llama 3-8B-Inst on the NQ-SWAPdataset from 59.1% to 71.6%, and reduces the output structure failure rate ofQwen 1.5-4B-Chat from 34.9% to 25.5%. Extensive experimental results highlightFastMem's potential to offer a robust solution to enhance the reliability andaccuracy of LLMs in various applications. Our code is available at:https://github.com/IAAR-Shanghai/FastMem</description><author>Junyi Zhu, Shuochen Liu, Yu Yu, Bo Tang, Yibo Yan, Zhiyu Li, Feiyu Xiong, Tong Xu, Matthew B. Blaschko</author><pubDate>Mon, 02 Sep 2024 09:13:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16069v2</guid></item><item><title>More is More: Addition Bias in Large Language Models</title><link>http://arxiv.org/abs/2409.02569v1</link><description>In this paper, we investigate the presence of additive bias in Large LanguageModels (LLMs), drawing a parallel to the cognitive bias observed in humanswhere individuals tend to favor additive over subtractive changes. Using aseries of controlled experiments, we tested various LLMs, including GPT-3.5Turbo, Claude 3.5 Sonnet, Mistral, Math$\Sigma$tral, and Llama 3.1, on tasksdesigned to measure their propensity for additive versus subtractivemodifications. Our findings demonstrate a significant preference for additivechanges across all tested models. For example, in a palindrome creation task,Llama 3.1 favored adding letters 97.85% of the time over removing them.Similarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick76.38% of the time rather than remove one. In a text summarization task,Mistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked toimprove its own or others' writing. These results indicate that, similar tohumans, LLMs exhibit a marked additive bias, which might have implications whenLLMs are used on a large scale. Addittive bias might increase resource use andenvironmental impact, leading to higher economic costs due to overconsumptionand waste. This bias should be considered in the development and application ofLLMs to ensure balanced and efficient problem-solving approaches.</description><author>Luca Santagata, Cristiano De Nobili</author><pubDate>Wed, 04 Sep 2024 09:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02569v1</guid></item><item><title>Abstractive Text Summarization: State of the Art, Challenges, and Improvements</title><link>http://arxiv.org/abs/2409.02413v1</link><description>Specifically focusing on the landscape of abstractive text summarization, asopposed to extractive techniques, this survey presents a comprehensiveoverview, delving into state-of-the-art techniques, prevailing challenges, andprospective research directions. We categorize the techniques into traditionalsequence-to-sequence models, pre-trained large language models, reinforcementlearning, hierarchical methods, and multi-modal summarization. Unlike priorworks that did not examine complexities, scalability and comparisons oftechniques in detail, this review takes a comprehensive approach encompassingstate-of-the-art methods, challenges, solutions, comparisons, limitations andcharts out future improvements - providing researchers an extensive overview toadvance abstractive summarization research. We provide vital comparison tablesacross techniques categorized - offering insights into model complexity,scalability and appropriate applications. The paper highlights challenges suchas inadequate meaning representation, factual consistency, controllable textsummarization, cross-lingual summarization, and evaluation metrics, amongothers. Solutions leveraging knowledge incorporation and other innovativestrategies are proposed to address these challenges. The paper concludes byhighlighting emerging research areas like factual inconsistency,domain-specific, cross-lingual, multilingual, and long-document summarization,as well as handling noisy data. Our objective is to provide researchers andpractitioners with a structured overview of the domain, enabling them to betterunderstand the current landscape and identify potential areas for furtherresearch and improvement.</description><author>Hassan Shakil, Ahmad Farooq, Jugal Kalita</author><pubDate>Wed, 04 Sep 2024 03:39:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02413v1</guid></item><item><title>Correction with Backtracking Reduces Hallucination in Summarization</title><link>http://arxiv.org/abs/2310.16176v3</link><description>Abstractive summarization aims at generating natural language summaries of asource document that are succinct while preserving the important elements.Despite recent advances, neural text summarization models are known to besusceptible to hallucinating (or more correctly confabulating), that is toproduce summaries with details that are not grounded in the source document. Inthis paper, we introduce a simple yet efficient technique, CoBa, to reducehallucination in abstractive summarization. The approach is based on two steps:hallucination detection and mitigation. We show that the former can be achievedthrough measuring simple statistics about conditional word probabilities anddistance to context words. Further, we demonstrate that straight-forwardbacktracking is surprisingly effective at mitigation. We thoroughly evaluatethe proposed method with prior art on three benchmark datasets for textsummarization. The results show that CoBa is effective and efficient inreducing hallucination, and offers great adaptability and flexibility. Code canbe found at https://github.com/zhenzhel/CoBa.</description><author>Zhenzhen Liu, Chao Wan, Varsha Kishore, Jin Peng Zhou, Minmin Chen, Kilian Q. Weinberger</author><pubDate>Tue, 03 Sep 2024 19:08:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16176v3</guid></item></channel></rss>