<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivtext summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 24 Aug 2024 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching</title><link>http://arxiv.org/abs/2407.17349v1</link><description>With the introduction of large language models (LLMs), automatic mathreasoning has seen tremendous success. However, current methods primarily focuson providing solutions or using techniques like Chain-of-Thought to enhanceproblem-solving accuracy. In this paper, we focus on improving the capabilityof mathematics teaching via a Socratic teaching-based LLM(\texttt{SocraticLLM}), which guides learners toward profound thinking withclarity and self-discovery via conversation. We collect and release ahigh-quality mathematical teaching dataset, named \texttt{SocraticMATH}, whichprovides Socratic-style conversations of problems with extra knowledge. Also,we propose a knowledge-enhanced LLM as a strong baseline to generate reliableresponses with review, guidance/heuristic, rectification, and summarization.Experimental results show the great advantages of \texttt{SocraticLLM} bycomparing it with several strong generative models. The codes and datasets areavailable on \url{https://github.com/ECNU-ICALK/SocraticMath}.</description><author>Yuyang Ding, Hanglei Hu, Jie Zhou, Qin Chen, Bo Jiang, Liang He</author><pubDate>Wed, 24 Jul 2024 15:18:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17349v1</guid></item><item><title>Zero-shot Factual Consistency Evaluation Across Domains</title><link>http://arxiv.org/abs/2408.04114v1</link><description>This work addresses the challenge of factual consistency in text generationsystems. We unify the tasks of Natural Language Inference, SummarizationEvaluation, Factuality Verification and Factual Consistency Evaluation to trainmodels capable of evaluating the factual consistency of source-target pairsacross diverse domains. We rigorously evaluate these against eight baselines ona comprehensive benchmark suite comprising 22 datasets that span various tasks,domains, and document lengths. Results demonstrate that our method achievesstate-of-the-art performance on this heterogeneous benchmark while addressingefficiency concerns and attaining cross-domain generalization.</description><author>Raunak Agarwal</author><pubDate>Wed, 07 Aug 2024 22:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04114v1</guid></item><item><title>MixSumm: Topic-based Data Augmentation using LLMs for Low-resource Extractive Text Summarization</title><link>http://arxiv.org/abs/2407.07341v1</link><description>Low-resource extractive text summarization is a vital but heavilyunderexplored area of research. Prior literature either focuses on abstractivetext summarization or prompts a large language model (LLM) like GPT-3 directlyto generate summaries. In this work, we propose MixSumm for low-resourceextractive text summarization. Specifically, MixSumm prompts an open-sourceLLM, LLaMA-3-70b, to generate documents that mix information from multipletopics as opposed to generating documents without mixup, and then trains asummarization model on the generated dataset. We use ROUGE scores and L-Eval, areference-free LLaMA-3-based evaluation method to measure the quality ofgenerated summaries. We conduct extensive experiments on a challenging textsummarization benchmark comprising the TweetSumm, WikiHow, and ArXiv/PubMeddatasets and show that our LLM-based data augmentation framework outperformsrecent prompt-based approaches for low-resource extractive summarization.Additionally, our results also demonstrate effective knowledge distillationfrom LLaMA-3-70b to a small BERT-based extractive summarizer.</description><author>Gaurav Sahu, Issam H. Laradji</author><pubDate>Wed, 10 Jul 2024 03:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07341v1</guid></item><item><title>IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization</title><link>http://arxiv.org/abs/2407.10486v1</link><description>Query-focused summarization (QFS) aims to produce summaries that answerparticular questions of interest, enabling greater user control andpersonalization. With the advent of large language models (LLMs), shows theirimpressive capability of textual understanding through large-scale pretraining,which implies the great potential of extractive snippet generation. In thispaper, we systematically investigated two indispensable characteristics thatthe LLMs-based QFS models should be harnessed, Lengthy Document Summarizationand Efficiently Fine-grained Query-LLM Alignment, respectively.Correspondingly, we propose two modules called Query-aware HyperExpert andQuery-focused Infini-attention to access the aforementioned characteristics.These innovations pave the way for broader application and accessibility in thefield of QFS technology. Extensive experiments conducted on existing QFSbenchmarks indicate the effectiveness and generalizability of the proposedapproach. Our code is publicly available athttps://github.com/DCDmllm/IDEAL_Summary.</description><author>Jie Cao, Dian Jiao, Qiang Yan, Wenqiao Zhang, Siliang Tang, Yueting Zhuang</author><pubDate>Mon, 15 Jul 2024 07:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10486v1</guid></item><item><title>What's Wrong? Refining Meeting Summaries with LLM Feedback</title><link>http://arxiv.org/abs/2407.11919v1</link><description>Meeting summarization has become a critical task since digital encountershave become a common practice. Large language models (LLMs) show greatpotential in summarization, offering enhanced coherence and contextunderstanding compared to traditional methods. However, they still struggle tomaintain relevance and avoid hallucination. We introduce a multi-LLM correctionapproach for meeting summarization using a two-phase process that mimics thehuman review process: mistake identification and summary refinement. We releaseQMSum Mistake, a dataset of 200 automatically generated meeting summariesannotated by humans on nine error types, including structural, omission, andirrelevance errors. Our experiments show that these errors can be identifiedwith high accuracy by an LLM. We transform identified mistakes into actionablefeedback to improve the quality of a given summary measured by relevance,informativeness, conciseness, and coherence. This post-hoc refinementeffectively improves summary quality by leveraging multiple LLMs to validateoutput quality. Our multi-LLM approach for meeting summarization showspotential for similar complex text generation tasks requiring robustness,action planning, and discussion towards a goal.</description><author>Frederic Kirstein, Terry Ruas, Bela Gipp</author><pubDate>Tue, 16 Jul 2024 17:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11919v1</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v1</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell Espuny</author><pubDate>Tue, 16 Jul 2024 10:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v1</guid></item><item><title>Semantic-Aware Representation of Multi-Modal Data for Data Ingress: A Literature Review</title><link>http://arxiv.org/abs/2407.12438v1</link><description>Machine Learning (ML) is continuously permeating a growing amount ofapplication domains. Generative AI such as Large Language Models (LLMs) alsosees broad adoption to process multi-modal data such as text, images, audio,and video. While the trend is to use ever-larger datasets for training,managing this data efficiently has become a significant practical challenge inthe industry-double as much data is certainly not double as good. Rather theopposite is important since getting an understanding of the inherent qualityand diversity of the underlying data lakes is a growing challenge forapplication-specific ML as well as for fine-tuning foundation models.Furthermore, information retrieval (IR) from expanding data lakes iscomplicated by the temporal dimension inherent in time-series data which mustbe considered to determine its semantic value. This study focuses on thedifferent semantic-aware techniques to extract embeddings from mono-modal,multi-modal, and cross-modal data to enhance IR capabilities in a growing datalake. Articles were collected to summarize information about thestate-of-the-art techniques focusing on applications of embedding for threedifferent categories of data modalities.</description><author>Pierre Lamart, Yinan Yu, Christian Berger</author><pubDate>Wed, 17 Jul 2024 09:49:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12438v1</guid></item><item><title>Turning Generative Models Degenerate: The Power of Data Poisoning Attacks</title><link>http://arxiv.org/abs/2407.12281v2</link><description>The increasing use of large language models (LLMs) trained by third partiesraises significant security concerns. In particular, malicious actors canintroduce backdoors through poisoning attacks to generate undesirable outputs.While such attacks have been extensively studied in image domains andclassification tasks, they remain underexplored for natural language generation(NLG) tasks. To address this gap, we conduct an investigation of variouspoisoning techniques targeting the LLM's fine-tuning phase via prefix-tuning, aParameter Efficient Fine-Tuning (PEFT) method. We assess their effectivenessacross two generative tasks: text summarization and text completion; and wealso introduce new metrics to quantify the success and stealthiness of such NLGpoisoning attacks. Through our experiments, we find that the prefix-tuninghyperparameters and trigger designs are the most crucial factors to influenceattack success and stealthiness. Moreover, we demonstrate that existing populardefenses are ineffective against our poisoning attacks. Our study presents thefirst systematic approach to understanding poisoning attacks targeting NLGtasks during fine-tuning via PEFT across a wide range of triggers and attacksettings. We hope our findings will aid the AI security community in developingeffective defenses against such threats.</description><author>Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Farhan Ahmed, Ling Cai, Nathalie Baracaldo</author><pubDate>Thu, 18 Jul 2024 05:52:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12281v2</guid></item><item><title>On Learning to Summarize with Large Language Models as References</title><link>http://arxiv.org/abs/2305.14239v3</link><description>Recent studies have found that summaries generated by large language models(LLMs) are favored by human annotators over the original reference summaries incommonly used summarization datasets. Therefore, we study an LLM-as-referencelearning setting for smaller text summarization models to investigate whethertheir performance can be substantially improved. To this end, we use LLMs asboth oracle summary generators for standard supervised fine-tuning and oraclesummary evaluators for efficient contrastive learning that leverages the LLMs'supervision signals. We conduct comprehensive experiments with source newsarticles and find that (1) summarization models trained under theLLM-as-reference setting achieve significant performance improvement in bothLLM and human evaluations; (2) contrastive learning outperforms standardsupervised fine-tuning under both low and high resource settings. Ourexperimental results also enable a meta-analysis of LLMs' summary evaluationcapacities under a challenging setting, showing that LLMs are not well-alignedwith human evaluators. Particularly, our expert human evaluation revealsremaining nuanced performance gaps between LLMs and our fine-tuned models,which LLMs fail to capture. Thus, we call for further studies into both thepotential and challenges of using LLMs in summarization model development.</description><author>Yixin Liu, Kejian Shi, Katherine S He, Longtian Ye, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, Arman Cohan</author><pubDate>Thu, 18 Jul 2024 17:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14239v3</guid></item><item><title>PLANTS: A Novel Problem and Dataset for Summarization of Planning-Like (PL) Tasks</title><link>http://arxiv.org/abs/2407.13597v1</link><description>Text summarization is a well-studied problem that deals with derivinginsights from unstructured text consumed by humans, and it has found extensivebusiness applications. However, many real-life tasks involve generating aseries of actions to achieve specific goals, such as workflows, recipes,dialogs, and travel plans. We refer to them as planning-like (PL) tasks notingthat the main commonality they share is control flow information. which may bepartially specified. Their structure presents an opportunity to create morepractical summaries to help users make quick decisions. We investigate thisobservation by introducing a novel plan summarization problem, presenting adataset, and providing a baseline method for generating PL summaries. Usingquantitative metrics and qualitative user studies to establish baselines, weevaluate the plan summaries from our method and large language models. Webelieve the novel problem and dataset can reinvigorate research insummarization, which some consider as a solved problem.</description><author>Vishal Pallagani, Biplav Srivastava, Nitin Gupta</author><pubDate>Thu, 18 Jul 2024 15:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13597v1</guid></item><item><title>Survey in Characterization of Semantic Change</title><link>http://arxiv.org/abs/2402.19088v3</link><description>Live languages continuously evolve to integrate the cultural change of humansocieties. This evolution manifests through neologisms (new words) or\textbf{semantic changes} of words (new meaning to existing words).Understanding the meaning of words is vital for interpreting texts coming fromdifferent cultures (regionalism or slang), domains (e.g., technical terms), orperiods. In computer science, these words are relevant to computationallinguistics algorithms such as translation, information retrieval, questionanswering, etc. Semantic changes can potentially impact the quality of theoutcomes of these algorithms. Therefore, it is important to understand andcharacterize these changes formally. The study of this impact is a recentproblem that has attracted the attention of the computational linguisticscommunity. Several approaches propose methods to detect semantic changes withgood precision, but more effort is needed to characterize how the meaning ofwords changes and to reason about how to reduce the impact of semantic change.This survey provides an understandable overview of existing approaches to the\textit{characterization of semantic changes} and also formally defines threeclasses of characterizations: if the meaning of a word becomes more general ornarrow (change in dimension) if the word is used in a more pejorative orpositive/ameliorated sense (change in orientation), and if there is a trend touse the word in a, for instance, metaphoric or metonymic context (change inrelation). We summarized the main aspects of the selected publications in atable and discussed the needs and trends in the research activities on semanticchange characterization.</description><author>Jader Martins Camboim de Sá, Marcos Da Silveira, Cédric Pruski</author><pubDate>Thu, 18 Jul 2024 12:28:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19088v3</guid></item><item><title>Multi-sentence Video Grounding for Long Video Generation</title><link>http://arxiv.org/abs/2407.13219v1</link><description>Video generation has witnessed great success recently, but their applicationin generating long videos still remains challenging due to the difficulty inmaintaining the temporal consistency of generated videos and the high memorycost during generation. To tackle the problems, in this paper, we propose abrave and new idea of Multi-sentence Video Grounding for Long Video Generation,connecting the massive video moment retrieval to the video generation task forthe first time, providing a new paradigm for long video generation. The methodof our work can be summarized as three steps: (i) We design sequential scenetext prompts as the queries for video grounding, utilizing the massive videomoment retrieval to search for video moment segments that meet the textrequirements in the video database. (ii) Based on the source frames ofretrieved video moment segments, we adopt video editing methods to create newvideo content while preserving the temporal consistency of the retrieved video.Since the editing can be conducted segment by segment, and even frame by frame,it largely reduces the memory cost. (iii) We also attempt video morphing andpersonalized generation methods to improve the subject consistency of longvideo generation, providing ablation experimental results for the subtasks oflong video generation. Our approach seamlessly extends the development inimage/video editing, video morphing and personalized generation, and videogrounding to the long video generation, offering effective solutions forgenerating long videos at low memory cost.</description><author>Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Wenwu Zhu</author><pubDate>Thu, 18 Jul 2024 07:05:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13219v1</guid></item><item><title>PassTSL: Modeling Human-Created Passwords through Two-Stage Learning</title><link>http://arxiv.org/abs/2407.14145v1</link><description>Textual passwords are still the most widely used user authenticationmechanism. Due to the close connections between textual passwords and naturallanguages, advanced technologies in natural language processing (NLP) andmachine learning (ML) could be used to model passwords for different purposessuch as studying human password-creation behaviors and developing more advancedpassword cracking methods for informing better defence mechanisms. In thispaper, we propose PassTSL (modeling human-created Passwords through Two-StageLearning), inspired by the popular pretraining-finetuning framework in NLP anddeep learning (DL). We report how different pretraining settings affectedPassTSL and proved its effectiveness by applying it to six large leakedpassword databases. Experimental results showed that it outperforms fivestate-of-the-art (SOTA) password cracking methods on password guessing by asignificant margin ranging from 4.11% to 64.69% at the maximum point. Based onPassTSL, we also implemented a password strength meter (PSM), and ourexperiments showed that it was able to estimate password strength moreaccurately, causing fewer unsafe errors (overestimating the password strength)than two other SOTA PSMs when they produce the same rate of safe errors(underestimating the password strength): a neural-network based method andzxcvbn. Furthermore, we explored multiple finetuning settings, and ourevaluations showed that, even a small amount of additional training data, e.g.,only 0.1% of the pretrained data, can lead to over 3% improvement in passwordguessing on average. We also proposed a heuristic approach to selectingfinetuning passwords based on JS (Jensen-Shannon) divergence and experimentalresults validated its usefulness. In summary, our contributions demonstrate thepotential and feasibility of applying advanced NLP and ML methods to passwordmodeling and cracking.</description><author>Yangde Wang, Haozhang Li, Weidong Qiu, Shujun Li, Peng Tang</author><pubDate>Fri, 19 Jul 2024 09:23:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14145v1</guid></item><item><title>AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description</title><link>http://arxiv.org/abs/2407.15850v1</link><description>Our objective is to generate Audio Descriptions (ADs) for both movies and TVseries in a training-free manner. We use the power of off-the-shelfVisual-Language Models (VLMs) and Large Language Models (LLMs), and developvisual and text prompting strategies for this task. Our contributions arethree-fold: (i) We demonstrate that a VLM can successfully name and refer tocharacters if directly prompted with character information through visualindications without requiring any fine-tuning; (ii) A two-stage process isdeveloped to generate ADs, with the first stage asking the VLM tocomprehensively describe the video, followed by a second stage utilising a LLMto summarise dense textual information into one succinct AD sentence; (iii) Anew dataset for TV audio description is formulated. Our approach, namedAutoAD-Zero, demonstrates outstanding performance (even competitive with somemodels fine-tuned on ground truth ADs) in AD generation for both movies and TVseries, achieving state-of-the-art CRITIC scores.</description><author>Junyu Xie, Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman</author><pubDate>Mon, 22 Jul 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15850v1</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v2</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell</author><pubDate>Mon, 22 Jul 2024 13:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v2</guid></item><item><title>AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations</title><link>http://arxiv.org/abs/2407.16010v1</link><description>For many use-cases, it is often important to explain the prediction of ablack-box model by identifying the most influential training data samples.Existing approaches lack customization for user intent and often provide ahomogeneous set of explanation samples, failing to reveal the model's reasoningfrom different angles. In this paper, we propose AIDE, an approach for providing antithetical (i.e.,contrastive), intent-based, diverse explanations for opaque and complex models.AIDE distinguishes three types of explainability intents: interpreting acorrect, investigating a wrong, and clarifying an ambiguous prediction. Foreach intent, AIDE selects an appropriate set of influential training samplesthat support or oppose the prediction either directly or by contrast. Toprovide a succinct summary, AIDE uses diversity-aware sampling to avoidredundancy and increase coverage of the training data. We demonstrate the effectiveness of AIDE on image and text classificationtasks, in three ways: quantitatively, assessing correctness and continuity;qualitatively, comparing anecdotal evidence from AIDE and other example-basedapproaches; and via a user study, evaluating multiple aspects of AIDE. Theresults show that AIDE addresses the limitations of existing methods andexhibits desirable traits for an explainability method.</description><author>Ikhtiyor Nematov, Dimitris Sacharidis, Tomer Sagi, Katja Hose</author><pubDate>Mon, 22 Jul 2024 19:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16010v1</guid></item><item><title>Speech Editing -- a Summary</title><link>http://arxiv.org/abs/2407.17172v1</link><description>With the rise of video production and social media, speech editing has becomecrucial for creators to address issues like mispronunciations, missing words,or stuttering in audio recordings. This paper explores text-based speechediting methods that modify audio via text transcripts without manual waveformediting. These approaches ensure edited audio is indistinguishable from theoriginal by altering the mel-spectrogram. Recent advancements, such ascontext-aware prosody correction and advanced attention mechanisms, haveimproved speech editing quality. This paper reviews state-of-the-art methods,compares key metrics, and examines widely used datasets. The aim is tohighlight ongoing issues and inspire further research and innovation in speechediting.</description><author>Tobias Kässmann, Yining Liu, Danni Liu</author><pubDate>Wed, 24 Jul 2024 11:22:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17172v1</guid></item><item><title>Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</title><link>http://arxiv.org/abs/2407.18003v1</link><description>Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,have revolutionized various industries with their advanced languagecomprehension. However, their efficiency is challenged by the Transformerarchitecture' s struggle with handling long texts. KV-Cache has emerged as apivotal solution to this issue, converting the time complexity of tokengeneration from quadratic to linear, albeit with increased GPU memory overheadproportional to conversation length. With the development of the LLM communityand academia, various KV-Cache compression methods have been proposed. In thisreview, we dissect the various properties of KV-Cache and elaborate on variousmethods currently used to optimize the KV-Cache space usage of LLMs. Thesemethods span the pre-training phase, deployment phase, and inference phase, andwe summarize the commonalities and differences among these methods.Additionally, we list some metrics for evaluating the long-text capabilities oflarge language models, from both efficiency and capability perspectives. Ourreview thus sheds light on the evolving landscape of LLM optimization, offeringinsights into future advancements in this dynamic field.</description><author>Shi Luohe, Zhang Hongyi, Yao Yao, Li Zuchao, Zhao Hai</author><pubDate>Thu, 25 Jul 2024 12:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18003v1</guid></item><item><title>An Iterative Approach to Topic Modelling</title><link>http://arxiv.org/abs/2407.17892v1</link><description>Topic modelling has become increasingly popular for summarizing text data,such as social media posts and articles. However, topic modelling is usuallycompleted in one shot. Assessing the quality of resulting topics ischallenging. No effective methods or measures have been developed for assessingthe results or for making further enhancements to the topics. In this research,we propose we propose to use an iterative process to perform topic modellingthat gives rise to a sense of completeness of the resulting topics when theprocess is complete. Using the BERTopic package, a popular method in topicmodelling, we demonstrate how the modelling process can be applied iterativelyto arrive at a set of topics that could not be further improved upon using oneof the three selected measures for clustering comparison as the decisioncriteria. This demonstration is conducted using a subset of the COVIDSenti-Adataset. The early success leads us to believe that further research using inusing this approach in conjunction with other topic modelling algorithms couldbe viable.</description><author>Albert Wong, Florence Wing Yau Cheng, Ashley Keung, Yamileth Hercules, Mary Alexandra Garcia, Yew-Wei Lim, Lien Pham</author><pubDate>Thu, 25 Jul 2024 09:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17892v1</guid></item><item><title>IgnitionInnovators at "Discharge Me!": Chain-of-Thought Instruction Finetuning Large Language Models for Discharge Summaries</title><link>http://arxiv.org/abs/2407.17636v1</link><description>This paper presents our proposed approach to the Discharge Me! shared task,collocated with the 23th Workshop on Biomedical Natural Language Processing(BioNLP). In this work, we develop an LLM-based framework for solving theDischarge Summary Documentation (DSD) task, i.e., generating the two criticaltarget sections `Brief Hospital Course' and `Discharge Instructions' in thedischarge summary. By streamlining the recent instruction-finetuning process onLLMs, we explore several prompting strategies for optimally adapting LLMs tospecific generation task of DSD. Experimental results show that providing aclear output structure, complimented by a set of comprehensiveChain-of-Thoughts (CoT) questions, effectively improves the model's reasoningcapability, and thereby, enhancing the structural correctness and faithfulnessof clinical information in the generated text. Source code is available at:https://github.com/antangrocket1312/Discharge_LLM</description><author>An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh</author><pubDate>Wed, 24 Jul 2024 21:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17636v1</guid></item><item><title>Assessing Brittleness of Image-Text Retrieval Benchmarks from Vision-Language Models Perspective</title><link>http://arxiv.org/abs/2407.15239v2</link><description>Image-text retrieval (ITR), an important task in information retrieval (IR),is driven by pretrained vision-language models (VLMs) that consistently achievestate-of-the-art performance. However, a significant challenge lies in thebrittleness of existing ITR benchmarks. In standard datasets for the task,captions often provide broad summaries of scenes, neglecting detailedinformation about specific concepts. Additionally, the current evaluation setupassumes simplistic binary matches between images and texts and focuses onintra-modality rather than cross-modal relationships, which can lead tomisinterpretations of model performance. Motivated by this gap, in this study,we focus on examining the brittleness of the ITR evaluation pipeline with afocus on concept granularity. We start by analyzing two common benchmarks,MS-COCO and Flickr30k, and compare them with their augmented versions,MS-COCO-FG and Flickr30k-FG, given a specified set of linguistic featurescapturing concept granularity. We discover that Flickr30k-FG and MS COCO-FGconsistently achieve higher scores across all the selected features. Toinvestigate the performance of VLMs on coarse and fine-grained datasets, weintroduce a taxonomy of perturbations. We apply these perturbations to theselected datasets. We evaluate four state-of-the-art models - ALIGN, AltCLIP,CLIP, and GroupViT - on the standard and fine-grained datasets under zero-shotconditions, with and without the applied perturbations. The results demonstratethat although perturbations generally degrade model performance, thefine-grained datasets exhibit a smaller performance drop than their standardcounterparts. Moreover, the relative performance drop across all setups isconsistent across all models and datasets, indicating that the issue lieswithin the benchmarks. We conclude the paper by providing an agenda forimproving ITR evaluation pipelines.</description><author>Mariya Hendriksen, Shuo Zhang, Ridho Reinanda, Mohamed Yahya, Edgar Meij, Maarten de Rijke</author><pubDate>Thu, 25 Jul 2024 19:52:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15239v2</guid></item><item><title>Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</title><link>http://arxiv.org/abs/2407.18003v2</link><description>Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,have revolutionized various industries with their advanced languagecomprehension. However, their efficiency is challenged by the Transformerarchitecture' s struggle with handling long texts. KV-Cache has emerged as apivotal solution to this issue, converting the time complexity of tokengeneration from quadratic to linear, albeit with increased GPU memory overheadproportional to conversation length. With the development of the LLM communityand academia, various KV-Cache compression methods have been proposed. In thisreview, we dissect the various properties of KV-Cache and elaborate on variousmethods currently used to optimize the KV-Cache space usage of LLMs. Thesemethods span the pre-training phase, deployment phase, and inference phase, andwe summarize the commonalities and differences among these methods.Additionally, we list some metrics for evaluating the long-text capabilities oflarge language models, from both efficiency and capability perspectives. Ourreview thus sheds light on the evolving landscape of LLM optimization, offeringinsights into future advancements in this dynamic field.</description><author>Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao</author><pubDate>Sun, 28 Jul 2024 14:42:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18003v2</guid></item><item><title>Synthesizing Scientific Summaries: An Extractive and Abstractive Approach</title><link>http://arxiv.org/abs/2407.19779v1</link><description>The availability of a vast array of research papers in any area of study,necessitates the need of automated summarisation systems that can present thekey research conducted and their corresponding findings. Scientific papersummarisation is a challenging task for various reasons including token lengthlimits in modern transformer models and corresponding memory and computerequirements for long text. A significant amount of work has been conducted inthis area, with approaches that modify the attention mechanisms of existingtransformer models and others that utilise discourse information to capturelong range dependencies in research papers. In this paper, we propose a hybridmethodology for research paper summarisation which incorporates an extractiveand abstractive approach. We use the extractive approach to capture the keyfindings of research, and pair it with the introduction of the paper whichcaptures the motivation for research. We use two models based on unsupervisedlearning for the extraction stage and two transformer language models,resulting in four combinations for our hybrid approach. The performances of themodels are evaluated on three metrics and we present our findings in thispaper. We find that using certain combinations of hyper parameters, it ispossible for automated summarisation systems to exceed the abstractiveness ofsummaries written by humans. Finally, we state our future scope of research inextending this methodology to summarisation of generalised long documents.</description><author>Grishma Sharma, Aditi Paretkar, Deepak Sharma</author><pubDate>Mon, 29 Jul 2024 08:21:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19779v1</guid></item><item><title>Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency</title><link>http://arxiv.org/abs/2407.21443v1</link><description>Despite large language models (LLMs) have demonstrated impressive performancein various tasks, they are still suffering from the factual inconsistencyproblem called hallucinations. For instance, LLMs occasionally generate contentthat diverges from source article, and prefer to extract information thatappears at the beginning and end of the context, especially in long documentsummarization. Inspired by these findings, we propose to improve thefaithfulness of LLMs in summarization by impelling them to process the entirearticle more fairly and faithfully. We present a novel summary generationstrategy, namely SliSum, which exploits the ideas of sliding windows andself-consistency. Specifically, SliSum divides the source article intooverlapping windows, and utilizes LLM to generate local summaries for thecontent in the windows. Finally, SliSum aggregates all local summaries usingclustering and majority voting algorithm to produce more faithful summary ofentire article. Extensive experiments demonstrate that SliSum significantlyimproves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 andGPT-3.5 in both short and long text summarization, while maintaining theirfluency and informativeness and without additional fine-tuning and resources.We further conduct qualitative and quantitative studies to investigate whySliSum works and impacts of hyperparameters in SliSum on performance.</description><author>Taiji Li, Zhi Li, Yin Zhang</author><pubDate>Wed, 31 Jul 2024 08:48:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21443v1</guid></item><item><title>Debiased Distribution Compression</title><link>http://arxiv.org/abs/2404.12290v3</link><description>Modern compression methods can summarize a target distribution $\mathbb{P}$more succinctly than i.i.d. sampling but require access to a low-bias inputsequence like a Markov chain converging quickly to $\mathbb{P}$. We introduce anew suite of compression methods suitable for compression with biased inputsequences. Given $n$ points targeting the wrong distribution and quadratictime, Stein kernel thinning (SKT) returns $\sqrt{n}$ equal-weighted points with$\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\mathbb{P}$. Forlarger-scale compression tasks, low-rank SKT achieves the same feat insub-quadratic time using an adaptive low-rank debiasing procedure that may beof independent interest. For downstream tasks that support simplex orconstant-preserving weights, Stein recombination and Stein Cholesky achieveeven greater parsimony, matching the guarantees of SKT with as few as$\text{poly-log}(n)$ weighted points. Underlying these advances are newguarantees for the quality of simplex-weighted coresets, the spectral decay ofkernel matrices, and the covering numbers of Stein kernel Hilbert spaces. Inour experiments, our techniques provide succinct and accurate posteriorsummaries while overcoming biases due to burn-in, approximate Markov chainMonte Carlo, and tempering.</description><author>Lingxiao Li, Raaz Dwivedi, Lester Mackey</author><pubDate>Wed, 31 Jul 2024 20:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12290v3</guid></item><item><title>Sentence-wise Speech Summarization: Task, Datasets, and End-to-End Modeling with LM Knowledge Distillation</title><link>http://arxiv.org/abs/2408.00205v1</link><description>This paper introduces a novel approach called sentence-wise speechsummarization (Sen-SSum), which generates text summaries from a spoken documentin a sentence-by-sentence manner. Sen-SSum combines the real-time processing ofautomatic speech recognition (ASR) with the conciseness of speechsummarization. To explore this approach, we present two datasets for Sen-SSum:Mega-SSum and CSJ-SSum. Using these datasets, our study evaluates two types ofTransformer-based models: 1) cascade models that combine ASR and strong textsummarization models, and 2) end-to-end (E2E) models that directly convertspeech into a text summary. While E2E models are appealing to developcompute-efficient models, they perform worse than cascade models. Therefore, wepropose knowledge distillation for E2E models using pseudo-summaries generatedby the cascade models. Our experiments show that this proposed knowledgedistillation effectively improves the performance of the E2E model on bothdatasets.</description><author>Kohei Matsuura, Takanori Ashihara, Takafumi Moriya, Masato Mimura, Takatomo Kano, Atsunori Ogawa, Marc Delcroix</author><pubDate>Thu, 01 Aug 2024 00:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00205v1</guid></item><item><title>From Attributes to Natural Language: A Survey and Foresight on Text-based Person Re-identification</title><link>http://arxiv.org/abs/2408.00096v1</link><description>Text-based person re-identification (Re-ID) is a challenging topic in thefield of complex multimodal analysis, its ultimate aim is to recognize specificpedestrians by scrutinizing attributes/natural language descriptions. Despitethe wide range of applicable areas such as security surveillance, videoretrieval, person tracking, and social media analytics, there is a notableabsence of comprehensive reviews dedicated to summarizing the text-based personRe-ID from a technical perspective. To address this gap, we propose tointroduce a taxonomy spanning Evaluation, Strategy, Architecture, andOptimization dimensions, providing a comprehensive survey of the text-basedperson Re-ID task. We start by laying the groundwork for text-based personRe-ID, elucidating fundamental concepts related to attribute/naturallanguage-based identification. Then a thorough examination of existingbenchmark datasets and metrics is presented. Subsequently, we further delveinto prevalent feature extraction strategies employed in text-based personRe-ID research, followed by a concise summary of common network architectureswithin the domain. Prevalent loss functions utilized for model optimization andmodality alignment in text-based person Re-ID are also scrutinized. Toconclude, we offer a concise summary of our findings, pinpointing challenges intext-based person Re-ID. In response to these challenges, we outline potentialavenues for future open-set text-based person Re-ID and present a baselinearchitecture for text-based pedestrian image generation-guidedre-identification(TBPGR).</description><author>Fanzhi Jiang, Su Yang, Mark W. Jones, Liumei Zhang</author><pubDate>Wed, 31 Jul 2024 18:16:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00096v1</guid></item><item><title>Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models</title><link>http://arxiv.org/abs/2408.01308v1</link><description>Learning token embeddings based on token co-occurrence statistics has proveneffective for both pre-training and fine-tuning in natural language processing.However, recent studies have pointed out the distribution of learned embeddingsdegenerates into anisotropy, and even pre-trained language models (PLMs) sufferfrom a loss of semantics-related information in embeddings for low-frequencytokens. This study first analyzes fine-tuning dynamics of a PLM, BART-large,and demonstrates its robustness against degeneration. On the basis of thisfinding, we propose DefinitionEMB, a method that utilizes definitions toconstruct isotropically distributed and semantics-related token embeddings forPLMs while maintaining original robustness during fine-tuning. Our experimentsdemonstrate the effectiveness of leveraging definitions from Wiktionary toconstruct such embeddings for RoBERTa-base and BART-large. Furthermore, theconstructed embeddings for low-frequency tokens improve the performance ofthese models across various GLUE and four text summarization datasets.</description><author>Ying Zhang, Dongyuan Li, Manabu Okumura</author><pubDate>Fri, 02 Aug 2024 15:00:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01308v1</guid></item><item><title>High-Throughput Phenotyping of Clinical Text Using Large Language Models</title><link>http://arxiv.org/abs/2408.01214v1</link><description>High-throughput phenotyping automates the mapping of patient signs tostandardized ontology concepts and is essential for precision medicine. Thisstudy evaluates the automation of phenotyping of clinical summaries from theOnline Mendelian Inheritance in Man (OMIM) database using large languagemodels. Due to their rich phenotype data, these summaries can be surrogates forphysician notes. We conduct a performance comparison of GPT-4 andGPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo inidentifying, categorizing, and normalizing signs, achieving concordance withmanual annotators comparable to inter-rater agreement. Despite some limitationsin sign normalization, the extensive pre-training of GPT-4 results in highperformance and generalizability across several phenotyping tasks whileobviating the need for manually annotated training data. Large language modelsare expected to be the dominant method for automating high-throughputphenotyping of clinical text.</description><author>Daniel B. Hier, S. Ilyas Munzir, Anne Stahlfeld, Tayo Obafemi-Ajayi, Michael D. Carrithers</author><pubDate>Fri, 02 Aug 2024 12:00:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01214v1</guid></item><item><title>Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with Accelerated LLMs</title><link>http://arxiv.org/abs/2408.01008v1</link><description>In recent years, Large Language Models (LLMs) have demonstrated remarkablecapabilities across a wide range of natural language processing (NLP) tasks,such as question-answering, sentiment analysis, text summarization, and machinetranslation. However, the ever-growing complexity of LLMs demands immensecomputational resources, hindering the broader research and application ofthese models. To address this, various parameter-efficient fine-tuningstrategies, such as Low-Rank Approximation (LoRA) and Adapters, have beendeveloped. Despite their potential, these methods often face limitations incompressibility. Specifically, LoRA struggles to scale effectively with theincreasing number of trainable parameters in modern large scale LLMs.Additionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), whichutilizes tensor train decomposition, has not yet achieved the level ofcompression necessary for fine-tuning very large scale models with limitedresources. This paper introduces Tensor Train Low-Rank Approximation (TT-LoRA),a novel parameter-efficient fine-tuning (PEFT) approach that extends LoRETTAwith optimized tensor train (TT) decomposition integration. By eliminatingAdapters and traditional LoRA-based structures, TT-LoRA achieves greater modelcompression without compromising downstream task performance, along withreduced inference latency and computational overhead. We conduct an exhaustiveparameter search to establish benchmarks that highlight the trade-off betweenmodel compression and performance. Our results demonstrate significantcompression of LLMs while maintaining comparable performance to larger models,facilitating their deployment on resource-constraint platforms.</description><author>Afia Anjum, Maksim E. Eren, Ismael Boureima, Boian Alexandrov, Manish Bhattarai</author><pubDate>Fri, 02 Aug 2024 04:45:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01008v1</guid></item><item><title>Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs</title><link>http://arxiv.org/abs/2408.02571v1</link><description>The emoticons are symbolic representations that generally accompany thetextual content to visually enhance or summarize the true intention of awritten message. Although widely utilized in the realm of social media, thecore semantics of these emoticons have not been extensively explored based onmultiple modalities. Incorporating textual and visual information within asingle message develops an advanced way of conveying information. Hence, thisresearch aims to analyze the relationship among sentences, visuals, andemoticons. For an orderly exposition, this paper initially provides a detailedexamination of the various techniques for extracting multimodal features,emphasizing the pros and cons of each method. Through conducting acomprehensive examination of several multimodal algorithms, with specificemphasis on the fusion approaches, we have proposed a novel contrastivelearning based multimodal architecture. The proposed model employs the jointtraining of dual-branch encoder along with the contrastive learning toaccurately map text and images into a common latent space. Our key finding isthat by integrating the principle of contrastive learning with that of theother two branches yields superior results. The experimental resultsdemonstrate that our suggested methodology surpasses existing multimodalapproaches in terms of accuracy and robustness. The proposed model attained anaccuracy of 91% and an MCC-score of 90% while assessing emoticons using theMultimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidencethat deep features acquired by contrastive learning are more efficient,suggesting that the proposed fusion technique also possesses stronggeneralisation capabilities for recognising emoticons across several modes.</description><author>Ananya Pandey, Dinesh Kumar Vishwakarma</author><pubDate>Mon, 05 Aug 2024 15:45:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02571v1</guid></item><item><title>MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance</title><link>http://arxiv.org/abs/2408.01869v1</link><description>In the era of Large Language Models (LLMs), given their remarkable textunderstanding and generation abilities, there is an unprecedented opportunityto develop new, LLM-based methods for trustworthy medical knowledge synthesis,extraction and summarization. This paper focuses on the problem ofPharmacovigilance (PhV), where the significance and challenges lie inidentifying Adverse Drug Events (ADEs) from diverse text sources, such asmedical literature, clinical notes, and drug labels. Unfortunately, this taskis hindered by factors including variations in the terminologies of drugs andoutcomes, and ADE descriptions often being buried in large amounts of narrativetext. We present MALADE, the first effective collaborative multi-agent systempowered by LLM with Retrieval Augmented Generation for ADE extraction from druglabel data. This technique involves augmenting a query to an LLM with relevantinformation extracted from text resources, and instructing the LLM to compose aresponse consistent with the augmented data. MALADE is a general LLM-agnosticarchitecture, and its unique capabilities are: (1) leveraging a variety ofexternal sources, such as medical literature, drug labels, and FDA tools (e.g.,OpenFDA drug information API), (2) extracting drug-outcome association in astructured format along with the strength of the association, and (3) providingexplanations for established associations. Instantiated with GPT-4 Turbo orGPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an AreaUnder ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Ourimplementation leverages the Langroid multi-agent LLM framework and can befound at https://github.com/jihyechoi77/malade.</description><author>Jihye Choi, Nils Palumbo, Prasad Chalasani, Matthew M. Engelhard, Somesh Jha, Anivarya Kumar, David Page</author><pubDate>Sat, 03 Aug 2024 22:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01869v1</guid></item><item><title>Graph Unfolding and Sampling for Transitory Video Summarization via Gershgorin Disc Alignment</title><link>http://arxiv.org/abs/2408.01859v1</link><description>User-generated videos (UGVs) uploaded from mobile phones to social mediasites like YouTube and TikTok are short and non-repetitive. We summarize atransitory UGV into several keyframes in linear time via fast graph samplingbased on Gershgorin disc alignment (GDA). Specifically, we first model asequence of $N$ frames in a UGV as an $M$-hop path graph $\mathcal{G}^o$ for $M\ll N$, where the similarity between two frames within $M$ time instants isencoded as a positive edge based on feature similarity. Towards efficientsampling, we then "unfold" $\mathcal{G}^o$ to a $1$-hop path graph$\mathcal{G}$, specified by a generalized graph Laplacian matrix $\mathcal{L}$,via one of two graph unfolding procedures with provable performance bounds. Weshow that maximizing the smallest eigenvalue $\lambda_{\min}(\mathbf{B})$ of acoefficient matrix $\mathbf{B} = \textit{diag}\left(\mathbf{h}\right) + \mu\mathcal{L}$, where $\mathbf{h}$ is the binary keyframe selection vector, isequivalent to minimizing a worst-case signal reconstruction error. We maximizeinstead the Gershgorin circle theorem (GCT) lower bound$\lambda^-_{\min}(\mathbf{B})$ by choosing $\mathbf{h}$ via a new fast graphsampling algorithm that iteratively aligns left-ends of Gershgorin discs forall graph nodes (frames). Extensive experiments on multiple short videodatasets show that our algorithm achieves comparable or better videosummarization performance compared to state-of-the-art methods, at asubstantially reduced complexity.</description><author>Sadid Sahami, Gene Cheung, Chia-Wen Lin</author><pubDate>Sat, 03 Aug 2024 20:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01859v1</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v7</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Tue, 06 Aug 2024 14:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v7</guid></item><item><title>Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization</title><link>http://arxiv.org/abs/2408.03149v1</link><description>The rapid increase in multimedia data has spurred advancements in MultimodalSummarization with Multimodal Output (MSMO), which aims to produce a multimodalsummary that integrates both text and relevant images. The inherentheterogeneity of content within multimodal inputs and outputs presents asignificant challenge to the execution of MSMO. Traditional approachestypically adopt a holistic perspective on coarse image-text data or individualvisual objects, overlooking the essential connections between objects and theentities they represent. To integrate the fine-grained entity knowledge, wepropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,building on BART, utilizes dual multimodal encoders with shared weights toprocess text-image and entity-image information concurrently. A gatingmechanism then combines visual data for enhanced textual summary generation,while image selection is refined through knowledge distillation from apre-trained vision-language model. Extensive experiments on public MSMO datasetvalidate the superiority of the EGMS method, which also prove the necessity toincorporate entity information into MSMO problem.</description><author>Yanghai Zhang, Ye Liu, Shiwei Wu, Kai Zhang, Xukai Liu, Qi Liu, Enhong Chen</author><pubDate>Tue, 06 Aug 2024 12:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03149v1</guid></item><item><title>L3iTC at the FinLLM Challenge Task: Quantization for Financial Text Classification &amp; Summarization</title><link>http://arxiv.org/abs/2408.03033v1</link><description>This article details our participation (L3iTC) in the FinLLM Challenge Task2024, focusing on two key areas: Task 1, financial text classification, andTask 2, financial text summarization. To address these challenges, wefine-tuned several large language models (LLMs) to optimize performance foreach task. Specifically, we used 4-bit quantization and LoRA to determine whichlayers of the LLMs should be trained at a lower precision. This approach notonly accelerated the fine-tuning process on the training data provided by theorganizers but also enabled us to run the models on low GPU memory. Ourfine-tuned models achieved third place for the financial classification taskwith an F1-score of 0.7543 and secured sixth place in the financialsummarization task on the official test datasets.</description><author>Elvys Linhares Pontes, Carlos-Emiliano González-Gallardo, Mohamed Benjannet, Caryn Qu, Antoine Doucet</author><pubDate>Tue, 06 Aug 2024 08:25:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03033v1</guid></item><item><title>ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval</title><link>http://arxiv.org/abs/2408.02978v1</link><description>E-commerce is increasingly multimedia-enriched, with products exhibited in abroad-domain manner as images, short videos, or live stream promotions. Aunified and vectorized cross-domain production representation is essential. Dueto large intra-product variance and high inter-product similarity in thebroad-domain scenario, a visual-only representation is inadequate. WhileAutomatic Speech Recognition (ASR) text derived from the short or live-streamvideos is readily accessible, how to de-noise the excessively noisy text formultimodal representation learning is mostly untouched. We propose ASR-enhancedMultimodal Product Representation Learning (AMPere). In order to extractproduct-specific information from the raw ASR text, AMPere uses aneasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,together with visual data, is then fed into a multi-branch network to generatecompact multimodal embeddings. Extensive experiments on a large-scaletri-domain dataset verify the effectiveness of AMPere in obtaining a unifiedmultimodal product representation that clearly improves cross-domain productretrieval.</description><author>Ruixiang Zhao, Jian Jia, Yan Li, Xuehan Bai, Quan Chen, Han Li, Peng Jiang, Xirong Li</author><pubDate>Tue, 06 Aug 2024 06:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02978v1</guid></item><item><title>A Framework for Fine-Tuning LLMs using Heterogeneous Feedback</title><link>http://arxiv.org/abs/2408.02861v1</link><description>Large language models (LLMs) have been applied to a wide range of tasks,including text summarization, web navigation, and chatbots. They havebenefitted from supervised fine-tuning (SFT) and reinforcement learning fromhuman feedback (RLHF) following an unsupervised pretraining. These datasets canbe difficult to collect, limited in scope, and vary in sample quality.Additionally, datasets can vary extensively in supervision format, fromnumerical to binary as well as multi-dimensional with many different values. Wepresent a framework for fine-tuning LLMs using heterogeneous feedback, whichhas two main components. First, we combine the heterogeneous feedback data intoa single supervision format, compatible with methods like SFT and RLHF. Next,given this unified feedback dataset, we extract a high-quality and diversesubset to obtain performance increases potentially exceeding the full dataset.We conduct extensive experiments to understand the effectiveness of thesetechniques for incorporating heterogeneous feedback, and demonstrateimprovements from using a high-quality and diverse subset of the data. We findthat our framework is able to improve models in multiple areas simultaneously,such as in instruction following and bias reduction.</description><author>Ryan Aponte, Ryan A. Rossi, Shunan Guo, Franck Dernoncourt, Tong Yu, Xiang Chen, Subrata Mitra, Nedim Lipka</author><pubDate>Mon, 05 Aug 2024 23:20:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02861v1</guid></item><item><title>'Finance Wizard' at the FinLLM Challenge Task: Financial Text Summarization</title><link>http://arxiv.org/abs/2408.03762v1</link><description>This paper presents our participation under the team name `Finance Wizard' inthe FinNLP-AgentScen 2024 shared task #2: Financial Text Summarization. Itdocuments our pipeline approach of fine-tuning a foundation model into atask-specific model for Financial Text Summarization. It involves (1) adaptingLlama3 8B, a foundation model, to the Finance domain via continuedpre-training, (2) multi-task instruction-tuning to further equip the model withmore finance-related capabilities, (3) finally fine-tuning the model into atask-specific `expert'. Our model, FinLlama3\_sum, yielded commendable results,securing the third position in its category with a ROUGE-1 score of 0.521.</description><author>Meisin Lee, Soon Lay-Ki</author><pubDate>Wed, 07 Aug 2024 13:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03762v1</guid></item><item><title>MMSummary: Multimodal Summary Generation for Fetal Ultrasound Video</title><link>http://arxiv.org/abs/2408.03761v1</link><description>We present the first automated multimodal summary generation system,MMSummary, for medical imaging video, particularly with a focus on fetalultrasound analysis. Imitating the examination process performed by a humansonographer, MMSummary is designed as a three-stage pipeline, progressing fromkeyframe detection to keyframe captioning and finally anatomy segmentation andmeasurement. In the keyframe detection stage, an innovative automated workflowis proposed to progressively select a concise set of keyframes, preservingsufficient video information without redundancy. Subsequently, we adapt a largelanguage model to generate meaningful captions for fetal ultrasound keyframesin the keyframe captioning stage. If a keyframe is captioned as fetal biometry,the segmentation and measurement stage estimates biometric parameters bysegmenting the region of interest according to the textual prior. The MMSummarysystem provides comprehensive summaries for fetal ultrasound examinations andbased on reported experiments is estimated to reduce scanning time byapproximately 31.5%, thereby suggesting the potential to enhance clinicalworkflow efficiency.</description><author>Xiaoqing Guo, Qianhui Men, J. Alison Noble</author><pubDate>Wed, 07 Aug 2024 13:30:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03761v1</guid></item><item><title>AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations</title><link>http://arxiv.org/abs/2407.16010v2</link><description>For many use-cases, it is often important to explain the prediction of ablack-box model by identifying the most influential training data samples.Existing approaches lack customization for user intent and often provide ahomogeneous set of explanation samples, failing to reveal the model's reasoningfrom different angles. In this paper, we propose AIDE, an approach for providing antithetical (i.e.,contrastive), intent-based, diverse explanations for opaque and complex models.AIDE distinguishes three types of explainability intents: interpreting acorrect, investigating a wrong, and clarifying an ambiguous prediction. Foreach intent, AIDE selects an appropriate set of influential training samplesthat support or oppose the prediction either directly or by contrast. Toprovide a succinct summary, AIDE uses diversity-aware sampling to avoidredundancy and increase coverage of the training data. We demonstrate the effectiveness of AIDE on image and text classificationtasks, in three ways: quantitatively, assessing correctness and continuity;qualitatively, comparing anecdotal evidence from AIDE and other example-basedapproaches; and via a user study, evaluating multiple aspects of AIDE. Theresults show that AIDE addresses the limitations of existing methods andexhibits desirable traits for an explainability method.</description><author>Ikhtiyor Nematov, Dimitris Sacharidis, Tomer Sagi, Katja Hose</author><pubDate>Thu, 08 Aug 2024 09:12:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16010v2</guid></item><item><title>A Review of Hybrid and Ensemble in Deep Learning for Natural Language Processing</title><link>http://arxiv.org/abs/2312.05589v2</link><description>This review presents a comprehensive exploration of hybrid and ensemble deeplearning models within Natural Language Processing (NLP), shedding light ontheir transformative potential across diverse tasks such as Sentiment Analysis,Named Entity Recognition, Machine Translation, Question Answering, TextClassification, Generation, Speech Recognition, Summarization, and LanguageModeling. The paper systematically introduces each task, delineates keyarchitectures from Recurrent Neural Networks (RNNs) to Transformer-based modelslike BERT, and evaluates their performance, challenges, and computationaldemands. The adaptability of ensemble techniques is emphasized, highlightingtheir capacity to enhance various NLP applications. Challenges inimplementation, including computational overhead, overfitting, and modelinterpretation complexities, are addressed alongside the trade-off betweeninterpretability and performance. Serving as a concise yet invaluable guide,this review synthesizes insights into tasks, architectures, and challenges,offering a holistic perspective for researchers and practitioners aiming toadvance language-driven applications through ensemble deep learning in NLP.</description><author>Jianguo Jia, Wen Liang, Youzhi Liang</author><pubDate>Thu, 08 Aug 2024 07:15:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05589v2</guid></item><item><title>Tell Me What's Next: Textual Foresight for Generic UI Representations</title><link>http://arxiv.org/abs/2406.07822v2</link><description>Mobile app user interfaces (UIs) are rich with action, text, structure, andimage content that can be utilized to learn generic UI representations fortasks like automating user commands, summarizing content, and evaluating theaccessibility of user interfaces. Prior work has learned strong visualrepresentations with local or global captioning losses, but fails to retainboth granularities. To combat this, we propose Textual Foresight, a novelpretraining objective for learning UI screen representations. Textual Foresightgenerates global text descriptions of future UI states given a current UI andlocal action taken. Our approach requires joint reasoning over elements andentire screens, resulting in improved UI features: on generation tasks, UIagents trained with Textual Foresight outperform state-of-the-art by 2% with28x fewer images. We train with our newly constructed mobile app dataset,OpenApp, which results in the first public dataset for app UI representationlearning. OpenApp enables new baselines, and we find Textual Foresight improvesaverage task performance over them by 5.7% while having access to 2x less data.</description><author>Andrea Burns, Kate Saenko, Bryan A. Plummer</author><pubDate>Thu, 08 Aug 2024 02:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07822v2</guid></item><item><title>Text Clustering with LLM Embeddings</title><link>http://arxiv.org/abs/2403.15112v4</link><description>Text clustering is an important method for organising the increasing volumeof digital content, aiding in the structuring and discovery of hidden patternsin uncategorised data. The effectiveness of text clustering largely depends onthe selection of textual embeddings and clustering algorithms. This studyargues that recent advancements in large language models (LLMs) have thepotential to enhance this task. The research investigates how different textualembeddings, particularly those utilised in LLMs, and various clusteringalgorithms influence the clustering of text datasets. A series of experimentswere conducted to evaluate the impact of embeddings on clustering results, therole of dimensionality reduction through summarisation, and the adjustment ofmodel size. The findings indicate that LLM embeddings are superior at capturingsubtleties in structured language. OpenAI's GPT-3.5 Turbo model yields betterresults in three out of five clustering metrics across most tested datasets.Most LLM embeddings show improvements in cluster purity and provide a moreinformative silhouette score, reflecting a refined structural understanding oftext data compared to traditional methods. Among the more lightweight models,BERT demonstrates leading performance. Additionally, it was observed thatincreasing model dimensionality and employing summarisation techniques do notconsistently enhance clustering efficiency, suggesting that these strategiesrequire careful consideration for practical application. These resultshighlight a complex balance between the need for refined text representationand computational feasibility in text clustering applications. This studyextends traditional text clustering frameworks by integrating embeddings fromLLMs, offering improved methodologies and suggesting new avenues for futureresearch in various types of textual analysis.</description><author>Alina Petukhova, João P. Matos-Carvalho, Nuno Fachada</author><pubDate>Fri, 09 Aug 2024 16:57:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15112v4</guid></item><item><title>Efficient automatic segmentation for multi-level pulmonary arteries: The PARSE challenge</title><link>http://arxiv.org/abs/2304.03708v2</link><description>Efficient automatic segmentation of multi-level (i.e. main and branch)pulmonary arteries (PA) in CTPA images plays a significant role in clinicalapplications. However, most existing methods concentrate only on main PA orbranch PA segmentation separately and ignore segmentation efficiency. Besides,there is no public large-scale dataset focused on PA segmentation, which makesit highly challenging to compare the different methods. To benchmarkmulti-level PA segmentation algorithms, we organized the first\textbf{P}ulmonary \textbf{AR}tery \textbf{SE}gmentation (PARSE) challenge. Onthe one hand, we focus on both the main PA and the branch PA segmentation. Onthe other hand, for better clinical application, we assign the same scoreweight to segmentation efficiency (mainly running time and GPU memoryconsumption during inference) while ensuring PA segmentation accuracy. Wepresent a summary of the top algorithms and offer some suggestions forefficient and accurate multi-level PA automatic segmentation. We provide thePARSE challenge as open-access for the community to benchmark future algorithmdevelopments at \url{https://parse2022.grand-challenge.org/Parse2022/}.</description><author>Gongning Luo, Kuanquan Wang, Jun Liu, Shuo Li, Xinjie Liang, Xiangyu Li, Shaowei Gan, Wei Wang, Suyu Dong, Wenyi Wang, Pengxin Yu, Enyou Liu, Hongrong Wei, Na Wang, Jia Guo, Huiqi Li, Zhao Zhang, Ziwei Zhao, Na Gao, Nan An, Ashkan Pakzad, Bojidar Rangelov, Jiaqi Dou, Song Tian, Zeyu Liu, Yi Wang, Ampatishan Sivalingam, Kumaradevan Punithakumar, Zhaowen Qiu, Xin Gao</author><pubDate>Fri, 09 Aug 2024 17:17:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03708v2</guid></item><item><title>Quantitative Information Extraction from Humanitarian Documents</title><link>http://arxiv.org/abs/2408.04941v1</link><description>Humanitarian action is accompanied by a mass of reports, summaries, news, andother documents. To guide its activities, important information must be quicklyextracted from such free-text resources. Quantities, such as the number ofpeople affected, amount of aid distributed, or the extent of infrastructuredamage, are central to emergency response and anticipatory action. In thiswork, we contribute an annotated dataset for the humanitarian domain for theextraction of such quantitative information, along side its important context,including units it refers to, any modifiers, and the relevant event. Further,we develop a custom Natural Language Processing pipeline to extract thequantities alongside their units, and evaluate it in comparison to baseline andrecent literature. The proposed model achieves a consistent improvement in theperformance, especially in the documents pertaining to the Dominican Republicand select African countries. We make the dataset and code available to theresearch community to continue the improvement of NLP tools for thehumanitarian domain.</description><author>Daniele Liberatore, Kyriaki Kalimeri, Derya Sever, Yelena Mejova</author><pubDate>Fri, 09 Aug 2024 08:46:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04941v1</guid></item><item><title>Exploration of Masked and Causal Language Modelling for Text Generation</title><link>http://arxiv.org/abs/2405.12630v2</link><description>Large Language Models (LLMs) have revolutionised the field of NaturalLanguage Processing (NLP) and have achieved state-of-the-art performance inpractically every task in this field. However, the prevalent approach used intext generation, Causal Language Modelling (CLM), which generates textsequentially from left to right, inherently limits the freedom of the model,which does not decide when and where each token is generated. In contrast,Masked Language Modelling (MLM), primarily used for language understandingtasks, can generate tokens anywhere in the text and any order. This paperconducts an extensive comparison of MLM and CLM approaches for text generationtasks. To do so, we pre-train several language models of comparable sizes onthree different datasets, namely 1) medical discharge summaries, 2) movie plotsynopses, and 3) authorship verification datasets. To assess the quality of thegenerations, we first employ quantitative metrics and then perform aqualitative human evaluation to analyse coherence and grammatical correctness.In addition, we evaluate the usefulness of the generated texts by using them inthree different downstream tasks: 1) Entity Recognition, 2) TextClassification, and 3) Authorship Verification. The results show that MLMconsistently outperforms CLM in text generation across all datasets, withhigher quantitative scores and better coherence in the generated text. Thestudy also finds \textit{no strong correlation} between the quality of thegenerated text and the performance of the models in the downstream tasks. Withthis study, we show that MLM for text generation has great potential for futureresearch and provides direction for future studies in this area.</description><author>Nicolo Micheletti, Samuel Belkadi, Lifeng Han, Goran Nenadic</author><pubDate>Fri, 09 Aug 2024 00:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.12630v2</guid></item><item><title>Tamil Language Computing: the Present and the Future</title><link>http://arxiv.org/abs/2407.08618v2</link><description>This paper delves into the text processing aspects of Language Computing,which enables computers to understand, interpret, and generate human language.Focusing on tasks such as speech recognition, machine translation, sentimentanalysis, text summarization, and language modelling, language computingintegrates disciplines including linguistics, computer science, and cognitivepsychology to create meaningful human-computer interactions. Recentadvancements in deep learning have made computers more accessible and capableof independent learning and adaptation. In examining the landscape of languagecomputing, the paper emphasises foundational work like encoding, where Tamiltransitioned from ASCII to Unicode, enhancing digital communication. Itdiscusses the development of computational resources, including raw data,dictionaries, glossaries, annotated data, and computational grammars, necessaryfor effective language processing. The challenges of linguistic annotation, thecreation of treebanks, and the training of large language models are alsocovered, emphasising the need for high-quality, annotated data and advancedlanguage models. The paper underscores the importance of building practicalapplications for languages like Tamil to address everyday communication needs,highlighting gaps in current technology. It calls for increased researchcollaboration, digitization of historical texts, and fostering digital usage toensure the comprehensive development of Tamil language processing, ultimatelyenhancing global communication and access to digital services.</description><author>Kengatharaiyer Sarveswaran</author><pubDate>Mon, 12 Aug 2024 09:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08618v2</guid></item><item><title>Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges</title><link>http://arxiv.org/abs/2405.15604v2</link><description>Text generation has become more accessible than ever, and the increasinginterest in these systems, especially those using large language models, hasspurred an increasing number of related publications. We provide a systematicliterature review comprising 244 selected papers between 2017 and 2024. Thisreview categorizes works in text generation into five main tasks: open-endedtext generation, summarization, translation, paraphrasing, and questionanswering. For each task, we review their relevant characteristics, sub-tasks,and specific challenges (e.g., missing datasets for multi-documentsummarization, coherence in story generation, and complex reasoning forquestion answering). Additionally, we assess current approaches for evaluatingtext generation systems and ascertain problems with current metrics. Ourinvestigation shows nine prominent challenges common to all tasks and sub-tasksin recent text generation publications: bias, reasoning, hallucinations,misuse, privacy, interpretability, transparency, datasets, and computing. Weprovide a detailed analysis of these challenges, their potential solutions, andwhich gaps still require further engagement from the community. This systematicliterature review targets two main audiences: early career researchers innatural language processing looking for an overview of the field and promisingresearch directions, as well as experienced researchers seeking a detailed viewof tasks, evaluation methodologies, open challenges, and recent mitigationstrategies.</description><author>Jonas Becker, Jan Philip Wahle, Bela Gipp, Terry Ruas</author><pubDate>Mon, 12 Aug 2024 08:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15604v2</guid></item><item><title>Creating Arabic LLM Prompts at Scale</title><link>http://arxiv.org/abs/2408.05882v1</link><description>The debut of chatGPT and BARD has popularized instruction following textgeneration using LLMs, where a user can interrogate an LLM using naturallanguage requests and obtain natural language answers that matches theirrequests. Training LLMs to respond in this manner requires a large number ofworked out examples of user requests (aka prompts) with corresponding goldresponses. In this paper, we introduce two methods for creating such promptsfor Arabic cheaply and quickly. The first methods entails automaticallytranslating existing prompt datasets from English, such as PromptSource andSuper-NaturalInstructions, and then using machine translation qualityestimation to retain high quality translations only. The second method involvescreating natural language prompts on top of existing Arabic NLP datasets. Usingthese two methods we were able to create more than 67.4 million Arabic promptsthat cover a variety of tasks including summarization, headline generation,grammar checking, open/closed question answering, creative writing, etc. Weshow that fine tuning an open 7 billion parameter large language model, namelybase Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameterinstruction tuned model, namely Llama3 70B, in handling Arabic prompts.</description><author>Abdelrahman El-Sheikh, Ahmed Elmogtaba, Kareem Darwish, Muhammad Elmallah, Ashraf Elneima, Hassan Sawaf</author><pubDate>Mon, 12 Aug 2024 00:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05882v1</guid></item><item><title>AutoTemplate: A Simple Recipe for Lexically Constrained Text Generation</title><link>http://arxiv.org/abs/2211.08387v2</link><description>Lexically constrained text generation is one of the constrained textgeneration tasks, which aims to generate text that covers all the givenconstraint lexicons. While the existing approaches tackle this problem using alexically constrained beam search algorithm or dedicated model usingnon-autoregressive decoding, there is a trade-off between the generated textquality and the hard constraint satisfaction. We introduce AutoTemplate, asimple yet effective lexically constrained text generation framework dividedinto template generation and lexicalization tasks. The template generation isto generate the text with the placeholders, and lexicalization replaces theminto the constraint lexicons to perform lexically constrained text generation.We conducted the experiments on two tasks: keywords-to-sentence generations andentity-guided summarization. Experimental results show that the AutoTemplateoutperforms the competitive baselines on both tasks while satisfying the hardlexical constraints. The code is available athttps://github.com/megagonlabs/autotemplate</description><author>Hayate Iso</author><pubDate>Fri, 09 Aug 2024 23:34:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.08387v2</guid></item><item><title>A Novel Computational and Modeling Foundation for Automatic Coherence Assessment</title><link>http://arxiv.org/abs/2310.00598v2</link><description>Coherence is an essential property of well-written texts, that refers to theway textual units relate to one another. In the era of generative AI, coherenceassessment is essential for many NLP tasks; summarization, generation,long-form question-answering, and more. However, in NLP {coherence} is anill-defined notion, not having a formal definition or evaluation metrics, thatwould allow for large-scale automatic and systematic coherence assessment. Tobridge this gap, in this work we employ the formal linguistic definition of\citet{Reinhart:1980} of what makes a discourse coherent, consisting of threeconditions -- {\em cohesion, consistency} and {\em relevance} -- and formalizethese conditions as respective computational tasks. We hypothesize that (i) amodel trained on all of these tasks will learn the features required forcoherence detection, and that (ii) a joint model for all tasks will exceed theperformance of models trained on each task individually. On two benchmarks forcoherence scoring rated by humans, one containing 500 automatically-generatedshort stories and another containing 4k real-world texts, our experimentsconfirm that jointly training on the proposed tasks leads to better performanceon each task compared with task-specific models, and to better performance onassessing coherence overall, compared with strong baselines. We conclude thatthe formal and computational setup of coherence as proposed here provides asolid foundation for advanced methods of large-scale automatic assessment ofcoherence.</description><author>Aviya Maimon, Reut Tsarfaty</author><pubDate>Tue, 13 Aug 2024 13:19:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00598v2</guid></item><item><title>Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</title><link>http://arxiv.org/abs/2407.18003v3</link><description>Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,have revolutionized various industries with their advanced languagecomprehension. However, their efficiency is challenged by the Transformerarchitecture' s struggle with handling long texts. KV-Cache has emerged as apivotal solution to this issue, converting the time complexity of tokengeneration from quadratic to linear, albeit with increased GPU memory overheadproportional to conversation length. With the development of the LLM communityand academia, various KV-Cache compression methods have been proposed. In thisreview, we dissect the various properties of KV-Cache and elaborate on variousmethods currently used to optimize the KV-Cache space usage of LLMs. Thesemethods span the pre-training phase, deployment phase, and inference phase, andwe summarize the commonalities and differences among these methods.Additionally, we list some metrics for evaluating the long-text capabilities oflarge language models, from both efficiency and capability perspectives. Ourreview thus sheds light on the evolving landscape of LLM optimization, offeringinsights into future advancements in this dynamic field.</description><author>Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao</author><pubDate>Tue, 13 Aug 2024 09:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18003v3</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v8</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Fri, 09 Aug 2024 14:48:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v8</guid></item><item><title>Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey</title><link>http://arxiv.org/abs/2408.07583v1</link><description>With significant advancements in Transformers LLMs, NLP has extended itsreach into many research fields due to its enhanced capabilities in textgeneration and user interaction. One field benefiting greatly from theseadvancements is cybersecurity. In cybersecurity, many parameters that need tobe protected and exchanged between senders and receivers are in the form oftext and tabular data, making NLP a valuable tool in enhancing the securitymeasures of communication protocols. This survey paper provides a comprehensiveanalysis of the utilization of Transformers and LLMs in cyber-threat detectionsystems. The methodology of paper selection and bibliometric analysis isoutlined to establish a rigorous framework for evaluating existing research.The fundamentals of Transformers are discussed, including backgroundinformation on various cyber-attacks and datasets commonly used in this field.The survey explores the application of Transformers in IDSs, focusing ondifferent architectures such as Attention-based models, LLMs like BERT and GPT,CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.Furthermore, it explores the diverse environments and applications whereTransformers and LLMs-based IDS have been implemented, including computernetworks, IoT devices, critical infrastructure protection, cloud computing,SDN, as well as in autonomous vehicles. The paper also addresses researchchallenges and future directions in this area, identifying key issues such asinterpretability, scalability, and adaptability to evolving threats, and more.Finally, the conclusion summarizes the findings and highlights the significanceof Transformers and LLMs in enhancing cyber-threat detection capabilities,while also outlining potential avenues for further research and development.</description><author>Hamza Kheddar</author><pubDate>Wed, 14 Aug 2024 14:28:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07583v1</guid></item><item><title>$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation</title><link>http://arxiv.org/abs/2402.19457v3</link><description>Assessing the quality of summarizers poses significant challenges. Inresponse, we propose a novel task-oriented evaluation approach that assessessummarizers based on their capacity to produce summaries that are useful fordownstream tasks, while preserving task outcomes. We theoretically establish adirect relationship between the resulting error probability of these tasks andthe mutual information between source texts and generated summaries. Weintroduce $\texttt{COSMIC}$ as a practical implementation of this metric,demonstrating its strong correlation with human judgment-based metrics and itseffectiveness in predicting downstream task performance. Comparative analysesagainst established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$highlight the competitive performance of $\texttt{COSMIC}$.</description><author>Maxime Darrin, Philippe Formont, Jackie Chi Kit Cheung, Pablo Piantanida</author><pubDate>Wed, 14 Aug 2024 14:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19457v3</guid></item><item><title>MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL</title><link>http://arxiv.org/abs/2408.07930v1</link><description>Recent In-Context Learning based methods have achieved remarkable success inText-to-SQL task. However, there is still a large gap between the performanceof these models and human performance on datasets with complex database schemaand difficult questions, such as BIRD. Besides, existing work has neglected tosupervise intermediate steps when solving questions iteratively with questiondecomposition methods, and the schema linking methods used in these works arevery rudimentary. To address these issues, we propose MAG-SQL, a multi-agentgenerative approach with soft schema linking and iterative Sub-SQL refinement.In our framework, an entity-based method with tables' summary is used to selectthe columns in database, and a novel targets-conditions decomposition method isintroduced to decompose those complex questions. Additionally, we build aiterative generating module which includes a Sub-SQL Generator and Sub-SQLRefiner, introducing external oversight for each step of generation. Through aseries of ablation studies, the effectiveness of each agent in our frameworkhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQLachieves an execution accuracy of 61.08\%, compared to the baseline accuracy of46.35\% for vanilla GPT-4 and the baseline accuracy of 57.56\% for MAC-SQL.Besides, our approach makes similar progress on Spider.</description><author>Wenxuan Xie, Gaochen Wu, Bowen Zhou</author><pubDate>Thu, 15 Aug 2024 04:57:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07930v1</guid></item><item><title>An Efficient and Explanatory Image and Text Clustering System with Multimodal Autoencoder Architecture</title><link>http://arxiv.org/abs/2408.07791v1</link><description>We demonstrate the efficiencies and explanatory abilities of extensions tothe common tools of Autoencoders and LLM interpreters, in the novel context ofcomparing different cultural approaches to the same international news event.We develop a new Convolutional-Recurrent Variational Autoencoder (CRVAE) modelthat extends the modalities of previous CVAE models, by using fully-connectedlatent layers to embed in parallel the CNN encodings of video frames, togetherwith the LSTM encodings of their related text derived from audio. Weincorporate the model within a larger system that includes frame-captionalignment, latent space vector clustering, and a novel LLM-based clusterinterpreter. We measure, tune, and apply this system to the task of summarizinga video into three to five thematic clusters, with each theme described by tenLLM-produced phrases. We apply this system to two news topics, COVID-19 and theWinter Olympics, and five other topics are in progress.</description><author>Tiancheng Shi, Yuanchen Wei, John R. Kender</author><pubDate>Wed, 14 Aug 2024 20:03:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07791v1</guid></item><item><title>MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL</title><link>http://arxiv.org/abs/2408.07930v2</link><description>Recent In-Context Learning based methods have achieved remarkable success inText-to-SQL task. However, there is still a large gap between the performanceof these models and human performance on datasets with complex database schemaand difficult questions, such as BIRD. Besides, existing work has neglected tosupervise intermediate steps when solving questions iteratively with questiondecomposition methods, and the schema linking methods used in these works arevery rudimentary. To address these issues, we propose MAG-SQL, a multi-agentgenerative approach with soft schema linking and iterative Sub-SQL refinement.In our framework, an entity-based method with tables' summary is used to selectthe columns in database, and a novel targets-conditions decomposition method isintroduced to decompose those complex questions. Additionally, we build aiterative generating module which includes a Sub-SQL Generator and Sub-SQLRefiner, introducing external oversight for each step of generation. Through aseries of ablation studies, the effectiveness of each agent in our frameworkhas been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQLachieves an execution accuracy of 61.08%, compared to the baseline accuracy of46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.Besides, our approach makes similar progress on Spider.</description><author>Wenxuan Xie, Gaochen Wu, Bowen Zhou</author><pubDate>Fri, 16 Aug 2024 02:55:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07930v2</guid></item><item><title>Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey</title><link>http://arxiv.org/abs/2406.08068v2</link><description>Compared to traditional sentiment analysis, which only considers text,multimodal sentiment analysis needs to consider emotional signals frommultimodal sources simultaneously and is therefore more consistent with the wayhow humans process sentiment in real-world scenarios. It involves processingemotional information from various sources such as natural language, images,videos, audio, physiological signals, etc. However, although other modalitiesalso contain diverse emotional cues, natural language usually contains richercontextual information and therefore always occupies a crucial position inmultimodal sentiment analysis. The emergence of ChatGPT has opened up immensepotential for applying large language models (LLMs) to text-centric multimodaltasks. However, it is still unclear how existing LLMs can adapt better totext-centric multimodal sentiment analysis tasks. This survey aims to (1)present a comprehensive review of recent research in text-centric multimodalsentiment analysis tasks, (2) examine the potential of LLMs for text-centricmultimodal sentiment analysis, outlining their approaches, advantages, andlimitations, (3) summarize the application scenarios of LLM-based multimodalsentiment analysis technology, and (4) explore the challenges and potentialresearch directions for multimodal sentiment analysis in the future.</description><author>Hao Yang, Yanyan Zhao, Yang Wu, Shilong Wang, Tian Zheng, Hongbo Zhang, Zongyang Ma, Wanxiang Che, Bing Qin</author><pubDate>Fri, 16 Aug 2024 10:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08068v2</guid></item><item><title>Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models</title><link>http://arxiv.org/abs/2408.10124v1</link><description>Molecular property prediction is a crucial foundation for drug discovery. Inrecent years, pre-trained deep learning models have been widely applied to thistask. Some approaches that incorporate prior biological domain knowledge intothe pre-training framework have achieved impressive results. However, thesemethods heavily rely on biochemical experts, and retrieving and summarizingvast amounts of domain knowledge literature is both time-consuming andexpensive. Large Language Models (LLMs) have demonstrated remarkableperformance in understanding and efficiently providing general knowledge.Nevertheless, they occasionally exhibit hallucinations and lack precision ingenerating domain-specific knowledge. Conversely, Domain-specific Small Models(DSMs) possess rich domain knowledge and can accurately calculate moleculardomain-related metrics. However, due to their limited model size and singularfunctionality, they lack the breadth of knowledge necessary for comprehensiverepresentation learning. To leverage the advantages of both approaches inmolecular property prediction, we propose a novel Molecular Graphrepresentation learning framework that integrates Large language models andDomain-specific small models (MolGraph-LarDo). Technically, we design atwo-stage prompt strategy where DSMs are introduced to calibrate the knowledgeprovided by LLMs, enhancing the accuracy of domain-specific information andthus enabling LLMs to generate more precise textual descriptions for molecularsamples. Subsequently, we employ a multi-modal alignment method to coordinatevarious modalities, including molecular graphs and their correspondingdescriptive texts, to guide the pre-training of molecular representations.Extensive experiments demonstrate the effectiveness of the proposed method.</description><author>Tianyu Zhang, Yuxiang Ren, Chengbin Hou, Hairong Lv, Xuegong Zhang</author><pubDate>Mon, 19 Aug 2024 16:11:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10124v1</guid></item><item><title>GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization</title><link>http://arxiv.org/abs/2408.10115v1</link><description>Pre-trained language models are increasingly being used in multi-documentsummarization tasks. However, these models need large-scale corpora forpre-training and are domain-dependent. Other non-neural unsupervisedsummarization approaches mostly rely on key sentence extraction, which can leadto information loss. To address these challenges, we propose a lightweight yeteffective unsupervised approach called GLIMMER: a Graph and LexIcal featuresbased unsupervised Multi-docuMEnt summaRization approach. It first constructs asentence graph from the source documents, then automatically identifiessemantic clusters by mining low-level features from raw texts, therebyimproving intra-cluster correlation and the fluency of generated sentences.Finally, it summarizes clusters into natural sentences. Experiments conductedon Multi-News, Multi-XScience and DUC-2004 demonstrate that our approachoutperforms existing unsupervised approaches. Furthermore, it surpassesstate-of-the-art pre-trained multi-document summarization models (e.g. PEGASUSand PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally,human evaluations indicate that summaries generated by GLIMMER achieve highreadability and informativeness scores. Our code is available athttps://github.com/Oswald1997/GLIMMER.</description><author>Ran Liu, Ming Liu, Min Yu, Jianguo Jiang, Gang Li, Dan Zhang, Jingyuan Li, Xiang Meng, Weiqing Huang</author><pubDate>Mon, 19 Aug 2024 16:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10115v1</guid></item><item><title>Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation</title><link>http://arxiv.org/abs/2408.09698v2</link><description>Recent advances in Large Language Models (LLMs) have demonstrated significantpotential in the field of Recommendation Systems (RSs). Most existing studieshave focused on converting user behavior logs into textual prompts andleveraging techniques such as prompt tuning to enable LLMs for recommendationtasks. Meanwhile, research interest has recently grown in multimodalrecommendation systems that integrate data from images, text, and other sourcesusing modality fusion techniques. This introduces new challenges to theexisting LLM-based recommendation paradigm which relies solely on text modalityinformation. Moreover, although Multimodal Large Language Models (MLLMs)capable of processing multi-modal inputs have emerged, how to equip MLLMs withmulti-modal recommendation capabilities remains largely unexplored. To thisend, in this paper, we propose the Multimodal Large Language Model-enhancedMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamicuser preference, we design a two-stage user preference summarization method.Specifically, we first utilize an MLLM-based item-summarizer to extract imagefeature given an item and convert the image into text. Then, we employ arecurrent user preference summarization generation paradigm to capture thedynamic changes in user preferences based on an LLM-based user-summarizer.Finally, to enable the MLLM for multi-modal recommendation task, we propose tofine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)techniques. Extensive evaluations across various datasets validate theeffectiveness of MLLM-MSR, showcasing its superior ability to capture and adaptto the evolving dynamics of user preferences.</description><author>Yuyang Ye, Zhi Zheng, Yishan Shen, Tianshu Wang, Hengruo Zhang, Peijun Zhu, Runlong Yu, Kai Zhang, Hui Xiong</author><pubDate>Tue, 20 Aug 2024 16:09:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.09698v2</guid></item><item><title>Language-Guided Self-Supervised Video Summarization Using Text Semantic Matching Considering the Diversity of the Video</title><link>http://arxiv.org/abs/2405.08890v2</link><description>Current video summarization methods rely heavily on supervised computervision techniques, which demands time-consuming and subjective manualannotations. To overcome these limitations, we investigated self-supervisedvideo summarization. Inspired by the success of Large Language Models (LLMs),we explored the feasibility in transforming the video summarization task into aNatural Language Processing (NLP) task. By leveraging the advantages of LLMs incontext understanding, we aim to enhance the effectiveness of self-supervisedvideo summarization. Our method begins by generating captions for individualvideo frames, which are then synthesized into text summaries by LLMs.Subsequently, we measure semantic distance between the captions and the textsummary. Notably, we propose a novel loss function to optimize our modelaccording to the diversity of the video. Finally, the summarized video can begenerated by selecting the frames with captions similar to the text summary.Our method achieves state-of-the-art performance on the SumMe dataset in rankcorrelation coefficients. In addition, our method has a novel feature of beingable to achieve personalized summarization.</description><author>Tomoya Sugihara, Shuntaro Masuda, Ling Xiao, Toshihiko Yamasaki</author><pubDate>Tue, 20 Aug 2024 14:19:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08890v2</guid></item><item><title>Towards Efficient Large Language Models for Scientific Text: A Review</title><link>http://arxiv.org/abs/2408.10729v1</link><description>Large language models (LLMs) have ushered in a new era for processing complexinformation in various fields, including science. The increasing amount ofscientific literature allows these models to acquire and understand scientificknowledge effectively, thus improving their performance in a wide range oftasks. Due to the power of LLMs, they require extremely expensive computationalresources, intense amounts of data, and training time. Therefore, in recentyears, researchers have proposed various methodologies to make scientific LLMsmore affordable. The most well-known approaches align in two directions. It canbe either focusing on the size of the models or enhancing the quality of data.To date, a comprehensive review of these two families of methods has not yetbeen undertaken. In this paper, we (I) summarize the current advances in theemerging abilities of LLMs into more accessible AI solutions for science, and(II) investigate the challenges and opportunities of developing affordablesolutions for scientific domains using LLMs.</description><author>Huy Quoc To, Ming Liu, Guangyan Huang</author><pubDate>Tue, 20 Aug 2024 10:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10729v1</guid></item><item><title>One Law, Many Languages: Benchmarking Multilingual Legal Reasoning for Judicial Support</title><link>http://arxiv.org/abs/2306.09237v3</link><description>Recent strides in Large Language Models (LLMs) have saturated many NaturalLanguage Processing (NLP) benchmarks, emphasizing the need for more challengingones to properly assess LLM capabilities. However, domain-specific andmultilingual benchmarks are rare because they require in-depth expertise todevelop. Still, most public models are trained predominantly on Englishcorpora, while other languages remain understudied, particularly for practicaldomain-specific NLP tasks. In this work, we introduce a novel NLP benchmark forthe legal domain that challenges LLMs in five key dimensions: processing\emph{long documents} (up to 50K tokens), using \emph{domain-specificknowledge} (embodied in legal texts), \emph{multilingual} understanding(covering five languages), \emph{multitasking} (comprising legaldocument-to-document Information Retrieval, Court View Generation, LeadingDecision Summarization, Citation Extraction, and eight challenging TextClassification tasks) and \emph{reasoning} (comprising especially Court ViewGeneration, but also the Text Classification tasks). Our benchmark containsdiverse datasets from the Swiss legal system, allowing for a comprehensivestudy of the underlying non-English, inherently multilingual legal system.Despite the large size of our datasets (some with hundreds of thousands ofexamples), existing publicly available multilingual models struggle with mosttasks, even after extensive in-domain pre-training and fine-tuning. We publishall resources (benchmark suite, pre-trained models, code) under permissive openCC BY-SA licenses.</description><author>Ronja Stern, Vishvaksenan Rasiah, Veton Matoshi, Srinanda Brügger Bose, Matthias Stürmer, Ilias Chalkidis, Daniel E. Ho, Joel Niklaus</author><pubDate>Wed, 21 Aug 2024 10:36:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09237v3</guid></item><item><title>V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning</title><link>http://arxiv.org/abs/2404.12353v2</link><description>Video summarization aims to create short, accurate, and cohesive summaries oflonger videos. Despite the existence of various video summarization datasets, anotable limitation is their limited amount of source videos, which hampers theeffective training of advanced large vision-language models (VLMs).Additionally, most existing datasets are created for video-to-videosummarization, overlooking the contemporary need for multimodal video contentsummarization. Recent efforts have been made to expand from unimodal tomultimodal video summarization, categorizing the task into three sub-tasksbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), anda combination of video and text summarization (V2VT). However, the textualsummaries in previous multimodal datasets are inadequate. To address theseissues, we introduce Instruct-V2Xum, a cross-modal video summarization datasetfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from40 to 940 seconds and an average summarization ratio of 16.39%. Each videosummary in Instruct-V2Xum is paired with a textual summary that referencesspecific frame indexes, facilitating the generation of aligned video andtextual summaries. In addition, we propose a new video summarization frameworknamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is thefirst framework that unifies different video summarization tasks into one largelanguage model's (LLM) text decoder and achieves task-controllable videosummarization with temporal prompts and task instructions. Experiments showthat V2Xum-LLaMA outperforms strong baseline models on multiple videosummarization tasks. Furthermore, we propose an enhanced evaluation metric forV2V and V2VT summarization tasks.</description><author>Hang Hua, Yunlong Tang, Chenliang Xu, Jiebo Luo</author><pubDate>Tue, 20 Aug 2024 23:47:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12353v2</guid></item><item><title>Controllable Text Generation for Large Language Models: A Survey</title><link>http://arxiv.org/abs/2408.12599v1</link><description>In Natural Language Processing (NLP), Large Language Models (LLMs) havedemonstrated high text generation quality. However, in real-world applications,LLMs must meet increasingly complex requirements. Beyond avoiding misleading orinappropriate content, LLMs are also expected to cater to specific user needs,such as imitating particular writing styles or generating text with poeticrichness. These varied demands have driven the development of Controllable TextGeneration (CTG) techniques, which ensure that outputs adhere to predefinedcontrol conditions--such as safety, sentiment, thematic consistency, andlinguistic style--while maintaining high standards of helpfulness, fluency, anddiversity. This paper systematically reviews the latest advancements in CTG for LLMs,offering a comprehensive definition of its core concepts and clarifying therequirements for control conditions and text quality. We categorize CTG tasksinto two primary types: content control and attribute control. The key methodsare discussed, including model retraining, fine-tuning, reinforcement learning,prompt engineering, latent space manipulation, and decoding-time intervention.We analyze each method's characteristics, advantages, and limitations,providing nuanced insights for achieving generation control. Additionally, wereview CTG evaluation methods, summarize its applications across domains, andaddress key challenges in current research, including reduced fluency andpracticality. We also propose several appeals, such as placing greater emphasison real-world applications in future research. This paper aims to offervaluable guidance to researchers and developers in the field. Our referencelist and Chinese version are open-sourced athttps://github.com/IAAR-Shanghai/CTGSurvey.</description><author>Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao Song, Jiawei Yang, Simin Niu, Jie Hu, Dan Liu, Shunyu Yao, Feiyu Xiong, Zhiyu Li</author><pubDate>Thu, 22 Aug 2024 17:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12599v1</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v9</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Thu, 22 Aug 2024 10:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v9</guid></item><item><title>Preference-Guided Reflective Sampling for Aligning Language Models</title><link>http://arxiv.org/abs/2408.12163v1</link><description>Large language models (LLMs) are aligned with human preferences byreinforcement learning from human feedback (RLHF). Effective data sampling iscrucial for RLHF, as it determines the efficiency of model training, ensuringthat models learn from the informative samples. To achieve better datageneration, we propose a new sampling method called Preference-GuidedReflective Sampling (PRS). PRS frames the response generation as anoptimization process to the explicitly specified user preference described innatural language. It employs a tree-based generation framework to enable anefficient sampling process, which guides the direction of generation throughpreference and better explores the sampling space with adaptiveself-refinement. Notably, PRS can align LLMs to diverse preferences. We studypreference-controlled text generation for instruction following andkeyword-focused document summarization. Our findings indicate that PRS, acrossdifferent LLM policies, generates training data with much higher rewards thanstrong baselines. PRS also excels in post-RL training.</description><author>Hai Ye, Hwee Tou Ng</author><pubDate>Thu, 22 Aug 2024 07:18:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12163v1</guid></item><item><title>Unlocking Attributes' Contribution to Successful Camouflage: A Combined Textual and VisualAnalysis Strategy</title><link>http://arxiv.org/abs/2408.12086v1</link><description>In the domain of Camouflaged Object Segmentation (COS), despite continuousimprovements in segmentation performance, the underlying mechanisms ofeffective camouflage remain poorly understood, akin to a black box. To addressthis gap, we present the first comprehensive study to examine the impact ofcamouflage attributes on the effectiveness of camouflage patterns, offering aquantitative framework for the evaluation of camouflage designs. To supportthis analysis, we have compiled the first dataset comprising descriptions ofcamouflaged objects and their attribute contributions, termed COD-Text AndX-attributions (COD-TAX). Moreover, drawing inspiration from the hierarchicalprocess by which humans process information: from high-level textualdescriptions of overarching scenarios, through mid-level summaries of localareas, to low-level pixel data for detailed analysis. We have developed arobust framework that combines textual and visual information for the task ofCOS, named Attribution CUe Modeling with Eye-fixation Network (ACUMEN). ACUMENdemonstrates superior performance, outperforming nine leading methods acrossthree widely-used datasets. We conclude by highlighting key insights derivedfrom the attributes identified in our study. Code:https://github.com/lyu-yx/ACUMEN.</description><author>Hong Zhang, Yixuan Lyu, Qian Yu, Hanyang Liu, Huimin Ma, Ding Yuan, Yifan Yang</author><pubDate>Thu, 22 Aug 2024 02:51:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12086v1</guid></item><item><title>Tamil Language Computing: the Present and the Future</title><link>http://arxiv.org/abs/2407.08618v1</link><description>This paper delves into the text processing aspects of Language Computing,which enables computers to understand, interpret, and generate human language.Focusing on tasks such as speech recognition, machine translation, sentimentanalysis, text summarization, and language modelling, language computingintegrates disciplines including linguistics, computer science, and cognitivepsychology to create meaningful human-computer interactions. Recentadvancements in deep learning have made computers more accessible and capableof independent learning and adaptation. In examining the landscape of languagecomputing, the paper emphasises foundational work like encoding, where Tamiltransitioned from ASCII to Unicode, enhancing digital communication. Itdiscusses the development of computational resources, including raw data,dictionaries, glossaries, annotated data, and computational grammars, necessaryfor effective language processing. The challenges of linguistic annotation, thecreation of treebanks, and the training of large language models are alsocovered, emphasising the need for high-quality, annotated data and advancedlanguage models. The paper underscores the importance of building practicalapplications for languages like Tamil to address everyday communication needs,highlighting gaps in current technology. It calls for increased researchcollaboration, digitization of historical texts, and fostering digital usage toensure the comprehensive development of Tamil language processing, ultimatelyenhancing global communication and access to digital services.</description><author>Kengatharaiyer Sarveswaran</author><pubDate>Thu, 11 Jul 2024 15:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08618v1</guid></item><item><title>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</title><link>http://arxiv.org/abs/2311.09184v2</link><description>While large language models (LLMs) can already achieve strong performance onstandard generic summarization benchmarks, their performance on more complexsummarization task settings is less studied. Therefore, we benchmark LLMs oninstruction controllable text summarization, where the model input consists ofboth a source article and a natural language requirement for desired summarycharacteristics. To this end, we curate an evaluation-only dataset for thistask setting and conduct human evaluations of five LLM-based systems to assesstheir instruction-following capabilities in controllable summarization. We thenbenchmark LLM-based automatic evaluation for this task with 4 differentevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our studyreveals that instruction controllable text summarization remains a challengingtask for LLMs, since (1) all LLMs evaluated still make factual and other typesof errors in their summaries; (2) no LLM-based evaluation methods can achieve astrong alignment with human annotators when judging the quality of candidatesummaries; (3) different LLMs show large performance gaps in summary generationand evaluation capabilities. We make our collected benchmark InstruSum publiclyavailable to facilitate future research in this direction.</description><author>Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan</author><pubDate>Fri, 12 Jul 2024 17:35:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09184v2</guid></item><item><title>Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot Whole Slide Image Classification</title><link>http://arxiv.org/abs/2407.10814v1</link><description>Current multi-instance learning algorithms for pathology image analysis oftenrequire a substantial number of Whole Slide Images for effective training butexhibit suboptimal performance in scenarios with limited learning data. Inclinical settings, restricted access to pathology slides is inevitable due topatient privacy concerns and the prevalence of rare or emerging diseases. Theemergence of the Few-shot Weakly Supervised WSI Classification accommodates thesignificant challenge of the limited slide data and sparse slide-level labelsfor diagnosis. Prompt learning based on the pre-trained models (\eg, CLIP)appears to be a promising scheme for this setting; however, current research inthis area is limited, and existing algorithms often focus solely on patch-levelprompts or confine themselves to language prompts. This paper proposes amulti-instance prompt learning framework enhanced with pathology knowledge,\ie, integrating visual and textual prior knowledge into prompts at both patchand slide levels. The training process employs a combination of static andlearnable prompts, effectively guiding the activation of pre-trained models andfurther facilitating the diagnosis of key pathology patterns. LightweightMessenger (self-attention) and Summary (attention-pooling) layers areintroduced to model relationships between patches and slides within the samepatient data. Additionally, alignment-wise contrastive losses ensure thefeature-level alignment between visual and textual learnable prompts for bothpatches and slides. Our method demonstrates superior performance in threechallenging clinical tasks, significantly outperforming comparative few-shotmethods.</description><author>Linhao Qu, Dingkang Yang, Dan Huang, Qinhao Guo, Rongkui Luo, Shaoting Zhang, Xiaosong Wang</author><pubDate>Mon, 15 Jul 2024 15:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10814v1</guid></item></channel></rss>