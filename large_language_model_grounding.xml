<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model grounding</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 16 Jul 2024 13:00:20 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard</title><link>http://arxiv.org/abs/2407.07796v1</link><description>We introduce a novel and extensible benchmark for large language models(LLMs) through grid-based games such as Tic-Tac-Toe, Connect-Four, and Gomoku.The open-source game simulation code, available on GitHub, allows LLMs tocompete and generates detailed data files in JSON, CSV, TXT, and PNG formatsfor leaderboard rankings and further analysis. We present the results of gamesamong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet byAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo andGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions ofresults from other LLMs. In total, we simulated 2,310 matches (5 sessions foreach pair among 7 LLMs and a random player) across three types of games, usingthree distinct prompt types: list, illustration, and image. The resultsrevealed significant variations in LLM performance across different games andprompt types, with analysis covering win and disqualification rates, missedopportunity analysis, and invalid move analysis. The details of the leaderboardand result matrix data are available as open-access data on GitHub. This studyenhances our understanding of LLMs' capabilities in playing games they were notspecifically trained for, helping to assess their rule comprehension andstrategic thinking. On the path to Artificial General Intelligence (AGI), thisstudy lays the groundwork for future exploration into their utility in complexdecision-making scenarios, illuminating their strategic thinking abilities andoffering directions for further inquiry into the limits of LLMs withingame-based frameworks.</description><author>Oguzhan Topsakal, Colby Jacob Edell, Jackson Bailey Harper</author><pubDate>Wed, 10 Jul 2024 16:14:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07796v1</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v3</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Wed, 10 Jul 2024 15:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v3</guid></item><item><title>Training A Small Emotional Vision Language Model for Visual Art Comprehension</title><link>http://arxiv.org/abs/2403.11150v2</link><description>This paper develops small vision language models to understand visual art,which, given an art work, aims to identify its emotion category and explainthis prediction with natural language. While small models are computationallyefficient, their capacity is much limited compared with large models. To breakthis trade-off, this paper builds a small emotional vision language model(SEVLM) by emotion modeling and input-output feature alignment. On the onehand, based on valence-arousal-dominance (VAD) knowledge annotated bypsychology experts, we introduce and fuse emotional features derived throughVAD dictionary and a VAD head to align VAD vectors of predicted emotionexplanation and the ground truth. This allows the vision language model tobetter understand and generate emotional texts, compared with using traditionaltext embeddings alone. On the other hand, we design a contrastive head to pullclose embeddings of the image, its emotion class, and explanation, which alignsmodel outputs and inputs. On two public affective explanation datasets, we showthat the proposed techniques consistently improve the visual art understandingperformance of baseline SEVLMs. Importantly, the proposed model can be trainedand evaluated on a single RTX 2080 Ti while exhibiting very strong performance:it not only outperforms the state-of-the-art small models but is alsocompetitive compared with LLaVA 7B after fine-tuning and GPT4(V). The code isavailable at https://github.com/BetterZH/SEVLM-code.</description><author>Jing Zhang, Liang Zheng, Meng Wang, Dan Guo</author><pubDate>Wed, 10 Jul 2024 13:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11150v2</guid></item><item><title>Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard</title><link>http://arxiv.org/abs/2407.07796v2</link><description>We introduce a novel and extensible benchmark for large language models(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.The open-source game simulation code, available on GitHub, allows LLMs tocompete and generates detailed data files in JSON, CSV, TXT, and PNG formatsfor leaderboard rankings and further analysis. We present the results of gamesamong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet byAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo andGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions ofresults from other LLMs. In total, we simulated 2,310 matches (5 sessions foreach pair among 7 LLMs and a random player) across three types of games, usingthree distinct prompt types: list, illustration, and image. The resultsrevealed significant variations in LLM performance across different games andprompt types, with analysis covering win and disqualification rates, missedopportunity analysis, and invalid move analysis. The details of the leaderboardand result matrix data are available as open-access data on GitHub. This studyenhances our understanding of LLMs' capabilities in playing games they were notspecifically trained for, helping to assess their rule comprehension andstrategic thinking. On the path to Artificial General Intelligence (AGI), thisstudy lays the groundwork for future exploration into their utility in complexdecision-making scenarios, illuminating their strategic thinking abilities andoffering directions for further inquiry into the limits of LLMs withingame-based frameworks.</description><author>Oguzhan Topsakal, Colby Jacob Edell, Jackson Bailey Harper</author><pubDate>Thu, 11 Jul 2024 03:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07796v2</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v4</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Thu, 11 Jul 2024 14:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v4</guid></item><item><title>Beyond Aesthetics: Cultural Competence in Text-to-Image Models</title><link>http://arxiv.org/abs/2407.06863v2</link><description>Text-to-Image (T2I) models are being increasingly adopted in diverse globalcommunities where they create visual representations of their unique cultures.Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realismof generated images, overlooking the critical dimension of cultural competence.In this work, we introduce a framework to evaluate cultural competence of T2Imodels along two crucial dimensions: cultural awareness and cultural diversity,and present a scalable approach using a combination of structured knowledgebases and large language models to build a large dataset of cultural artifactsto enable this evaluation. In particular, we apply this approach to build CUBE(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark toevaluate cultural competence of T2I models. CUBE covers cultural artifactsassociated with 8 countries across different geo-cultural regions and along 3concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set ofhigh-quality prompts that enable the evaluation of cultural awareness, and 2)CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding toevaluate cultural diversity. We also introduce cultural diversity as a novelT2I evaluation component, leveraging quality-weighted Vendi score. Ourevaluations reveal significant gaps in the cultural awareness of existingmodels across countries and provide valuable insights into the culturaldiversity of T2I outputs for under-specified prompts. Our methodology isextendable to other cultural regions and concepts, and can facilitate thedevelopment of T2I models that better cater to the global population.</description><author>Nithish Kannen, Arif Ahmad, Marco Andreetto, Vinodkumar Prabhakaran, Utsav Prabhu, Adji Bousso Dieng, Pushpak Bhattacharyya, Shachi Dave</author><pubDate>Thu, 11 Jul 2024 17:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06863v2</guid></item><item><title>Robotic Control via Embodied Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2407.08693v1</link><description>A key limitation of learned robot control policies is their inability togeneralize outside their training data. Recent works on vision-language-actionmodels (VLAs) have shown that the use of large, internet pre-trainedvision-language models as the backbone of learned robot policies cansubstantially improve their robustness and generalization ability. Yet, one ofthe most exciting capabilities of large vision-language models in other domainsis their ability to reason iteratively through complex problems. Can that samecapability be brought into robotics to allow policies to improve performance byreasoning about a given task before acting? Naive use of "chain-of-thought"(CoT) style prompting is significantly less effective with standard VLAsbecause of the relatively simple training examples that are available to them.Additionally, purely semantic reasoning about sub-tasks, as is common inregular CoT, is insufficient for robot policies that need to ground theirreasoning in sensory observations and the robot state. To this end, weintroduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which wetrain VLAs to perform multiple steps of reasoning about plans, sub-tasks,motions, and visually grounded features like object bounding boxes and endeffector positions, before predicting the robot action. We design a scalablepipeline for generating synthetic training data for ECoT on large robotdatasets. We demonstrate, that ECoT increases the absolute success rate ofOpenVLA, the current strongest open-source VLA policy, by 28% acrosschallenging generalization tasks, without any additional robot training data.Additionally, ECoT makes it easier for humans to interpret a policy's failuresand correct its behavior using natural language.</description><author>Zawalski Michał, Chen William, Pertsch Karl, Mees Oier, Finn Chelsea, Levine Sergey</author><pubDate>Thu, 11 Jul 2024 17:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08693v1</guid></item><item><title>ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</title><link>http://arxiv.org/abs/2402.17766v3</link><description>This paper presents ShapeLLM, the first 3D Multimodal Large Language Model(LLM) designed for embodied interaction, exploring a universal 3D objectunderstanding with 3D point clouds and languages. ShapeLLM is built upon animproved 3D encoder by extending ReCon to ReCon++ that benefits from multi-viewimage distillation for enhanced geometry understanding. By utilizing ReCon++ asthe 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructedinstruction-following data and tested on our newly human-curated benchmark, 3DMM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3Dgeometry understanding and language-unified 3D interaction tasks, such asembodied visual grounding. Project page: https://qizekun.github.io/shapellm/</description><author>Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, Kaisheng Ma</author><pubDate>Fri, 12 Jul 2024 15:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17766v3</guid></item><item><title>Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors</title><link>http://arxiv.org/abs/2407.09136v1</link><description>Large language models (LLMs) present an opportunity to scale high-qualitypersonalized education to all. A promising approach towards this means is tobuild dialog tutoring models that scaffold students' problem-solving. However,even though existing LLMs perform well in solving reasoning questions, theystruggle to precisely detect student's errors and tailor their feedback tothese errors. Inspired by real-world teaching practice where teachers identifystudent errors and customize their response based on them, we focus onverifying student solutions and show how grounding to such verificationimproves the overall quality of tutor response generation. We collect a datasetof 1K stepwise math reasoning chains with the first error step annotated byteachers. We show empirically that finding the mistake in a student solution ischallenging for current models. We propose and evaluate several verifiers fordetecting these errors. Using both automatic and human evaluation we show thatthe student solution verifiers steer the generation model towards highlytargeted responses to student errors which are more often correct with lesshallucinations compared to existing baselines.</description><author>Nico Daheim, Jakub Macina, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan</author><pubDate>Fri, 12 Jul 2024 10:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09136v1</guid></item><item><title>Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</title><link>http://arxiv.org/abs/2403.14608v6</link><description>Large models represent a groundbreaking advancement in multiple applicationfields, enabling remarkable achievements across various tasks. However, theirunprecedented scale comes with significant computational costs. These models,often consisting of billions of parameters, require vast amounts ofcomputational resources for execution. Especially, the expansive scale andcomputational demands pose considerable challenges when customizing them forparticular downstream tasks, particularly over the hardware platformsconstrained by computational capabilities. Parameter Efficient Fine-Tuning(PEFT) provides a practical solution by efficiently adjusting the large modelsover the various downstream tasks. In particular, PEFT refers to the process ofadjusting the parameters of a pre-trained large models to adapt it to aspecific task or domain while minimizing the number of additional parametersintroduced or computational resources required. This approach is particularlyimportant when dealing with large-scale language models with high parametercounts, as fine-tuning these models from scratch can be computationallyexpensive and resource-intensive, posing considerable challenges in thesupporting system platform design. In this survey, we present comprehensivestudies of various PEFT algorithms, examining their performance andcomputational overhead. Moreover, we provide an overview of applicationsdeveloped using different PEFT algorithms and discuss common techniquesemployed to mitigate computation costs for PEFT. In addition to providing anextensive survey from an algorithmic standpoint, we also examine variousreal-world system designs to investigate the implementation costs associatedwith different PEFT approaches. This survey serves as an indispensable resourcefor researchers aiming to understand both the PEFT algorithm and its systemimplementation, offering detailed ......</description><author>Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang</author><pubDate>Fri, 12 Jul 2024 09:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14608v6</guid></item><item><title>Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations</title><link>http://arxiv.org/abs/2407.08983v1</link><description>Trustworthiness and interpretability are inextricably linked concepts forLLMs. The more interpretable an LLM is, the more trustworthy it becomes.However, current techniques for interpreting LLMs when applied to code-relatedtasks largely focus on accuracy measurements, measures of how models react tochange, or individual task performance instead of the fine-grained explanationsneeded at prediction time for greater interpretability, and hence trust. Toimprove upon this status quo, this paper introduces ASTrust, aninterpretability method for LLMs of code that generates explanations groundedin the relationship between model confidence and syntactic structures ofprogramming languages. ASTrust explains generated code in the context of syntaxcategories based on Abstract Syntax Trees and aids practitioners inunderstanding model predictions at both local (individual code snippets) andglobal (larger datasets of code) levels. By distributing and assigning modelconfidence scores to well-known syntactic structures that exist within ASTs,our approach moves beyond prior techniques that perform token-level confidencemapping by offering a view of model confidence that directly aligns withprogramming language concepts with which developers are familiar. To putASTrust into practice, we developed an automated visualization that illustratesthe aggregated model confidence scores superimposed on sequence, heat-map, andgraph-based visuals of syntactic structures from ASTs. We examine both thepractical benefit that ASTrust can provide through a data science study on 12popular LLMs on a curated set of GitHub repos and the usefulness of ASTrustthrough a human study.</description><author>David N. Palacio, Daniel Rodriguez-Cardenas, Alejandro Velasco, Dipin Khati, Kevin Moran, Denys Poshyvanyk</author><pubDate>Fri, 12 Jul 2024 04:38:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08983v1</guid></item><item><title>Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing</title><link>http://arxiv.org/abs/2406.14230v2</link><description>Warning: this paper contains model outputs exhibiting unethical information.Large Language Models (LLMs) have achieved significant breakthroughs, but theirgenerated unethical content poses potential risks. Measuring value alignment ofLLMs becomes crucial for their regulation and responsible deployment. Numerousdatasets have been constructed to assess social bias, toxicity, and ethics inLLMs, but they suffer from evaluation chronoeffect, that is, as models rapidlyevolve, existing data becomes leaked or undemanding, overestimatingever-developing LLMs. To tackle this problem, we propose GETA, a novelgenerative evolving testing approach that dynamically probes the underlyingmoral baselines of LLMs. Distinct from previous adaptive testing methods thatrely on static datasets with limited difficulty, GETA incorporates aniteratively-updated item generator which infers each LLM's moral boundaries andgenerates difficulty-tailored testing items, accurately reflecting the truealignment extent. This process theoretically learns a joint distribution ofitem and model response, with item difficulty and value conformity as latentvariables, where the generator co-evolves with the LLM, addressingchronoeffect. We evaluate various popular LLMs with diverse capabilities anddemonstrate that GETA can create difficulty-matching testing items and moreaccurately assess LLMs' values, better consistent with their performance onunseen OOD and i.i.d. items, laying the groundwork for future evaluationparadigms.</description><author>Han Jiang, Xiaoyuan Yi, Zhihua Wei, Shu Wang, Xing Xie</author><pubDate>Fri, 12 Jul 2024 03:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14230v2</guid></item><item><title>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation</title><link>http://arxiv.org/abs/2407.08940v1</link><description>The rapid growth of biomedical knowledge has outpaced our ability toefficiently extract insights and generate novel hypotheses. Large languagemodels (LLMs) have emerged as a promising tool to revolutionize knowledgeinteraction and potentially accelerate biomedical discovery. In this paper, wepresent a comprehensive evaluation of LLMs as biomedical hypothesis generators.We construct a dataset of background-hypothesis pairs from biomedicalliterature, carefully partitioned into training, seen, and unseen test setsbased on publication date to mitigate data contamination. Using this dataset,we assess the hypothesis generation capabilities of top-tier instructed modelsin zero-shot, few-shot, and fine-tuning settings. To enhance the exploration ofuncertainty, a crucial aspect of scientific discovery, we incorporate tool useand multi-agent interactions in our evaluation framework. Furthermore, wepropose four novel metrics grounded in extensive literature review to evaluatethe quality of generated hypotheses, considering both LLM-based and humanassessments. Our experiments yield two key findings: 1) LLMs can generate noveland validated hypotheses, even when tested on literature unseen duringtraining, and 2) Increasing uncertainty through multi-agent interactions andtool use can facilitate diverse candidate generation and improve zero-shothypothesis generation performance. However, we also observe that theintegration of additional knowledge through few-shot learning and tool use maynot always lead to performance gains, highlighting the need for carefulconsideration of the type and scope of external knowledge incorporated. Thesefindings underscore the potential of LLMs as powerful aids in biomedicalhypothesis generation and provide valuable insights to guide further researchin this area.</description><author>Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Hu Jinfang, Bowen Zhou</author><pubDate>Fri, 12 Jul 2024 02:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08940v1</guid></item><item><title>CXR-Agent: Vision-language models for chest X-ray interpretation with uncertainty aware radiology reporting</title><link>http://arxiv.org/abs/2407.08811v1</link><description>Recently large vision-language models have shown potential when interpretingcomplex images and generating natural language descriptions using advancedreasoning. Medicine's inherently multimodal nature incorporating scans andtext-based medical histories to write reports makes it conducive to benefitfrom these leaps in AI capabilities. We evaluate the publicly available, stateof the art, foundational vision-language models for chest X-ray interpretationacross several datasets and benchmarks. We use linear probes to evaluate theperformance of various components including CheXagent's vision transformer andQ-former, which outperform the industry-standard Torch X-ray Vision modelsacross many different datasets showing robust generalisation capabilities.Importantly, we find that vision-language models often hallucinate withconfident language, which slows down clinical interpretation. Based on thesefindings, we develop an agent-based vision-language approach for reportgeneration using CheXagent's linear probes and BioViL-T's phrase groundingtools to generate uncertainty-aware radiology reports with pathologieslocalised and described based on their likelihood. We thoroughly evaluate ourvision-language agents using NLP metrics, chest X-ray benchmarks and clinicalevaluations by developing an evaluation platform to perform a user study withrespiratory specialists. Our results show considerable improvements inaccuracy, interpretability and safety of the AI-generated reports. We stressthe importance of analysing results for normal and abnormal scans separately.Finally, we emphasise the need for larger paired (scan and report) datasetsalongside data augmentation to tackle overfitting seen in these largevision-language models.</description><author>Naman Sharma</author><pubDate>Thu, 11 Jul 2024 18:39:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08811v1</guid></item><item><title>SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding</title><link>http://arxiv.org/abs/2407.05118v2</link><description>Temporal grounding, also known as video moment retrieval, aims at locatingvideo segments corresponding to a given query sentence. The compositionalnature of natural language enables the localization beyond predefined events,posing a certain challenge to the compositional generalizability of existingmethods. Recent studies establish the correspondence between videos and queriesthrough a decompose-reconstruct manner to achieve compositional generalization.However, they only consider dominant primitives and build negative queriesthrough random sampling and recombination, resulting in semanticallyimplausible negatives that hinder the models from learning rationalcompositions. In addition, recent DETR-based methods still underperform incompositional temporal grounding, showing irrational saliency responses whengiven negative queries that have subtle differences from positive queries. Toaddress these limitations, we first propose a large language model-drivenmethod for negative query construction, utilizing GPT-3.5-Turbo to generatesemantically plausible hard negative queries. Subsequently, we introduce acoarse-to-fine saliency ranking strategy, which encourages the model to learnthe multi-granularity semantic relationships between videos and hierarchicalnegative queries to boost compositional generalization. Extensive experimentson two challenging benchmarks validate the effectiveness and generalizabilityof our proposed method. Our code is available athttps://github.com/zxccade/SHINE.</description><author>Zixu Cheng, Yujiang Pu, Shaogang Gong, Parisa Kordjamshidi, Yu Kong</author><pubDate>Mon, 15 Jul 2024 16:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05118v2</guid></item><item><title>Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models</title><link>http://arxiv.org/abs/2407.10873v1</link><description>Automated heuristic design (AHD) has gained considerable attention for itspotential to automate the development of effective heuristics. The recentadvent of large language models (LLMs) has paved a new avenue for AHD, withinitial efforts focusing on framing AHD as an evolutionary program search (EPS)problem. However, inconsistent benchmark settings, inadequate baselines, and alack of detailed component analysis have left the necessity of integrating LLMswith search strategies and the true progress achieved by existing LLM-based EPSmethods to be inadequately justified. This work seeks to fulfill these researchqueries by conducting a large-scale benchmark comprising four LLM-based EPSmethods and four AHD problems across nine LLMs and five independent runs. Ourextensive experiments yield meaningful insights, providing empirical groundingfor the importance of evolutionary search in LLM-based AHD approaches, whilealso contributing to the advancement of future EPS algorithmic development. Tofoster accessibility and reproducibility, we have fully open-sourced ourbenchmark and corresponding results.</description><author>Rui Zhang, Fei Liu, Xi Lin, Zhenkun Wang, Zhichao Lu, Qingfu Zhang</author><pubDate>Mon, 15 Jul 2024 16:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10873v1</guid></item><item><title>3D Feature Distillation with Object-Centric Priors</title><link>http://arxiv.org/abs/2406.18742v3</link><description>Grounding natural language to the physical world is a ubiquitous topic with awide range of applications in computer vision and robotics. Recently, 2Dvision-language models such as CLIP have been widely popularized, due to theirimpressive capabilities for open-vocabulary grounding in 2D images. Recentworks aim to elevate 2D CLIP features to 3D via feature distillation, buteither learn neural fields that are scene-specific and hence lackgeneralization, or focus on indoor room scan data that require access tomultiple camera views, which is not practical in robot manipulation scenarios.Additionally, related methods typically fuse features at pixel-level and assumethat all camera views are equally informative. In this work, we show that thisapproach leads to sub-optimal 3D features, both in terms of grounding accuracy,as well as segmentation crispness. To alleviate this, we propose a multi-viewfeature fusion strategy that employs object-centric priors to eliminateuninformative views based on semantic information, and fuse features atobject-level via instance segmentation masks. To distill our object-centric 3Dfeatures, we generate a large-scale synthetic multi-view dataset of clutteredtabletop scenes, spawning 15k scenes from over 3300 unique object instances,which we make publicly available. We show that our method reconstructs 3D CLIPfeatures with improved grounding capacity and spatial consistency, while doingso from single-view RGB-D, thus departing from the assumption of multiplecamera views at test time. Finally, we show that our approach can generalize tonovel tabletop domains and be re-purposed for 3D instance segmentation withoutfine-tuning, and demonstrate its utility for language-guided robotic graspingin clutter</description><author>Georgios Tziafas, Yucheng Xu, Zhibin Li, Hamidreza Kasaei</author><pubDate>Mon, 15 Jul 2024 14:23:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18742v3</guid></item><item><title>Towards Open-World Grasping with Large Vision-Language Models</title><link>http://arxiv.org/abs/2406.18722v3</link><description>The ability to grasp objects in-the-wild from open-ended languageinstructions constitutes a fundamental challenge in robotics. An open-worldgrasping system should be able to combine high-level contextual with low-levelphysical-geometric reasoning in order to be applicable in arbitrary scenarios.Recent works exploit the web-scale knowledge inherent in large language models(LLMs) to plan and reason in robotic context, but rely on external vision andaction models to ground such knowledge into the environment and parameterizeactuation. This setup suffers from two major bottlenecks: a) the LLM'sreasoning capacity is constrained by the quality of visual grounding, and b)LLMs do not contain low-level spatial understanding of the world, which isessential for grasping in contact-rich scenarios. In this work we demonstratethat modern vision-language models (VLMs) are capable of tackling suchlimitations, as they are implicitly grounded and can jointly reason aboutsemantics and geometry. We propose OWG, an open-world grasping pipeline thatcombines VLMs with segmentation and grasp synthesis models to unlock groundedworld understanding in three stages: open-ended referring segmentation,grounded grasp planning and grasp ranking via contact reasoning, all of whichcan be applied zero-shot via suitable visual prompting mechanisms. We conductextensive evaluation in cluttered indoor scene datasets to showcase OWG'srobustness in grounding from open-ended language, as well as open-world roboticgrasping experiments in both simulation and hardware that demonstrate superiorperformance compared to previous supervised and zero-shot LLM-based methods.</description><author>Georgios Tziafas, Hamidreza Kasaei</author><pubDate>Mon, 15 Jul 2024 14:21:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18722v3</guid></item><item><title>Large Language Models and Games: A Survey and Roadmap</title><link>http://arxiv.org/abs/2402.18659v2</link><description>Recent years have seen an explosive increase in research on large languagemodels (LLMs), and accompanying public engagement on the topic. While startingas a niche area within natural language processing, LLMs have shown remarkablepotential across a broad range of applications and domains, including games.This paper surveys the current state of the art across the various applicationsof LLMs in and for games, and identifies the different roles LLMs can takewithin a game. Importantly, we discuss underexplored areas and promisingdirections for future uses of LLMs in games and we reconcile the potential andlimitations of LLMs within the games domain. As the first comprehensive surveyand roadmap at the intersection of LLMs and games, we are hopeful that thispaper will serve as the basis for groundbreaking research and innovation inthis exciting new field.</description><author>Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis</author><pubDate>Mon, 15 Jul 2024 13:10:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18659v2</guid></item><item><title>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation</title><link>http://arxiv.org/abs/2407.08940v2</link><description>The rapid growth of biomedical knowledge has outpaced our ability toefficiently extract insights and generate novel hypotheses. Large languagemodels (LLMs) have emerged as a promising tool to revolutionize knowledgeinteraction and potentially accelerate biomedical discovery. In this paper, wepresent a comprehensive evaluation of LLMs as biomedical hypothesis generators.We construct a dataset of background-hypothesis pairs from biomedicalliterature, carefully partitioned into training, seen, and unseen test setsbased on publication date to mitigate data contamination. Using this dataset,we assess the hypothesis generation capabilities of top-tier instructed modelsin zero-shot, few-shot, and fine-tuning settings. To enhance the exploration ofuncertainty, a crucial aspect of scientific discovery, we incorporate tool useand multi-agent interactions in our evaluation framework. Furthermore, wepropose four novel metrics grounded in extensive literature review to evaluatethe quality of generated hypotheses, considering both LLM-based and humanassessments. Our experiments yield two key findings: 1) LLMs can generate noveland validated hypotheses, even when tested on literature unseen duringtraining, and 2) Increasing uncertainty through multi-agent interactions andtool use can facilitate diverse candidate generation and improve zero-shothypothesis generation performance. However, we also observe that theintegration of additional knowledge through few-shot learning and tool use maynot always lead to performance gains, highlighting the need for carefulconsideration of the type and scope of external knowledge incorporated. Thesefindings underscore the potential of LLMs as powerful aids in biomedicalhypothesis generation and provide valuable insights to guide further researchin this area.</description><author>Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Hu Jinfang, Bowen Zhou</author><pubDate>Mon, 15 Jul 2024 06:27:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08940v2</guid></item><item><title>By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting</title><link>http://arxiv.org/abs/2407.10385v1</link><description>Large language models (LLMs) have demonstrated exceptional abilities acrossvarious domains. However, utilizing LLMs for ubiquitous sensing applicationsremains challenging as existing text-prompt methods show significantperformance degradation when handling long sensor data sequences. We propose avisual prompting approach for sensor data using multimodal LLMs (MLLMs). Wedesign a visual prompt that directs MLLMs to utilize visualized sensor dataalongside the target sensory task descriptions. Additionally, we introduce avisualization generator that automates the creation of optimal visualizationstailored to a given sensory task, eliminating the need for prior task-specificknowledge. We evaluated our approach on nine sensory tasks involving foursensing modalities, achieving an average of 10% higher accuracy than text-basedprompts and reducing token costs by 15.8x. Our findings highlight theeffectiveness and cost-efficiency of visual prompts with MLLMs for varioussensory tasks.</description><author>Hyungjun Yoon, Biniyam Aschalew Tolera, Taesik Gong, Kimin Lee, Sung-Ju Lee</author><pubDate>Mon, 15 Jul 2024 01:33:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10385v1</guid></item></channel></rss>