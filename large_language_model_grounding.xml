<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model grounding</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 23 Apr 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>GROUNDHOG: Grounding Large Language Models to Holistic Segmentation</title><link>http://arxiv.org/abs/2402.16846v2</link><description>Most multimodal large language models (MLLMs) learn language-to-objectgrounding through causal language modeling where grounded objects are capturedby bounding boxes as sequences of location tokens. This paradigm lackspixel-level representations that are important for fine-grained visualunderstanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLMdeveloped by grounding Large Language Models to holistic segmentation.GROUNDHOG incorporates a masked feature extractor and converts extractedfeatures into visual entity tokens for the MLLM backbone, which then connectsgroundable phrases to unified grounding masks by retrieving and merging theentity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visualinstruction tuning dataset with Multi-Modal Multi-Grained Grounding, byharvesting a collection of segmentation-grounded datasets with richannotations. Our experimental results show that GROUNDHOG achieves superiorperformance on various language grounding tasks without task-specificfine-tuning, and significantly reduces object hallucination. GROUNDHOG alsodemonstrates better grounding towards complex forms of visual input andprovides easy-to-understand diagnosis in failure cases.</description><author>Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, Joyce Chai</author><pubDate>Tue, 16 Apr 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16846v2</guid></item><item><title>SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments</title><link>http://arxiv.org/abs/2309.04077v4</link><description>Semantic reasoning and dynamic planning capabilities are crucial for anautonomous agent to perform complex navigation tasks in unknown environments.It requires a large amount of common-sense knowledge, that humans possess, tosucceed in these tasks. We present SayNav, a new approach that leverages humanknowledge from Large Language Models (LLMs) for efficient generalization tocomplex navigation tasks in unknown large-scale environments. SayNav uses anovel grounding mechanism, that incrementally builds a 3D scene graph of theexplored environment as inputs to LLMs, for generating feasible andcontextually appropriate high-level plans for navigation. The LLM-generatedplan is then executed by a pre-trained low-level planner, that treats eachplanned step as a short-distance point-goal navigation sub-task. SayNavdynamically generates step-by-step instructions during navigation andcontinuously refines future steps based on newly perceived information. Weevaluate SayNav on multi-object navigation (MultiON) task, that requires theagent to utilize a massive amount of human knowledge to efficiently searchmultiple different objects in an unknown environment. We also introduce abenchmark dataset for MultiON task employing ProcTHOR framework that provideslarge photo-realistic indoor environments with variety of objects. SayNavachieves state-of-the-art results and even outperforms an oracle based baselinewith strong ground-truth assumptions by more than 8% in terms of success rate,highlighting its ability to generate dynamic plans for successfully locatingobjects in large-scale new environments. The code, benchmark dataset anddemonstration videos are accessible athttps://www.sri.com/ics/computer-vision/saynav.</description><author>Abhinav Rajvanshi, Karan Sikka, Xiao Lin, Bhoram Lee, Han-Pang Chiu, Alvaro Velasquez</author><pubDate>Wed, 03 Apr 2024 21:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04077v4</guid></item><item><title>Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources</title><link>http://arxiv.org/abs/2305.13269v4</link><description>We present chain-of-knowledge (CoK), a novel framework that augments largelanguage models (LLMs) by dynamically incorporating grounding information fromheterogeneous sources. It results in more factual rationales and reducedhallucination in generation. Specifically, CoK consists of three stages:reasoning preparation, dynamic knowledge adapting, and answer consolidation.Given a knowledge-intensive question, CoK first prepares several preliminaryrationales and answers while identifying the relevant knowledge domains. Ifthere is no majority consensus among the answers from samples, CoK corrects therationales step by step by adapting knowledge from the identified domains.These corrected rationales can plausibly serve as a better foundation for thefinal answer consolidation. Unlike prior studies that primarily useunstructured data, CoK also leverages structured knowledge sources such asWikidata and tables that provide more reliable factual information. To accessboth unstructured and structured knowledge sources in the dynamic knowledgeadapting stage, we propose an adaptive query generator that allows thegeneration of queries for various types of query languages, including SPARQL,SQL, and natural sentences. Moreover, to minimize error propagation betweenrationales, CoK corrects the rationales progressively using preceding correctedrationales to generate and correct subsequent rationales. Extensive experimentsshow that CoK consistently improves the performance of LLMs onknowledge-intensive tasks across different domains.</description><author>Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, Lidong Bing</author><pubDate>Wed, 21 Feb 2024 07:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13269v4</guid></item><item><title>Ranking Large Language Models without Ground Truth</title><link>http://arxiv.org/abs/2402.14860v2</link><description>Evaluation and ranking of large language models (LLMs) has become animportant problem with the proliferation of these models and their impact.Evaluation methods either require human responses which are expensive toacquire or use pairs of LLMs to evaluate each other which can be unreliable. Inthis paper, we provide a novel perspective where, given a dataset of prompts(viz. questions, instructions, etc.) and a set of LLMs, we rank them withoutaccess to any ground truth or reference responses. Inspired by real life whereboth an expert and a knowledgeable person can identify a novice our main ideais to consider triplets of models, where each one of them evaluates the othertwo, correctly identifying the worst model in the triplet with highprobability. We also analyze our idea and provide sufficient conditions for itto succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.In experiments on different generative tasks (summarization, multiple-choice,and dialog), our methods reliably recover close to true rankings withoutreference data. This points to a viable low-resource mechanism for practicaluse.</description><author>Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan Ramamurthy</author><pubDate>Wed, 06 Mar 2024 20:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14860v2</guid></item><item><title>Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models</title><link>http://arxiv.org/abs/2404.13013v1</link><description>We introduce Groma, a Multimodal Large Language Model (MLLM) with groundedand fine-grained visual perception ability. Beyond holistic imageunderstanding, Groma is adept at region-level tasks such as region captioningand visual grounding. Such capabilities are built upon a localized visualtokenization mechanism, where an image input is decomposed into regions ofinterest and subsequently encoded into region tokens. By integrating regiontokens into user instructions and model responses, we seamlessly enable Gromato understand user-specified region inputs and ground its textual output toimages. Besides, to enhance the grounded chat ability of Groma, we curate avisually grounded instruction dataset by leveraging the powerful GPT-4V andvisual prompting techniques. Compared with MLLMs that rely on the languagemodel or external module for localization, Groma consistently demonstratessuperior performances in standard referring and grounding benchmarks,highlighting the advantages of embedding localization into image tokenization.Project page: https://groma-mllm.github.io/.</description><author>Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, Xiaojuan Qi</author><pubDate>Fri, 19 Apr 2024 18:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13013v1</guid></item><item><title>A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia</title><link>http://arxiv.org/abs/2312.02073v2</link><description>Large language models (LLMs) have an impressive ability to draw on novelinformation supplied in their context. Yet the mechanisms underlying thiscontextual grounding remain unknown, especially in situations where contextualinformation contradicts factual knowledge stored in the parameters, which LLMsalso excel at recalling. Favoring the contextual information is critical forretrieval-augmented generation methods, which enrich the context withup-to-date information, hoping that grounding can rectify outdated or noisystored knowledge. We present a novel method to study grounding abilities usingFakepedia, a dataset of counterfactual texts constructed to clash with amodel's internal parametric knowledge. We benchmark various LLMs with Fakepediaand then we conduct a causal mediation analysis, based on our Masked GroupedCausal Tracing (MGCT), on LLM components when answering Fakepedia queries.Within this analysis, we identify distinct computational patterns betweengrounded and ungrounded responses. We finally demonstrate that distinguishinggrounded from ungrounded responses is achievable through computational analysisalone. Our results, together with existing findings about factual recallmechanisms, provide a coherent narrative of how grounding and factual recallmechanisms interact within LLMs.</description><author>Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre Kıcıman, Hamid Palangi, Barun Patra, Robert West</author><pubDate>Tue, 20 Feb 2024 17:27:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02073v2</guid></item><item><title>"Understanding AI": Semantic Grounding in Large Language Models</title><link>http://arxiv.org/abs/2402.10992v1</link><description>Do LLMs understand the meaning of the texts they generate? Do they possess asemantic grounding? And how could we understand whether and what theyunderstand? I start the paper with the observation that we have recentlywitnessed a generative turn in AI, since generative models, including LLMs, arekey for self-supervised learning. To assess the question of semantic grounding,I distinguish and discuss five methodological ways. The most promising way isto apply core assumptions of theories of meaning in philosophy of mind andlanguage to LLMs. Grounding proves to be a gradual affair with athree-dimensional distinction between functional, social and causal grounding.LLMs show basic evidence in all three dimensions. A strong argument is thatLLMs develop world models. Hence, LLMs are neither stochastic parrots norsemantic zombies, but already understand the language they generate, at leastin an elementary sense.</description><author>Holger Lyre</author><pubDate>Fri, 16 Feb 2024 14:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10992v1</guid></item><item><title>Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models</title><link>http://arxiv.org/abs/2404.07973v1</link><description>While Ferret seamlessly integrates regional understanding into the LargeLanguage Model (LLM) to facilitate its referring and grounding capability, itposes certain limitations: constrained by the pre-trained fixed visual encoderand failed to perform well on broader tasks. In this work, we unveil Ferret-v2,a significant upgrade to Ferret, with three key designs. (1) Any resolutiongrounding and referring: A flexible approach that effortlessly handles higherimage resolution, improving the model's ability to process and understandimages in greater detail. (2) Multi-granularity visual encoding: By integratingthe additional DINOv2 encoder, the model learns better and diverse underlyingcontexts for global and fine-grained visual information. (3) A three-stagetraining paradigm: Besides image-caption alignment, an additional stage isproposed for high-resolution dense alignment before the final instructiontuning. Experiments show that Ferret-v2 provides substantial improvements overFerret and other state-of-the-art methods, thanks to its high-resolutionscaling and fine-grained visual processing.</description><author>Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, Yinfei Yang</author><pubDate>Thu, 11 Apr 2024 18:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07973v1</guid></item><item><title>Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?</title><link>http://arxiv.org/abs/2404.06510v1</link><description>Enhancing semantic grounding abilities in Vision-Language Models (VLMs) ofteninvolves collecting domain-specific training data, refining the networkarchitectures, or modifying the training recipes. In this work, we venture intoan orthogonal direction and explore whether VLMs can improve their semanticgrounding by "receiving" feedback, without requiring in-domain data,fine-tuning, or modifications to the network architectures. We systematicallyanalyze this hypothesis using a feedback mechanism composed of a binary signal.We find that if prompted appropriately, VLMs can utilize feedback both in asingle step and iteratively, showcasing the potential of feedback as analternative technique to improve grounding in internet-scale VLMs. Furthermore,VLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, wefind that this issue can be mitigated via a binary verification mechanism.Finally, we explore the potential and limitations of amalgamating thesefindings and applying them iteratively to automatically enhance VLMs' groundingperformance, showing grounding accuracy consistently improves using automatedfeedback across all models in all settings investigated. Overall, our iterativeframework improves semantic grounding in VLMs by more than 15 accuracy pointsunder noise-free feedback and up to 5 accuracy points under a simple automatedbinary verification mechanism. The project website is hosted athttps://andrewliao11.github.io/vlms_feedback</description><author>Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna</author><pubDate>Tue, 09 Apr 2024 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06510v1</guid></item><item><title>EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model</title><link>http://arxiv.org/abs/2312.02483v2</link><description>Early weakly supervised video grounding (WSVG) methods often struggle withincomplete boundary detection due to the absence of temporal boundaryannotations. To bridge the gap between video-level and boundary-levelannotation, explicit-supervision methods, i.e., generating pseudo-temporalboundaries for training, have achieved great success. However, dataaugmentations in these methods might disrupt critical temporal information,yielding poor pseudo boundaries. In this paper, we propose a new perspectivethat maintains the integrity of the original temporal content while introducingmore valuable information for expanding the incomplete boundaries. To this end,we propose EtC (Expand then Clarify), first use the additional information toexpand the initial incomplete pseudo boundaries, and subsequently refine theseexpanded ones to achieve precise boundaries. Motivated by video continuity,i.e., visual similarity across adjacent frames, we use powerful multimodallarge language models (MLLMs) to annotate each frame within initial pseudoboundaries, yielding more comprehensive descriptions for expanded boundaries.To further clarify the noise of expanded boundaries, we combine mutual learningwith a tailored proposal-level contrastive objective to use a learnableapproach to harmonize a balance between incomplete yet clean (initial) andcomprehensive yet noisy (expanded) boundaries for more precise ones.Experiments demonstrate the superiority of our method on two challenging WSVGdatasets.</description><author>Guozhang Li, Xinpeng Ding, De Cheng, Jie Li, Nannan Wang, Xinbo Gao</author><pubDate>Wed, 06 Mar 2024 08:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02483v2</guid></item><item><title>VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models</title><link>http://arxiv.org/abs/2402.18374v1</link><description>Recent approaches in domain-specific named entity recognition (NER), such asbiomedical NER, have shown remarkable advances. However, they still lack offaithfulness, producing erroneous predictions. We assume that knowledge ofentities can be useful in verifying the correctness of the predictions. Despitethe usefulness of knowledge, resolving such errors with knowledge isnontrivial, since the knowledge itself does not directly indicate theground-truth label. To this end, we propose VerifiNER, a post-hoc verificationframework that identifies errors from existing NER methods using knowledge andrevises them into more faithful predictions. Our framework leverages thereasoning abilities of large language models to adequately ground on knowledgeand the contextual information in the verification process. We validateeffectiveness of VerifiNER through extensive experiments on biomedicaldatasets. The results suggest that VerifiNER can successfully verify errorsfrom existing models as a model-agnostic approach. Further analyses onout-of-domain and low-resource settings show the usefulness of VerifiNER onreal-world applications.</description><author>Seoyeon Kim, Kwangwook Seo, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee</author><pubDate>Wed, 28 Feb 2024 14:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18374v1</guid></item><item><title>From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility</title><link>http://arxiv.org/abs/2402.16142v1</link><description>This groundbreaking study explores the expanse of Large Language Models(LLMs), such as Generative Pre-Trained Transformer (GPT) and BidirectionalEncoder Representations from Transformers (BERT) across varied domains rangingfrom technology, finance, healthcare to education. Despite their establishedprowess in Natural Language Processing (NLP), these LLMs have not beensystematically examined for their impact on domains such as fitness, andholistic well-being, urban planning, climate modelling as well as disastermanagement. This review paper, in addition to furnishing a comprehensiveanalysis of the vast expanse and extent of LLMs' utility in diverse domains,recognizes the research gaps and realms where the potential of LLMs is yet tobe harnessed. This study uncovers innovative ways in which LLMs can leave amark in the fields like fitness and wellbeing, urban planning, climatemodelling and disaster response which could inspire future researches andapplications in the said avenues.</description><author>Pravneet Kaur, Gautam Siddharth Kashyap, Ankit Kumar, Md Tabrez Nafis, Sandeep Kumar, Vikrant Shokeen</author><pubDate>Sun, 25 Feb 2024 16:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16142v1</guid></item><item><title>NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2312.11361v2</link><description>Retrieval-augmented generation (RAG) grounds large language model (LLM)output by leveraging external knowledge sources to reduce factualhallucinations. However, prior works lack a comprehensive evaluation ofdifferent language families, making it challenging to evaluate LLM robustnessagainst errors in external retrieved knowledge. To overcome this, we establishNoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across18 typologically diverse languages. NoMIRACL includes both a non-relevant and arelevant subset. Queries in the non-relevant subset contain passages judged asnon-relevant, whereas queries in the relevant subset include at least a singlejudged relevant passage. We measure LLM robustness using two metrics: (i)hallucination rate, measuring model tendency to hallucinate an answer, when theanswer is not present in passages in the non-relevant subset, and (ii) errorrate, measuring model inaccuracy to recognize relevant passages in the relevantsubset. In our work, we measure robustness for a wide variety ofmultilingual-focused LLMs and observe that most of the models struggle tobalance the two capacities. Models such as LLAMA-2, Orca-2, and FLAN-T5 observemore than an 88% hallucination rate on the non-relevant subset, whereas,Mistral overall hallucinates less, but can achieve up to a 74.9% error rate onthe relevant subset. Overall, GPT-4 is observed to provide the best tradeoff onboth subsets, highlighting future work necessary to improve LLM robustness.</description><author>Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, Jimmy Lin</author><pubDate>Mon, 04 Mar 2024 16:32:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11361v2</guid></item><item><title>RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models</title><link>http://arxiv.org/abs/2310.10221v2</link><description>Robotic vision applications often necessitate a wide range of visualperception tasks, such as object detection, segmentation, and identification.While there have been substantial advances in these individual tasks,integrating specialized models into a unified vision pipeline presentssignificant engineering challenges and costs. Recently, Multimodal LargeLanguage Models (MLLMs) have emerged as novel backbones for various downstreamtasks. We argue that leveraging the pre-training capabilities of MLLMs enablesthe creation of a simplified framework, thus mitigating the need fortask-specific encoders. Specifically, the large-scale pretrained knowledge inMLLMs allows for easier fine-tuning to downstream robotic vision tasks andyields superior performance. We introduce the RoboLLM framework, equipped witha BEiT-3 backbone, to address all visual perception tasks in the ARMBenchchallenge-a large-scale robotic manipulation dataset about real-world warehousescenarios. RoboLLM not only outperforms existing baselines but alsosubstantially reduces the engineering burden associated with model selectionand tuning. The source code is publicly available athttps://github.com/longkukuhi/armbench.</description><author>Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon Camarasa</author><pubDate>Fri, 23 Feb 2024 15:18:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10221v2</guid></item><item><title>When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment</title><link>http://arxiv.org/abs/2401.07764v2</link><description>AI agents based on multimodal large language models (LLMs) are expected torevolutionize human-computer interaction and offer more personalized assistantservices across various domains like healthcare, education, manufacturing, andentertainment. Deploying LLM agents in 6G networks enables users to accesspreviously expensive AI assistant services via mobile devices democratically,thereby reducing interaction latency and better preserving user privacy.Nevertheless, the limited capacity of mobile devices constrains theeffectiveness of deploying and executing local LLMs, which necessitatesoffloading complex tasks to global LLMs running on edge servers duringlong-horizon interactions. In this article, we propose a split learning systemfor LLM agents in 6G networks leveraging the collaboration between mobiledevices and edge servers, where multiple LLMs with different roles aredistributed across mobile devices and edge servers to perform user-agentinteractive tasks collaboratively. In the proposed system, LLM agents are splitinto perception, grounding, and alignment modules, facilitating inter-modulecommunications to meet extended user requirements on 6G network functions,including integrated sensing and communication, digital twins, andtask-oriented communications. Furthermore, we introduce a novel model cachingalgorithm for LLMs within the proposed system to improve model utilization incontext, thus reducing network costs of the collaborative mobile and edge LLMagents.</description><author>Minrui Xu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Dong In Kim, Khaled B. Letaief</author><pubDate>Fri, 16 Feb 2024 19:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07764v2</guid></item><item><title>GroundingGPT:Language Enhanced Multi-modal Grounding Model</title><link>http://arxiv.org/abs/2401.06071v5</link><description>Multi-modal large language models have demonstrated impressive performanceacross various tasks in different modalities. However, existing multi-modalmodels primarily emphasize capturing global information within each modalitywhile neglecting the importance of perceiving local information acrossmodalities. Consequently, these models lack the ability to effectivelyunderstand the fine-grained details of input data, limiting their performancein tasks that require a more nuanced understanding. To address this limitation,there is a compelling need to develop models that enable fine-grainedunderstanding across multiple modalities, thereby enhancing their applicabilityto a wide range of tasks. In this paper, we propose GroundingGPT, a languageenhanced multi-modal grounding model. Beyond capturing global information likeother multi-modal models, our proposed model excels at tasks demanding adetailed understanding of local information within the input. It demonstratesprecise identification and localization of specific regions in images ormoments in videos. To achieve this objective, we design a diversified datasetconstruction pipeline, resulting in a multi-modal, multi-granularity datasetfor model training. The code, dataset, and demo of our model can be found athttps: //github.com/lzw-lzw/GroundingGPT.</description><author>Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, Tao Wang</author><pubDate>Tue, 05 Mar 2024 14:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06071v5</guid></item><item><title>VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?</title><link>http://arxiv.org/abs/2404.05955v1</link><description>Multimodal Large Language models (MLLMs) have shown promise in web-relatedtasks, but evaluating their performance in the web domain remains a challengedue to the lack of comprehensive benchmarks. Existing benchmarks are eitherdesigned for general multimodal tasks, failing to capture the uniquecharacteristics of web pages, or focus on end-to-end web agent tasks, unable tomeasure fine-grained abilities such as OCR, understanding, and grounding. Inthis paper, we introduce \bench{}, a multimodal benchmark designed to assessthe capabilities of MLLMs across a variety of web tasks. \bench{} consists ofseven tasks, and comprises 1.5K human-curated instances from 139 real websites,covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3series, and GPT-4V(ision) on \bench{}, revealing significant challenges andperformance gaps. Further analysis highlights the limitations of current MLLMs,including inadequate grounding in text-rich environments and subpar performancewith low-resolution image inputs. We believe \bench{} will serve as a valuableresource for the research community and contribute to the creation of morepowerful and versatile MLLMs for web-related applications.</description><author>Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, Xiang Yue</author><pubDate>Tue, 09 Apr 2024 03:29:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05955v1</guid></item><item><title>LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models</title><link>http://arxiv.org/abs/2305.13655v3</link><description>Recent advancements in text-to-image diffusion models have yielded impressiveresults in generating realistic and diverse images. However, these models stillstruggle with complex prompts, such as those that involve numeracy and spatialreasoning. This work proposes to enhance prompt understanding capabilities indiffusion models. Our method leverages a pretrained large language model (LLM)for grounded generation in a novel two-stage process. In the first stage, theLLM generates a scene layout that comprises captioned bounding boxes from agiven prompt describing the desired image. In the second stage, a novelcontroller guides an off-the-shelf diffusion model for layout-grounded imagegeneration. Both stages utilize existing pretrained models without additionalmodel parameter optimization. Our method significantly outperforms the basediffusion model and several strong baselines in accurately generating imagesaccording to prompts that require various capabilities, doubling the generationaccuracy across four tasks on average. Furthermore, our method enablesinstruction-based multi-round scene specification and can handle prompts inlanguages not supported by the underlying diffusion model. We anticipate thatour method will unleash users' creativity by accurately following more complexprompts. Our code, demo, and benchmark are available at:https://llm-grounded-diffusion.github.io</description><author>Long Lian, Boyi Li, Adam Yala, Trevor Darrell</author><pubDate>Mon, 04 Mar 2024 18:43:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13655v3</guid></item><item><title>KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models</title><link>http://arxiv.org/abs/2402.15043v1</link><description>Automatic evaluation methods for large language models (LLMs) are hindered bydata contamination, leading to inflated assessments of their effectiveness.Existing strategies, which aim to detect contaminated texts, focus onquantifying contamination status instead of accurately gauging modelperformance. In this paper, we introduce KIEval, a Knowledge-groundedInteractive Evaluation framework, which incorporates an LLM-powered"interactor" role for the first time to accomplish a dynamiccontamination-resilient evaluation. Starting with a question in a conventionalLLM benchmark involving domain-specific knowledge, KIEval utilizes dynamicallygenerated, multi-round, and knowledge-focused dialogues to determine whether amodel's response is merely a recall of benchmark answers or demonstrates a deepcomprehension to apply knowledge in more complex conversations. Extensiveexperiments on seven leading LLMs across five datasets validate KIEval'seffectiveness and generalization. We also reveal that data contamination bringsno contribution or even negative effect to models' real-world applicability andunderstanding, and existing contamination detection methods for LLMs can onlyidentify contamination in pre-training but not during supervised fine-tuning.</description><author>Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang</author><pubDate>Fri, 23 Feb 2024 01:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15043v1</guid></item><item><title>Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models</title><link>http://arxiv.org/abs/2403.19322v1</link><description>The surge of Multimodal Large Language Models (MLLMs), given their prominentemergent capabilities in instruction following and reasoning, has greatlyadvanced the field of visual reasoning. However, constrained by theirnon-lossless image tokenization, most MLLMs fall short of comprehensivelycapturing details of text and objects, especially in high-resolution images. Toaddress this, we propose P2G, a novel framework for plug-and-play grounding ofreasoning in MLLMs. Specifically, P2G exploits the tool-usage potential ofMLLMs to employ expert agents to achieve on-the-fly grounding to criticalvisual and textual objects of image, thus achieving deliberate reasoning viamultimodal prompting. We further create P2GB, a benchmark aimed at assessingMLLMs' ability to understand inter-object relationships and text in challenginghigh-resolution images. Comprehensive experiments on visual reasoning tasksdemonstrate the superiority of P2G. Noteworthy, P2G achieved comparableperformance with GPT-4V on P2GB, with a 7B backbone. Our work highlights thepotential of plug-and-play grounding of reasoning and opens up a promisingalternative beyond model scaling.</description><author>Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Ziyong Feng, Yongle Zhao, Yin Xie</author><pubDate>Thu, 28 Mar 2024 12:26:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19322v1</guid></item><item><title>MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal</title><link>http://arxiv.org/abs/2402.11297v1</link><description>Our contribution introduces a groundbreaking multimodal large language modeldesigned to comprehend multi-images, multi-audio, and multi-images-multi-audiowithin a single multiturn session. Leveraging state-of-the-art models, weutilize the SigLIP encoder for visual inputs and the Whisper Encoder for audioinputs. Notably, this multimodal large language model is bilingual, proficientin understanding both English and Malay simultaneously. We proudly unveil twoversions of this model: TinyLlama with 1.1B parameters, and Mistral with 7Bparameters. With its ability to navigate diverse modalities and languages, ourmodel represents a significant advancement for the Malaysian context andbeyond. All models released athttps://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859</description><author>Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan</author><pubDate>Sat, 17 Feb 2024 14:37:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11297v1</guid></item><item><title>Uncertainty Quantification for In-Context Learning of Large Language Models</title><link>http://arxiv.org/abs/2402.10189v2</link><description>In-context learning has emerged as a groundbreaking ability of Large LanguageModels (LLMs) and revolutionized various fields by providing a fewtask-relevant demonstrations in the prompt. However, trustworthy issues withLLM's response, such as hallucination, have also been actively discussed.Existing works have been devoted to quantifying the uncertainty in LLM'sresponse, but they often overlook the complex nature of LLMs and the uniquenessof in-context learning. In this work, we delve into the predictive uncertaintyof LLMs associated with in-context learning, highlighting that suchuncertainties may stem from both the provided demonstrations (aleatoricuncertainty) and ambiguities tied to the model's configurations (epistemicuncertainty). We propose a novel formulation and corresponding estimationmethod to quantify both types of uncertainties. The proposed method offers anunsupervised way to understand the prediction of in-context learning in aplug-and-play fashion. Extensive experiments are conducted to demonstrate theeffectiveness of the decomposition. The code and data are available at:https://github.com/lingchen0331/UQ_ICL.</description><author>Chen Ling, Xujiang Zhao, Xuchao Zhang, Wei Cheng, Yanchi Liu, Yiyou Sun, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</author><pubDate>Thu, 28 Mar 2024 20:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10189v2</guid></item><item><title>Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models</title><link>http://arxiv.org/abs/2402.10189v1</link><description>In-context learning has emerged as a groundbreaking ability of Large LanguageModels (LLMs) and revolutionized various fields by providing a fewtask-relevant demonstrations in the prompt. However, trustworthy issues withLLM's response, such as hallucination, have also been actively discussed.Existing works have been devoted to quantifying the uncertainty in LLM'sresponse, but they often overlook the complex nature of LLMs and the uniquenessof in-context learning. In this work, we delve into the predictive uncertaintyof LLMs associated with in-context learning, highlighting that suchuncertainties may stem from both the provided demonstrations (aleatoricuncertainty) and ambiguities tied to the model's configurations (epistemicuncertainty). We propose a novel formulation and corresponding estimationmethod to quantify both types of uncertainties. The proposed method offers anunsupervised way to understand the prediction of in-context learning in aplug-and-play fashion. Extensive experiments are conducted to demonstrate theeffectiveness of the decomposition. The code and data are available at:\url{https://github.com/lingchen0331/UQ_ICL}.</description><author>Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</author><pubDate>Thu, 15 Feb 2024 18:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10189v1</guid></item><item><title>Event Grounded Criminal Court View Generation withCooperative (Large) Language Models</title><link>http://arxiv.org/abs/2404.07001v1</link><description>With the development of legal intelligence, Criminal Court View Generationhas attracted much attention as a crucial task of legal intelligence, whichaims to generate concise and coherent texts that summarize case facts andprovide explanations for verdicts. Existing researches explore the keyinformation in case facts to yield the court views. Most of them employ acoarse-grained approach that partitions the facts into broad segments (e.g.,verdict-related sentences) to make predictions. However, this approach fails tocapture the complex details present in the case facts, such as various criminalelements and legal events. To this end, in this paper, we propose an EventGrounded Generation (EGG) method for criminal court view generation withcooperative (Large) Language Models, which introduces the fine-grained eventinformation into the generation. Specifically, we first design a LLMs-basedextraction method that can extract events in case facts without massiveannotated events. Then, we incorporate the extracted events into court viewgeneration by merging case facts and events. Besides, considering thecomputational burden posed by the use of LLMs in the extraction phase of EGG,we propose a LLMs-free EGG method that can eliminate the requirement for eventextraction using LLMs in the inference phase. Extensive experimental results ona real-world dataset clearly validate the effectiveness of our proposed method.</description><author>Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, Yanqing An</author><pubDate>Wed, 10 Apr 2024 14:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07001v1</guid></item><item><title>SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models</title><link>http://arxiv.org/abs/2311.09818v2</link><description>While most conversational agents are grounded on either free-text orstructured knowledge, many knowledge corpora consist of hybrid sources. Thispaper presents the first conversational agent that supports the full generalityof hybrid data access for large knowledge corpora, through a language wedeveloped called SUQL (Structured and Unstructured Query Language).Specifically, SUQL extends SQL with free-text primitives (summary and answer),so information retrieval can be composed with structured data accessesarbitrarily in a formal, succinct, precise, and interpretable notation. WithSUQL, we propose the first semantic parser, an LLM with in-context learning,that can handle hybrid data sources. Our in-context learning-based approach, when applied to the HybridQA dataset,comes within 8.9% exact match and 7.1% F1 of the SOTA, which was trained on 62Kdata samples. More significantly, unlike previous approaches, our technique isapplicable to large databases and free-text corpora. We introduce a datasetconsisting of crowdsourced questions and conversations on Yelp, a large, realrestaurant knowledge base with structured and unstructured data. We show thatour few-shot conversational agent based on SUQL finds an entity satisfying alluser requirements 90.3% of the time, compared to 63.4% for a baseline based onlinearization.</description><author>Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina J. Semnani, Chen Jie Yu, Monica S. Lam</author><pubDate>Wed, 13 Mar 2024 18:07:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09818v2</guid></item><item><title>Event Grounded Criminal Court View Generation with Cooperative (Large) Language Models</title><link>http://arxiv.org/abs/2404.07001v3</link><description>With the development of legal intelligence, Criminal Court View Generationhas attracted much attention as a crucial task of legal intelligence, whichaims to generate concise and coherent texts that summarize case facts andprovide explanations for verdicts. Existing researches explore the keyinformation in case facts to yield the court views. Most of them employ acoarse-grained approach that partitions the facts into broad segments (e.g.,verdict-related sentences) to make predictions. However, this approach fails tocapture the complex details present in the case facts, such as various criminalelements and legal events. To this end, in this paper, we propose an EventGrounded Generation (EGG) method for criminal court view generation withcooperative (Large) Language Models, which introduces the fine-grained eventinformation into the generation. Specifically, we first design a LLMs-basedextraction method that can extract events in case facts without massiveannotated events. Then, we incorporate the extracted events into court viewgeneration by merging case facts and events. Besides, considering thecomputational burden posed by the use of LLMs in the extraction phase of EGG,we propose a LLMs-free EGG method that can eliminate the requirement for eventextraction using LLMs in the inference phase. Extensive experimental results ona real-world dataset clearly validate the effectiveness of our proposed method.</description><author>Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, Yanqing An</author><pubDate>Tue, 16 Apr 2024 07:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07001v3</guid></item><item><title>Event Grounded Criminal Court View Generation with Cooperative (Large) Language Models</title><link>http://arxiv.org/abs/2404.07001v2</link><description>With the development of legal intelligence, Criminal Court View Generationhas attracted much attention as a crucial task of legal intelligence, whichaims to generate concise and coherent texts that summarize case facts andprovide explanations for verdicts. Existing researches explore the keyinformation in case facts to yield the court views. Most of them employ acoarse-grained approach that partitions the facts into broad segments (e.g.,verdict-related sentences) to make predictions. However, this approach fails tocapture the complex details present in the case facts, such as various criminalelements and legal events. To this end, in this paper, we propose an EventGrounded Generation (EGG) method for criminal court view generation withcooperative (Large) Language Models, which introduces the fine-grained eventinformation into the generation. Specifically, we first design a LLMs-basedextraction method that can extract events in case facts without massiveannotated events. Then, we incorporate the extracted events into court viewgeneration by merging case facts and events. Besides, considering thecomputational burden posed by the use of LLMs in the extraction phase of EGG,we propose a LLMs-free EGG method that can eliminate the requirement for eventextraction using LLMs in the inference phase. Extensive experimental results ona real-world dataset clearly validate the effectiveness of our proposed method.</description><author>Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, Yanqing An</author><pubDate>Sat, 13 Apr 2024 14:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07001v2</guid></item><item><title>OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web</title><link>http://arxiv.org/abs/2402.17553v2</link><description>For decades, human-computer interaction has fundamentally been manual. Eventoday, almost all productive work done on the computer necessitates human inputat every step. Autonomous virtual agents represent an exciting step inautomating many of these menial tasks. Virtual agents would empower users withlimited technical proficiency to harness the full possibilities of computersystems. They could also enable the efficient streamlining of numerous computertasks, ranging from calendar management to complex travel bookings, withminimal human intervention. In this paper, we introduce OmniACT, thefirst-of-a-kind dataset and benchmark for assessing an agent's capability togenerate executable programs to accomplish computer tasks. Our scope extendsbeyond traditional web automation, covering a diverse range of desktopapplications. The dataset consists of fundamental tasks such as "Play the nextsong", as well as longer horizon tasks such as "Send an email to John Doementioning the time and place to meet". Specifically, given a pair of screenimage and a visually-grounded natural language task, the goal is to generate ascript capable of fully executing the task. We run several strong baselinelanguage model agents on our benchmark. The strongest baseline, GPT-4, performsthe best on our benchmark However, its performance level still reaches only 15%of the human proficiency in generating executable scripts capable of completingthe task, demonstrating the challenge of our task for conventional web agents.Our benchmark provides a platform to measure and evaluate the progress oflanguage model agents in automating computer tasks and motivates future worktowards building multimodal models that bridge large language models and thevisual grounding of computer screens.</description><author>Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, Ruslan Salakhutdinov</author><pubDate>Wed, 28 Feb 2024 17:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17553v2</guid></item><item><title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title><link>http://arxiv.org/abs/2402.14207v2</link><description>We study how to apply large language models to write grounded and organizedlong-form articles from scratch, with comparable breadth and depth to Wikipediapages. This underexplored problem poses new challenges at the pre-writingstage, including how to research the topic and prepare an outline prior towriting. We propose STORM, a writing system for the Synthesis of Topic Outlinesthrough Retrieval and Multi-perspective Question Asking. STORM models thepre-writing stage by (1) discovering diverse perspectives in researching thegiven topic, (2) simulating conversations where writers carrying differentperspectives pose questions to a topic expert grounded on trusted Internetsources, (3) curating the collected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-qualityWikipedia articles, and formulate outline assessments to evaluate thepre-writing stage. We further gather feedback from experienced Wikipediaeditors. Compared to articles generated by an outline-drivenretrieval-augmented baseline, more of STORM's articles are deemed to beorganized (by a 25% absolute increase) and broad in coverage (by 10%). Theexpert feedback also helps identify new challenges for generating grounded longarticles, such as source bias transfer and over-association of unrelated facts.</description><author>Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, Monica S. Lam</author><pubDate>Mon, 08 Apr 2024 06:38:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14207v2</guid></item><item><title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title><link>http://arxiv.org/abs/2402.14207v1</link><description>We study how to apply large language models to write grounded and organizedlong-form articles from scratch, with comparable breadth and depth to Wikipediapages. This underexplored problem poses new challenges at the pre-writingstage, including how to research the topic and prepare an outline prior towriting. We propose STORM, a writing system for the Synthesis of Topic Outlinesthrough Retrieval and Multi-perspective Question Asking. STORM models thepre-writing stage by (1) discovering diverse perspectives in researching thegiven topic, (2) simulating conversations where writers carrying differentperspectives pose questions to a topic expert grounded on trusted Internetsources, (3) curating the collected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-qualityWikipedia articles, and formulate outline assessments to evaluate thepre-writing stage. We further gather feedback from experienced Wikipediaeditors. Compared to articles generated by an outline-drivenretrieval-augmented baseline, more of STORM's articles are deemed to beorganized (by a 25% absolute increase) and broad in coverage (by 10%). Theexpert feedback also helps identify new challenges for generating grounded longarticles, such as source bias transfer and over-association of unrelated facts.</description><author>Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, Monica S. Lam</author><pubDate>Thu, 22 Feb 2024 01:20:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14207v1</guid></item><item><title>HawkEye: Training Video-Text LLMs for Grounding Text in Videos</title><link>http://arxiv.org/abs/2403.10228v1</link><description>Video-text Large Language Models (video-text LLMs) have shown remarkableperformance in answering questions and holding conversations on simple videos.However, they perform almost the same as random on grounding text queries inlong and complicated videos, having little ability to understand and reasonabout temporal information, which is the most fundamental difference betweenvideos and images. In this paper, we propose HawkEye, one of the firstvideo-text LLMs that can perform temporal video grounding in a fullytext-to-text manner. To collect training data that is applicable for temporalvideo grounding, we construct InternVid-G, a large-scale video-text corpus withsegment-level captions and negative spans, with which we introduce two newtime-aware training objectives to video-text LLMs. We also propose acoarse-grained method of representing segments in videos, which is more robustand easier for LLMs to learn and follow than other alternatives. Extensiveexperiments show that HawkEye is better at temporal video grounding andcomparable on other video-text tasks with existing video-text LLMs, whichverifies its superior video-text multi-modal understanding abilities.</description><author>Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, Dongyan Zhao</author><pubDate>Fri, 15 Mar 2024 12:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10228v1</guid></item><item><title>Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models</title><link>http://arxiv.org/abs/2402.16315v1</link><description>Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)have imbued the models with the ability to generate high-level, image-groundedexplanations with ease. While such capability is largely attributed to the richworld knowledge contained within the Large Language Models (LLMs), our workreveals their shortcomings in fine-grained visual categorization (FGVC) acrosssix different benchmark settings. Most recent state-of-the-art LVLMs likeLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms ofclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogsfor LLaVA-1.5, but also struggle to generate an accurate explanation withdetailed attributes based on the concept that appears within an input imagedespite their capability to generate holistic image-level descriptions.In-depth analyses show that instruction-tuned LVLMs exhibit modality gap,showing discrepancy when given textual and visual inputs that correspond to thesame concept, preventing the image modality from leveraging the rich parametricknowledge within the LLMs. In an effort to further the community's endeavor inthis direction, we propose a multiple granularity attribute-centric evaluationbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'fine-grained visual comprehension ability and provide significantly improvedexplainability.</description><author>Jeonghwan Kim, Heng Ji</author><pubDate>Mon, 26 Feb 2024 05:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16315v1</guid></item><item><title>Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models</title><link>http://arxiv.org/abs/2308.09778v3</link><description>Large vision-and-language models (VLMs) trained to match images with text onlarge-scale datasets of image-text pairs have shown impressive generalizationability on several vision and language tasks. Several recent works, however,showed that these models lack fine-grained understanding, such as the abilityto count and recognize verbs, attributes, or relationships. The focus of thiswork is to study the understanding of spatial relations. This has been tackledpreviously using image-text matching (e.g., Visual Spatial Reasoning benchmark)or visual question answering (e.g., GQA or VQAv2), both showing poorperformance and a large gap compared to human performance. In this work, weshow qualitatively (using explainability tools) and quantitatively (usingobject detectors) that the poor object localization "grounding" ability of themodels is a contributing factor to the poor image-text matching performance. Wepropose an alternative fine-grained, compositional approach for recognizing andranking spatial clauses that combines the evidence from grounding noun phrasescorresponding to objects and their locations to compute the final rank of thespatial clause. We demonstrate the approach on representative VLMs (such asLXMERT, GPV, and MDETR) and compare and highlight their abilities to reasonabout spatial relationships.</description><author>Navid Rajabi, Jana Kosecka</author><pubDate>Wed, 06 Mar 2024 00:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09778v3</guid></item><item><title>Auto-Vocabulary Semantic Segmentation</title><link>http://arxiv.org/abs/2312.04539v2</link><description>Open-ended image understanding tasks gained significant attention from theresearch community, particularly with the emergence of Vision-Language Models.Open-Vocabulary Segmentation (OVS) methods are capable of performing semanticsegmentation without relying on a fixed vocabulary, and in some cases, theyoperate without the need for training or fine-tuning. However, OVS methodstypically require users to specify the vocabulary based on the task or datasetat hand. In this paper, we introduce \textit{Auto-Vocabulary SemanticSegmentation (AVS)}, advancing open-ended image understanding by eliminatingthe necessity to predefine object categories for segmentation. Our approach,\ours, presents a framework that autonomously identifies relevant class namesusing enhanced BLIP embeddings, which are utilized for segmentation afterwards.Given that open-ended object category predictions cannot be directly comparedwith a fixed ground truth, we develop a Large Language Model-basedAuto-Vocabulary Evaluator (LAVE) to efficiently evaluate the automaticallygenerated class names and their corresponding segments. Our method sets newbenchmarks on datasets such as PASCAL VOC and Context, ADE20K, and Cityscapesfor AVS and showcases competitive performance to OVS methods that requirespecified class names.</description><author>Osman Ülger, Maksymilian Kulicki, Yuki Asano, Martin R. Oswald</author><pubDate>Wed, 20 Mar 2024 17:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04539v2</guid></item><item><title>Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study</title><link>http://arxiv.org/abs/2404.07060v1</link><description>We present an empirical study of groundedness in long-form question answering(LFQA) by retrieval-augmented large language models (LLMs). In particular, weevaluate whether every generated sentence is grounded in the retrieveddocuments or the model's pre-training data. Across 3 datasets and 4 modelfamilies, our findings reveal that a significant fraction of generatedsentences are consistently ungrounded, even when those sentences containcorrect ground-truth answers. Additionally, we examine the impacts of factorssuch as model size, decoding strategy, and instruction tuning on groundedness.Our results show that while larger models tend to ground their outputs moreeffectively, a significant portion of correct answers remains compromised byhallucinations. This study provides novel insights into the groundednesschallenges in LFQA and underscores the necessity for more robust mechanisms inLLMs to mitigate the generation of ungrounded content.</description><author>Alessandro Stolfo</author><pubDate>Wed, 10 Apr 2024 15:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07060v1</guid></item><item><title>VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning</title><link>http://arxiv.org/abs/2404.07078v1</link><description>Recognising emotions in context involves identifying the apparent emotions ofan individual, taking into account contextual cues from the surrounding scene.Previous approaches to this task have involved the design of explicitscene-encoding architectures or the incorporation of external scene-relatedinformation, such as captions. However, these methods often utilise limitedcontextual information or rely on intricate training pipelines. In this work,we leverage the groundbreaking capabilities of Vision-and-Large-Language Models(VLLMs) to enhance in-context emotion classification without introducingcomplexity to the training process in a two-stage approach. In the first stage,we propose prompting VLLMs to generate descriptions in natural language of thesubject's apparent emotion relative to the visual context. In the second stage,the descriptions are used as contextual information and, along with the imageinput, are used to train a transformer-based architecture that fuses text andvisual features before the final classification task. Our experimental resultsshow that the text and image features have complementary information, and ourfused architecture significantly outperforms the individual modalities withoutany complex training methods. We evaluate our approach on three differentdatasets, namely, EMOTIC, CAER-S, and BoLD, and achieve state-of-the-art orcomparable accuracy across all datasets and metrics compared to much morecomplex approaches. The code will be made publicly available on github:https://github.com/NickyFot/EmoCommonSense.git</description><author>Alexandros Xenos, Niki Maria Foteinopoulou, Ioanna Ntinou, Ioannis Patras, Georgios Tzimiropoulos</author><pubDate>Wed, 10 Apr 2024 16:09:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07078v1</guid></item><item><title>Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment</title><link>http://arxiv.org/abs/2312.09625v2</link><description>Learning to ground natural language queries to target objects or regions in3D point clouds is quite essential for 3D scene understanding. Nevertheless,existing 3D visual grounding approaches require a substantial number ofbounding box annotations for text queries, which is time-consuming andlabor-intensive to obtain. In this paper, we propose \textbf{3D-VLA}, a weaklysupervised approach for \textbf{3D} visual grounding based on \textbf{V}isual\textbf{L}inguistic \textbf{A}lignment. Our 3D-VLA exploits the superiorability of current large-scale vision-language models (VLMs) on aligning thesemantics between texts and 2D images, as well as the naturally existingcorrespondences between 2D images and 3D point clouds, and thus implicitlyconstructs correspondences between texts and 3D point clouds with no need forfine-grained box annotations in the training procedure. During the inferencestage, the learned text-3D correspondence will help us ground the text queriesto the 3D target objects even without 2D images. To the best of our knowledge,this is the first work to investigate 3D visual grounding in a weaklysupervised manner by involving large scale vision-language models, andextensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our3D-VLA achieves comparable and even superior results over the fully supervisedmethods.</description><author>Xiaoxu Xu, Yitian Yuan, Qiudan Zhang, Wenhui Wu, Zequn Jie, Lin Ma, Xu Wang</author><pubDate>Sat, 13 Apr 2024 09:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09625v2</guid></item><item><title>Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization</title><link>http://arxiv.org/abs/2403.18120v1</link><description>Large language models (LLM), such as Google's Minerva and OpenAI's GPTfamilies, are becoming increasingly capable of solving mathematicalquantitative reasoning problems. However, they still make unjustified logicaland computational errors in their reasoning steps and answers. In this paper,we leverage the fact that if the training corpus of LLMs contained sufficientlymany examples of formal mathematics (e.g. in Isabelle, a formal theorem provingenvironment), they can be prompted to translate i.e. autoformalize informalmathematical statements into formal Isabelle code -- which can be verifiedautomatically for internal consistency. This provides a mechanism toautomatically reject solutions whose formalized versions are inconsistentwithin themselves or with the formalized problem statement. We evaluate ourmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approachprovides a consistently better heuristic than vanilla majority voting -- thepreviously best method to identify correct answers, by more than 12% on GSM8K.In our experiments it improves results consistently across all datasets and LLMmodel sizes. The code can be found at https://github.com/jinpz/dtv.</description><author>Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu</author><pubDate>Tue, 26 Mar 2024 23:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18120v1</guid></item><item><title>A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation</title><link>http://arxiv.org/abs/2404.03491v1</link><description>Empowered by the large-scale pretrained language models, existing dialoguesystems have demonstrated impressive performance conducting fluent andnatural-sounding conversations. However, they are still plagued by thehallucination problem, causing unpredictable factual errors in the generatedresponses. Recently, knowledge-grounded dialogue generation models, thatintentionally invoke external knowledge resources to more informativeresponses, are also proven to be effective in reducing hallucination. Followingthe idea of getting high-quality knowledge, a few efforts have achieved prettygood performance on this issue. As some inevitable knowledge noises may alsolead to hallucinations, it is emergent to investigate the reason and futuredirections for building noise-tolerant methods in KGD tasks. In this paper, weanalyze the causal story behind this problem with counterfactual reasoningmethods. Based on the causal effect analysis, we propose a possible solutionfor alleviating the hallucination in KGD by exploiting the dialogue-knowledgeinteraction. Experimental results of our example implementation show that thismethod can reduce hallucination without disrupting other dialogue performance,while keeping adaptive to different generation models. We hope our efforts cansupport and call for more attention to developing lightweight techniquestowards robust and trusty dialogue systems.</description><author>Jifan Yu, Xiaohan Zhang, Yifan Xu, Xuanyu Lei, Zijun Yao, Jing Zhang, Lei Hou, Juanzi Li</author><pubDate>Thu, 04 Apr 2024 15:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03491v1</guid></item><item><title>Large Language Models Cannot Self-Correct Reasoning Yet</title><link>http://arxiv.org/abs/2310.01798v2</link><description>Large Language Models (LLMs) have emerged as a groundbreaking technology withtheir unparalleled text generation capabilities across various applications.Nevertheless, concerns persist regarding the accuracy and appropriateness oftheir generated content. A contemporary methodology, self-correction, has beenproposed as a remedy to these issues. Building upon this premise, this papercritically examines the role and efficacy of self-correction within LLMs,shedding light on its true potential and limitations. Central to ourinvestigation is the notion of intrinsic self-correction, whereby an LLMattempts to correct its initial responses based solely on its inherentcapabilities, without the crutch of external feedback. In the context ofreasoning, our research indicates that LLMs struggle to self-correct theirresponses without external feedback, and at times, their performance evendegrades after self-correction. Drawing from these insights, we offersuggestions for future research and practical applications in this field.</description><author>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, Denny Zhou</author><pubDate>Thu, 14 Mar 2024 05:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01798v2</guid></item><item><title>Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units</title><link>http://arxiv.org/abs/2403.16609v1</link><description>Successful conversations often rest on common understanding, where allparties are on the same page about the information being shared. This process,known as conversational grounding, is crucial for building trustworthy dialogsystems that can accurately keep track of and recall the shared information.The proficiencies of an agent in grounding the conveyed informationsignificantly contribute to building a reliable dialog system. Despite recentadvancements in dialog systems, there exists a noticeable deficit in theirgrounding capabilities. Traum provided a framework for conversational groundingintroducing Grounding Acts and Grounding Units, but substantial progress,especially in the realm of Large Language Models, remains lacking. To bridgethis gap, we present the annotation of two dialog corpora employing GroundingActs, Grounding Units, and a measure of their degree of grounding. We discussour key findings during the annotation and also provide a baseline model totest the performance of current Language Models in categorizing the groundingacts of the dialogs. Our work aims to provide a useful resource for furtherresearch in making conversations with machines better understood and morereliable in natural day-to-day collaborative dialogs.</description><author>Biswesh Mohapatra, Seemab Hassan, Laurent Romary, Justine Cassell</author><pubDate>Mon, 25 Mar 2024 11:39:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16609v1</guid></item><item><title>Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps</title><link>http://arxiv.org/abs/2307.05052v3</link><description>We investigate the role of various demonstration components in the in-contextlearning (ICL) performance of large language models (LLMs). Specifically, weexplore the impacts of ground-truth labels, input distribution, andcomplementary explanations, particularly when these are altered or perturbed.We build on previous work, which offers mixed findings on how these elementsinfluence ICL. To probe these questions, we employ explainable NLP (XNLP)methods and utilize saliency maps of contrastive demonstrations for bothqualitative and quantitative analysis. Our findings reveal that flippingground-truth labels significantly affects the saliency, though it's morenoticeable in larger LLMs. Our analysis of the input distribution at a granularlevel reveals that changing sentiment-indicative terms in a sentiment analysistask to neutral ones does not have as substantial an impact as alteringground-truth labels. Finally, we find that the effectiveness of complementaryexplanations in boosting ICL performance is task-dependent, with limitedbenefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.These insights are critical for understanding the functionality of LLMs andguiding the development of effective demonstrations, which is increasinglyrelevant in light of the growing use of LLMs in applications such as ChatGPT.Our research code is publicly available at https://github.com/paihengxu/XICL.</description><author>Fuxiao Liu, Paiheng Xu, Zongxia Li, Yue Feng</author><pubDate>Mon, 15 Apr 2024 16:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05052v3</guid></item><item><title>SwissNYF: Tool Grounded LLM Agents for Black Box Setting</title><link>http://arxiv.org/abs/2402.10051v1</link><description>While Large Language Models (LLMs) have demonstrated enhanced capabilities infunction-calling, these advancements primarily rely on accessing the functions'responses. This methodology is practical for simpler APIs but faces scalabilityissues with irreversible APIs that significantly impact the system, such as adatabase deletion API. Similarly, processes requiring extensive time for eachAPI call and those necessitating forward planning, like automated actionpipelines, present complex challenges. Furthermore, scenarios often arise wherea generalized approach is needed because algorithms lack direct access to thespecific implementations of these functions or secrets to use them. Traditionaltool planning methods are inadequate in these cases, compelling the need tooperate within black-box environments. Unlike their performance in toolmanipulation, LLMs excel in black-box tasks, such as program synthesis.Therefore, we harness the program synthesis capabilities of LLMs to strategizetool usage in black-box settings, ensuring solutions are verified prior toimplementation. We introduce TOPGUN, an ingeniously crafted approach leveragingprogram synthesis for black box tool planning. Accompanied by SwissNYF, acomprehensive suite that integrates black-box algorithms for planning andverification tasks, addressing the aforementioned challenges and enhancing theversatility and effectiveness of LLMs in complex API interactions. The publiccode for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.</description><author>Somnath Sendhil Kumar, Dhruv Jain, Eshaan Agarwal, Raunak Pandey</author><pubDate>Thu, 15 Feb 2024 16:15:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10051v1</guid></item><item><title>Exploring ChatGPT and its Impact on Society</title><link>http://arxiv.org/abs/2403.14643v2</link><description>Artificial intelligence has been around for a while, but suddenly it hasreceived more attention than ever before. Thanks to innovations from companieslike Google, Microsoft, Meta, and other major brands in technology. OpenAI,though, has triggered the button with its ground-breaking invention ChatGPT.ChatGPT is a Large Language Model (LLM) based on Transformer architecture thathas the ability to generate human-like responses in a conversational context.It uses deep learning algorithms to generate natural language responses toinput text. Its large number of parameters, contextual generation, andopen-domain training make it a versatile and effective tool for a wide range ofapplications, from chatbots to customer service to language translation. It hasthe potential to revolutionize various industries and transform the way weinteract with technology. However, the use of ChatGPT has also raised severalconcerns, including ethical, social, and employment challenges, which must becarefully considered to ensure the responsible use of this technology. Thearticle provides an overview of ChatGPT, delving into its architecture andtraining process. It highlights the potential impacts of ChatGPT on thesociety. In this paper, we suggest some approaches involving technology,regulation, education, and ethics in an effort to maximize ChatGPT's benefitswhile minimizing its negative impacts. This study is expected to contribute toa greater understanding of ChatGPT and aid in predicting the potential changesit may bring about.</description><author>Md. Asraful Haque, Shuai Li</author><pubDate>Mon, 25 Mar 2024 06:35:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14643v2</guid></item><item><title>GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment</title><link>http://arxiv.org/abs/2403.11075v1</link><description>Verbal communication plays a crucial role in human cooperation, particularlywhen the partners only have incomplete information about the task, environment,and each other's mental state. In this paper, we propose a novel cooperativecommunication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulatesverbal communication as a planning problem that minimizes the misalignmentbetween the parts of agents' mental states that are relevant to the goals. Thisapproach enables an embodied assistant to reason about when and how toproactively initialize communication with humans verbally using naturallanguage to help achieve better cooperation. We evaluate our approach againststrong baselines in two challenging environments, Overcooked (a multiplayergame) and VirtualHome (a household simulator). Our experimental resultsdemonstrate that large language models struggle with generating meaningfulcommunication that is grounded in the social and physical context. In contrast,our approach can successfully generate concise verbal communication for theembodied assistant to effectively boost the performance of the cooperation aswell as human users' perception of the assistant.</description><author>Lance Ying, Kunal Jha, Shivam Aarya, Joshua B. Tenenbaum, Antonio Torralba, Tianmin Shu</author><pubDate>Sun, 17 Mar 2024 04:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11075v1</guid></item><item><title>DOrA: 3D Visual Grounding with Order-Aware Referring</title><link>http://arxiv.org/abs/2403.16539v1</link><description>3D visual grounding aims to identify the target object within a 3D pointcloud scene referred to by a natural language description. While previous worksattempt to exploit the verbo-visual relation with proposed cross-modaltransformers, unstructured natural utterances and scattered objects might leadto undesirable performances. In this paper, we introduce DOrA, a novel 3Dvisual grounding framework with Order-Aware referring. DOrA is designed toleverage Large Language Models (LLMs) to parse language description, suggestinga referential order of anchor objects. Such ordered anchor objects allow DOrAto update visual features and locate the target object during the groundingprocess. Experimental results on the NR3D and ScanRefer datasets demonstrateour superiority in both low-resource and full-data scenarios. In particular,DOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% groundingaccuracy under 1% data and 10% data settings, respectively.</description><author>Tung-Yu Wu, Sheng-Yu Huang, Yu-Chiang Frank Wang</author><pubDate>Mon, 25 Mar 2024 09:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16539v1</guid></item><item><title>Toward Grounded Commonsense Reasoning</title><link>http://arxiv.org/abs/2306.08651v2</link><description>Consider a robot tasked with tidying a desk with a meticulously constructedLego sports car. A human may recognize that it is not appropriate todisassemble the sports car and put it away as part of the "tidying." How can arobot reach that conclusion? Although large language models (LLMs) haverecently been used to enable commonsense reasoning, grounding this reasoning inthe real world has been challenging. To reason in the real world, robots mustgo beyond passively querying LLMs and actively gather information from theenvironment that is required to make the right decision. For instance, afterdetecting that there is an occluded car, the robot may need to activelyperceive the car to know whether it is an advanced model car made out of Legosor a toy car built by a toddler. We propose an approach that leverages an LLMand vision language model (VLM) to help a robot actively perceive itsenvironment to perform grounded commonsense reasoning. To evaluate ourframework at scale, we release the MessySurfaces dataset which contains imagesof 70 real-world surfaces that need to be cleaned. We additionally illustrateour approach with a robot on 2 carefully designed surfaces. We find an average12.9% improvement on the MessySurfaces benchmark and an average 15% improvementon the robot experiments over baselines that do not use active perception. Thedataset, code, and videos of our approach can be found athttps://minaek.github.io/grounded_commonsense_reasoning.</description><author>Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan, Dorsa Sadigh</author><pubDate>Mon, 19 Feb 2024 02:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08651v2</guid></item><item><title>Attribute First, then Generate: Locally-attributable Grounded Text Generation</title><link>http://arxiv.org/abs/2403.17104v2</link><description>Recent efforts to address hallucinations in Large Language Models (LLMs) havefocused on attributed text generation, which supplements generated texts withcitations of supporting sources for post-generation fact-checking andcorrections. Yet, these citations often point to entire documents orparagraphs, burdening users with extensive verification work. In this paper, weintroduce a locally-attributable text generation approach, prioritizing conciseattributions. Our method, named ``Attribute First, then Generate'', breaks downthe conventional end-to-end generation process into three intuitive steps:content selection, sentence planning, and sequential sentence generation. Byinitially identifying relevant source segments (``select first'') and thenconditioning the generation process on them (``then generate''), we ensurethese segments also act as the output's fine-grained attributions (``select''becomes ``attribute''). Tested on Multi-document Summarization and Long-formQuestion-answering, our method not only yields more concise citations than thebaselines but also maintains - and in some cases enhances - both generationquality and attribution accuracy. Furthermore, it significantly reduces thetime required for fact verification by human assessors.</description><author>Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, Ido Dagan</author><pubDate>Mon, 01 Apr 2024 18:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17104v2</guid></item><item><title>Grounding Data Science Code Generation with Input-Output Specifications</title><link>http://arxiv.org/abs/2402.08073v2</link><description>Large language models (LLMs) have recently demonstrated a remarkable abilityto generate code from natural language (NL) prompts. However, in the realworld, NL is often too ambiguous to capture the true intent behind programmingproblems, requiring additional input-output (I/O) specifications.Unfortunately, LLMs can have difficulty aligning their outputs with both the NLprompt and the I/O specification. In this paper, we give a way to mitigate thisissue in the context of data science programming, where tasks require explicitI/O specifications for clarity. Specifically, we propose GIFT4Code, a novelapproach for the instruction fine-tuning of LLMs with respect to I/Ospecifications. Our method leverages synthetic data produced by the LLM itselfand utilizes execution-derived feedback as a key learning signal. Thisfeedback, in the form of program I/O specifications, is provided to the LLM tofacilitate instruction fine-tuning. We evaluated our approach on twochallenging data science benchmarks, Arcade and DS-1000. The resultsdemonstrate a significant improvement in the LLM's ability to generate codethat is not only executable but also accurately aligned with userspecifications, substantially improving the quality of code generation forcomplex data science tasks.</description><author>Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski, Swarat Chaudhuri, Alex Polozov</author><pubDate>Fri, 15 Mar 2024 02:18:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08073v2</guid></item><item><title>Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs</title><link>http://arxiv.org/abs/2404.05719v1</link><description>Recent advancements in multimodal large language models (MLLMs) have beennoteworthy, yet, these general-domain MLLMs often fall short in their abilityto comprehend and interact effectively with user interface (UI) screens. Inthis paper, we present Ferret-UI, a new MLLM tailored for enhancedunderstanding of mobile UI screens, equipped with referring, grounding, andreasoning capabilities. Given that UI screens typically exhibit a moreelongated aspect ratio and contain smaller objects of interest (e.g., icons,texts) than natural images, we incorporate "any resolution" on top of Ferret tomagnify details and leverage enhanced visual features. Specifically, eachscreen is divided into 2 sub-images based on the original aspect ratio (i.e.,horizontal division for portrait screens and vertical division for landscapescreens). Both sub-images are encoded separately before being sent to LLMs. Wemeticulously gather training samples from an extensive range of elementary UItasks, such as icon recognition, find text, and widget listing. These samplesare formatted for instruction-following with region annotations to facilitateprecise referring and grounding. To augment the model's reasoning ability, wefurther compile a dataset for advanced tasks, including detailed description,perception/interaction conversations, and function inference. After training onthe curated datasets, Ferret-UI exhibits outstanding comprehension of UIscreens and the capability to execute open-ended instructions. For modelevaluation, we establish a comprehensive benchmark encompassing all theaforementioned tasks. Ferret-UI excels not only beyond most open-source UIMLLMs, but also surpasses GPT-4V on all the elementary UI tasks.</description><author>Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, Zhe Gan</author><pubDate>Mon, 08 Apr 2024 18:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05719v1</guid></item><item><title>GrounDial: Human-norm Grounded Safe Dialog Response Generation</title><link>http://arxiv.org/abs/2402.08968v1</link><description>Current conversational AI systems based on large language models (LLMs) areknown to generate unsafe responses, agreeing to offensive user input orincluding toxic content. Previous research aimed to alleviate the toxicity, byfine-tuning LLM with manually annotated safe dialogue histories. However, thedependency on additional tuning requires substantial costs. To remove thedependency, we propose GrounDial, where response safety is achieved bygrounding responses to commonsense social rules without requiring fine-tuning.A hybrid approach of in-context learning and human-norm-guided decoding ofGrounDial enables the response to be quantitatively and qualitatively safereven without additional data or tuning.</description><author>Siwon Kim, Shuyang Dai, Mohammad Kachuee, Shayan Ray, Tara Taghavi, Sungroh Yoon</author><pubDate>Wed, 14 Feb 2024 06:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08968v1</guid></item><item><title>Learning from Models and Data for Visual Grounding</title><link>http://arxiv.org/abs/2403.13804v1</link><description>We introduce SynGround, a novel framework that combines data-driven learningand knowledge transfer from various large-scale pretrained models to enhancethe visual grounding capabilities of a pretrained vision-and-language model.The knowledge transfer from the models initiates the generation of imagedescriptions through an image description generator. These descriptions servedual purposes: they act as prompts for synthesizing images through atext-to-image generator, and as queries for synthesizing text, from whichphrases are extracted using a large language model. Finally, we leverage anopen-vocabulary object detector to generate synthetic bounding boxes for thesynthetic images and texts. We finetune a pretrained vision-and-language modelon this dataset by optimizing a mask-attention consistency objective thataligns region annotations with gradient-based model explanations. The resultingmodel improves the grounding capabilities of an off-the-shelfvision-and-language model. Particularly, SynGround improves the pointing gameaccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and onRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to63.67%.</description><author>Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez</author><pubDate>Wed, 20 Mar 2024 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13804v1</guid></item><item><title>Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations</title><link>http://arxiv.org/abs/2402.11770v2</link><description>We introduce a structured chain-of-thought (SCoT) prompting approach togenerating content-grounded multi-turn question-answer conversations using apre-trained large language model (LLM). At the core of our proposal is astructured breakdown of the complex task into a number of states in a statemachine, so that actions corresponding to various subtasks, e.g., contentreading and utterance generation, can be executed in their own dedicatedstates. Each state leverages a unique set of resources including prompts and(optionally) additional tools to augment the generation process. Ourexperimental results show that SCoT prompting with designated states forhallucination mitigation increases agent faithfulness to grounding documents byup to 16.8%. When used as training data, our open-domain conversationssynthesized from only 6 Wikipedia-based seed demonstrations train strongconversational QA agents; in out-of-domain evaluation, for example, we observeimprovements of up to 13.9% over target domain gold data when the latter isaugmented with our generated examples.</description><author>Md Arafat Sultan, Jatin Ganhotra, Ramón Fernandez Astudillo</author><pubDate>Tue, 20 Feb 2024 02:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11770v2</guid></item><item><title>Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations</title><link>http://arxiv.org/abs/2402.11770v1</link><description>We introduce a structured chain-of-thought (SCoT) prompting approach togenerating content-grounded multi-turn question-answer conversations using apre-trained large language model (LLM). At the core of our proposal is astructured breakdown of the complex task into a number of states in a statemachine, so that actions corresponding to various subtasks, e.g., contentreading and utterance generation, can be executed in their own dedicatedstates. Each state leverages a unique set of resources including prompts and(optionally) additional tools to augment the generation process. Ourexperimental results show that SCoT prompting with designated states forhallucination mitigation increases agent faithfulness to grounding documents byup to 16.8%. When used as training data, our open-domain conversationssynthesized from only 6 Wikipedia-based seed demonstrations train strongconversational QA agents; in out-of-domain evaluation, for example, we observeimprovements of up to 13.9% over target domain gold data when the latter isaugmented with our generated examples.</description><author>Md Arafat Sultan, Jatin Ganhotra, Ramón Fernandez Astudillo</author><pubDate>Mon, 19 Feb 2024 01:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11770v1</guid></item><item><title>Explainable Traffic Flow Prediction with Large Language Models</title><link>http://arxiv.org/abs/2404.02937v1</link><description>Traffic flow prediction provides essential future views in the intelligenttransportation system. Explainable predictions offer valuable insights into thefactors influencing traffic patterns, which help urban planners, trafficengineers, and policymakers make informed decisions about infrastructuredevelopment, traffic management strategies, and public transportation planning.Despite their widespread popularity and commendable accuracy, predictionmethods grounded in deep learning frequently disappoint in terms oftransparency and interpretability. Recently, the availability of large-scalespatio-temporal data and the development of large language models (LLMs) haveopened up new opportunities for urban traffic prediction. With the popularityof LLMs, people witnessed the potential reasoning and generating ability offoundation models in various tasks. Considering text as input and output, LLMshave advantages in generating more intuitive and interpretable predictions.Hence, this work introduces TP-LLM, an explainable foundation-model-basedmethod for traffic prediction, aiming at more direct and reasonableforecasting. TP-LLM presents a framework to unify multi-modality factors aslanguage-based inputs, TP-LLM avoids complex spatial-temporal data programmingand outperforms state-of-art baselines merely under fine-tuning foundationmodels. Also, TP-LLM can generate input-dependency explanations for moreconfident prediction and can be easily generalized to different city dynamicsfor zero-shot prediction with a similar framework. These findings demonstratethe potential of LLMs for explainable traffic prediction.</description><author>Xusen Guo, Qiming Zhang, Mingxing Peng, Meixin Zhua, Hao, Yang</author><pubDate>Wed, 03 Apr 2024 08:14:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02937v1</guid></item><item><title>LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding</title><link>http://arxiv.org/abs/2402.16050v1</link><description>Despite progress in video-language modeling, the computational challenge ofinterpreting long-form videos in response to task-specific linguistic queriespersists, largely due to the complexity of high-dimensional video data and themisalignment between language and visual cues over space and time. To tacklethis issue, we introduce a novel approach called Language-guidedSpatial-Temporal Prompt Learning (LSTP). This approach features two keycomponents: a Temporal Prompt Sampler (TPS) with optical flow prior thatleverages temporal information to efficiently extract relevant video content,and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatialrelationships between visual and textual elements. By harmonizing TPS and SPSwith a cohesive training strategy, our framework significantly enhancescomputational efficiency, temporal understanding, and spatial-temporalalignment. Empirical evaluations across two challenging tasks--video questionanswering and temporal question grounding in videos--using a variety ofvideo-language pretrainings (VLPs) and large language models (LLMs) demonstratethe superior performance, speed, and versatility of our proposed LSTP paradigm.</description><author>Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Zilong Zheng</author><pubDate>Sun, 25 Feb 2024 10:27:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16050v1</guid></item><item><title>GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding</title><link>http://arxiv.org/abs/2402.06764v2</link><description>Integrating large language models (LLMs) with knowledge graphs derived fromdomain-specific data represents an important advancement towards more powerfuland factual reasoning. As these models grow more capable, it is crucial toenable them to perform multi-step inferences over real-world knowledge graphswhile minimizing hallucination. While large language models excel atconversation and text generation, their ability to reason overdomain-specialized graphs of interconnected entities remains limited. Forexample, can we query a LLM to identify the optimal contact in a professionalnetwork for a specific goal, based on relationships and attributes in a privatedatabase? The answer is no--such capabilities lie beyond current methods.However, this question underscores a critical technical gap that must beaddressed. Many high-value applications in areas such as science, security, ande-commerce rely on proprietary knowledge graphs encoding unique structures,relationships, and logical constraints. We introduce a fine-tuning frameworkfor developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledgegraph into an alternate text representation with labeled question-answer pairs.We demonstrate that grounding the models in specific graph-based knowledgeexpands the models' capacity for structure-based reasoning. Our methodologyleverages the large-language model's generative capabilities to create thedataset and proposes an efficient alternate to retrieval-augmented generationstyled methods.</description><author>Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, Sutanay Choudhury</author><pubDate>Fri, 16 Feb 2024 17:23:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06764v2</guid></item><item><title>RelationVLM: Making Large Vision-Language Models Understand Visual Relations</title><link>http://arxiv.org/abs/2403.12801v1</link><description>The development of Large Vision-Language Models (LVLMs) is striving to catchup with the success of Large Language Models (LLMs), yet it faces morechallenges to be resolved. Very recent works enable LVLMs to localizeobject-level visual contents and ground text to them. Nonetheless, currentLVLMs still struggle to precisely understand visual relations due to the lackof relevant data. In this work, we present RelationVLM, a large vision-languagemodel capable of comprehending various levels and types of relations whetheracross multiple images or within a video. Specifically, we devise a multi-stagerelation-aware training scheme and a series of corresponding data configurationstrategies to bestow RelationVLM with the capabilities of understandingsemantic relations, temporal associations and geometric transforms. Extensivecase studies and quantitative evaluations show RelationVLM has strongcapability in understanding such relations and emerges impressive in-contextcapability of reasoning from few-shot examples by comparison. This work fostersthe advancements of LVLMs by enabling them to support a wider range ofdownstream applications toward artificial general intelligence.</description><author>Zhipeng Huang, Zhizheng Zhang, Zheng-Jun Zha, Yan Lu, Baining Guo</author><pubDate>Tue, 19 Mar 2024 16:01:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12801v1</guid></item><item><title>Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving</title><link>http://arxiv.org/abs/2402.13602v1</link><description>Large Language Models (LLMs) have garnered significant attention for theirability to understand text and images, generate human-like text, and performcomplex reasoning tasks. However, their ability to generalize this advancedreasoning with a combination of natural language text for decision-making indynamic situations requires further exploration. In this study, we investigatehow well LLMs can adapt and apply a combination of arithmetic and common-sensereasoning, particularly in autonomous driving scenarios. We hypothesize thatLLMs hybrid reasoning abilities can improve autonomous driving by enabling themto analyze detected object and sensor data, understand driving regulations andphysical laws, and offer additional context. This addresses complex scenarios,like decisions in low visibility (due to weather conditions), where traditionalmethods might fall short. We evaluated Large Language Models (LLMs) based onaccuracy by comparing their answers with human-generated ground truth insideCARLA. The results showed that when a combination of images (detected objects)and sensor data is fed into the LLM, it can offer precise information for brakeand throttle control in autonomous vehicles across various weather conditions.This formulation and answers can assist in decision-making for auto-pilotsystems.</description><author>Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg</author><pubDate>Wed, 21 Feb 2024 08:09:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13602v1</guid></item><item><title>Grounded Question-Answering in Long Egocentric Videos</title><link>http://arxiv.org/abs/2312.06505v3</link><description>Existing approaches to video understanding, mainly designed for short videosfrom a third-person perspective, are limited in their applicability in certainfields, such as robotics. In this paper, we delve into open-endedquestion-answering (QA) in long, egocentric videos, which allows individuals orrobots to inquire about their own past visual experiences. This task presentsunique challenges, including the complexity of temporally grounding querieswithin extensive video content, the high resource demands for precise dataannotation, and the inherent difficulty of evaluating open-ended answers due totheir ambiguous nature. Our proposed approach tackles these challenges by (i)integrating query grounding and answering within a unified model to reduceerror propagation; (ii) employing large language models for efficient andscalable data synthesis; and (iii) introducing a close-ended QA task forevaluation, to manage answer ambiguity. Extensive experiments demonstrate theeffectiveness of our method, which also achieves state-of-the-art performanceon the QAEgo4D and Ego4D-NLQ benchmarks. Code, data, and models are availableat https://github.com/Becomebright/GroundVQA.</description><author>Shangzhe Di, Weidi Xie</author><pubDate>Thu, 15 Feb 2024 15:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06505v3</guid></item><item><title>DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</title><link>http://arxiv.org/abs/2310.02954v5</link><description>Recent advances in natural language processing, primarily propelled by LargeLanguage Models (LLMs), have showcased their remarkable capabilities groundedin in-context learning. A promising avenue for guiding LLMs in intricatereasoning tasks involves the utilization of intermediate reasoning steps withinthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge liesin the effective selection of exemplars for facilitating in-context learning.In this study, we introduce a framework that leverages Dual Queries andLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplarsfor in-context learning. Dual Queries first query LLM to obtain LLM-generatedknowledge such as CoT, then query the retriever to obtain the final exemplarsvia both question and the knowledge. Moreover, for the second query, LoReemploys dimensionality reduction techniques to refine exemplar selection,ensuring close alignment with the input question's knowledge. Through extensiveexperiments, we demonstrate that DQ-LoRe significantly outperforms priorstate-of-the-art methods in the automatic selection of exemplars for GPT-4,enhancing performance from 92.5% to 94.2%. Our comprehensive analysis furtherreveals that DQ-LoRe consistently outperforms retrieval-based approaches interms of both performance and adaptability, especially in scenarioscharacterized by distribution shifts. DQ-LoRe pushes the boundary of in-contextlearning and opens up new avenues for addressing complex reasoning challenges.Our code is released athttps://github.com/AI4fun/DQ-LoRe}{https://github.com/AI4fun/DQ-LoRe.</description><author>Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang</author><pubDate>Sat, 02 Mar 2024 14:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02954v5</guid></item><item><title>Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving</title><link>http://arxiv.org/abs/2402.13602v2</link><description>Large Language Models (LLMs) have garnered significant attention for theirability to understand text and images, generate human-like text, and performcomplex reasoning tasks. However, their ability to generalize this advancedreasoning with a combination of natural language text for decision-making indynamic situations requires further exploration. In this study, we investigatehow well LLMs can adapt and apply a combination of arithmetic and common-sensereasoning, particularly in autonomous driving scenarios. We hypothesize thatLLMs hybrid reasoning abilities can improve autonomous driving by enabling themto analyze detected object and sensor data, understand driving regulations andphysical laws, and offer additional context. This addresses complex scenarios,like decisions in low visibility (due to weather conditions), where traditionalmethods might fall short. We evaluated Large Language Models (LLMs) based onaccuracy by comparing their answers with human-generated ground truth insideCARLA. The results showed that when a combination of images (detected objects)and sensor data is fed into the LLM, it can offer precise information for brakeand throttle control in autonomous vehicles across various weather conditions.This formulation and answers can assist in decision-making for auto-pilotsystems.</description><author>Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg</author><pubDate>Thu, 07 Mar 2024 12:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13602v2</guid></item><item><title>Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving</title><link>http://arxiv.org/abs/2402.13602v3</link><description>Large Language Models (LLMs) have garnered significant attention for theirability to understand text and images, generate human-like text, and performcomplex reasoning tasks. However, their ability to generalize this advancedreasoning with a combination of natural language text for decision-making indynamic situations requires further exploration. In this study, we investigatehow well LLMs can adapt and apply a combination of arithmetic and common-sensereasoning, particularly in autonomous driving scenarios. We hypothesize thatLLMs hybrid reasoning abilities can improve autonomous driving by enabling themto analyze detected object and sensor data, understand driving regulations andphysical laws, and offer additional context. This addresses complex scenarios,like decisions in low visibility (due to weather conditions), where traditionalmethods might fall short. We evaluated Large Language Models (LLMs) based onaccuracy by comparing their answers with human-generated ground truth insideCARLA. The results showed that when a combination of images (detected objects)and sensor data is fed into the LLM, it can offer precise information for brakeand throttle control in autonomous vehicles across various weather conditions.This formulation and answers can assist in decision-making for auto-pilotsystems.</description><author>Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg</author><pubDate>Mon, 18 Mar 2024 10:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13602v3</guid></item><item><title>VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning</title><link>http://arxiv.org/abs/2403.13164v1</link><description>Large language models (LLMs) famously exhibit emergent in-context learning(ICL) -- the ability to rapidly adapt to new tasks using few-shot examplesprovided as a prompt, without updating the model's weights. Built on top ofLLMs, vision large language models (VLLMs) have advanced significantly in areassuch as recognition, reasoning, and grounding. However, investigations into\emph{multimodal ICL} have predominantly focused on few-shot visual questionanswering (VQA), and image captioning, which we will show neither exploit thestrengths of ICL, nor test its limitations. The broader capabilities andlimitations of multimodal ICL remain under-explored. In this study, weintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-contextlearning, encompassing a broad spectrum of tasks that involve both images andtext as inputs and outputs, and different types of challenges, from {perceptionto reasoning and long context length}. We evaluate the abilities ofstate-of-the-art VLLMs against this benchmark suite, revealing their diversestrengths and weaknesses, and showing that even the most advanced models, suchas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,and the associated strengths and limitations of existing models, we hope thatour dataset will inspire future work on enhancing the in-context learningcapabilities of VLLMs, as well as inspire new applications that leverage VLLMICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.</description><author>Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales</author><pubDate>Tue, 19 Mar 2024 22:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13164v1</guid></item><item><title>Grounded Question-Answering in Long Egocentric Videos</title><link>http://arxiv.org/abs/2312.06505v4</link><description>Existing approaches to video understanding, mainly designed for short videosfrom a third-person perspective, are limited in their applicability in certainfields, such as robotics. In this paper, we delve into open-endedquestion-answering (QA) in long, egocentric videos, which allows individuals orrobots to inquire about their own past visual experiences. This task presentsunique challenges, including the complexity of temporally grounding querieswithin extensive video content, the high resource demands for precise dataannotation, and the inherent difficulty of evaluating open-ended answers due totheir ambiguous nature. Our proposed approach tackles these challenges by (i)integrating query grounding and answering within a unified model to reduceerror propagation; (ii) employing large language models for efficient andscalable data synthesis; and (iii) introducing a close-ended QA task forevaluation, to manage answer ambiguity. Extensive experiments demonstrate theeffectiveness of our method, which also achieves state-of-the-art performanceon the QaEgo4D and Ego4D-NLQ benchmarks. Code, data, and models are availableat https://github.com/Becomebright/GroundVQA.</description><author>Shangzhe Di, Weidi Xie</author><pubDate>Mon, 01 Apr 2024 14:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06505v4</guid></item><item><title>Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark</title><link>http://arxiv.org/abs/2402.14359v1</link><description>The summarization capabilities of pretrained and large language models (LLMs)have been widely validated in general areas, but their use in scientificcorpus, which involves complex sentences and specialized knowledge, has beenless assessed. This paper presents conceptual and experimental analyses ofscientific summarization, highlighting the inadequacies of traditionalevaluation methods, such as $n$-gram, embedding comparison, and QA,particularly in providing explanations, grasping scientific concepts, oridentifying key content. Subsequently, we introduce the Facet-aware Metric(FM), employing LLMs for advanced semantic matching to evaluate summaries basedon different aspects. This facet-aware approach offers a thorough evaluation ofabstracts by decomposing the evaluation task into simpler subtasks.Recognizingthe absence of an evaluation benchmark in this domain, we curate a Facet-basedscientific summarization Dataset (FD) with facet-level annotations. Ourfindings confirm that FM offers a more logical approach to evaluatingscientific summaries. In addition, fine-tuned smaller models can compete withLLMs in scientific contexts, while LLMs have limitations in learning fromin-context information in scientific domains. This suggests an area for futureenhancement of LLMs.</description><author>Xiuying Chen, Tairan Wang, Qingqing Zhu, Taicheng Guo, Shen Gao, Zhiyong Lu, Xin Gao, Xiangliang Zhang</author><pubDate>Thu, 22 Feb 2024 07:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14359v1</guid></item><item><title>Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling</title><link>http://arxiv.org/abs/2402.19471v1</link><description>Questions combine our mastery of language with our remarkable facility forreasoning about uncertainty. How do people navigate vast hypothesis spaces topose informative questions given limited cognitive resources? We study thesetradeoffs in a classic grounded question-asking task based on the board gameBattleship. Our language-informed program sampling (LIPS) model uses largelanguage models (LLMs) to generate natural language questions, translate theminto symbolic programs, and evaluate their expected information gain. We findthat with a surprisingly modest resource budget, this simple Monte Carlooptimization strategy yields informative questions that mirror humanperformance across varied Battleship board scenarios. In contrast, LLM-onlybaselines struggle to ground questions in the board state; notably, GPT-4Vprovides no improvement over non-visual baselines. Our results illustrate howBayesian models of question-asking can leverage the statistics of language tocapture human priors, while highlighting some shortcomings of pure LLMs asgrounded reasoners.</description><author>Gabriel Grand, Valerio Pepe, Jacob Andreas, Joshua B. Tenenbaum</author><pubDate>Thu, 29 Feb 2024 18:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19471v1</guid></item><item><title>ALOHa: A New Measure for Hallucination in Captioning Models</title><link>http://arxiv.org/abs/2404.02904v1</link><description>Despite recent advances in multimodal pre-training for visual description,state-of-the-art models still produce captions containing errors, such ashallucinating objects not present in a scene. The existing prominent metric forobject hallucination, CHAIR, is limited to a fixed set of MS COCO objects andsynonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,which leverages large language models (LLMs) to measure object hallucinations.Specifically, we use an LLM to extract groundable objects from a candidatecaption, measure their semantic similarity to reference objects from captionsand object detections, and use Hungarian matching to produce a finalhallucination score. We show that ALOHa correctly identifies 13.6% morehallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCOCaptions annotated for hallucinations, and 30.8% more on nocaps, where objectsextend beyond MS COCO categories. Our code is available athttps://davidmchan.github.io/aloha/.</description><author>Suzanne Petryk, David M. Chan, Anish Kachinthaya, Haodi Zou, John Canny, Joseph E. Gonzalez, Trevor Darrell</author><pubDate>Wed, 03 Apr 2024 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02904v1</guid></item><item><title>InMD-X: Large Language Models for Internal Medicine Doctors</title><link>http://arxiv.org/abs/2402.11883v2</link><description>In this paper, we introduce InMD-X, a collection of multiple large languagemodels specifically designed to cater to the unique characteristics and demandsof Internal Medicine Doctors (IMD). InMD-X represents a groundbreakingdevelopment in natural language processing, offering a suite of language modelsfine-tuned for various aspects of the internal medicine field. These modelsencompass a wide range of medical sub-specialties, enabling IMDs to performmore efficient and accurate research, diagnosis, and documentation. InMD-X'sversatility and adaptability make it a valuable tool for improving thehealthcare industry, enhancing communication between healthcare professionals,and advancing medical research. Each model within InMD-X is meticulouslytailored to address specific challenges faced by IMDs, ensuring the highestlevel of precision and comprehensiveness in clinical text analysis and decisionsupport. This paper provides an overview of the design, development, andevaluation of InMD-X, showcasing its potential to revolutionize the wayinternal medicine practitioners interact with medical data and information. Wepresent results from extensive testing, demonstrating the effectiveness andpractical utility of InMD-X in real-world medical scenarios.</description><author>Hansle Gwon, Imjin Ahn, Hyoje Jung, Byeolhee Kim, Young-Hak Kim, Tae Joon Jun</author><pubDate>Tue, 20 Feb 2024 02:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11883v2</guid></item><item><title>InMD-X: Large Language Models for Internal Medicine Doctors</title><link>http://arxiv.org/abs/2402.11883v1</link><description>In this paper, we introduce InMD-X, a collection of multiple large languagemodels specifically designed to cater to the unique characteristics and demandsof Internal Medicine Doctors (IMD). InMD-X represents a groundbreakingdevelopment in natural language processing, offering a suite of language modelsfine-tuned for various aspects of the internal medicine field. These modelsencompass a wide range of medical sub-specialties, enabling IMDs to performmore efficient and accurate research, diagnosis, and documentation. InMD-X'sversatility and adaptability make it a valuable tool for improving thehealthcare industry, enhancing communication between healthcare professionals,and advancing medical research. Each model within InMD-X is meticulouslytailored to address specific challenges faced by IMDs, ensuring the highestlevel of precision and comprehensiveness in clinical text analysis and decisionsupport. This paper provides an overview of the design, development, andevaluation of InMD-X, showcasing its potential to revolutionize the wayinternal medicine practitioners interact with medical data and information. Wepresent results from extensive testing, demonstrating the effectiveness andpractical utility of InMD-X in real-world medical scenarios.</description><author>Hansle Gwon, Imjin Ahn, Hyoje Jung, Byeolhee Kim, Young-Hak Kim, Tae Joon Jun</author><pubDate>Mon, 19 Feb 2024 06:46:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11883v1</guid></item><item><title>Grounding Language Models for Visual Entity Recognition</title><link>http://arxiv.org/abs/2402.18695v1</link><description>We introduce AutoVER, an Autoregressive model for Visual Entity Recognition.Our model extends an autoregressive Multi-modal Large Language Model byemploying retrieval augmented constrained generation. It mitigates lowperformance on out-of-domain entities while excelling in queries that requirevisually-situated reasoning. Our method learns to distinguish similar entitieswithin a vast label space by contrastively training on hard negative pairs inparallel with a sequence-to-sequence objective without an external retriever.During inference, a list of retrieved candidate answers explicitly guideslanguage generation by removing invalid decoding paths. The proposed methodachieves significant improvements across different dataset splits in therecently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split risesfrom 32.7% to 61.5%. It also demonstrates superior performance on the unseenand query splits by a substantial double-digit margin.</description><author>Zilin Xiao, Ming Gong, Paola Cascante-Bonilla, Xingyao Zhang, Jie Wu, Vicente Ordonez</author><pubDate>Wed, 28 Feb 2024 20:22:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18695v1</guid></item><item><title>Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models</title><link>http://arxiv.org/abs/2310.05861v2</link><description>An increasing number of vision-language tasks can be handled with little tono training, i.e., in a zero and few-shot manner, by marrying large languagemodels (LLMs) to vision encoders, resulting in large vision-language models(LVLMs). While this has huge upsides, such as not requiring training data orcustom architectures, how an input is presented to an LVLM can have a majorimpact on zero-shot model performance. In particular, inputs phrased in anunderspecified way can result in incorrect answers due to factors like missingvisual information, complex implicit reasoning, or linguistic ambiguity.Therefore, adding visually-grounded information to the input as a preemptiveclarification should improve model performance by reducing underspecification,e.g., by localizing objects and disambiguating references. Similarly, in theVQA setting, changing the way questions are framed can make them easier formodels to answer. To this end, we present Rephrase, Augment and Reason(RepARe), a gradient-free framework that extracts salient details about theimage using the underlying LVLM as a captioner and reasoner, in order topropose modifications to the original question. We then use the LVLM'sconfidence over a generated answer as an unsupervised scoring function toselect the rephrased question most likely to improve zero-shot performance.Focusing on three visual question answering tasks, we show that RepARe canresult in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%,and 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, wefind that using gold answers for oracle question candidate selection achieves asubstantial gain in VQA accuracy by up to 14.41%. Through extensive analysis,we demonstrate that outputs from RepARe increase syntactic complexity, andeffectively utilize vision-language interaction and the frozen LLM.</description><author>Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</author><pubDate>Tue, 02 Apr 2024 18:37:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05861v2</guid></item><item><title>A novel approach to measuring the scope of patent claims based on probabilities obtained from (large) language models</title><link>http://arxiv.org/abs/2309.10003v4</link><description>This work proposes to measure the scope of a patent claim as the reciprocalof self-information contained in this claim. Self-information is calculatedbased on a probability of occurrence of the claim, where this probability isobtained from a language model. Grounded in information theory, this approachis based on the assumption that an unlikely concept is more informative than ausual concept, insofar as it is more surprising. In turn, the more surprisingthe information required to define the claim, the narrower its scope. Sevenlanguage models are considered, ranging from simplest models (each word orcharacter has an identical probability) to intermediate models (based onaverage word or character frequencies), to large language models (LLMs) such asGPT2 and davinci-002. Remarkably, when using the simplest language models tocompute the probabilities, the scope becomes proportional to the reciprocal ofthe number of words or characters involved in the claim, a metric already usedin previous works. Application is made to multiple series of patent claimsdirected to distinct inventions, where each series consists of claims devisedto have a gradually decreasing scope. The performance of the language models isthen assessed through several ad hoc tests. The LLMs outperform models based onword and character frequencies, which themselves outdo the simplest modelsbased on word or character counts. Interestingly, however, the character countappears to be a more reliable indicator than the word count.</description><author>Sébastien Ragot</author><pubDate>Mon, 15 Apr 2024 19:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10003v4</guid></item><item><title>Unified Scene Representation and Reconstruction for 3D Large Language Models</title><link>http://arxiv.org/abs/2404.13044v1</link><description>Enabling Large Language Models (LLMs) to interact with 3D environments ischallenging. Existing approaches extract point clouds either from ground truth(GT) geometry or 3D scenes reconstructed by auxiliary models. Text-imagealigned 2D features from CLIP are then lifted to point clouds, which serve asinputs for LLMs. However, this solution lacks the establishment of 3Dpoint-to-point connections, leading to a deficiency of spatial structureinformation. Concurrently, the absence of integration and unification betweenthe geometric and semantic representations of the scene culminates in adiminished level of 3D scene understanding. In this paper, we demonstrate theimportance of having a unified scene representation and reconstructionframework, which is essential for LLMs in 3D scenes. Specifically, we introduceUni3DR^2 extracts 3D geometric and semantic aware representation features viathe frozen pre-trained 2D foundation models (e.g., CLIP and SAM) and amulti-scale aggregate 3D decoder. Our learned 3D representations not onlycontribute to the reconstruction process but also provide valuable knowledgefor LLMs. Experimental results validate that our Uni3DR^2 yields convincinggains over the baseline on the 3D reconstruction dataset ScanNet (increasingF-Score by +1.8\%). When applied to LLMs, our Uni3DR^2-LLM exhibits superiorperformance over the baseline on the 3D vision-language understanding datasetScanQA (increasing BLEU-1 by +4.0\% and +4.2\% on the val set and test set,respectively). Furthermore, it outperforms the state-of-the-art method thatuses additional GT point clouds on both ScanQA and 3DMV-VQA.</description><author>Tao Chu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Qiong Liu, Jiaqi Wang</author><pubDate>Fri, 19 Apr 2024 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13044v1</guid></item><item><title>LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model</title><link>http://arxiv.org/abs/2403.07581v1</link><description>Personality detection aims to detect one's personality traits underlying insocial media posts. One challenge of this task is the scarcity of ground-truthpersonality traits which are collected from self-report questionnaires. Mostexisting methods learn post features directly by fine-tuning the pre-trainedlanguage models under the supervision of limited personality labels. This leadsto inferior quality of post features and consequently affects the performance.In addition, they treat personality traits as one-hot classification labels,overlooking the semantic information within them. In this paper, we propose alarge language model (LLM) based text augmentation enhanced personalitydetection model, which distills the LLM's knowledge to enhance the small modelfor personality detection, even when the LLM fails in this task. Specifically,we enable LLM to generate post analyses (augmentations) from the aspects ofsemantic, sentiment, and linguistic, which are critical for personalitydetection. By using contrastive learning to pull them together in the embeddingspace, the post encoder can better capture the psycho-linguistic informationwithin the post representations, thus improving personality detection.Furthermore, we utilize the LLM to enrich the information of personality labelsfor enhancing the detection performance. Experimental results on the benchmarkdatasets demonstrate that our model outperforms the state-of-the-art methods onpersonality detection.</description><author>Linmei Hu, Hongyu He, Duokang Wang, Ziwang Zhao, Yingxia Shao, Liqiang Nie</author><pubDate>Tue, 12 Mar 2024 13:10:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07581v1</guid></item><item><title>Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph</title><link>http://arxiv.org/abs/2402.14424v1</link><description>Leveraging the synergy between causal knowledge graphs and a large languagemodel (LLM), our study introduces a groundbreaking approach for computationalhypothesis generation in psychology. We analyzed 43,312 psychology articlesusing a LLM to extract causal relation pairs. This analysis produced aspecialized causal graph for psychology. Applying link prediction algorithms,we generated 130 potential psychological hypotheses focusing on `well-being',then compared them against research ideas conceived by doctoral scholars andthose produced solely by the LLM. Interestingly, our combined approach of a LLMand causal graphs mirrored the expert-level insights in terms of novelty,clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) =4.32, p&lt;0.001, respectively). This alignment was further corroborated usingdeep semantic analysis. Our results show that combining LLM with machinelearning techniques such as causal knowledge graphs can revolutionize automateddiscovery in psychology, extracting novel insights from the extensiveliterature. This work stands at the crossroads of psychology and artificialintelligence, championing a new enriched paradigm for data-driven hypothesisgeneration in psychological research.</description><author>Song Tong, Kai Mao, Zhen Huang, Yukun Zhao, Kaiping Peng</author><pubDate>Thu, 22 Feb 2024 10:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14424v1</guid></item><item><title>Cognitive Architectures for Language Agents</title><link>http://arxiv.org/abs/2309.02427v3</link><description>Recent efforts have augmented large language models (LLMs) with externalresources (e.g., the Internet) or internal control flows (e.g., promptchaining) for tasks requiring grounding or reasoning, leading to a new class oflanguage agents. While these agents have achieved substantial empiricalsuccess, we lack a systematic framework to organize existing agents and planfuture developments. In this paper, we draw on the rich history of cognitivescience and symbolic artificial intelligence to propose Cognitive Architecturesfor Language Agents (CoALA). CoALA describes a language agent with modularmemory components, a structured action space to interact with internal memoryand external environments, and a generalized decision-making process to chooseactions. We use CoALA to retrospectively survey and organize a large body ofrecent work, and prospectively identify actionable directions towards morecapable agents. Taken together, CoALA contextualizes today's language agentswithin the broader history of AI and outlines a path towards language-basedgeneral intelligence.</description><author>Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths</author><pubDate>Fri, 15 Mar 2024 16:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02427v3</guid></item><item><title>Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph</title><link>http://arxiv.org/abs/2402.14424v2</link><description>Leveraging the synergy between causal knowledge graphs and a large languagemodel (LLM), our study introduces a groundbreaking approach for computationalhypothesis generation in psychology. We analyzed 43,312 psychology articlesusing a LLM to extract causal relation pairs. This analysis produced aspecialized causal graph for psychology. Applying link prediction algorithms,we generated 130 potential psychological hypotheses focusing on `well-being',then compared them against research ideas conceived by doctoral scholars andthose produced solely by the LLM. Interestingly, our combined approach of a LLMand causal graphs mirrored the expert-level insights in terms of novelty,clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) =4.32, p&lt;0.001, respectively). This alignment was further corroborated usingdeep semantic analysis. Our results show that combining LLM with machinelearning techniques such as causal knowledge graphs can revolutionize automateddiscovery in psychology, extracting novel insights from the extensiveliterature. This work stands at the crossroads of psychology and artificialintelligence, championing a new enriched paradigm for data-driven hypothesisgeneration in psychological research.</description><author>Song Tong, Kai Mao, Zhen Huang, Yukun Zhao, Kaiping Peng</author><pubDate>Sun, 17 Mar 2024 05:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14424v2</guid></item><item><title>Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models</title><link>http://arxiv.org/abs/2403.09750v1</link><description>Declarative knowledge and procedural knowledge are two key parts inmeta-cognitive theory, and these two hold significant importance inpre-training and inference of LLMs. However, a comprehensive analysis comparingthese two types of knowledge is lacking, primarily due to challenges indefinition, probing and quantitative assessment. In this paper, we explore froma new perspective by providing ground-truth knowledge for LLMs and evaluatingthe effective score. Through extensive experiments with widely-used datasetsand models, we get conclusions: (1) In most tasks, benefits from declarativeknowledge are greater than those from procedural knowledge. (2) Profits ofprocedural knowledge are larger than declarative knowledge only in reasoningtasks with simple logic. (3) As pre-training progresses and size increases,model ability to utilize both kinds of knowledge significantly improves, but indifferent speed. We do detailed analysis for the findings and this can provideprimary guidance for evaluation and enhancement of large language models.</description><author>Zhuoqun Li, Hongyu Lin, Yaojie Lu, Hao Xiang, Xianpei Han, Le Sun</author><pubDate>Thu, 14 Mar 2024 06:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09750v1</guid></item><item><title>LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation</title><link>http://arxiv.org/abs/2310.10404v6</link><description>Weakly-Supervised Scene Graph Generation (WSSGG) research has recentlyemerged as an alternative to the fully-supervised approach that heavily relieson costly annotations. In this regard, studies on WSSGG have utilized imagecaptions to obtain unlocalized triplets while primarily focusing on groundingthe unlocalized triplets over image regions. However, they have overlooked thetwo issues involved in the triplet formation process from the captions: 1)Semantic over-simplification issue arises when extracting triplets fromcaptions, where fine-grained predicates in captions are undesirably convertedinto coarse-grained predicates, resulting in a long-tailed predicatedistribution, and 2) Low-density scene graph issue arises when aligning thetriplets in the caption with entity/predicate classes of interest, where manytriplets are discarded and not used in training, leading to insufficientsupervision. To tackle the two issues, we propose a new approach, i.e., LargeLanguage Model for weakly-supervised SGG (LLM4SGG), where we mitigate the twoissues by leveraging the LLM's in-depth understanding of language and reasoningability during the extraction of triplets from captions and alignment ofentity/predicate classes with target data. To further engage the LLM in theseprocesses, we adopt the idea of Chain-of-Thought and the in-context few-shotlearning strategy. To validate the effectiveness of LLM4SGG, we conductextensive experiments on Visual Genome and GQA datasets, showing significantimprovements in both Recall@K and mean Recall@K compared to thestate-of-the-art WSSGG methods. A further appeal is that LLM4SGG isdata-efficient, enabling effective model training with a small amount oftraining images.</description><author>Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park</author><pubDate>Thu, 21 Mar 2024 13:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10404v6</guid></item><item><title>Rehearsal: Simulating Conflict to Teach Conflict Resolution</title><link>http://arxiv.org/abs/2309.12309v2</link><description>Interpersonal conflict is an uncomfortable but unavoidable fact of life.Navigating conflict successfully is a skill -- one that can be learned throughdeliberate practice -- but few have access to effective training or feedback.To expand this access, we introduce Rehearsal, a system that allows users torehearse conflicts with a believable simulated interlocutor, explorecounterfactual "what if?" scenarios to identify alternative conversationalpaths, and learn through feedback on how and when to apply specific conflictstrategies. Users can utilize Rehearsal to practice handling a variety ofpredefined conflict scenarios, from office disputes to relationship issues, orthey can choose to create their own setting. To enable Rehearsal, we developIRP prompting, a method of conditioning output of a large language model on theinfluential Interest-Rights-Power (IRP) theory from conflict resolution.Rehearsal uses IRP to generate utterances grounded in conflict resolutiontheory, guiding users towards counterfactual conflict resolution strategiesthat help de-escalate difficult conversations. In a between-subjectsevaluation, 40 participants engaged in an actual conflict with a confederateafter training. Compared to a control group with lecture material covering thesame IRP theory, participants with simulated training from Rehearsalsignificantly improved their performance in the unaided conflict: they reducedtheir use of escalating competitive strategies by an average of 67%, whiledoubling their use of cooperative strategies. Overall, Rehearsal highlights thepotential effectiveness of language models as tools for learning and practicinginterpersonal skills.</description><author>Omar Shaikh, Valentino Chai, Michele J. Gelfand, Diyi Yang, Michael S. Bernstein</author><pubDate>Thu, 29 Feb 2024 06:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12309v2</guid></item><item><title>Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty</title><link>http://arxiv.org/abs/2402.06529v2</link><description>Large language models (LLMs) exhibit advanced reasoning skills, enablingrobots to comprehend natural language instructions and strategically planhigh-level actions through proper grounding. However, LLM hallucination mayresult in robots confidently executing plans that are misaligned with usergoals or, in extreme cases, unsafe. Additionally, inherent ambiguity in naturallanguage instructions can induce task uncertainty, particularly in situationswhere multiple valid options exist. To address this issue, LLMs must identifysuch uncertainty and proactively seek clarification. This paper explores theconcept of introspective planning as a systematic method for guiding LLMs informing uncertainty--aware plans for robotic task execution without the needfor fine-tuning. We investigate uncertainty quantification in task-level robotplanning and demonstrate that introspection significantly improves both successrates and safety compared to state-of-the-art LLM-based planning approaches.Furthermore, we assess the effectiveness of introspective planning inconjunction with conformal prediction, revealing that this combination yieldstighter confidence bounds, thereby maintaining statistical success guaranteeswith fewer superfluous user clarification queries.</description><author>Kaiqu Liang, Zixu Zhang, Jaime Fernández Fisac</author><pubDate>Sun, 18 Feb 2024 07:01:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06529v2</guid></item><item><title>Physically Grounded Vision-Language Models for Robotic Manipulation</title><link>http://arxiv.org/abs/2309.02561v3</link><description>Recent advances in vision-language models (VLMs) have led to improvedperformance on tasks such as visual question answering and image captioning.Consequently, these models are now well-positioned to reason about the physicalworld, particularly within domains such as robotic manipulation. However,current VLMs are limited in their understanding of the physical concepts (e.g.,material, fragility) of common objects, which restricts their usefulness forrobotic manipulation tasks that involve interaction and physical reasoningabout such objects. To address this limitation, we propose PhysObjects, anobject-centric dataset of 39.6K crowd-sourced and 417K automated physicalconcept annotations of common household objects. We demonstrate thatfine-tuning a VLM on PhysObjects improves its understanding of physical objectconcepts, including generalization to held-out concepts, by capturing humanpriors of these concepts from visual appearance. We incorporate this physicallygrounded VLM in an interactive framework with a large language model-basedrobotic planner, and show improved planning performance on tasks that requirereasoning about physical object concepts, compared to baselines that do notleverage physically grounded VLMs. We additionally illustrate the benefits ofour physically grounded VLM on a real robot, where it improves task successrates. We release our dataset and provide further details and visualizations ofour results at https://iliad.stanford.edu/pg-vlm/.</description><author>Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh</author><pubDate>Thu, 29 Feb 2024 08:44:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02561v3</guid></item><item><title>Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception</title><link>http://arxiv.org/abs/2403.02969v1</link><description>Multimodal Large Language Model (MLLMs) leverages Large Language Models as acognitive framework for diverse visual-language tasks. Recent efforts have beenmade to equip MLLMs with visual perceiving and grounding capabilities. However,there still remains a gap in providing fine-grained pixel-level perceptions andextending interactions beyond text-specific inputs. In this work, we propose{\bf{AnyRef}}, a general MLLM model that can generate pixel-wise objectperceptions and natural language descriptions from multi-modality references,such as texts, boxes, images, or audio. This innovation empowers users withgreater flexibility to engage with the model beyond textual and regionalprompts, without modality-specific designs. Through our proposed refocusingmechanism, the generated grounding output is guided to better focus on thereferenced object, implicitly incorporating additional pixel-level supervision.This simple modification utilizes attention scores generated during theinference of LLM, eliminating the need for extra computations while exhibitingperformance enhancements in both grounding masks and referring expressions.With only publicly available training data, our model achieves state-of-the-artresults across multiple benchmarks, including diverse modality referringsegmentation and region-level referring expression generation.</description><author>Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, Xuansong Xie</author><pubDate>Tue, 05 Mar 2024 13:45:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02969v1</guid></item><item><title>Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception</title><link>http://arxiv.org/abs/2403.02969v2</link><description>Multimodal Large Language Model (MLLMs) leverages Large Language Models as acognitive framework for diverse visual-language tasks. Recent efforts have beenmade to equip MLLMs with visual perceiving and grounding capabilities. However,there still remains a gap in providing fine-grained pixel-level perceptions andextending interactions beyond text-specific inputs. In this work, we propose{\bf{AnyRef}}, a general MLLM model that can generate pixel-wise objectperceptions and natural language descriptions from multi-modality references,such as texts, boxes, images, or audio. This innovation empowers users withgreater flexibility to engage with the model beyond textual and regionalprompts, without modality-specific designs. Through our proposed refocusingmechanism, the generated grounding output is guided to better focus on thereferenced object, implicitly incorporating additional pixel-level supervision.This simple modification utilizes attention scores generated during theinference of LLM, eliminating the need for extra computations while exhibitingperformance enhancements in both grounding masks and referring expressions.With only publicly available training data, our model achieves state-of-the-artresults across multiple benchmarks, including diverse modality referringsegmentation and region-level referring expression generation.</description><author>Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, Xuansong Xie</author><pubDate>Mon, 25 Mar 2024 13:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02969v2</guid></item><item><title>BloomVQA: Assessing Hierarchical Multi-modal Comprehension</title><link>http://arxiv.org/abs/2312.12716v2</link><description>We propose a novel VQA dataset, BloomVQA, to facilitate comprehensiveevaluation of large vision-language models on comprehension tasks. Unlikecurrent benchmarks that often focus on fact-based memorization and simplereasoning tasks without theoretical grounding, we collect multiple-choicesamples based on picture stories that reflect different levels ofcomprehension, as laid out in Bloom's Taxonomy, a classic framework forlearning assessment widely adopted in education research. Our data maps to anovel hierarchical graph representation which enables automatic dataaugmentation and novel measures characterizing model consistency. We performgraded evaluation and reliability analysis on recent multi-modal models. Incomparison to low-level tasks, we observe decreased performance on tasksrequiring advanced comprehension and cognitive skills with up to 38.0% drop inVQA accuracy. In comparison to earlier models, GPT-4V demonstrates improvedaccuracy over all comprehension levels while also shows a tendency of bypassingvisual inputs especially for higher-level tasks. Current models also showconsistency patterns misaligned with human comprehension in various scenarios,demonstrating the need of improvement based on theoretically-grounded criteria.</description><author>Yunye Gong, Robik Shrestha, Jared Claypoole, Michael Cogswell, Arijit Ray, Christopher Kanan, Ajay Divakaran</author><pubDate>Fri, 16 Feb 2024 17:33:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12716v2</guid></item><item><title>"We Demand Justice!": Towards Social Context Grounding of Political Texts</title><link>http://arxiv.org/abs/2311.09106v2</link><description>Social media discourse frequently consists of 'seemingly similar languageused by opposing sides of the political spectrum', often translating to starklycontrasting perspectives. E.g., 'thoughts and prayers', could express sympathyfor mass-shooting victims, or criticize the lack of legislative action on theissue. This paper defines the context required to fully understand suchambiguous statements in a computational setting and ground them in real-worldentities, actions, and attitudes. We propose two challenging datasets thatrequire an understanding of the real-world context of the text. We benchmarkthese datasets against models built upon large pre-trained models, such asRoBERTa and GPT-3. Additionally, we develop and benchmark more structuredmodels building upon existing Discourse Contextualization Framework andPolitical Actor Representation models. We analyze the datasets and thepredictions to obtain further insights into the pragmatic languageunderstanding challenges posed by the proposed social grounding tasks.</description><author>Rajkumar Pujari, Chengfei Wu, Dan Goldwasser</author><pubDate>Mon, 26 Feb 2024 09:34:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09106v2</guid></item><item><title>Leveraging Prompt-Based Large Language Models: Predicting Pandemic Health Decisions and Outcomes Through Social Media Language</title><link>http://arxiv.org/abs/2403.00994v1</link><description>We introduce a multi-step reasoning framework using prompt-based LLMs toexamine the relationship between social media language patterns and trends innational health outcomes. Grounded in fuzzy-trace theory, which emphasizes theimportance of gists of causal coherence in effective health communication, weintroduce Role-Based Incremental Coaching (RBIC), a prompt-based LLM framework,to identify gists at-scale. Using RBIC, we systematically extract gists fromsubreddit discussions opposing COVID-19 health measures (Study 1). We thentrack how these gists evolve across key events (Study 2) and assess theirinfluence on online engagement (Study 3). Finally, we investigate how thevolume of gists is associated with national health trends like vaccine uptakeand hospitalizations (Study 4). Our work is the first to empirically linksocial media linguistic patterns to real-world public health trends,highlighting the potential of prompt-based LLMs in identifying critical onlinediscussion patterns that can form the basis of public health communicationstrategies.</description><author>Xiaohan Ding, Buse Carik, Uma Sushmitha Gunturi, Valerie Reyna, Eugenia H. Rho</author><pubDate>Fri, 01 Mar 2024 21:29:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00994v1</guid></item><item><title>Stance Detection on Social Media with Fine-Tuned Large Language Models</title><link>http://arxiv.org/abs/2404.12171v1</link><description>Stance detection, a key task in natural language processing, determines anauthor's viewpoint based on textual analysis. This study evaluates theevolution of stance detection methods, transitioning from early machinelearning approaches to the groundbreaking BERT model, and eventually to modernLarge Language Models (LLMs) such as ChatGPT, LLaMa-2, and Mistral-7B. WhileChatGPT's closed-source nature and associated costs present challenges, theopen-source models like LLaMa-2 and Mistral-7B offers an encouragingalternative. Initially, our research focused on fine-tuning ChatGPT, LLaMa-2,and Mistral-7B using several publicly available datasets. Subsequently, toprovide a comprehensive comparison, we assess the performance of these modelsin zero-shot and few-shot learning scenarios. The results underscore theexceptional ability of LLMs in accurately detecting stance, with all testedmodels surpassing existing benchmarks. Notably, LLaMa-2 and Mistral-7Bdemonstrate remarkable efficiency and potential for stance detection, despitetheir smaller sizes compared to ChatGPT. This study emphasizes the potential ofLLMs in stance detection and calls for more extensive research in this field.</description><author>İlker Gül, Rémi Lebret, Karl Aberer</author><pubDate>Thu, 18 Apr 2024 14:25:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12171v1</guid></item><item><title>TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</title><link>http://arxiv.org/abs/2312.02051v2</link><description>This work proposes TimeChat, a time-sensitive multimodal large language modelspecifically designed for long video understanding. Our model incorporates twokey architectural contributions: (1) a timestamp-aware frame encoder that bindsvisual content with the timestamp of each frame, and (2) a sliding videoQ-Former that produces a video token sequence of varying lengths to accommodatevideos of various durations. Additionally, we construct an instruction-tuningdataset, encompassing 6 tasks and a total of 125K instances, to further enhanceTimeChat's instruction-following performance. Experiment results across variousvideo understanding tasks, such as dense captioning, temporal grounding, andhighlight detection, demonstrate TimeChat's strong zero-shot temporallocalization and reasoning capabilities. For example, it achieves +9.2 F1 scoreand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)on Charades-STA, compared to state-of-the-art video large language models,holding the potential to serve as a versatile video assistant for long-formvideo comprehension tasks and satisfy realistic user requirements.</description><author>Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou</author><pubDate>Thu, 28 Mar 2024 13:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02051v2</guid></item><item><title>Language Models Represent Space and Time</title><link>http://arxiv.org/abs/2310.02207v3</link><description>The capabilities of large language models (LLMs) have sparked debate overwhether such systems just learn an enormous collection of superficialstatistics or a set of more coherent and grounded representations that reflectthe real world. We find evidence for the latter by analyzing the learnedrepresentations of three spatial datasets (world, US, NYC places) and threetemporal datasets (historical figures, artworks, news headlines) in the Llama-2family of models. We discover that LLMs learn linear representations of spaceand time across multiple scales. These representations are robust to promptingvariations and unified across different entity types (e.g. cities andlandmarks). In addition, we identify individual "space neurons" and "timeneurons" that reliably encode spatial and temporal coordinates. While furtherinvestigation is needed, our results suggest modern LLMs learn richspatiotemporal representations of the real world and possess basic ingredientsof a world model.</description><author>Wes Gurnee, Max Tegmark</author><pubDate>Mon, 04 Mar 2024 18:25:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02207v3</guid></item><item><title>Find The Gap: Knowledge Base Reasoning For Visual Question Answering</title><link>http://arxiv.org/abs/2404.10226v1</link><description>We analyze knowledge-based visual question answering, for which given aquestion, the models need to ground it into the visual modality and retrievethe relevant knowledge from a given large knowledge base (KB) to be able toanswer. Our analysis has two folds, one based on designing neural architecturesand training them from scratch, and another based on large pre-trained languagemodels (LLMs). Our research questions are: 1) Can we effectively augment modelsby explicit supervised retrieval of the relevant KB information to solve theKB-VQA problem? 2) How do task-specific and LLM-based models perform in theintegration of visual and external knowledge, and multi-hop reasoning over bothsources of information? 3) Is the implicit knowledge of LLMs sufficient forKB-VQA and to what extent it can replace the explicit KB? Our resultsdemonstrate the positive impact of empowering task-specific and LLM models withsupervised external and visual knowledge retrieval models. Our findings showthat though LLMs are stronger in 1-hop reasoning, they suffer in 2-hopreasoning in comparison with our fine-tuned NN model even if the relevantinformation from both modalities is available to the model. Moreover, weobserved that LLM models outperform the NN model for KB-related questions whichconfirms the effectiveness of implicit knowledge in LLMs however, they do notalleviate the need for external KB.</description><author>Elham J. Barezi, Parisa Kordjamshidi</author><pubDate>Tue, 16 Apr 2024 03:11:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10226v1</guid></item><item><title>Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings</title><link>http://arxiv.org/abs/2309.08591v2</link><description>Large language models (LLMs) are highly adept at question answering andreasoning tasks, but when reasoning in a situational context, humanexpectations vary depending on the relevant cultural common ground. Aslanguages are associated with diverse cultures, LLMs should also beculturally-diverse reasoners. In this paper, we study the ability of a widerange of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs andsayings in a conversational context. Our experiments reveal that: (1) mLLMs"know" limited proverbs and memorizing proverbs does not mean understandingthem within a conversational context; (2) mLLMs struggle to reason withfigurative proverbs and sayings, and when asked to select the wrong answer(instead of asking it to select the correct answer); and (3) there is a"culture gap" in mLLMs when reasoning about proverbs and sayings translatedfrom other languages. We construct and release our evaluation dataset MAPS(MulticultrAl Proverbs and Sayings) for proverb understanding withconversational context for six different languages.</description><author>Chen Cecilia Liu, Fajri Koto, Timothy Baldwin, Iryna Gurevych</author><pubDate>Sat, 30 Mar 2024 18:13:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08591v2</guid></item><item><title>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback</title><link>http://arxiv.org/abs/2312.00849v2</link><description>Multimodal Large Language Models (MLLMs) have recently demonstratedimpressive capabilities in multimodal understanding, reasoning, andinteraction. However, existing MLLMs prevalently suffer from serioushallucination problems, generating text that is not factually grounded inassociated images. The problem makes existing MLLMs untrustworthy and thusimpractical in real-world (especially high-stakes) applications. To address thechallenge, we present RLHF-V, which enhances MLLM trustworthiness via behavioralignment from fine-grained correctional human feedback. Specifically, RLHF-Vcollects human preference in the form of segment-level corrections onhallucinations, and performs dense direct preference optimization over thehuman feedback. Comprehensive experiments on five benchmarks in both automaticand human evaluation show that, RLHF-V can enable substantially moretrustworthy MLLM behaviors with promising data and computation efficiency.Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces thehallucination rate of the base MLLM by 34.8%, outperforming the concurrentLLaVA-RLHF trained on 10k annotated data. The final model achievesstate-of-the-art performance in trustworthiness among open-source MLLMs, andshows better robustness than GPT-4V in preventing hallucinations aroused fromover-generalization. We open-source our code, model, and data athttps://github.com/RLHF-V/RLHF-V.</description><author>Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, Tat-Seng Chua</author><pubDate>Fri, 08 Mar 2024 06:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00849v2</guid></item><item><title>MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion</title><link>http://arxiv.org/abs/2402.12741v1</link><description>Existing text-to-image models still struggle to generate images of multipleobjects, especially in handling their spatial positions, relative sizes,overlapping, and attribute bindings. In this paper, we develop a training-freeMultimodal-LLM agent (MuLan) to address these challenges by progressivemulti-object generation with planning and feedback control, like a humanpainter. MuLan harnesses a large language model (LLM) to decompose a prompt toa sequence of sub-tasks, each generating only one object conditioned onpreviously generated objects by stable diffusion. Unlike existing LLM-groundedmethods, MuLan only produces a high-level plan at the beginning while the exactsize and location of each object are determined by an LLM and attentionguidance upon each sub-task. Moreover, MuLan adopts a vision-language model(VLM) to provide feedback to the image generated in each sub-task and controlthe diffusion model to re-generate the image if it violates the originalprompt. Hence, each model in every step of MuLan only needs to address an easysub-task it is specialized for. We collect 200 prompts containing multi-objectswith spatial relationships and attribute bindings from different benchmarks toevaluate MuLan. The results demonstrate the superiority of MuLan in generatingmultiple objects over baselines. The code is available onhttps://github.com/measure-infinity/mulan-code.</description><author>Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, Tianyi Zhou</author><pubDate>Tue, 20 Feb 2024 06:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12741v1</guid></item><item><title>Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis</title><link>http://arxiv.org/abs/2306.06770v4</link><description>Large language models (LLMs) offer significant promise as a knowledge sourcefor task learning. Prompt engineering has been shown to be effective foreliciting knowledge from an LLM, but alone it is insufficient for acquiringrelevant, situationally grounded knowledge for an embodied agent learning noveltasks. We describe a cognitive-agent approach, STARS, that extends andcomplements prompt engineering, mitigating its limitations and thus enabling anagent to acquire new task knowledge matched to its native languagecapabilities, embodiment, environment, and user preferences. The STARS approachis to increase the response space of LLMs and deploy general strategies,embedded within the autonomous agent, to evaluate, repair, and select amongcandidate responses produced by the LLM. We describe the approach andexperiments that show how an agent, by retrieving and evaluating a breadth ofresponses from the LLM, can achieve 77-94% task completion in one-shot learningwithout user oversight. The approach achieves 100% task completion when humanoversight (such as an indication of preference) is provided. Further, the typeof oversight largely shifts from explicit, natural language instruction tosimple confirmation/discomfirmation of high-quality responses that have beenvetted by the agent before presentation to a user.</description><author>James R. Kirk, Robert E. Wray, Peter Lindes, John E. Laird</author><pubDate>Tue, 20 Feb 2024 14:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06770v4</guid></item><item><title>FACTOID: FACtual enTailment fOr hallucInation Detection</title><link>http://arxiv.org/abs/2403.19113v1</link><description>The widespread adoption of Large Language Models (LLMs) has facilitatednumerous benefits. However, hallucination is a significant concern. Inresponse, Retrieval Augmented Generation (RAG) has emerged as a highlypromising paradigm to improve LLM outputs by grounding them in factualinformation. RAG relies on textual entailment (TE) or similar methods to checkif the text produced by LLMs is supported or contradicted, compared toretrieved documents. This paper argues that conventional TE methods areinadequate for spotting hallucinations in content generated by LLMs. Forinstance, consider a prompt about the 'USA's stance on the Ukraine war''. TheAI-generated text states, ...U.S. President Barack Obama says the U.S. will notput troops in Ukraine...'' However, during the war the U.S. president is JoeBiden which contradicts factual reality. Moreover, current TE systems areunable to accurately annotate the given text and identify the exact portionthat is contradicted. To address this, we introduces a new type of TE called``Factual Entailment (FE).'', aims to detect factual inaccuracies in contentgenerated by LLMs while also highlighting the specific text segment thatcontradicts reality. We present FACTOID (FACTual enTAILment for hallucInationDetection), a benchmark dataset for FE. We propose a multi-task learning (MTL)framework for FE, incorporating state-of-the-art (SoTA) long text embeddingssuch as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. Theproposed MTL architecture for FE achieves an avg. 40\% improvement in accuracyon the FACTOID benchmark compared to SoTA TE methods. As FE automaticallydetects hallucinations, we assessed 15 modern LLMs and ranked them using ourproposed Auto Hallucination Vulnerability Index (HVI_auto). This indexquantifies and offers a comparative scale to evaluate and rank LLMs accordingto their hallucinations.</description><author>Vipula Rawte, S. M Towhidul Islam Tonmoy, Krishnav Rajbangshi, Shravani Nag, Aman Chadha, Amit P. Sheth, Amitava Das</author><pubDate>Thu, 28 Mar 2024 04:09:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19113v1</guid></item><item><title>Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective</title><link>http://arxiv.org/abs/2402.10184v2</link><description>There is a trilemma in reinforcement learning from human feedback (RLHF): theincompatibility between highly diverse contexts, low labeling cost, andreliable alignment performance. Here we aim to mitigate such incompatibilitythrough the design of dataset information structures during reward modeling,and meanwhile propose new methods of analysis that have wider applications,including potentially shedding light on goal misgeneralization. Specifically,we first reexamine the RLHF process and propose a theoretical frameworkportraying it as an autoencoding process over text distributions. Our frameworkformalizes the RLHF objective of ensuring distributional consistency betweenhuman preference and large language model (LLM) behavior. Under this framework,we introduce a new method based on random graph theory, the induced Bayesiannetwork (IBN). It models generalization in the semantic space and enablesempirically grounded analysis of generalization error bounds, aiming to shedlight on reward generalization in RLHF. An insight from our analysis is thesuperiority of the tree-based information structure in reward modeling,compared to chain-based baselines in conventional RLHF methods. We derive thatin complex contexts with limited data, the tree-based reward model (RM) inducesup to $\Theta(\log n/\log\log n)$ times less variance than chain-based RM where$n$ is the dataset size. As validation, we demonstrate that on three NLP tasks,the tree-based RM achieves 65% win rate on average against chain-basedbaselines.</description><author>Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou, Han Yang, Josef Dai, Xuehai Pan, Yaodong Yang</author><pubDate>Sat, 17 Feb 2024 03:26:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10184v2</guid></item><item><title>Development of a Reliable and Accessible Caregiving Language Model (CaLM)</title><link>http://arxiv.org/abs/2403.06857v1</link><description>Unlike professional caregivers, family caregivers often assume this rolewithout formal preparation or training. Because of this, there is an urgentneed to enhance the capacity of family caregivers to provide quality care.Large language models can potentially be used as a foundation technology forsupporting caregivers as educational tools or as adjunct to care. This studyaimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and acaregiving knowledge base, develop an accessible CaLM using a small FM thatrequires fewer computing resources, and evaluate the performance of the modelcompared to a large FM. We developed CaLM using the Retrieval AugmentedGeneration (RAG) framework combined with FM fine-tuning for improving thequality of FM answers by grounding the model on a caregiving knowledge base. Weused two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7Bparameters) and larger FM GPT-3.5 as a benchmark. We developed the caregivingknowledge base by gathering various types of documents from the Internet. Inthis study, we focused on caregivers of individuals with Alzheimer's DiseaseRelated Dementias. We evaluated the models' performance using the benchmarkmetrics commonly used in evaluating language models and their reliability toprovide accurate references with the answers. The RAG framework improved theperformance of all FMs used in this study across all measures. As expected, thelarge FM performed better than small FMs across all metrics. The mostinteresting result is that small fine-tuned FMs with RAG performedsignificantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2small FM performed better than GPT 3.5 (even with RAG) in returning referenceswith the answers. The study shows that reliable and accessible CaLM can bedeveloped by using small FMs with a knowledge base specific to the caregivingdomain.</description><author>Bambang Parmanto, Bayu Aryoyudanta, Wilbert Soekinto, I Made Agus Setiawan, Yuhan Wang, Haomin Hu, Andi Saptono, Yong K. Choi</author><pubDate>Mon, 11 Mar 2024 17:12:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06857v1</guid></item><item><title>Policy Improvement using Language Feedback Models</title><link>http://arxiv.org/abs/2402.07876v2</link><description>We introduce Language Feedback Models (LFMs) that identify desirablebehaviour - actions that help achieve tasks specified in the instruction - forimitation learning in instruction following. To train LFMs, we obtain feedbackfrom Large Language Models (LLMs) on visual trajectories verbalized to languagedescriptions. First, by using LFMs to identify desirable behaviour to imitate,we improve in task-completion rate over strong behavioural cloning baselines onthree distinct language grounding environments (Touchdown, ScienceWorld, andALFWorld). Second, LFMs outperform using LLMs as experts to directly predictactions, when controlling for the number of LLM output tokens. Third, LFMsgeneralize to unseen environments, improving task-completion rate by 3.5-12.0%through one round of adaptation. Finally, LFM can be modified to providehuman-interpretable feedback without performance loss, allowing humanverification of desirable behaviour for imitation learning.</description><author>Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre Côté</author><pubDate>Thu, 15 Feb 2024 17:20:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07876v2</guid></item></channel></rss>