<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model grounding</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 28 Jun 2024 06:00:09 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>GROUNDHOG: Grounding Large Language Models to Holistic Segmentation</title><link>http://arxiv.org/abs/2402.16846v2</link><description>Most multimodal large language models (MLLMs) learn language-to-objectgrounding through causal language modeling where grounded objects are capturedby bounding boxes as sequences of location tokens. This paradigm lackspixel-level representations that are important for fine-grained visualunderstanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLMdeveloped by grounding Large Language Models to holistic segmentation.GROUNDHOG incorporates a masked feature extractor and converts extractedfeatures into visual entity tokens for the MLLM backbone, which then connectsgroundable phrases to unified grounding masks by retrieving and merging theentity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visualinstruction tuning dataset with Multi-Modal Multi-Grained Grounding, byharvesting a collection of segmentation-grounded datasets with richannotations. Our experimental results show that GROUNDHOG achieves superiorperformance on various language grounding tasks without task-specificfine-tuning, and significantly reduces object hallucination. GROUNDHOG alsodemonstrates better grounding towards complex forms of visual input andprovides easy-to-understand diagnosis in failure cases.</description><author>Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, Joyce Chai</author><pubDate>Tue, 16 Apr 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16846v2</guid></item><item><title>SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments</title><link>http://arxiv.org/abs/2309.04077v4</link><description>Semantic reasoning and dynamic planning capabilities are crucial for anautonomous agent to perform complex navigation tasks in unknown environments.It requires a large amount of common-sense knowledge, that humans possess, tosucceed in these tasks. We present SayNav, a new approach that leverages humanknowledge from Large Language Models (LLMs) for efficient generalization tocomplex navigation tasks in unknown large-scale environments. SayNav uses anovel grounding mechanism, that incrementally builds a 3D scene graph of theexplored environment as inputs to LLMs, for generating feasible andcontextually appropriate high-level plans for navigation. The LLM-generatedplan is then executed by a pre-trained low-level planner, that treats eachplanned step as a short-distance point-goal navigation sub-task. SayNavdynamically generates step-by-step instructions during navigation andcontinuously refines future steps based on newly perceived information. Weevaluate SayNav on multi-object navigation (MultiON) task, that requires theagent to utilize a massive amount of human knowledge to efficiently searchmultiple different objects in an unknown environment. We also introduce abenchmark dataset for MultiON task employing ProcTHOR framework that provideslarge photo-realistic indoor environments with variety of objects. SayNavachieves state-of-the-art results and even outperforms an oracle based baselinewith strong ground-truth assumptions by more than 8% in terms of success rate,highlighting its ability to generate dynamic plans for successfully locatingobjects in large-scale new environments. The code, benchmark dataset anddemonstration videos are accessible athttps://www.sri.com/ics/computer-vision/saynav.</description><author>Abhinav Rajvanshi, Karan Sikka, Xiao Lin, Bhoram Lee, Han-Pang Chiu, Alvaro Velasquez</author><pubDate>Wed, 03 Apr 2024 21:53:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.04077v4</guid></item><item><title>Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources</title><link>http://arxiv.org/abs/2305.13269v4</link><description>We present chain-of-knowledge (CoK), a novel framework that augments largelanguage models (LLMs) by dynamically incorporating grounding information fromheterogeneous sources. It results in more factual rationales and reducedhallucination in generation. Specifically, CoK consists of three stages:reasoning preparation, dynamic knowledge adapting, and answer consolidation.Given a knowledge-intensive question, CoK first prepares several preliminaryrationales and answers while identifying the relevant knowledge domains. Ifthere is no majority consensus among the answers from samples, CoK corrects therationales step by step by adapting knowledge from the identified domains.These corrected rationales can plausibly serve as a better foundation for thefinal answer consolidation. Unlike prior studies that primarily useunstructured data, CoK also leverages structured knowledge sources such asWikidata and tables that provide more reliable factual information. To accessboth unstructured and structured knowledge sources in the dynamic knowledgeadapting stage, we propose an adaptive query generator that allows thegeneration of queries for various types of query languages, including SPARQL,SQL, and natural sentences. Moreover, to minimize error propagation betweenrationales, CoK corrects the rationales progressively using preceding correctedrationales to generate and correct subsequent rationales. Extensive experimentsshow that CoK consistently improves the performance of LLMs onknowledge-intensive tasks across different domains.</description><author>Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, Lidong Bing</author><pubDate>Wed, 21 Feb 2024 07:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13269v4</guid></item><item><title>Ranking Large Language Models without Ground Truth</title><link>http://arxiv.org/abs/2402.14860v2</link><description>Evaluation and ranking of large language models (LLMs) has become animportant problem with the proliferation of these models and their impact.Evaluation methods either require human responses which are expensive toacquire or use pairs of LLMs to evaluate each other which can be unreliable. Inthis paper, we provide a novel perspective where, given a dataset of prompts(viz. questions, instructions, etc.) and a set of LLMs, we rank them withoutaccess to any ground truth or reference responses. Inspired by real life whereboth an expert and a knowledgeable person can identify a novice our main ideais to consider triplets of models, where each one of them evaluates the othertwo, correctly identifying the worst model in the triplet with highprobability. We also analyze our idea and provide sufficient conditions for itto succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.In experiments on different generative tasks (summarization, multiple-choice,and dialog), our methods reliably recover close to true rankings withoutreference data. This points to a viable low-resource mechanism for practicaluse.</description><author>Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan Ramamurthy</author><pubDate>Wed, 06 Mar 2024 20:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14860v2</guid></item><item><title>Ranking Large Language Models without Ground Truth</title><link>http://arxiv.org/abs/2402.14860v3</link><description>Evaluation and ranking of large language models (LLMs) has become animportant problem with the proliferation of these models and their impact.Evaluation methods either require human responses which are expensive toacquire or use pairs of LLMs to evaluate each other which can be unreliable. Inthis paper, we provide a novel perspective where, given a dataset of prompts(viz. questions, instructions, etc.) and a set of LLMs, we rank them withoutaccess to any ground truth or reference responses. Inspired by real life whereboth an expert and a knowledgeable person can identify a novice our main ideais to consider triplets of models, where each one of them evaluates the othertwo, correctly identifying the worst model in the triplet with highprobability. We also analyze our idea and provide sufficient conditions for itto succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.In experiments on different generative tasks (summarization, multiple-choice,and dialog), our methods reliably recover close to true rankings withoutreference data. This points to a viable low-resource mechanism for practicaluse.</description><author>Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan Ramamurthy</author><pubDate>Wed, 05 Jun 2024 16:56:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14860v3</guid></item><item><title>Ranking Large Language Models without Ground Truth</title><link>http://arxiv.org/abs/2402.14860v4</link><description>Evaluation and ranking of large language models (LLMs) has become animportant problem with the proliferation of these models and their impact.Evaluation methods either require human responses which are expensive toacquire or use pairs of LLMs to evaluate each other which can be unreliable. Inthis paper, we provide a novel perspective where, given a dataset of prompts(viz. questions, instructions, etc.) and a set of LLMs, we rank them withoutaccess to any ground truth or reference responses. Inspired by real life whereboth an expert and a knowledgeable person can identify a novice our main ideais to consider triplets of models, where each one of them evaluates the othertwo, correctly identifying the worst model in the triplet with highprobability. We also analyze our idea and provide sufficient conditions for itto succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.In experiments on different generative tasks (summarization, multiple-choice,and dialog), our methods reliably recover close to true rankings withoutreference data. This points to a viable low-resource mechanism for practicaluse.</description><author>Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan Ramamurthy</author><pubDate>Mon, 10 Jun 2024 17:25:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14860v4</guid></item><item><title>A Refer-and-Ground Multimodal Large Language Model for Biomedicine</title><link>http://arxiv.org/abs/2406.18146v1</link><description>With the rapid development of multimodal large language models (MLLMs),especially their capabilities in visual chat through refer and groundfunctionalities, their significance is increasingly recognized. However, thebiomedical field currently exhibits a substantial gap in this area, primarilydue to the absence of a dedicated refer and ground dataset for biomedicalimages. To address this challenge, we devised the Med-GRIT-270k dataset. Itcomprises 270k question-and-answer pairs and spans eight distinct medicalimaging modalities. Most importantly, it is the first dedicated to thebiomedical domain and integrating refer and ground conversations. The key ideais to sample large-scale biomedical image-mask pairs from medical segmentationdatasets and generate instruction datasets from text using chatGPT.Additionally, we introduce a Refer-and-Ground Multimodal Large Language Modelfor Biomedicine (BiRD) by using this dataset and multi-task instructionlearning. Extensive experiments have corroborated the efficacy of theMed-GRIT-270k dataset and the multi-modal, fine-grained interactivecapabilities of the BiRD model. This holds significant reference value for theexploration and development of intelligent biomedical assistants.</description><author>Xiaoshuang Huang, Haifeng Huang, Lingdong Shen, Yehui Yang, Fangxin Shang, Junwei Liu, Jia Liu</author><pubDate>Wed, 26 Jun 2024 08:56:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18146v1</guid></item><item><title>Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models</title><link>http://arxiv.org/abs/2404.13013v1</link><description>We introduce Groma, a Multimodal Large Language Model (MLLM) with groundedand fine-grained visual perception ability. Beyond holistic imageunderstanding, Groma is adept at region-level tasks such as region captioningand visual grounding. Such capabilities are built upon a localized visualtokenization mechanism, where an image input is decomposed into regions ofinterest and subsequently encoded into region tokens. By integrating regiontokens into user instructions and model responses, we seamlessly enable Gromato understand user-specified region inputs and ground its textual output toimages. Besides, to enhance the grounded chat ability of Groma, we curate avisually grounded instruction dataset by leveraging the powerful GPT-4V andvisual prompting techniques. Compared with MLLMs that rely on the languagemodel or external module for localization, Groma consistently demonstratessuperior performances in standard referring and grounding benchmarks,highlighting the advantages of embedding localization into image tokenization.Project page: https://groma-mllm.github.io/.</description><author>Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, Xiaojuan Qi</author><pubDate>Fri, 19 Apr 2024 18:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13013v1</guid></item><item><title>A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia</title><link>http://arxiv.org/abs/2312.02073v2</link><description>Large language models (LLMs) have an impressive ability to draw on novelinformation supplied in their context. Yet the mechanisms underlying thiscontextual grounding remain unknown, especially in situations where contextualinformation contradicts factual knowledge stored in the parameters, which LLMsalso excel at recalling. Favoring the contextual information is critical forretrieval-augmented generation methods, which enrich the context withup-to-date information, hoping that grounding can rectify outdated or noisystored knowledge. We present a novel method to study grounding abilities usingFakepedia, a dataset of counterfactual texts constructed to clash with amodel's internal parametric knowledge. We benchmark various LLMs with Fakepediaand then we conduct a causal mediation analysis, based on our Masked GroupedCausal Tracing (MGCT), on LLM components when answering Fakepedia queries.Within this analysis, we identify distinct computational patterns betweengrounded and ungrounded responses. We finally demonstrate that distinguishinggrounded from ungrounded responses is achievable through computational analysisalone. Our results, together with existing findings about factual recallmechanisms, provide a coherent narrative of how grounding and factual recallmechanisms interact within LLMs.</description><author>Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre Kıcıman, Hamid Palangi, Barun Patra, Robert West</author><pubDate>Tue, 20 Feb 2024 17:27:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02073v2</guid></item><item><title>"Understanding AI": Semantic Grounding in Large Language Models</title><link>http://arxiv.org/abs/2402.10992v1</link><description>Do LLMs understand the meaning of the texts they generate? Do they possess asemantic grounding? And how could we understand whether and what theyunderstand? I start the paper with the observation that we have recentlywitnessed a generative turn in AI, since generative models, including LLMs, arekey for self-supervised learning. To assess the question of semantic grounding,I distinguish and discuss five methodological ways. The most promising way isto apply core assumptions of theories of meaning in philosophy of mind andlanguage to LLMs. Grounding proves to be a gradual affair with athree-dimensional distinction between functional, social and causal grounding.LLMs show basic evidence in all three dimensions. A strong argument is thatLLMs develop world models. Hence, LLMs are neither stochastic parrots norsemantic zombies, but already understand the language they generate, at leastin an elementary sense.</description><author>Holger Lyre</author><pubDate>Fri, 16 Feb 2024 14:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10992v1</guid></item><item><title>Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models</title><link>http://arxiv.org/abs/2404.07973v1</link><description>While Ferret seamlessly integrates regional understanding into the LargeLanguage Model (LLM) to facilitate its referring and grounding capability, itposes certain limitations: constrained by the pre-trained fixed visual encoderand failed to perform well on broader tasks. In this work, we unveil Ferret-v2,a significant upgrade to Ferret, with three key designs. (1) Any resolutiongrounding and referring: A flexible approach that effortlessly handles higherimage resolution, improving the model's ability to process and understandimages in greater detail. (2) Multi-granularity visual encoding: By integratingthe additional DINOv2 encoder, the model learns better and diverse underlyingcontexts for global and fine-grained visual information. (3) A three-stagetraining paradigm: Besides image-caption alignment, an additional stage isproposed for high-resolution dense alignment before the final instructiontuning. Experiments show that Ferret-v2 provides substantial improvements overFerret and other state-of-the-art methods, thanks to its high-resolutionscaling and fine-grained visual processing.</description><author>Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, Yinfei Yang</author><pubDate>Thu, 11 Apr 2024 18:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07973v1</guid></item><item><title>Grounding Multimodal Large Language Models in Actions</title><link>http://arxiv.org/abs/2406.07904v1</link><description>Multimodal Large Language Models (MLLMs) have demonstrated a wide range ofcapabilities across many domains, including Embodied AI. In this work, we studyhow to best ground a MLLM into different embodiments and their associatedaction spaces, with the goal of leveraging the multimodal world knowledge ofthe MLLM. We first generalize a number of methods through a unifiedarchitecture and the lens of action space adaptors. For continuous actions, weshow that a learned tokenization allows for sufficient modeling precision,yielding the best performance on downstream tasks. For discrete actions, wedemonstrate that semantically aligning these actions with the native outputtoken space of the MLLM leads to the strongest performance. We arrive at theselessons via a thorough study of seven action space adapters on five differentenvironments, encompassing over 114 embodied tasks.</description><author>Andrew Szot, Bogdan Mazoure, Harsh Agrawal, Devon Hjelm, Zsolt Kira, Alexander Toshev</author><pubDate>Wed, 12 Jun 2024 07:12:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07904v1</guid></item><item><title>Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?</title><link>http://arxiv.org/abs/2404.06510v1</link><description>Enhancing semantic grounding abilities in Vision-Language Models (VLMs) ofteninvolves collecting domain-specific training data, refining the networkarchitectures, or modifying the training recipes. In this work, we venture intoan orthogonal direction and explore whether VLMs can improve their semanticgrounding by "receiving" feedback, without requiring in-domain data,fine-tuning, or modifications to the network architectures. We systematicallyanalyze this hypothesis using a feedback mechanism composed of a binary signal.We find that if prompted appropriately, VLMs can utilize feedback both in asingle step and iteratively, showcasing the potential of feedback as analternative technique to improve grounding in internet-scale VLMs. Furthermore,VLMs, like LLMs, struggle to self-correct errors out-of-the-box. However, wefind that this issue can be mitigated via a binary verification mechanism.Finally, we explore the potential and limitations of amalgamating thesefindings and applying them iteratively to automatically enhance VLMs' groundingperformance, showing grounding accuracy consistently improves using automatedfeedback across all models in all settings investigated. Overall, our iterativeframework improves semantic grounding in VLMs by more than 15 accuracy pointsunder noise-free feedback and up to 5 accuracy points under a simple automatedbinary verification mechanism. The project website is hosted athttps://andrewliao11.github.io/vlms_feedback</description><author>Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna</author><pubDate>Tue, 09 Apr 2024 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06510v1</guid></item><item><title>GestureGPT: Toward Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents</title><link>http://arxiv.org/abs/2310.12821v4</link><description>Current gesture interfaces typically demand users to learn and performgestures from a predefined set, which leads to a less natural experience.Interfaces supporting user-defined gestures eliminate the learning process, butusers still need to demonstrate and associate the gesture to a specific systemfunction themselves. We introduce GestureGPT, a free-form hand gestureunderstanding framework that does not require users to learn, demonstrate, orassociate gestures. Our framework leverages the large language model's (LLM)astute common sense and strong inference ability to understand a spontaneouslyperformed gesture from its natural language descriptions, and automaticallymaps it to a function provided by the interface. More specifically, ourtriple-agent framework involves a Gesture Description Agent that automaticallysegments and formulates natural language descriptions of hand poses andmovements based on hand landmark coordinates. The description is deciphered bya Gesture Inference Agent through self-reasoning and querying about theinteraction context (e.g., interaction history, gaze data), which a ContextManagement Agent organizes and provides. Following iterative exchanges, theGesture Inference Agent discerns user intent, grounding it to an interactivefunction. We validated our conceptual framework under two real-world scenarios:smart home controlling and online video streaming. The average zero-shot Top-5grounding accuracies are 83.59% for smart home tasks and 73.44% for videostreaming. We also provided an extensive discussion of our framework includingmodel selection rationale, generated description quality, generalizability etc.</description><author>Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, Yiqiang Chen</author><pubDate>Fri, 21 Jun 2024 11:12:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12821v4</guid></item><item><title>EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model</title><link>http://arxiv.org/abs/2312.02483v2</link><description>Early weakly supervised video grounding (WSVG) methods often struggle withincomplete boundary detection due to the absence of temporal boundaryannotations. To bridge the gap between video-level and boundary-levelannotation, explicit-supervision methods, i.e., generating pseudo-temporalboundaries for training, have achieved great success. However, dataaugmentations in these methods might disrupt critical temporal information,yielding poor pseudo boundaries. In this paper, we propose a new perspectivethat maintains the integrity of the original temporal content while introducingmore valuable information for expanding the incomplete boundaries. To this end,we propose EtC (Expand then Clarify), first use the additional information toexpand the initial incomplete pseudo boundaries, and subsequently refine theseexpanded ones to achieve precise boundaries. Motivated by video continuity,i.e., visual similarity across adjacent frames, we use powerful multimodallarge language models (MLLMs) to annotate each frame within initial pseudoboundaries, yielding more comprehensive descriptions for expanded boundaries.To further clarify the noise of expanded boundaries, we combine mutual learningwith a tailored proposal-level contrastive objective to use a learnableapproach to harmonize a balance between incomplete yet clean (initial) andcomprehensive yet noisy (expanded) boundaries for more precise ones.Experiments demonstrate the superiority of our method on two challenging WSVGdatasets.</description><author>Guozhang Li, Xinpeng Ding, De Cheng, Jie Li, Nannan Wang, Xinbo Gao</author><pubDate>Wed, 06 Mar 2024 08:23:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02483v2</guid></item><item><title>VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models</title><link>http://arxiv.org/abs/2402.18374v1</link><description>Recent approaches in domain-specific named entity recognition (NER), such asbiomedical NER, have shown remarkable advances. However, they still lack offaithfulness, producing erroneous predictions. We assume that knowledge ofentities can be useful in verifying the correctness of the predictions. Despitethe usefulness of knowledge, resolving such errors with knowledge isnontrivial, since the knowledge itself does not directly indicate theground-truth label. To this end, we propose VerifiNER, a post-hoc verificationframework that identifies errors from existing NER methods using knowledge andrevises them into more faithful predictions. Our framework leverages thereasoning abilities of large language models to adequately ground on knowledgeand the contextual information in the verification process. We validateeffectiveness of VerifiNER through extensive experiments on biomedicaldatasets. The results suggest that VerifiNER can successfully verify errorsfrom existing models as a model-agnostic approach. Further analyses onout-of-domain and low-resource settings show the usefulness of VerifiNER onreal-world applications.</description><author>Seoyeon Kim, Kwangwook Seo, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee</author><pubDate>Wed, 28 Feb 2024 14:49:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18374v1</guid></item><item><title>From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility</title><link>http://arxiv.org/abs/2402.16142v1</link><description>This groundbreaking study explores the expanse of Large Language Models(LLMs), such as Generative Pre-Trained Transformer (GPT) and BidirectionalEncoder Representations from Transformers (BERT) across varied domains rangingfrom technology, finance, healthcare to education. Despite their establishedprowess in Natural Language Processing (NLP), these LLMs have not beensystematically examined for their impact on domains such as fitness, andholistic well-being, urban planning, climate modelling as well as disastermanagement. This review paper, in addition to furnishing a comprehensiveanalysis of the vast expanse and extent of LLMs' utility in diverse domains,recognizes the research gaps and realms where the potential of LLMs is yet tobe harnessed. This study uncovers innovative ways in which LLMs can leave amark in the fields like fitness and wellbeing, urban planning, climatemodelling and disaster response which could inspire future researches andapplications in the said avenues.</description><author>Pravneet Kaur, Gautam Siddharth Kashyap, Ankit Kumar, Md Tabrez Nafis, Sandeep Kumar, Vikrant Shokeen</author><pubDate>Sun, 25 Feb 2024 16:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16142v1</guid></item><item><title>NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2312.11361v2</link><description>Retrieval-augmented generation (RAG) grounds large language model (LLM)output by leveraging external knowledge sources to reduce factualhallucinations. However, prior works lack a comprehensive evaluation ofdifferent language families, making it challenging to evaluate LLM robustnessagainst errors in external retrieved knowledge. To overcome this, we establishNoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across18 typologically diverse languages. NoMIRACL includes both a non-relevant and arelevant subset. Queries in the non-relevant subset contain passages judged asnon-relevant, whereas queries in the relevant subset include at least a singlejudged relevant passage. We measure LLM robustness using two metrics: (i)hallucination rate, measuring model tendency to hallucinate an answer, when theanswer is not present in passages in the non-relevant subset, and (ii) errorrate, measuring model inaccuracy to recognize relevant passages in the relevantsubset. In our work, we measure robustness for a wide variety ofmultilingual-focused LLMs and observe that most of the models struggle tobalance the two capacities. Models such as LLAMA-2, Orca-2, and FLAN-T5 observemore than an 88% hallucination rate on the non-relevant subset, whereas,Mistral overall hallucinates less, but can achieve up to a 74.9% error rate onthe relevant subset. Overall, GPT-4 is observed to provide the best tradeoff onboth subsets, highlighting future work necessary to improve LLM robustness.</description><author>Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, Jimmy Lin</author><pubDate>Mon, 04 Mar 2024 16:32:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11361v2</guid></item><item><title>Adversarial Robustness for Visual Grounding of Multimodal Large Language Models</title><link>http://arxiv.org/abs/2405.09981v1</link><description>Multi-modal Large Language Models (MLLMs) have recently achieved enhancedperformance across various vision-language tasks including visual groundingcapabilities. However, the adversarial robustness of visual grounding remainsunexplored in MLLMs. To fill this gap, we use referring expressioncomprehension (REC) as an example task in visual grounding and propose threeadversarial attack paradigms as follows. Firstly, untargeted adversarialattacks induce MLLMs to generate incorrect bounding boxes for each object.Besides, exclusive targeted adversarial attacks cause all generated outputs tothe same target bounding box. In addition, permuted targeted adversarialattacks aim to permute all bounding boxes among different objects within asingle image. Extensive experiments demonstrate that the proposed methods cansuccessfully attack visual grounding capabilities of MLLMs. Our methods notonly provide a new perspective for designing novel attacks but also serve as astrong baseline for improving the adversarial robustness for visual groundingof MLLMs.</description><author>Kuofeng Gao, Yang Bai, Jiawang Bai, Yong Yang, Shu-Tao Xia</author><pubDate>Thu, 16 May 2024 11:54:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09981v1</guid></item><item><title>RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models</title><link>http://arxiv.org/abs/2310.10221v2</link><description>Robotic vision applications often necessitate a wide range of visualperception tasks, such as object detection, segmentation, and identification.While there have been substantial advances in these individual tasks,integrating specialized models into a unified vision pipeline presentssignificant engineering challenges and costs. Recently, Multimodal LargeLanguage Models (MLLMs) have emerged as novel backbones for various downstreamtasks. We argue that leveraging the pre-training capabilities of MLLMs enablesthe creation of a simplified framework, thus mitigating the need fortask-specific encoders. Specifically, the large-scale pretrained knowledge inMLLMs allows for easier fine-tuning to downstream robotic vision tasks andyields superior performance. We introduce the RoboLLM framework, equipped witha BEiT-3 backbone, to address all visual perception tasks in the ARMBenchchallenge-a large-scale robotic manipulation dataset about real-world warehousescenarios. RoboLLM not only outperforms existing baselines but alsosubstantially reduces the engineering burden associated with model selectionand tuning. The source code is publicly available athttps://github.com/longkukuhi/armbench.</description><author>Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon Camarasa</author><pubDate>Fri, 23 Feb 2024 15:18:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10221v2</guid></item><item><title>LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding</title><link>http://arxiv.org/abs/2405.17104v1</link><description>Visual grounding is an essential tool that links user-provided text querieswith query-specific regions within an image. Despite advancements in visualgrounding models, their ability to comprehend complex queries remains limited.To overcome this limitation, we introduce LLM-Optic, an innovative method thatutilizes Large Language Models (LLMs) as an optical lens to enhance existingvisual grounding models in comprehending complex text queries involvingintricate text structures, multiple objects, or object spatial relationships,situations that current models struggle with. LLM-Optic first employs an LLM asa Text Grounder to interpret complex text queries and accurately identifyobjects the user intends to locate. Then a pre-trained visual grounding modelis used to generate candidate bounding boxes given the refined query by theText Grounder. After that, LLM-Optic annotates the candidate bounding boxeswith numerical marks to establish a connection between text and specific imageregions, thereby linking two distinct modalities. Finally, it employs a LargeMultimodal Model (LMM) as a Visual Grounder to select the marked candidateobjects that best correspond to the original text query. Through LLM-Optic, wehave achieved universal visual grounding, which allows for the detection ofarbitrary objects specified by arbitrary human language input. Importantly, ourmethod achieves this enhancement without requiring additional training orfine-tuning. Extensive experiments across various challenging benchmarksdemonstrate that LLM-Optic achieves state-of-the-art zero-shot visual groundingcapabilities.</description><author>Haoyu Zhao, Wenhang Ge, Ying-cong Chen</author><pubDate>Mon, 27 May 2024 13:23:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17104v1</guid></item><item><title>When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment</title><link>http://arxiv.org/abs/2401.07764v2</link><description>AI agents based on multimodal large language models (LLMs) are expected torevolutionize human-computer interaction and offer more personalized assistantservices across various domains like healthcare, education, manufacturing, andentertainment. Deploying LLM agents in 6G networks enables users to accesspreviously expensive AI assistant services via mobile devices democratically,thereby reducing interaction latency and better preserving user privacy.Nevertheless, the limited capacity of mobile devices constrains theeffectiveness of deploying and executing local LLMs, which necessitatesoffloading complex tasks to global LLMs running on edge servers duringlong-horizon interactions. In this article, we propose a split learning systemfor LLM agents in 6G networks leveraging the collaboration between mobiledevices and edge servers, where multiple LLMs with different roles aredistributed across mobile devices and edge servers to perform user-agentinteractive tasks collaboratively. In the proposed system, LLM agents are splitinto perception, grounding, and alignment modules, facilitating inter-modulecommunications to meet extended user requirements on 6G network functions,including integrated sensing and communication, digital twins, andtask-oriented communications. Furthermore, we introduce a novel model cachingalgorithm for LLMs within the proposed system to improve model utilization incontext, thus reducing network costs of the collaborative mobile and edge LLMagents.</description><author>Minrui Xu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Dong In Kim, Khaled B. Letaief</author><pubDate>Fri, 16 Feb 2024 19:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07764v2</guid></item><item><title>GroundingGPT:Language Enhanced Multi-modal Grounding Model</title><link>http://arxiv.org/abs/2401.06071v5</link><description>Multi-modal large language models have demonstrated impressive performanceacross various tasks in different modalities. However, existing multi-modalmodels primarily emphasize capturing global information within each modalitywhile neglecting the importance of perceiving local information acrossmodalities. Consequently, these models lack the ability to effectivelyunderstand the fine-grained details of input data, limiting their performancein tasks that require a more nuanced understanding. To address this limitation,there is a compelling need to develop models that enable fine-grainedunderstanding across multiple modalities, thereby enhancing their applicabilityto a wide range of tasks. In this paper, we propose GroundingGPT, a languageenhanced multi-modal grounding model. Beyond capturing global information likeother multi-modal models, our proposed model excels at tasks demanding adetailed understanding of local information within the input. It demonstratesprecise identification and localization of specific regions in images ormoments in videos. To achieve this objective, we design a diversified datasetconstruction pipeline, resulting in a multi-modal, multi-granularity datasetfor model training. The code, dataset, and demo of our model can be found athttps: //github.com/lzw-lzw/GroundingGPT.</description><author>Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, Tao Wang</author><pubDate>Tue, 05 Mar 2024 14:36:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06071v5</guid></item><item><title>VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?</title><link>http://arxiv.org/abs/2404.05955v1</link><description>Multimodal Large Language models (MLLMs) have shown promise in web-relatedtasks, but evaluating their performance in the web domain remains a challengedue to the lack of comprehensive benchmarks. Existing benchmarks are eitherdesigned for general multimodal tasks, failing to capture the uniquecharacteristics of web pages, or focus on end-to-end web agent tasks, unable tomeasure fine-grained abilities such as OCR, understanding, and grounding. Inthis paper, we introduce \bench{}, a multimodal benchmark designed to assessthe capabilities of MLLMs across a variety of web tasks. \bench{} consists ofseven tasks, and comprises 1.5K human-curated instances from 139 real websites,covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3series, and GPT-4V(ision) on \bench{}, revealing significant challenges andperformance gaps. Further analysis highlights the limitations of current MLLMs,including inadequate grounding in text-rich environments and subpar performancewith low-resolution image inputs. We believe \bench{} will serve as a valuableresource for the research community and contribute to the creation of morepowerful and versatile MLLMs for web-related applications.</description><author>Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, Xiang Yue</author><pubDate>Tue, 09 Apr 2024 03:29:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05955v1</guid></item><item><title>LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models</title><link>http://arxiv.org/abs/2305.13655v3</link><description>Recent advancements in text-to-image diffusion models have yielded impressiveresults in generating realistic and diverse images. However, these models stillstruggle with complex prompts, such as those that involve numeracy and spatialreasoning. This work proposes to enhance prompt understanding capabilities indiffusion models. Our method leverages a pretrained large language model (LLM)for grounded generation in a novel two-stage process. In the first stage, theLLM generates a scene layout that comprises captioned bounding boxes from agiven prompt describing the desired image. In the second stage, a novelcontroller guides an off-the-shelf diffusion model for layout-grounded imagegeneration. Both stages utilize existing pretrained models without additionalmodel parameter optimization. Our method significantly outperforms the basediffusion model and several strong baselines in accurately generating imagesaccording to prompts that require various capabilities, doubling the generationaccuracy across four tasks on average. Furthermore, our method enablesinstruction-based multi-round scene specification and can handle prompts inlanguages not supported by the underlying diffusion model. We anticipate thatour method will unleash users' creativity by accurately following more complexprompts. Our code, demo, and benchmark are available at:https://llm-grounded-diffusion.github.io</description><author>Long Lian, Boyi Li, Adam Yala, Trevor Darrell</author><pubDate>Mon, 04 Mar 2024 18:43:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13655v3</guid></item><item><title>KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models</title><link>http://arxiv.org/abs/2402.15043v1</link><description>Automatic evaluation methods for large language models (LLMs) are hindered bydata contamination, leading to inflated assessments of their effectiveness.Existing strategies, which aim to detect contaminated texts, focus onquantifying contamination status instead of accurately gauging modelperformance. In this paper, we introduce KIEval, a Knowledge-groundedInteractive Evaluation framework, which incorporates an LLM-powered"interactor" role for the first time to accomplish a dynamiccontamination-resilient evaluation. Starting with a question in a conventionalLLM benchmark involving domain-specific knowledge, KIEval utilizes dynamicallygenerated, multi-round, and knowledge-focused dialogues to determine whether amodel's response is merely a recall of benchmark answers or demonstrates a deepcomprehension to apply knowledge in more complex conversations. Extensiveexperiments on seven leading LLMs across five datasets validate KIEval'seffectiveness and generalization. We also reveal that data contamination bringsno contribution or even negative effect to models' real-world applicability andunderstanding, and existing contamination detection methods for LLMs can onlyidentify contamination in pre-training but not during supervised fine-tuning.</description><author>Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang</author><pubDate>Fri, 23 Feb 2024 01:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15043v1</guid></item><item><title>KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models</title><link>http://arxiv.org/abs/2402.15043v2</link><description>Automatic evaluation methods for large language models (LLMs) are hindered bydata contamination, leading to inflated assessments of their effectiveness.Existing strategies, which aim to detect contaminated texts, focus onquantifying contamination status instead of accurately gauging modelperformance. In this paper, we introduce KIEval, a Knowledge-groundedInteractive Evaluation framework, which incorporates an LLM-powered"interactor" role for the first time to accomplish a dynamiccontamination-resilient evaluation. Starting with a question in a conventionalLLM benchmark involving domain-specific knowledge, KIEval utilizes dynamicallygenerated, multi-round, and knowledge-focused dialogues to determine whether amodel's response is merely a recall of benchmark answers or demonstrates a deepcomprehension to apply knowledge in more complex conversations. Extensiveexperiments on seven leading LLMs across five datasets validate KIEval'seffectiveness and generalization. We also reveal that data contamination bringsno contribution or even negative effect to models' real-world applicability andunderstanding, and existing contamination detection methods for LLMs can onlyidentify contamination in pre-training but not during supervised fine-tuning.</description><author>Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang</author><pubDate>Mon, 03 Jun 2024 07:02:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15043v2</guid></item><item><title>Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models</title><link>http://arxiv.org/abs/2403.19322v1</link><description>The surge of Multimodal Large Language Models (MLLMs), given their prominentemergent capabilities in instruction following and reasoning, has greatlyadvanced the field of visual reasoning. However, constrained by theirnon-lossless image tokenization, most MLLMs fall short of comprehensivelycapturing details of text and objects, especially in high-resolution images. Toaddress this, we propose P2G, a novel framework for plug-and-play grounding ofreasoning in MLLMs. Specifically, P2G exploits the tool-usage potential ofMLLMs to employ expert agents to achieve on-the-fly grounding to criticalvisual and textual objects of image, thus achieving deliberate reasoning viamultimodal prompting. We further create P2GB, a benchmark aimed at assessingMLLMs' ability to understand inter-object relationships and text in challenginghigh-resolution images. Comprehensive experiments on visual reasoning tasksdemonstrate the superiority of P2G. Noteworthy, P2G achieved comparableperformance with GPT-4V on P2GB, with a 7B backbone. Our work highlights thepotential of plug-and-play grounding of reasoning and opens up a promisingalternative beyond model scaling.</description><author>Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Ziyong Feng, Yongle Zhao, Yin Xie</author><pubDate>Thu, 28 Mar 2024 12:26:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19322v1</guid></item><item><title>Grounded 3D-LLM with Referent Tokens</title><link>http://arxiv.org/abs/2405.10370v1</link><description>Prior studies on 3D scene understanding have primarily developed specializedmodels for specific tasks or required task-specific fine-tuning. In this study,we propose Grounded 3D-LLM, which explores the potential of 3D largemulti-modal models (3D LMMs) to consolidate various 3D vision tasks within aunified generative framework. The model uses scene referent tokens as specialnoun phrases to reference 3D scenes, enabling the handling of sequences thatinterleave 3D and textual data. It offers a natural approach for translating 3Dvision tasks into language formats using task-specific instruction templates.To facilitate the use of referent tokens in subsequent language modeling, wehave curated large-scale grounded language datasets that offer finer scene-textcorrespondence at the phrase level by bootstrapping existing object labels.Subsequently, we introduced Contrastive LAnguage-Scene Pre-training (CLASP) toeffectively leverage this data, thereby integrating 3D vision with languagemodels. Our comprehensive evaluation covers open-ended tasks like densecaptioning and 3D QA, alongside close-ended tasks such as object detection andlanguage grounding. Experiments across multiple 3D benchmarks reveal theleading performance and the broad applicability of Grounded 3D-LLM. Code anddatasets will be released on the project page:https://groundedscenellm.github.io/grounded_3d-llm.github.io.</description><author>Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Ruiyuan Lyu, Runsen Xu, Dahua Lin, Jiangmiao Pang</author><pubDate>Thu, 16 May 2024 19:03:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10370v1</guid></item><item><title>CaLM: Contrasting Large and Small Language Models to Verify Grounded Generation</title><link>http://arxiv.org/abs/2406.05365v2</link><description>Grounded generation aims to equip language models (LMs) with the ability toproduce more credible and accountable responses by accurately citing verifiablesources. However, existing methods, by either feeding LMs with raw orpreprocessed materials, remain prone to errors. To address this, we introduceCaLM, a novel verification framework. CaLM leverages the insight that a robustgrounded response should be consistent with information derived solely from itscited sources. Our framework empowers smaller LMs, which rely less onparametric memory and excel at processing relevant information given a query,to validate the output of larger LMs. Larger LM responses that closely alignwith the smaller LMs' output, which relies exclusively on cited documents, areverified. Responses showing discrepancies are iteratively refined through afeedback loop. Experiments on three open-domain question-answering datasetsdemonstrate significant performance gains of 1.5% to 7% absolute averagewithout any required model fine-tuning.</description><author>I-Hung Hsu, Zifeng Wang, Long T. Le, Lesly Miculicich, Nanyun Peng, Chen-Yu Lee, Tomas Pfister</author><pubDate>Mon, 24 Jun 2024 08:39:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05365v2</guid></item><item><title>MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal</title><link>http://arxiv.org/abs/2402.11297v1</link><description>Our contribution introduces a groundbreaking multimodal large language modeldesigned to comprehend multi-images, multi-audio, and multi-images-multi-audiowithin a single multiturn session. Leveraging state-of-the-art models, weutilize the SigLIP encoder for visual inputs and the Whisper Encoder for audioinputs. Notably, this multimodal large language model is bilingual, proficientin understanding both English and Malay simultaneously. We proudly unveil twoversions of this model: TinyLlama with 1.1B parameters, and Mistral with 7Bparameters. With its ability to navigate diverse modalities and languages, ourmodel represents a significant advancement for the Malaysian context andbeyond. All models released athttps://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859</description><author>Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan</author><pubDate>Sat, 17 Feb 2024 14:37:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11297v1</guid></item><item><title>Uncertainty Quantification for In-Context Learning of Large Language Models</title><link>http://arxiv.org/abs/2402.10189v2</link><description>In-context learning has emerged as a groundbreaking ability of Large LanguageModels (LLMs) and revolutionized various fields by providing a fewtask-relevant demonstrations in the prompt. However, trustworthy issues withLLM's response, such as hallucination, have also been actively discussed.Existing works have been devoted to quantifying the uncertainty in LLM'sresponse, but they often overlook the complex nature of LLMs and the uniquenessof in-context learning. In this work, we delve into the predictive uncertaintyof LLMs associated with in-context learning, highlighting that suchuncertainties may stem from both the provided demonstrations (aleatoricuncertainty) and ambiguities tied to the model's configurations (epistemicuncertainty). We propose a novel formulation and corresponding estimationmethod to quantify both types of uncertainties. The proposed method offers anunsupervised way to understand the prediction of in-context learning in aplug-and-play fashion. Extensive experiments are conducted to demonstrate theeffectiveness of the decomposition. The code and data are available at:https://github.com/lingchen0331/UQ_ICL.</description><author>Chen Ling, Xujiang Zhao, Xuchao Zhang, Wei Cheng, Yanchi Liu, Yiyou Sun, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</author><pubDate>Thu, 28 Mar 2024 20:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10189v2</guid></item><item><title>Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models</title><link>http://arxiv.org/abs/2402.10189v1</link><description>In-context learning has emerged as a groundbreaking ability of Large LanguageModels (LLMs) and revolutionized various fields by providing a fewtask-relevant demonstrations in the prompt. However, trustworthy issues withLLM's response, such as hallucination, have also been actively discussed.Existing works have been devoted to quantifying the uncertainty in LLM'sresponse, but they often overlook the complex nature of LLMs and the uniquenessof in-context learning. In this work, we delve into the predictive uncertaintyof LLMs associated with in-context learning, highlighting that suchuncertainties may stem from both the provided demonstrations (aleatoricuncertainty) and ambiguities tied to the model's configurations (epistemicuncertainty). We propose a novel formulation and corresponding estimationmethod to quantify both types of uncertainties. The proposed method offers anunsupervised way to understand the prediction of in-context learning in aplug-and-play fashion. Extensive experiments are conducted to demonstrate theeffectiveness of the decomposition. The code and data are available at:\url{https://github.com/lingchen0331/UQ_ICL}.</description><author>Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</author><pubDate>Thu, 15 Feb 2024 18:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10189v1</guid></item><item><title>Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?</title><link>http://arxiv.org/abs/2406.14492v1</link><description>Large vision-language models (LVLMs) have recently dramatically pushed thestate of the art in image captioning and many image understanding tasks (e.g.,visual question answering). LVLMs, however, often \textit{hallucinate} andproduce captions that mention concepts that cannot be found in the image. Thesehallucinations erode the trustworthiness of LVLMs and are arguably among themain obstacles to their ubiquitous adoption. Recent work suggests that additionof grounding objectives -- those that explicitly align image regions or objectsto text spans -- reduces the amount of LVLM hallucination. Although intuitive,this claim is not empirically justified as the reduction effects have beenestablished, we argue, with flawed evaluation protocols that (i) rely on data(i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measurehallucination via question answering rather than open-ended caption generation.In this work, in contrast, we offer the first systematic analysis of the effectof fine-grained object grounding on LVLM hallucination under an evaluationprotocol that more realistically captures LVLM hallucination in opengeneration. Our extensive experiments over three backbone LLMs reveal thatgrounding objectives have little to no effect on object hallucination in opencaption generation.</description><author>Gregor Geigle, Radu Timofte, Goran Glavaš</author><pubDate>Thu, 20 Jun 2024 17:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14492v1</guid></item><item><title>Event Grounded Criminal Court View Generation withCooperative (Large) Language Models</title><link>http://arxiv.org/abs/2404.07001v1</link><description>With the development of legal intelligence, Criminal Court View Generationhas attracted much attention as a crucial task of legal intelligence, whichaims to generate concise and coherent texts that summarize case facts andprovide explanations for verdicts. Existing researches explore the keyinformation in case facts to yield the court views. Most of them employ acoarse-grained approach that partitions the facts into broad segments (e.g.,verdict-related sentences) to make predictions. However, this approach fails tocapture the complex details present in the case facts, such as various criminalelements and legal events. To this end, in this paper, we propose an EventGrounded Generation (EGG) method for criminal court view generation withcooperative (Large) Language Models, which introduces the fine-grained eventinformation into the generation. Specifically, we first design a LLMs-basedextraction method that can extract events in case facts without massiveannotated events. Then, we incorporate the extracted events into court viewgeneration by merging case facts and events. Besides, considering thecomputational burden posed by the use of LLMs in the extraction phase of EGG,we propose a LLMs-free EGG method that can eliminate the requirement for eventextraction using LLMs in the inference phase. Extensive experimental results ona real-world dataset clearly validate the effectiveness of our proposed method.</description><author>Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, Yanqing An</author><pubDate>Wed, 10 Apr 2024 14:31:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07001v1</guid></item><item><title>Towards an End-to-End Framework for Invasive Brain Signal Decoding with Large Language Models</title><link>http://arxiv.org/abs/2406.11568v1</link><description>In this paper, we introduce a groundbreaking end-to-end (E2E) framework fordecoding invasive brain signals, marking a significant advancement in the fieldof speech neuroprosthesis. Our methodology leverages the comprehensivereasoning abilities of large language models (LLMs) to facilitate directdecoding. By fully integrating LLMs, we achieve results comparable to thestate-of-the-art cascade models. Our findings underscore the immense potentialof E2E frameworks in speech neuroprosthesis, particularly as the technologybehind brain-computer interfaces (BCIs) and the availability of relevantdatasets continue to evolve. This work not only showcases the efficacy ofcombining LLMs with E2E decoding for enhancing speech neuroprosthesis but alsosets a new direction for future research in BCI applications, underscoring theimpact of LLMs in decoding complex neural signals for communicationrestoration. Code will be made available athttps://github.com/FsFrancis15/BrainLLM.</description><author>Sheng Feng, Heyang Liu, Yu Wang, Yanfeng Wang</author><pubDate>Mon, 17 Jun 2024 15:04:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11568v1</guid></item><item><title>SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models</title><link>http://arxiv.org/abs/2311.09818v2</link><description>While most conversational agents are grounded on either free-text orstructured knowledge, many knowledge corpora consist of hybrid sources. Thispaper presents the first conversational agent that supports the full generalityof hybrid data access for large knowledge corpora, through a language wedeveloped called SUQL (Structured and Unstructured Query Language).Specifically, SUQL extends SQL with free-text primitives (summary and answer),so information retrieval can be composed with structured data accessesarbitrarily in a formal, succinct, precise, and interpretable notation. WithSUQL, we propose the first semantic parser, an LLM with in-context learning,that can handle hybrid data sources. Our in-context learning-based approach, when applied to the HybridQA dataset,comes within 8.9% exact match and 7.1% F1 of the SOTA, which was trained on 62Kdata samples. More significantly, unlike previous approaches, our technique isapplicable to large databases and free-text corpora. We introduce a datasetconsisting of crowdsourced questions and conversations on Yelp, a large, realrestaurant knowledge base with structured and unstructured data. We show thatour few-shot conversational agent based on SUQL finds an entity satisfying alluser requirements 90.3% of the time, compared to 63.4% for a baseline based onlinearization.</description><author>Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina J. Semnani, Chen Jie Yu, Monica S. Lam</author><pubDate>Wed, 13 Mar 2024 18:07:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09818v2</guid></item><item><title>Event Grounded Criminal Court View Generation with Cooperative (Large) Language Models</title><link>http://arxiv.org/abs/2404.07001v3</link><description>With the development of legal intelligence, Criminal Court View Generationhas attracted much attention as a crucial task of legal intelligence, whichaims to generate concise and coherent texts that summarize case facts andprovide explanations for verdicts. Existing researches explore the keyinformation in case facts to yield the court views. Most of them employ acoarse-grained approach that partitions the facts into broad segments (e.g.,verdict-related sentences) to make predictions. However, this approach fails tocapture the complex details present in the case facts, such as various criminalelements and legal events. To this end, in this paper, we propose an EventGrounded Generation (EGG) method for criminal court view generation withcooperative (Large) Language Models, which introduces the fine-grained eventinformation into the generation. Specifically, we first design a LLMs-basedextraction method that can extract events in case facts without massiveannotated events. Then, we incorporate the extracted events into court viewgeneration by merging case facts and events. Besides, considering thecomputational burden posed by the use of LLMs in the extraction phase of EGG,we propose a LLMs-free EGG method that can eliminate the requirement for eventextraction using LLMs in the inference phase. Extensive experimental results ona real-world dataset clearly validate the effectiveness of our proposed method.</description><author>Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, Yanqing An</author><pubDate>Tue, 16 Apr 2024 07:34:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07001v3</guid></item><item><title>Event Grounded Criminal Court View Generation with Cooperative (Large) Language Models</title><link>http://arxiv.org/abs/2404.07001v2</link><description>With the development of legal intelligence, Criminal Court View Generationhas attracted much attention as a crucial task of legal intelligence, whichaims to generate concise and coherent texts that summarize case facts andprovide explanations for verdicts. Existing researches explore the keyinformation in case facts to yield the court views. Most of them employ acoarse-grained approach that partitions the facts into broad segments (e.g.,verdict-related sentences) to make predictions. However, this approach fails tocapture the complex details present in the case facts, such as various criminalelements and legal events. To this end, in this paper, we propose an EventGrounded Generation (EGG) method for criminal court view generation withcooperative (Large) Language Models, which introduces the fine-grained eventinformation into the generation. Specifically, we first design a LLMs-basedextraction method that can extract events in case facts without massiveannotated events. Then, we incorporate the extracted events into court viewgeneration by merging case facts and events. Besides, considering thecomputational burden posed by the use of LLMs in the extraction phase of EGG,we propose a LLMs-free EGG method that can eliminate the requirement for eventextraction using LLMs in the inference phase. Extensive experimental results ona real-world dataset clearly validate the effectiveness of our proposed method.</description><author>Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao, Yanqing An</author><pubDate>Sat, 13 Apr 2024 14:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07001v2</guid></item><item><title>OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web</title><link>http://arxiv.org/abs/2402.17553v2</link><description>For decades, human-computer interaction has fundamentally been manual. Eventoday, almost all productive work done on the computer necessitates human inputat every step. Autonomous virtual agents represent an exciting step inautomating many of these menial tasks. Virtual agents would empower users withlimited technical proficiency to harness the full possibilities of computersystems. They could also enable the efficient streamlining of numerous computertasks, ranging from calendar management to complex travel bookings, withminimal human intervention. In this paper, we introduce OmniACT, thefirst-of-a-kind dataset and benchmark for assessing an agent's capability togenerate executable programs to accomplish computer tasks. Our scope extendsbeyond traditional web automation, covering a diverse range of desktopapplications. The dataset consists of fundamental tasks such as "Play the nextsong", as well as longer horizon tasks such as "Send an email to John Doementioning the time and place to meet". Specifically, given a pair of screenimage and a visually-grounded natural language task, the goal is to generate ascript capable of fully executing the task. We run several strong baselinelanguage model agents on our benchmark. The strongest baseline, GPT-4, performsthe best on our benchmark However, its performance level still reaches only 15%of the human proficiency in generating executable scripts capable of completingthe task, demonstrating the challenge of our task for conventional web agents.Our benchmark provides a platform to measure and evaluate the progress oflanguage model agents in automating computer tasks and motivates future worktowards building multimodal models that bridge large language models and thevisual grounding of computer screens.</description><author>Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, Ruslan Salakhutdinov</author><pubDate>Wed, 28 Feb 2024 17:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17553v2</guid></item><item><title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title><link>http://arxiv.org/abs/2402.14207v2</link><description>We study how to apply large language models to write grounded and organizedlong-form articles from scratch, with comparable breadth and depth to Wikipediapages. This underexplored problem poses new challenges at the pre-writingstage, including how to research the topic and prepare an outline prior towriting. We propose STORM, a writing system for the Synthesis of Topic Outlinesthrough Retrieval and Multi-perspective Question Asking. STORM models thepre-writing stage by (1) discovering diverse perspectives in researching thegiven topic, (2) simulating conversations where writers carrying differentperspectives pose questions to a topic expert grounded on trusted Internetsources, (3) curating the collected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-qualityWikipedia articles, and formulate outline assessments to evaluate thepre-writing stage. We further gather feedback from experienced Wikipediaeditors. Compared to articles generated by an outline-drivenretrieval-augmented baseline, more of STORM's articles are deemed to beorganized (by a 25% absolute increase) and broad in coverage (by 10%). Theexpert feedback also helps identify new challenges for generating grounded longarticles, such as source bias transfer and over-association of unrelated facts.</description><author>Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, Monica S. Lam</author><pubDate>Mon, 08 Apr 2024 06:38:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14207v2</guid></item><item><title>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</title><link>http://arxiv.org/abs/2402.14207v1</link><description>We study how to apply large language models to write grounded and organizedlong-form articles from scratch, with comparable breadth and depth to Wikipediapages. This underexplored problem poses new challenges at the pre-writingstage, including how to research the topic and prepare an outline prior towriting. We propose STORM, a writing system for the Synthesis of Topic Outlinesthrough Retrieval and Multi-perspective Question Asking. STORM models thepre-writing stage by (1) discovering diverse perspectives in researching thegiven topic, (2) simulating conversations where writers carrying differentperspectives pose questions to a topic expert grounded on trusted Internetsources, (3) curating the collected information to create an outline. For evaluation, we curate FreshWiki, a dataset of recent high-qualityWikipedia articles, and formulate outline assessments to evaluate thepre-writing stage. We further gather feedback from experienced Wikipediaeditors. Compared to articles generated by an outline-drivenretrieval-augmented baseline, more of STORM's articles are deemed to beorganized (by a 25% absolute increase) and broad in coverage (by 10%). Theexpert feedback also helps identify new challenges for generating grounded longarticles, such as source bias transfer and over-association of unrelated facts.</description><author>Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, Monica S. Lam</author><pubDate>Thu, 22 Feb 2024 01:20:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14207v1</guid></item><item><title>DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification</title><link>http://arxiv.org/abs/2305.15957v3</link><description>Large pre-trained models have had a significant impact on computer vision byenabling multi-modal learning, where the CLIP model has achieved impressiveresults in image classification, object detection, and semantic segmentation.However, the model's performance on 3D point cloud processing tasks is limiteddue to the domain gap between depth maps from 3D projection and training imagesof CLIP. This paper proposes DiffCLIP, a new pre-training framework thatincorporates stable diffusion with ControlNet to minimize the domain gap in thevisual branch. Additionally, a style-prompt generation module is introduced forfew-shot tasks in the textual branch. Extensive experiments on the ModelNet10,ModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilitiesfor 3D understanding. By using stable diffusion and style-prompt generation,DiffCLIP achieves an accuracy of 43.2\% for zero-shot classification on OBJ\_BGof ScanObjectNN, which is state-of-the-art performance, and an accuracy of80.6\% for zero-shot classification on ModelNet10, which is comparable tostate-of-the-art performance.</description><author>Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, Xinxiao Wu</author><pubDate>Mon, 06 May 2024 17:15:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15957v3</guid></item><item><title>HawkEye: Training Video-Text LLMs for Grounding Text in Videos</title><link>http://arxiv.org/abs/2403.10228v1</link><description>Video-text Large Language Models (video-text LLMs) have shown remarkableperformance in answering questions and holding conversations on simple videos.However, they perform almost the same as random on grounding text queries inlong and complicated videos, having little ability to understand and reasonabout temporal information, which is the most fundamental difference betweenvideos and images. In this paper, we propose HawkEye, one of the firstvideo-text LLMs that can perform temporal video grounding in a fullytext-to-text manner. To collect training data that is applicable for temporalvideo grounding, we construct InternVid-G, a large-scale video-text corpus withsegment-level captions and negative spans, with which we introduce two newtime-aware training objectives to video-text LLMs. We also propose acoarse-grained method of representing segments in videos, which is more robustand easier for LLMs to learn and follow than other alternatives. Extensiveexperiments show that HawkEye is better at temporal video grounding andcomparable on other video-text tasks with existing video-text LLMs, whichverifies its superior video-text multi-modal understanding abilities.</description><author>Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, Dongyan Zhao</author><pubDate>Fri, 15 Mar 2024 12:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10228v1</guid></item><item><title>Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models</title><link>http://arxiv.org/abs/2402.16315v1</link><description>Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)have imbued the models with the ability to generate high-level, image-groundedexplanations with ease. While such capability is largely attributed to the richworld knowledge contained within the Large Language Models (LLMs), our workreveals their shortcomings in fine-grained visual categorization (FGVC) acrosssix different benchmark settings. Most recent state-of-the-art LVLMs likeLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms ofclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogsfor LLaVA-1.5, but also struggle to generate an accurate explanation withdetailed attributes based on the concept that appears within an input imagedespite their capability to generate holistic image-level descriptions.In-depth analyses show that instruction-tuned LVLMs exhibit modality gap,showing discrepancy when given textual and visual inputs that correspond to thesame concept, preventing the image modality from leveraging the rich parametricknowledge within the LLMs. In an effort to further the community's endeavor inthis direction, we propose a multiple granularity attribute-centric evaluationbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'fine-grained visual comprehension ability and provide significantly improvedexplainability.</description><author>Jeonghwan Kim, Heng Ji</author><pubDate>Mon, 26 Feb 2024 05:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16315v1</guid></item><item><title>Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models</title><link>http://arxiv.org/abs/2308.09778v3</link><description>Large vision-and-language models (VLMs) trained to match images with text onlarge-scale datasets of image-text pairs have shown impressive generalizationability on several vision and language tasks. Several recent works, however,showed that these models lack fine-grained understanding, such as the abilityto count and recognize verbs, attributes, or relationships. The focus of thiswork is to study the understanding of spatial relations. This has been tackledpreviously using image-text matching (e.g., Visual Spatial Reasoning benchmark)or visual question answering (e.g., GQA or VQAv2), both showing poorperformance and a large gap compared to human performance. In this work, weshow qualitatively (using explainability tools) and quantitatively (usingobject detectors) that the poor object localization "grounding" ability of themodels is a contributing factor to the poor image-text matching performance. Wepropose an alternative fine-grained, compositional approach for recognizing andranking spatial clauses that combines the evidence from grounding noun phrasescorresponding to objects and their locations to compute the final rank of thespatial clause. We demonstrate the approach on representative VLMs (such asLXMERT, GPV, and MDETR) and compare and highlight their abilities to reasonabout spatial relationships.</description><author>Navid Rajabi, Jana Kosecka</author><pubDate>Wed, 06 Mar 2024 00:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09778v3</guid></item><item><title>GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning</title><link>http://arxiv.org/abs/2405.20139v1</link><description>Knowledge Graphs (KGs) represent human-crafted factual knowledge in the formof triplets (head, relation, tail), which collectively form a graph. QuestionAnswering over KGs (KGQA) is the task of answering natural questions groundingthe reasoning to the information provided by the KG. Large Language Models(LLMs) are the state-of-the-art models for QA tasks due to their remarkableability to understand natural language. On the other hand, Graph NeuralNetworks (GNNs) have been widely used for KGQA as they can handle the complexgraph information stored in the KG. In this work, we introduce GNN-RAG, a novelmethod for combining language understanding abilities of LLMs with thereasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.First, a GNN reasons over a dense KG subgraph to retrieve answer candidates fora given question. Second, the shortest paths in the KG that connect questionentities and answer candidates are extracted to represent KG reasoning paths.The extracted paths are verbalized and given as input for LLM reasoning withRAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner toextract useful graph information, while the LLM leverages its natural languageprocessing ability for ultimate KGQA. Furthermore, we develop a retrievalaugmentation (RA) technique to further boost KGQA performance with GNN-RAG.Experimental results show that GNN-RAG achieves state-of-the-art performance intwo widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matchingGPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hopand multi-entity questions outperforming competing approaches by 8.9--15.5%points at answer F1.</description><author>Costas Mavromatis, George Karypis</author><pubDate>Thu, 30 May 2024 16:14:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20139v1</guid></item><item><title>Auto-Vocabulary Semantic Segmentation</title><link>http://arxiv.org/abs/2312.04539v2</link><description>Open-ended image understanding tasks gained significant attention from theresearch community, particularly with the emergence of Vision-Language Models.Open-Vocabulary Segmentation (OVS) methods are capable of performing semanticsegmentation without relying on a fixed vocabulary, and in some cases, theyoperate without the need for training or fine-tuning. However, OVS methodstypically require users to specify the vocabulary based on the task or datasetat hand. In this paper, we introduce \textit{Auto-Vocabulary SemanticSegmentation (AVS)}, advancing open-ended image understanding by eliminatingthe necessity to predefine object categories for segmentation. Our approach,\ours, presents a framework that autonomously identifies relevant class namesusing enhanced BLIP embeddings, which are utilized for segmentation afterwards.Given that open-ended object category predictions cannot be directly comparedwith a fixed ground truth, we develop a Large Language Model-basedAuto-Vocabulary Evaluator (LAVE) to efficiently evaluate the automaticallygenerated class names and their corresponding segments. Our method sets newbenchmarks on datasets such as PASCAL VOC and Context, ADE20K, and Cityscapesfor AVS and showcases competitive performance to OVS methods that requirespecified class names.</description><author>Osman Ülger, Maksymilian Kulicki, Yuki Asano, Martin R. Oswald</author><pubDate>Wed, 20 Mar 2024 17:11:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04539v2</guid></item><item><title>Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study</title><link>http://arxiv.org/abs/2404.07060v1</link><description>We present an empirical study of groundedness in long-form question answering(LFQA) by retrieval-augmented large language models (LLMs). In particular, weevaluate whether every generated sentence is grounded in the retrieveddocuments or the model's pre-training data. Across 3 datasets and 4 modelfamilies, our findings reveal that a significant fraction of generatedsentences are consistently ungrounded, even when those sentences containcorrect ground-truth answers. Additionally, we examine the impacts of factorssuch as model size, decoding strategy, and instruction tuning on groundedness.Our results show that while larger models tend to ground their outputs moreeffectively, a significant portion of correct answers remains compromised byhallucinations. This study provides novel insights into the groundednesschallenges in LFQA and underscores the necessity for more robust mechanisms inLLMs to mitigate the generation of ungrounded content.</description><author>Alessandro Stolfo</author><pubDate>Wed, 10 Apr 2024 15:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07060v1</guid></item><item><title>Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2405.14314v1</link><description>Grounding the reasoning ability of large language models (LLMs) for embodiedtasks is challenging due to the complexity of the physical world. Especially,LLM planning for multi-agent collaboration requires communication of agents orcredit assignment as the feedback to re-adjust the proposed plans and achieveeffective coordination. However, existing methods that overly rely on physicalverification or self-reflection suffer from excessive and inefficient queryingof LLMs. In this paper, we propose a novel framework for multi-agentcollaboration that introduces Reinforced Advantage feedback (ReAd) forefficient self-refinement of plans. Specifically, we perform critic regressionto learn a sequential advantage function from LLM-planned data, and then treatthe LLM planner as an optimizer to generate actions that maximize the advantagefunction. It endows the LLM with the foresight to discern whether the actioncontributes to accomplishing the final task. We provide theoretical analysis byextending advantage-weighted regression in reinforcement learning tomulti-agent systems. Experiments on Overcooked-AI and a difficult variant ofRoCoBench show that ReAd surpasses baselines in success rate, and alsosignificantly decreases the interaction steps of agents and query rounds ofLLMs, demonstrating its high efficiency for grounding LLMs. More results aregiven at \url{https://read-llm.github.io/}.</description><author>Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong Li, Zhen Wang</author><pubDate>Thu, 23 May 2024 09:33:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14314v1</guid></item><item><title>VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning</title><link>http://arxiv.org/abs/2404.07078v1</link><description>Recognising emotions in context involves identifying the apparent emotions ofan individual, taking into account contextual cues from the surrounding scene.Previous approaches to this task have involved the design of explicitscene-encoding architectures or the incorporation of external scene-relatedinformation, such as captions. However, these methods often utilise limitedcontextual information or rely on intricate training pipelines. In this work,we leverage the groundbreaking capabilities of Vision-and-Large-Language Models(VLLMs) to enhance in-context emotion classification without introducingcomplexity to the training process in a two-stage approach. In the first stage,we propose prompting VLLMs to generate descriptions in natural language of thesubject's apparent emotion relative to the visual context. In the second stage,the descriptions are used as contextual information and, along with the imageinput, are used to train a transformer-based architecture that fuses text andvisual features before the final classification task. Our experimental resultsshow that the text and image features have complementary information, and ourfused architecture significantly outperforms the individual modalities withoutany complex training methods. We evaluate our approach on three differentdatasets, namely, EMOTIC, CAER-S, and BoLD, and achieve state-of-the-art orcomparable accuracy across all datasets and metrics compared to much morecomplex approaches. The code will be made publicly available on github:https://github.com/NickyFot/EmoCommonSense.git</description><author>Alexandros Xenos, Niki Maria Foteinopoulou, Ioanna Ntinou, Ioannis Patras, Georgios Tzimiropoulos</author><pubDate>Wed, 10 Apr 2024 16:09:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07078v1</guid></item><item><title>ElicitationGPT: Text Elicitation Mechanisms via Language Models</title><link>http://arxiv.org/abs/2406.09363v1</link><description>Scoring rules evaluate probabilistic forecasts of an unknown state againstthe realized state and are a fundamental building block in the incentivizedelicitation of information and the training of machine learning models. Thispaper develops mechanisms for scoring elicited text against ground truth textusing domain-knowledge-free queries to a large language model (specificallyChatGPT) and empirically evaluates their alignment with human preferences. Theempirical evaluation is conducted on peer reviews from a peer-grading datasetand in comparison to manual instructor scores for the peer reviews.</description><author>Yifan Wu, Jason Hartline</author><pubDate>Thu, 13 Jun 2024 18:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09363v1</guid></item><item><title>Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment</title><link>http://arxiv.org/abs/2312.09625v2</link><description>Learning to ground natural language queries to target objects or regions in3D point clouds is quite essential for 3D scene understanding. Nevertheless,existing 3D visual grounding approaches require a substantial number ofbounding box annotations for text queries, which is time-consuming andlabor-intensive to obtain. In this paper, we propose \textbf{3D-VLA}, a weaklysupervised approach for \textbf{3D} visual grounding based on \textbf{V}isual\textbf{L}inguistic \textbf{A}lignment. Our 3D-VLA exploits the superiorability of current large-scale vision-language models (VLMs) on aligning thesemantics between texts and 2D images, as well as the naturally existingcorrespondences between 2D images and 3D point clouds, and thus implicitlyconstructs correspondences between texts and 3D point clouds with no need forfine-grained box annotations in the training procedure. During the inferencestage, the learned text-3D correspondence will help us ground the text queriesto the 3D target objects even without 2D images. To the best of our knowledge,this is the first work to investigate 3D visual grounding in a weaklysupervised manner by involving large scale vision-language models, andextensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our3D-VLA achieves comparable and even superior results over the fully supervisedmethods.</description><author>Xiaoxu Xu, Yitian Yuan, Qiudan Zhang, Wenhui Wu, Zequn Jie, Lin Ma, Xu Wang</author><pubDate>Sat, 13 Apr 2024 09:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09625v2</guid></item><item><title>Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization</title><link>http://arxiv.org/abs/2403.18120v1</link><description>Large language models (LLM), such as Google's Minerva and OpenAI's GPTfamilies, are becoming increasingly capable of solving mathematicalquantitative reasoning problems. However, they still make unjustified logicaland computational errors in their reasoning steps and answers. In this paper,we leverage the fact that if the training corpus of LLMs contained sufficientlymany examples of formal mathematics (e.g. in Isabelle, a formal theorem provingenvironment), they can be prompted to translate i.e. autoformalize informalmathematical statements into formal Isabelle code -- which can be verifiedautomatically for internal consistency. This provides a mechanism toautomatically reject solutions whose formalized versions are inconsistentwithin themselves or with the formalized problem statement. We evaluate ourmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approachprovides a consistently better heuristic than vanilla majority voting -- thepreviously best method to identify correct answers, by more than 12% on GSM8K.In our experiments it improves results consistently across all datasets and LLMmodel sizes. The code can be found at https://github.com/jinpz/dtv.</description><author>Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu</author><pubDate>Tue, 26 Mar 2024 23:01:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18120v1</guid></item><item><title>Large Language Models can Contrastively Refine their Generation for Better Sentence Representation Learning</title><link>http://arxiv.org/abs/2310.10962v2</link><description>Recently, large language models (LLMs) have emerged as a groundbreakingtechnology and their unparalleled text generation capabilities have sparkedinterest in their application to the fundamental sentence representationlearning task. Existing methods have explored utilizing LLMs as data annotatorsto generate synthesized data for training contrastive learning based sentenceembedding models such as SimCSE. However, since contrastive learning models aresensitive to the quality of sentence pairs, the effectiveness of these methodsis largely influenced by the content generated from LLMs, highlighting the needfor more refined generation in the context of sentence representation learning.Building upon this premise, we propose MultiCSR, a multi-level contrastivesentence representation learning framework that decomposes the process ofprompting LLMs to generate a corpus for training base sentence embedding modelsinto three stages (i.e., sentence generation, sentence pair construction,in-batch training) and refines the generated content at these three distinctstages, ensuring only high-quality sentence pairs are utilized to train a basecontrastive learning model. Our extensive experiments reveal that MultiCSRenables a less advanced LLM to surpass the performance of ChatGPT, whileapplying it to ChatGPT achieves better state-of-the-art results. Comprehensiveanalyses further underscore the potential of our framework in variousapplication scenarios and achieving better sentence representation learningwith LLMs.</description><author>Huiming Wang, Zhaodonghui Li, Liying Cheng, Soh De Wen, Lidong Bing</author><pubDate>Fri, 17 May 2024 07:47:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10962v2</guid></item><item><title>MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation</title><link>http://arxiv.org/abs/2306.15253v4</link><description>Humans talk in daily conversations while aligning and negotiating theexpressed meanings or common ground. Despite the impressive conversationalabilities of the large generative language models, they do not consider theindividual differences in contextual understanding in a shared situatedenvironment. In this work, we propose MindDial, a novel conversationalframework that can generate situated free-form responses with theory-of-mindmodeling. We introduce an explicit mind module that can track the speaker'sbelief and the speaker's prediction of the listener's belief. Then the nextresponse is generated to resolve the belief difference and take task-relatedaction. Our framework is applied to both prompting and fine-tuning-basedmodels, and is evaluated across scenarios involving both common groundalignment and negotiation. Experiments show that models with mind modeling canachieve higher task outcomes when aligning and negotiating common ground. Theablation study further validates the three-level belief design can aggregateinformation and improve task outcomes in both cooperative and negotiatingsettings.</description><author>Shuwen Qiu, Mingdian Liu, Hengli Li, Song-Chun Zhu, Zilong Zheng</author><pubDate>Fri, 24 May 2024 08:46:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15253v4</guid></item><item><title>A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation</title><link>http://arxiv.org/abs/2404.03491v1</link><description>Empowered by the large-scale pretrained language models, existing dialoguesystems have demonstrated impressive performance conducting fluent andnatural-sounding conversations. However, they are still plagued by thehallucination problem, causing unpredictable factual errors in the generatedresponses. Recently, knowledge-grounded dialogue generation models, thatintentionally invoke external knowledge resources to more informativeresponses, are also proven to be effective in reducing hallucination. Followingthe idea of getting high-quality knowledge, a few efforts have achieved prettygood performance on this issue. As some inevitable knowledge noises may alsolead to hallucinations, it is emergent to investigate the reason and futuredirections for building noise-tolerant methods in KGD tasks. In this paper, weanalyze the causal story behind this problem with counterfactual reasoningmethods. Based on the causal effect analysis, we propose a possible solutionfor alleviating the hallucination in KGD by exploiting the dialogue-knowledgeinteraction. Experimental results of our example implementation show that thismethod can reduce hallucination without disrupting other dialogue performance,while keeping adaptive to different generation models. We hope our efforts cansupport and call for more attention to developing lightweight techniquestowards robust and trusty dialogue systems.</description><author>Jifan Yu, Xiaohan Zhang, Yifan Xu, Xuanyu Lei, Zijun Yao, Jing Zhang, Lei Hou, Juanzi Li</author><pubDate>Thu, 04 Apr 2024 15:45:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03491v1</guid></item><item><title>Grounding Language Plans in Demonstrations Through Counterfactual Perturbations</title><link>http://arxiv.org/abs/2403.17124v2</link><description>Grounding the common-sense reasoning of Large Language Models (LLMs) inphysical domains remains a pivotal yet unsolved problem for embodied AI.Whereas prior works have focused on leveraging LLMs directly for planning insymbolic spaces, this work uses LLMs to guide the search of task structures andconstraints implicit in multi-step demonstrations. Specifically, we borrow frommanipulation planning literature the concept of mode families, which grouprobot configurations by specific motion constraints, to serve as an abstractionlayer between the high-level language representations of an LLM and thelow-level physical trajectories of a robot. By replaying a few humandemonstrations with synthetic perturbations, we generate coverage over thedemonstrations' state space with additional successful executions as well ascounterfactuals that fail the task. Our explanation-based learning frameworktrains an end-to-end differentiable neural network to predict successfultrajectories from failures and as a by-product learns classifiers that groundlow-level states and images in mode families without dense labeling. Thelearned grounding classifiers can further be used to translate language plansinto reactive policies in the physical domain in an interpretable manner. Weshow our approach improves the interpretability and reactivity of imitationlearning through 2D navigation and simulated and real robot manipulation tasks.Website: https://yanweiw.github.io/glide</description><author>Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah</author><pubDate>Mon, 29 Apr 2024 05:34:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17124v2</guid></item><item><title>Large Language Models Cannot Self-Correct Reasoning Yet</title><link>http://arxiv.org/abs/2310.01798v2</link><description>Large Language Models (LLMs) have emerged as a groundbreaking technology withtheir unparalleled text generation capabilities across various applications.Nevertheless, concerns persist regarding the accuracy and appropriateness oftheir generated content. A contemporary methodology, self-correction, has beenproposed as a remedy to these issues. Building upon this premise, this papercritically examines the role and efficacy of self-correction within LLMs,shedding light on its true potential and limitations. Central to ourinvestigation is the notion of intrinsic self-correction, whereby an LLMattempts to correct its initial responses based solely on its inherentcapabilities, without the crutch of external feedback. In the context ofreasoning, our research indicates that LLMs struggle to self-correct theirresponses without external feedback, and at times, their performance evendegrades after self-correction. Drawing from these insights, we offersuggestions for future research and practical applications in this field.</description><author>Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, Denny Zhou</author><pubDate>Thu, 14 Mar 2024 05:27:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01798v2</guid></item><item><title>Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units</title><link>http://arxiv.org/abs/2403.16609v1</link><description>Successful conversations often rest on common understanding, where allparties are on the same page about the information being shared. This process,known as conversational grounding, is crucial for building trustworthy dialogsystems that can accurately keep track of and recall the shared information.The proficiencies of an agent in grounding the conveyed informationsignificantly contribute to building a reliable dialog system. Despite recentadvancements in dialog systems, there exists a noticeable deficit in theirgrounding capabilities. Traum provided a framework for conversational groundingintroducing Grounding Acts and Grounding Units, but substantial progress,especially in the realm of Large Language Models, remains lacking. To bridgethis gap, we present the annotation of two dialog corpora employing GroundingActs, Grounding Units, and a measure of their degree of grounding. We discussour key findings during the annotation and also provide a baseline model totest the performance of current Language Models in categorizing the groundingacts of the dialogs. Our work aims to provide a useful resource for furtherresearch in making conversations with machines better understood and morereliable in natural day-to-day collaborative dialogs.</description><author>Biswesh Mohapatra, Seemab Hassan, Laurent Romary, Justine Cassell</author><pubDate>Mon, 25 Mar 2024 11:39:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16609v1</guid></item><item><title>Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps</title><link>http://arxiv.org/abs/2307.05052v3</link><description>We investigate the role of various demonstration components in the in-contextlearning (ICL) performance of large language models (LLMs). Specifically, weexplore the impacts of ground-truth labels, input distribution, andcomplementary explanations, particularly when these are altered or perturbed.We build on previous work, which offers mixed findings on how these elementsinfluence ICL. To probe these questions, we employ explainable NLP (XNLP)methods and utilize saliency maps of contrastive demonstrations for bothqualitative and quantitative analysis. Our findings reveal that flippingground-truth labels significantly affects the saliency, though it's morenoticeable in larger LLMs. Our analysis of the input distribution at a granularlevel reveals that changing sentiment-indicative terms in a sentiment analysistask to neutral ones does not have as substantial an impact as alteringground-truth labels. Finally, we find that the effectiveness of complementaryexplanations in boosting ICL performance is task-dependent, with limitedbenefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.These insights are critical for understanding the functionality of LLMs andguiding the development of effective demonstrations, which is increasinglyrelevant in light of the growing use of LLMs in applications such as ChatGPT.Our research code is publicly available at https://github.com/paihengxu/XICL.</description><author>Fuxiao Liu, Paiheng Xu, Zongxia Li, Yue Feng</author><pubDate>Mon, 15 Apr 2024 16:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05052v3</guid></item><item><title>Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps</title><link>http://arxiv.org/abs/2307.05052v4</link><description>We investigate the role of various demonstration components in the in-contextlearning (ICL) performance of large language models (LLMs). Specifically, weexplore the impacts of ground-truth labels, input distribution, andcomplementary explanations, particularly when these are altered or perturbed.We build on previous work, which offers mixed findings on how these elementsinfluence ICL. To probe these questions, we employ explainable NLP (XNLP)methods and utilize saliency maps of contrastive demonstrations for bothqualitative and quantitative analysis. Our findings reveal that flippingground-truth labels significantly affects the saliency, though it's morenoticeable in larger LLMs. Our analysis of the input distribution at a granularlevel reveals that changing sentiment-indicative terms in a sentiment analysistask to neutral ones does not have as substantial an impact as alteringground-truth labels. Finally, we find that the effectiveness of complementaryexplanations in boosting ICL performance is task-dependent, with limitedbenefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.These insights are critical for understanding the functionality of LLMs andguiding the development of effective demonstrations, which is increasinglyrelevant in light of the growing use of LLMs in applications such as ChatGPT.Our research code is publicly available at https://github.com/paihengxu/XICL.</description><author>Fuxiao Liu, Paiheng Xu, Zongxia Li, Yue Feng, Hyemi Song</author><pubDate>Fri, 26 Apr 2024 02:18:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05052v4</guid></item><item><title>SwissNYF: Tool Grounded LLM Agents for Black Box Setting</title><link>http://arxiv.org/abs/2402.10051v1</link><description>While Large Language Models (LLMs) have demonstrated enhanced capabilities infunction-calling, these advancements primarily rely on accessing the functions'responses. This methodology is practical for simpler APIs but faces scalabilityissues with irreversible APIs that significantly impact the system, such as adatabase deletion API. Similarly, processes requiring extensive time for eachAPI call and those necessitating forward planning, like automated actionpipelines, present complex challenges. Furthermore, scenarios often arise wherea generalized approach is needed because algorithms lack direct access to thespecific implementations of these functions or secrets to use them. Traditionaltool planning methods are inadequate in these cases, compelling the need tooperate within black-box environments. Unlike their performance in toolmanipulation, LLMs excel in black-box tasks, such as program synthesis.Therefore, we harness the program synthesis capabilities of LLMs to strategizetool usage in black-box settings, ensuring solutions are verified prior toimplementation. We introduce TOPGUN, an ingeniously crafted approach leveragingprogram synthesis for black box tool planning. Accompanied by SwissNYF, acomprehensive suite that integrates black-box algorithms for planning andverification tasks, addressing the aforementioned challenges and enhancing theversatility and effectiveness of LLMs in complex API interactions. The publiccode for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.</description><author>Somnath Sendhil Kumar, Dhruv Jain, Eshaan Agarwal, Raunak Pandey</author><pubDate>Thu, 15 Feb 2024 16:15:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10051v1</guid></item><item><title>Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models</title><link>http://arxiv.org/abs/2405.19326v1</link><description>In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentationfor parts searching and localization for objects, which is a new paradigm to 3Dsegmentation that transcends limitations for previous category-specific 3Dsemantic segmentation, 3D instance segmentation, and open-vocabulary 3Dsegmentation. We design a simple baseline method, Reasoning3D, with thecapability to understand and execute complex commands for (fine-grained)segmenting specific parts for 3D meshes with contextual awareness and reasonedanswers for interactive segmentation. Specifically, Reasoning3D leverages anoff-the-shelf pre-trained 2D segmentation network, powered by Large LanguageModels (LLMs), to interpret user input queries in a zero-shot manner. Previousresearch have shown that extensive pre-training endows foundation models withprior world knowledge, enabling them to comprehend complex commands, acapability we can harness to "segment anything" in 3D with limited 3D datasets(source efficient). Experimentation reveals that our approach is generalizableand can effectively localize and highlight parts of 3D objects (in 3D mesh)based on implicit textual queries, including these articulated 3d objects andreal-world scanned data. Our method can also generate natural languageexplanations corresponding to these 3D models and the decomposition. Moreover,our training-free approach allows rapid deployment and serves as a viableuniversal baseline for future research of part-level 3d (semantic) objectunderstanding in various fields including robotics, object manipulation, partassembly, autonomous driving applications, augment reality and virtual reality(AR/VR), and medical applications. The code, the model weight, the deploymentguide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/</description><author>Tianrun Chen, Chunan Yu, Jing Li, Jianqi Zhang, Lanyun Zhu, Deyi Ji, Yong Zhang, Ying Zang, Zejian Li, Lingyun Sun</author><pubDate>Wed, 29 May 2024 18:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19326v1</guid></item><item><title>ComplexTempQA: A Large-Scale Dataset for Complex Temporal Question Answering</title><link>http://arxiv.org/abs/2406.04866v1</link><description>We introduce ComplexTempQA,a large-scale dataset consisting of over 100million question-answer pairs designed to tackle the challenges in temporalquestion answering. ComplexTempQA significantly surpasses existing benchmarkslike HOTPOTQA, TORQUE, and TEQUILA in scale and scope. Utilizing data fromWikipedia and Wikidata, the dataset covers questions spanning over two decadesand offers an unmatched breadth of topics. We introduce a unique taxonomy thatcategorizes questions as attributes, comparisons, and counting questions, eachrevolving around events, entities, and time periods. One standout feature ofComplexTempQA is the high complexity of its questions, which demand effectivecapabilities for answering such as across-time comparison, temporalaggregation, and multi-hop reasoning involving temporal event ordering andentity recognition. Additionally, each question is accompanied by detailedmetadata, including specific time scopes, allowing for comprehensive evaluationand enhancement of the temporal reasoning abilities of large language models.ComplexTempQA serves both as a testing ground for developing sophisticated AImodels and as a foundation for advancing research in question answering,information retrieval, and language understanding. Dataset and code are freelyavailable at: https://github.com/DataScienceUIBK/ComplexTempQA.</description><author>Raphael Gruber, Abdelrahman Abdallah, Michael Färber, Adam Jatowt</author><pubDate>Fri, 07 Jun 2024 13:01:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04866v1</guid></item><item><title>GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment</title><link>http://arxiv.org/abs/2403.11075v1</link><description>Verbal communication plays a crucial role in human cooperation, particularlywhen the partners only have incomplete information about the task, environment,and each other's mental state. In this paper, we propose a novel cooperativecommunication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulatesverbal communication as a planning problem that minimizes the misalignmentbetween the parts of agents' mental states that are relevant to the goals. Thisapproach enables an embodied assistant to reason about when and how toproactively initialize communication with humans verbally using naturallanguage to help achieve better cooperation. We evaluate our approach againststrong baselines in two challenging environments, Overcooked (a multiplayergame) and VirtualHome (a household simulator). Our experimental resultsdemonstrate that large language models struggle with generating meaningfulcommunication that is grounded in the social and physical context. In contrast,our approach can successfully generate concise verbal communication for theembodied assistant to effectively boost the performance of the cooperation aswell as human users' perception of the assistant.</description><author>Lance Ying, Kunal Jha, Shivam Aarya, Joshua B. Tenenbaum, Antonio Torralba, Tianmin Shu</author><pubDate>Sun, 17 Mar 2024 04:52:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11075v1</guid></item><item><title>Exploring ChatGPT and its Impact on Society</title><link>http://arxiv.org/abs/2403.14643v2</link><description>Artificial intelligence has been around for a while, but suddenly it hasreceived more attention than ever before. Thanks to innovations from companieslike Google, Microsoft, Meta, and other major brands in technology. OpenAI,though, has triggered the button with its ground-breaking invention ChatGPT.ChatGPT is a Large Language Model (LLM) based on Transformer architecture thathas the ability to generate human-like responses in a conversational context.It uses deep learning algorithms to generate natural language responses toinput text. Its large number of parameters, contextual generation, andopen-domain training make it a versatile and effective tool for a wide range ofapplications, from chatbots to customer service to language translation. It hasthe potential to revolutionize various industries and transform the way weinteract with technology. However, the use of ChatGPT has also raised severalconcerns, including ethical, social, and employment challenges, which must becarefully considered to ensure the responsible use of this technology. Thearticle provides an overview of ChatGPT, delving into its architecture andtraining process. It highlights the potential impacts of ChatGPT on thesociety. In this paper, we suggest some approaches involving technology,regulation, education, and ethics in an effort to maximize ChatGPT's benefitswhile minimizing its negative impacts. This study is expected to contribute toa greater understanding of ChatGPT and aid in predicting the potential changesit may bring about.</description><author>Md. Asraful Haque, Shuai Li</author><pubDate>Mon, 25 Mar 2024 06:35:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14643v2</guid></item><item><title>Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts</title><link>http://arxiv.org/abs/2405.01121v2</link><description>Automating data generation with Large Language Models (LLMs) has becomeincreasingly popular. In this work, we investigate the feasibility andeffectiveness of LLM-based data generation in the challenging setting ofsource-grounded information-seeking dialogs, with response attribution, overlong documents. Our source texts consist of long and noisy meeting transcripts,adding to the task complexity. Since automating attribution remains difficult,we propose a semi-automatic approach: dialog queries and responses aregenerated with LLMs, followed by human verification and identification ofattribution spans. Using this approach, we created MISeD -- Meeting InformationSeeking Dialogs dataset -- a dataset of information-seeking dialogs focused onmeeting transcripts. Models finetuned with MISeD demonstrate superiorperformance compared to off-the-shelf models, even those of larger size.Finetuning on MISeD gives comparable response generation quality to finetuningon fully manual data, while improving attribution quality and reducing time andeffort.</description><author>Lotem Golany, Filippo Galgani, Maya Mamo, Nimrod Parasol, Omer Vandsburger, Nadav Bar, Ido Dagan</author><pubDate>Fri, 21 Jun 2024 10:10:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.01121v2</guid></item><item><title>DOrA: 3D Visual Grounding with Order-Aware Referring</title><link>http://arxiv.org/abs/2403.16539v1</link><description>3D visual grounding aims to identify the target object within a 3D pointcloud scene referred to by a natural language description. While previous worksattempt to exploit the verbo-visual relation with proposed cross-modaltransformers, unstructured natural utterances and scattered objects might leadto undesirable performances. In this paper, we introduce DOrA, a novel 3Dvisual grounding framework with Order-Aware referring. DOrA is designed toleverage Large Language Models (LLMs) to parse language description, suggestinga referential order of anchor objects. Such ordered anchor objects allow DOrAto update visual features and locate the target object during the groundingprocess. Experimental results on the NR3D and ScanRefer datasets demonstrateour superiority in both low-resource and full-data scenarios. In particular,DOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% groundingaccuracy under 1% data and 10% data settings, respectively.</description><author>Tung-Yu Wu, Sheng-Yu Huang, Yu-Chiang Frank Wang</author><pubDate>Mon, 25 Mar 2024 09:31:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16539v1</guid></item><item><title>Toward Grounded Commonsense Reasoning</title><link>http://arxiv.org/abs/2306.08651v2</link><description>Consider a robot tasked with tidying a desk with a meticulously constructedLego sports car. A human may recognize that it is not appropriate todisassemble the sports car and put it away as part of the "tidying." How can arobot reach that conclusion? Although large language models (LLMs) haverecently been used to enable commonsense reasoning, grounding this reasoning inthe real world has been challenging. To reason in the real world, robots mustgo beyond passively querying LLMs and actively gather information from theenvironment that is required to make the right decision. For instance, afterdetecting that there is an occluded car, the robot may need to activelyperceive the car to know whether it is an advanced model car made out of Legosor a toy car built by a toddler. We propose an approach that leverages an LLMand vision language model (VLM) to help a robot actively perceive itsenvironment to perform grounded commonsense reasoning. To evaluate ourframework at scale, we release the MessySurfaces dataset which contains imagesof 70 real-world surfaces that need to be cleaned. We additionally illustrateour approach with a robot on 2 carefully designed surfaces. We find an average12.9% improvement on the MessySurfaces benchmark and an average 15% improvementon the robot experiments over baselines that do not use active perception. Thedataset, code, and videos of our approach can be found athttps://minaek.github.io/grounded_commonsense_reasoning.</description><author>Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan, Dorsa Sadigh</author><pubDate>Mon, 19 Feb 2024 02:39:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08651v2</guid></item><item><title>LEGENT: Open Platform for Embodied Agents</title><link>http://arxiv.org/abs/2404.18243v1</link><description>Despite advancements in Large Language Models (LLMs) and Large MultimodalModels (LMMs), their integration into language-grounded, human-like embodiedagents remains incomplete, hindering complex real-life task performance inphysical environments. Existing integrations often feature limited opensourcing, challenging collective progress in this field. We introduce LEGENT,an open, scalable platform for developing embodied agents using LLMs and LMMs.LEGENT offers a dual approach: a rich, interactive 3D environment withcommunicable and actionable agents, paired with a user-friendly interface, anda sophisticated data generation pipeline utilizing advanced algorithms toexploit supervision from simulated worlds at scale. In our experiments, anembryonic vision-language-action model trained on LEGENT-generated datasurpasses GPT-4V in embodied tasks, showcasing promising generalizationcapabilities.</description><author>Zhili Cheng, Zhitong Wang, Jinyi Hu, Shengding Hu, An Liu, Yuge Tu, Pengkai Li, Lei Shi, Zhiyuan Liu, Maosong Sun</author><pubDate>Sun, 28 Apr 2024 17:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18243v1</guid></item><item><title>3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination</title><link>http://arxiv.org/abs/2406.05132v2</link><description>The integration of language and 3D perception is crucial for developingembodied agents and robots that comprehend and interact with the physicalworld. While large language models (LLMs) have demonstrated impressive languageunderstanding and generation capabilities, their adaptation to 3D environments(3D-LLMs) remains in its early stages. A primary challenge is the absence oflarge-scale datasets that provide dense grounding between language and 3Dscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale datasetcomprising 40,087 household scenes paired with 6.2 million densely-groundedscene-language instructions. Our results show that instruction tuning with3D-GRAND significantly enhances grounding capabilities and reduceshallucinations in 3D-LLMs. As part of our contributions, we propose acomprehensive benchmark 3D-POPE to systematically evaluate hallucination in3D-LLMs, enabling fair comparisons among future models. Our experimentshighlight a scaling effect between dataset size and 3D-LLM performance,emphasizing the critical role of large-scale 3D-text datasets in advancingembodied AI research. Notably, our results demonstrate early signals foreffective sim-to-real transfer, indicating that models trained on largesynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and3D-POPE, we aim to equip the embodied AI community with essential resources andinsights, setting the stage for more reliable and better-grounded 3D-LLMs.Project website: https://3d-grand.github.io</description><author>Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F. Fouhey, Joyce Chai</author><pubDate>Wed, 12 Jun 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05132v2</guid></item><item><title>"Set It Up!": Functional Object Arrangement with Compositional Generative Models</title><link>http://arxiv.org/abs/2405.11928v1</link><description>This paper studies the challenge of developing robots capable ofunderstanding under-specified instructions for creating functional objectarrangements, such as "set up a dining table for two"; previous arrangementapproaches have focused on much more explicit instructions, such as "put objectA on the table." We introduce a framework, SetItUp, for learning to interpretunder-specified instructions. SetItUp takes a small number of training examplesand a human-crafted program sketch to uncover arrangement rules for specificscene types. By leveraging an intermediate graph-like representation ofabstract spatial relationships among objects, SetItUp decomposes thearrangement problem into two subproblems: i) learning the arrangement patternsfrom limited data and ii) grounding these abstract relationships into objectposes. SetItUp leverages large language models (LLMs) to propose the abstractspatial relationships among objects in novel scenes as the constraints to besatisfied; then, it composes a library of diffusion models associated withthese abstract relationships to find object poses that satisfy the constraints.We validate our framework on a dataset comprising study desks, dining tables,and coffee tables, with the results showing superior performance in generatingphysically plausible, functional, and aesthetically pleasing objectarrangements compared to existing models.</description><author>Yiqing Xu, Jiayuan Mao, Yilun Du, Tomas Lozáno-Pérez, Leslie Pack Kaebling, David Hsu</author><pubDate>Mon, 20 May 2024 11:06:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11928v1</guid></item><item><title>3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs</title><link>http://arxiv.org/abs/2406.05132v1</link><description>The integration of language and 3D perception is crucial for developingembodied agents and robots that comprehend and interact with the physicalworld. While large language models (LLMs) have demonstrated impressive languageunderstanding and generation capabilities, their adaptation to 3D environments(3D-LLMs) remains in its early stages. A primary challenge is the absence oflarge-scale datasets that provide dense grounding between language and 3Dscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale datasetcomprising 40,087 household scenes paired with 6.2 million densely-groundedscene-language instructions. Our results show that instruction tuning with3D-GRAND significantly enhances grounding capabilities and reduceshallucinations in 3D-LLMs. As part of our contributions, we propose acomprehensive benchmark 3D-POPE to systematically evaluate hallucination in3D-LLMs, enabling fair comparisons among future models. Our experimentshighlight a scaling effect between dataset size and 3D-LLM performance,emphasizing the critical role of large-scale 3D-text datasets in advancingembodied AI research. Notably, our results demonstrate early signals foreffective sim-to-real transfer, indicating that models trained on largesynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and3D-POPE, we aim to equip the embodied AI community with essential resources andinsights, setting the stage for more reliable and better-grounded 3D-LLMs.Project website: https://3d-grand.github.io</description><author>Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F. Fouhey, Joyce Chai</author><pubDate>Fri, 07 Jun 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05132v1</guid></item><item><title>Grounding Data Science Code Generation with Input-Output Specifications</title><link>http://arxiv.org/abs/2402.08073v2</link><description>Large language models (LLMs) have recently demonstrated a remarkable abilityto generate code from natural language (NL) prompts. However, in the realworld, NL is often too ambiguous to capture the true intent behind programmingproblems, requiring additional input-output (I/O) specifications.Unfortunately, LLMs can have difficulty aligning their outputs with both the NLprompt and the I/O specification. In this paper, we give a way to mitigate thisissue in the context of data science programming, where tasks require explicitI/O specifications for clarity. Specifically, we propose GIFT4Code, a novelapproach for the instruction fine-tuning of LLMs with respect to I/Ospecifications. Our method leverages synthetic data produced by the LLM itselfand utilizes execution-derived feedback as a key learning signal. Thisfeedback, in the form of program I/O specifications, is provided to the LLM tofacilitate instruction fine-tuning. We evaluated our approach on twochallenging data science benchmarks, Arcade and DS-1000. The resultsdemonstrate a significant improvement in the LLM's ability to generate codethat is not only executable but also accurately aligned with userspecifications, substantially improving the quality of code generation forcomplex data science tasks.</description><author>Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski, Swarat Chaudhuri, Alex Polozov</author><pubDate>Fri, 15 Mar 2024 02:18:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08073v2</guid></item><item><title>Attribute First, then Generate: Locally-attributable Grounded Text Generation</title><link>http://arxiv.org/abs/2403.17104v2</link><description>Recent efforts to address hallucinations in Large Language Models (LLMs) havefocused on attributed text generation, which supplements generated texts withcitations of supporting sources for post-generation fact-checking andcorrections. Yet, these citations often point to entire documents orparagraphs, burdening users with extensive verification work. In this paper, weintroduce a locally-attributable text generation approach, prioritizing conciseattributions. Our method, named ``Attribute First, then Generate'', breaks downthe conventional end-to-end generation process into three intuitive steps:content selection, sentence planning, and sequential sentence generation. Byinitially identifying relevant source segments (``select first'') and thenconditioning the generation process on them (``then generate''), we ensurethese segments also act as the output's fine-grained attributions (``select''becomes ``attribute''). Tested on Multi-document Summarization and Long-formQuestion-answering, our method not only yields more concise citations than thebaselines but also maintains - and in some cases enhances - both generationquality and attribution accuracy. Furthermore, it significantly reduces thetime required for fact verification by human assessors.</description><author>Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, Ido Dagan</author><pubDate>Mon, 01 Apr 2024 18:57:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17104v2</guid></item><item><title>Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs</title><link>http://arxiv.org/abs/2404.05719v1</link><description>Recent advancements in multimodal large language models (MLLMs) have beennoteworthy, yet, these general-domain MLLMs often fall short in their abilityto comprehend and interact effectively with user interface (UI) screens. Inthis paper, we present Ferret-UI, a new MLLM tailored for enhancedunderstanding of mobile UI screens, equipped with referring, grounding, andreasoning capabilities. Given that UI screens typically exhibit a moreelongated aspect ratio and contain smaller objects of interest (e.g., icons,texts) than natural images, we incorporate "any resolution" on top of Ferret tomagnify details and leverage enhanced visual features. Specifically, eachscreen is divided into 2 sub-images based on the original aspect ratio (i.e.,horizontal division for portrait screens and vertical division for landscapescreens). Both sub-images are encoded separately before being sent to LLMs. Wemeticulously gather training samples from an extensive range of elementary UItasks, such as icon recognition, find text, and widget listing. These samplesare formatted for instruction-following with region annotations to facilitateprecise referring and grounding. To augment the model's reasoning ability, wefurther compile a dataset for advanced tasks, including detailed description,perception/interaction conversations, and function inference. After training onthe curated datasets, Ferret-UI exhibits outstanding comprehension of UIscreens and the capability to execute open-ended instructions. For modelevaluation, we establish a comprehensive benchmark encompassing all theaforementioned tasks. Ferret-UI excels not only beyond most open-source UIMLLMs, but also surpasses GPT-4V on all the elementary UI tasks.</description><author>Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, Zhe Gan</author><pubDate>Mon, 08 Apr 2024 18:55:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05719v1</guid></item><item><title>Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering</title><link>http://arxiv.org/abs/2401.10711v3</link><description>Video Question Answering (VideoQA) aims to answer natural language questionsbased on the information observed in videos. Despite the recent success ofLarge Multimodal Models (LMMs) in image-language understanding and reasoning,they deal with VideoQA insufficiently, by simply taking uniformly sampledframes as visual inputs, which ignores question-relevant visual clues.Moreover, there are no human annotations for question-critical timestamps inexisting VideoQA datasets. In light of this, we propose a novel weaklysupervised framework to enforce the LMMs to reason out the answers withquestion-critical moments as visual inputs. Specifically, we first fuse thequestion and answer pairs as event descriptions to find multiple keyframes astarget moments and pseudo-labels, with the visual-language alignment capabilityof the CLIP models. With these pseudo-labeled keyframes as additionally weaksupervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG)module. GCG learns multiple Gaussian functions to characterize the temporalstructure of the video, and sample question-critical frames as positive momentsto be the visual inputs of LMMs. Extensive experiments on several benchmarksverify the effectiveness of our framework, and we achieve substantialimprovements compared to previous state-of-the-art methods.</description><author>Haibo Wang, Chenghang Lai, Yixuan Sun, Weifeng Ge</author><pubDate>Fri, 26 Apr 2024 10:38:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10711v3</guid></item><item><title>GrounDial: Human-norm Grounded Safe Dialog Response Generation</title><link>http://arxiv.org/abs/2402.08968v1</link><description>Current conversational AI systems based on large language models (LLMs) areknown to generate unsafe responses, agreeing to offensive user input orincluding toxic content. Previous research aimed to alleviate the toxicity, byfine-tuning LLM with manually annotated safe dialogue histories. However, thedependency on additional tuning requires substantial costs. To remove thedependency, we propose GrounDial, where response safety is achieved bygrounding responses to commonsense social rules without requiring fine-tuning.A hybrid approach of in-context learning and human-norm-guided decoding ofGrounDial enables the response to be quantitatively and qualitatively safereven without additional data or tuning.</description><author>Siwon Kim, Shuyang Dai, Mohammad Kachuee, Shayan Ray, Tara Taghavi, Sungroh Yoon</author><pubDate>Wed, 14 Feb 2024 06:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08968v1</guid></item><item><title>Learning from Models and Data for Visual Grounding</title><link>http://arxiv.org/abs/2403.13804v1</link><description>We introduce SynGround, a novel framework that combines data-driven learningand knowledge transfer from various large-scale pretrained models to enhancethe visual grounding capabilities of a pretrained vision-and-language model.The knowledge transfer from the models initiates the generation of imagedescriptions through an image description generator. These descriptions servedual purposes: they act as prompts for synthesizing images through atext-to-image generator, and as queries for synthesizing text, from whichphrases are extracted using a large language model. Finally, we leverage anopen-vocabulary object detector to generate synthetic bounding boxes for thesynthetic images and texts. We finetune a pretrained vision-and-language modelon this dataset by optimizing a mask-attention consistency objective thataligns region annotations with gradient-based model explanations. The resultingmodel improves the grounding capabilities of an off-the-shelfvision-and-language model. Particularly, SynGround improves the pointing gameaccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and onRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to63.67%.</description><author>Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez</author><pubDate>Wed, 20 Mar 2024 18:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13804v1</guid></item><item><title>Can We Use Large Language Models to Fill Relevance Judgment Holes?</title><link>http://arxiv.org/abs/2405.05600v1</link><description>Incomplete relevance judgments limit the re-usability of test collections.When new systems are compared against previous systems used to build the poolof judged documents, they often do so at a disadvantage due to the ``holes'' intest collection (i.e., pockets of un-assessed documents returned by the newsystem). In this paper, we take initial steps towards extending existing testcollections by employing Large Language Models (LLM) to fill the holes byleveraging and grounding the method using existing human judgments. We explorethis problem in the context of Conversational Search using TREC iKAT, whereinformation needs are highly dynamic and the responses (and, the resultsretrieved) are much more varied (leaving bigger holes). While previous work hasshown that automatic judgments from LLMs result in highly correlated rankings,we find substantially lower correlates when human plus automatic judgments areused (regardless of LLM, one/two/few shot, or fine-tuned). We further findthat, depending on the LLM employed, new runs will be highly favored (orpenalized), and this effect is magnified proportionally to the size of theholes. Instead, one should generate the LLM annotations on the whole documentpool to achieve more consistent rankings with human-generated labels. Futurework is required to prompt engineering and fine-tuning LLMs to reflect andrepresent the human annotations, in order to ground and align the models, suchthat they are more fit for purpose.</description><author>Zahra Abbasiantaeb, Chuan Meng, Leif Azzopardi, Mohammad Aliannejadi</author><pubDate>Thu, 09 May 2024 08:39:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05600v1</guid></item><item><title>Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations</title><link>http://arxiv.org/abs/2402.11770v2</link><description>We introduce a structured chain-of-thought (SCoT) prompting approach togenerating content-grounded multi-turn question-answer conversations using apre-trained large language model (LLM). At the core of our proposal is astructured breakdown of the complex task into a number of states in a statemachine, so that actions corresponding to various subtasks, e.g., contentreading and utterance generation, can be executed in their own dedicatedstates. Each state leverages a unique set of resources including prompts and(optionally) additional tools to augment the generation process. Ourexperimental results show that SCoT prompting with designated states forhallucination mitigation increases agent faithfulness to grounding documents byup to 16.8%. When used as training data, our open-domain conversationssynthesized from only 6 Wikipedia-based seed demonstrations train strongconversational QA agents; in out-of-domain evaluation, for example, we observeimprovements of up to 13.9% over target domain gold data when the latter isaugmented with our generated examples.</description><author>Md Arafat Sultan, Jatin Ganhotra, Ramón Fernandez Astudillo</author><pubDate>Tue, 20 Feb 2024 02:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11770v2</guid></item><item><title>Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations</title><link>http://arxiv.org/abs/2402.11770v1</link><description>We introduce a structured chain-of-thought (SCoT) prompting approach togenerating content-grounded multi-turn question-answer conversations using apre-trained large language model (LLM). At the core of our proposal is astructured breakdown of the complex task into a number of states in a statemachine, so that actions corresponding to various subtasks, e.g., contentreading and utterance generation, can be executed in their own dedicatedstates. Each state leverages a unique set of resources including prompts and(optionally) additional tools to augment the generation process. Ourexperimental results show that SCoT prompting with designated states forhallucination mitigation increases agent faithfulness to grounding documents byup to 16.8%. When used as training data, our open-domain conversationssynthesized from only 6 Wikipedia-based seed demonstrations train strongconversational QA agents; in out-of-domain evaluation, for example, we observeimprovements of up to 13.9% over target domain gold data when the latter isaugmented with our generated examples.</description><author>Md Arafat Sultan, Jatin Ganhotra, Ramón Fernandez Astudillo</author><pubDate>Mon, 19 Feb 2024 01:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11770v1</guid></item><item><title>Explainable Traffic Flow Prediction with Large Language Models</title><link>http://arxiv.org/abs/2404.02937v1</link><description>Traffic flow prediction provides essential future views in the intelligenttransportation system. Explainable predictions offer valuable insights into thefactors influencing traffic patterns, which help urban planners, trafficengineers, and policymakers make informed decisions about infrastructuredevelopment, traffic management strategies, and public transportation planning.Despite their widespread popularity and commendable accuracy, predictionmethods grounded in deep learning frequently disappoint in terms oftransparency and interpretability. Recently, the availability of large-scalespatio-temporal data and the development of large language models (LLMs) haveopened up new opportunities for urban traffic prediction. With the popularityof LLMs, people witnessed the potential reasoning and generating ability offoundation models in various tasks. Considering text as input and output, LLMshave advantages in generating more intuitive and interpretable predictions.Hence, this work introduces TP-LLM, an explainable foundation-model-basedmethod for traffic prediction, aiming at more direct and reasonableforecasting. TP-LLM presents a framework to unify multi-modality factors aslanguage-based inputs, TP-LLM avoids complex spatial-temporal data programmingand outperforms state-of-art baselines merely under fine-tuning foundationmodels. Also, TP-LLM can generate input-dependency explanations for moreconfident prediction and can be easily generalized to different city dynamicsfor zero-shot prediction with a similar framework. These findings demonstratethe potential of LLMs for explainable traffic prediction.</description><author>Xusen Guo, Qiming Zhang, Mingxing Peng, Meixin Zhua, Hao, Yang</author><pubDate>Wed, 03 Apr 2024 08:14:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02937v1</guid></item><item><title>LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision</title><link>http://arxiv.org/abs/2304.07647v4</link><description>We propose LASER, a neuro-symbolic approach to learn semantic videorepresentations that capture rich spatial and temporal properties in video databy leveraging high-level logic specifications. In particular, we formulate theproblem in terms of alignment between raw videos and spatio-temporal logicspecifications. The alignment algorithm leverages a differentiable symbolicreasoner and a combination of contrastive, temporal, and semantics losses. Iteffectively and efficiently trains low-level perception models to extract afine-grained video representation in the form of a spatio-temporal scene graphthat conforms to the desired high-level specification. To practically reducethe manual effort of obtaining ground truth labels, we derive logicspecifications from captions by employing a large language model with a genericprompting template. In doing so, we explore a novel methodology that weaklysupervises the learning of spatio-temporal scene graphs with widely accessiblevideo-caption data. We evaluate our method on three datasets with rich spatialand temporal specifications: 20BN-Something-Something, MUGEN, and OpenPVSG. Wedemonstrate that our method learns better fine-grained video semantics thanexisting baselines.</description><author>Jiani Huang, Ziyang Li, Mayur Naik, Ser-Nam Lim</author><pubDate>Wed, 12 Jun 2024 18:16:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07647v4</guid></item><item><title>Generating Human Motion in 3D Scenes from Text Descriptions</title><link>http://arxiv.org/abs/2405.07784v1</link><description>Generating human motions from textual descriptions has gained growingresearch interest due to its wide range of applications. However, only a fewworks consider human-scene interactions together with text conditions, which iscrucial for visual and physical realism. This paper focuses on the task ofgenerating human motions in 3D indoor scenes given text descriptions of thehuman-scene interactions. This task presents challenges due to themulti-modality nature of text, scene, and motion, as well as the need forspatial reasoning. To address these challenges, we propose a new approach thatdecomposes the complex problem into two more manageable sub-problems: (1)language grounding of the target object and (2) object-centric motiongeneration. For language grounding of the target object, we leverage the powerof large language models. For motion generation, we design an object-centricscene representation for the generative model to focus on the target object,thereby reducing the scene complexity and facilitating the modeling of therelationship between human motions and the object. Experiments demonstrate thebetter motion quality of our approach compared to baselines and validate ourdesign choices.</description><author>Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, Xiaowei Zhou</author><pubDate>Mon, 13 May 2024 15:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07784v1</guid></item><item><title>Solution for SMART-101 Challenge of CVPR Multi-modal Algorithmic Reasoning Task 2024</title><link>http://arxiv.org/abs/2406.05963v1</link><description>In this paper, the solution of HYU MLLAB KT Team to the MultimodalAlgorithmic Reasoning Task: SMART-101 CVPR 2024 Challenge is presented. Beyondconventional visual question-answering problems, the SMART-101 challenge aimsto achieve human-level multimodal understanding by tackling complexvisio-linguistic puzzles designed for children in the 6-8 age group. To solvethis problem, we suggest two main ideas. First, to utilize the reasoningability of a large-scale language model (LLM), the given visual cues (images)are grounded in the text modality. For this purpose, we generate highlydetailed text captions that describe the context of the image and use thesecaptions as input for the LLM. Second, due to the nature of puzzle images,which often contain various geometric visual patterns, we utilize an objectdetection algorithm to ensure these patterns are not overlooked in thecaptioning process. We employed the SAM algorithm, which can detectvarious-size objects, to capture the visual features of these geometricpatterns and used this information as input for the LLM. Under the puzzle splitconfiguration, we achieved an option selection accuracy Oacc of 29.5 on thetest set and a weighted option selection accuracy (WOSA) of 27.1 on thechallenge set.</description><author>Jinwoo Ahn, Junhyeok Park, Min-Jun Kim, Kang-Hyeon Kim, So-Yeong Sohn, Yun-Ji Lee, Du-Seong Chang, Yu-Jung Heo, Eun-Sol Kim</author><pubDate>Mon, 10 Jun 2024 02:45:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05963v1</guid></item><item><title>Automated Data Visualization from Natural Language via Large Language Models: An Exploratory Study</title><link>http://arxiv.org/abs/2404.17136v1</link><description>The Natural Language to Visualization (NL2Vis) task aims to transformnatural-language descriptions into visual representations for a grounded table,enabling users to gain insights from vast amounts of data. Recently, many deeplearning-based approaches have been developed for NL2Vis. Despite theconsiderable efforts made by these approaches, challenges persist invisualizing data sourced from unseen databases or spanning multiple tables.Taking inspiration from the remarkable generation capabilities of LargeLanguage Models (LLMs), this paper conducts an empirical study to evaluatetheir potential in generating visualizations, and explore the effectiveness ofin-context learning prompts for enhancing this task. In particular, we firstexplore the ways of transforming structured tabular data into sequential textprompts, as to feed them into LLMs and analyze which table content contributesmost to the NL2Vis. Our findings suggest that transforming structured tabulardata into programs is effective, and it is essential to consider the tableschema when formulating prompts. Furthermore, we evaluate two types of LLMs:finetuned models (e.g., T5-Small) and inference-only models (e.g., GPT-3.5),against state-of-the-art methods, using the NL2Vis benchmarks (i.e., nvBench).The experimental results reveal that LLMs outperform baselines, withinference-only models consistently exhibiting performance improvements, attimes even surpassing fine-tuned models when provided with certain few-shotdemonstrations through in-context learning. Finally, we analyze when the LLMsfail in NL2Vis, and propose to iteratively update the results using strategiessuch as chain-of-thought, role-playing, and code-interpreter. The experimentalresults confirm the efficacy of iterative updates and hold great potential forfuture study.</description><author>Yang Wu, Yao Wan, Hongyu Zhang, Yulei Sui, Wucai Wei, Wei Zhao, Guandong Xu, Hai Jin</author><pubDate>Fri, 26 Apr 2024 04:25:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17136v1</guid></item><item><title>LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding</title><link>http://arxiv.org/abs/2402.16050v1</link><description>Despite progress in video-language modeling, the computational challenge ofinterpreting long-form videos in response to task-specific linguistic queriespersists, largely due to the complexity of high-dimensional video data and themisalignment between language and visual cues over space and time. To tacklethis issue, we introduce a novel approach called Language-guidedSpatial-Temporal Prompt Learning (LSTP). This approach features two keycomponents: a Temporal Prompt Sampler (TPS) with optical flow prior thatleverages temporal information to efficiently extract relevant video content,and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatialrelationships between visual and textual elements. By harmonizing TPS and SPSwith a cohesive training strategy, our framework significantly enhancescomputational efficiency, temporal understanding, and spatial-temporalalignment. Empirical evaluations across two challenging tasks--video questionanswering and temporal question grounding in videos--using a variety ofvideo-language pretrainings (VLPs) and large language models (LLMs) demonstratethe superior performance, speed, and versatility of our proposed LSTP paradigm.</description><author>Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Zilong Zheng</author><pubDate>Sun, 25 Feb 2024 10:27:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16050v1</guid></item><item><title>GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding</title><link>http://arxiv.org/abs/2402.06764v2</link><description>Integrating large language models (LLMs) with knowledge graphs derived fromdomain-specific data represents an important advancement towards more powerfuland factual reasoning. As these models grow more capable, it is crucial toenable them to perform multi-step inferences over real-world knowledge graphswhile minimizing hallucination. While large language models excel atconversation and text generation, their ability to reason overdomain-specialized graphs of interconnected entities remains limited. Forexample, can we query a LLM to identify the optimal contact in a professionalnetwork for a specific goal, based on relationships and attributes in a privatedatabase? The answer is no--such capabilities lie beyond current methods.However, this question underscores a critical technical gap that must beaddressed. Many high-value applications in areas such as science, security, ande-commerce rely on proprietary knowledge graphs encoding unique structures,relationships, and logical constraints. We introduce a fine-tuning frameworkfor developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledgegraph into an alternate text representation with labeled question-answer pairs.We demonstrate that grounding the models in specific graph-based knowledgeexpands the models' capacity for structure-based reasoning. Our methodologyleverages the large-language model's generative capabilities to create thedataset and proposes an efficient alternate to retrieval-augmented generationstyled methods.</description><author>Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, Sutanay Choudhury</author><pubDate>Fri, 16 Feb 2024 17:23:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06764v2</guid></item><item><title>Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model</title><link>http://arxiv.org/abs/2405.09215v1</link><description>We introduce Xmodel-VLM, a cutting-edge multimodal vision language model. Itis designed for efficient deployment on consumer GPU servers. Our work directlyconfronts a pivotal industry issue by grappling with the prohibitive servicecosts that hinder the broad adoption of large-scale multimodal systems. Throughrigorous training, we have developed a 1B-scale language model from the groundup, employing the LLaVA paradigm for modal alignment. The result, which we callXmodel-VLM, is a lightweight yet powerful multimodal vision language model.Extensive testing across numerous classic multimodal benchmarks has revealedthat despite its smaller size and faster execution, Xmodel-VLM deliversperformance comparable to that of larger models. Our model checkpoints and codeare publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelVLM.</description><author>Wanting Xu, Yang Liu, Langping He, Xucheng Huang, Ling Jiang</author><pubDate>Wed, 15 May 2024 10:47:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09215v1</guid></item><item><title>RelationVLM: Making Large Vision-Language Models Understand Visual Relations</title><link>http://arxiv.org/abs/2403.12801v1</link><description>The development of Large Vision-Language Models (LVLMs) is striving to catchup with the success of Large Language Models (LLMs), yet it faces morechallenges to be resolved. Very recent works enable LVLMs to localizeobject-level visual contents and ground text to them. Nonetheless, currentLVLMs still struggle to precisely understand visual relations due to the lackof relevant data. In this work, we present RelationVLM, a large vision-languagemodel capable of comprehending various levels and types of relations whetheracross multiple images or within a video. Specifically, we devise a multi-stagerelation-aware training scheme and a series of corresponding data configurationstrategies to bestow RelationVLM with the capabilities of understandingsemantic relations, temporal associations and geometric transforms. Extensivecase studies and quantitative evaluations show RelationVLM has strongcapability in understanding such relations and emerges impressive in-contextcapability of reasoning from few-shot examples by comparison. This work fostersthe advancements of LVLMs by enabling them to support a wider range ofdownstream applications toward artificial general intelligence.</description><author>Zhipeng Huang, Zhizheng Zhang, Zheng-Jun Zha, Yan Lu, Baining Guo</author><pubDate>Tue, 19 Mar 2024 16:01:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12801v1</guid></item><item><title>Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving</title><link>http://arxiv.org/abs/2402.13602v1</link><description>Large Language Models (LLMs) have garnered significant attention for theirability to understand text and images, generate human-like text, and performcomplex reasoning tasks. However, their ability to generalize this advancedreasoning with a combination of natural language text for decision-making indynamic situations requires further exploration. In this study, we investigatehow well LLMs can adapt and apply a combination of arithmetic and common-sensereasoning, particularly in autonomous driving scenarios. We hypothesize thatLLMs hybrid reasoning abilities can improve autonomous driving by enabling themto analyze detected object and sensor data, understand driving regulations andphysical laws, and offer additional context. This addresses complex scenarios,like decisions in low visibility (due to weather conditions), where traditionalmethods might fall short. We evaluated Large Language Models (LLMs) based onaccuracy by comparing their answers with human-generated ground truth insideCARLA. The results showed that when a combination of images (detected objects)and sensor data is fed into the LLM, it can offer precise information for brakeand throttle control in autonomous vehicles across various weather conditions.This formulation and answers can assist in decision-making for auto-pilotsystems.</description><author>Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg</author><pubDate>Wed, 21 Feb 2024 08:09:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13602v1</guid></item><item><title>Grounded Question-Answering in Long Egocentric Videos</title><link>http://arxiv.org/abs/2312.06505v3</link><description>Existing approaches to video understanding, mainly designed for short videosfrom a third-person perspective, are limited in their applicability in certainfields, such as robotics. In this paper, we delve into open-endedquestion-answering (QA) in long, egocentric videos, which allows individuals orrobots to inquire about their own past visual experiences. This task presentsunique challenges, including the complexity of temporally grounding querieswithin extensive video content, the high resource demands for precise dataannotation, and the inherent difficulty of evaluating open-ended answers due totheir ambiguous nature. Our proposed approach tackles these challenges by (i)integrating query grounding and answering within a unified model to reduceerror propagation; (ii) employing large language models for efficient andscalable data synthesis; and (iii) introducing a close-ended QA task forevaluation, to manage answer ambiguity. Extensive experiments demonstrate theeffectiveness of our method, which also achieves state-of-the-art performanceon the QAEgo4D and Ego4D-NLQ benchmarks. Code, data, and models are availableat https://github.com/Becomebright/GroundVQA.</description><author>Shangzhe Di, Weidi Xie</author><pubDate>Thu, 15 Feb 2024 15:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06505v3</guid></item><item><title>DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</title><link>http://arxiv.org/abs/2310.02954v5</link><description>Recent advances in natural language processing, primarily propelled by LargeLanguage Models (LLMs), have showcased their remarkable capabilities groundedin in-context learning. A promising avenue for guiding LLMs in intricatereasoning tasks involves the utilization of intermediate reasoning steps withinthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge liesin the effective selection of exemplars for facilitating in-context learning.In this study, we introduce a framework that leverages Dual Queries andLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplarsfor in-context learning. Dual Queries first query LLM to obtain LLM-generatedknowledge such as CoT, then query the retriever to obtain the final exemplarsvia both question and the knowledge. Moreover, for the second query, LoReemploys dimensionality reduction techniques to refine exemplar selection,ensuring close alignment with the input question's knowledge. Through extensiveexperiments, we demonstrate that DQ-LoRe significantly outperforms priorstate-of-the-art methods in the automatic selection of exemplars for GPT-4,enhancing performance from 92.5% to 94.2%. Our comprehensive analysis furtherreveals that DQ-LoRe consistently outperforms retrieval-based approaches interms of both performance and adaptability, especially in scenarioscharacterized by distribution shifts. DQ-LoRe pushes the boundary of in-contextlearning and opens up new avenues for addressing complex reasoning challenges.Our code is released athttps://github.com/AI4fun/DQ-LoRe}{https://github.com/AI4fun/DQ-LoRe.</description><author>Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang</author><pubDate>Sat, 02 Mar 2024 14:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02954v5</guid></item><item><title>Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving</title><link>http://arxiv.org/abs/2402.13602v2</link><description>Large Language Models (LLMs) have garnered significant attention for theirability to understand text and images, generate human-like text, and performcomplex reasoning tasks. However, their ability to generalize this advancedreasoning with a combination of natural language text for decision-making indynamic situations requires further exploration. In this study, we investigatehow well LLMs can adapt and apply a combination of arithmetic and common-sensereasoning, particularly in autonomous driving scenarios. We hypothesize thatLLMs hybrid reasoning abilities can improve autonomous driving by enabling themto analyze detected object and sensor data, understand driving regulations andphysical laws, and offer additional context. This addresses complex scenarios,like decisions in low visibility (due to weather conditions), where traditionalmethods might fall short. We evaluated Large Language Models (LLMs) based onaccuracy by comparing their answers with human-generated ground truth insideCARLA. The results showed that when a combination of images (detected objects)and sensor data is fed into the LLM, it can offer precise information for brakeand throttle control in autonomous vehicles across various weather conditions.This formulation and answers can assist in decision-making for auto-pilotsystems.</description><author>Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg</author><pubDate>Thu, 07 Mar 2024 12:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13602v2</guid></item><item><title>Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving</title><link>http://arxiv.org/abs/2402.13602v3</link><description>Large Language Models (LLMs) have garnered significant attention for theirability to understand text and images, generate human-like text, and performcomplex reasoning tasks. However, their ability to generalize this advancedreasoning with a combination of natural language text for decision-making indynamic situations requires further exploration. In this study, we investigatehow well LLMs can adapt and apply a combination of arithmetic and common-sensereasoning, particularly in autonomous driving scenarios. We hypothesize thatLLMs hybrid reasoning abilities can improve autonomous driving by enabling themto analyze detected object and sensor data, understand driving regulations andphysical laws, and offer additional context. This addresses complex scenarios,like decisions in low visibility (due to weather conditions), where traditionalmethods might fall short. We evaluated Large Language Models (LLMs) based onaccuracy by comparing their answers with human-generated ground truth insideCARLA. The results showed that when a combination of images (detected objects)and sensor data is fed into the LLM, it can offer precise information for brakeand throttle control in autonomous vehicles across various weather conditions.This formulation and answers can assist in decision-making for auto-pilotsystems.</description><author>Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg</author><pubDate>Mon, 18 Mar 2024 10:50:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13602v3</guid></item><item><title>VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning</title><link>http://arxiv.org/abs/2403.13164v1</link><description>Large language models (LLMs) famously exhibit emergent in-context learning(ICL) -- the ability to rapidly adapt to new tasks using few-shot examplesprovided as a prompt, without updating the model's weights. Built on top ofLLMs, vision large language models (VLLMs) have advanced significantly in areassuch as recognition, reasoning, and grounding. However, investigations into\emph{multimodal ICL} have predominantly focused on few-shot visual questionanswering (VQA), and image captioning, which we will show neither exploit thestrengths of ICL, nor test its limitations. The broader capabilities andlimitations of multimodal ICL remain under-explored. In this study, weintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-contextlearning, encompassing a broad spectrum of tasks that involve both images andtext as inputs and outputs, and different types of challenges, from {perceptionto reasoning and long context length}. We evaluate the abilities ofstate-of-the-art VLLMs against this benchmark suite, revealing their diversestrengths and weaknesses, and showing that even the most advanced models, suchas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,and the associated strengths and limitations of existing models, we hope thatour dataset will inspire future work on enhancing the in-context learningcapabilities of VLLMs, as well as inspire new applications that leverage VLLMICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.</description><author>Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales</author><pubDate>Tue, 19 Mar 2024 22:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13164v1</guid></item><item><title>Grounded Question-Answering in Long Egocentric Videos</title><link>http://arxiv.org/abs/2312.06505v4</link><description>Existing approaches to video understanding, mainly designed for short videosfrom a third-person perspective, are limited in their applicability in certainfields, such as robotics. In this paper, we delve into open-endedquestion-answering (QA) in long, egocentric videos, which allows individuals orrobots to inquire about their own past visual experiences. This task presentsunique challenges, including the complexity of temporally grounding querieswithin extensive video content, the high resource demands for precise dataannotation, and the inherent difficulty of evaluating open-ended answers due totheir ambiguous nature. Our proposed approach tackles these challenges by (i)integrating query grounding and answering within a unified model to reduceerror propagation; (ii) employing large language models for efficient andscalable data synthesis; and (iii) introducing a close-ended QA task forevaluation, to manage answer ambiguity. Extensive experiments demonstrate theeffectiveness of our method, which also achieves state-of-the-art performanceon the QaEgo4D and Ego4D-NLQ benchmarks. Code, data, and models are availableat https://github.com/Becomebright/GroundVQA.</description><author>Shangzhe Di, Weidi Xie</author><pubDate>Mon, 01 Apr 2024 14:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06505v4</guid></item><item><title>Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark</title><link>http://arxiv.org/abs/2402.14359v1</link><description>The summarization capabilities of pretrained and large language models (LLMs)have been widely validated in general areas, but their use in scientificcorpus, which involves complex sentences and specialized knowledge, has beenless assessed. This paper presents conceptual and experimental analyses ofscientific summarization, highlighting the inadequacies of traditionalevaluation methods, such as $n$-gram, embedding comparison, and QA,particularly in providing explanations, grasping scientific concepts, oridentifying key content. Subsequently, we introduce the Facet-aware Metric(FM), employing LLMs for advanced semantic matching to evaluate summaries basedon different aspects. This facet-aware approach offers a thorough evaluation ofabstracts by decomposing the evaluation task into simpler subtasks.Recognizingthe absence of an evaluation benchmark in this domain, we curate a Facet-basedscientific summarization Dataset (FD) with facet-level annotations. Ourfindings confirm that FM offers a more logical approach to evaluatingscientific summaries. In addition, fine-tuned smaller models can compete withLLMs in scientific contexts, while LLMs have limitations in learning fromin-context information in scientific domains. This suggests an area for futureenhancement of LLMs.</description><author>Xiuying Chen, Tairan Wang, Qingqing Zhu, Taicheng Guo, Shen Gao, Zhiyong Lu, Xin Gao, Xiangliang Zhang</author><pubDate>Thu, 22 Feb 2024 07:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14359v1</guid></item></channel></rss>