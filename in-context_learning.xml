<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 12 Jul 2024 01:00:20 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models</title><link>http://arxiv.org/abs/2405.15984v2</link><description>With the emergence of large language models, such as LLaMA and OpenAI GPT-3,In-Context Learning (ICL) gained significant attention due to its effectivenessand efficiency. However, ICL is very sensitive to the choice, order, andverbaliser used to encode the demonstrations in the prompt. Retrieval-AugmentedICL methods try to address this problem by leveraging retrievers to extractsemantically related examples as demonstrations. While this approach yieldsmore accurate results, its robustness against various types of adversarialattacks, including perturbations on test samples, demonstrations, and retrieveddata, remains under-explored. Our study reveals that retrieval-augmented modelscan enhance robustness against test sample attacks, outperforming vanilla ICLwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibitoverconfidence in the demonstrations, leading to a 2% increase in ASR fordemonstration attacks. Adversarial training can help improve the robustness ofICL methods to adversarial attacks; however, such a training scheme can be toocostly in the context of LLMs. As an alternative, we introduce an effectivetraining-free adversarial defence method, DARD, which enriches the example poolwith those attacked samples. We show that DARD yields improvements inperformance and robustness, achieving a 15% reduction in ASR over thebaselines. Code and data are released to encourage further research:https://github.com/simonucl/adv-retreival-icl</description><author>Simon Chi Lok Yu, Jie He, Pasquale Minervini, Jeff Z. Pan</author><pubDate>Wed, 10 Jul 2024 11:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15984v2</guid></item><item><title>Chain-of-Dictionary Prompting Elicits Translation in Large Language Models</title><link>http://arxiv.org/abs/2305.06575v5</link><description>Large language models (LLMs) have shown surprisingly good performance inmultilingual neural machine translation (MNMT) even when trained withoutparallel data. Yet, despite the fact that the amount of training data isgigantic, they still struggle with translating rare words, particularly forlow-resource languages. Even worse, it is usually unrealistic to retrieverelevant demonstrations for in-context learning with low-resource languages onLLMs, which restricts the practical use of LLMs for translation -- how shouldwe mitigate this problem? To this end, we present a novel method, CoD, whichaugments LLMs with prior knowledge with the chains of multilingual dictionariesfor a subset of input words to elicit translation abilities for LLMs. Extensiveexperiments indicate that augmenting ChatGPT with CoD elicits large gains by upto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written inCyrillic script) on FLORES-200 full devtest set. We further demonstrate theimportance of chaining the multilingual dictionaries, as well as thesuperiority of CoD to few-shot demonstration for low-resource languages.</description><author>Hongyuan Lu, Haoran Yang, Haoyang Huang, Dongdong Zhang, Wai Lam, Furu Wei</author><pubDate>Wed, 10 Jul 2024 09:53:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06575v5</guid></item><item><title>Video In-context Learning</title><link>http://arxiv.org/abs/2407.07356v1</link><description>In-context learning for vision data has been underexplored compared with thatin natural language. Previous works studied image in-context learning, urgingmodels to generate a single image guided by demonstrations. In this paper, wepropose and study video in-context learning, where the model starts from anexisting video clip and generates diverse potential future sequences, eachsemantically guided by the prompted video demonstrations. To achieve this, weprovide a clear definition of the task, and train an autoregressive Transformeron video datasets. We thoroughly analyze the effect of different datasets andrepresent frames as discrete tokens, and then model them by next tokenpredictions. We design various evaluation metrics, including both objective andsubjective measures, to demonstrate the visual quality and semantic accuracy ofgeneration results. Our model follows the scaling law and generateshigh-quality video clips that accurately align with the semantic guidanceprovided by in-context examples.</description><author>Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, Jiang Bian</author><pubDate>Wed, 10 Jul 2024 04:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07356v1</guid></item><item><title>From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models</title><link>http://arxiv.org/abs/2310.07338v4</link><description>Tabular data is foundational to predictive modeling in various crucialindustries, including healthcare, finance, retail, sustainability, etc. Despitethe progress made in specialized models, there is an increasing demand foruniversal models that can transfer knowledge, generalize from limited data, andfollow human instructions. These are challenges that current tabular deeplearning approaches have not fully tackled. Here we introduce GenerativeTabular Learning (GTL), a novel framework that integrates the advancedfunctionalities of large language models (LLMs)-such as prompt-based zero-shotgeneralization and in-context learning-into tabular deep learning. GTLcapitalizes on the pre-training of LLMs on diverse tabular data, enhancingtheir understanding of domain-specific knowledge, numerical sequences, andstatistical dependencies critical for accurate predictions. Our empirical studyspans 384 public datasets, rigorously analyzing GTL's convergence and scalingbehaviors and assessing the impact of varied data templates. The GTL-enhancedLLaMA-2 model demonstrates superior zero-shot and in-context learningcapabilities across numerous classification and regression tasks. Notably, itachieves this without fine-tuning, outperforming traditional methods andrivaling state-of-the-art models like GPT-4 in certain cases. Through GTL, wenot only foster a deeper integration of LLMs' sophisticated abilities intotabular data comprehension and application but also offer a new trainingresource and a test bed for LLMs to enhance their ability to comprehend tabulardata. To facilitate reproducible research, we release our code, data, and modelcheckpoints at https://github.com/microsoft/Industrial-Foundation-Models.</description><author>Xumeng Wen, Han Zhang, Shun Zheng, Wei Xu, Jiang Bian</author><pubDate>Thu, 11 Jul 2024 04:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07338v4</guid></item></channel></rss>