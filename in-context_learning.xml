<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 16 Jul 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models</title><link>http://arxiv.org/abs/2405.15984v2</link><description>With the emergence of large language models, such as LLaMA and OpenAI GPT-3,In-Context Learning (ICL) gained significant attention due to its effectivenessand efficiency. However, ICL is very sensitive to the choice, order, andverbaliser used to encode the demonstrations in the prompt. Retrieval-AugmentedICL methods try to address this problem by leveraging retrievers to extractsemantically related examples as demonstrations. While this approach yieldsmore accurate results, its robustness against various types of adversarialattacks, including perturbations on test samples, demonstrations, and retrieveddata, remains under-explored. Our study reveals that retrieval-augmented modelscan enhance robustness against test sample attacks, outperforming vanilla ICLwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibitoverconfidence in the demonstrations, leading to a 2% increase in ASR fordemonstration attacks. Adversarial training can help improve the robustness ofICL methods to adversarial attacks; however, such a training scheme can be toocostly in the context of LLMs. As an alternative, we introduce an effectivetraining-free adversarial defence method, DARD, which enriches the example poolwith those attacked samples. We show that DARD yields improvements inperformance and robustness, achieving a 15% reduction in ASR over thebaselines. Code and data are released to encourage further research:https://github.com/simonucl/adv-retreival-icl</description><author>Simon Chi Lok Yu, Jie He, Pasquale Minervini, Jeff Z. Pan</author><pubDate>Wed, 10 Jul 2024 11:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15984v2</guid></item><item><title>Chain-of-Dictionary Prompting Elicits Translation in Large Language Models</title><link>http://arxiv.org/abs/2305.06575v5</link><description>Large language models (LLMs) have shown surprisingly good performance inmultilingual neural machine translation (MNMT) even when trained withoutparallel data. Yet, despite the fact that the amount of training data isgigantic, they still struggle with translating rare words, particularly forlow-resource languages. Even worse, it is usually unrealistic to retrieverelevant demonstrations for in-context learning with low-resource languages onLLMs, which restricts the practical use of LLMs for translation -- how shouldwe mitigate this problem? To this end, we present a novel method, CoD, whichaugments LLMs with prior knowledge with the chains of multilingual dictionariesfor a subset of input words to elicit translation abilities for LLMs. Extensiveexperiments indicate that augmenting ChatGPT with CoD elicits large gains by upto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written inCyrillic script) on FLORES-200 full devtest set. We further demonstrate theimportance of chaining the multilingual dictionaries, as well as thesuperiority of CoD to few-shot demonstration for low-resource languages.</description><author>Hongyuan Lu, Haoran Yang, Haoyang Huang, Dongdong Zhang, Wai Lam, Furu Wei</author><pubDate>Wed, 10 Jul 2024 09:53:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06575v5</guid></item><item><title>Video In-context Learning</title><link>http://arxiv.org/abs/2407.07356v1</link><description>In-context learning for vision data has been underexplored compared with thatin natural language. Previous works studied image in-context learning, urgingmodels to generate a single image guided by demonstrations. In this paper, wepropose and study video in-context learning, where the model starts from anexisting video clip and generates diverse potential future sequences, eachsemantically guided by the prompted video demonstrations. To achieve this, weprovide a clear definition of the task, and train an autoregressive Transformeron video datasets. We thoroughly analyze the effect of different datasets andrepresent frames as discrete tokens, and then model them by next tokenpredictions. We design various evaluation metrics, including both objective andsubjective measures, to demonstrate the visual quality and semantic accuracy ofgeneration results. Our model follows the scaling law and generateshigh-quality video clips that accurately align with the semantic guidanceprovided by in-context examples.</description><author>Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, Jiang Bian</author><pubDate>Wed, 10 Jul 2024 04:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07356v1</guid></item><item><title>From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models</title><link>http://arxiv.org/abs/2310.07338v4</link><description>Tabular data is foundational to predictive modeling in various crucialindustries, including healthcare, finance, retail, sustainability, etc. Despitethe progress made in specialized models, there is an increasing demand foruniversal models that can transfer knowledge, generalize from limited data, andfollow human instructions. These are challenges that current tabular deeplearning approaches have not fully tackled. Here we introduce GenerativeTabular Learning (GTL), a novel framework that integrates the advancedfunctionalities of large language models (LLMs)-such as prompt-based zero-shotgeneralization and in-context learning-into tabular deep learning. GTLcapitalizes on the pre-training of LLMs on diverse tabular data, enhancingtheir understanding of domain-specific knowledge, numerical sequences, andstatistical dependencies critical for accurate predictions. Our empirical studyspans 384 public datasets, rigorously analyzing GTL's convergence and scalingbehaviors and assessing the impact of varied data templates. The GTL-enhancedLLaMA-2 model demonstrates superior zero-shot and in-context learningcapabilities across numerous classification and regression tasks. Notably, itachieves this without fine-tuning, outperforming traditional methods andrivaling state-of-the-art models like GPT-4 in certain cases. Through GTL, wenot only foster a deeper integration of LLMs' sophisticated abilities intotabular data comprehension and application but also offer a new trainingresource and a test bed for LLMs to enhance their ability to comprehend tabulardata. To facilitate reproducible research, we release our code, data, and modelcheckpoints at https://github.com/microsoft/Industrial-Foundation-Models.</description><author>Xumeng Wen, Han Zhang, Shun Zheng, Wei Xu, Jiang Bian</author><pubDate>Thu, 11 Jul 2024 04:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07338v4</guid></item><item><title>HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context</title><link>http://arxiv.org/abs/2407.09375v1</link><description>This work explores the in-context learning capabilities of State Space Models(SSMs) and presents, to the best of our knowledge, the first theoreticalexplanation of a possible underlying mechanism. We introduce a novel weightconstruction for SSMs, enabling them to predict the next state of any dynamicalsystem after observing previous states without parameter fine-tuning. This isaccomplished by extending the HiPPO framework to demonstrate that continuousSSMs can approximate the derivative of any input signal. Specifically, we findan explicit weight construction for continuous SSMs and provide an asymptoticerror bound on the derivative approximation. The discretization of thiscontinuous SSM subsequently yields a discrete SSM that predicts the next state.Finally, we demonstrate the effectiveness of our parameterization empirically.This work should be an initial step toward understanding how sequence modelsbased on SSMs learn in context.</description><author>Federico Arangath Joseph, Kilian Haefeli, Noah Liniger, Caglar Gulcehre</author><pubDate>Fri, 12 Jul 2024 15:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09375v1</guid></item><item><title>Can large language models explore in-context?</title><link>http://arxiv.org/abs/2403.15371v2</link><description>We investigate the extent to which contemporary Large Language Models (LLMs)can engage in exploration, a core capability in reinforcement learning anddecision making. We focus on native performance of existing LLMs, withouttraining interventions. We deploy LLMs as agents in simple multi-armed banditenvironments, specifying the environment description and interaction historyentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,GPT-4, and Llama2, using a variety of prompt designs, and find that the modelsdo not robustly engage in exploration without substantial interventions: i)Across all of our experiments, only one configuration resulted in satisfactoryexploratory behavior: GPT-4 with chain-of-thought reasoning and an externallysummarized interaction history, presented as sufficient statistics; ii) Allother configurations did not result in robust exploratory behavior, includingthose with chain-of-thought reasoning but unsummarized history. Although thesefindings can be interpreted positively, they suggest that externalsummarization -- which may not be possible in more complex settings -- isimportant for obtaining desirable behavior from LLM agents. We conclude thatnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,may be required to empower LLM-based decision making agents in complexsettings.</description><author>Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins</author><pubDate>Fri, 12 Jul 2024 14:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15371v2</guid></item><item><title>OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text</title><link>http://arxiv.org/abs/2406.08418v3</link><description>Image-text interleaved data, consisting of multiple images and texts arrangedin a natural document format, aligns with the presentation paradigm of internetdata and closely resembles human reading habits. Recent studies have shown thatsuch data aids multimodal in-context learning and maintains the capabilities oflarge language models during multimodal fine-tuning. However, the limited scaleand diversity of current image-text interleaved data restrict the developmentof multimodal large language models. In this paper, we introduce OmniCorpus, a10 billion-scale image-text interleaved dataset. Using an efficient dataengine, we filter and extract large-scale high-quality documents, which contain8.6 billion images and 1,696 billion text tokens. Compared to counterparts(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales whilemaintaining good data quality; 2) features more diverse sources, including bothEnglish and non-English websites as well as video-centric websites; 3) is moreflexible, easily degradable from an image-text interleaved format to pure textcorpus and image-text pairs. Through comprehensive analysis and experiments, wevalidate the quality, usability, and effectiveness of the proposed dataset. Wehope this could provide a solid data foundation for future multimodal modelresearch. Code and data are released athttps://github.com/OpenGVLab/OmniCorpus.</description><author>Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Zhenxiang Li, Pei Chu, Yi Wang, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Zhongying Tu, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, Jifeng Dai</author><pubDate>Fri, 12 Jul 2024 08:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08418v3</guid></item><item><title>SpreadsheetLLM: Encoding Spreadsheets for Large Language Models</title><link>http://arxiv.org/abs/2407.09025v1</link><description>Spreadsheets, with their extensive two-dimensional grids, various layouts,and diverse formatting options, present notable challenges for large languagemodels (LLMs). In response, we introduce SpreadsheetLLM, pioneering anefficient encoding method designed to unleash and optimize LLMs' powerfulunderstanding and reasoning capability on spreadsheets. Initially, we propose avanilla serialization approach that incorporates cell addresses, values, andformats. However, this approach was limited by LLMs' token constraints, makingit impractical for most applications. To tackle this challenge, we developSheetCompressor, an innovative encoding framework that compresses spreadsheetseffectively for LLMs. It comprises three modules: structural-anchor-basedcompression, inverse index translation, and data-format-aware aggregation. Itsignificantly improves performance in spreadsheet table detection task,outperforming the vanilla approach by 25.6% in GPT4's in-context learningsetting. Moreover, fine-tuned LLM with SheetCompressor has an averagecompression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score,surpassing the best existing models by 12.3%. Finally, we propose Chain ofSpreadsheet for downstream tasks of spreadsheet understanding and validate in anew and demanding spreadsheet QA task. We methodically leverage the inherentlayout and structure of spreadsheets, demonstrating that SpreadsheetLLM ishighly effective across a variety of spreadsheet tasks.</description><author>Yuzhang Tian, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, José Cambronero, Yeye He, Shi Han, Dongmei Zhang</author><pubDate>Fri, 12 Jul 2024 06:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09025v1</guid></item><item><title>RB-SQL: A Retrieval-based LLM Framework for Text-to-SQL</title><link>http://arxiv.org/abs/2407.08273v2</link><description>Large language models (LLMs) with in-context learning have significantlyimproved the performance of text-to-SQL task. Previous works generally focus onusing exclusive SQL generation prompt to improve the LLMs' reasoning ability.However, they are mostly hard to handle large databases with numerous tablesand columns, and usually ignore the significance of pre-processing database andextracting valuable information for more efficient prompt engineering. Based onabove analysis, we propose RB-SQL, a novel retrieval-based LLM framework forin-context prompt engineering, which consists of three modules that retrieveconcise tables and columns as schema, and targeted examples for in-contextlearning. Experiment results demonstrate that our model achieves betterperformance than several competitive baselines on public datasets BIRD andSpider.</description><author>Zhenhe Wu, Zhongqiu Li, Jie Zhang, Mengxiang Li, Yu Zhao, Ruiyu Fang, Zhongjiang He, Xuelong Li, Zhoujun Li, Shuangyong Song</author><pubDate>Fri, 12 Jul 2024 06:24:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08273v2</guid></item><item><title>Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection</title><link>http://arxiv.org/abs/2407.08952v1</link><description>Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate newsfrom real ones in extremely low-resource scenarios. This task has garneredincreased attention due to the widespread dissemination and harmful impact offake news on social media. Large Language Models (LLMs) have demonstratedcompetitive performance with the help of their rich prior knowledge andexcellent in-context learning abilities. However, existing methods facesignificant limitations, such as the Understanding Ambiguity and InformationScarcity, which significantly undermine the potential of LLMs. To address theseshortcomings, we propose a Dual-perspective Augmented Fake News Detection(DAFND) model, designed to enhance LLMs from both inside and outsideperspectives. Specifically, DAFND first identifies the keywords of each newsarticle through a Detection Module. Subsequently, DAFND creatively designs anInvestigation Module to retrieve inside and outside valuable informationconcerning to the current news, followed by another Judge Module to derive itsrespective two prediction results. Finally, a Determination Module furtherintegrates these two predictions and derives the final result. Extensiveexperiments on two publicly available datasets show the efficacy of ourproposed method, particularly in low-resource settings.</description><author>Ye Liu, Jiajun Zhu, Kai Zhang, Haoyu Tang, Yanghai Zhang, Xukai Liu, Qi Liu, Enhong Chen</author><pubDate>Fri, 12 Jul 2024 03:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08952v1</guid></item><item><title>DG-PIC: Domain Generalized Point-In-Context Learning for Point Cloud Understanding</title><link>http://arxiv.org/abs/2407.08801v1</link><description>Recent point cloud understanding research suffers from performance drops onunseen data, due to the distribution shifts across different domains. Whilerecent studies use Domain Generalization (DG) techniques to mitigate this bylearning domain-invariant features, most are designed for a single task andneglect the potential of testing data. Despite In-Context Learning (ICL)showcasing multi-task learning capability, it usually relies on high-qualitycontext-rich data and considers a single dataset, and has rarely been studiedin point cloud understanding. In this paper, we introduce a novel, practical,multi-domain multi-task setting, handling multiple domains and multiple taskswithin one unified model for domain generalized point cloud understanding. Tothis end, we propose Domain Generalized Point-In-Context Learning (DG-PIC) thatboosts the generalizability across various tasks and domains at testing time.In particular, we develop dual-level source prototype estimation that considersboth global-level shape contextual and local-level geometrical structures forrepresenting source domains and a dual-level test-time feature shiftingmechanism that leverages both macro-level domain semantic information andmicro-level patch positional relationships to pull the target data closer tothe source ones during the testing. Our DG-PIC does not require any modelupdates during the testing and can handle unseen domains and multiple tasks,\textit{i.e.,} point cloud reconstruction, denoising, and registration, withinone unified model. We also introduce a benchmark for this new setting.Comprehensive experiments demonstrate that DG-PIC outperforms state-of-the-arttechniques significantly.</description><author>Jincen Jiang, Qianyu Zhou, Yuhang Li, Xuequan Lu, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang</author><pubDate>Thu, 11 Jul 2024 18:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08801v1</guid></item></channel></rss>