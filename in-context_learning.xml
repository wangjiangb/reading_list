<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 11 Jul 2024 18:35:02 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models</title><link>http://arxiv.org/abs/2405.15984v2</link><description>With the emergence of large language models, such as LLaMA and OpenAI GPT-3,In-Context Learning (ICL) gained significant attention due to its effectivenessand efficiency. However, ICL is very sensitive to the choice, order, andverbaliser used to encode the demonstrations in the prompt. Retrieval-AugmentedICL methods try to address this problem by leveraging retrievers to extractsemantically related examples as demonstrations. While this approach yieldsmore accurate results, its robustness against various types of adversarialattacks, including perturbations on test samples, demonstrations, and retrieveddata, remains under-explored. Our study reveals that retrieval-augmented modelscan enhance robustness against test sample attacks, outperforming vanilla ICLwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibitoverconfidence in the demonstrations, leading to a 2% increase in ASR fordemonstration attacks. Adversarial training can help improve the robustness ofICL methods to adversarial attacks; however, such a training scheme can be toocostly in the context of LLMs. As an alternative, we introduce an effectivetraining-free adversarial defence method, DARD, which enriches the example poolwith those attacked samples. We show that DARD yields improvements inperformance and robustness, achieving a 15% reduction in ASR over thebaselines. Code and data are released to encourage further research:https://github.com/simonucl/adv-retreival-icl</description><author>Simon Chi Lok Yu, Jie He, Pasquale Minervini, Jeff Z. Pan</author><pubDate>Wed, 10 Jul 2024 11:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15984v2</guid></item><item><title>Chain-of-Dictionary Prompting Elicits Translation in Large Language Models</title><link>http://arxiv.org/abs/2305.06575v5</link><description>Large language models (LLMs) have shown surprisingly good performance inmultilingual neural machine translation (MNMT) even when trained withoutparallel data. Yet, despite the fact that the amount of training data isgigantic, they still struggle with translating rare words, particularly forlow-resource languages. Even worse, it is usually unrealistic to retrieverelevant demonstrations for in-context learning with low-resource languages onLLMs, which restricts the practical use of LLMs for translation -- how shouldwe mitigate this problem? To this end, we present a novel method, CoD, whichaugments LLMs with prior knowledge with the chains of multilingual dictionariesfor a subset of input words to elicit translation abilities for LLMs. Extensiveexperiments indicate that augmenting ChatGPT with CoD elicits large gains by upto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written inCyrillic script) on FLORES-200 full devtest set. We further demonstrate theimportance of chaining the multilingual dictionaries, as well as thesuperiority of CoD to few-shot demonstration for low-resource languages.</description><author>Hongyuan Lu, Haoran Yang, Haoyang Huang, Dongdong Zhang, Wai Lam, Furu Wei</author><pubDate>Wed, 10 Jul 2024 09:53:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06575v5</guid></item></channel></rss>