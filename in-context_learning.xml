<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 01 Aug 2024 13:00:21 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models</title><link>http://arxiv.org/abs/2405.15984v2</link><description>With the emergence of large language models, such as LLaMA and OpenAI GPT-3,In-Context Learning (ICL) gained significant attention due to its effectivenessand efficiency. However, ICL is very sensitive to the choice, order, andverbaliser used to encode the demonstrations in the prompt. Retrieval-AugmentedICL methods try to address this problem by leveraging retrievers to extractsemantically related examples as demonstrations. While this approach yieldsmore accurate results, its robustness against various types of adversarialattacks, including perturbations on test samples, demonstrations, and retrieveddata, remains under-explored. Our study reveals that retrieval-augmented modelscan enhance robustness against test sample attacks, outperforming vanilla ICLwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibitoverconfidence in the demonstrations, leading to a 2% increase in ASR fordemonstration attacks. Adversarial training can help improve the robustness ofICL methods to adversarial attacks; however, such a training scheme can be toocostly in the context of LLMs. As an alternative, we introduce an effectivetraining-free adversarial defence method, DARD, which enriches the example poolwith those attacked samples. We show that DARD yields improvements inperformance and robustness, achieving a 15% reduction in ASR over thebaselines. Code and data are released to encourage further research:https://github.com/simonucl/adv-retreival-icl</description><author>Simon Chi Lok Yu, Jie He, Pasquale Minervini, Jeff Z. Pan</author><pubDate>Wed, 10 Jul 2024 11:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15984v2</guid></item><item><title>Chain-of-Dictionary Prompting Elicits Translation in Large Language Models</title><link>http://arxiv.org/abs/2305.06575v5</link><description>Large language models (LLMs) have shown surprisingly good performance inmultilingual neural machine translation (MNMT) even when trained withoutparallel data. Yet, despite the fact that the amount of training data isgigantic, they still struggle with translating rare words, particularly forlow-resource languages. Even worse, it is usually unrealistic to retrieverelevant demonstrations for in-context learning with low-resource languages onLLMs, which restricts the practical use of LLMs for translation -- how shouldwe mitigate this problem? To this end, we present a novel method, CoD, whichaugments LLMs with prior knowledge with the chains of multilingual dictionariesfor a subset of input words to elicit translation abilities for LLMs. Extensiveexperiments indicate that augmenting ChatGPT with CoD elicits large gains by upto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written inCyrillic script) on FLORES-200 full devtest set. We further demonstrate theimportance of chaining the multilingual dictionaries, as well as thesuperiority of CoD to few-shot demonstration for low-resource languages.</description><author>Hongyuan Lu, Haoran Yang, Haoyang Huang, Dongdong Zhang, Wai Lam, Furu Wei</author><pubDate>Wed, 10 Jul 2024 09:53:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06575v5</guid></item><item><title>Video In-context Learning</title><link>http://arxiv.org/abs/2407.07356v1</link><description>In-context learning for vision data has been underexplored compared with thatin natural language. Previous works studied image in-context learning, urgingmodels to generate a single image guided by demonstrations. In this paper, wepropose and study video in-context learning, where the model starts from anexisting video clip and generates diverse potential future sequences, eachsemantically guided by the prompted video demonstrations. To achieve this, weprovide a clear definition of the task, and train an autoregressive Transformeron video datasets. We thoroughly analyze the effect of different datasets andrepresent frames as discrete tokens, and then model them by next tokenpredictions. We design various evaluation metrics, including both objective andsubjective measures, to demonstrate the visual quality and semantic accuracy ofgeneration results. Our model follows the scaling law and generateshigh-quality video clips that accurately align with the semantic guidanceprovided by in-context examples.</description><author>Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, Jiang Bian</author><pubDate>Wed, 10 Jul 2024 04:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07356v1</guid></item><item><title>From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models</title><link>http://arxiv.org/abs/2310.07338v4</link><description>Tabular data is foundational to predictive modeling in various crucialindustries, including healthcare, finance, retail, sustainability, etc. Despitethe progress made in specialized models, there is an increasing demand foruniversal models that can transfer knowledge, generalize from limited data, andfollow human instructions. These are challenges that current tabular deeplearning approaches have not fully tackled. Here we introduce GenerativeTabular Learning (GTL), a novel framework that integrates the advancedfunctionalities of large language models (LLMs)-such as prompt-based zero-shotgeneralization and in-context learning-into tabular deep learning. GTLcapitalizes on the pre-training of LLMs on diverse tabular data, enhancingtheir understanding of domain-specific knowledge, numerical sequences, andstatistical dependencies critical for accurate predictions. Our empirical studyspans 384 public datasets, rigorously analyzing GTL's convergence and scalingbehaviors and assessing the impact of varied data templates. The GTL-enhancedLLaMA-2 model demonstrates superior zero-shot and in-context learningcapabilities across numerous classification and regression tasks. Notably, itachieves this without fine-tuning, outperforming traditional methods andrivaling state-of-the-art models like GPT-4 in certain cases. Through GTL, wenot only foster a deeper integration of LLMs' sophisticated abilities intotabular data comprehension and application but also offer a new trainingresource and a test bed for LLMs to enhance their ability to comprehend tabulardata. To facilitate reproducible research, we release our code, data, and modelcheckpoints at https://github.com/microsoft/Industrial-Foundation-Models.</description><author>Xumeng Wen, Han Zhang, Shun Zheng, Wei Xu, Jiang Bian</author><pubDate>Thu, 11 Jul 2024 04:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07338v4</guid></item><item><title>HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context</title><link>http://arxiv.org/abs/2407.09375v1</link><description>This work explores the in-context learning capabilities of State Space Models(SSMs) and presents, to the best of our knowledge, the first theoreticalexplanation of a possible underlying mechanism. We introduce a novel weightconstruction for SSMs, enabling them to predict the next state of any dynamicalsystem after observing previous states without parameter fine-tuning. This isaccomplished by extending the HiPPO framework to demonstrate that continuousSSMs can approximate the derivative of any input signal. Specifically, we findan explicit weight construction for continuous SSMs and provide an asymptoticerror bound on the derivative approximation. The discretization of thiscontinuous SSM subsequently yields a discrete SSM that predicts the next state.Finally, we demonstrate the effectiveness of our parameterization empirically.This work should be an initial step toward understanding how sequence modelsbased on SSMs learn in context.</description><author>Federico Arangath Joseph, Kilian Haefeli, Noah Liniger, Caglar Gulcehre</author><pubDate>Fri, 12 Jul 2024 15:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09375v1</guid></item><item><title>Can large language models explore in-context?</title><link>http://arxiv.org/abs/2403.15371v2</link><description>We investigate the extent to which contemporary Large Language Models (LLMs)can engage in exploration, a core capability in reinforcement learning anddecision making. We focus on native performance of existing LLMs, withouttraining interventions. We deploy LLMs as agents in simple multi-armed banditenvironments, specifying the environment description and interaction historyentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,GPT-4, and Llama2, using a variety of prompt designs, and find that the modelsdo not robustly engage in exploration without substantial interventions: i)Across all of our experiments, only one configuration resulted in satisfactoryexploratory behavior: GPT-4 with chain-of-thought reasoning and an externallysummarized interaction history, presented as sufficient statistics; ii) Allother configurations did not result in robust exploratory behavior, includingthose with chain-of-thought reasoning but unsummarized history. Although thesefindings can be interpreted positively, they suggest that externalsummarization -- which may not be possible in more complex settings -- isimportant for obtaining desirable behavior from LLM agents. We conclude thatnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,may be required to empower LLM-based decision making agents in complexsettings.</description><author>Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins</author><pubDate>Fri, 12 Jul 2024 14:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15371v2</guid></item><item><title>OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text</title><link>http://arxiv.org/abs/2406.08418v3</link><description>Image-text interleaved data, consisting of multiple images and texts arrangedin a natural document format, aligns with the presentation paradigm of internetdata and closely resembles human reading habits. Recent studies have shown thatsuch data aids multimodal in-context learning and maintains the capabilities oflarge language models during multimodal fine-tuning. However, the limited scaleand diversity of current image-text interleaved data restrict the developmentof multimodal large language models. In this paper, we introduce OmniCorpus, a10 billion-scale image-text interleaved dataset. Using an efficient dataengine, we filter and extract large-scale high-quality documents, which contain8.6 billion images and 1,696 billion text tokens. Compared to counterparts(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales whilemaintaining good data quality; 2) features more diverse sources, including bothEnglish and non-English websites as well as video-centric websites; 3) is moreflexible, easily degradable from an image-text interleaved format to pure textcorpus and image-text pairs. Through comprehensive analysis and experiments, wevalidate the quality, usability, and effectiveness of the proposed dataset. Wehope this could provide a solid data foundation for future multimodal modelresearch. Code and data are released athttps://github.com/OpenGVLab/OmniCorpus.</description><author>Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Zhenxiang Li, Pei Chu, Yi Wang, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Zhongying Tu, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, Jifeng Dai</author><pubDate>Fri, 12 Jul 2024 08:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08418v3</guid></item><item><title>SpreadsheetLLM: Encoding Spreadsheets for Large Language Models</title><link>http://arxiv.org/abs/2407.09025v1</link><description>Spreadsheets, with their extensive two-dimensional grids, various layouts,and diverse formatting options, present notable challenges for large languagemodels (LLMs). In response, we introduce SpreadsheetLLM, pioneering anefficient encoding method designed to unleash and optimize LLMs' powerfulunderstanding and reasoning capability on spreadsheets. Initially, we propose avanilla serialization approach that incorporates cell addresses, values, andformats. However, this approach was limited by LLMs' token constraints, makingit impractical for most applications. To tackle this challenge, we developSheetCompressor, an innovative encoding framework that compresses spreadsheetseffectively for LLMs. It comprises three modules: structural-anchor-basedcompression, inverse index translation, and data-format-aware aggregation. Itsignificantly improves performance in spreadsheet table detection task,outperforming the vanilla approach by 25.6% in GPT4's in-context learningsetting. Moreover, fine-tuned LLM with SheetCompressor has an averagecompression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score,surpassing the best existing models by 12.3%. Finally, we propose Chain ofSpreadsheet for downstream tasks of spreadsheet understanding and validate in anew and demanding spreadsheet QA task. We methodically leverage the inherentlayout and structure of spreadsheets, demonstrating that SpreadsheetLLM ishighly effective across a variety of spreadsheet tasks.</description><author>Yuzhang Tian, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, José Cambronero, Yeye He, Shi Han, Dongmei Zhang</author><pubDate>Fri, 12 Jul 2024 06:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09025v1</guid></item><item><title>RB-SQL: A Retrieval-based LLM Framework for Text-to-SQL</title><link>http://arxiv.org/abs/2407.08273v2</link><description>Large language models (LLMs) with in-context learning have significantlyimproved the performance of text-to-SQL task. Previous works generally focus onusing exclusive SQL generation prompt to improve the LLMs' reasoning ability.However, they are mostly hard to handle large databases with numerous tablesand columns, and usually ignore the significance of pre-processing database andextracting valuable information for more efficient prompt engineering. Based onabove analysis, we propose RB-SQL, a novel retrieval-based LLM framework forin-context prompt engineering, which consists of three modules that retrieveconcise tables and columns as schema, and targeted examples for in-contextlearning. Experiment results demonstrate that our model achieves betterperformance than several competitive baselines on public datasets BIRD andSpider.</description><author>Zhenhe Wu, Zhongqiu Li, Jie Zhang, Mengxiang Li, Yu Zhao, Ruiyu Fang, Zhongjiang He, Xuelong Li, Zhoujun Li, Shuangyong Song</author><pubDate>Fri, 12 Jul 2024 06:24:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08273v2</guid></item><item><title>Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection</title><link>http://arxiv.org/abs/2407.08952v1</link><description>Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate newsfrom real ones in extremely low-resource scenarios. This task has garneredincreased attention due to the widespread dissemination and harmful impact offake news on social media. Large Language Models (LLMs) have demonstratedcompetitive performance with the help of their rich prior knowledge andexcellent in-context learning abilities. However, existing methods facesignificant limitations, such as the Understanding Ambiguity and InformationScarcity, which significantly undermine the potential of LLMs. To address theseshortcomings, we propose a Dual-perspective Augmented Fake News Detection(DAFND) model, designed to enhance LLMs from both inside and outsideperspectives. Specifically, DAFND first identifies the keywords of each newsarticle through a Detection Module. Subsequently, DAFND creatively designs anInvestigation Module to retrieve inside and outside valuable informationconcerning to the current news, followed by another Judge Module to derive itsrespective two prediction results. Finally, a Determination Module furtherintegrates these two predictions and derives the final result. Extensiveexperiments on two publicly available datasets show the efficacy of ourproposed method, particularly in low-resource settings.</description><author>Ye Liu, Jiajun Zhu, Kai Zhang, Haoyu Tang, Yanghai Zhang, Xukai Liu, Qi Liu, Enhong Chen</author><pubDate>Fri, 12 Jul 2024 03:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08952v1</guid></item><item><title>DG-PIC: Domain Generalized Point-In-Context Learning for Point Cloud Understanding</title><link>http://arxiv.org/abs/2407.08801v1</link><description>Recent point cloud understanding research suffers from performance drops onunseen data, due to the distribution shifts across different domains. Whilerecent studies use Domain Generalization (DG) techniques to mitigate this bylearning domain-invariant features, most are designed for a single task andneglect the potential of testing data. Despite In-Context Learning (ICL)showcasing multi-task learning capability, it usually relies on high-qualitycontext-rich data and considers a single dataset, and has rarely been studiedin point cloud understanding. In this paper, we introduce a novel, practical,multi-domain multi-task setting, handling multiple domains and multiple taskswithin one unified model for domain generalized point cloud understanding. Tothis end, we propose Domain Generalized Point-In-Context Learning (DG-PIC) thatboosts the generalizability across various tasks and domains at testing time.In particular, we develop dual-level source prototype estimation that considersboth global-level shape contextual and local-level geometrical structures forrepresenting source domains and a dual-level test-time feature shiftingmechanism that leverages both macro-level domain semantic information andmicro-level patch positional relationships to pull the target data closer tothe source ones during the testing. Our DG-PIC does not require any modelupdates during the testing and can handle unseen domains and multiple tasks,\textit{i.e.,} point cloud reconstruction, denoising, and registration, withinone unified model. We also introduce a benchmark for this new setting.Comprehensive experiments demonstrate that DG-PIC outperforms state-of-the-arttechniques significantly.</description><author>Jincen Jiang, Qianyu Zhou, Yuhang Li, Xuequan Lu, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang</author><pubDate>Thu, 11 Jul 2024 18:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08801v1</guid></item><item><title>GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM</title><link>http://arxiv.org/abs/2407.10870v1</link><description>Large vision-language models (LVLMs), such as the Generative Pre-trainedTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models whichhave great potential as powerful artificial-intelligence (AI) assistance toolsfor a myriad of applications, including healthcare, industrial, and academicsectors. Although such foundation models perform well in a wide range ofgeneral tasks, their capability without fine-tuning is often limited inspecialized tasks. However, full fine-tuning of large foundation models ischallenging due to enormous computation/memory/dataset requirements. We showthat GPT-4o can decode hand gestures from forearm ultrasound data even with nofine-tuning, and improves with few-shot, in-context learning.</description><author>Keshav Bimbraw, Ye Wang, Jing Liu, Toshiaki Koike-Akino</author><pubDate>Mon, 15 Jul 2024 16:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10870v1</guid></item><item><title>Learning to Generate Answers with Citations via Factual Consistency Models</title><link>http://arxiv.org/abs/2406.13124v2</link><description>Large Language Models (LLMs) frequently hallucinate, impeding theirreliability in mission-critical situations. One approach to address this issueis to provide citations to relevant sources alongside generated content,enhancing the verifiability of generations. However, citing passages accuratelyin answers remains a substantial challenge. This paper proposes aweakly-supervised fine-tuning method leveraging factual consistency models(FCMs). Our approach alternates between generating texts with citations andsupervised fine-tuning with FCM-filtered citation data. Focused learning isintegrated into the objective, directing the fine-tuning process to emphasisethe factual unit tokens, as measured by an FCM. Results on the ALCE few-shotcitation benchmark with various instruction-tuned LLMs demonstrate superiorperformance compared to in-context learning, vanilla supervised fine-tuning,and state-of-the-art methods, with an average improvement of $34.1$, $15.5$,and $10.5$ citation F$_1$ points, respectively. Moreover, in a domain transfersetting we show that the obtained citation generation ability robustlytransfers to unseen datasets. Notably, our citation improvements contribute tothe lowest factual error rate across baselines.</description><author>Rami Aly, Zhiqiang Tang, Samson Tan, George Karypis</author><pubDate>Mon, 15 Jul 2024 16:04:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13124v2</guid></item><item><title>Are Emergent Abilities in Large Language Models just In-Context Learning?</title><link>http://arxiv.org/abs/2309.01809v2</link><description>Large language models, comprising billions of parameters and pre-trained onextensive web-scale corpora, have been claimed to acquire certain capabilitieswithout having been specifically trained on them. These capabilities, referredto as "emergent abilities," have been a driving force in discussions regardingthe potentials and risks of language models. A key challenge in evaluatingemergent abilities is that they are confounded by model competencies that arisethrough alternative prompting techniques, including in-context learning, whichis the ability of models to complete a task based on a few examples. We presenta novel theory that explains emergent abilities, taking into account theirpotential confounding factors, and rigorously substantiate this theory throughover 1000 experiments. Our findings suggest that purported emergent abilitiesare not truly emergent, but result from a combination of in-context learning,model memory, and linguistic knowledge. Our work is a foundational step inexplaining language model performance, providing a template for their efficientuse and clarifying the paradox of their ability to excel in some instanceswhile faltering in others. Thus, we demonstrate that their capabilities shouldnot be overestimated.</description><author>Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, Iryna Gurevych</author><pubDate>Mon, 15 Jul 2024 12:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01809v2</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v1</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell Espuny</author><pubDate>Tue, 16 Jul 2024 10:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v1</guid></item><item><title>Reasoning with Large Language Models, a Survey</title><link>http://arxiv.org/abs/2407.11511v1</link><description>Scaling up language models to billions of parameters has opened uppossibilities for in-context learning, allowing instruction tuning and few-shotlearning on tasks that the model was not specifically trained for. This hasachieved breakthrough performance on language tasks such as translation,summarization, and question-answering. Furthermore, in addition to theseassociative "System 1" tasks, recent advances in Chain-of-thought promptlearning have demonstrated strong "System 2" reasoning abilities, answering aquestion in the field of artificial general intelligence whether LLMs canreason. The field started with the question whether LLMs can solve grade schoolmath word problems. This paper reviews the rapidly expanding field ofprompt-based reasoning with LLMs. Our taxonomy identifies different ways togenerate, evaluate, and control multi-step reasoning. We provide an in-depthcoverage of core approaches and open problems, and we propose a research agendafor the near future. Finally, we highlight the relation between reasoning andprompt-based learning, and we discuss the relation between reasoning,sequential decision processes, and reinforcement learning. We find thatself-improvement, self-reflection, and some metacognitive abilities of thereasoning processes are possible through the judicious use of prompts. Trueself-improvement and self-reasoning, to go from reasoning with LLMs toreasoning by LLMs, remains future work.</description><author>Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back</author><pubDate>Tue, 16 Jul 2024 08:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11511v1</guid></item><item><title>Cross-lingual QA: A Key to Unlocking In-context Cross-lingual Performance</title><link>http://arxiv.org/abs/2305.15233v3</link><description>Multilingual large language models (MLLMs) have demonstrated significantcross-lingual capabilities through in-context learning. Existing approachestypically construct monolingual in-context examples, either in the source ortarget language. However, translating entire in-context examples into thetarget language might compromise contextual integrity and be costly in the caseof long-context passages. To address this, we introduce Cross-lingual QA, across-lingual prompting method that translates only the question and answerparts, thus reducing translation costs. Experiments on four typologicallydiverse multilingual benchmarks show that Cross-lingual QA promptingeffectively stimulates models to elicit their cross-lingual knowledge,outperforming prior monolingual prompting approaches. Furthermore, we show thatprompting open-source MLLMs with cross-lingual in-context examples enhancesperformance as the model scale increases.</description><author>Sunkyoung Kim, Dayeon Ki, Yireun Kim, Jinsik Lee</author><pubDate>Tue, 16 Jul 2024 08:18:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15233v3</guid></item><item><title>Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of Few-Shot Learning</title><link>http://arxiv.org/abs/2407.12498v1</link><description>The linguistic capabilities of Multimodal Large Language Models (MLLMs) arecritical for their effective application across diverse tasks. This study aimsto evaluate the performance of MLLMs on the VALSE benchmark, focusing on theefficacy of few-shot In-Context Learning (ICL), and Chain-of-Thought (CoT)prompting. We conducted a comprehensive assessment of state-of-the-art MLLMs,varying in model size and pretraining datasets. The experimental results revealthat ICL and CoT prompting significantly boost model performance, particularlyin tasks requiring complex reasoning and contextual understanding. Modelspretrained on captioning datasets show superior zero-shot performance, whilethose trained on interleaved image-text data benefit from few-shot learning.Our findings provide valuable insights into optimizing MLLMs for bettergrounding of language in visual contexts, highlighting the importance of thecomposition of pretraining data and the potential of few-shot learningstrategies to improve the reasoning abilities of MLLMs.</description><author>Mustafa Dogan, Ilker Kesen, Iacer Calixto, Aykut Erdem, Erkut Erdem</author><pubDate>Wed, 17 Jul 2024 11:26:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12498v1</guid></item><item><title>Towards Multimodal In-Context Learning for Vision &amp; Language Models</title><link>http://arxiv.org/abs/2403.12736v2</link><description>State-of-the-art Vision-Language Models (VLMs) ground the vision and thelanguage modality primarily via projecting the vision tokens from the encoderto language-like tokens, which are directly fed to the Large Language Model(LLM) decoder. While these models have shown unprecedented performance in manydownstream zero-shot tasks (eg image captioning, question answers, etc), stilllittle emphasis has been put on transferring one of the core LLM capability ofIn-Context Learning (ICL). ICL is the ability of a model to reason about adownstream task with a few examples demonstrations embedded in the prompt. Inthis work, through extensive evaluations, we find that the state-of-the-artVLMs somewhat lack the ability to follow ICL instructions. In particular, wediscover that even models that underwent large-scale mixed modalitypre-training and were implicitly guided to make use of interleaved image andtext information (intended to consume helpful context from multiple images)under-perform when prompted with few-shot demonstrations (in an ICL way),likely due to their lack of direct ICL instruction tuning. To enhance the ICLabilities of the present VLM, we propose a simple yet surprisingly effectivemulti-turn curriculum-based learning methodology with effective data mixes,leading up to a significant 21.03% (and 11.3% on average) ICL performance boostover the strongest VLM baselines and a variety of ICL benchmarks. Furthermore,we also contribute new benchmarks for ICL evaluation in VLMs and discuss theiradvantages over the prior art.</description><author>Sivan Doveh, Shaked Perek, M. Jehanzeb Mirza, Wei Lin, Amit Alfassy, Assaf Arbelle, Shimon Ullman, Leonid Karlinsky</author><pubDate>Wed, 17 Jul 2024 08:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12736v2</guid></item><item><title>When can transformers compositionally generalize in-context?</title><link>http://arxiv.org/abs/2407.12275v1</link><description>Many tasks can be composed from a few independent components. This gives riseto a combinatorial explosion of possible tasks, only some of which might beencountered during training. Under what circumstances can transformerscompositionally generalize from a subset of tasks to all possible combinationsof tasks that share similar components? Here we study a modular multitasksetting that allows us to precisely control compositional structure in the datageneration process. We present evidence that transformers learning in-contextstruggle to generalize compositionally on this task despite being in principleexpressive enough to do so. Compositional generalization becomes possible onlywhen introducing a bottleneck that enforces an explicit separation between taskinference and task execution.</description><author>Seijin Kobayashi, Simon Schug, Yassir Akram, Florian Redhardt, Johannes von Oswald, Razvan Pascanu, Guillaume Lajoie, João Sacramento</author><pubDate>Wed, 17 Jul 2024 02:49:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12275v1</guid></item><item><title>Large Language Models as Reliable Knowledge Bases?</title><link>http://arxiv.org/abs/2407.13578v1</link><description>The NLP community has recently shown a growing interest in leveraging LargeLanguage Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potentialknowledge bases (KBs). However, the reliability and extent to which LLMs canfunction as KBs remain underexplored. While previous studies suggest LLMs canencode knowledge within their parameters, the amount of parametric knowledgealone is not sufficient to evaluate their effectiveness as KBs. This studydefines criteria that a reliable LLM-as-KB should meet, focusing on factualityand consistency, and covering both seen and unseen knowledge. We developseveral metrics based on these criteria and use them to evaluate 26 popularLLMs, while providing a comprehensive analysis of the effects of model size,instruction tuning, and in-context learning (ICL). Our results paint a worryingpicture. Even a high-performant model like GPT-3.5-turbo is not factual orconsistent, and strategies like ICL and fine-tuning are unsuccessful at makingLLMs better KBs.</description><author>Danna Zheng, Mirella Lapata, Jeff Z. Pan</author><pubDate>Thu, 18 Jul 2024 15:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13578v1</guid></item><item><title>Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks</title><link>http://arxiv.org/abs/2407.13511v1</link><description>Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPTand Anthropic's Claude 3 Opus, have dominated natural language processing (NLP)benchmarks across different domains. New competing Open-Source alternativeslike Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap whileoften offering higher throughput and being less costly to use. Open-Source LLMscan also be self-hosted, which makes them interesting for enterprise andclinical use cases where sensitive data should not be processed by thirdparties. We participated in the 12th BioASQ challenge, which is a retrievalaugmented generation (RAG) setting, and explored the performance of current GPTmodels Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning(zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additionalrelevant knowledge from Wikipedia added to the context-window of the LLM mightimprove their performance. Mixtral 8x7b was competitive in the 10-shot setting,both with and without fine-tuning, but failed to produce usable results in thezero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead tomeasurable performance gains. Our results indicate that the performance gapbetween commercial and open-source models in RAG setups exists mainly in thezero-shot setting and can be closed by simply collecting few-shot examples fordomain-specific use cases. The code needed to rerun these experiments isavailable through GitHub.</description><author>Samy Ateia, Udo Kruschwitz</author><pubDate>Thu, 18 Jul 2024 13:43:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13511v1</guid></item><item><title>PALM: Predicting Actions through Language Models</title><link>http://arxiv.org/abs/2311.17944v2</link><description>Understanding human activity is a crucial yet intricate task in egocentricvision, a field that focuses on capturing visual perspectives from the camerawearer's viewpoint. Traditional methods heavily rely on representation learningthat is trained on a large amount of video data. However, a major challengearises from the difficulty of obtaining effective video representation. Thisdifficulty stems from the complex and variable nature of human activities,which contrasts with the limited availability of data. In this study, weintroduce PALM, an approach that tackles the task of long-term actionanticipation, which aims to forecast forthcoming sequences of actions over anextended period. Our method PALM incorporates an action recognition model totrack previous action sequences and a vision-language model to articulaterelevant environmental details. By leveraging the context provided by thesepast events, we devise a prompting strategy for action anticipation using largelanguage models (LLMs). Moreover, we implement maximal marginal relevance forexample selection to facilitate in-context learning of the LLMs. Ourexperimental results demonstrate that PALM surpasses the state-of-the-artmethods in the task of long-term action anticipation on the Ego4D benchmark. Wefurther validate PALM on two additional benchmarks, affirming its capacity forgeneralization across intricate activities with different sets of taxonomies.</description><author>Sanghwan Kim, Daoji Huang, Yongqin Xian, Otmar Hilliges, Luc Van Gool, Xi Wang</author><pubDate>Thu, 18 Jul 2024 10:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17944v2</guid></item><item><title>Learning-From-Mistakes Prompting for Indigenous Language Translation</title><link>http://arxiv.org/abs/2407.13343v1</link><description>Using large language models, this paper presents techniques to improveextremely low-resourced indigenous language translations. Our approaches aregrounded in the use of (1) the presence of a datastore consisting of a limitednumber of parallel translation examples, (2) the inherent capabilities of LLMslike GPT-3.5, and (3) a word-level translation dictionary. We harness thepotential of LLMs and in-context learning techniques in such a setting forusing LLMs as universal translators for extremely low-resourced languages. Ourmethodology hinges on utilizing LLMs as language compilers for selectedlanguage pairs, hypothesizing that they could internalize syntactic structuresto facilitate accurate translation. We introduce three techniques: KNNPromptingwith Retrieved Prompting Context, Chain-of-Thought Prompting andLearningfrom-Mistakes Prompting, with the last method addressing past errors.The evaluation results suggest that, even with limited corpora, LLMs caneffectively translate extremely low-resource languages when paired with properprompting.</description><author>You-Cheng Liao, Chen-Jui Yu, Chi-Yi Lin, He-Feng Yun, Yen-Hsiang Wang, Hsiao-Min Li, Yao-Chung Fan</author><pubDate>Thu, 18 Jul 2024 09:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13343v1</guid></item><item><title>SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning</title><link>http://arxiv.org/abs/2407.13297v1</link><description>Specialized lexicons are collections of words with associated constraintssuch as special definitions, specific roles, and intended target audiences.These constraints are necessary for content generation and documentation tasks(e.g., writing technical manuals or children's books), where the goal is toreduce the ambiguity of text content and increase its overall readability for aspecific group of audience. Understanding how large language models can capturethese constraints can help researchers build better, more impactful tools forwider use beyond the NLP community. Towards this end, we introduce SpeciaLex, abenchmark for evaluating a language model's ability to follow specializedlexicon-based constraints across 18 diverse subtasks with 1,285 test instancescovering core tasks of Checking, Identification, Rewriting, and OpenGeneration. We present an empirical evaluation of 15 open and closed-sourceLLMs and discuss insights on how factors such as model scale, openness, setup,and recency affect performance upon evaluating with the benchmark.</description><author>Joseph Marvin Imperial, Harish Tayyar Madabushi</author><pubDate>Thu, 18 Jul 2024 08:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13297v1</guid></item><item><title>QuRating: Selecting High-Quality Data for Training Language Models</title><link>http://arxiv.org/abs/2402.09739v3</link><description>Selecting high-quality pre-training data is important for creating capablelanguage models, but existing methods rely on simple heuristics. We introduceQuRating, a method for selecting pre-training data that can capture humanintuitions about data quality. In this paper, we investigate four qualities -writing style, required expertise, facts &amp; trivia, and educational value - andfind that LLMs are able to discern these qualities, especially when makingpairwise judgments of texts. We train a QuRater model to learn scalar ratingsfrom pairwise judgments, and use it to annotate a 260B training corpus withquality ratings for each of the four criteria. In our experiments, we select30B tokens according to the different quality ratings and train 1.3B-parameterlanguage models on the selected data. We find that it is important to balancequality and diversity. When we sample using quality ratings as logits overdocuments, our models obtain lower perplexity and stronger in-context learningperformance than baselines. Our best model is based on educational value andperforms similarly to a model trained with uniform sampling for 50% more steps.Beyond data selection, we use the quality ratings to construct a trainingcurriculum which improves performance without changing the training dataset. Weextensively analyze the quality ratings and discuss their characteristics,biases, and wider implications.</description><author>Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen</author><pubDate>Wed, 17 Jul 2024 20:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09739v3</guid></item><item><title>R+X: Retrieval and Execution from Everyday Human Videos</title><link>http://arxiv.org/abs/2407.12957v1</link><description>We present R+X, a framework which enables robots to learn skills from long,unlabelled, first-person videos of humans performing everyday tasks. Given alanguage command from a human, R+X first retrieves short video clips containingrelevant behaviour, and then executes the skill by conditioning an in-contextimitation learning method on this behaviour. By leveraging a Vision LanguageModel (VLM) for retrieval, R+X does not require any manual annotation of thevideos, and by leveraging in-context learning for execution, robots can performcommanded skills immediately, without requiring a period of training on theretrieved videos. Experiments studying a range of everyday household tasks showthat R+X succeeds at translating unlabelled human videos into robust robotskills, and that R+X outperforms several recent alternative methods. Videos areavailable at https://www.robot-learning.uk/r-plus-x.</description><author>Georgios Papagiannis, Norman Di Palo, Pietro Vitiello, Edward Johns</author><pubDate>Wed, 17 Jul 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12957v1</guid></item><item><title>HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context</title><link>http://arxiv.org/abs/2407.09375v2</link><description>This work explores the in-context learning capabilities of State Space Models(SSMs) and presents, to the best of our knowledge, the first theoreticalexplanation of a possible underlying mechanism. We introduce a novel weightconstruction for SSMs, enabling them to predict the next state of any dynamicalsystem after observing previous states without parameter fine-tuning. This isaccomplished by extending the HiPPO framework to demonstrate that continuousSSMs can approximate the derivative of any input signal. Specifically, we findan explicit weight construction for continuous SSMs and provide an asymptoticerror bound on the derivative approximation. The discretization of thiscontinuous SSM subsequently yields a discrete SSM that predicts the next state.Finally, we demonstrate the effectiveness of our parameterization empirically.This work should be an initial step toward understanding how sequence modelsbased on SSMs learn in context.</description><author>Federico Arangath Joseph, Kilian Konstantin Haefeli, Noah Liniger, Caglar Gulcehre</author><pubDate>Fri, 19 Jul 2024 15:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09375v2</guid></item><item><title>Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer</title><link>http://arxiv.org/abs/2309.07566v2</link><description>Direct speech-to-speech translation (S2ST) with discrete self-supervisedrepresentations has achieved remarkable accuracy, but is unable to preserve thespeaker timbre of the source speech. Meanwhile, the scarcity of high-qualityspeaker-parallel data poses a challenge for learning style transfer duringtranslation. We design an S2ST pipeline with style-transfer capability on thebasis of discrete self-supervised speech representations and codec units. Theacoustic language model we introduce for style transfer leveragesself-supervised in-context learning, acquiring style transfer ability withoutrelying on any speaker-parallel data, thereby overcoming data scarcity. Byusing extensive training data, our model achieves zero-shot cross-lingual styletransfer on previously unseen source languages. Experiments show that our modelgenerates translated speeches with high fidelity and speaker similarity. Audiosamples are available at http://stylelm.github.io/ .</description><author>Yongqi Wang, Jionghao Bai, Rongjie Huang, Ruiqi Li, Zhiqing Hong, Zhou Zhao</author><pubDate>Fri, 19 Jul 2024 12:11:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07566v2</guid></item><item><title>Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability</title><link>http://arxiv.org/abs/2407.15720v1</link><description>Large language models (LLMs) have emerged as powerful tools for many AIproblems and exhibit remarkable in-context learning (ICL) capabilities.Compositional ability, solving unseen complex tasks that combine two or moresimple tasks, is an essential reasoning ability for Artificial GeneralIntelligence. Despite LLM's tremendous success, how they approach compositetasks, especially those not encountered during the pretraining phase, remainsan open question and largely ununderstood. In this study, we delve into the ICLcapabilities of LLMs on composite tasks, with only simple tasks as in-contextexamples. We develop a test suite of composite tasks that include linguisticand logical challenges and perform empirical studies across different LLMfamilies. We observe that models exhibit divergent behaviors: (1) For simplercomposite tasks that apply distinct mapping mechanisms to different inputsegments, the models demonstrate decent compositional ability, while scaling upthe model enhances this ability; (2) for more complex composite tasks thatinvolving reasoning multiple steps, where each step represent one task, modelstypically underperform, and scaling up generally provide no improvements. Weoffer theoretical analysis in a simplified setting, explaining that modelsexhibit compositional capability when the task handles different input partsseparately. We believe our work sheds new light on the capabilities of LLMs insolving composite tasks regarding the nature of the tasks and model scale. Ourdataset and code are available at{\url{https://github.com/OliverXUZY/LLM_Compose}}.</description><author>Zhuoyan Xu, Zhenmei Shi, Yingyu Liang</author><pubDate>Mon, 22 Jul 2024 15:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15720v1</guid></item><item><title>SEGIC: Unleashing the Emergent Correspondence for In-Context Segmentation</title><link>http://arxiv.org/abs/2311.14671v3</link><description>In-context segmentation aims at segmenting novel images using a few labeledexample images, termed as "in-context examples", exploring content similaritiesbetween examples and the target. The resulting models can be generalizedseamlessly to novel segmentation tasks, significantly reducing the labeling andtraining costs compared with conventional pipelines. However, in-contextsegmentation is more challenging than classic ones requiring the model to learnsegmentation rules conditioned on a few samples. Unlike previous work withad-hoc or non-end-to-end designs, we propose SEGIC, an end-to-endsegment-in-context framework built upon a single vision foundation model (VFM).In particular, SEGIC leverages the emergent correspondence within VFM tocapture dense relationships between target images and in-context samples. Assuch, information from in-context samples is then extracted into three types ofinstructions, i.e. geometric, visual, and meta instructions, serving asexplicit conditions for the final mask prediction. SEGIC is a straightforwardyet effective approach that yields state-of-the-art performance on one-shotsegmentation benchmarks. Notably, SEGIC can be easily generalized to diversetasks, including video object segmentation and open-vocabulary segmentation.Code will be available at https://github.com/MengLcool/SEGIC.</description><author>Lingchen Meng, Shiyi Lan, Hengduo Li, Jose M. Alvarez, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Mon, 22 Jul 2024 15:16:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14671v3</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v2</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell</author><pubDate>Mon, 22 Jul 2024 13:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v2</guid></item><item><title>Can Large Language Models Automatically Jailbreak GPT-4V?</title><link>http://arxiv.org/abs/2407.16686v1</link><description>GPT-4V has attracted considerable attention due to its extraordinary capacityfor integrating and processing multimodal information. At the same time, itsability of face recognition raises new safety concerns of privacy leakage.Despite researchers' efforts in safety alignment through RLHF or preprocessingfilters, vulnerabilities might still be exploited. In our study, we introduceAutoJailbreak, an innovative automatic jailbreak technique inspired by promptoptimization. We leverage Large Language Models (LLMs) for red-teaming torefine the jailbreak prompt and employ weak-to-strong in-context learningprompts to boost efficiency. Furthermore, we present an effective search methodthat incorporates early stopping to minimize optimization time and tokenexpenditure. Our experiments demonstrate that AutoJailbreak significantlysurpasses conventional methods, achieving an Attack Success Rate (ASR)exceeding 95.3\%. This research sheds light on strengthening GPT-4V security,underscoring the potential for LLMs to be exploited in compromising GPT-4Vintegrity.</description><author>Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, Lichao Sun</author><pubDate>Tue, 23 Jul 2024 17:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16686v1</guid></item><item><title>Interpretable Machine Learning for TabPFN</title><link>http://arxiv.org/abs/2403.10923v2</link><description>The recently developed Prior-Data Fitted Networks (PFNs) have shown verypromising results for applications in low-data regimes. The TabPFN model, aspecial case of PFNs for tabular data, is able to achieve state-of-the-artperformance on a variety of classification tasks while producing posteriorpredictive distributions in mere seconds by in-context learning without theneed for learning parameters or hyperparameter tuning. This makes TabPFN a veryattractive option for a wide range of domain applications. However, a majordrawback of the method is its lack of interpretability. Therefore, we proposeseveral adaptations of popular interpretability methods that we specificallydesign for TabPFN. By taking advantage of the unique properties of the model,our adaptations allow for more efficient computations than existingimplementations. In particular, we show how in-context learning facilitates theestimation of Shapley values by avoiding approximate retraining and enables theuse of Leave-One-Covariate-Out (LOCO) even when working with large-scaleTransformers. In addition, we demonstrate how data valuation methods can beused to address scalability challenges of TabPFN. Our proposed methods areimplemented in a package tabpfn_iml and made available athttps://github.com/david-rundel/tabpfn_iml.</description><author>David Rundel, Julius Kobialka, Constantin von Crailsheim, Matthias Feurer, Thomas Nagler, David Rügamer</author><pubDate>Tue, 23 Jul 2024 16:10:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10923v2</guid></item><item><title>Assessing In-context Learning and Fine-tuning for Topic Classification of German Web Data</title><link>http://arxiv.org/abs/2407.16516v1</link><description>Researchers in the political and social sciences often rely on classificationmodels to analyze trends in information consumption by examining browsinghistories of millions of webpages. Automated scalable methods are necessary dueto the impracticality of manual labeling. In this paper, we model the detectionof topic-related content as a binary classification task and compare theaccuracy of fine-tuned pre-trained encoder models against in-context learningstrategies. Using only a few hundred annotated data points per topic, we detectcontent related to three German policies in a database of scraped webpages. Wecompare multilingual and monolingual models, as well as zero and few-shotapproaches, and investigate the impact of negative sampling strategies and thecombination of URL &amp; content-based features. Our results show that a smallsample of annotated data is sufficient to train an effective classifier.Fine-tuning encoder-based models yields better results than in-contextlearning. Classifiers using both URL &amp; content-based features perform best,while using URLs alone provides adequate results when content is unavailable.</description><author>Julian Schelb, Roberto Ulloa, Andreas Spitz</author><pubDate>Tue, 23 Jul 2024 14:31:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16516v1</guid></item><item><title>Does In-Context Learning Really Learn? Rethinking How Large Language Models Respond and Solve Tasks via In-Context Learning</title><link>http://arxiv.org/abs/2404.07546v2</link><description>In-context Learning (ICL) has emerged as a powerful capability alongside thedevelopment of scaled-up large language models (LLMs). By instructing LLMsusing few-shot demonstrative examples, ICL enables them to perform a wide rangeof tasks without updating millions of parameters. However, the precisecontributions of demonstrations towards improving end-task performance have notbeen thoroughly investigated in recent analytical studies. In this paper, weempirically decompose the overall performance of ICL into three dimensions,label space, format, and discrimination, and we evaluate four general-purposeLLMs across a diverse range of tasks. Counter-intuitively, we find that thedemonstrations have a marginal impact on provoking discriminative knowledge oflanguage models. However, ICL exhibits significant efficacy in regulating thelabel space and format, which helps LLMs respond to desired label words. Wethen demonstrate that this ability functions similar to detailed instructionsfor LLMs to follow. We additionally provide an in-depth analysis of themechanism of retrieval helping with ICL. Our findings demonstrate thatretrieving the semantically similar examples notably boosts the model'sdiscriminative capability. However, we also observe a trade-off in selectinggood in-context examples regarding label diversity.</description><author>Quanyu Long, Yin Wu, Wenya Wang, Sinno Jialin Pan</author><pubDate>Tue, 23 Jul 2024 12:28:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07546v2</guid></item><item><title>Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction</title><link>http://arxiv.org/abs/2407.16370v1</link><description>Building upon the strength of modern large language models (LLMs), generativeerror correction (GEC) has emerged as a promising paradigm that can elevate theperformance of modern automatic speech recognition (ASR) systems. Onerepresentative approach is to leverage in-context learning to prompt LLMs sothat a better hypothesis can be generated by the LLMs based on acarefully-designed prompt and an $N$-best list of hypotheses produced by ASRsystems. However, it is yet unknown whether the existing prompts are the mosteffective ones for the task of post-ASR error correction. In this context, thispaper first explores alternative prompts to identify an initial set ofeffective prompts, and then proposes to employ an evolutionary promptoptimization algorithm to refine the initial prompts. Evaluations results onthe CHiME-4 subset of the Task $1$ of the SLT $2024$ GenSEC challenge show theeffectiveness and potential of the proposed algorithms.</description><author>Rithik Sachdev, Zhong-Qiu Wang, Chao-Han Huck Yang</author><pubDate>Tue, 23 Jul 2024 10:38:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16370v1</guid></item><item><title>PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing</title><link>http://arxiv.org/abs/2407.16318v1</link><description>Deploying language models (LMs) necessitates outputs to be both high-qualityand compliant with safety guidelines. Although Inference-Time Guardrails (ITG)offer solutions that shift model output distributions towards compliance, wefind that current methods struggle in balancing safety with helpfulness. ITGMethods that safely address non-compliant queries exhibit lower helpfulnesswhile those that prioritize helpfulness compromise on safety. We refer to thistrade-off as the guardrail tax, analogous to the alignment tax. To addressthis, we propose PrimeGuard, a novel ITG method that utilizes structuredcontrol flow. PrimeGuard routes requests to different self-instantiations of the LM withvarying instructions, leveraging its inherent instruction-followingcapabilities and in-context learning. Our tuning-free approach dynamicallycompiles system-designer guidelines for each query. We construct and releasesafe-eval, a diverse red-team safety benchmark. Extensive evaluationsdemonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail taxby (1) significantly increasing resistance to iterative jailbreak attacks and(2) achieving state-of-the-art results in safety guardrailing while (3)matching helpfulness scores of alignment-tuned models. Extensive evaluationsdemonstrate that PrimeGuard, without fine-tuning, outperforms all competingbaselines and overcomes the guardrail tax by improving the fraction of saferesponses from 61% to 97% and increasing average helpfulness scores from 4.17to 4.29 on the largest models, while reducing attack success rate from 100% to8%. PrimeGuard implementation is available athttps://github.com/dynamofl/PrimeGuard and safe-eval dataset is available athttps://huggingface.co/datasets/dynamoai/safe_eval.</description><author>Blazej Manczak, Eliott Zemour, Eric Lin, Vaikkunth Mugunthan</author><pubDate>Tue, 23 Jul 2024 09:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16318v1</guid></item><item><title>Grammar-based Game Description Generation using Large Language Models</title><link>http://arxiv.org/abs/2407.17404v1</link><description>To lower the barriers to game design development, automated game design,which generates game designs through computational processes, has beenexplored. In automated game design, machine learning-based techniques such asevolutionary algorithms have achieved success. Benefiting from the remarkableadvancements in deep learning, applications in computer vision and naturallanguage processing have progressed in level generation. However, due to thelimited amount of data in game design, the application of deep learning hasbeen insufficient for tasks such as game description generation. To pioneer anew approach for handling limited data in automated game design, we focus onthe in-context learning of large language models (LLMs). LLMs can capture thefeatures of a task from a few demonstration examples and apply the capabilitiesacquired during pre-training. We introduce the grammar of game descriptions,which effectively structures the game design space, into the LLMs' reasoningprocess. Grammar helps LLMs capture the characteristics of the complex task ofgame description generation. Furthermore, we propose a decoding method thatiteratively improves the generated output by leveraging the grammar. Ourexperiments demonstrate that this approach performs well in generating gamedescriptions.</description><author>Tsunehiko Tanaka, Edgar Simo-Serra</author><pubDate>Wed, 24 Jul 2024 16:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17404v1</guid></item><item><title>Identifying Semantic Induction Heads to Understand In-Context Learning</title><link>http://arxiv.org/abs/2402.13055v2</link><description>Although large language models (LLMs) have demonstrated remarkableperformance, the lack of transparency in their inference logic raises concernsabout their trustworthiness. To gain a better understanding of LLMs, we conducta detailed analysis of the operations of attention heads and aim to betterunderstand the in-context learning of LLMs. Specifically, we investigatewhether attention heads encode two types of relationships between tokenspresent in natural languages: the syntactic dependency parsed from sentencesand the relation within knowledge graphs. We find that certain attention headsexhibit a pattern where, when attending to head tokens, they recall tail tokensand increase the output logits of those tail tokens. More crucially, theformulation of such semantic induction heads has a close correlation with theemergence of the in-context learning ability of language models. The study ofsemantic attention heads advances our understanding of the intricate operationsof attention heads in transformers, and further provides new insights into thein-context learning of LLMs.</description><author>Jie Ren, Qipeng Guo, Hang Yan, Dongrui Liu, Quanshi Zhang, Xipeng Qiu, Dahua Lin</author><pubDate>Thu, 25 Jul 2024 08:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13055v2</guid></item><item><title>Unified Lexical Representation for Interpretable Visual-Language Alignment</title><link>http://arxiv.org/abs/2407.17827v1</link><description>Visual-Language Alignment (VLA) has gained a lot of attention since CLIP'sgroundbreaking work. Although CLIP performs well, the typical direct latentfeature alignment lacks clarity in its representation and similarity scores. Onthe other hand, lexical representation, a vector whose element represents thesimilarity between the sample and a word from the vocabulary, is a naturalsparse representation and interpretable, providing exact matches for individualwords. However, lexical representations is difficult to learn due to noground-truth supervision and false-discovery issues, and thus requires complexdesign to train effectively. In this paper, we introduce LexVLA, a moreinterpretable VLA framework by learning a unified lexical representation forboth modalities without complex design. We use DINOv2 as our visual model forits local-inclined features and Llama 2, a generative language model, toleverage its in-context lexical prediction ability. To avoid the falsediscovery, we propose an overuse penalty to refrain the lexical representationfrom falsely frequently activating meaningless words. We demonstrate that thesetwo pre-trained uni-modal models can be well-aligned by fine-tuning on modestmulti-modal dataset and avoid intricate training configurations. On cross-modalretrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset,outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and thosetrained from scratch on even bigger datasets (e.g., 1.1B data, includingCC-12M). We conduct extensive experiments to analyze LexVLA.</description><author>Yifan Li, Yikai Wang, Yanwei Fu, Dongyu Ru, Zheng Zhang, Tong He</author><pubDate>Thu, 25 Jul 2024 07:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17827v1</guid></item><item><title>Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples</title><link>http://arxiv.org/abs/2402.07386v2</link><description>Automatic taxonomy induction is crucial for web search, recommendationsystems, and question answering. Manual curation of taxonomies is expensive interms of human effort, making automatic taxonomy construction highly desirable.In this work, we introduce Chain-of-Layer which is an in-context learningframework designed to induct taxonomies from a given set of entities.Chain-of-Layer breaks down the task into selecting relevant candidate entitiesin each layer and gradually building the taxonomy from top to bottom. Tominimize errors, we introduce the Ensemble-based Ranking Filter to reduce thehallucinated content generated at each iteration. Through extensiveexperiments, we demonstrate that Chain-of-Layer achieves state-of-the-artperformance on four real-world benchmarks.</description><author>Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Zhenwen Liang, Zhihan Zhang, Meng Jiang</author><pubDate>Thu, 25 Jul 2024 02:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07386v2</guid></item><item><title>Transformers on Markov Data: Constant Depth Suffices</title><link>http://arxiv.org/abs/2407.17686v1</link><description>Attention-based transformers have been remarkably successful at modelinggenerative processes across various domains and modalities. In this paper, westudy the behavior of transformers on data drawn from \kth Markov processes,where the conditional distribution of the next symbol in a sequence depends onthe previous $k$ symbols observed. We observe a surprising phenomenonempirically which contradicts previous findings: when trained for sufficientlylong, a transformer with a fixed depth and $1$ head per layer is able toachieve low test loss on sequences drawn from \kth Markov sources, even as $k$grows. Furthermore, this low test loss is achieved by the transformer's abilityto represent and learn the in-context conditional empirical distribution. Onthe theoretical side, our main result is that a transformer with a single headand three layers can represent the in-context conditional empiricaldistribution for \kth Markov sources, concurring with our empiricalobservations. Along the way, we prove that \textit{attention-only} transformerswith $O(\log_2(k))$ layers can represent the in-context conditional empiricaldistribution by composing induction heads to track the previous $k$ symbols inthe sequence. These results provide more insight into our current understandingof the mechanisms by which transformers learn to capture context, byunderstanding their behavior on Markov sources.</description><author>Nived Rajaraman, Marco Bondaschi, Kannan Ramchandran, Michael Gastpar, Ashok Vardhan Makkuva</author><pubDate>Thu, 25 Jul 2024 01:07:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17686v1</guid></item><item><title>Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements</title><link>http://arxiv.org/abs/2310.05140v4</link><description>Empathetic dialogue is an indispensable part of building harmonious socialrelationships and contributes to the development of a helpful AI. Previousapproaches are mainly based on fine small-scale language models. With theadvent of ChatGPT, the application effect of large language models (LLMs) inthis field has attracted great attention. This work empirically investigatesthe performance of LLMs in generating empathetic responses and proposes threeimprovement methods of semantically similar in-context learning, two-stageinteractive generation, and combination with the knowledge base. Extensiveexperiments show that LLMs can significantly benefit from our proposed methodsand is able to achieve state-of-the-art performance in both automatic and humanevaluations. Additionally, we explore the possibility of GPT-4 simulating humanevaluators.</description><author>Yushan Qian, Wei-Nan Zhang, Ting Liu</author><pubDate>Fri, 26 Jul 2024 15:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05140v4</guid></item><item><title>TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting</title><link>http://arxiv.org/abs/2406.01805v2</link><description>Tabular data is prevalent in many critical domains, yet it is oftenchallenging to acquire in large quantities. This scarcity usually results inpoor performance of machine learning models on such data. Data augmentation, acommon strategy for performance improvement in vision and language tasks,typically underperforms for tabular data due to the lack of explicit symmetriesin the input space. To overcome this challenge, we introduce TabMDA, a novelmethod for manifold data augmentation on tabular data. This method utilises apre-trained in-context model, such as TabPFN, to map the data into an embeddingspace. TabMDA performs label-invariant transformations by encoding the datamultiple times with varied contexts. This process explores the learnedembedding space of the underlying in-context models, thereby enlarging thetraining dataset. TabMDA is a training-free method, making it applicable to anyclassifier. We evaluate TabMDA on five standard classifiers and observesignificant performance improvements across various tabular datasets. Ourresults demonstrate that TabMDA provides an effective way to leverageinformation from pre-trained in-context models to enhance the performance ofdownstream classifiers. Code is available athttps://github.com/AdrianBZG/TabMDA.</description><author>Andrei Margeloiu, Adrián Bazaga, Nikola Simidjievski, Pietro Liò, Mateja Jamnik</author><pubDate>Mon, 29 Jul 2024 15:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01805v2</guid></item><item><title>MICL: Improving In-Context Learning through Multiple-Label Words in Demonstration</title><link>http://arxiv.org/abs/2406.10908v2</link><description>In-context learning (ICL) enables large language models (LLMs) to perform newtasks by using sample-label pairs as demonstrations. However, variations indemonstrations can lead to significantly different performances. Currentresearch mainly focuses on selecting demonstration samples, preassuming theclass name to be the label word when creating sample-label pairs. However, thechoice of label words is crucial for ICL performance. In addition, we observethat using a single class name in demonstration may not yield optimal results.In this paper, we propose to use multiple label words in one sample-label pairto enhance ICL performance. Further, we select and order sample-label pairsbased on LLM's output distribution, aiming to optimize the demonstrationexamples from both the samples' and labels' perspectives. Evaluation results onseven classification datasets show that the use of multiple label words,strategically organized by their selection, order and quantity, improves ICLperformance through diverse label information.</description><author>Zhu Zixiao, Feng Zijian, Zhou Hanzhang, Qian Junlang, Mao Kezhi</author><pubDate>Mon, 29 Jul 2024 13:05:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10908v2</guid></item><item><title>AgEval: A Benchmark for Zero-Shot and Few-Shot Plant Stress Phenotyping with Multimodal LLMs</title><link>http://arxiv.org/abs/2407.19617v1</link><description>Plant stress phenotyping traditionally relies on expert assessments andspecialized models, limiting scalability in agriculture. Recent advances inmultimodal large language models (LLMs) offer potential solutions to thischallenge. We present AgEval, a benchmark comprising 12 diverse plant stressphenotyping tasks, to evaluate these models' capabilities. Our study assesseszero-shot and few-shot in-context learning performance of state-of-the-artmodels, including Claude, GPT, Gemini, and LLaVA. Results show significantperformance improvements with few-shot learning, with F1 scores increasing from46.24% to 73.37% in 8-shot identification for the best-performing model.Few-shot examples from other classes in the dataset have negligible or negativeimpacts, although having the exact category example helps to increaseperformance by 15.38%. We also quantify the consistency of model performanceacross different classes within each task, finding that the coefficient ofvariance (CV) ranges from 26.02% to 58.03% across models, implying that subjectmatter expertise is needed - of 'difficult' classes - to achieve reliability inperformance. AgEval establishes baseline metrics for multimodal LLMs inagricultural applications, offering insights into their promise for enhancingplant stress phenotyping at scale. Benchmark and code can be accessed at:https://anonymous.4open.science/r/AgEval/</description><author>Muhammad Arbab Arshad, Talukder Zaki Jubery, Tirtho Roy, Rim Nassiri, Asheesh K. Singh, Arti Singh, Chinmay Hegde, Baskar Ganapathysubramanian, Aditya Balu, Adarsh Krishnamurthy, Soumik Sarkar</author><pubDate>Mon, 29 Jul 2024 00:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19617v1</guid></item><item><title>Polynomial Regression as a Task for Understanding In-context Learning Through Finetuning and Alignment</title><link>http://arxiv.org/abs/2407.19346v1</link><description>Simple function classes have emerged as toy problems to better understandin-context-learning in transformer-based architectures used for large languagemodels. But previously proposed simple function classes like linear regressionor multi-layer-perceptrons lack the structure required to explore things likeprompting and alignment within models capable of in-context-learning. Wepropose univariate polynomial regression as a function class that is just richenough to study prompting and alignment, while allowing us to visualize andunderstand what is going on clearly.</description><author>Max Wilcoxson, Morten Svendgård, Ria Doshi, Dylan Davis, Reya Vir, Anant Sahai</author><pubDate>Sat, 27 Jul 2024 22:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19346v1</guid></item><item><title>Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications</title><link>http://arxiv.org/abs/2407.19262v1</link><description>Understanding whether and to what extent large language models (LLMs) havememorised training data has important implications for the reliability of theiroutput and the privacy of their training data. In order to cleanly measure anddisentangle memorisation from other phenomena (e.g. in-context learning), wecreate an experimental framework that is based on repeatedly exposing LLMs torandom strings. Our framework allows us to better understand the dynamics,i.e., the behaviour of the model, when repeatedly exposing it to randomstrings. Using our framework, we make several striking observations: (a) wefind consistent phases of the dynamics across families of models (Pythia, Phiand Llama2), (b) we identify factors that make some strings easier to memorisethan others, and (c) we identify the role of local prefixes and global contextin memorisation. We also show that sequential exposition to different randomstrings has a significant effect on memorisation. Our results, oftensurprising, have significant downstream implications in the study and usage ofLLMs.</description><author>Till Speicher, Mohammad Aflah Khan, Qinyuan Wu, Vedant Nanda, Soumi Das, Bishwamittra Ghosh, Krishna P. Gummadi, Evimaria Terzi</author><pubDate>Sat, 27 Jul 2024 14:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19262v1</guid></item><item><title>SSPA: Split-and-Synthesize Prompting with Gated Alignments for Multi-Label Image Recognition</title><link>http://arxiv.org/abs/2407.20920v1</link><description>Multi-label image recognition is a fundamental task in computer vision.Recently, Vision-Language Models (VLMs) have made notable advancements in thisarea. However, previous methods fail to effectively leverage the rich knowledgein language models and often incorporate label semantics into visual featuresunidirectionally. To overcome these problems, we propose a Split-and-SynthesizePrompting with Gated Alignments (SSPA) framework to amplify the potential ofVLMs. Specifically, we develop an in-context learning approach to associate theinherent knowledge from LLMs. Then we propose a novel Split-and-SynthesizePrompting (SSP) strategy to first model the generic knowledge and downstreamlabel semantics individually and then aggregate them carefully through thequaternion network. Moreover, we present Gated Dual-Modal Alignments (GDMA) tobidirectionally interact visual and linguistic modalities while eliminatingredundant cross-modal information, enabling more efficient region-levelalignments. Rather than making the final prediction by a sharp manner inprevious works, we propose a soft aggregator to jointly consider results fromall image regions. With the help of flexible prompting and gated alignments,SSPA is generalizable to specific domains. Extensive experiments on ninedatasets from three domains (i.e., natural, pedestrian attributes and remotesensing) demonstrate the state-of-the-art performance of SSPA. Further analysesverify the effectiveness of SSP and the interpretability of GDMA. The code willbe made public.</description><author>Hao Tan, Zichang Tan, Jun Li, Jun Wan, Zhen Lei, Stan Z. Li</author><pubDate>Tue, 30 Jul 2024 15:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20920v1</guid></item><item><title>SceneTeller: Language-to-3D Scene Generation</title><link>http://arxiv.org/abs/2407.20727v1</link><description>Designing high-quality indoor 3D scenes is important in many practicalapplications, such as room planning or game development. Conventionally, thishas been a time-consuming process which requires both artistic skill andfamiliarity with professional software, making it hardly accessible for laymanusers. However, recent advances in generative AI have established solidfoundation for democratizing 3D design. In this paper, we propose a pioneeringapproach for text-based 3D room design. Given a prompt in natural languagedescribing the object placement in the room, our method produces a high-quality3D scene corresponding to it. With an additional text prompt the users canchange the appearance of the entire scene or of individual objects in it. Builtusing in-context learning, CAD model retrieval and 3D-Gaussian-Splatting-basedstylization, our turnkey pipeline produces state-of-the-art 3D scenes, whilebeing easy to use even for novices. Our project page is available athttps://sceneteller.github.io/.</description><author>Başak Melis Öcal, Maxim Tatarchenko, Sezer Karaoglu, Theo Gevers</author><pubDate>Tue, 30 Jul 2024 10:45:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20727v1</guid></item><item><title>Chat2Layout: Interactive 3D Furniture Layout with a Multimodal LLM</title><link>http://arxiv.org/abs/2407.21333v1</link><description>Automatic furniture layout is long desired for convenient interior design.Leveraging the remarkable visual reasoning capabilities of multimodal largelanguage models (MLLMs), recent methods address layout generation in a staticmanner, lacking the feedback-driven refinement essential for interactive userengagement. We introduce Chat2Layout, a novel interactive furniture layoutgeneration system that extends the functionality of MLLMs into the realm ofinteractive layout design. To achieve this, we establish a unifiedvision-question paradigm for in-context learning, enabling seamlesscommunication with MLLMs to steer their behavior without altering modelweights. Within this framework, we present a novel training-free visualprompting mechanism. This involves a visual-text prompting technique thatassist MLLMs in reasoning about plausible layout plans, followed by anOffline-to-Online search (O2O-Search) method, which automatically identifiesthe minimal set of informative references to provide exemplars for visual-textprompting. By employing an agent system with MLLMs as the core controller, weenable bidirectional interaction. The agent not only comprehends the 3Denvironment and user requirements through linguistic and visual perception butalso plans tasks and reasons about actions to generate and arrange furniturewithin the virtual space. Furthermore, the agent iteratively updates based onvisual feedback from execution results. Experimental results demonstrate thatour approach facilitates language-interactive generation and arrangement fordiverse and complex 3D furniture.</description><author>Can Wang, Hongliang Zhong, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao</author><pubDate>Wed, 31 Jul 2024 04:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21333v1</guid></item></channel></rss>