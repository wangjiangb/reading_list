<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 04 Jun 2024 06:00:14 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>In-Context Learning Dynamics with Random Binary Sequences</title><link>http://arxiv.org/abs/2310.17639v3</link><description>Large language models (LLMs) trained on huge corpora of text datasetsdemonstrate intriguing capabilities, achieving state-of-the-art performance ontasks they were not explicitly trained for. The precise nature of LLMcapabilities is often mysterious, and different prompts can elicit differentcapabilities through in-context learning. We propose a framework that enablesus to analyze in-context learning dynamics to understand latent conceptsunderlying LLMs' behavioral patterns. This provides a more nuancedunderstanding than success-or-failure evaluation benchmarks, but does notrequire observing internal activations as a mechanistic interpretation ofcircuits would. Inspired by the cognitive science of human randomnessperception, we use random binary sequences as context and study dynamics ofin-context learning by manipulating properties of context data, such assequence length. In the latest GPT-3.5+ models, we find emergent abilities togenerate seemingly random numbers and learn basic formal languages, withstriking in-context learning dynamics where model outputs transition sharplyfrom seemingly random behaviors to deterministic repetition.</description><author>Eric J. Bigelow, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Tomer D. Ullman</author><pubDate>Tue, 16 Apr 2024 02:35:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17639v3</guid></item><item><title>In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering</title><link>http://arxiv.org/abs/2311.06668v3</link><description>Large language models (LLMs) demonstrate emergent in-context learningcapabilities, where they adapt to new tasks based on example demonstrations.However, in-context learning has seen limited effectiveness in many settings,is difficult to quantitatively control and takes up context window space. Toovercome these limitations, we propose an alternative approach that recastsin-context learning as in-context vectors (ICV). Using ICV has two steps. Wefirst use a forward pass on demonstration examples to create the in-contextvector from the latent embedding of the LLM. This vector captures essentialinformation about the intended task. On a new query, instead of addingdemonstrations to the prompt, we shift the latent states of the LLM using theICV. The ICV approach has several benefits: 1) it enables the LLM to moreeffectively follow the demonstration examples; 2) it's easy to control byadjusting the magnitude of the ICV; 3) it reduces the length of the prompt byremoving the in-context demonstrations; 4) ICV is computationally much moreefficient than fine-tuning. We demonstrate that ICV achieves better performancecompared to standard in-context learning and fine-tuning on diverse tasksincluding safety, style transfer, role-playing and formatting. Moreover, weshow that we can flexibly teach LLM to simultaneously follow different types ofinstructions by simple vector arithmetics on the corresponding ICVs.</description><author>Sheng Liu, Haotian Ye, Lei Xing, James Zou</author><pubDate>Tue, 13 Feb 2024 22:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06668v3</guid></item><item><title>The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis</title><link>http://arxiv.org/abs/2402.12976v1</link><description>In-context learning is a popular inference strategy where large languagemodels solve a task using only a few labelled demonstrations without needingany parameter updates. Compared to work on monolingual (English) in-contextlearning, multilingual in-context learning is under-explored, and we lack anin-depth understanding of the role of demonstrations in this context. Toaddress this gap, we conduct a multidimensional analysis of multilingualin-context learning, experimenting with 5 models from different model families,9 datasets covering classification and generation tasks, and 56 typologicallydiverse languages. Our results reveal that the effectiveness of demonstrationsvaries significantly across models, tasks, and languages. We also find thatLlama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality ofdemonstrations. Instead, a carefully crafted template often eliminates thebenefits of demonstrations for some tasks and languages altogether. Thesefindings show that the importance of demonstrations might be overestimated. Ourwork highlights the need for granular evaluation across multiple axes towards abetter understanding of in-context learning.</description><author>Miaoran Zhang, Vagrant Gautam, Mingyang Wang, Jesujoba O. Alabi, Xiaoyu Shen, Dietrich Klakow, Marius Mosbach</author><pubDate>Tue, 20 Feb 2024 12:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12976v1</guid></item><item><title>Uncertainty Quantification for In-Context Learning of Large Language Models</title><link>http://arxiv.org/abs/2402.10189v2</link><description>In-context learning has emerged as a groundbreaking ability of Large LanguageModels (LLMs) and revolutionized various fields by providing a fewtask-relevant demonstrations in the prompt. However, trustworthy issues withLLM's response, such as hallucination, have also been actively discussed.Existing works have been devoted to quantifying the uncertainty in LLM'sresponse, but they often overlook the complex nature of LLMs and the uniquenessof in-context learning. In this work, we delve into the predictive uncertaintyof LLMs associated with in-context learning, highlighting that suchuncertainties may stem from both the provided demonstrations (aleatoricuncertainty) and ambiguities tied to the model's configurations (epistemicuncertainty). We propose a novel formulation and corresponding estimationmethod to quantify both types of uncertainties. The proposed method offers anunsupervised way to understand the prediction of in-context learning in aplug-and-play fashion. Extensive experiments are conducted to demonstrate theeffectiveness of the decomposition. The code and data are available at:https://github.com/lingchen0331/UQ_ICL.</description><author>Chen Ling, Xujiang Zhao, Xuchao Zhang, Wei Cheng, Yanchi Liu, Yiyou Sun, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</author><pubDate>Thu, 28 Mar 2024 20:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10189v2</guid></item><item><title>Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models</title><link>http://arxiv.org/abs/2402.10189v1</link><description>In-context learning has emerged as a groundbreaking ability of Large LanguageModels (LLMs) and revolutionized various fields by providing a fewtask-relevant demonstrations in the prompt. However, trustworthy issues withLLM's response, such as hallucination, have also been actively discussed.Existing works have been devoted to quantifying the uncertainty in LLM'sresponse, but they often overlook the complex nature of LLMs and the uniquenessof in-context learning. In this work, we delve into the predictive uncertaintyof LLMs associated with in-context learning, highlighting that suchuncertainties may stem from both the provided demonstrations (aleatoricuncertainty) and ambiguities tied to the model's configurations (epistemicuncertainty). We propose a novel formulation and corresponding estimationmethod to quantify both types of uncertainties. The proposed method offers anunsupervised way to understand the prediction of in-context learning in aplug-and-play fashion. Extensive experiments are conducted to demonstrate theeffectiveness of the decomposition. The code and data are available at:\url{https://github.com/lingchen0331/UQ_ICL}.</description><author>Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</author><pubDate>Thu, 15 Feb 2024 18:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10189v1</guid></item><item><title>Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability</title><link>http://arxiv.org/abs/2310.08049v3</link><description>What is the relationship between model architecture and the ability toperform in-context learning? In this empirical study, we take the first stepstoward answering this question. We evaluate thirteen model architecturescapable of causal language modeling across a suite of synthetic in-contextlearning tasks. These selected architectures represent a broad range ofparadigms, including recurrent and convolution-based neural networks,transformers, state space model inspired, and other emerging attentionalternatives. We discover that all the considered architectures can performin-context learning under a wider range of conditions than previouslydocumented. Additionally, we observe stark differences in statisticalefficiency and consistency by varying the number of in-context examples andtask difficulty. We also measure each architecture's predisposition towardsin-context learning when presented with the option to memorize rather thanleverage in-context examples. Finally, and somewhat surprisingly, we find thatseveral attention alternatives are sometimes competitive with or betterin-context learners than transformers. However, no single architecturedemonstrates consistency across all tasks, with performance either plateauingor declining when confronted with a significantly larger number of in-contextexamples than those encountered during gradient-based training.</description><author>Ivan Lee, Nan Jiang, Taylor Berg-Kirkpatrick</author><pubDate>Tue, 02 Apr 2024 02:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08049v3</guid></item><item><title>Understanding In-Context Learning from Repetitions</title><link>http://arxiv.org/abs/2310.00297v3</link><description>This paper explores the elusive mechanism underpinning in-context learning inLarge Language Models (LLMs). Our work provides a novel perspective byexamining in-context learning via the lens of surface repetitions. Wequantitatively investigate the role of surface features in text generation, andempirically establish the existence of \emph{token co-occurrencereinforcement}, a principle that strengthens the relationship between twotokens based on their contextual co-occurrences. By investigating the dualimpacts of these features, our research illuminates the internal workings ofin-context learning and expounds on the reasons for its failures. This paperprovides an essential contribution to the understanding of in-context learningand its potential limitations, providing a fresh perspective on this excitingcapability.</description><author>Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, Yue Zhang</author><pubDate>Wed, 21 Feb 2024 09:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00297v3</guid></item><item><title>Exploring the Robustness of In-Context Learning with Noisy Labels</title><link>http://arxiv.org/abs/2404.18191v1</link><description>Recently, the mysterious In-Context Learning (ICL) ability exhibited byTransformer architectures, especially in large language models (LLMs), hassparked significant research interest. However, the resilience of Transformers'in-context learning capabilities in the presence of noisy samples, prevalent inboth training corpora and prompt demonstrations, remains underexplored. In thispaper, inspired by prior research that studies ICL ability using simplefunction classes, we take a closer look at this problem by investigating therobustness of Transformers against noisy labels. Specifically, we first conducta thorough evaluation and analysis of the robustness of Transformers againstnoisy labels during in-context learning and show that they exhibit notableresilience against diverse types of noise in demonstration labels. Furthermore,we delve deeper into this problem by exploring whether introducing noise intothe training set, akin to a form of data augmentation, enhances such robustnessduring inference, and find that such noise can indeed improve the robustness ofICL. Overall, our fruitful analysis and findings provide a comprehensiveunderstanding of the resilience of Transformer models against label noisesduring ICL and provide valuable insights into the research on Transformers innatural language processing. Our code is available athttps://github.com/InezYu0928/in-context-learning.</description><author>Chen Cheng, Xinzhi Yu, Haodong Wen, Jinsong Sun, Guanzhang Yue, Yihao Zhang, Zeming Wei</author><pubDate>Sun, 28 Apr 2024 15:05:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18191v1</guid></item><item><title>Exploring the Robustness of In-Context Learning with Noisy Labels</title><link>http://arxiv.org/abs/2404.18191v2</link><description>Recently, the mysterious In-Context Learning (ICL) ability exhibited byTransformer architectures, especially in large language models (LLMs), hassparked significant research interest. However, the resilience of Transformers'in-context learning capabilities in the presence of noisy samples, prevalent inboth training corpora and prompt demonstrations, remains underexplored. In thispaper, inspired by prior research that studies ICL ability using simplefunction classes, we take a closer look at this problem by investigating therobustness of Transformers against noisy labels. Specifically, we first conducta thorough evaluation and analysis of the robustness of Transformers againstnoisy labels during in-context learning and show that they exhibit notableresilience against diverse types of noise in demonstration labels. Furthermore,we delve deeper into this problem by exploring whether introducing noise intothe training set, akin to a form of data augmentation, enhances such robustnessduring inference, and find that such noise can indeed improve the robustness ofICL. Overall, our fruitful analysis and findings provide a comprehensiveunderstanding of the resilience of Transformer models against label noisesduring ICL and provide valuable insights into the research on Transformers innatural language processing. Our code is available athttps://github.com/InezYu0928/in-context-learning.</description><author>Chen Cheng, Xinzhi Yu, Haodong Wen, Jingsong Sun, Guanzhang Yue, Yihao Zhang, Zeming Wei</author><pubDate>Wed, 01 May 2024 10:15:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18191v2</guid></item><item><title>The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis</title><link>http://arxiv.org/abs/2311.00237v2</link><description>Understanding in-context learning (ICL) capability that enables largelanguage models (LLMs) to excel in proficiency through demonstration examplesis of utmost importance. This importance stems not only from the betterutilization of this capability across various tasks, but also from theproactive identification and mitigation of potential risks, including concernsregarding truthfulness, bias, and toxicity, that may arise alongside thecapability. In this paper, we present a thorough survey on the interpretationand analysis of in-context learning. First, we provide a concise introductionto the background and definition of in-context learning. Then, we give anoverview of advancements from two perspectives: 1) a theoretical perspective,emphasizing studies on mechanistic interpretability and delving into themathematical foundations behind ICL; and 2) an empirical perspective,concerning studies that empirically analyze factors associated with ICL. Weconclude by highlighting the challenges encountered and suggesting potentialavenues for future research. We believe that our work establishes the basis forfurther exploration into the interpretation of in-context learning.Additionally, we have created a repository containing the resources referencedin our survey.</description><author>Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He</author><pubDate>Fri, 16 Feb 2024 00:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00237v2</guid></item><item><title>Supervised Knowledge Makes Large Language Models Better In-context Learners</title><link>http://arxiv.org/abs/2312.15918v2</link><description>Large Language Models (LLMs) exhibit emerging in-context learning abilitiesthrough prompt engineering. The recent progress in large-scale generativemodels has further expanded their use in real-world language applications.However, the critical challenge of improving the generalizability andfactuality of LLMs in natural language understanding and question answeringremains under-explored. While previous in-context learning research has focusedon enhancing models to adhere to users' specific instructions and qualityexpectations, and to avoid undesired outputs, little to no work has exploredthe use of task-Specific fine-tuned Language Models (SLMs) to improve LLMs'in-context learning during the inference stage. Our primary contribution is theestablishment of a simple yet effective framework that enhances the reliabilityof LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMsbenefit from discriminative models, and 3) minimizes hallucinations ingenerative tasks. Using our proposed plug-in method, enhanced versions of Llama2 and ChatGPT surpass their original versions regarding generalizability andfactuality. We offer a comprehensive suite of resources, including 16 curateddatasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks.The code and data are released at:https://github.com/YangLinyi/Supervised-Knowledge-Makes-Large-Language-Models-Better-In-context-Learners.Our empirical analysis sheds light on the advantages of incorporatingdiscriminative models into LLMs and highlights the potential of our methodologyin fostering more reliable LLMs.</description><author>Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, Yue Zhang</author><pubDate>Thu, 11 Apr 2024 07:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15918v2</guid></item><item><title>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</title><link>http://arxiv.org/abs/2309.07915v3</link><description>Since the resurgence of deep learning, vision-language models (VLMs) enhancedby large language models (LLMs) have grown exponentially in popularity.However, while LLMs can utilize extensive background knowledge and taskinformation with in-context learning, most VLMs still struggle withunderstanding complex multi-modal prompts with multiple images, making VLMsless effective in downstream vision-language tasks. In this paper, we addressthe limitation above by 1) introducing vision-language Model with Multi-ModalIn-Context Learning(MMICL), a new approach to allow the VLM to deal withmulti-modal inputs efficiently; 2) proposing a novel context scheme to augmentthe in-context learning ability of the VLM; 3) constructing the Multi-modalIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability tounderstand complex multi-modal prompts. Our experiments confirm that MMICLachieves new state-of-the-art zero-shot performance on a wide range of generalvision-language tasks, especially for complex benchmarks, including MME andMMBench. Our analysis demonstrates that MMICL effectively tackles the challengeof complex multi-modal prompt understanding and emerges the impressive ICLability. Furthermore, we observe that MMICL successfully alleviates languagebias in VLMs, a common issue for VLMs that often leads to hallucination whenfaced with extensive textual context. Our code, dataset, dataset tool, andmodel are available at https://github.com/PKUnlp-icler/MIC</description><author>Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang</author><pubDate>Wed, 20 Mar 2024 17:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07915v3</guid></item><item><title>Understanding In-Context Learning with a Pelican Soup Framework</title><link>http://arxiv.org/abs/2402.10424v1</link><description>Many existing theoretical analyses of in-context learning for naturallanguage processing are based on latent variable models that leaves gapsbetween theory and practice. We aim to close these gaps by proposing atheoretical framework, the Pelican Soup Framework. In this framework, weintroduce (1) the notion of a common sense knowledge base, (2) a generalformalism for natural language classification tasks, and the notion of (3)meaning association. Under this framework, we can establish a$\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the numberof example-label pairs in the demonstration. Compared with previous works, ourbound reflects the effect of the choice of verbalizers and the effect ofinstruction tuning. An additional notion of \textit{atom concepts} makes ourframework possible to explain the generalization to tasks unseen in thelanguage model training data. Finally, we propose a toy setup, Calcutec, and adigit addition task that mimics types of distribution shifts a model needs toovercome to perform in-context learning. We also experiment with GPT2-Large onreal-world NLP tasks. Our empirical results demonstrate the efficacy of ourframework to explain in-context learning.</description><author>Ting-Rui Chiang, Dani Yogatama</author><pubDate>Fri, 16 Feb 2024 03:20:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10424v1</guid></item><item><title>C-ICL: Contrastive In-context Learning for Information Extraction</title><link>http://arxiv.org/abs/2402.11254v1</link><description>Recently, there has been increasing interest in exploring the capabilities ofadvanced large language models (LLMs) in the field of information extraction(IE), specifically focusing on tasks related to named entity recognition (NER)and relation extraction (RE). Although researchers are exploring the use offew-shot information extraction through in-context learning with LLMs, theytend to focus only on using correct or positive examples for demonstration,neglecting the potential value of incorporating incorrect or negative examplesinto the learning process. In this paper, we present c-ICL, a novel few-shottechnique that leverages both correct and incorrect sample constructions tocreate in-context learning demonstrations. This approach enhances the abilityof LLMs to extract entities and relations by utilizing prompts that incorporatenot only the positive samples but also the reasoning behind them. This methodallows for the identification and correction of potential interface errors.Specifically, our proposed method taps into the inherent contextual informationand valuable information in hard negative samples and the nearest positiveneighbors to the test and then applies the in-context learning demonstrationsbased on LLMs. Our experiments on various datasets indicate that c-ICLoutperforms previous few-shot in-context learning methods, deliveringsubstantial enhancements in performance across a broad spectrum of relatedtasks. These improvements are noteworthy, showcasing the versatility of ourapproach in miscellaneous scenarios.</description><author>Ying Mo, Jian Yang, Jiahao Liu, Shun Zhang, Jingang Wang, Zhoujun Li</author><pubDate>Sat, 17 Feb 2024 11:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11254v1</guid></item><item><title>Benchmarking General Purpose In-Context Learning</title><link>http://arxiv.org/abs/2405.17234v2</link><description>In-context learning (ICL) capabilities are becoming increasingly appealingfor building general intelligence due to their sample efficiency andindependence from artificial optimization skills. To enhance generalization,biological neural systems primarily inherit learning capabilities andsubsequently refine their memory, acquiring diverse skills and knowledgethrough extensive lifelong experiences. This process gives rise to the conceptof general-purpose in-context learning (GPICL). Compared to standard ICL, GPICLaddresses a broader range of tasks, extends learning horizons, and starts at alower zero-shot baseline. We introduce two lightweight but insightfulbenchmarks specifically crafted to train and evaluate GPICL functionalities.Each benchmark includes a vast number of tasks characterized by significanttask variance and minimal transferable knowledge among tasks, facilitatinglifelong in-context learning through continuous generation and interaction.These features pose significant challenges for models that rely on context orinteractions to improve their proficiency, including language models, decisionmodels, and world models. Our experiments reveal that parameter scale alone maynot be crucial for ICL or GPICL, suggesting alternative approaches such asincreasing the scale of contexts and memory states.</description><author>Fan Wang, Chuan Lin, Yang Cao, Yu Kang</author><pubDate>Wed, 29 May 2024 14:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17234v2</guid></item><item><title>Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning</title><link>http://arxiv.org/abs/2401.05949v4</link><description>In-context learning, a paradigm bridging the gap between pre-training andfine-tuning, has demonstrated high efficacy in several NLP tasks, especially infew-shot settings. Despite being widely applied, in-context learning isvulnerable to malicious attacks. In this work, we raise security concernsregarding this paradigm. Our studies demonstrate that an attacker canmanipulate the behavior of large language models by poisoning the demonstrationcontext, without the need for fine-tuning the model. Specifically, we design anew backdoor attack method, named ICLAttack, to target large language modelsbased on in-context learning. Our method encompasses two types of attacks:poisoning demonstration examples and poisoning demonstration prompts, which canmake models behave in alignment with predefined intentions. ICLAttack does notrequire additional fine-tuning to implant a backdoor, thus preserving themodel's generality. Furthermore, the poisoned examples are correctly labeled,enhancing the natural stealth of our attack method. Extensive experimentalresults across several language models, ranging in size from 1.3B to 180Bparameters, demonstrate the effectiveness of our attack method, exemplified bya high average attack success rate of 95.0% across the three datasets on OPTmodels.</description><author>Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, Jinming Wen</author><pubDate>Fri, 16 Feb 2024 13:45:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05949v4</guid></item><item><title>In-Context Learning with Long-Context Models: An In-Depth Exploration</title><link>http://arxiv.org/abs/2405.00200v1</link><description>As model context lengths continue to increase, the number of demonstrationsthat can be provided in-context approaches the size of entire trainingdatasets. We study the behavior of in-context learning (ICL) at this extremescale on multiple datasets and models. We show that, for many datasets withlarge label spaces, performance continues to increase with hundreds orthousands of demonstrations. We contrast this with example retrieval andfinetuning: example retrieval shows excellent performance at low contextlengths but has diminished gains with more demonstrations; finetuning is moredata hungry than ICL but can sometimes exceed long-context ICL performance withadditional data. We use this ICL setting as a testbed to study severalproperties of both in-context learning and long-context models. We show thatlong-context ICL is less sensitive to random input shuffling than short-contextICL, that grouping of same-label examples can negatively impact performance,and that the performance boosts we see do not arise from cumulative gain fromencoding many examples together. We conclude that although long-context ICL canbe surprisingly effective, most of this gain comes from attending back tosimilar examples rather than task learning.</description><author>Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R. Gormley, Graham Neubig</author><pubDate>Tue, 30 Apr 2024 22:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00200v1</guid></item><item><title>IM-Context: In-Context Learning for Imbalanced Regression Tasks</title><link>http://arxiv.org/abs/2405.18202v1</link><description>Regression models often fail to generalize effectively in regionscharacterized by highly imbalanced label distributions. Previous methods fordeep imbalanced regression rely on gradient-based weight updates, which tend tooverfit in underrepresented regions. This paper proposes a paradigm shifttowards in-context learning as an effective alternative to conventionalin-weight learning methods, particularly for addressing imbalanced regression.In-context learning refers to the ability of a model to condition itself, givena prompt sequence composed of in-context samples (input-label pairs) alongsidea new query input to generate predictions, without requiring any parameterupdates. In this paper, we study the impact of the prompt sequence on the modelperformance from both theoretical and empirical perspectives. We emphasize theimportance of localized context in reducing bias within regions of highimbalance. Empirical evaluations across a variety of real-world datasetsdemonstrate that in-context learning substantially outperforms existingin-weight learning methods in scenarios with high levels of imbalance.</description><author>Ismail Nejjar, Faez Ahmed, Olga Fink</author><pubDate>Tue, 28 May 2024 15:10:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.18202v1</guid></item><item><title>RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models</title><link>http://arxiv.org/abs/2308.07922v2</link><description>In this paper, we investigate the in-context learning ability ofretrieval-augmented encoder-decoder language models. We first conduct acomprehensive analysis of existing models and identify their limitations inin-context learning, primarily due to a mismatch between pretraining andinference, as well as a restricted context length. To address these issues, wepropose RAVEN, a model that combines retrieval-augmented masked languagemodeling and prefix language modeling. We further introduce Fusion-in-ContextLearning to enhance the few-shot performance by enabling the model to leveragemore in-context examples without requiring additional training. Throughextensive experiments, we demonstrate that our simple yet effective designsignificantly improves performance, achieving results comparable to the mostadvanced language models in certain scenarios, despite having substantiallyfewer parameters. Our work underscores the potential of retrieval-augmentedencoder-decoder language models for in-context learning and encourages furtherresearch in this direction.</description><author>Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, Bryan Catanzaro</author><pubDate>Mon, 01 Apr 2024 07:32:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07922v2</guid></item><item><title>On the Noise Robustness of In-Context Learning for Text Generation</title><link>http://arxiv.org/abs/2405.17264v1</link><description>Large language models (LLMs) have shown impressive performance on downstreamtasks by in-context learning (ICL), which heavily relies on the quality ofdemonstrations selected from a large set of annotated examples. Recent worksclaim that in-context learning is robust to noisy demonstrations in textclassification. In this work, we show that, on text generation tasks, noisyannotations significantly hurt the performance of in-context learning. Tocircumvent the issue, we propose a simple and effective approach called LocalPerplexity Ranking (LPR), which replaces the "noisy" candidates with theirnearest neighbors that are more likely to be clean. Our method is motivated byanalyzing the perplexity deviation caused by noisy labels and decomposingperplexity into inherent perplexity and matching perplexity. Our key ideabehind LPR is thus to decouple the matching perplexity by performing theranking among the neighbors in semantic space. Our approach can prevent theselected demonstrations from including mismatched input-label pairs whilepreserving the effectiveness of the original selection methods. Extensiveexperiments demonstrate the effectiveness of LPR, improving the EM score by upto 18.75 on common benchmarks with noisy annotations.</description><author>Hongfu Gao, Feipeng Zhang, Wenyu Jiang, Jun Shu, Feng Zheng, Hongxin Wei</author><pubDate>Mon, 27 May 2024 16:22:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17264v1</guid></item><item><title>SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging</title><link>http://arxiv.org/abs/2403.16578v1</link><description>Medical image segmentation models adapting to new tasks in a training-freemanner through in-context learning is an exciting advancement. Universalsegmentation models aim to generalize across the diverse modality of medicalimages, yet their effectiveness often diminishes when applied toout-of-distribution (OOD) data modalities and tasks, requiring intricatefine-tuning of model for optimal performance. For addressing this challenge, weintroduce SegICL, a novel approach leveraging In-Context Learning (ICL) forimage segmentation. Unlike existing methods, SegICL has the capability toemploy text-guided segmentation and conduct in-context learning with a smallset of image-mask pairs, eliminating the need for training the model fromscratch or fine-tuning for OOD tasks (including OOD modality and dataset).Extensive experimental validation of SegICL demonstrates a positive correlationbetween the number of prompt samples and segmentation performance on OODmodalities and tasks. This indicates that SegICL effectively address newsegmentation tasks based on contextual information. Additionally, SegICL alsoexhibits comparable segmentation performance to mainstream models on OOD andin-distribution tasks. Our code will be released soon.</description><author>Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shining Xiang</author><pubDate>Mon, 25 Mar 2024 10:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16578v1</guid></item><item><title>SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging</title><link>http://arxiv.org/abs/2403.16578v2</link><description>Medical image segmentation models adapting to new tasks in a training-freemanner through in-context learning is an exciting advancement. Universalsegmentation models aim to generalize across the diverse modality of medicalimages, yet their effectiveness often diminishes when applied toout-of-distribution (OOD) data modalities and tasks, requiring intricatefine-tuning of model for optimal performance. For addressing this challenge, weintroduce SegICL, a novel approach leveraging In-Context Learning (ICL) forimage segmentation. Unlike existing methods, SegICL has the capability toemploy text-guided segmentation and conduct in-context learning with a smallset of image-mask pairs, eliminating the need for training the model fromscratch or fine-tuning for OOD tasks (including OOD modality and dataset).Extensive experimental validation of SegICL demonstrates a positive correlationbetween the number of prompt samples and segmentation performance on OODmodalities and tasks. This indicates that SegICL effectively address newsegmentation tasks based on contextual information. Additionally, SegICL alsoexhibits comparable segmentation performance to mainstream models on OOD andin-distribution tasks. Our code will be released soon.</description><author>Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shiming Xiang</author><pubDate>Tue, 02 Apr 2024 10:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16578v2</guid></item><item><title>How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?</title><link>http://arxiv.org/abs/2404.12866v1</link><description>The increase in parameter size of multimodal large language models (MLLMs)introduces significant capabilities, particularly in-context learning, whereMLLMs enhance task performance without updating pre-trained parameters. Thiseffectiveness, however, hinges on the appropriate selection of in-contextexamples, a process that is currently biased towards visual data, overlookingtextual information. Furthermore, the area of supervised retrievers for MLLMs,crucial for optimal in-context example selection, continues to beuninvestigated. Our study offers an in-depth evaluation of the impact oftextual information on the unsupervised selection of in-context examples inmultimodal contexts, uncovering a notable sensitivity of retriever performanceto the employed modalities. Responding to this, we introduce a novel supervisedMLLM-retriever MSIER that employs a neural network to select examples thatenhance multimodal in-context learning efficiency. This approach is validatedthrough extensive testing across three distinct tasks, demonstrating themethod's effectiveness. Additionally, we investigate the influence ofmodalities on our supervised retrieval method's training and pinpoint factorscontributing to our model's success. This exploration paves the way for futureadvancements, highlighting the potential for refined in-context learning inMLLMs through the strategic use of multimodal data.</description><author>Yang Luo, Zangwei Zheng, Zirui Zhu, Yang You</author><pubDate>Fri, 19 Apr 2024 14:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12866v1</guid></item><item><title>DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</title><link>http://arxiv.org/abs/2310.02954v5</link><description>Recent advances in natural language processing, primarily propelled by LargeLanguage Models (LLMs), have showcased their remarkable capabilities groundedin in-context learning. A promising avenue for guiding LLMs in intricatereasoning tasks involves the utilization of intermediate reasoning steps withinthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge liesin the effective selection of exemplars for facilitating in-context learning.In this study, we introduce a framework that leverages Dual Queries andLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplarsfor in-context learning. Dual Queries first query LLM to obtain LLM-generatedknowledge such as CoT, then query the retriever to obtain the final exemplarsvia both question and the knowledge. Moreover, for the second query, LoReemploys dimensionality reduction techniques to refine exemplar selection,ensuring close alignment with the input question's knowledge. Through extensiveexperiments, we demonstrate that DQ-LoRe significantly outperforms priorstate-of-the-art methods in the automatic selection of exemplars for GPT-4,enhancing performance from 92.5% to 94.2%. Our comprehensive analysis furtherreveals that DQ-LoRe consistently outperforms retrieval-based approaches interms of both performance and adaptability, especially in scenarioscharacterized by distribution shifts. DQ-LoRe pushes the boundary of in-contextlearning and opens up new avenues for addressing complex reasoning challenges.Our code is released athttps://github.com/AI4fun/DQ-LoRe}{https://github.com/AI4fun/DQ-LoRe.</description><author>Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang</author><pubDate>Sat, 02 Mar 2024 14:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02954v5</guid></item><item><title>HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation</title><link>http://arxiv.org/abs/2402.09390v1</link><description>With the widespread adoption of large language models (LLMs) in numerousapplications, the challenge of factuality and the propensity for hallucinationsraises significant concerns. To address this issue, particularly inretrieval-augmented in-context learning, we introduce the hierarchical graph ofthoughts (HGOT), a structured, multi-layered graph approach designed to enhancethe retrieval of pertinent passages during in-context learning. The frameworkutilizes the emergent planning capabilities of LLMs, employing thedivide-and-conquer strategy to break down complex queries into manageablesub-queries. It refines self-consistency majority voting for answer selection,which incorporates the recently proposed citation recall and precision metricsto assess the quality of thoughts, linking an answer's credibilityintrinsically to the thought's quality. This methodology introduces a weightedsystem in majority voting, prioritizing answers based on the citation qualityof their thoughts. Additionally, we propose a scoring mechanism for evaluatingretrieved passages, considering factors such as citation frequency and quality,self-consistency confidence, and the retrieval module's ranking. Experimentsreveal that HGOT outperforms other retrieval-augmented in-context learningmethods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, andRetrieve-then-Read on different datasets by as much as $7\%$, demonstrating itsefficacy in enhancing the factuality of LLMs.</description><author>Yihao Fang, Stephen W. Thomas, Xiaodan Zhu</author><pubDate>Wed, 14 Feb 2024 18:41:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09390v1</guid></item><item><title>Identifying Semantic Induction Heads to Understand In-Context Learning</title><link>http://arxiv.org/abs/2402.13055v1</link><description>Although large language models (LLMs) have demonstrated remarkableperformance, the lack of transparency in their inference logic raises concernsabout their trustworthiness. To gain a better understanding of LLMs, we conducta detailed analysis of the operations of attention heads and aim to betterunderstand the in-context learning of LLMs. Specifically, we investigatewhether attention heads encode two types of relationships between tokenspresent in natural languages: the syntactic dependency parsed from sentencesand the relation within knowledge graphs. We find that certain attention headsexhibit a pattern where, when attending to head tokens, they recall tail tokensand increase the output logits of those tail tokens. More crucially, theformulation of such semantic induction heads has a close correlation with theemergence of the in-context learning ability of language models. The study ofsemantic attention heads advances our understanding of the intricate operationsof attention heads in transformers, and further provides new insights into thein-context learning of LLMs.</description><author>Jie Ren, Qipeng Guo, Hang Yan, Dongrui Liu, Xipeng Qiu, Dahua Lin</author><pubDate>Tue, 20 Feb 2024 14:43:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13055v1</guid></item><item><title>Dual Operating Modes of In-Context Learning</title><link>http://arxiv.org/abs/2402.18819v1</link><description>In-context learning (ICL) exhibits dual operating modes: task learning, i.e.,acquiring a new skill from in-context samples, and task retrieval, i.e.,locating and activating a relevant pretrained skill. Recent theoretical workinvestigates various mathematical models to analyze ICL, but existing modelsexplain only one operating mode at a time. We introduce a probabilistic model,with which one can explain the dual operating modes of ICL simultaneously.Focusing on in-context learning of linear functions, we extend existing modelsfor pretraining data by introducing multiple task groups and task-dependentinput distributions. We then analyze the behavior of the optimally pretrainedmodel under the squared loss, i.e., the MMSE estimator of the label givenin-context examples. Regarding pretraining task distribution as prior andin-context examples as the observation, we derive the closed-form expression ofthe task posterior distribution. With the closed-form expression, we obtain aquantitative understanding of the two operating modes of ICL. Furthermore, weshed light on an unexplained phenomenon observed in practice: under certainsettings, the ICL risk initially increases and then decreases with morein-context examples. Our model offers a plausible explanation for this "earlyascent" phenomenon: a limited number of in-context samples may lead to theretrieval of an incorrect skill, thereby increasing the risk, which willeventually diminish as task learning takes effect with more in-context samples.We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL,where in-context examples are assigned random labels. Lastly, we validate ourfindings and predictions via experiments involving Transformers and largelanguage models.</description><author>Ziqian Lin, Kangwook Lee</author><pubDate>Thu, 29 Feb 2024 03:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18819v1</guid></item><item><title>SQL-Encoder: Improving NL2SQL In-Context Learning Through a Context-Aware Encoder</title><link>http://arxiv.org/abs/2403.16204v1</link><description>Detecting structural similarity between queries is essential for selectingexamples in in-context learning models. However, assessing structuralsimilarity based solely on the natural language expressions of queries, withoutconsidering SQL queries, presents a significant challenge. This paper exploresthe significance of this similarity metric and proposes a model for accuratelyestimating it. To achieve this, we leverage a dataset comprising 170k questionpairs, meticulously curated to train a similarity prediction model. Ourcomprehensive evaluation demonstrates that the proposed model adeptly capturesthe structural similarity between questions, as evidenced by improvements inKendall-Tau distance and precision@k metrics. Notably, our model outperformsstrong competitive embedding models from OpenAI and Cohere. Furthermore,compared to these competitive models, our proposed encoder enhances thedownstream performance of NL2SQL models in 1-shot in-context learning scenariosby 1-2\% for GPT-3.5-turbo, 4-8\% for CodeLlama-7B, and 2-3\% forCodeLlama-13B.</description><author>Mohammadreza Pourreza, Davood Rafiei, Yuxi Feng, Raymond Li, Zhenan Fan, Weiwei Zhang</author><pubDate>Sun, 24 Mar 2024 16:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16204v1</guid></item><item><title>Implicit In-context Learning</title><link>http://arxiv.org/abs/2405.14660v1</link><description>In-context Learning (ICL) empowers large language models (LLMs) to adapt tounseen tasks during inference by prefixing a few demonstration examples priorto test queries. Despite its versatility, ICL incurs substantial computationaland memory overheads compared to zero-shot learning and is susceptible to theselection and order of demonstration examples. In this work, we introduceImplicit In-context Learning (I2CL), an innovative paradigm that addresses thechallenges associated with traditional ICL by absorbing demonstration exampleswithin the activation space. I2CL first generates a condensed vectorrepresentation, namely a context vector, from the demonstration examples. Itthen integrates the context vector during inference by injecting a linearcombination of the context vector and query activations into the model'sresidual streams. Empirical evaluation on nine real-world tasks across threemodel architectures demonstrates that I2CL achieves few-shot performance withzero-shot cost and exhibits robustness against the variation of demonstrationexamples. Furthermore, I2CL facilitates a novel representation of "task-ids",enhancing task similarity detection and enabling effective transfer learning.We provide a comprehensive analysis of I2CL, offering deeper insights into itsmechanisms and broader implications for ICL. The source code is available at:https://github.com/LzVv123456/I2CL.</description><author>Zhuowei Li, Zihao Xu, Ligong Han, Yunhe Gao, Song Wen, Di Liu, Hao Wang, Dimitris N. Metaxas</author><pubDate>Thu, 23 May 2024 15:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14660v1</guid></item><item><title>Feature-Adaptive and Data-Scalable In-Context Learning</title><link>http://arxiv.org/abs/2405.10738v1</link><description>In-context learning (ICL), which promotes inference with severaldemonstrations, has become a widespread paradigm to stimulate LLM capabilitiesfor downstream tasks. Due to context length constraints, it cannot be furtherimproved in spite of more training data, and general features directly fromLLMs in ICL are not adaptive to the specific downstream task. In this paper, wepropose a feature-adaptive and data-scalable in-context learning framework(FADS-ICL), which can leverage task-adaptive features to promote inference onthe downstream task, with the supervision of beyond-context samples.Specifically, it first extracts general features of beyond-context samples viathe LLM with ICL input form one by one, and introduces a task-specificmodulator to perform feature refinement and prediction after fitting a specificdownstream task. We conduct extensive experiments on FADS-ICL under varyingdata settings (4$\sim$128 shots) and LLM scale (0.8$\sim$70B) settings.Experimental results show that FADS-ICL consistently outperforms previousstate-of-the-art methods by a significant margin under all settings, verifyingthe effectiveness and superiority of FADS-ICL. For example, under the 1.5B and32 shots setting, FADS-ICL can achieve \textbf{+14.3} average accuracy fromfeature adaptation over vanilla ICL on 10 datasets, with \textbf{+6.2} averageaccuracy over the previous state-of-the-art method, and the performance canfurther improve with increasing training data. Code and data are publiclyavailable at \url{https://github.com/jiahaozhenbang/FADS-ICL}.</description><author>Jiahao Li, Quan Wang, Licheng Zhang, Guoqing Jin, Zhendong Mao</author><pubDate>Fri, 17 May 2024 13:32:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10738v1</guid></item><item><title>Feature-Adaptive and Data-Scalable In-Context Learning</title><link>http://arxiv.org/abs/2405.10738v2</link><description>In-context learning (ICL), which promotes inference with severaldemonstrations, has become a widespread paradigm to stimulate LLM capabilitiesfor downstream tasks. Due to context length constraints, it cannot be furtherimproved in spite of more training data, and general features directly fromLLMs in ICL are not adaptive to the specific downstream task. In this paper, wepropose a feature-adaptive and data-scalable in-context learning framework(FADS-ICL), which can leverage task-adaptive features to promote inference onthe downstream task, with the supervision of beyond-context samples.Specifically, it first extracts general features of beyond-context samples viathe LLM with ICL input form one by one, and introduces a task-specificmodulator to perform feature refinement and prediction after fitting a specificdownstream task. We conduct extensive experiments on FADS-ICL under varyingdata settings (4$\sim$128 shots) and LLM scale (0.8$\sim$70B) settings.Experimental results show that FADS-ICL consistently outperforms previousstate-of-the-art methods by a significant margin under all settings, verifyingthe effectiveness and superiority of FADS-ICL. For example, under the 1.5B and32 shots setting, FADS-ICL can achieve \textbf{+14.3} average accuracy fromfeature adaptation over vanilla ICL on 10 datasets, with \textbf{+6.2} averageaccuracy over the previous state-of-the-art method, and the performance canfurther improve with increasing training data. Code and data are publiclyavailable at \url{https://github.com/jiahaozhenbang/FADS-ICL}.</description><author>Jiahao Li, Quan Wang, Licheng Zhang, Guoqing Jin, Zhendong Mao</author><pubDate>Mon, 03 Jun 2024 07:42:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10738v2</guid></item><item><title>Meta In-Context Learning Makes Large Language Models Better Zero and Few-Shot Relation Extractors</title><link>http://arxiv.org/abs/2404.17807v1</link><description>Relation extraction (RE) is an important task that aims to identify therelationships between entities in texts. While large language models (LLMs)have revealed remarkable in-context learning (ICL) capability for general zeroand few-shot learning, recent studies indicate that current LLMs still strugglewith zero and few-shot RE. Previous studies are mainly dedicated to designprompt formats and select good examples for improving ICL-based RE. Althoughboth factors are vital for ICL, if one can fundamentally boost the ICLcapability of LLMs in RE, the zero and few-shot RE performance via ICL would besignificantly improved. To this end, we introduce \textsc{Micre} (\textbf{M}eta\textbf{I}n-\textbf{C}ontext learning of LLMs for \textbf{R}elation\textbf{E}xtraction), a new meta-training framework for zero and few-shot REwhere an LLM is tuned to do ICL on a diverse collection of RE datasets (i.e.,learning to learn in context for RE). Through meta-training, the model becomesmore effectively to learn a new RE task in context by conditioning on a fewtraining examples with no parameter updates or task-specific templates atinference time, enabling better zero and few-shot task generalization. Weexperiment \textsc{Micre} on various LLMs with different model scales and 12public RE datasets, and then evaluate it on unseen RE benchmarks under zero andfew-shot settings. \textsc{Micre} delivers comparable or superior performancecompared to a range of baselines including supervised fine-tuning and typicalin-context learning methods. We find that the gains are particular significantfor larger model scales, and using a diverse set of the meta-training REdatasets is key to improvements. Empirically, we show that \textsc{Micre} cantransfer the relation semantic knowledge via relation label name duringinference on target RE datasets.</description><author>Guozheng Li, Peng Wang, Jiajun Liu, Yikai Guo, Ke Ji, Ziyu Shang, Zijie Xu</author><pubDate>Sat, 27 Apr 2024 08:06:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17807v1</guid></item><item><title>Benchmarking General Purpose In-Context Learning</title><link>http://arxiv.org/abs/2405.17234v1</link><description>In-context learning (ICL) capabilities is becoming increasingly appealingtowards building general intelligence. Taking this concept one step further, wedraw a parallel to humans and many animals, who inherit primarily learningcapabilities but refine their memory and acquire diverse skills and knowledgethrough extensive lifelong experiences. This parallel inspires our approach togeneral purpose in-context learning (GPICL). This paper introduces twolightweight but insightful benchmarks specifically crafted to train andevaluate GPICL functionalities. Each benchmark encompasses a wide range ofdiverse tasks characterized by generation and interaction, minimal transferableknowledge, and long-term dependency. These features present significantchallenges for models that primarily rely on context or interactions to enhancetheir proficiency. We hope that these benchmarks will not only advance researchin GPICL but also contribute significantly to the broader field of generalintelligence.</description><author>Fan Wang, Chuan Lin, Yang Cao, Yu Kang</author><pubDate>Mon, 27 May 2024 15:50:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17234v1</guid></item><item><title>Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning</title><link>http://arxiv.org/abs/2402.15734v1</link><description>Recent years have witnessed the promise of coupling machine learning methodsand physical domain-specific insight for solving scientific problems based onpartial differential equations (PDEs). However, being data-intensive, thesemethods still require a large amount of PDE data. This reintroduces the needfor expensive numerical PDE solutions, partially undermining the original goalof avoiding these expensive simulations. In this work, seeking data efficiency,we design unsupervised pretraining and in-context learning methods for PDEoperator learning. To reduce the need for training data with simulatedsolutions, we pretrain neural operators on unlabeled PDE data usingreconstruction-based proxy tasks. To improve out-of-distribution performance,we further assist neural operators in flexibly leveraging in-context learningmethods, without incurring extra training costs or designs. Extensive empiricalevaluations on a diverse set of PDEs demonstrate that our method is highlydata-efficient, more generalizable, and even outperforms conventionalvision-pretrained models.</description><author>Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy Morozov, Michael W. Mahoney</author><pubDate>Sat, 24 Feb 2024 06:27:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15734v1</guid></item><item><title>Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model</title><link>http://arxiv.org/abs/2404.09045v1</link><description>Timely identification is essential for the efficient handling of mentalhealth illnesses such as depression. However, the current research fails toadequately address the prediction of mental health conditions from social mediadata in low-resource African languages like Swahili. This study introduces twodistinct approaches utilising model-agnostic meta-learning and leveraging largelanguage models (LLMs) to address this gap. Experiments are conducted on threedatasets translated to low-resource language and applied to four mental healthtasks, which include stress, depression, depression severity and suicidalideation prediction. we first apply a meta-learning model withself-supervision, which results in improved model initialisation for rapidadaptation and cross-lingual transfer. The results show that our meta-trainedmodel performs significantly better than standard fine-tuning methods,outperforming the baseline fine-tuning in macro F1 score with 18\% and 0.8\%over XLM-R and mBERT. In parallel, we use LLMs' in-context learningcapabilities to assess their performance accuracy across the Swahili mentalhealth prediction tasks by analysing different cross-lingual promptingapproaches. Our analysis showed that Swahili prompts performed better thancross-lingual prompts but less than English prompts. Our findings show thatin-context learning can be achieved through cross-lingual transfer throughcarefully crafted prompt templates with examples and instructions.</description><author>Zita Lifelo, Huansheng Ning, Sahraoui Dhelim</author><pubDate>Sat, 13 Apr 2024 18:11:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09045v1</guid></item><item><title>The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains</title><link>http://arxiv.org/abs/2402.11004v1</link><description>Large language models have the ability to generate text that mimics patternsin their inputs. We introduce a simple Markov Chain sequence modeling task inorder to study how this in-context learning (ICL) capability emerges. In oursetting, each example is sampled from a Markov chain drawn from a priordistribution over Markov chains. Transformers trained on this task form\emph{statistical induction heads} which compute accurate next-tokenprobabilities given the bigram statistics of the context. During the course oftraining, models pass through multiple phases: after an initial stage in whichpredictions are uniform, they learn to sub-optimally predict using in-contextsingle-token statistics (unigrams); then, there is a rapid phase transition tothe correct in-context bigram solution. We conduct an empirical and theoreticalinvestigation of this multi-phase process, showing how successful learningresults from the interaction between the transformer's layers, and uncoveringevidence that the presence of the simpler unigram solution may delay formationof the final bigram solution. We examine how learning is affected by varyingthe prior distribution over Markov chains, and consider the generalization ofour in-context learning of Markov chains (ICL-MC) task to $n$-grams for $n &gt;2$.</description><author>Benjamin L. Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, Nikolaos Tsilivis</author><pubDate>Fri, 16 Feb 2024 18:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11004v1</guid></item><item><title>Prompt Optimization via Adversarial In-Context Learning</title><link>http://arxiv.org/abs/2312.02614v2</link><description>We propose a new method, Adversarial In-Context Learning (adv-ICL), tooptimize prompt for in-context learning (ICL) by employing one LLM as agenerator, another as a discriminator, and a third as a prompt modifier. As intraditional adversarial learning, adv-ICL is implemented as a two-player gamebetween the generator and discriminator, where the generator tries to generaterealistic enough output to fool the discriminator. In each round, given aninput prefixed by task instructions and several exemplars, the generatorproduces an output. The discriminator is then tasked with classifying thegenerator input-output pair as model-generated or real data. Based on thediscriminator loss, the prompt modifier proposes possible edits to thegenerator and discriminator prompts, and the edits that most improve theadversarial loss are selected. We show that adv-ICL results in significantimprovements over state-of-the-art prompt optimization techniques for both openand closed-source models on 11 generation and classification tasks includingsummarization, arithmetic reasoning, machine translation, data-to-textgeneration, and the MMLU and big-bench hard benchmarks. In addition, becauseour method uses pre-trained models and updates only prompts rather than modelparameters, it is computationally efficient, easy to extend to any LLM andtask, and effective in low-resource settings.</description><author>Xuan Long Do, Yiran Zhao, Hannah Brown, Yuxi Xie, James Xu Zhao, Nancy F. Chen, Kenji Kawaguchi, Michael Qizhe Xie, Junxian He</author><pubDate>Wed, 28 Feb 2024 04:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02614v2</guid></item><item><title>Can Whisper perform speech-based in-context learning?</title><link>http://arxiv.org/abs/2309.07081v2</link><description>This paper investigates the in-context learning abilities of the Whisperautomatic speech recognition (ASR) models released by OpenAI. A novelspeech-based in-context learning (SICL) approach is proposed for test-timeadaptation, which can reduce the word error rates (WERs) with only a smallnumber of labelled speech samples without gradient descent. Language-leveladaptation experiments using Chinese dialects showed that when applying SICL toisolated word ASR, consistent and considerable relative WER reductions can beachieved using Whisper models of any size on two dialects, which is on average32.3%. A k-nearest-neighbours-based in-context example selection technique canbe applied to further improve the efficiency of SICL, which can increase theaverage relative WER reduction to 36.4%. The findings are verified usingspeaker adaptation or continuous speech recognition tasks, and both achievedconsiderable relative WER reductions. Detailed quantitative analyses are alsoprovided to shed light on SICL's adaptability to phonological variances anddialect-specific lexical nuances.</description><author>Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang</author><pubDate>Wed, 20 Mar 2024 04:04:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07081v2</guid></item><item><title>What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation</title><link>http://arxiv.org/abs/2404.07129v1</link><description>In-context learning is a powerful emergent ability in transformer models.Prior work in mechanistic interpretability has identified a circuit elementthat may be critical for in-context learning -- the induction head (IH), whichperforms a match-and-copy operation. During training of large transformers onnatural language data, IHs emerge around the same time as a notable phasechange in the loss. Despite the robust evidence for IHs and this interestingcoincidence with the phase change, relatively little is known about thediversity and emergence dynamics of IHs. Why is there more than one IH, and howare they dependent on each other? Why do IHs appear all of a sudden, and whatare the subcircuits that enable them to emerge? We answer these questions bystudying IH emergence dynamics in a controlled setting by training on syntheticdata. In doing so, we develop and share a novel optogenetics-inspired causalframework for modifying activations throughout training. Using this framework,we delineate the diverse and additive nature of IHs. By clamping subsets ofactivations throughout training, we then identify three underlying subcircuitsthat interact to drive IH formation, yielding the phase change. Furthermore,these subcircuits shed light on data-dependent properties of formation, such asphase change timing, already showing the promise of this more in-depthunderstanding of subcircuits that need to "go right" for an induction head.</description><author>Aaditya K. Singh, Ted Moskovitz, Felix Hill, Stephanie C. Y. Chan, Andrew M. Saxe</author><pubDate>Wed, 10 Apr 2024 17:07:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07129v1</guid></item><item><title>Is Mamba Capable of In-Context Learning?</title><link>http://arxiv.org/abs/2402.03170v2</link><description>State of the art foundation models such as GPT-4 perform surprisingly well atin-context learning (ICL), a variant of meta-learning concerning the learnedability to solve tasks during a neural network forward pass, exploitingcontextual information provided as input to the model. This useful abilityemerges as a side product of the foundation model's massive pretraining. Whiletransformer models are currently the state of the art in ICL, this workprovides empirical evidence that Mamba, a newly proposed state space modelwhich scales better than transformers w.r.t. the input sequence length, hassimilar ICL capabilities. We evaluated Mamba on tasks involving simple functionapproximation as well as more complex natural language processing problems. Ourresults demonstrate that, across both categories of tasks, Mamba closelymatches the performance of transformer models for ICL. Further analysis revealsthat, like transformers, Mamba appears to solve ICL problems by incrementallyoptimizing its internal representations. Overall, our work suggests that Mambacan be an efficient alternative to transformers for ICL tasks involving longinput sequences. This is an exciting finding in meta-learning and may enablegeneralizations of in-context learned AutoML algorithms (like TabPFN orOptformer) to long input sequences.</description><author>Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, Frank Hutter</author><pubDate>Wed, 24 Apr 2024 13:21:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03170v2</guid></item><item><title>Asymptotic theory of in-context learning by linear attention</title><link>http://arxiv.org/abs/2405.11751v1</link><description>Transformers have a remarkable ability to learn and execute tasks based onexamples provided within the input itself, without explicit prior training. Ithas been argued that this capability, known as in-context learning (ICL), is acornerstone of Transformers' success, yet questions about the necessary samplecomplexity, pretraining task diversity, and context length for successful ICLremain unresolved. Here, we provide a precise answer to these questions in anexactly solvable model of ICL of a linear regression task by linear attention.We derive sharp asymptotics for the learning curve in a phenomenologically-richscaling regime where the token dimension is taken to infinity; the contextlength and pretraining task diversity scale proportionally with the tokendimension; and the number of pretraining examples scales quadratically. Wedemonstrate a double-descent learning curve with increasing pretrainingexamples, and uncover a phase transition in the model's behavior between lowand high task diversity regimes: In the low diversity regime, the model tendstoward memorization of training tasks, whereas in the high diversity regime, itachieves genuine in-context learning and generalization beyond the scope ofpretrained tasks. These theoretical insights are empirically validated throughexperiments with both linear attention and full nonlinear Transformerarchitectures.</description><author>Yue M. Lu, Mary I. Letey, Jacob A. Zavatone-Veth, Anindita Maiti, Cengiz Pehlevan</author><pubDate>Mon, 20 May 2024 04:24:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.11751v1</guid></item><item><title>MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning</title><link>http://arxiv.org/abs/2403.06914v2</link><description>Large Language models (LLMs) have demonstrated impressive in-context learning(ICL) capabilities, where a LLM makes predictions for a given test inputtogether with a few input-output pairs (demonstrations). Nevertheless, theinclusion of demonstrations leads to a quadratic increase in the computationaloverhead of the self-attention mechanism. Existing solutions attempt to distilllengthy demonstrations into compact vectors. However, they often requiretask-specific retraining or compromise LLM's in-context learning performance.To mitigate these challenges, we present Meta dEmonstratioN Distillation(MEND), where a language model learns to distill any lengthy demonstrationsinto vectors without retraining for a new downstream task. We exploit theknowledge distillation to enhance alignment between MEND and LLM, achievingboth efficiency and effectiveness simultaneously. MEND is endowed with themeta-knowledge of distilling demonstrations through a two-stage trainingprocess, which includes meta-distillation pretraining and fine-tuning.Comprehensive evaluations across seven diverse ICL task partitions usingdecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It notonly matches but often outperforms the Vanilla ICL as well as otherstate-of-the-art distillation models, while significantly reducing thecomputational demands. This innovation promises enhanced scalability andefficiency for the practical deployment of large language models</description><author>Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo</author><pubDate>Tue, 12 Mar 2024 16:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06914v2</guid></item><item><title>MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning</title><link>http://arxiv.org/abs/2403.06914v1</link><description>Large Language models (LLMs) have demonstrated impressive in-context learning(ICL) capabilities, where a LLM makes predictions for a given test inputtogether with a few input-output pairs (demonstrations). Nevertheless, theinclusion of demonstrations leads to a quadratic increase in the computationaloverhead of the self-attention mechanism. Existing solutions attempt to distilllengthy demonstrations into compact vectors. However, they often requiretask-specific retraining or compromise LLM's in-context learning performance.To mitigate these challenges, we present Meta dEmonstratioN Distillation(MEND), where a language model learns to distill any lengthy demonstrationsinto vectors without retraining for a new downstream task. We exploit theknowledge distillation to enhance alignment between MEND and LLM, achievingboth efficiency and effectiveness simultaneously. MEND is endowed with themeta-knowledge of distilling demonstrations through a two-stage trainingprocess, which includes meta-distillation pretraining and fine-tuning.Comprehensive evaluations across seven diverse ICL task partitions usingdecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It notonly matches but often outperforms the Vanilla ICL as well as otherstate-of-the-art distillation models, while significantly reducing thecomputational demands. This innovation promises enhanced scalability andefficiency for the practical deployment of large language models</description><author>Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo</author><pubDate>Mon, 11 Mar 2024 18:03:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06914v1</guid></item><item><title>Point-In-Context: Understanding Point Cloud via In-Context Learning</title><link>http://arxiv.org/abs/2404.12352v1</link><description>With the emergence of large-scale models trained on diverse datasets,in-context learning has emerged as a promising paradigm for multitasking,notably in natural language processing and image processing. However, itsapplication in 3D point cloud tasks remains largely unexplored. In this work,we introduce Point-In-Context (PIC), a novel framework for 3D point cloudunderstanding via in-context learning. We address the technical challenge ofeffectively extending masked point modeling to 3D point clouds by introducing aJoint Sampling module and proposing a vanilla version of PIC calledPoint-In-Context-Generalist (PIC-G). PIC-G is designed as a generalist modelfor various 3D point cloud tasks, with inputs and outputs modeled ascoordinates. In this paradigm, the challenging segmentation task is achieved byassigning label points with XYZ coordinates for each category; the finalprediction is then chosen based on the label point closest to the predictions.To break the limitation by the fixed label-coordinate assignment, which haspoor generalization upon novel classes, we propose two novel trainingstrategies, In-Context Labeling and In-Context Enhancing, forming an extendedversion of PIC named Point-In-Context-Segmenter (PIC-S), targeting improvingdynamic context labeling and model training. By utilizing dynamic in-contextlabels and extra in-context pairs, PIC-S achieves enhanced performance andgeneralization capability in and across part segmentation datasets. PIC is ageneral framework so that other tasks or datasets can be seamlessly introducedinto our PIC through a unified data format. We conduct extensive experiments tovalidate the versatility and adaptability of our proposed methods in handling awide range of tasks and segmenting multi-datasets. Our PIC-S is capable ofgeneralizing unseen datasets and performing novel part segmentation bycustomizing prompts.</description><author>Mengyuan Liu, Zhongbin Fang, Xia Li, Joachim M. Buhmann, Xiangtai Li, Chen Change Loy</author><pubDate>Thu, 18 Apr 2024 18:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12352v1</guid></item><item><title>VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning</title><link>http://arxiv.org/abs/2403.13164v1</link><description>Large language models (LLMs) famously exhibit emergent in-context learning(ICL) -- the ability to rapidly adapt to new tasks using few-shot examplesprovided as a prompt, without updating the model's weights. Built on top ofLLMs, vision large language models (VLLMs) have advanced significantly in areassuch as recognition, reasoning, and grounding. However, investigations into\emph{multimodal ICL} have predominantly focused on few-shot visual questionanswering (VQA), and image captioning, which we will show neither exploit thestrengths of ICL, nor test its limitations. The broader capabilities andlimitations of multimodal ICL remain under-explored. In this study, weintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-contextlearning, encompassing a broad spectrum of tasks that involve both images andtext as inputs and outputs, and different types of challenges, from {perceptionto reasoning and long context length}. We evaluate the abilities ofstate-of-the-art VLLMs against this benchmark suite, revealing their diversestrengths and weaknesses, and showing that even the most advanced models, suchas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,and the associated strengths and limitations of existing models, we hope thatour dataset will inspire future work on enhancing the in-context learningcapabilities of VLLMs, as well as inspire new applications that leverage VLLMICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.</description><author>Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales</author><pubDate>Tue, 19 Mar 2024 22:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13164v1</guid></item><item><title>Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks</title><link>http://arxiv.org/abs/2311.01949v2</link><description>In-context learning (ICL) ability has emerged with the increasing scale oflarge language models (LLMs), enabling them to learn input-label mappings fromdemonstrations and perform well on downstream tasks. However, under thestandard ICL setting, LLMs may sometimes neglect query-related information indemonstrations, leading to incorrect predictions. To address this limitation,we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) toexplore the power of ICL in open-domain question answering, an important formin knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extractquery-related knowledge from demonstrations, then concatenates the knowledge toprompt LLMs in a more explicit way. Furthermore, we track the source of thisknowledge to identify specific examples, and introduce a Hint-related ExampleRetriever (HER) to select informative examples for enhanced demonstrations. Weevaluate HICL with HER on 3 open-domain QA benchmarks, and observe averageperformance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EMscore and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.</description><author>Yifan Wang, Qingyan Guo, Xinzhe Ni, Chufan Shi, Lemao Liu, Haiyun Jiang, Yujiu Yang</author><pubDate>Thu, 18 Apr 2024 16:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01949v2</guid></item><item><title>Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction</title><link>http://arxiv.org/abs/2402.13741v1</link><description>The in-context learning (ICL) for relational triple extraction (RTE) hasachieved promising performance, but still encounters two key challenges: (1)how to design effective prompts and (2) how to select proper demonstrations.Existing methods, however, fail to address these challenges appropriately. Onthe one hand, they usually recast RTE task to text-to-text prompting formats,which is unnatural and results in a mismatch between the output format at thepre-training time and the inference time for large language models (LLMs). Onthe other hand, they only utilize surface natural language features and lackconsideration of triple semantics in sample selection. These issues areblocking improved performance in ICL for RTE, thus we aim to tackle promptdesigning and sample selection challenges simultaneously. To this end, wedevise a tabular prompting for RTE (\textsc{TableIE}) which frames RTE taskinto a table generation task to incorporate explicit structured informationinto ICL, facilitating conversion of outputs to RTE structures. Then we proposeinstructive in-context learning (I$^2$CL) which only selects and annotates afew samples considering internal triple semantics in massive unlabeled samples.</description><author>Guozheng Li, Wenjun Ke, Peng Wang, Zijie Xu, Ke Ji, Jiajun Liu, Ziyu Shang, Qiqing Luo</author><pubDate>Wed, 21 Feb 2024 12:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13741v1</guid></item><item><title>Visual In-Context Learning for Large Vision-Language Models</title><link>http://arxiv.org/abs/2402.11574v1</link><description>In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning(ICL) remains limited by challenges in cross-modal interactions andrepresentation disparities. To overcome these challenges, we introduce a novelVisual In-Context Learning (VICL) method comprising Visual DemonstrationRetrieval, Intent-Oriented Image Summarization, and Intent-OrientedDemonstration Composition. Our approach retrieves images via ''Retrieval &amp;Rerank'' paradigm, summarises images with task intent and task-specific visualparsing, and composes language-based demonstrations that reduce token count andalleviate cross-modal interaction problem. Experimental evaluations on fivevisual reasoning datasets demonstrate the effectiveness of our method.Moreover, our extensive experiments leverage information flow analysis toelucidate the effectiveness of our method, and investigate the impact of lengthand position of demonstrations for LVLM. The use of in-context unlearningfurther shows promise in resetting specific model knowledge without retraining.</description><author>Yucheng Zhou, Xiang Li, Qianning Wang, Jianbing Shen</author><pubDate>Sun, 18 Feb 2024 12:43:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11574v1</guid></item><item><title>Transformers Learn Temporal Difference Methods for In-Context Reinforcement Learning</title><link>http://arxiv.org/abs/2405.13861v1</link><description>In-context learning refers to the learning ability of a model duringinference time without adapting its parameters. The input (i.e., prompt) to themodel (e.g., transformers) consists of both a context (i.e., instance-labelpairs) and a query instance. The model is then able to output a label for thequery instance according to the context during inference. A possibleexplanation for in-context learning is that the forward pass of (linear)transformers implements iterations of gradient descent on the instance-labelpairs in the context. In this paper, we prove by construction that transformerscan also implement temporal difference (TD) learning in the forward pass, aphenomenon we refer to as in-context TD. We demonstrate the emergence ofin-context TD after training the transformer with a multi-task TD algorithm,accompanied by theoretical analysis. Furthermore, we prove that transformersare expressive enough to implement many other policy evaluation algorithms inthe forward pass, including residual gradient, TD with eligibility trace, andaverage-reward TD.</description><author>Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, Shangtong Zhang</author><pubDate>Wed, 22 May 2024 18:38:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13861v1</guid></item><item><title>Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems</title><link>http://arxiv.org/abs/2405.15585v1</link><description>Large language models (LLM) based end-to-end task-oriented dialog (TOD)systems built using few-shot (in-context) learning perform better thansupervised models only when the train data is limited. This is due to theinherent ability of LLMs to learn any task with just a few demonstrations. Asthe number of train dialogs increases, supervised SoTA models surpassin-context learning LLMs as they learn to better align with the style of thesystem responses in the training data, which LLMs struggle to mimic. Inresponse, we propose SyncTOD, which synergizes LLMs with useful hints about thetask for improved alignment. At a high level, SyncTOD trains auxiliary modelsto provide these hints and select exemplars for the in-context prompts. WithChatGPT, SyncTOD achieves superior performance compared to LLM-based baselinesand SoTA models in low-data settings, while retaining competitive performancein full-data settings</description><author>Vishal Vivek Saley, Rocktim Jyoti Das, Dinesh Raghu, Mausam</author><pubDate>Fri, 24 May 2024 15:13:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15585v1</guid></item><item><title>Decoding In-Context Learning: Neuroscience-inspired Analysis of Representations in Large Language Models</title><link>http://arxiv.org/abs/2310.00313v4</link><description>Large language models (LLMs) exhibit remarkable performance improvementthrough in-context learning (ICL) by leveraging task-specific examples in theinput. However, the mechanisms behind this improvement remain elusive. In thiswork, we investigate how LLM embeddings and attention representations changefollowing in-context-learning, and how these changes mediate improvement inbehavior. We employ neuroscience-inspired techniques such as representationalsimilarity analysis (RSA) and propose novel methods for parameterized probingand measuring ratio of attention to relevant vs. irrelevant information inLlama-2 70B and Vicuna 13B. We designed two tasks with a priori relationshipsamong their conditions: linear regression and reading comprehension. We formedhypotheses about expected similarities in task representations and measuredhypothesis alignment of LLM representations before and after ICL as well aschanges in attention. Our analyses revealed a meaningful correlation betweenimprovements in behavior after ICL and changes in both embeddings and attentionweights across LLM layers. This empirical framework empowers a nuancedunderstanding of how latent representations shape LLM behavior, offeringvaluable tools and insights for future research and practical applications.</description><author>Safoora Yousefi, Leo Betthauser, Hosein Hasanbeig, Raphaël Millière, Ida Momennejad</author><pubDate>Wed, 21 Feb 2024 19:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00313v4</guid></item><item><title>An Empirical Study of In-context Learning in LLMs for Machine Translation</title><link>http://arxiv.org/abs/2401.12097v2</link><description>Recent interest has surged in employing Large Language Models (LLMs) formachine translation (MT) via in-context learning (ICL) (Vilar et al., 2023).Most prior studies primarily focus on optimizing translation quality, withlimited attention to understanding the specific aspects of ICL that influencethe said quality. To this end, we perform the first of its kind, exhaustivestudy of in-context learning for machine translation. We first establish thatICL is primarily example-driven and not instruction-driven. Following this, weconduct an extensive exploration of various aspects of the examples tounderstand their influence on downstream performance. Our analysis includesfactors such as quality and quantity of demonstrations, spatial proximity, andsource versus target originality. Further, we also investigate challengingscenarios involving indirectness and misalignment of examples to understand thelimits of ICL. While we establish the significance of the quality of the targetdistribution over the source distribution of demonstrations, we further observethat perturbations sometimes act as regularizers, resulting in performanceimprovements. Surprisingly, ICL does not necessitate examples from the sametask, and a related task with the same target distribution proves sufficient.We hope that our study acts as a guiding resource for considerations inutilizing ICL for MT.</description><author>Pranjal A. Chitale, Jay Gala, Raj Dabre</author><pubDate>Sat, 17 Feb 2024 07:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12097v2</guid></item><item><title>FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis</title><link>http://arxiv.org/abs/2403.01063v1</link><description>Multi-domain aspect-based sentiment analysis (ABSA) seeks to capturefine-grained sentiment across diverse domains. While existing research narrowlyfocuses on single-domain applications constrained by methodological limitationsand data scarcity, the reality is that sentiment naturally traverses multipledomains. Although large language models (LLMs) offer a promising solution forABSA, it is difficult to integrate effectively with established techniques,including graph-based models and linguistics, because modifying their internalarchitecture is not easy. To alleviate this problem, we propose a novelframework, Feature-aware In-context Learning for Multi-domain ABSA (FaiMA). Thecore insight of FaiMA is to utilize in-context learning (ICL) as afeature-aware mechanism that facilitates adaptive learning in multi-domain ABSAtasks. Specifically, we employ a multi-head graph attention network as a textencoder optimized by heuristic rules for linguistic, domain, and sentimentfeatures. Through contrastive learning, we optimize sentence representations byfocusing on these diverse features. Additionally, we construct an efficientindexing mechanism, allowing FaiMA to stably retrieve highly relevant examplesacross multiple dimensions for any given input. To evaluate the efficacy ofFaiMA, we build the first multi-domain ABSA benchmark dataset. Extensiveexperimental results demonstrate that FaiMA achieves significant performanceimprovements in multiple domains compared to baselines, increasing F1 by 2.07%on average. Source code and data sets are anonymously available athttps://github.com/SupritYoung/FaiMA.</description><author>Songhua Yang, Xinke Jiang, Hanjie Zhao, Wenxuan Zeng, Hongde Liu, Yuxiang Jia</author><pubDate>Sat, 02 Mar 2024 02:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01063v1</guid></item><item><title>XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples</title><link>http://arxiv.org/abs/2405.05116v1</link><description>Recent studies have shown that leveraging off-the-shelf or fine-tunedretrievers, capable of retrieving high-quality in-context examples,significantly improves in-context learning of English. However, adapting thesemethods to other languages, especially low-resource ones, presents challengesdue to the scarcity of available cross-lingual retrievers and annotated data.In this paper, we introduce XAMPLER: Cross-Lingual Example Retrieval, a methodtailored to tackle the challenge of cross-lingual in-context learning usingonly annotated English data. XAMPLER first trains a retriever withpositive/negative English samples, which are constructed based on thepredictions of the multilingual large language model for in-context learning.Then, the trained retriever is directly employed to retrieve English examplesas few-shot examples for in-context learning of target languages. Experimentson the massively multilingual text classification benchmark of SIB200 with 176languages demonstrate that XAMPLER substantially improves the in-contextlearning performance across languages. Our code is available athttps://github.com/cisnlp/XAMPLER.</description><author>Peiqin Lin, André F. T. Martins, Hinrich Schütze</author><pubDate>Wed, 08 May 2024 16:13:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.05116v1</guid></item><item><title>Self-Augmented In-Context Learning for Unsupervised Word Translation</title><link>http://arxiv.org/abs/2402.10024v1</link><description>Recent work has shown that, while large language models (LLMs) demonstratestrong word translation or bilingual lexicon induction (BLI) capabilities infew-shot setups, they still cannot match the performance of 'traditional'mapping-based approaches in the unsupervised scenario where no seed translationpairs are available, especially for lower-resource languages. To address thischallenge with LLMs, we propose self-augmented in-context learning (SAIL) forunsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces aset of high-confidence word translation pairs for in-context learning (ICL)from an LLM, which it then reapplies to the same LLM in the ICL fashion. Ourmethod shows substantial gains over zero-shot prompting of LLMs on twoestablished BLI benchmarks spanning a wide range of language pairs, alsooutperforming mapping-based baselines across the board. In addition toachieving state-of-the-art unsupervised BLI performance, we also conductcomprehensive analyses on SAIL and discuss its limitations.</description><author>Yaoyiran Li, Anna Korhonen, Ivan Vulić</author><pubDate>Thu, 15 Feb 2024 15:43:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10024v1</guid></item><item><title>Automatic Domain Adaptation by Transformers in In-Context Learning</title><link>http://arxiv.org/abs/2405.16819v1</link><description>Selecting or designing an appropriate domain adaptation algorithm for a givenproblem remains challenging. This paper presents a Transformer model that canprovably approximate and opt for domain adaptation methods for a given datasetin the in-context learning framework, where a foundation model performs newtasks without updating its parameters at test time. Specifically, we prove thatTransformers can approximate instance-based and feature-based unsuperviseddomain adaptation algorithms and automatically select an algorithm suited for agiven dataset. Numerical results indicate that in-context learning demonstratesan adaptive domain adaptation surpassing existing methods.</description><author>Ryuichiro Hataya, Kota Matsui, Masaaki Imaizumi</author><pubDate>Mon, 27 May 2024 05:33:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16819v1</guid></item><item><title>OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examples</title><link>http://arxiv.org/abs/2307.11729v3</link><description>Large Language Models (LLMs) have achieved human-level fluency in textgeneration, making it difficult to distinguish between human-written andLLM-generated texts. This poses a growing risk of misuse of LLMs and demandsthe development of detectors to identify LLM-generated texts. However, existingdetectors lack robustness against attacks: they degrade detection accuracy bysimply paraphrasing LLM-generated texts. Furthermore, a malicious user mightattempt to deliberately evade the detectors based on detection results, butthis has not been assumed in previous studies. In this paper, we proposeOUTFOX, a framework that improves the robustness of LLM-generated-textdetectors by allowing both the detector and the attacker to consider eachother's output. In this framework, the attacker uses the detector's predictionlabels as examples for in-context learning and adversarially generates essaysthat are harder to detect, while the detector uses the adversarially generatedessays as examples for in-context learning to learn to detect essays from astrong attacker. Experiments in the domain of student essays show that theproposed detector improves the detection performance on the attacker-generatedtexts by up to +41.3 points F1-score. Furthermore, the proposed detector showsa state-of-the-art detection performance: up to 96.9 points F1-score, beatingexisting detectors on non-attacked texts. Finally, the proposed attackerdrastically degrades the performance of detectors by up to -57.0 pointsF1-score, massively outperforming the baseline paraphrasing method for evadingdetection.</description><author>Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki</author><pubDate>Sun, 18 Feb 2024 12:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11729v3</guid></item><item><title>GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification</title><link>http://arxiv.org/abs/2404.03052v1</link><description>Harmful and offensive communication or content is detrimental to socialbonding and the mental state of users on social media platforms. Textdetoxification is a crucial task in natural language processing (NLP), wherethe goal is removing profanity and toxicity from text while preserving itscontent. Supervised and unsupervised learning are common approaches fordesigning text detoxification solutions. However, these methods necessitatefine-tuning, leading to computational overhead. In this paper, we proposeGPT-DETOX as a framework for prompt-based in-context learning for textdetoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot promptingtechniques for detoxifying input sentences. To generate few-shot prompts, wepropose two methods: word-matching example selection (WMES) andcontext-matching example selection (CMES). We additionally take into accountensemble in-context learning (EICL) where the ensemble is shaped by baseprompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIAas benchmark detoxification datasets. Our experimental results show that thezero-shot solution achieves promising performance, while our best few-shotsetting outperforms the state-of-the-art models on ParaDetox and showscomparable results on APPDIA. Our EICL solutions obtain the greatestperformance, adding at least 10% improvement, against both datasets.</description><author>Ali Pesaranghader, Nikhil Verma, Manasa Bharadwaj</author><pubDate>Wed, 03 Apr 2024 21:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03052v1</guid></item><item><title>EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning</title><link>http://arxiv.org/abs/2309.10687v3</link><description>Language models are achieving impressive performance on various tasks byaggressively adopting inference-time prompting techniques, such as zero-shotand few-shot prompting. In this work, we introduce EchoPrompt, a simple yeteffective approach that prompts the model to rephrase its queries beforeanswering them. EchoPrompt is adapted for both zero-shot and few-shotin-context learning with standard and chain-of-thought prompting. Experimentalresults show that EchoPrompt yields substantial improvements across all thesesettings for four families of causal language models. These improvements areobserved across various numerical reasoning (e.g. GSM8K, SVAMP), readingcomprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. Onaverage, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002by 5% in numerical tasks and 13% in reading comprehension tasks. We investigatethe factors contributing to EchoPrompt's effectiveness through ablationstudies, which reveal that both the original query and the model-generatedrephrased version are instrumental in its performance gains. Our empiricalresults indicate that EchoPrompt is an effective technique that enhancesin-context learning performance. We recommend incorporating EchoPrompt intovarious baseline prompting strategies to achieve performance boosts.</description><author>Rajasekhar Reddy Mekala, Yasaman Razeghi, Sameer Singh</author><pubDate>Tue, 20 Feb 2024 20:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10687v3</guid></item><item><title>MLPs Learn In-Context</title><link>http://arxiv.org/abs/2405.15618v1</link><description>In-context learning (ICL), the remarkable ability to solve a task from onlyinput exemplars, has commonly been assumed to be a unique hallmark ofTransformer models. In this study, we demonstrate that multi-layer perceptrons(MLPs) can also learn in-context. Moreover, we find that MLPs, and the closelyrelated MLP-Mixer models, learn in-context competitively with Transformersgiven the same compute budget. We further show that MLPs outperformTransformers on a subset of ICL tasks designed to test relational reasoning.These results suggest that in-context learning is not exclusive to Transformersand highlight the potential of exploring this phenomenon beyond attention-basedarchitectures. In addition, MLPs' surprising success on relational taskschallenges prior assumptions about simple connectionist models. Altogether, ourresults endorse the broad trend that ``less inductive bias is better" andcontribute to the growing interest in all-MLP alternatives to task-specificarchitectures.</description><author>William L. Tong, Cengiz Pehlevan</author><pubDate>Fri, 24 May 2024 16:04:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15618v1</guid></item><item><title>Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning</title><link>http://arxiv.org/abs/2404.07546v1</link><description>In-context Learning (ICL) has emerged as a powerful capability alongside thedevelopment of scaled-up large language models (LLMs). By instructing LLMsusing few-shot demonstrative examples, ICL enables them to perform a wide rangeof tasks without updating millions of parameters. However, the precisecontributions of demonstrations towards improving end-task performance have notbeen thoroughly investigated in recent analytical studies. In this paper, weempirically decompose the overall performance of ICL into three dimensions,label space, format, and discrimination, and we evaluate four general-purposeLLMs across a diverse range of tasks. Counter-intuitively, we find that thedemonstrations have a marginal impact on provoking discriminative knowledge oflanguage models. However, ICL exhibits significant efficacy in regulating thelabel space and format which helps LLMs to respond in desired label words. Wethen demonstrate this ability functions similar to detailed instructions forLLMs to follow. We additionally provide an in-depth analysis of the mechanismof retrieval helping with ICL and find that retrieving the most semanticallysimilar examples notably boosts model's discriminative capability.</description><author>Quanyu Long, Yin Wu, Wenya Wang, Sinno Jialin Pan</author><pubDate>Thu, 11 Apr 2024 09:20:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07546v1</guid></item><item><title>NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning</title><link>http://arxiv.org/abs/2402.05515v2</link><description>In-Context Learning (ICL) is suffering from unsatisfactory performance andunder-calibration due to high prior bias and unfaithful confidence. Someprevious works fine-tuned language models for better ICL performance withenormous datasets and computing costs. In this paper, we propose NoisyICL,simply perturbing the model parameters by random noises to strive for betterperformance and calibration. Our experiments on two models and 12 downstreamdatasets show that NoisyICL can help ICL produce more accurate predictions. Ourfurther analysis indicates that NoisyICL enables the model to provide more fairpredictions, and also with more faithful confidence. Therefore, we believe thatNoisyICL is an effective calibration of ICL. Our experimental code is uploadedto Github.</description><author>Yufeng Zhao, Yoshihiro Sakai, Naoya Inoue</author><pubDate>Thu, 15 Feb 2024 15:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05515v2</guid></item><item><title>Unifying Demonstration Selection and Compression for In-Context Learning</title><link>http://arxiv.org/abs/2405.17062v1</link><description>In-context learning (ICL) facilitates large language models (LLMs) exhibitingspectacular emergent capabilities in various scenarios. Unfortunately,introducing demonstrations easily makes the prompt length explode, bringing asignificant burden to hardware. In addition, random demonstrations usuallyachieve limited improvements in ICL, necessitating demonstration selectionamong accessible candidates. Previous studies introduce extra modules toperform demonstration compression or selection independently. In this paper, wepropose an ICL framework UniICL, which Unifies demonstration selection andcompression, and final response generation via a single frozen LLM.Specifically, UniICL first projects actual demonstrations and inference textinputs into short virtual tokens, respectively. Then, virtual tokens areapplied to select suitable demonstrations by measuring semantic similaritywithin latent space among candidate demonstrations and inference input.Finally, inference text inputs together with selected virtual demonstrationsare fed into the same frozen LLM for response generation. Notably, UniICL is aparameter-efficient framework that only contains 17M trainable parametersoriginating from the projection layer. We conduct experiments and analysis overin- and out-domain datasets of both generative and understanding tasks,encompassing ICL scenarios with plentiful and limited demonstration candidates.Results show that UniICL effectively unifies $12 \times$ compression,demonstration selection, and response generation, efficiently scaling up thebaseline from 4-shot to 64-shot ICL in IMDb with 24 GB CUDA allocation</description><author>Jun Gao</author><pubDate>Mon, 27 May 2024 12:31:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17062v1</guid></item><item><title>Towards Global Optimal Visual In-Context Learning Prompt Selection</title><link>http://arxiv.org/abs/2405.15279v1</link><description>Visual In-Context Learning (VICL) is a prevailing way to transfer visualfoundation models to new tasks by leveraging contextual information containedin in-context examples to enhance learning and prediction of query sample. Thefundamental problem in VICL is how to select the best prompt to activate itspower as much as possible, which is equivalent to the ranking problem to testthe in-context behavior of each candidate in the alternative set and select thebest one. To utilize more appropriate ranking metric and leverage morecomprehensive information among the alternative set, we propose a novelin-context example selection framework to approximately identify the globaloptimal prompt, i.e. choosing the best performing in-context examples from allalternatives for each query sample. Our method, dubbed Partial2Global, adopts atransformer-based list-wise ranker to provide a more comprehensive comparisonwithin several alternatives, and a consistency-aware ranking aggregator togenerate globally consistent ranking. The effectiveness of Partial2Global isvalidated through experiments on foreground segmentation, single objectdetection and image colorization, demonstrating that Partial2Global selectsconsistently better in-context examples compared with other methods, and thusestablish the new state-of-the-arts.</description><author>Chengming Xu, Chen Liu, Yikai Wang, Yanwei Fu</author><pubDate>Fri, 24 May 2024 08:07:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15279v1</guid></item><item><title>In-Context Learning Learns Label Relationships but Is Not Conventional Learning</title><link>http://arxiv.org/abs/2307.12375v4</link><description>The predictions of Large Language Models (LLMs) on downstream tasks oftenimprove significantly when including examples of the input--label relationshipin the context. However, there is currently no consensus about how thisin-context learning (ICL) ability of LLMs works. For example, while Xie et al.(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)argue ICL does not even learn label relationships from in-context examples. Inthis paper, we provide novel insights into how ICL leverages label information,revealing both capabilities and limitations. To ensure we obtain acomprehensive picture of ICL behavior, we study probabilistic aspects of ICLpredictions and thoroughly examine the dynamics of ICL as more examples areprovided. Our experiments show that ICL predictions almost always depend onin-context labels and that ICL can learn truly novel tasks in-context. However,we also find that ICL struggles to fully overcome prediction preferencesacquired from pre-training data and, further, that ICL does not consider allin-context information equally.</description><author>Jannik Kossen, Yarin Gal, Tom Rainforth</author><pubDate>Wed, 13 Mar 2024 16:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12375v4</guid></item><item><title>Does learning the right latent variables necessarily improve in-context learning?</title><link>http://arxiv.org/abs/2405.19162v1</link><description>Large autoregressive models like Transformers can solve tasks throughin-context learning (ICL) without learning new weights, suggesting avenues forefficiently solving new tasks. For many tasks, e.g., linear regression, thedata factorizes: examples are independent given a task latent that generatesthe data, e.g., linear coefficients. While an optimal predictor leverages thisfactorization by inferring task latents, it is unclear if Transformersimplicitly do so or if they instead exploit heuristics and statisticalshortcuts enabled by attention layers. Both scenarios have inspired activeongoing work. In this paper, we systematically investigate the effect ofexplicitly inferring task latents. We minimally modify the Transformerarchitecture with a bottleneck designed to prevent shortcuts in favor of morestructured solutions, and then compare performance against standardTransformers across various ICL tasks. Contrary to intuition and some recentworks, we find little discernible difference between the two; biasing towardstask-relevant latent variables does not lead to better out-of-distributionperformance, in general. Curiously, we find that while the bottleneckeffectively learns to extract latent task variables from context, downstreamprocessing struggles to utilize them for robust prediction. Our studyhighlights the intrinsic limitations of Transformers in achieving structuredICL solutions that generalize, and shows that while inferring the right latentsaids interpretability, it is not sufficient to alleviate this problem.</description><author>Sarthak Mittal, Eric Elmoznino, Leo Gagnon, Sangnie Bhardwaj, Dhanya Sridhar, Guillaume Lajoie</author><pubDate>Wed, 29 May 2024 16:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19162v1</guid></item><item><title>Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks</title><link>http://arxiv.org/abs/2402.04248v2</link><description>State-space models (SSMs), such as Mamba (Gu &amp; Dao, 2023), have been proposedas alternatives to Transformer networks in language modeling, by incorporatinggating, convolutions, and input-dependent token selection to mitigate thequadratic cost of multi-head attention. Although SSMs exhibit competitiveperformance, their in-context learning (ICL) capabilities, a remarkableemergent property of modern language models that enables task execution withoutparameter optimization, remain underexplored compared to Transformers. In thisstudy, we evaluate the ICL performance of SSMs, focusing on Mamba, againstTransformer models across various tasks. Our results show that SSMs performcomparably to Transformers in standard regression ICL tasks, whileoutperforming them in tasks like sparse parity learning. However, SSMs fallshort in tasks involving non-standard retrieval functionality. To address theselimitations, we introduce a hybrid model, MambaFormer, that combines Mamba withattention blocks, surpassing individual models in tasks where they struggleindependently. Our findings suggest that hybrid architectures offer promisingavenues for enhancing ICL in language models.</description><author>Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos</author><pubDate>Thu, 25 Apr 2024 18:01:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04248v2</guid></item><item><title>ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification</title><link>http://arxiv.org/abs/2311.09649v2</link><description>This paper focuses on the task of Extreme Multi-Label Classification (XMC)whose goal is to predict multiple labels for each instance from an extremelylarge label space. While existing research has primarily focused on fullysupervised XMC, real-world scenarios often lack supervision signals,highlighting the importance of zero-shot settings. Given the large label space,utilizing in-context learning approaches is not trivial. We address this issueby introducing In-Context Extreme Multilabel Learning (ICXML), a two-stageframework that cuts down the search space by generating a set of candidatelabels through incontext learning and then reranks them. Extensive experimentssuggest that ICXML advances the state of the art on two diverse publicbenchmarks.</description><author>Yaxin Zhu, Hamed Zamani</author><pubDate>Mon, 15 Apr 2024 14:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09649v2</guid></item><item><title>Stronger Random Baselines for In-Context Learning</title><link>http://arxiv.org/abs/2404.13020v1</link><description>Evaluating the in-context learning classification performance of languagemodels poses challenges due to small dataset sizes, extensive prompt-selectionusing the validation set, and intentionally difficult tasks that lead tonear-random performance. The standard random baseline -- the expected accuracyof guessing labels uniformly at random -- is stable when the evaluation set isused only once or when the dataset is large. We account for the common practiceof validation set reuse and existing small datasets with a stronger randombaseline: the expected maximum accuracy across multiple random classifiers.When choosing the best prompt demonstrations across six quantized languagemodels applied to 16 BIG-bench Lite tasks, more than 20\% of the few-shotresults that exceed the standard baseline do not exceed this stronger randombaseline. When held-out test sets are available, this stronger baseline is alsoa better predictor of held-out performance than the standard baseline, avoidingunnecessary test set evaluations. This maximum random baseline provides aneasily calculated drop-in replacement for the standard baseline.</description><author>Gregory Yauney, David Mimno</author><pubDate>Fri, 19 Apr 2024 18:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13020v1</guid></item><item><title>XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare</title><link>http://arxiv.org/abs/2405.06270v1</link><description>The integration of Large Language Models (LLMs) into healthcare diagnosticsoffers a promising avenue for clinical decision-making. This study outlines thedevelopment of a novel method for zero-shot/few-shot in-context learning (ICL)by integrating medical domain knowledge using a multi-layered structuredprompt. We also explore the efficacy of two communication styles between theuser and LLMs: the Numerical Conversational (NC) style, which processes dataincrementally, and the Natural Language Single-Turn (NL-ST) style, whichemploys long narrative prompts. Our study systematically evaluates thediagnostic accuracy and risk factors, including gender bias and false negativerates, using a dataset of 920 patient records in various few-shot scenarios.Results indicate that traditional clinical machine learning (ML) modelsgenerally outperform LLMs in zero-shot and few-shot settings. However, theperformance gap narrows significantly when employing few-shot examplesalongside effective explainable AI (XAI) methods as sources of domainknowledge. Moreover, with sufficient time and an increased number of examples,the conversational style (NC) nearly matches the performance of ML models. Mostnotably, LLMs demonstrate comparable or superior cost-sensitive accuracyrelative to ML models. This research confirms that, with appropriate domainknowledge and tailored communication strategies, LLMs can significantly enhancediagnostic processes. The findings highlight the importance of optimizing thenumber of training examples and communication styles to improve accuracy andreduce biases in LLM applications.</description><author>Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio</author><pubDate>Fri, 10 May 2024 07:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06270v1</guid></item><item><title>XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare</title><link>http://arxiv.org/abs/2405.06270v2</link><description>The integration of Large Language Models (LLMs) into healthcare diagnosticsoffers a promising avenue for clinical decision-making. This study outlines thedevelopment of a novel method for zero-shot/few-shot in-context learning (ICL)by integrating medical domain knowledge using a multi-layered structuredprompt. We also explore the efficacy of two communication styles between theuser and LLMs: the Numerical Conversational (NC) style, which processes dataincrementally, and the Natural Language Single-Turn (NL-ST) style, whichemploys long narrative prompts. Our study systematically evaluates the diagnostic accuracy and risk factors,including gender bias and false negative rates, using a dataset of 920 patientrecords in various few-shot scenarios. Results indicate that traditionalclinical machine learning (ML) models generally outperform LLMs in zero-shotand few-shot settings. However, the performance gap narrows significantly whenemploying few-shot examples alongside effective explainable AI (XAI) methods assources of domain knowledge. Moreover, with sufficient time and an increasednumber of examples, the conversational style (NC) nearly matches theperformance of ML models. Most notably, LLMs demonstrate comparable or superiorcost-sensitive accuracy relative to ML models. This research confirms that, with appropriate domain knowledge and tailoredcommunication strategies, LLMs can significantly enhance diagnostic processes.The findings highlight the importance of optimizing the number of trainingexamples and communication styles to improve accuracy and reduce biases in LLMapplications.</description><author>Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio</author><pubDate>Wed, 15 May 2024 12:59:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06270v2</guid></item><item><title>XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare</title><link>http://arxiv.org/abs/2405.06270v3</link><description>The integration of Large Language Models (LLMs) into healthcare diagnosticsoffers a promising avenue for clinical decision-making. This study outlines thedevelopment of a novel method for zero-shot/few-shot in-context learning (ICL)by integrating medical domain knowledge using a multi-layered structuredprompt. We also explore the efficacy of two communication styles between theuser and LLMs: the Numerical Conversational (NC) style, which processes dataincrementally, and the Natural Language Single-Turn (NL-ST) style, whichemploys long narrative prompts. Our study systematically evaluates the diagnostic accuracy and risk factors,including gender bias and false negative rates, using a dataset of 920 patientrecords in various few-shot scenarios. Results indicate that traditionalclinical machine learning (ML) models generally outperform LLMs in zero-shotand few-shot settings. However, the performance gap narrows significantly whenemploying few-shot examples alongside effective explainable AI (XAI) methods assources of domain knowledge. Moreover, with sufficient time and an increasednumber of examples, the conversational style (NC) nearly matches theperformance of ML models. Most notably, LLMs demonstrate comparable or superiorcost-sensitive accuracy relative to ML models. This research confirms that, with appropriate domain knowledge and tailoredcommunication strategies, LLMs can significantly enhance diagnostic processes.The findings highlight the importance of optimizing the number of trainingexamples and communication styles to improve accuracy and reduce biases in LLMapplications.</description><author>Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia, Eugenio di Sciascio</author><pubDate>Mon, 03 Jun 2024 17:23:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.06270v3</guid></item><item><title>Contextrast: Contextual Contrastive Learning for Semantic Segmentation</title><link>http://arxiv.org/abs/2404.10633v1</link><description>Despite great improvements in semantic segmentation, challenges persistbecause of the lack of local/global contexts and the relationship between them.In this paper, we propose Contextrast, a contrastive learning-based semanticsegmentation method that allows to capture local/global contexts and comprehendtheir relationships. Our proposed method comprises two parts: a) contextualcontrastive learning (CCL) and b) boundary-aware negative (BANE) sampling.Contextual contrastive learning obtains local/global context from multi-scalefeature aggregation and inter/intra-relationship of features for betterdiscrimination capabilities. Meanwhile, BANE sampling selects embeddingfeatures along the boundaries of incorrectly predicted regions to employ themas harder negative samples on our contrastive learning, resolving segmentationissues along the boundary region by exploiting fine-grained details. Wedemonstrate that our Contextrast substantially enhances the performance ofsemantic segmentation networks, outperforming state-of-the-art contrastivelearning approaches on diverse public datasets, e.g. Cityscapes, CamVid,PASCAL-C, COCO-Stuff, and ADE20K, without an increase in computational costduring inference.</description><author>Changki Sung, Wanhee Kim, Jungho An, Wooju Lee, Hyungtae Lim, Hyun Myung</author><pubDate>Tue, 16 Apr 2024 16:04:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10633v1</guid></item><item><title>Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge for In-Context Learning</title><link>http://arxiv.org/abs/2309.14771v2</link><description>Large language models (LLMs) enable in-context learning (ICL) by conditioningon a few labeled training examples as a text-based prompt, eliminating the needfor parameter updates and achieving competitive performance. In this paper, wedemonstrate that factual knowledge is imperative for the performance of ICL inthree core facets: the inherent knowledge learned in LLMs, the factualknowledge derived from the selected in-context examples, and the knowledgebiases in LLMs for output generation. To unleash the power of LLMs in few-shotlearning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT)framework to further improve the performance of ICL: 1) injecting knowledgeinto LLMs during continual self-supervised pre-training, 2) judiciouslyselecting the examples for ICL with high knowledge relevance, and 3)calibrating the prediction results based on prior knowledge. We evaluate theproposed approaches on autoregressive models (e.g., GPT-style LLMs) overmultiple text classification and question-answering tasks. Experimental resultsdemonstrate that KICT substantially outperforms strong baselines and improvesby more than 13% and 7% on text classification and question-answering tasks,respectively.</description><author>Jianing Wang, Chengyu Wang, Chuanqi Tan, Jun Huang, Ming Gao</author><pubDate>Sun, 31 Mar 2024 14:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14771v2</guid></item><item><title>RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning</title><link>http://arxiv.org/abs/2305.14502v2</link><description>Recent developments in large pre-trained language models have enabledunprecedented performance on a variety of downstream tasks. Achieving bestperformance with these models often leverages in-context learning, where amodel performs a (possibly new) task given one or more examples. However,recent work has shown that the choice of examples can have a large impact ontask performance and that finding an optimal set of examples is non-trivial.While there are many existing methods for selecting in-context examples, theygenerally score examples independently, ignoring the dependency between themand the order in which they are provided to the model. In this work, we proposeRetrieval for In-Context Learning (RetICL), a learnable method for modeling andoptimally selecting examples sequentially for in-context learning. We frame theproblem of sequential example selection as a Markov decision process and trainan example retriever using reinforcement learning. We evaluate RetICL on mathword problem solving and scientific question answering tasks and show that itconsistently outperforms or matches heuristic and learnable baselines. We alsouse case studies to show that RetICL implicitly learns representations ofproblem solving strategies.</description><author>Alexander Scarlatos, Andrew Lan</author><pubDate>Tue, 16 Apr 2024 18:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14502v2</guid></item><item><title>Many-Shot In-Context Learning</title><link>http://arxiv.org/abs/2404.11018v1</link><description>Large language models (LLMs) excel at few-shot in-context learning (ICL) --learning from a few examples provided in context at inference, without anyweight updates. Newly expanded context windows allow us to investigate ICL withhundreds or thousands of examples -- the many-shot regime. Going from few-shotto many-shot, we observe significant performance gains across a wide variety ofgenerative and discriminative tasks. While promising, many-shot ICL can bebottlenecked by the available amount of human-generated examples. To mitigatethis limitation, we explore two new settings: Reinforced and Unsupervised ICL.Reinforced ICL uses model-generated chain-of-thought rationales in place ofhuman examples. Unsupervised ICL removes rationales from the prompt altogether,and prompts the model only with domain-specific questions. We find that bothReinforced and Unsupervised ICL can be quite effective in the many-shot regime,particularly on complex reasoning tasks. Finally, we demonstrate that, unlikefew-shot learning, many-shot learning is effective at overriding pretrainingbiases and can learn high-dimensional functions with numerical inputs. Ouranalysis also reveals the limitations of next-token prediction loss as anindicator of downstream ICL performance.</description><author>Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, Hugo Larochelle</author><pubDate>Wed, 17 Apr 2024 03:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11018v1</guid></item><item><title>Many-Shot In-Context Learning</title><link>http://arxiv.org/abs/2404.11018v2</link><description>Large language models (LLMs) excel at few-shot in-context learning (ICL) --learning from a few examples provided in context at inference, without anyweight updates. Newly expanded context windows allow us to investigate ICL withhundreds or thousands of examples -- the many-shot regime. Going from few-shotto many-shot, we observe significant performance gains across a wide variety ofgenerative and discriminative tasks. While promising, many-shot ICL can bebottlenecked by the available amount of human-generated examples. To mitigatethis limitation, we explore two new settings: Reinforced and Unsupervised ICL.Reinforced ICL uses model-generated chain-of-thought rationales in place ofhuman examples. Unsupervised ICL removes rationales from the prompt altogether,and prompts the model only with domain-specific questions. We find that bothReinforced and Unsupervised ICL can be quite effective in the many-shot regime,particularly on complex reasoning tasks. Finally, we demonstrate that, unlikefew-shot learning, many-shot learning is effective at overriding pretrainingbiases, can learn high-dimensional functions with numerical inputs, andperforms comparably to fine-tuning. Our analysis also reveals the limitationsof next-token prediction loss as an indicator of downstream ICL performance.</description><author>Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, Hugo Larochelle</author><pubDate>Wed, 22 May 2024 18:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11018v2</guid></item><item><title>Leveraging Code to Improve In-context Learning for Semantic Parsing</title><link>http://arxiv.org/abs/2311.09519v2</link><description>In-context learning (ICL) is an appealing approach for semantic parsing dueto its few-shot nature and improved generalization. However, learning to parseto rare domain-specific languages (DSLs) from just a few demonstrations ischallenging, limiting the performance of even the most capable LLMs. In thiswork, we improve the effectiveness of ICL for semantic parsing by (1) usinggeneral-purpose programming languages such as Python instead of DSLs, and (2)augmenting prompts with a structured domain description that includes, e.g.,the available classes and functions. We show that both these changessignificantly improve accuracy across three popular datasets. Combined, theylead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositionalsplit), nearly closing the performance gap between easier i.i.d.\ and hardercompositional splits when used with a strong model, and reducing the need for alarge number of demonstrations. We find that the resemblance of the targetparse language to general-purpose code is a more important factor than thelanguage's popularity in pre-training corpora. Our findings provide an improvedmethodology for building semantic parsers in the modern context of ICL withLLMs.</description><author>Ben Bogin, Shivanshu Gupta, Peter Clark, Ashish Sabharwal</author><pubDate>Wed, 27 Mar 2024 22:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09519v2</guid></item><item><title>In-context Exploration-Exploitation for Reinforcement Learning</title><link>http://arxiv.org/abs/2403.06826v1</link><description>In-context learning is a promising approach for online policy learning ofoffline reinforcement learning (RL) methods, which can be achieved at inferencetime without gradient optimization. However, this method is hindered bysignificant computational costs resulting from the gathering of large trainingtrajectory sets and the need to train large Transformer models. We address thischallenge by introducing an In-context Exploration-Exploitation (ICEE)algorithm, designed to optimize the efficiency of in-context policy learning.Unlike existing models, ICEE performs an exploration-exploitation trade-off atinference time within a Transformer model, without the need for explicitBayesian inference. Consequently, ICEE can solve Bayesian optimization problemsas efficiently as Gaussian process biased methods do, but in significantly lesstime. Through experiments in grid world environments, we demonstrate that ICEEcan learn to solve new RL tasks using only tens of episodes, marking asubstantial improvement over the hundreds of episodes needed by the previousin-context learning method.</description><author>Zhenwen Dai, Federico Tomasi, Sina Ghiassian</author><pubDate>Mon, 11 Mar 2024 16:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06826v1</guid></item><item><title>Human Curriculum Effects Emerge with In-Context Learning in Neural Networks</title><link>http://arxiv.org/abs/2402.08674v2</link><description>Human learning is sensitive to rule-like structure and the curriculum ofexamples used for training. In tasks governed by succinct rules, learning ismore robust when related examples are blocked across trials, but in the absenceof such rules, interleaving is more effective. To date, no neural model hassimultaneously captured these seemingly contradictory effects. Here we showthat this same tradeoff spontaneously emerges with ``in-context learning''(ICL) both in neural networks trained with metalearning and in large languagemodels (LLMs). ICL is the ability to learn new tasks ``in context'' -- withoutweight changes -- via an inner-loop algorithm implemented in activationdynamics. Experiments with pretrained LLMs and metalearning transformers showthat ICL exhibits the blocking advantage demonstrated in humans on a taskinvolving rule-like structure, and conversely, that concurrent in-weightlearning reproduces the interleaving advantage observed in humans on taskslacking such structure.</description><author>Jacob Russin, Ellie Pavlick, Michael J. Frank</author><pubDate>Sun, 12 May 2024 09:24:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08674v2</guid></item><item><title>Rectifying Demonstration Shortcut in In-Context Learning</title><link>http://arxiv.org/abs/2403.09488v1</link><description>Large language models (LLMs) are able to solve various tasks with only a fewdemonstrations utilizing their in-context learning (ICL) abilities. However,LLMs often rely on their pre-trained semantic priors of demonstrations ratherthan on the input-label relationships to proceed with ICL prediction. In thiswork, we term this phenomenon as the `Demonstration Shortcut'. While previousworks have primarily focused on improving ICL prediction results for predefinedtasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLMto effectively learn new input-label relationships from demonstrations. Toachieve this, we introduce In-Context Calibration, a demonstration-awarecalibration method. We evaluate the effectiveness of the proposed method in twosettings: (1) the Original ICL Task using the standard label space and (2) theTask Learning setting, where the label space is replaced with semanticallyunrelated tokens. In both settings, In-Context Calibration demonstratessubstantial improvements, with results generalized across three LLM families(OPT, GPT, and Llama2) under various configurations.</description><author>Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon, Hwanjo Yu</author><pubDate>Thu, 14 Mar 2024 16:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09488v1</guid></item><item><title>Rectifying Demonstration Shortcut in In-Context Learning</title><link>http://arxiv.org/abs/2403.09488v2</link><description>Large language models (LLMs) are able to solve various tasks with only a fewdemonstrations utilizing their in-context learning (ICL) abilities. However,LLMs often rely on their pre-trained semantic priors of demonstrations ratherthan on the input-label relationships to proceed with ICL prediction. In thiswork, we term this phenomenon as the 'Demonstration Shortcut'. While previousworks have primarily focused on improving ICL prediction results for predefinedtasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLMto effectively learn new input-label relationships from demonstrations. Toachieve this, we introduce In-Context Calibration, a demonstration-awarecalibration method. We evaluate the effectiveness of the proposed method in twosettings: (1) the Original ICL Task using the standard label space and (2) theTask Learning setting, where the label space is replaced with semanticallyunrelated tokens. In both settings, In-Context Calibration demonstratessubstantial improvements, with results generalized across three LLM families(OPT, GPT, and Llama2) under various configurations.</description><author>Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon, Hwanjo Yu</author><pubDate>Fri, 29 Mar 2024 06:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09488v2</guid></item><item><title>Rectifying Demonstration Shortcut in In-Context Learning</title><link>http://arxiv.org/abs/2403.09488v3</link><description>Large language models (LLMs) are able to solve various tasks with only a fewdemonstrations utilizing their in-context learning (ICL) abilities. However,LLMs often rely on their pre-trained semantic priors of demonstrations ratherthan on the input-label relationships to proceed with ICL prediction. In thiswork, we term this phenomenon as the 'Demonstration Shortcut'. While previousworks have primarily focused on improving ICL prediction results for predefinedtasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLMto effectively learn new input-label relationships from demonstrations. Toachieve this, we introduce In-Context Calibration, a demonstration-awarecalibration method. We evaluate the effectiveness of the proposed method in twosettings: (1) the Original ICL Task using the standard label space and (2) theTask Learning setting, where the label space is replaced with semanticallyunrelated tokens. In both settings, In-Context Calibration demonstratessubstantial improvements, with results generalized across three LLM families(OPT, GPT, and Llama2) under various configurations.</description><author>Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon, Hwanjo Yu</author><pubDate>Mon, 15 Apr 2024 05:29:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09488v3</guid></item><item><title>In-Context Learning through the Bayesian Prism</title><link>http://arxiv.org/abs/2306.04891v2</link><description>In-context learning (ICL) is one of the surprising and useful features oflarge language models and subject of intense research. Recently, stylizedmeta-learning-like ICL setups have been devised that train transformers onsequences of input-output pairs $(x, f(x))$. The function $f$ comes from afunction class and generalization is checked by evaluating on sequencesgenerated from unseen functions from the same class. One of the maindiscoveries in this line of research has been that for several functionclasses, such as linear regression, transformers successfully generalize to newfunctions in the class. However, the inductive biases of these models resultingin this behavior are not clearly understood. A model with unlimited trainingdata and compute is a Bayesian predictor: it learns the pretrainingdistribution. In this paper we empirically examine how far this Bayesianperspective can help us understand ICL. To this end, we generalize the previousmeta-ICL setup to hierarchical meta-ICL setup which involve unions of multipletask families. We instantiate this setup on a diverse range of linear andnonlinear function families and find that transformers can do ICL in thissetting as well. Where Bayesian inference is tractable, we find evidence thathigh-capacity transformers mimic the Bayesian predictor. The Bayesianperspective provides insights into the inductive bias of ICL and howtransformers perform a particular task when they are trained on multiple tasks.We also find that transformers can learn to generalize to new function classesthat were not seen during pretraining. This involves deviation from theBayesian predictor. We examine these deviations in more depth offering newinsights and hypotheses.</description><author>Madhur Panwar, Kabir Ahuja, Navin Goyal</author><pubDate>Sun, 14 Apr 2024 06:12:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04891v2</guid></item><item><title>In-context Learning and Gradient Descent Revisited</title><link>http://arxiv.org/abs/2311.07772v4</link><description>In-context learning (ICL) has shown impressive results in few-shot learningtasks, yet its underlying mechanism is still not fully understood. A recentline of work suggests that ICL performs gradient descent (GD)-basedoptimization implicitly. While appealing, much of the research focuses onsimplified settings, where the parameters of a shallow model are optimized. Inthis work, we revisit evidence for ICL-GD correspondence on realistic NLP tasksand models. We find gaps in evaluation, both in terms of problematic metricsand insufficient baselines. We show that surprisingly, even untrained modelsachieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next,we explore a major discrepancy in the flow of information throughout the modelbetween ICL and GD, which we term Layer Causality. We propose a simple GD-basedoptimization procedure that respects layer causality, and show it improvessimilarity scores significantly.</description><author>Gilad Deutch, Nadav Magar, Tomer Bar Natan, Guy Dar</author><pubDate>Sun, 31 Mar 2024 20:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07772v4</guid></item><item><title>Analogist: Out-of-the-box Visual In-Context Learning with Image Diffusion Model</title><link>http://arxiv.org/abs/2405.10316v1</link><description>Visual In-Context Learning (ICL) has emerged as a promising research area dueto its capability to accomplish various tasks with limited example pairsthrough analogical reasoning. However, training-based visual ICL haslimitations in its ability to generalize to unseen tasks and requires thecollection of a diverse task dataset. On the other hand, existing methods inthe inference-based visual ICL category solely rely on textual prompts, whichfail to capture fine-grained contextual information from given examples and canbe time-consuming when converting from images to text prompts. To address thesechallenges, we propose Analogist, a novel inference-based visual ICL approachthat exploits both visual and textual prompting techniques using atext-to-image diffusion model pretrained for image inpainting. For visualprompting, we propose a self-attention cloning (SAC) method to guide thefine-grained structural-level analogy between image examples. For textualprompting, we leverage GPT-4V's visual reasoning capability to efficientlygenerate text prompts and introduce a cross-attention masking (CAM) operationto enhance the accuracy of semantic-level analogy guided by text prompts. Ourmethod is out-of-the-box and does not require fine-tuning or optimization. Itis also generic and flexible, enabling a wide range of visual tasks to beperformed in an in-context manner. Extensive experiments demonstrate thesuperiority of our method over existing approaches, both qualitatively andquantitatively.</description><author>Zheng Gu, Shiyuan Yang, Jing Liao, Jing Huo, Yang Gao</author><pubDate>Thu, 16 May 2024 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10316v1</guid></item><item><title>Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation</title><link>http://arxiv.org/abs/2402.09954v1</link><description>Previous in-context learning (ICL) research has focused on tasks such asclassification, machine translation, text2table, etc., while studies on whetherICL can improve human-like dialogue generation are scarce. Our work fills thisgap by systematically investigating the ICL capabilities of large languagemodels (LLMs) in persona-based dialogue generation, conducting extensiveexperiments on high-quality real human Chinese dialogue datasets. Fromexperimental results, we draw three conclusions: 1) adjusting promptinstructions is the most direct, effective, and economical way to improvegeneration quality; 2) randomly retrieving demonstrations (demos) achieves thebest results, possibly due to the greater diversity and the amount of effectiveinformation; counter-intuitively, retrieving demos with a context identical tothe query performs the worst; 3) even when we destroy the multi-turnassociations and single-turn semantics in the demos, increasing the number ofdemos still improves dialogue performance, proving that LLMs can learn fromcorrupted dialogue demos. Previous explanations of the ICL mechanism, such as$n$-gram induction head, cannot fully account for this phenomenon.</description><author>Jiashu Pu, Yajing Wan, Yuru Zhang, Jing Chen, Ling Cheng, Qian Shao, Yongzhu Chang, Tangjie Lv, Rongsheng Zhang</author><pubDate>Thu, 15 Feb 2024 14:03:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09954v1</guid></item><item><title>Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation</title><link>http://arxiv.org/abs/2402.09954v2</link><description>Previous in-context learning (ICL) research has focused on tasks such asclassification, machine translation, text2table, etc., while studies on whetherICL can improve human-like dialogue generation are scarce. Our work fills thisgap by systematically investigating the ICL capabilities of large languagemodels (LLMs) in persona-based dialogue generation, conducting extensiveexperiments on high-quality real human Chinese dialogue datasets. Fromexperimental results, we draw three conclusions: 1) adjusting promptinstructions is the most direct, effective, and economical way to improvegeneration quality; 2) randomly retrieving demonstrations (demos) achieves thebest results, possibly due to the greater diversity and the amount of effectiveinformation; counter-intuitively, retrieving demos with a context identical tothe query performs the worst; 3) even when we destroy the multi-turnassociations and single-turn semantics in the demos, increasing the number ofdemos still improves dialogue performance, proving that LLMs can learn fromcorrupted dialogue demos. Previous explanations of the ICL mechanism, such as$n$-gram induction head, cannot fully account for this phenomenon.</description><author>Jiashu Pu, Yajing Wan, Yuru Zhang, Jing Chen, Ling Cheng, Qian Shao, Yongzhu Chang, Tangjie Lv, Rongsheng Zhang</author><pubDate>Sat, 17 Feb 2024 06:11:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09954v2</guid></item><item><title>Locally Differentially Private In-Context Learning</title><link>http://arxiv.org/abs/2405.04032v1</link><description>Large pretrained language models (LLMs) have shown surprising In-ContextLearning (ICL) ability. An important application in deploying large languagemodels is to augment LLMs with a private database for some specific task. Themain problem with this promising commercial use is that LLMs have been shown tomemorize their training data and their prompt data are vulnerable to membershipinference attacks (MIA) and prompt leaking attacks. In order to deal with thisproblem, we treat LLMs as untrusted in privacy and propose a locallydifferentially private framework of in-context learning(LDP-ICL) in thesettings where labels are sensitive. Considering the mechanisms of in-contextlearning in Transformers by gradient descent, we provide an analysis of thetrade-off between privacy and utility in such LDP-ICL for classification.Moreover, we apply LDP-ICL to the discrete distribution estimation problem. Inthe end, we perform several experiments to demonstrate our analysis results.</description><author>Chunyan Zheng, Keke Sun, Wenhao Zhao, Haibo Zhou, Lixin Jiang, Shaoyang Song, Chunlai Zhou</author><pubDate>Tue, 07 May 2024 07:05:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04032v1</guid></item><item><title>Locally Differentially Private In-Context Learning</title><link>http://arxiv.org/abs/2405.04032v2</link><description>Large pretrained language models (LLMs) have shown surprising In-ContextLearning (ICL) ability. An important application in deploying large languagemodels is to augment LLMs with a private database for some specific task. Themain problem with this promising commercial use is that LLMs have been shown tomemorize their training data and their prompt data are vulnerable to membershipinference attacks (MIA) and prompt leaking attacks. In order to deal with thisproblem, we treat LLMs as untrusted in privacy and propose a locallydifferentially private framework of in-context learning(LDP-ICL) in thesettings where labels are sensitive. Considering the mechanisms of in-contextlearning in Transformers by gradient descent, we provide an analysis of thetrade-off between privacy and utility in such LDP-ICL for classification.Moreover, we apply LDP-ICL to the discrete distribution estimation problem. Inthe end, we perform several experiments to demonstrate our analysis results.</description><author>Chunyan Zheng, Keke Sun, Wenhao Zhao, Haibo Zhou, Lixin Jiang, Shaoyang Song, Chunlai Zhou</author><pubDate>Wed, 08 May 2024 18:10:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04032v2</guid></item><item><title>Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning</title><link>http://arxiv.org/abs/2401.06469v2</link><description>In this paper, by treating in-context learning (ICL) as a meta-optimizationprocess, we explain why LLMs are sensitive to the order of ICL examples. Thisunderstanding leads us to the development of Batch-ICL, an effective,efficient, and order-agnostic inference algorithm for ICL. Differing from thestandard N-shot learning approach, Batch-ICL employs $N$ separate 1-shotforward computations and aggregates the resulting meta-gradients. Theseaggregated meta-gradients are then applied to the forward computation of azero-shot query to generate the final prediction. This batch processingapproach renders the LLM agnostic to the order of ICL examples. Throughextensive experiments and analysis, we demonstrate that Batch-ICL consistentlyoutperforms most permutations of ICL examples. In some cases, it even exceedsthe performance of the best order for standard ICL, all while reducing thecomputational resources required. Furthermore, we develop a novel variant ofBatch-ICL featuring multiple "epochs" of meta-optimization. This variantimplicitly explores permutations of ICL examples, further enhancing ICLperformance.</description><author>Kaiyi Zhang, Ang Lv, Yuhan Chen, Hansen Ha, Tao Xu, Rui Yan</author><pubDate>Fri, 16 Feb 2024 10:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06469v2</guid></item><item><title>In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax</title><link>http://arxiv.org/abs/2311.07811v2</link><description>In-context learning (ICL) is now a common method for teaching large languagemodels (LLMs) new tasks: given labeled examples in the input context, the LLMlearns to perform the task without weight updates. Do models guided via ICLinfer the underlying structure of the task defined by the context, or do theyrely on superficial heuristics that only generalize to identically distributedexamples? We address this question using transformations tasks and an NLI taskthat assess sensitivity to syntax - a requirement for robust languageunderstanding. We further investigate whether out-of-distributiongeneralization can be improved via chain-of-thought prompting, where the modelis provided with a sequence of intermediate computation steps that illustratehow the task ought to be performed. In experiments with models from the GPT,PaLM, and Llama 2 families, we find large variance across LMs. The variance isexplained more by the composition of the pre-training corpus and supervisionmethods than by model size; in particular, models pre-trained on codegeneralize better, and benefit more from chain-of-thought prompting.</description><author>Aaron Mueller, Albert Webson, Jackson Petty, Tal Linzen</author><pubDate>Wed, 10 Apr 2024 16:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07811v2</guid></item><item><title>Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective</title><link>http://arxiv.org/abs/2306.06615v2</link><description>Molecule discovery plays a crucial role in various scientific fields,advancing the design of tailored materials and drugs. However, most of theexisting methods heavily rely on domain experts, require excessivecomputational cost, or suffer from sub-optimal performance. On the other hand,Large Language Models (LLMs), like ChatGPT, have shown remarkable performancein various cross-modal tasks due to their powerful capabilities in naturallanguage understanding, generalization, and in-context learning (ICL), whichprovides unprecedented opportunities to advance molecule discovery. Despiteseveral previous works trying to apply LLMs in this task, the lack ofdomain-specific corpus and difficulties in training specialized LLMs stillremain challenges. In this work, we propose a novel LLM-based framework(MolReGPT) for molecule-caption translation, where an In-Context Few-ShotMolecule Learning paradigm is introduced to empower molecule discovery withLLMs like ChatGPT to perform their in-context learning capability withoutdomain-specific pre-training and fine-tuning. MolReGPT leverages the principleof molecular similarity to retrieve similar molecules and their textdescriptions from a local database to enable LLMs to learn the task knowledgefrom context examples. We evaluate the effectiveness of MolReGPT onmolecule-caption translation, including molecule understanding and text-basedmolecule generation. Experimental results show that compared to fine-tunedmodels, MolReGPT outperforms MolT5-base and is comparable to MolT5-largewithout additional training. To the best of our knowledge, MolReGPT is thefirst work to leverage LLMs via in-context learning in molecule-captiontranslation for advancing molecule discovery. Our work expands the scope of LLMapplications, as well as providing a new paradigm for molecule discovery anddesign.</description><author>Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, Qing Li</author><pubDate>Mon, 22 Apr 2024 18:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06615v2</guid></item><item><title>Parallel Structures in Pre-training Data Yield In-Context Learning</title><link>http://arxiv.org/abs/2402.12530v1</link><description>Pre-trained language models (LMs) are capable of in-context learning (ICL):they can adapt to a task with only a few examples given in the prompt withoutany parameter update. However, it is unclear where this capability comes fromas there is a stark distribution shift between pre-training text and ICLprompts. In this work, we study what patterns of the pre-training datacontribute to ICL. We find that LMs' ICL ability depends on $\textit{parallelstructures}$ in the pre-training data -- pairs of phrases following similartemplates in the same context window. Specifically, we detect parallelstructures by checking whether training on one phrase improves prediction ofthe other, and conduct ablation experiments to study their effect on ICL. Weshow that removing parallel structures in the pre-training data reduces LMs'ICL accuracy by 51% (vs 2% from random ablation). This drop persists even whenexcluding common patterns such as n-gram repetitions and long-range dependency,showing the diversity and generality of parallel structures. A closer look atthe detected parallel structures indicates that they cover diverse linguistictasks and span long distances in the data.</description><author>Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, He He</author><pubDate>Mon, 19 Feb 2024 20:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12530v1</guid></item><item><title>Cell-Free Multi-User MIMO Equalization via In-Context Learning</title><link>http://arxiv.org/abs/2404.05538v1</link><description>Large pre-trained sequence models, such as transformers, excel as few-shotlearners capable of in-context learning (ICL). In ICL, a model is trained toadapt its operation to a new task based on limited contextual information,typically in the form of a few training examples for the given task. Previouswork has explored the use of ICL for channel equalization in single-usermulti-input and multiple-output (MIMO) systems. In this work, we demonstratethat ICL can be also used to tackle the problem of multi-user equalization incell-free MIMO systems with limited fronthaul capacity. In this scenario, atask is defined by channel statistics, signal-to-noise ratio, and modulationschemes. The context encompasses the users' pilot sequences, the correspondingquantized received signals, and the current received data signal. Differentprompt design strategies are proposed and evaluated that encompass alsolarge-scale fading and modulation information. Experiments demonstrate thatICL-based equalization provides estimates with lower mean squared error ascompared to the linear minimum mean squared error equalizer, especially in thepresence of limited fronthaul capacity and pilot contamination.</description><author>Matteo Zecchin, Kai Zu, Osvaldo Simeone</author><pubDate>Mon, 08 Apr 2024 15:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05538v1</guid></item><item><title>Cell-Free Multi-User MIMO Equalization via In-Context Learning</title><link>http://arxiv.org/abs/2404.05538v2</link><description>Large pre-trained sequence models, such as transformers, excel as few-shotlearners capable of in-context learning (ICL). In ICL, a model is trained toadapt its operation to a new task based on limited contextual information,typically in the form of a few training examples for the given task. Previouswork has explored the use of ICL for channel equalization in single-usermulti-input and multiple-output (MIMO) systems. In this work, we demonstratethat ICL can be also used to tackle the problem of multi-user equalization incell-free MIMO systems with limited fronthaul capacity. In this scenario, atask is defined by channel statistics, signal-to-noise ratio, and modulationschemes. The context encompasses the users' pilot sequences, the correspondingquantized received signals, and the current received data signal. Differentprompt design strategies are proposed and evaluated that encompass alsolarge-scale fading and modulation information. Experiments demonstrate thatICL-based equalization provides estimates with lower mean squared error ascompared to the linear minimum mean squared error equalizer, especially in thepresence of limited fronthaul capacity and pilot contamination.</description><author>Matteo Zecchin, Kai Yu, Osvaldo Simeone</author><pubDate>Thu, 11 Apr 2024 10:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05538v2</guid></item><item><title>In-context Learning with Retrieved Demonstrations for Language Models: A Survey</title><link>http://arxiv.org/abs/2401.11624v3</link><description>Language models, especially pre-trained large language models, have showcasedremarkable abilities as few-shot in-context learners (ICL), adept at adaptingto new tasks with just a few demonstrations in the input context. However, themodel's ability to perform ICL is sensitive to the choice of the few-shotdemonstrations. Instead of using a fixed set of demonstrations, one recentdevelopment is to retrieve demonstrations tailored to each input query. Theimplementation of demonstration retrieval is relatively straightforward,leveraging existing databases and retrieval systems. This not only improves theefficiency and scalability of the learning process but also has been shown toreduce biases inherent in manual example selection. In light of the encouragingresults and growing research in ICL with retrieved demonstrations, we conductan extensive review of studies in this area. In this survey, we discuss andcompare different design choices for retrieval models, retrieval trainingprocedures, and inference algorithms.</description><author>Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi</author><pubDate>Tue, 13 Feb 2024 20:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11624v3</guid></item><item><title>Transformer In-Context Learning for Categorical Data</title><link>http://arxiv.org/abs/2405.17248v1</link><description>Recent research has sought to understand Transformers through the lens ofin-context learning with functional data. We extend that line of work with thegoal of moving closer to language models, considering categorical outcomes,nonlinear underlying models, and nonlinear attention. The contextual data areof the form $\textsf{C}=(x_1,c_1,\dots,x_N,c_{N})$ where each$c_i\in\{0,\dots,C-1\}$ is drawn from a categorical distribution that dependson covariates $x_i\in\mathbb{R}^d$. Contextual outcomes in the $m$th set ofcontextual data, $\textsf{C}_m$, are modeled in terms of latent function$f_m(x)\in\textsf{F}$, where $\textsf{F}$ is a functional class with$(C-1)$-dimensional vector output. The probability of observing class$c\in\{0,\dots,C-1\}$ is modeled in terms of the output components of $f_m(x)$via the softmax. The Transformer parameters may be trained with $M$ contextualexamples, $\{\textsf{C}_m\}_{m=1,M}$, and the trained model is then applied tonew contextual data $\textsf{C}_{M+1}$ for new $f_{M+1}(x)\in\textsf{F}$. Thegoal is for the Transformer to constitute the probability of each category$c\in\{0,\dots,C-1\}$ for a new query $x_{N_{M+1}+1}$. We assume each componentof $f_m(x)$ resides in a reproducing kernel Hilbert space (RKHS), specifying$\textsf{F}$. Analysis and an extensive set of experiments suggest that on itsforward pass the Transformer (with attention defined by the RKHS kernel)implements a form of gradient descent of the underlying function, connected tothe latent vector function associated with the softmax. We present what isbelieved to be the first real-world demonstration of this few-shot-learningmethodology, using the ImageNet dataset.</description><author>Aaron T. Wang, Ricardo Henao, Lawrence Carin</author><pubDate>Mon, 27 May 2024 16:03:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17248v1</guid></item><item><title>ARC: A Generalist Graph Anomaly Detector with In-Context Learning</title><link>http://arxiv.org/abs/2405.16771v1</link><description>Graph anomaly detection (GAD), which aims to identify abnormal nodes thatdiffer from the majority within a graph, has garnered significant attention.However, current GAD methods necessitate training specific to each dataset,resulting in high training costs, substantial data requirements, and limitedgeneralizability when being applied to new datasets and domains. To addressthese limitations, this paper proposes ARC, a generalist GAD approach thatenables a ``one-for-all'' GAD model to detect anomalies across various graphdatasets on-the-fly. Equipped with in-context learning, ARC can directlyextract dataset-specific patterns from the target dataset using few-shot normalsamples at the inference stage, without the need for retraining or fine-tuningon the target dataset. ARC comprises three components that are well-crafted forcapturing universal graph anomaly patterns: 1) smoothness-based featureAlignment module that unifies the features of different datasets into a commonand anomaly-sensitive space; 2) ego-neighbor Residual graph encoder that learnsabnormality-related node embeddings; and 3) cross-attentive in-Context anomalyscoring module that predicts node abnormality by leveraging few-shot normalsamples. Extensive experiments on multiple benchmark datasets from variousdomains demonstrate the superior anomaly detection performance, efficiency, andgeneralizability of ARC.</description><author>Yixin Liu, Shiyuan Li, Yu Zheng, Qingfeng Chen, Chengqi Zhang, Shirui Pan</author><pubDate>Mon, 27 May 2024 03:42:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16771v1</guid></item></channel></rss>