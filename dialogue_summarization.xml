<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivdialogue summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 06 Sep 2024 13:00:26 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Cultural Commonsense Knowledge for Intercultural Dialogues</title><link>http://arxiv.org/abs/2402.10689v3</link><description>Despite recent progress, large language models (LLMs) still face thechallenge of appropriately reacting to the intricacies of social and culturalconventions. This paper presents MANGO, a methodology for distillinghigh-accuracy, high-recall assertions of cultural knowledge. We judiciously anditeratively prompt LLMs for this purpose from two entry points, concepts andcultures. Outputs are consolidated via clustering and generative summarization.Running the MANGO method with GPT-3.5 as underlying LLM yields 167Khigh-accuracy assertions for 30K concepts and 11K cultures, surpassing priorresources by a large margin in quality and size. In an extrinsic evaluation forintercultural dialogues, we explore augmenting dialogue systems with culturalknowledge assertions. Notably, despite LLMs inherently possessing culturalknowledge, we find that adding knowledge from MANGO improves the overallquality, specificity, and cultural sensitivity of dialogue responses, as judgedby human annotators. Data and code are available for download.</description><author>Tuan-Phong Nguyen, Simon Razniewski, Gerhard Weikum</author><pubDate>Tue, 23 Jul 2024 10:28:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10689v3</guid></item><item><title>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</title><link>http://arxiv.org/abs/2305.18290v3</link><description>While large-scale unsupervised language models (LMs) learn broad worldknowledge and some reasoning skills, achieving precise control of theirbehavior is difficult due to the completely unsupervised nature of theirtraining. Existing methods for gaining such steerability collect human labelsof the relative quality of model generations and fine-tune the unsupervised LMto align with these preferences, often with reinforcement learning from humanfeedback (RLHF). However, RLHF is a complex and often unstable procedure, firstfitting a reward model that reflects the human preferences, and thenfine-tuning the large unsupervised LM using reinforcement learning to maximizethis estimated reward without drifting too far from the original model. In thispaper we introduce a new parameterization of the reward model in RLHF thatenables extraction of the corresponding optimal policy in closed form, allowingus to solve the standard RLHF problem with only a simple classification loss.The resulting algorithm, which we call Direct Preference Optimization (DPO), isstable, performant, and computationally lightweight, eliminating the need forsampling from the LM during fine-tuning or performing significanthyperparameter tuning. Our experiments show that DPO can fine-tune LMs to alignwith human preferences as well as or better than existing methods. Notably,fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment ofgenerations, and matches or improves response quality in summarization andsingle-turn dialogue while being substantially simpler to implement and train.</description><author>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn</author><pubDate>Mon, 29 Jul 2024 22:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18290v3</guid></item><item><title>Instructive Dialogue Summarization with Query Aggregations</title><link>http://arxiv.org/abs/2310.10981v3</link><description>Conventional dialogue summarization methods directly generate summaries anddo not consider user's specific interests. This poses challenges in cases wherethe users are more focused on particular topics or aspects. With theadvancement of instruction-finetuned language models, we introduceinstruction-tuning to dialogues to expand the capability set of dialoguesummarization models. To overcome the scarcity of instructive dialoguesummarization data, we propose a three-step approach to synthesize high-qualityquery-based summarization triples. This process involves summary-anchored querygeneration, query filtering, and query-based summary generation. By training aunified model called InstructDS (Instructive Dialogue Summarization) on threesummarization datasets with multi-purpose instructive triples, we expand thecapability of dialogue summarization models. We evaluate our method on fourdatasets, including dialogue summarization and dialogue reading comprehension.Experimental results show that our approach outperforms the state-of-the-artmodels and even models with larger sizes. Additionally, our model exhibitshigher generalizability and faithfulness, as confirmed by human subjectiveevaluations.</description><author>Bin Wang, Zhengyuan Liu, Nancy F. Chen</author><pubDate>Thu, 01 Aug 2024 09:53:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10981v3</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v7</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Tue, 06 Aug 2024 14:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v7</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v8</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Fri, 09 Aug 2024 14:48:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v8</guid></item><item><title>A Modular Approach for Multimodal Summarization of TV Shows</title><link>http://arxiv.org/abs/2403.03823v9</link><description>In this paper we address the task of summarizing television shows, whichtouches key areas in AI research: complex reasoning, multiple modalities, andlong narratives. We present a modular approach where separate componentsperform specialized sub-tasks which we argue affords greater flexibilitycompared to end-to-end methods. Our modules involve detecting scene boundaries,reordering scenes so as to minimize the number of cuts between differentevents, converting visual information to text, summarizing the dialogue in eachscene, and fusing the scene summaries into a final summary for the entireepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIonof Summary FActs), to measure both precision and recall of generated summaries,which we decompose into atomic facts. Tested on the recently releasedSummScreen3D dataset, our method produces higher quality summaries thancomparison models, as measured with ROUGE and our new fact-based metric, and asassessed by human evaluators.</description><author>Louis Mahon, Mirella Lapata</author><pubDate>Thu, 22 Aug 2024 10:00:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03823v9</guid></item><item><title>MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues</title><link>http://arxiv.org/abs/2408.14418v1</link><description>Automatic Speech Recognition (ASR) systems are pivotal in transcribing speechinto text, yet the errors they introduce can significantly degrade theperformance of downstream tasks like summarization. This issue is particularlypronounced in clinical dialogue summarization, a low-resource domain wheresupervised data for fine-tuning is scarce, necessitating the use of ASR modelsas black-box solutions. Employing conventional data augmentation for enhancingthe noise robustness of summarization models is not feasible either due to theunavailability of sufficient medical dialogue audio recordings andcorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,an approach for generating synthetic samples for data augmentation using LargeLanguage Models (LLMs). Specifically, we leverage the in-context learningcapabilities of LLMs and instruct them to generate ASR-like errors based on afew available medical dialogue examples with audio recordings. Experimentalresults show that LLMs can effectively model ASR noise, and incorporating thisnoisy data into the training process significantly improves the robustness andaccuracy of medical dialogue summarization systems. This approach addresses thechallenges of noisy ASR outputs in critical applications, offering a robustsolution to enhance the reliability of clinical dialogue summarization.</description><author>Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler</author><pubDate>Mon, 26 Aug 2024 17:04:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14418v1</guid></item><item><title>Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system</title><link>http://arxiv.org/abs/2307.15793v2</link><description>Meetings play a critical infrastructural role in the coordination of work. Inrecent years, due to shift to hybrid and remote work, more meetings are movingto online Computer Mediated Spaces. This has led to new problems (e.g. moretime spent in less engaging meetings) and new opportunities (e.g. automatedtranscription/captioning and recap support). Recent advances in large languagemodels (LLMs) for dialog summarization have the potential to improve theexperience of meetings by reducing individuals' meeting load and increasing theclarity and alignment of meeting outputs. Despite this potential, they facetechnological limitation due to long transcripts and inability to capturediverse recap needs based on user's context. To address these gaps, we design,implement and evaluate in-context a meeting recap system. We firstconceptualize two salient recap representations -- important highlights, and astructured, hierarchical minutes view. We develop a system to operationalizethe representations with dialogue summarization as its building blocks.Finally, we evaluate the effectiveness of the system with seven users in thecontext of their work meetings. Our findings show promise in using LLM-baseddialogue summarization for meeting recap and the need for both representationsin different contexts. However, we find that LLM-based recap still lacks anunderstanding of whats personally relevant to participants, can miss importantdetails, and mis-attributions can be detrimental to group dynamics. We identifycollaboration opportunities such as a shared recap document that a high qualityrecap enables. We report on implications for designing AI systems to partnerwith users to learn and improve from natural interactions to overcome thelimitations related to personal relevance and summarization quality.</description><author>Sumit Asthana, Sagih Hilleli, Pengcheng He, Aaron Halfaker</author><pubDate>Thu, 29 Aug 2024 00:32:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15793v2</guid></item></channel></rss>