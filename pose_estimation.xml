<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivpose estimation</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 01 Sep 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ZeroPose: CAD-Model-based Zero-Shot Pose Estimation</title><link>http://arxiv.org/abs/2305.17934v2</link><description>In this paper, we present a CAD model-based zero-shot pose estimationpipeline called ZeroPose. Existing pose estimation methods remain to requireexpensive training when applied to an unseen object, which greatly hinderstheir scalability in the practical application of industry. In contrast, theproposed method enables the accurate estimation of pose parameters forpreviously unseen objects without the need for training. Specifically, wedesign a two-step pipeline consisting of CAD model-based zero-shot instancesegmentation and a zero-shot pose estimator. For the first step, there is asimple but effective way to leverage CAD models and visual foundation modelsSAM and Imagebind to segment the interest unseen object at the instance level.For the second step, we based on the intensive geometric information in the CADmodel of the rigid object to propose a lightweight hierarchical geometricstructure matching mechanism achieving zero-shot pose estimation. Extensiveexperimental results on the seven core datasets on the BOP challenge show thatthe proposed zero-shot instance segmentation methods achieve comparableperformance with supervised MaskRCNN and the zero-shot pose estimation resultsoutperform the SOTA pose estimators with better efficiency.</description><author>Jianqiu Chen, Mingshan Sun, Tianpeng Bao, Rui Zhao, Liwei Wu, Zhenyu He</author><pubDate>Mon, 21 Aug 2023 10:18:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17934v2</guid></item><item><title>An Efficient Convex Hull-based Vehicle Pose Estimation Method for 3D LiDAR</title><link>http://arxiv.org/abs/2302.01034v2</link><description>Vehicle pose estimation with LiDAR is essential in the perception technologyof autonomous driving. However, due to incomplete observation measurements andsparsity of the LiDAR point cloud, it is challenging to achieve satisfactorypose extraction based on 3D LiDAR by using the existing pose estimationmethods. In addition, the requirement for real-time performance furtherincreases the difficulty of the pose estimation task. In this paper, weproposed a novel convex hull-based vehicle pose estimation method. Theextracted 3D cluster is reduced to the convex hull, reducing the computationburden and retaining contour information. Then a novel criterion based on theminimum occlusion area is developed for the search-based algorithm, which canachieve accurate pose estimation. This criterion also makes the proposedalgorithm especially suitable for obstacle avoidance. The proposed algorithm isvalidated on the KITTI dataset and a manually labeled dataset acquired at anindustrial park. The results show that our proposed method can achieve betteraccuracy than the state-of-the-art pose estimation method while maintainingreal-time speed.</description><author>Ningning Ding</author><pubDate>Sun, 02 Jul 2023 00:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01034v2</guid></item><item><title>Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF)</title><link>http://arxiv.org/abs/2308.00214v2</link><description>Many tasks performed in image-guided, mini-invasive, medical procedures canbe cast as pose estimation problems, where an X-ray projection is utilized toreach a target in 3D space. Expanding on recent advances in the differentiablerendering of optically reflective materials, we introduce new methods for poseestimation of radiolucent objects using X-ray projections, and we demonstratethe critical role of optimal view synthesis in performing this task. We firstdevelop an algorithm (DiffDRR) that efficiently computes DigitallyReconstructed Radiographs (DRRs) and leverages automatic differentiation withinTensorFlow. Pose estimation is performed by iterative gradient descent using aloss function that quantifies the similarity of the DRR synthesized from arandomly initialized pose and the true fluoroscopic image at the target pose.We propose two novel methods for high-fidelity view synthesis, Neural TunedTomography (NeTT) and masked Neural Radiance Fields (mNeRF). Both methods relyon classic Cone-Beam Computerized Tomography (CBCT); NeTT directly optimizesthe CBCT densities, while the non-zero values of mNeRF are constrained by a 3Dmask of the anatomic region segmented from CBCT. We demonstrate that both NeTTand mNeRF distinctly improve pose estimation within our framework. By defininga successful pose estimate to be a 3D angle error of less than 3 deg, we findthat NeTT and mNeRF can achieve similar results, both with overall successrates more than 93%. However, the computational cost of NeTT is significantlylower than mNeRF in both training and pose estimation. Furthermore, we showthat a NeTT trained for a single subject can generalize to synthesizehigh-fidelity DRRs and ensure robust pose estimations for all other subjects.Therefore, we suggest that NeTT is an attractive option for robust poseestimation using fluoroscopic projections.</description><author>Chaochao Zhou, Syed Hasib Akhter Faruqui, Abhinav Patel, Ramez N. Abdalla, Michael C. Hurley, Ali Shaibani, Matthew B. Potts, Babak S. Jahromi, Leon Cho, Sameer A. Ansari, Donald R. Cantrell</author><pubDate>Fri, 18 Aug 2023 05:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00214v2</guid></item><item><title>Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF)</title><link>http://arxiv.org/abs/2308.00214v1</link><description>Many tasks performed in image-guided, mini-invasive, medical procedures canbe cast as pose estimation problems, where an X-ray projection is utilized toreach a target in 3D space. Recent advances in the differentiable rendering ofoptically reflective materials have enabled state-of-the-art performance in RGBcamera view synthesis and pose estimation. Expanding on these prior works, weintroduce new methods for pose estimation of radiolucent objects using X-rayprojections, and we demonstrate the critical role of optimal view synthesis inperforming this task. We first develop an algorithm (DiffDRR) that efficientlycomputes Digitally Reconstructed Radiographs (DRRs) and leverages automaticdifferentiation within TensorFlow. In conjunction with classic CBCTreconstruction algorithms, we perform pose estimation by gradient descent usinga loss function that quantifies the similarity of the DRR synthesized from arandomly initialized pose and the true fluoroscopic image at the target pose.We propose two novel methods for high-fidelity view synthesis, Neural TunedTomography (NeTT) and masked Neural Radiance Fields (mNeRF). Both methods relyon classic CBCT; NeTT directly optimizes the CBCT densities, while the non-zerovalues of mNeRF are constrained by a 3D mask of the anatomic region segmentedfrom CBCT. We demonstrate that both NeTT and mNeRF distinctly improve poseestimation within our framework. By defining a successful pose estimate to be a3D angle error of less than 3 deg, we find that NeTT and mNeRF can achievesimilar results, both with overall success rates more than 93%. Furthermore, weshow that a NeTT trained for a single subject can generalize to synthesizehigh-fidelity DRRs and ensure robust pose estimations for all other subjects.Therefore, we suggest that NeTT is an attractive option for robust poseestimation using fluoroscopic projections.</description><author>Chaochao Zhou, Syed Hasib Akhter Faruqui, Abhinav Patel, Ramez N. Abdalla, Michael C. Hurley, Ali Shaibani, Matthew B. Potts, Babak S. Jahromi, Leon Cho, Sameer A. Ansari, Donald R. Cantrell</author><pubDate>Tue, 01 Aug 2023 02:12:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00214v1</guid></item><item><title>Pose-Free Neural Radiance Fields via Implicit Pose Regularization</title><link>http://arxiv.org/abs/2308.15049v1</link><description>Pose-free neural radiance fields (NeRF) aim to train NeRF with unposedmulti-view images and it has achieved very impressive success in recent years.Most existing works share the pipeline of training a coarse pose estimator withrendered images at first, followed by a joint optimization of estimated posesand neural radiance field. However, as the pose estimator is trained with onlyrendered images, the pose estimation is usually biased or inaccurate for realimages due to the domain gap between real images and rendered images, leadingto poor robustness for the pose estimation of real images and further localminima in joint optimization. We design IR-NeRF, an innovative pose-free NeRFthat introduces implicit pose regularization to refine pose estimator withunposed real images and improve the robustness of the pose estimation for realimages. With a collection of 2D images of a specific scene, IR-NeRF constructsa scene codebook that stores scene features and captures the scene-specificpose distribution implicitly as priors. Thus, the robustness of pose estimationcan be promoted with the scene priors according to the rationale that a 2D realimage can be well reconstructed from the scene codebook only when its estimatedpose lies within the pose distribution. Extensive experiments show that IR-NeRFachieves superior novel view synthesis and outperforms the state-of-the-artconsistently across multiple synthetic and real datasets.</description><author>Jiahui Zhang, Fangneng Zhan, Yingchen Yu, Kunhao Liu, Rongliang Wu, Xiaoqin Zhang, Ling Shao, Shijian Lu</author><pubDate>Tue, 29 Aug 2023 07:14:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15049v1</guid></item><item><title>TransNet: Transparent Object Manipulation Through Category-Level Pose Estimation</title><link>http://arxiv.org/abs/2307.12400v1</link><description>Transparent objects present multiple distinct challenges to visual perceptionsystems. First, their lack of distinguishing visual features makes transparentobjects harder to detect and localize than opaque objects. Even humans findcertain transparent surfaces with little specular reflection or refraction,like glass doors, difficult to perceive. A second challenge is that depthsensors typically used for opaque object perception cannot obtain accuratedepth measurements on transparent surfaces due to their unique reflectiveproperties. Stemming from these challenges, we observe that transparent objectinstances within the same category, such as cups, look more similar to eachother than to ordinary opaque objects of that same category. Given thisobservation, the present paper explores the possibility of category-leveltransparent object pose estimation rather than instance-level pose estimation.We propose \textit{\textbf{TransNet}}, a two-stage pipeline that estimatescategory-level transparent object pose using localized depth completion andsurface normal estimation. TransNet is evaluated in terms of pose estimationaccuracy on a large-scale transparent object dataset and compared to astate-of-the-art category-level pose estimation approach. Results from thiscomparison demonstrate that TransNet achieves improved pose estimation accuracyon transparent objects. Moreover, we use TransNet to build an autonomoustransparent object manipulation system for robotic pick-and-place and pouringtasks.</description><author>Huijie Zhang, Anthony Opipari, Xiaotong Chen, Jiyue Zhu, Zeren Yu, Odest Chadwicke Jenkins</author><pubDate>Sun, 23 Jul 2023 19:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12400v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video</title><link>http://arxiv.org/abs/2307.05853v2</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 14% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively). GitHub:https://github.com/bruceyo/GLA-GCN.</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Sat, 22 Jul 2023 02:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v2</guid></item><item><title>Open Challenges for Monocular Single-shot 6D Object Pose Estimation</title><link>http://arxiv.org/abs/2302.11827v2</link><description>Object pose estimation is a non-trivial task that enables roboticmanipulation, bin picking, augmented reality, and scene understanding, to namea few use cases. Monocular object pose estimation gained considerable momentumwith the rise of high-performing deep learning-based solutions and isparticularly interesting for the community since sensors are inexpensive andinference is fast. Prior works establish the comprehensive state of the art fordiverse pose estimation problems. Their broad scopes make it difficult toidentify promising future directions. We narrow down the scope to the problemof single-shot monocular 6D object pose estimation, which is commonly used inrobotics, and thus are able to identify such trends. By reviewing recentpublications in robotics and computer vision, the state of the art isestablished at the union of both fields. Following that, we identify promisingresearch directions in order to help researchers to formulate relevant researchideas and effectively advance the state of the art. Findings include thatmethods are sophisticated enough to overcome the domain shift and thatocclusion handling is a fundamental challenge. We also highlight problems suchas novel object pose estimation and challenging materials handling as centralchallenges to advance robotics.</description><author>Stefan Thalhammer, Peter Hönig, Jean-Baptiste Weibel, Markus Vincze</author><pubDate>Thu, 20 Jul 2023 20:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11827v2</guid></item><item><title>Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training</title><link>http://arxiv.org/abs/2206.11215v4</link><description>We consider a certifiable object pose estimation problem, where -- given apartial point cloud of an object -- the goal is to not only estimate the objectpose, but also to provide a certificate of correctness for the resultingestimate. Our first contribution is a general theory of certification forend-to-end perception models. In particular, we introduce the notion of$\zeta$-correctness, which bounds the distance between an estimate and theground truth. We show that $\zeta$-correctness can be assessed by implementingtwo certificates: (i) a certificate of observable correctness, that asserts ifthe model output is consistent with the input data and prior information, (ii)a certificate of non-degeneracy, that asserts whether the input data issufficient to compute a unique estimate. Our second contribution is to applythis theory and design a new learning-based certifiable pose estimator. Wepropose C-3PO, a semantic-keypoint-based pose estimation model, augmented withthe two certificates, to solve the certifiable pose estimation problem. C-3POalso includes a keypoint corrector, implemented as a differentiableoptimization layer, that can correct large detection errors (e.g. due to thesim-to-real gap). Our third contribution is a novel self-supervised trainingapproach that uses our certificate of observable correctness to provide thesupervisory signal to C-3PO during training. In it, the model trains only onthe observably correct input-output pairs, in each training iteration. Astraining progresses, we see that the observably correct input-output pairsgrow, eventually reaching near 100% in many cases. Our experiments show that(i) standard semantic-keypoint-based methods outperform more recentalternatives, (ii) C-3PO further improves performance and significantlyoutperforms all the baselines, and (iii) C-3PO's certificates are able todiscern correct pose estimates.</description><author>Rajat Talak, Lisa Peng, Luca Carlone</author><pubDate>Fri, 28 Apr 2023 20:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11215v4</guid></item><item><title>A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training</title><link>http://arxiv.org/abs/2302.06019v2</link><description>Real-world robotics applications demand object pose estimation methods thatwork reliably across a variety of scenarios. Modern learning-based approachesrequire large labeled datasets and tend to perform poorly outside the trainingdomain. Our first contribution is to develop a robust corrector module thatcorrects pose estimates using depth information, thus enabling existing methodsto better generalize to new test domains; the corrector operates on semantickeypoints (but is also applicable to other pose estimators) and is fullydifferentiable. Our second contribution is an ensemble self-training approachthat simultaneously trains multiple pose estimators in a self-supervisedmanner. Our ensemble self-training architecture uses the robust corrector torefine the output of each pose estimator; then, it evaluates the quality of theoutputs using observable correctness certificates; finally, it uses theobservably correct outputs for further training, without requiring externalsupervision. As an additional contribution, we propose small improvements to aregression-based keypoint detection architecture, to enhance its robustness tooutliers; these improvements include a robust pooling scheme and a robustcentroid computation. Experiments on the YCBV and TLESS datasets show theproposed ensemble self-training outperforms fully supervised baselines whilenot requiring 3D annotations on real data.</description><author>Jingnan Shi, Rajat Talak, Dominic Maggio, Luca Carlone</author><pubDate>Thu, 11 May 2023 19:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06019v2</guid></item><item><title>Ego-Body Pose Estimation via Ego-Head Pose Estimation</title><link>http://arxiv.org/abs/2212.04636v3</link><description>Estimating 3D human motion from an egocentric video sequence plays a criticalrole in human behavior understanding and has various applications in VR/AR.However, naively learning a mapping between egocentric videos and human motionsis challenging, because the user's body is often unobserved by the front-facingcamera placed on the head of the user. In addition, collecting large-scale,high-quality datasets with paired egocentric videos and 3D human motionsrequires accurate motion capture devices, which often limit the variety ofscenes in the videos to lab-like environments. To eliminate the need for pairedegocentric video and human motions, we propose a new method, Ego-Body PoseEstimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the probleminto two stages, connected by the head motion as an intermediaterepresentation. EgoEgo first integrates SLAM and a learning approach toestimate accurate head motion. Subsequently, leveraging the estimated head poseas input, EgoEgo utilizes conditional diffusion to generate multiple plausiblefull-body motions. This disentanglement of head and body pose eliminates theneed for training datasets with paired egocentric videos and 3D human motion,enabling us to leverage large-scale egocentric video datasets and motioncapture datasets separately. Moreover, for systematic benchmarking, we developa synthetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentricvideos and human motion. On both ARES and real data, our EgoEgo model performssignificantly better than the current state-of-the-art methods.</description><author>Jiaman Li, C. Karen Liu, Jiajun Wu</author><pubDate>Mon, 28 Aug 2023 03:51:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.04636v3</guid></item><item><title>YOLOPose V2: Understanding and Improving Transformer-based 6D Pose Estimation</title><link>http://arxiv.org/abs/2307.11550v1</link><description>6D object pose estimation is a crucial prerequisite for autonomous robotmanipulation applications. The state-of-the-art models for pose estimation areconvolutional neural network (CNN)-based. Lately, Transformers, an architectureoriginally proposed for natural language processing, is achievingstate-of-the-art results in many computer vision tasks as well. Equipped withthe multi-head self-attention mechanism, Transformers enable simplesingle-stage end-to-end architectures for learning object detection and 6Dobject pose estimation jointly. In this work, we propose YOLOPose (short formfor You Only Look Once Pose estimation), a Transformer-based multi-object 6Dpose estimation method based on keypoint regression and an improved variant ofthe YOLOPose model. In contrast to the standard heatmaps for predictingkeypoints in an image, we directly regress the keypoints. Additionally, weemploy a learnable orientation estimation module to predict the orientationfrom the keypoints. Along with a separate translation estimation module, ourmodel is end-to-end differentiable. Our method is suitable for real-timeapplications and achieves results comparable to state-of-the-art methods. Weanalyze the role of object queries in our architecture and reveal that theobject queries specialize in detecting objects in specific image regions.Furthermore, we quantify the accuracy trade-off of using datasets of smallersizes to train our model.</description><author>Arul Selvam Periyasamy, Arash Amini, Vladimir Tsaturyan, Sven Behnke</author><pubDate>Fri, 21 Jul 2023 13:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11550v1</guid></item><item><title>Denoising Diffusion for 3D Hand Pose Estimation from Images</title><link>http://arxiv.org/abs/2308.09523v1</link><description>Hand pose estimation from a single image has many applications. However,approaches to full 3D body pose estimation are typically trained on day-to-dayactivities or actions. As such, detailed hand-to-hand interactions are poorlyrepresented, especially during motion. We see this in the failure cases oftechniques such as OpenPose or MediaPipe. However, accurate hand poseestimation is crucial for many applications where the global body motion isless important than accurate hand pose estimation. This paper addresses the problem of 3D hand pose estimation from monocularimages or sequences. We present a novel end-to-end framework for 3D handregression that employs diffusion models that have shown excellent ability tocapture the distribution of data for generative purposes. Moreover, we enforcekinematic constraints to ensure realistic poses are generated by incorporatingan explicit forward kinematic layer as part of the network. The proposed modelprovides state-of-the-art performance when lifting a 2D single-hand image to3D. However, when sequence data is available, we add a Transformer module overa temporal window of consecutive frames to refine the results, overcomingjittering and further increasing accuracy. The method is quantitatively and qualitatively evaluated showingstate-of-the-art robustness, generalization, and accuracy on several differentdatasets.</description><author>Maksym Ivashechkin, Oscar Mendez, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 13:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09523v1</guid></item><item><title>Affine Correspondences between Multi-Camera Systems for Relative Pose Estimation</title><link>http://arxiv.org/abs/2306.12996v1</link><description>We present a novel method to compute the relative pose of multi-camerasystems using two affine correspondences (ACs). Existing solutions to themulti-camera relative pose estimation are either restricted to special cases ofmotion, have too high computational complexity, or require too many pointcorrespondences (PCs). Thus, these solvers impede an efficient or accuraterelative pose estimation when applying RANSAC as a robust estimator. This papershows that the 6DOF relative pose estimation problem using ACs permits afeasible minimal solution, when exploiting the geometric constraints betweenACs and multi-camera systems using a special parameterization. We present aproblem formulation based on two ACs that encompass two common types of ACsacross two views, i.e., inter-camera and intra-camera. Moreover, the frameworkfor generating the minimal solvers can be extended to solve various relativepose estimation problems, e.g., 5DOF relative pose estimation with knownrotation angle prior. Experiments on both virtual and real multi-camera systemsprove that the proposed solvers are more efficient than the state-of-the-artalgorithms, while resulting in a better relative pose accuracy. Source code isavailable at https://github.com/jizhaox/relpose-mcs-depth.</description><author>Banglei Guan, Ji Zhao</author><pubDate>Thu, 22 Jun 2023 16:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12996v1</guid></item><item><title>GoalieNet: A Multi-Stage Network for Joint Goalie, Equipment, and Net Pose Estimation in Ice Hockey</title><link>http://arxiv.org/abs/2306.15853v1</link><description>In the field of computer vision-driven ice hockey analytics, one of the mostchallenging and least studied tasks is goalie pose estimation. Unlike generalhuman pose estimation, goalie pose estimation is much more complex as itinvolves not only the detection of keypoints corresponding to the joints of thegoalie concealed under thick padding and mask, but also a large number ofnon-human keypoints corresponding to the large leg pads and gloves worn, thestick, as well as the hockey net. To tackle this challenge, we introduceGoalieNet, a multi-stage deep neural network for jointly estimating the pose ofthe goalie, their equipment, and the net. Experimental results using NHLbenchmark data demonstrate that the proposed GoalieNet can achieve an averageof 84\% accuracy across all keypoints, where 22 out of 29 keypoints aredetected with more than 80\% accuracy. This indicates that such a joint poseestimation approach can be a promising research direction.</description><author>Marjan Shahi, David Clausi, Alexander Wong</author><pubDate>Wed, 28 Jun 2023 02:00:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15853v1</guid></item><item><title>Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video</title><link>http://arxiv.org/abs/2303.08475v2</link><description>Temporal modeling is crucial for multi-frame human pose estimation. Mostexisting methods directly employ optical flow or deformable convolution topredict full-spectrum motion fields, which might incur numerous irrelevantcues, such as a nearby person or background. Without further efforts toexcavate meaningful motion priors, their results are suboptimal, especially incomplicated spatiotemporal interactions. On the other hand, the temporaldifference has the ability to encode representative motion information whichcan potentially be valuable for pose estimation but has not been fullyexploited. In this paper, we present a novel multi-frame human pose estimationframework, which employs temporal differences across frames to model dynamiccontexts and engages mutual information objectively to facilitate useful motioninformation disentanglement. To be specific, we design a multi-stage TemporalDifference Encoder that performs incremental cascaded learning conditioned onmulti-stage feature difference sequences to derive informative motionrepresentation. We further propose a Representation Disentanglement module fromthe mutual information perspective, which can grasp discriminativetask-relevant motion signals by explicitly defining useful and noisyconstituents of the raw motion features and minimizing their mutualinformation. These place us to rank No.1 in the Crowd Pose Estimation inComplex Events Challenge on benchmark dataset HiEve, and achievestate-of-the-art performance on three benchmarks PoseTrack2017, PoseTrack2018,and PoseTrack21.</description><author>Runyang Feng, Yixing Gao, Xueqing Ma, Tze Ho Elden Tse, Hyung Jin Chang</author><pubDate>Mon, 08 May 2023 14:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08475v2</guid></item><item><title>HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning</title><link>http://arxiv.org/abs/2302.00988v2</link><description>Recent advancements in 3D hand pose estimation have shown promising results,but its effectiveness has primarily relied on the availability of large-scaleannotated datasets, the creation of which is a laborious and costly process. Toalleviate the label-hungry limitation, we propose a self-supervised learningframework, HaMuCo, that learns a single-view hand pose estimator frommulti-view pseudo 2D labels. However, one of the main challenges ofself-supervised learning is the presence of noisy labels and the ``groupthink''effect from multiple views. To overcome these issues, we introduce a cross-viewinteraction network that distills the single-view estimator by utilizing thecross-view correlated features and enforcing multi-view consistency to achievecollaborative learning. Both the single-view estimator and the cross-viewinteraction network are trained jointly in an end-to-end manner. Extensiveexperiments show that our method can achieve state-of-the-art performance onmulti-view self-supervised hand pose estimation. Furthermore, the proposedcross-view interaction network can also be applied to hand pose estimation frommulti-view input and outperforms previous methods under the same settings.</description><author>Xiaozheng Zheng, Chao Wen, Zhou Xue, Pengfei Ren, Jingyu Wang</author><pubDate>Tue, 15 Aug 2023 05:51:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00988v2</guid></item><item><title>Perspective-1-Ellipsoid: Formulation, Analysis and Solutions of the Camera Pose Estimation Problem from One Ellipse-Ellipsoid Correspondence</title><link>http://arxiv.org/abs/2208.12513v3</link><description>In computer vision, camera pose estimation from correspondences between 3Dgeometric entities and their projections into the image has been a widelyinvestigated problem. Although most state-of-the-art methods exploit low-levelprimitives such as points or lines, the emergence of very effective CNN-basedobject detectors in the recent years has paved the way to the use ofhigher-level features carrying semantically meaningful information. Pioneeringworks in that direction have shown that modelling 3D objects by ellipsoids and2D detections by ellipses offers a convenient manner to link 2D and 3D data.However, the mathematical formalism most often used in the related litteraturedoes not enable to easily distinguish ellipsoids and ellipses from otherquadrics and conics, leading to a loss of specificity potentially detrimentalin some developments. Moreover, the linearization process of the projectionequation creates an over-representation of the camera parameters, also possiblycausing an efficiency loss. In this paper, we therefore introduce anellipsoid-specific theoretical framework and demonstrate its beneficialproperties in the context of pose estimation. More precisely, we first showthat the proposed formalism enables to reduce the pose estimation problem to aposition or orientation-only estimation problem in which the remaining unknownscan be derived in closed-form. Then, we demonstrate that it can be furtherreduced to a 1 Degree-of-Freedom (1DoF) problem and provide the analyticalderivations of the pose as a function of that unique scalar unknown. Weillustrate our theoretical considerations by visual examples and include adiscussion on the practical aspects. Finally, we release this paper along withthe corresponding source code in order to contribute towards more efficientresolutions of ellipsoid-related pose estimation problems.</description><author>Vincent Gaudillière, Gilles Simon, Marie-Odile Berger</author><pubDate>Wed, 14 Jun 2023 13:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12513v3</guid></item><item><title>DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16687v2</link><description>Denoising diffusion probabilistic models that were initially proposed forrealistic image generation have recently shown success in various perceptiontasks (e.g., object detection and image segmentation) and are increasinglygaining attention in computer vision. However, extending such models tomulti-frame human pose estimation is non-trivial due to the presence of theadditional temporal dimension in videos. More importantly, learningrepresentations that focus on keypoint regions is crucial for accuratelocalization of human joints. Nevertheless, the adaptation of thediffusion-based methods remains unclear on how to achieve such objective. Inthis paper, we present DiffPose, a novel diffusion architecture that formulatesvideo-based human pose estimation as a conditional heatmap generation problem.First, to better leverage temporal information, we propose SpatioTemporalRepresentation Learner which aggregates visual evidences across frames and usesthe resulting features in each denoising step as a condition. In addition, wepresent a mechanism called Lookup-based MultiScale Feature Interaction thatdetermines the correlations between local joints and global contexts acrossmultiple scales. This mechanism generates delicate representations that focuson keypoint regions. Altogether, by extending diffusion models, we show twounique characteristics from DiffPose on pose estimation task: (i) the abilityto combine multiple sets of pose estimates to improve prediction accuracy,particularly for challenging joints, and (ii) the ability to adjust the numberof iterative steps for feature refinement without retraining the model.DiffPose sets new state-of-the-art results on three benchmarks: PoseTrack2017,PoseTrack2018, and PoseTrack21.</description><author>Runyang Feng, Yixing Gao, Tze Ho Elden Tse, Xueqing Ma, Hyung Jin Chang</author><pubDate>Sat, 05 Aug 2023 11:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16687v2</guid></item><item><title>KVN: Keypoints Voting Network with Differentiable RANSAC for Stereo Pose Estimation</title><link>http://arxiv.org/abs/2307.11543v1</link><description>Object pose estimation is a fundamental computer vision task exploited inseveral robotics and augmented reality applications. Many establishedapproaches rely on predicting 2D-3D keypoint correspondences using RANSAC(Random sample consensus) and estimating the object pose using the PnP(Perspective-n-Point) algorithm. Being RANSAC non-differentiable,correspondences cannot be directly learned in an end-to-end fashion. In thispaper, we address the stereo image-based object pose estimation problem by (i)introducing a differentiable RANSAC layer into a well-known monocular poseestimation network; (ii) exploiting an uncertainty-driven multi-view PnP solverwhich can fuse information from multiple views. We evaluate our approach on achallenging public stereo object pose estimation dataset, yieldingstate-of-the-art results against other recent approaches. Furthermore, in ourablation study, we show that the differentiable RANSAC layer plays asignificant role in the accuracy of the proposed method. We release with thispaper the open-source implementation of our method.</description><author>Ivano Donadi, Alberto Pretto</author><pubDate>Fri, 21 Jul 2023 13:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11543v1</guid></item><item><title>GenPose: Generative Category-level Object Pose Estimation via Diffusion Models</title><link>http://arxiv.org/abs/2306.10531v2</link><description>Object pose estimation plays a vital role in embodied AI and computer vision,enabling intelligent agents to comprehend and interact with their surroundings.Despite the practicality of category-level pose estimation, current approachesencounter challenges with partially observed point clouds, known as themultihypothesis issue. In this study, we propose a novel solution by reframingcategorylevel object pose estimation as conditional generative modeling,departing from traditional point-to-point regression. Leveraging score-baseddiffusion models, we estimate object poses by sampling candidates from thediffusion model and aggregating them through a two-step process: filtering outoutliers via likelihood estimation and subsequently mean-pooling the remainingcandidates. To avoid the costly integration process when estimating thelikelihood, we introduce an alternative method that trains an energy-basedmodel from the original score-based model, enabling end-to-end likelihoodestimation. Our approach achieves state-of-the-art performance on the REAL275dataset, surpassing 50% and 60% on strict 5d2cm and 5d5cm metrics,respectively. Furthermore, our method demonstrates strong generalizability tonovel categories sharing similar symmetric properties without fine-tuning andcan readily adapt to object pose tracking tasks, yielding comparable results tothe current state-of-the-art baselines.</description><author>Jiyao Zhang, Mingdong Wu, Hao Dong</author><pubDate>Sat, 22 Jul 2023 17:16:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10531v2</guid></item><item><title>SPAC-Net: Synthetic Pose-aware Animal ControlNet for Enhanced Pose Estimation</title><link>http://arxiv.org/abs/2305.17845v1</link><description>Animal pose estimation has become a crucial area of research, but thescarcity of annotated data is a significant challenge in developing accuratemodels. Synthetic data has emerged as a promising alternative, but itfrequently exhibits domain discrepancies with real data. Style transferalgorithms have been proposed to address this issue, but they suffer frominsufficient spatial correspondence, leading to the loss of label information.In this work, we present a new approach called Synthetic Pose-aware AnimalControlNet (SPAC-Net), which incorporates ControlNet into the previouslyproposed Prior-Aware Synthetic animal data generation (PASyn) pipeline. Weleverage the plausible pose data generated by the Variational Auto-Encoder(VAE)-based data generation pipeline as input for the ControlNetHolistically-nested Edge Detection (HED) boundary task model to generatesynthetic data with pose labels that are closer to real data, making itpossible to train a high-precision pose estimation network without the need forreal data. In addition, we propose the Bi-ControlNet structure to separatelydetect the HED boundary of animals and backgrounds, improving the precision andstability of the generated data. Using the SPAC-Net pipeline, we generatesynthetic zebra and rhino images and test them on the AP10K real dataset,demonstrating superior performance compared to using only real images orsynthetic data generated by other methods. Our work demonstrates the potentialfor synthetic data to overcome the challenge of limited annotated data inanimal pose estimation.</description><author>Le Jiang, Sarah Ostadabbas</author><pubDate>Mon, 29 May 2023 02:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17845v1</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v2</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Mon, 15 May 2023 16:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v2</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v1</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Fri, 12 May 2023 10:52:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v1</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v3</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Wed, 17 May 2023 09:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v3</guid></item><item><title>CHSEL: Producing Diverse Plausible Pose Estimates from Contact and Free Space Data</title><link>http://arxiv.org/abs/2305.08042v1</link><description>This paper proposes a novel method for estimating the set of plausible posesof a rigid object from a set of points with volumetric information, such aswhether each point is in free space or on the surface of the object. Inparticular, we study how pose can be estimated from force and tactile dataarising from contact. Using data derived from contact is challenging because itis inherently less information-dense than visual data, and thus the poseestimation problem is severely under-constrained when there are few contacts.Rather than attempting to estimate the true pose of the object, which is nottractable without a large number of contacts, we seek to estimate a plausibleset of poses which obey the constraints imposed by the sensor data. Existingmethods struggle to estimate this set because they are either designed forsingle pose estimates or require informative priors to be effective. Ourapproach to this problem, Constrained pose Hypothesis Set Elimination (CHSEL),has three key attributes: 1) It considers volumetric information, which allowsus to account for known free space; 2) It uses a novel differentiablevolumetric cost function to take advantage of powerful gradient-basedoptimization tools; and 3) It uses methods from the Quality Diversity (QD)optimization literature to produce a diverse set of high-quality poses. To ourknowledge, QD methods have not been used previously for pose registration. Wealso show how to update our plausible pose estimates online as more data isgathered by the robot. Our experiments suggest that CHSEL shows largeperformance improvements over several baseline methods for both simulated andreal-world data.</description><author>Sheng Zhong, Nima Fazeli, Dmitry Berenson</author><pubDate>Sun, 14 May 2023 02:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08042v1</guid></item><item><title>EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes</title><link>http://arxiv.org/abs/2308.06493v1</link><description>Full-body ego-pose estimation from head and hand poses alone has become anactive area of research to power articulate avatar representation onheadset-based platforms. However, existing methods over-rely on the confines ofthe motion-capture spaces in which datasets were recorded, while simultaneouslyassuming continuous capture of joint motions and uniform body dimensions. Inthis paper, we propose EgoPoser, which overcomes these limitations by 1)rethinking the input representation for headset-based ego-pose estimation andintroducing a novel motion decomposition method that predicts full-body poseindependent of global positions, 2) robustly modeling body pose fromintermittent hand position and orientation tracking only when inside aheadset's field of view, and 3) generalizing across various body sizes fordifferent users. Our experiments show that EgoPoser outperformsstate-of-the-art methods both qualitatively and quantitatively, whilemaintaining a high inference speed of over 600 fps. EgoPoser establishes arobust baseline for future work, where full-body pose estimation needs nolonger rely on outside-in capture and can scale to large-scene environments.</description><author>Jiaxi Jiang, Paul Streli, Manuel Meier, Andreas Fender, Christian Holz</author><pubDate>Sat, 12 Aug 2023 08:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06493v1</guid></item><item><title>EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes</title><link>http://arxiv.org/abs/2308.06493v2</link><description>Full-body ego-pose estimation from head and hand poses alone has become anactive area of research to power articulate avatar representation onheadset-based platforms. However, existing methods over-rely on the confines ofthe motion-capture spaces in which datasets were recorded, while simultaneouslyassuming continuous capture of joint motions and uniform body dimensions. Inthis paper, we propose EgoPoser, which overcomes these limitations by 1)rethinking the input representation for headset-based ego-pose estimation andintroducing a novel motion decomposition method that predicts full-body poseindependent of global positions, 2) robustly modeling body pose fromintermittent hand position and orientation tracking only when inside aheadset's field of view, and 3) generalizing across various body sizes fordifferent users. Our experiments show that EgoPoser outperformsstate-of-the-art methods both qualitatively and quantitatively, whilemaintaining a high inference speed of over 600 fps. EgoPoser establishes arobust baseline for future work, where full-body pose estimation needs nolonger rely on outside-in capture and can scale to large-scene environments.</description><author>Jiaxi Jiang, Paul Streli, Manuel Meier, Christian Holz</author><pubDate>Thu, 17 Aug 2023 15:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06493v2</guid></item><item><title>Neural Voting Field for Camera-Space 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2305.04328v1</link><description>We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.</description><author>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</author><pubDate>Sun, 07 May 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04328v1</guid></item><item><title>Relation-Based Associative Joint Location for Human Pose Estimation in Videos</title><link>http://arxiv.org/abs/2107.03591v3</link><description>Video-based human pose estimation (VHPE) is a vital yet challenging task.While deep learning methods have made significant progress for the VHPE, mostapproaches to this task implicitly model the long-range interaction betweenjoints by enlarging the receptive field of the convolution. Unlike priormethods, we design a lightweight and plug-and-play joint relation extractor(JRE) to model the associative relationship between joints explicitly andautomatically. The JRE takes the pseudo heatmaps of joints as input andcalculates the similarity between pseudo heatmaps. In this way, the JREflexibly learns the relationship between any two joints, allowing it to learnthe rich spatial configuration of human poses. Moreover, the JRE can inferinvisible joints according to the relationship between joints, which isbeneficial for the model to locate occluded joints. Then, combined withtemporal semantic continuity modeling, we propose a Relation-based PoseSemantics Transfer Network (RPSTN) for video-based human pose estimation.Specifically, to capture the temporal dynamics of poses, the pose semanticinformation of the current frame is transferred to the next with a jointrelation guided pose semantics propagator (JRPSP). The proposed model cantransfer the pose semantic features from the non-occluded frame to the occludedframe, making our method robust to the occlusion. Furthermore, the proposed JREmodule is also suitable for image-based human pose estimation. The proposedRPSTN achieves state-of-the-art results on the video-based Penn Action dataset,Sub-JHMDB dataset, and PoseTrack2018 dataset. Moreover, the proposed JREimproves the performance of backbones on the image-based COCO2017 dataset. Codeis available at https://github.com/YHDang/pose-estimation.</description><author>Yonghao Dang, Jianqin Yin, Shaojie Zhang</author><pubDate>Fri, 30 Jun 2023 10:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.03591v3</guid></item><item><title>POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference</title><link>http://arxiv.org/abs/2305.15727v1</link><description>Despite the significant progress in six degrees-of-freedom (6DoF) object poseestimation, existing methods have limited applicability in real-world scenariosinvolving embodied agents and downstream 3D vision tasks. These limitationsmainly come from the necessity of 3D models, closed-category detection, and alarge number of densely annotated support views. To mitigate this issue, wepropose a general paradigm for object pose estimation, called Promptable ObjectPose Estimation (POPE). The proposed approach POPE enables zero-shot 6DoFobject pose estimation for any target object in any scene, while only a singlereference is adopted as the support view. To achieve this, POPE leverages thepower of the pre-trained large-scale 2D foundation model, employs a frameworkwith hierarchical feature representation and 3D geometry principles. Moreover,it estimates the relative camera pose between object prompts and the targetobject in new views, enabling both two-view and multi-view 6DoF pose estimationtasks. Comprehensive experimental results demonstrate that POPE exhibitsunrivaled robust performance in zero-shot settings, by achieving a significantreduction in the averaged Median Pose Error by 52.38% and 50.47% on the LINEMODand OnePose datasets, respectively. We also conduct more challenging testingsin causally captured images (see Figure 1), which further demonstrates therobustness of POPE. Project page can be found withhttps://paulpanwang.github.io/POPE/.</description><author>Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Dejia Xu, Hanwen Jiang, Zhangyang Wang</author><pubDate>Thu, 25 May 2023 06:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15727v1</guid></item><item><title>Generalizable Pose Estimation Using Implicit Scene Representations</title><link>http://arxiv.org/abs/2305.17252v1</link><description>6-DoF pose estimation is an essential component of robotic manipulationpipelines. However, it usually suffers from a lack of generalization to newinstances and object types. Most widely used methods learn to infer the objectpose in a discriminative setup where the model filters useful information toinfer the exact pose of the object. While such methods offer accurate poses,the model does not store enough information to generalize to new objects. Inthis work, we address the generalization capability of pose estimation usingmodels that contain enough information about the object to render it indifferent poses. We follow the line of work that inverts neural renderers toinfer the pose. We propose i-$\sigma$SRN to maximize the information flowingfrom the input pose to the rendered scene and invert them to infer the posegiven an input image. Specifically, we extend Scene Representation Networks(SRNs) by incorporating a separate network for density estimation and introducea new way of obtaining a weighted scene representation. We investigate severalways of initial pose estimates and losses for the neural renderer. Our finalevaluation shows a significant improvement in inference performance and speedcompared to existing approaches.</description><author>Vaibhav Saxena, Kamal Rahimi Malekshan, Linh Tran, Yotto Koga</author><pubDate>Fri, 26 May 2023 21:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17252v1</guid></item><item><title>Challenges for Monocular 6D Object Pose Estimation in Robotics</title><link>http://arxiv.org/abs/2307.12172v1</link><description>Object pose estimation is a core perception task that enables, for example,object grasping and scene understanding. The widely available, inexpensive andhigh-resolution RGB sensors and CNNs that allow for fast inference based onthis modality make monocular approaches especially well suited for roboticsapplications. We observe that previous surveys on object pose estimationestablish the state of the art for varying modalities, single- and multi-viewsettings, and datasets and metrics that consider a multitude of applications.We argue, however, that those works' broad scope hinders the identification ofopen challenges that are specific to monocular approaches and the derivation ofpromising future challenges for their application in robotics. By providing aunified view on recent publications from both robotics and computer vision, wefind that occlusion handling, novel pose representations, and formalizing andimproving category-level pose estimation are still fundamental challenges thatare highly relevant for robotics. Moreover, to further improve roboticperformance, large object sets, novel objects, refractive materials, anduncertainty estimates are central, largely unsolved open challenges. In orderto address them, ontological reasoning, deformability handling, scene-levelreasoning, realistic datasets, and the ecological footprint of algorithms needto be improved.</description><author>Stefan Thalhammer, Dominik Bauer, Peter Hönig, Jean-Baptiste Weibel, José García-Rodríguez, Markus Vincze</author><pubDate>Sat, 22 Jul 2023 22:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12172v1</guid></item><item><title>Depth-based 6DoF Object Pose Estimation using Swin Transformer</title><link>http://arxiv.org/abs/2303.02133v2</link><description>Accurately estimating the 6D pose of objects is crucial for manyapplications, such as robotic grasping, autonomous driving, and augmentedreality. However, this task becomes more challenging in poor lightingconditions or when dealing with textureless objects. To address this issue,depth images are becoming an increasingly popular choice due to theirinvariance to a scene's appearance and the implicit incorporation of essentialgeometric characteristics. However, fully leveraging depth information toimprove the performance of pose estimation remains a difficult andunder-investigated problem. To tackle this challenge, we propose a novelframework called SwinDePose, that uses only geometric information from depthimages to achieve accurate 6D pose estimation. SwinDePose first calculates theangles between each normal vector defined in a depth image and the threecoordinate axes in the camera coordinate system. The resulting angles are thenformed into an image, which is encoded using Swin Transformer. Additionally, weapply RandLA-Net to learn the representations from point clouds. The resultingimage and point clouds embeddings are concatenated and fed into a semanticsegmentation module and a 3D keypoints localization module. Finally, weestimate 6D poses using a least-square fitting approach based on the targetobject's predicted semantic mask and 3D keypoints. In experiments on theLineMod and Occlusion LineMod datasets, SwinDePose outperforms existingstate-of-the-art methods for 6D object pose estimation using depth images. Thisdemonstrates the effectiveness of our approach and highlights its potential forimproving performance in real-world scenarios. Our code is athttps://github.com/zhujunli1993/SwinDePose.</description><author>Zhujun Li, Ioannis Stamos</author><pubDate>Thu, 27 Apr 2023 19:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02133v2</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v1</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Tue, 01 Aug 2023 16:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v1</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v2</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenxuan Guo, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Sun, 06 Aug 2023 15:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v2</guid></item><item><title>TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement</title><link>http://arxiv.org/abs/2307.05561v1</link><description>As demand for robotics manipulation application increases, accuratevision-based 6D pose estimation becomes essential for autonomous operations.Convolutional Neural Networks (CNNs) based approaches for pose estimation havebeen previously introduced. However, the quest for better performance stillpersists especially for accurate robotics manipulation. This quest extends tothe Agri-robotics domain. In this paper, we propose TransPose, an improvedTransformer-based 6D pose estimation with a depth refinement module. Thearchitecture takes in only an RGB image as input with no additionalsupplementing modalities such as depth or thermal images. The architectureencompasses an innovative lighter depth estimation network that estimates depthfrom an RGB image using feature pyramid with an up-sampling method. Atransformer-based detection network with additional prediction heads isproposed to directly regress the object's centre and predict the 6D pose of thetarget. A novel depth refinement module is then used alongside the predictedcenters, 6D poses and depth patches to refine the accuracy of the estimated 6Dpose. We extensively compared our results with other state-of-the-art methodsand analysed our results for fruit-picking applications. The results weachieved show that our proposed technique outperforms the other methodsavailable in the literature.</description><author>Mahmoud Abdulsalam, Nabil Aouf</author><pubDate>Sun, 09 Jul 2023 18:33:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05561v1</guid></item><item><title>Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation</title><link>http://arxiv.org/abs/2303.11579v2</link><description>In this paper, a novel Diffusion-based 3D Pose estimation (D3DP) method withJoint-wise reProjection-based Multi-hypothesis Aggregation (JPMA) is proposedfor probabilistic 3D human pose estimation. On the one hand, D3DP generatesmultiple possible 3D pose hypotheses for a single 2D observation. It graduallydiffuses the ground truth 3D poses to a random distribution, and learns adenoiser conditioned on 2D keypoints to recover the uncontaminated 3D poses.The proposed D3DP is compatible with existing 3D pose estimators and supportsusers to balance efficiency and accuracy during inference through twocustomizable parameters. On the other hand, JPMA is proposed to assemblemultiple hypotheses generated by D3DP into a single 3D pose for practical use.It reprojects 3D pose hypotheses to the 2D camera plane, selects the besthypothesis joint-by-joint based on the reprojection errors, and combines theselected joints into the final pose. The proposed JPMA conducts aggregation atthe joint level and makes use of the 2D prior information, both of which havebeen overlooked by previous approaches. Extensive experiments on Human3.6M andMPI-INF-3DHP datasets show that our method outperforms the state-of-the-artdeterministic and probabilistic approaches by 1.5% and 8.9%, respectively. Codeis available at https://github.com/paTRICK-swk/D3DP.</description><author>Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang, Kai Han, Shanshe Wang, Siwei Ma, Wen Gao</author><pubDate>Wed, 23 Aug 2023 04:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11579v2</guid></item><item><title>RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose</title><link>http://arxiv.org/abs/2303.07399v2</link><description>Recent studies on 2D pose estimation have achieved excellent performance onpublic benchmarks, yet its application in the industrial community stillsuffers from heavy model parameters and high latency. In order to bridge thisgap, we empirically explore key factors in pose estimation including paradigm,model architecture, training strategy, and deployment, and present ahigh-performance real-time multi-person pose estimation framework, RTMPose,based on MMPose. Our RTMPose-m achieves 75.8% AP on COCO with 90+ FPS on anIntel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU, and RTMPose-lachieves 67.0% AP on COCO-WholeBody with 130+ FPS. To further evaluateRTMPose's capability in critical real-time applications, we also report theperformance after deploying on the mobile device. Our RTMPose-s achieves 72.2%AP on COCO with 70+ FPS on a Snapdragon 865 chip, outperforming existingopen-source libraries. Code and models are released athttps://github.com/open-mmlab/mmpose/tree/1.x/projects/rtmpose.</description><author>Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, Kai Chen</author><pubDate>Mon, 03 Jul 2023 04:06:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07399v2</guid></item><item><title>LAMP: Leveraging Language Prompts for Multi-person Pose Estimation</title><link>http://arxiv.org/abs/2307.11934v1</link><description>Human-centric visual understanding is an important desideratum for effectivehuman-robot interaction. In order to navigate crowded public places, socialrobots must be able to interpret the activity of the surrounding humans. Thispaper addresses one key aspect of human-centric visual understanding,multi-person pose estimation. Achieving good performance on multi-person poseestimation in crowded scenes is difficult due to the challenges of occludedjoints and instance separation. In order to tackle these challenges andovercome the limitations of image features in representing invisible bodyparts, we propose a novel prompt-based pose inference strategy called LAMP(Language Assisted Multi-person Pose estimation). By utilizing the textrepresentations generated by a well-trained language model (CLIP), LAMP canfacilitate the understanding of poses on the instance and joint levels, andlearn more robust visual representations that are less susceptible toocclusion. This paper demonstrates that language-supervised training boosts theperformance of single-stage multi-person pose estimation, and bothinstance-level and joint-level prompts are valuable for training. The code isavailable at https://github.com/shengnanh20/LAMP.</description><author>Shengnan Hu, Ce Zheng, Zixiang Zhou, Chen Chen, Gita Sukthankar</author><pubDate>Sat, 22 Jul 2023 00:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11934v1</guid></item><item><title>LAMP: Leveraging Language Prompts for Multi-person Pose Estimation</title><link>http://arxiv.org/abs/2307.11934v2</link><description>Human-centric visual understanding is an important desideratum for effectivehuman-robot interaction. In order to navigate crowded public places, socialrobots must be able to interpret the activity of the surrounding humans. Thispaper addresses one key aspect of human-centric visual understanding,multi-person pose estimation. Achieving good performance on multi-person poseestimation in crowded scenes is difficult due to the challenges of occludedjoints and instance separation. In order to tackle these challenges andovercome the limitations of image features in representing invisible bodyparts, we propose a novel prompt-based pose inference strategy called LAMP(Language Assisted Multi-person Pose estimation). By utilizing the textrepresentations generated by a well-trained language model (CLIP), LAMP canfacilitate the understanding of poses on the instance and joint levels, andlearn more robust visual representations that are less susceptible toocclusion. This paper demonstrates that language-supervised training boosts theperformance of single-stage multi-person pose estimation, and bothinstance-level and joint-level prompts are valuable for training. The code isavailable at https://github.com/shengnanh20/LAMP.</description><author>Shengnan Hu, Ce Zheng, Zixiang Zhou, Chen Chen, Gita Sukthankar</author><pubDate>Wed, 26 Jul 2023 19:08:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11934v2</guid></item><item><title>EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision</title><link>http://arxiv.org/abs/2303.02862v2</link><description>Event camera shows great potential in 3D hand pose estimation, especiallyaddressing the challenges of fast motion and high dynamic range in a low-powerway. However, due to the asynchronous differential imaging mechanism, it ischallenging to design event representation to encode hand motion informationespecially when the hands are not moving (causing motion ambiguity), and it isinfeasible to fully annotate the temporally dense event stream. In this paper,we propose EvHandPose with novel hand flow representations in Event-to-Posemodule for accurate hand pose estimation and alleviating the motion ambiguityissue. To solve the problem under sparse annotation, we design contrastmaximization and hand-edge constraints in Pose-to-IWE (Image with WarpedEvents) module and formulate EvHandPose in a weakly-supervision framework. Wefurther build EvRealHands, the first large-scale real-world event-based handpose dataset on several challenging scenes to bridge the real-synthetic domaingap. Experiments on EvRealHands demonstrate that EvHandPose outperformsprevious event-based methods under all evaluation scenes, achieves accurate andstable hand pose estimation with high temporal resolution in fast motion andstrong light scenes compared with RGB-based methods, generalizes well tooutdoor scenes and another type of event camera, and shows the potential forthe hand gesture recognition task.</description><author>Jianping Jiang, Jiahe Li, Baowen Zhang, Xiaoming Deng, Boxin Shi</author><pubDate>Wed, 30 Aug 2023 04:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02862v2</guid></item><item><title>You Only Look at One: Category-Level Object Representations for Pose Estimation From a Single Example</title><link>http://arxiv.org/abs/2305.12626v1</link><description>In order to meaningfully interact with the world, robot manipulators must beable to interpret objects they encounter. A critical aspect of thisinterpretation is pose estimation: inferring quantities that describe theposition and orientation of an object in 3D space. Most existing approaches topose estimation make limiting assumptions, often working only for specific,known object instances, or at best generalising to an object category usinglarge pose-labelled datasets. In this work, we present a method for achievingcategory-level pose estimation by inspection of just a single object from adesired category. We show that we can subsequently perform accurate poseestimation for unseen objects from an inspected category, and considerablyoutperform prior work by exploiting multi-view correspondences. We demonstratethat our method runs in real-time, enabling a robot manipulator equipped withan RGBD sensor to perform online 6D pose estimation for novel objects. Finally,we showcase our method in a continual learning setting, with a robot able todetermine whether objects belong to known categories, and if not, use activeperception to produce a one-shot category representation for subsequent poseestimation.</description><author>Walter Goodwin, Ioannis Havoutis, Ingmar Posner</author><pubDate>Mon, 22 May 2023 02:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12626v1</guid></item><item><title>Iterative Graph Filtering Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16074v1</link><description>Graph convolutional networks (GCNs) have proven to be an effective approachfor 3D human pose estimation. By naturally modeling the skeleton structure ofthe human body as a graph, GCNs are able to capture the spatial relationshipsbetween joints and learn an efficient representation of the underlying pose.However, most GCN-based methods use a shared weight matrix, making itchallenging to accurately capture the different and complex relationshipsbetween joints. In this paper, we introduce an iterative graph filteringframework for 3D human pose estimation, which aims to predict the 3D jointpositions given a set of 2D joint locations in images. Our approach builds uponthe idea of iteratively solving graph filtering with Laplacian regularizationvia the Gauss-Seidel iterative method. Motivated by this iterative solution, wedesign a Gauss-Seidel network (GS-Net) architecture, which makes use of weightand adjacency modulation, skip connection, and a pure convolutional block withlayer normalization. Adjacency modulation facilitates the learning of edgesthat go beyond the inherent connections of body joints, resulting in anadjusted graph structure that reflects the human skeleton, while skipconnections help maintain crucial information from the input layer's initialfeatures as the network depth increases. We evaluate our proposed model on twostandard benchmark datasets, and compare it with a comprehensive set of strongbaseline methods for 3D human pose estimation. Our experimental resultsdemonstrate that our approach outperforms the baseline methods on bothdatasets, achieving state-of-the-art performance. Furthermore, we conductablation studies to analyze the contributions of different components of ourmodel architecture and show that the skip connection and adjacency modulationhelp improve the model performance.</description><author>Zaedul Islam, A. Ben Hamza</author><pubDate>Sat, 29 Jul 2023 21:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16074v1</guid></item><item><title>Iterative Graph Filtering Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16074v2</link><description>Graph convolutional networks (GCNs) have proven to be an effective approachfor 3D human pose estimation. By naturally modeling the skeleton structure ofthe human body as a graph, GCNs are able to capture the spatial relationshipsbetween joints and learn an efficient representation of the underlying pose.However, most GCN-based methods use a shared weight matrix, making itchallenging to accurately capture the different and complex relationshipsbetween joints. In this paper, we introduce an iterative graph filteringframework for 3D human pose estimation, which aims to predict the 3D jointpositions given a set of 2D joint locations in images. Our approach builds uponthe idea of iteratively solving graph filtering with Laplacian regularizationvia the Gauss-Seidel iterative method. Motivated by this iterative solution, wedesign a Gauss-Seidel network (GS-Net) architecture, which makes use of weightand adjacency modulation, skip connection, and a pure convolutional block withlayer normalization. Adjacency modulation facilitates the learning of edgesthat go beyond the inherent connections of body joints, resulting in anadjusted graph structure that reflects the human skeleton, while skipconnections help maintain crucial information from the input layer's initialfeatures as the network depth increases. We evaluate our proposed model on twostandard benchmark datasets, and compare it with a comprehensive set of strongbaseline methods for 3D human pose estimation. Our experimental resultsdemonstrate that our approach outperforms the baseline methods on bothdatasets, achieving state-of-the-art performance. Furthermore, we conductablation studies to analyze the contributions of different components of ourmodel architecture and show that the skip connection and adjacency modulationhelp improve the model performance.</description><author>Zaedul Islam, A. Ben Hamza</author><pubDate>Mon, 07 Aug 2023 23:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16074v2</guid></item><item><title>Video-Based Hand Pose Estimation for Remote Assessment of Bradykinesia in Parkinson's Disease</title><link>http://arxiv.org/abs/2308.14679v1</link><description>There is a growing interest in using pose estimation algorithms forvideo-based assessment of Bradykinesia in Parkinson's Disease (PD) tofacilitate remote disease assessment and monitoring. However, the accuracy ofpose estimation algorithms in videos from video streaming services duringTelehealth appointments has not been studied. In this study, we used sevenoff-the-shelf hand pose estimation models to estimate the movement of the thumband index fingers in videos of the finger-tapping (FT) test recorded fromHealthy Controls (HC) and participants with PD and under two differentconditions: streaming (videos recorded during a live Zoom meeting) andon-device (videos recorded locally with high-quality cameras). The accuracy andreliability of the models were estimated by comparing the models' output withmanual results. Three of the seven models demonstrated good accuracy foron-device recordings, and the accuracy decreased significantly for streamingrecordings. We observed a negative correlation between movement speed and themodel's accuracy for the streaming recordings. Additionally, we evaluated thereliability of ten movement features related to bradykinesia extracted fromvideo recordings of PD patients performing the FT test. While most of thefeatures demonstrated excellent reliability for on-device recordings, most ofthe features demonstrated poor to moderate reliability for streamingrecordings. Our findings highlight the limitations of pose estimationalgorithms when applied to video recordings obtained during Telehealth visits,and demonstrate that on-device recordings can be used for automaticvideo-assessment of bradykinesia in PD.</description><author>Gabriela T. Acevedo Trebbau, Andrea Bandini, Diego L. Guarin</author><pubDate>Mon, 28 Aug 2023 17:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14679v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human</title><link>http://arxiv.org/abs/2307.05853v1</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively).</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Wed, 12 Jul 2023 01:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v1</guid></item><item><title>IMP: Iterative Matching and Pose Estimation with Adaptive Pooling</title><link>http://arxiv.org/abs/2304.14837v2</link><description>Previous methods solve feature matching and pose estimation using a two-stageprocess by first finding matches and then estimating the pose. As they ignorethe geometric relationships between the two tasks, they focus on eitherimproving the quality of matches or filtering potential outliers, leading tolimited efficiency or accuracy. In contrast, we propose an iterative matchingand pose estimation framework (IMP) leveraging the geometric connectionsbetween the two tasks: a few good matches are enough for a roughly accuratepose estimation; a roughly accurate pose can be used to guide the matching byproviding geometric constraints. To this end, we implement a geometry-awarerecurrent attention-based module which jointly outputs sparse matches andcamera poses. Specifically, for each iteration, we first implicitly embedgeometric information into the module via a pose-consistency loss, allowing itto predict geometry-aware matches progressively. Second, we introduce an\textbf{e}fficient IMP, called EIMP, to dynamically discard keypoints withoutpotential matches, avoiding redundant updating and significantly reducing thequadratic time complexity of attention computation in transformers. Experimentson YFCC100m, Scannet, and Aachen Day-Night datasets demonstrate that theproposed method outperforms previous approaches in terms of accuracy andefficiency.</description><author>Fei Xue, Ignas Budvytis, Roberto Cipolla</author><pubDate>Sun, 11 Jun 2023 17:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14837v2</guid></item><item><title>IMP: Iterative Matching and Pose Estimation with Adaptive Pooling</title><link>http://arxiv.org/abs/2304.14837v1</link><description>Previous methods solve feature matching and pose estimation using a two-stageprocess by first finding matches and then estimating the pose. As they ignorethe geometric relationships between the two tasks, they focus on eitherimproving the quality of matches or filtering potential outliers, leading tolimited efficiency or accuracy. In contrast, we propose an iterative matchingand pose estimation framework (IMP) leveraging the geometric connectionsbetween the two tasks: a few good matches are enough for a roughly accuratepose estimation; a roughly accurate pose can be used to guide the matching byproviding geometric constraints. To this end, we implement a geometry-awarerecurrent attention-based module which jointly outputs sparse matches andcamera poses. Specifically, for each iteration, we first implicitly embedgeometric information into the module via a pose-consistency loss, allowing itto predict geometry-aware matches progressively. Second, we introduce an\textbf{e}fficient IMP, called EIMP, to dynamically discard keypoints withoutpotential matches, avoiding redundant updating and significantly reducing thequadratic time complexity of attention computation in transformers. Experimentson YFCC100m, Scannet, and Aachen Day-Night datasets demonstrate that theproposed method outperforms previous approaches in terms of accuracy andefficiency.</description><author>Fei Xue, Ignas Budvytis, Roberto Cipolla</author><pubDate>Fri, 28 Apr 2023 14:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14837v1</guid></item><item><title>Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.03833v2</link><description>Learning-based methods have dominated the 3D human pose estimation (HPE)tasks with significantly better performance in most benchmarks than traditionaloptimization-based methods. Nonetheless, 3D HPE in the wild is still thebiggest challenge of learning-based models, whether with 2D-3D lifting,image-to-3D, or diffusion-based methods, since the trained networks implicitlylearn camera intrinsic parameters and domain-based 3D human pose distributionsand estimate poses by statistical average. On the other hand, theoptimization-based methods estimate results case-by-case, which can predictmore diverse and sophisticated human poses in the wild. By combining theadvantages of optimization-based and learning-based methods, we propose theZero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve theproblem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDOachieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mmwithout training with any 2D-3D or image-3D pairs. Moreover, oursingle-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE$42.6$mm on cross-dataset evaluation, which even outperforms learning-basedmethods trained on 3DPW.</description><author>Zhongyu Jiang, Zhuoran Zhou, Lei Li, Wenhao Chai, Cheng-Yen Yang, Jenq-Neng Hwang</author><pubDate>Wed, 23 Aug 2023 18:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03833v2</guid></item><item><title>HDFormer: High-order Directed Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.01825v2</link><description>Human pose estimation is a challenging task due to its structured datasequence nature. Existing methods primarily focus on pair-wise interaction ofbody joints, which is insufficient for scenarios involving overlapping jointsand rapidly changing poses. To overcome these issues, we introduce a novelapproach, the High-order Directed Transformer (HDFormer), which leverageshigh-order bone and joint relationships for improved pose estimation.Specifically, HDFormer incorporates both self-attention and high-orderattention to formulate a multi-order attention module. This module facilitatesfirst-order "joint$\leftrightarrow$joint", second-order"bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint"interactions, effectively addressing issues in complex and occlusion-heavysituations. In addition, modern CNN techniques are integrated into thetransformer-based architecture, balancing the trade-off between performance andefficiency. HDFormer significantly outperforms state-of-the-art (SOTA) modelson Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parametersand significantly lower computational costs. Moreover, HDFormer demonstratesbroad real-world applicability, enabling real-time, accurate 3D poseestimation. The source code is in https://github.com/hyer/HDFormer</description><author>Hanyuan Chen, Jun-Yan He, Wangmeng Xiang, Zhi-Qi Cheng, Wei Liu, Hanbing Liu, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 22 May 2023 07:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01825v2</guid></item><item><title>Geo6D: Geometric Constraints Learning for 6D Pose Estimation</title><link>http://arxiv.org/abs/2210.10959v6</link><description>Numerous 6D pose estimation methods have been proposed that employ end-to-endregression to directly estimate the target pose parameters. Since the visiblefeatures of objects are implicitly influenced by their poses, the networkallows inferring the pose by analyzing the differences in features in thevisible region. However, due to the unpredictable and unrestricted range ofpose variations, the implicitly learned visible feature-pose constraints areinsufficiently covered by the training samples, making the network vulnerableto unseen object poses. To tackle these challenges, we proposed a novelgeometric constraints learning approach called Geo6D for direct regression 6Dpose estimation methods. It introduces a pose transformation formula expressedin relative offset representation, which is leveraged as geometric constraintsto reconstruct the input and output targets of the network. These reconstructeddata enable the network to estimate the pose based on explicit geometricconstraints and relative offset representation mitigates the issue of the posedistribution gap. Extensive experimental results show that when equipped withGeo6D, the direct 6D methods achieve state-of-the-art performance on multipledatasets and demonstrate significant effectiveness, even with only 10% amountof data.</description><author>Jianqiu Chen, Mingshan Sun, Ye Zheng, Tianpeng Bao, Zhenyu He, Donghai Li, Guoqiang Jin, Rui Zhao, Liwei Wu, Xiaoke Jiang</author><pubDate>Tue, 22 Aug 2023 02:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.10959v6</guid></item><item><title>Regular Splitting Graph Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2305.05785v1</link><description>In human pose estimation methods based on graph convolutional architectures,the human skeleton is usually modeled as an undirected graph whose nodes arebody joints and edges are connections between neighboring joints. However, mostof these methods tend to focus on learning relationships between body joints ofthe skeleton using first-order neighbors, ignoring higher-order neighbors andhence limiting their ability to exploit relationships between distant joints.In this paper, we introduce a higher-order regular splitting graph network(RS-Net) for 2D-to-3D human pose estimation using matrix splitting inconjunction with weight and adjacency modulation. The core idea is to capturelong-range dependencies between body joints using multi-hop neighborhoods andalso to learn different modulation vectors for different body joints as well asa modulation matrix added to the adjacency matrix associated to the skeleton.This learnable modulation matrix helps adjust the graph structure by addingextra graph edges in an effort to learn additional connections between bodyjoints. Instead of using a shared weight matrix for all neighboring bodyjoints, the proposed RS-Net model applies weight unsharing before aggregatingthe feature vectors associated to the joints in order to capture the differentrelations between them. Experiments and ablations studies performed on twobenchmark datasets demonstrate the effectiveness of our model, achievingsuperior performance over recent state-of-the-art methods for 3D human poseestimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 09 May 2023 23:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05785v1</guid></item><item><title>EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With Kinematic Structure Priors</title><link>http://arxiv.org/abs/2306.09615v1</link><description>Transformer is popular in recent 3D human pose estimation, which utilizeslong-term modeling to lift 2D keypoints into the 3D space. However, currenttransformer-based methods do not fully exploit the prior knowledge of the humanskeleton provided by the kinematic structure. In this paper, we propose a noveltransformer-based model EvoPose to introduce the human body prior knowledge for3D human pose estimation effectively. Specifically, a Structural PriorsRepresentation (SPR) module represents human priors as structural featurescarrying rich body patterns, e.g. joint relationships. The structural featuresare interacted with 2D pose sequences and help the model to achieve moreinformative spatiotemporal features. Moreover, a Recursive Refinement (RR)module is applied to refine the 3D pose outputs by utilizing estimated resultsand further injects human priors simultaneously. Extensive experimentsdemonstrate the effectiveness of EvoPose which achieves a new state of the arton two most popular benchmarks, Human3.6M and MPI-INF-3DHP.</description><author>Yaqi Zhang, Yan Lu, Bin Liu, Zhiwei Zhao, Qi Chu, Nenghai Yu</author><pubDate>Fri, 16 Jun 2023 05:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09615v1</guid></item><item><title>Hybrid model for Single-Stage Multi-Person Pose Estimation</title><link>http://arxiv.org/abs/2305.01167v2</link><description>In general, human pose estimation methods are categorized into two approachesaccording to their architectures: regression (i.e., heatmap-free) andheatmap-based methods. The former one directly estimates precise coordinates ofeach keypoint using convolutional and fully-connected layers. Although thisapproach is able to detect overlapped and dense keypoints, unexpected resultscan be obtained by non-existent keypoints in a scene. On the other hand, thelatter one is able to filter the non-existent ones out by utilizing predictedheatmaps for each keypoint. Nevertheless, it suffers from quantization errorwhen obtaining the keypoint coordinates from its heatmaps. In addition, unlikethe regression one, it is difficult to distinguish densely placed keypoints inan image. To this end, we propose a hybrid model for single-stage multi-personpose estimation, named HybridPose, which mutually overcomes each drawback ofboth approaches by maximizing their strengths. Furthermore, we introduceself-correlation loss to inject spatial dependencies between keypointcoordinates and their visibility. Therefore, HybridPose is capable of not onlydetecting densely placed keypoints, but also filtering the non-existentkeypoints in an image. Experimental results demonstrate that proposedHybridPose exhibits the keypoints visibility without performance degradation interms of the pose estimation accuracy.</description><author>Jonghyun Kim, Bosang Kim, Hyotae Lee, Jungpyo Kim, Wonhyeok Im, Lanying Jin, Dowoo Kwon, Jungho Lee</author><pubDate>Mon, 19 Jun 2023 01:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01167v2</guid></item><item><title>Hybrid model for Single-Stage Multi-Person Pose Estimation</title><link>http://arxiv.org/abs/2305.01167v1</link><description>In general, human pose estimation methods are categorized into two approachesaccording to their architectures: regression (i.e., heatmap-free) andheatmap-based methods. The former one directly estimates precise coordinates ofeach keypoint using convolutional and fully-connected layers. Although thisapproach is able to detect overlapped and dense keypoints, unexpected resultscan be obtained by non-existent keypoints in a scene. On the other hand, thelatter one is able to filter the non-existent ones out by utilizing predictedheatmaps for each keypoint. Nevertheless, it suffers from quantization errorwhen obtaining the keypoint coordinates from its heatmaps. In addition, unlikethe regression one, it is difficult to distinguish densely placed keypoints inan image. To this end, we propose a hybrid model for single-stage multi-personpose estimation, named HybridPose, which mutually overcomes each drawback ofboth approaches by maximizing their strengths. Furthermore, we introduceself-correlation loss to inject spatial dependencies between keypointcoordinates and their visibility. Therefore, HybridPose is capable of not onlydetecting densely placed keypoints, but also filtering the non-existentkeypoints in an image. Experimental results demonstrate that proposedHybridPose exhibits the keypoints visibility without performance degradation interms of the pose estimation accuracy.</description><author>Jonghyun Kim, Bosang Kim, Hyotae Lee, Jungpyo Kim, Wonhyeok Im, Lanying Jin, Dowoo Kwon, Jungho Lee</author><pubDate>Tue, 02 May 2023 03:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01167v1</guid></item><item><title>Effective Whole-body Pose Estimation with Two-stages Distillation</title><link>http://arxiv.org/abs/2307.15880v2</link><description>Whole-body pose estimation localizes the human body, hand, face, and footkeypoints in an image. This task is challenging due to multi-scale body parts,fine-grained localization for low-resolution regions, and data scarcity.Meanwhile, applying a highly efficient and accurate pose estimator to widelyhuman-centric understanding and generation tasks is urgent. In this work, wepresent a two-stage pose \textbf{D}istillation for \textbf{W}hole-body\textbf{P}ose estimators, named \textbf{DWPose}, to improve their effectivenessand efficiency. The first-stage distillation designs a weight-decay strategywhile utilizing a teacher's intermediate feature and final logits with bothvisible and invisible keypoints to supervise the student from scratch. Thesecond stage distills the student model itself to further improve performance.Different from the previous self-knowledge distillation, this stage finetunesthe student's head with only 20% training time as a plug-and-play trainingstrategy. For data limitations, we explore the UBody dataset that containsdiverse facial expressions and hand gestures for real-life applications.Comprehensive experiments show the superiority of our proposed simple yeteffective methods. We achieve new state-of-the-art performance onCOCO-WholeBody, significantly boosting the whole-body AP of RTMPose-l from64.8% to 66.5%, even surpassing RTMPose-x teacher with 65.3% AP. We release aseries of models with different sizes, from tiny to large, for satisfyingvarious downstream tasks. Our codes and models are available athttps://github.com/IDEA-Research/DWPose.</description><author>Zhendong Yang, Ailing Zeng, Chun Yuan, Yu Li</author><pubDate>Fri, 25 Aug 2023 03:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15880v2</guid></item><item><title>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2308.09678v1</link><description>The current 3D human pose estimators face challenges in adapting to newdatasets due to the scarcity of 2D-3D pose pairs in target domain trainingsets. We present the \textit{Multi-Hypothesis \textbf{P}ose \textbf{Syn}thesis\textbf{D}omain \textbf{A}daptation} (\textbf{PoSynDA}) framework to overcomethis issue without extensive target domain annotation. Utilizing adiffusion-centric structure, PoSynDA simulates the 3D pose distribution in thetarget domain, filling the data diversity gap. By incorporating amulti-hypothesis network, it creates diverse pose hypotheses and aligns themwith the target domain. Target-specific source augmentation obtains the targetdomain distribution data from the source domain by decoupling the scale andposition parameters. The teacher-student paradigm and low-rank adaptationfurther refine the process. PoSynDA demonstrates competitive performance onbenchmarks, such as Human3.6M, MPI-INF-3DHP, and 3DPW, even comparable with thetarget-trained MixSTE model~\cite{zhang2022mixste}. This work paves the way forthe practical application of 3D human pose estimation. The code is available athttps://github.com/hbing-l/PoSynDA.</description><author>Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Fri, 18 Aug 2023 17:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09678v1</guid></item><item><title>Hierarchical Graph Neural Networks for Proprioceptive 6D Pose Estimation of In-hand Objects</title><link>http://arxiv.org/abs/2306.15858v1</link><description>Robotic manipulation, in particular in-hand object manipulation, oftenrequires an accurate estimate of the object's 6D pose. To improve the accuracyof the estimated pose, state-of-the-art approaches in 6D object pose estimationuse observational data from one or more modalities, e.g., RGB images, depth,and tactile readings. However, existing approaches make limited use of theunderlying geometric structure of the object captured by these modalities,thereby, increasing their reliance on visual features. This results in poorperformance when presented with objects that lack such visual features or whenvisual features are simply occluded. Furthermore, current approaches do nottake advantage of the proprioceptive information embedded in the position ofthe fingers. To address these limitations, in this paper: (1) we introduce ahierarchical graph neural network architecture for combining multimodal (visionand touch) data that allows for a geometrically informed 6D object poseestimation, (2) we introduce a hierarchical message passing operation thatflows the information within and across modalities to learn a graph-basedobject representation, and (3) we introduce a method that accounts for theproprioceptive information for in-hand object representation. We evaluate ourmodel on a diverse subset of objects from the YCB Object and Model Set, andshow that our method substantially outperforms existing state-of-the-art workin accuracy and robustness to occlusion. We also deploy our proposed frameworkon a real robot and qualitatively demonstrate successful transfer to realsettings.</description><author>Alireza Rezazadeh, Snehal Dikhale, Soshi Iba, Nawid Jamali</author><pubDate>Wed, 28 Jun 2023 02:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15858v1</guid></item><item><title>CheckerPose: Progressive Dense Keypoint Localization for Object Pose Estimation with Graph Neural Network</title><link>http://arxiv.org/abs/2303.16874v2</link><description>Estimating the 6-DoF pose of a rigid object from a single RGB image is acrucial yet challenging task. Recent studies have shown the great potential ofdense correspondence-based solutions, yet improvements are still needed toreach practical deployment. In this paper, we propose a novel pose estimationalgorithm named CheckerPose, which improves on three main aspects. Firstly,CheckerPose densely samples 3D keypoints from the surface of the 3D object andfinds their 2D correspondences progressively in the 2D image. Compared toprevious solutions that conduct dense sampling in the image space, our strategyenables the correspondence searching in a 2D grid (i.e., pixel coordinate).Secondly, for our 3D-to-2D correspondence, we design a compact binary coderepresentation for 2D image locations. This representation not only allows forprogressive correspondence refinement but also converts the correspondenceregression to a more efficient classification problem. Thirdly, we adopt agraph neural network to explicitly model the interactions among the sampled 3Dkeypoints, further boosting the reliability and accuracy of thecorrespondences. Together, these novel components make CheckerPose a strongpose estimation algorithm. When evaluated on the popular Linemod, Linemod-O,and YCB-V object pose estimation benchmarks, CheckerPose clearly boosts theaccuracy of correspondence-based methods and achieves state-of-the-artperformances. Code is available at https://github.com/RuyiLian/CheckerPose.</description><author>Ruyi Lian, Haibin Ling</author><pubDate>Sun, 13 Aug 2023 21:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16874v2</guid></item><item><title>Panoptic Mapping with Fruit Completion and Pose Estimation for Horticultural Robots</title><link>http://arxiv.org/abs/2303.08923v2</link><description>Monitoring plants and fruits at high resolution play a key role in the futureof agriculture. Accurate 3D information can pave the way to a diverse number ofrobotic applications in agriculture ranging from autonomous harvesting toprecise yield estimation. Obtaining such 3D information is non-trivial asagricultural environments are often repetitive and cluttered, and one has toaccount for the partial observability of fruit and plants. In this paper, weaddress the problem of jointly estimating complete 3D shapes of fruit and theirpose in a 3D multi-resolution map built by a mobile robot. To this end, wepropose an online multi-resolution panoptic mapping system where regions ofinterest are represented with a higher resolution. We exploit data to learn ageneral fruit shape representation that we use at inference time together withan occlusion-aware differentiable rendering pipeline to complete partial fruitobservations and estimate the 7 DoF pose of each fruit in the map. Theexperiments presented in this paper evaluated both in the controlledenvironment and in a commercial greenhouse, show that our novel algorithmyields higher completion and pose estimation accuracy than existing methods,with an improvement of 41% in completion accuracy and 52% in pose estimationaccuracy while keeping a low inference time of 0.6s in average. Codes areavailable at: https://github.com/PRBonn/HortiMapping.</description><author>Yue Pan, Federico Magistri, Thomas Läbe, Elias Marks, Claus Smitt, Chris McCool, Jens Behley, Cyrill Stachniss</author><pubDate>Tue, 22 Aug 2023 09:26:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08923v2</guid></item><item><title>In-Rack Test Tube Pose Estimation Using RGB-D Data</title><link>http://arxiv.org/abs/2308.10411v1</link><description>Accurate robotic manipulation of test tubes in biology and medical industriesis becoming increasingly important to address workforce shortages and improveworker safety. The detection and localization of test tubes are essential forthe robots to successfully manipulate test tubes. In this paper, we present aframework to detect and estimate poses for the in-rack test tubes using colorand depth data. The methodology involves the utilization of a YOLO objectdetector to effectively classify and localize both the test tubes and the tuberacks within the provided image data. Subsequently, the pose of the tube rackis estimated through point cloud registration techniques. During the process ofestimating the poses of the test tubes, we capitalize on constraints derivedfrom the arrangement of rack slots. By employing an optimization-basedalgorithm, we effectively evaluate and refine the pose of the test tubes. Thisstrategic approach ensures the robustness of pose estimation, even whenconfronted with noisy and incomplete point cloud data.</description><author>Hao Chen, Weiwei Wan, Masaki Matsushita, Takeyuki Kotaka, Kensuke Harada</author><pubDate>Mon, 21 Aug 2023 02:35:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10411v1</guid></item><item><title>3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking</title><link>http://arxiv.org/abs/2308.15316v1</link><description>Markerless methods for animal posture tracking have been developing recently,but frameworks and benchmarks for tracking large animal groups in 3D are stilllacking. To overcome this gap in the literature, we present 3D-MuPPET, aframework to estimate and track 3D poses of up to 10 pigeons at interactivespeed using multiple-views. We train a pose estimator to infer 2D keypoints andbounding boxes of multiple pigeons, then triangulate the keypoints to 3D. Forcorrespondence matching, we first dynamically match 2D detections to globalidentities in the first frame, then use a 2D tracker to maintaincorrespondences accross views in subsequent frames. We achieve comparableaccuracy to a state of the art 3D pose estimator for Root Mean Square Error(RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel usecase where our model trained with data of single pigeons provides comparableresults on data containing multiple pigeons. This can simplify the domain shiftto new species because annotating single animal data is less labour intensivethan multi-animal data. Additionally, we benchmark the inference speed of3D-MuPPET, with up to 10 fps in 2D and 1.5 fps in 3D, and perform quantitativetracking evaluation, which yields encouraging results. Finally, we show that3D-MuPPET also works in natural environments without model fine-tuning onadditional annotations. To the best of our knowledge we are the first topresent a framework for 2D/3D posture and trajectory tracking that works inboth indoor and outdoor environments.</description><author>Urs Waldmann, Alex Hoi Hang Chan, Hemal Naik, Máté Nagy, Iain D. Couzin, Oliver Deussen, Bastian Goldluecke, Fumihiro Kano</author><pubDate>Tue, 29 Aug 2023 15:02:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15316v1</guid></item><item><title>Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap</title><link>http://arxiv.org/abs/2203.04275v6</link><description>This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional NeuralNetwork (CNN) for pose estimation of noncooperative spacecraft across domaingap. SPNv2 is a multi-scale, multi-task CNN which consists of a sharedmulti-scale feature encoder and multiple prediction heads that performdifferent tasks on a shared feature output. These tasks are all related todetection and pose estimation of a target spacecraft from an image, such asprediction of pre-defined satellite keypoints, direct pose regression, andbinary segmentation of the satellite foreground. It is shown that by jointlytraining on different yet related tasks with extensive data augmentations onsynthetic images only, the shared encoder learns features that are commonacross image domains that have fundamentally different visual characteristicscompared to synthetic images. This work also introduces Online DomainRefinement (ODR) which refines the parameters of the normalization layers ofSPNv2 on the target domain images online at deployment. Specifically, ODRperforms self-supervised entropy minimization of the predicted satelliteforeground, thereby improving the CNN's performance on the target domain imageswithout their pose labels and with minimal computational efforts. The GitHubrepository for SPNv2 is available at https://github.com/tpark94/spnv2.</description><author>Tae Ha Park, Simone D'Amico</author><pubDate>Thu, 17 Aug 2023 23:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.04275v6</guid></item><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Group Pose: A Simple Baseline for End-to-End Multi-person Pose Estimation</title><link>http://arxiv.org/abs/2308.07313v1</link><description>In this paper, we study the problem of end-to-end multi-person poseestimation. State-of-the-art solutions adopt the DETR-like framework, andmainly develop the complex decoder, e.g., regarding pose estimation as keypointbox detection and combining with human detection in ED-Pose, hierarchicallypredicting with pose decoder and joint (keypoint) decoder in PETR. We present asimple yet effective transformer approach, named Group Pose. We simply regard$K$-keypoint pose estimation as predicting a set of $N\times K$ keypointpositions, each from a keypoint query, as well as representing each pose withan instance query for scoring $N$ pose predictions. Motivated by the intuitionthat the interaction, among across-instance queries of different types, is notdirectly helpful, we make a simple modification to decoder self-attention. Wereplace single self-attention over all the $N\times(K+1)$ queries with twosubsequent group self-attentions: (i) $N$ within-instance self-attention, witheach over $K$ keypoint queries and one instance query, and (ii) $(K+1)$same-type across-instance self-attention, each over $N$ queries of the sametype. The resulting decoder removes the interaction among across-instancetype-different queries, easing the optimization and thus improving theperformance. Experimental results on MS COCO and CrowdPose show that ourapproach without human box supervision is superior to previous methods withcomplex decoders, and even is slightly better than ED-Pose that uses human boxsupervision. $\href{https://github.com/Michel-liu/GroupPose-Paddle}{\rmPaddle}$ and $\href{https://github.com/Michel-liu/GroupPose}{\rm PyTorch}$ codeare available.</description><author>Huan Liu, Qiang Chen, Zichang Tan, Jiang-Jiang Liu, Jian Wang, Xiangbo Su, Xiaolong Li, Kun Yao, Junyu Han, Errui Ding, Yao Zhao, Jingdong Wang</author><pubDate>Mon, 14 Aug 2023 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07313v1</guid></item><item><title>SuperAnimal pretrained pose estimation models for behavioral analysis</title><link>http://arxiv.org/abs/2203.07436v3</link><description>Quantification of behavior is critical in applications ranging fromneuroscience, veterinary medicine and animal conservation efforts. A common keystep for behavioral analysis is first extracting relevant keypoints on animals,known as pose estimation. However, reliable inference of poses currentlyrequires domain knowledge and manual labeling effort to build supervisedmodels. We present a series of technical innovations that enable a new method,collectively called SuperAnimal, to develop and deploy deep learning modelsthat require zero additional human labels and model training. SuperAnimalallows video inference on over 45 species with only two global classes ofanimal pose models. If the models need fine-tuning, we show SuperAnimal modelsare 10$\times$ more data efficient and outperform prior transfer-learning-basedapproaches. Moreover, we provide an unsupervised video-adaptation method torefine keypoints in videos. We illustrate the utility of our model inbehavioral classification in mice and gait analysis in horses. Collectively,this presents a data-efficient solution for animal pose estimation fordownstream behavioral analysis.</description><author>Shaokai Ye, Anastasiia Filippova, Jessy Lauer, Maxime Vidal, Steffen Schneider, Tian Qiu, Alexander Mathis, Mackenzie Weygandt Mathis</author><pubDate>Thu, 17 Aug 2023 20:12:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.07436v3</guid></item><item><title>Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation</title><link>http://arxiv.org/abs/2308.05438v1</link><description>One critical challenge in 6D object pose estimation from a single RGBD imageis efficient integration of two different modalities, i.e., color and depth. Inthis work, we tackle this problem by a novel Deep Fusion Transformer~(DFTr)block that can aggregate cross-modality features for improving pose estimation.Unlike existing fusion methods, the proposed DFTr can better modelcross-modality semantic correlation by leveraging their semantic similarity,such that globally enhanced features from different modalities can be betterintegrated for improved information extraction. Moreover, to further improverobustness and efficiency, we introduce a novel weighted vector-wise votingalgorithm that employs a non-iterative global optimization strategy for precise3D keypoint localization while achieving near real-time inference. Extensiveexperiments show the effectiveness and strong generalization capability of ourproposed 3D keypoint voting algorithm. Results on four widely used benchmarksalso demonstrate that our method outperforms the state-of-the-art methods bylarge margins.</description><author>Jun Zhou, Kai Chen, Linlin Xu, Qi Dou, Jing Qin</author><pubDate>Thu, 10 Aug 2023 09:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05438v1</guid></item><item><title>Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)</title><link>http://arxiv.org/abs/2305.15873v1</link><description>Addressing accuracy limitations and pose ambiguity in 6D object poseestimation from single RGB images presents a significant challenge,particularly due to object symmetries or occlusions. In response, we introducea novel score-based diffusion method applied to the $SE(3)$ group, marking thefirst application of diffusion models to $SE(3)$ within the image domain,specifically tailored for pose estimation tasks. Extensive evaluationsdemonstrate the method's efficacy in handling pose ambiguity, mitigatingperspective-induced ambiguity, and showcasing the robustness of our surrogateStein score formulation on $SE(3)$. This formulation not only improves theconvergence of Langevin dynamics but also enhances computational efficiency.Thus, we pioneer a promising strategy for 6D object pose estimation.</description><author>Tsu-Ching Hsiao, Hao-Wei Chen, Hsuan-Kung Yang, Chun-Yi Lee</author><pubDate>Thu, 25 May 2023 10:09:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15873v1</guid></item><item><title>Polarimetric Information for Multi-Modal 6D Pose Estimation of Photometrically Challenging Objects with Limited Data</title><link>http://arxiv.org/abs/2308.10627v1</link><description>6D pose estimation pipelines that rely on RGB-only or RGB-D data showlimitations for photometrically challenging objects with e.g. texturelesssurfaces, reflections or transparency. A supervised learning-based methodutilising complementary polarisation information as input modality is proposedto overcome such limitations. This supervised approach is then extended to aself-supervised paradigm by leveraging physical characteristics of polarisedlight, thus eliminating the need for annotated real data. The methods achievesignificant advancements in pose estimation by leveraging geometric informationfrom polarised light and incorporating shape priors and invertible physicalconstraints.</description><author>Patrick Ruhkamp, Daoyi Gao, HyunJun Jung, Nassir Navab, Benjamin Busam</author><pubDate>Mon, 21 Aug 2023 11:56:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10627v1</guid></item><item><title>Graph-CoVis: GNN-based Multi-view Panorama Global Pose Estimation</title><link>http://arxiv.org/abs/2304.13201v1</link><description>In this paper, we address the problem of wide-baseline camera pose estimationfrom a group of 360$^\circ$ panoramas under upright-camera assumption. Recentwork has demonstrated the merit of deep-learning for end-to-end direct relativepose regression in 360$^\circ$ panorama pairs [11]. To exploit the benefits ofmulti-view logic in a learning-based framework, we introduce Graph-CoVis, whichnon-trivially extends CoVisPose [11] from relative two-view to globalmulti-view spherical camera pose estimation. Graph-CoVis is a novel GraphNeural Network based architecture that jointly learns the co-visible structureand global motion in an end-to-end and fully-supervised approach. Using theZInD [4] dataset, which features real homes presenting wide-baselines,occlusion, and limited visual overlap, we show that our model performscompetitively to state-of-the-art approaches.</description><author>Negar Nejatishahidin, Will Hutchcroft, Manjunath Narayana, Ivaylo Boyadzhiev, Yuguang Li, Naji Khosravan, Jana Kosecka, Sing Bing Kang</author><pubDate>Wed, 26 Apr 2023 01:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13201v1</guid></item><item><title>MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model</title><link>http://arxiv.org/abs/2302.08751v2</link><description>One of the major challenges in multi-person pose estimation is instance-awarekeypoint estimation. Previous methods address this problem by leveraging anoff-the-shelf detector, heuristic post-grouping process or explicit instanceidentification process, hindering further improvements in the inference speedwhich is an important factor for practical applications. From the statisticalpoint of view, those additional processes for identifying instances arenecessary to bypass learning the high-dimensional joint distribution of humankeypoints, which is a critical factor for another major challenge, theocclusion scenario. In this work, we propose a novel framework of single-stageinstance-aware pose estimation by modeling the joint distribution of humankeypoints with a mixture density model, termed as MDPose. Our MDPose estimatesthe distribution of human keypoints' coordinates using a mixture density modelwith an instance-aware keypoint head consisting simply of 8 convolutionallayers. It is trained by minimizing the negative log-likelihood of the groundtruth keypoints. Also, we propose a simple yet effective training strategy,Random Keypoint Grouping (RKG), which significantly alleviates the underflowproblem leading to successful learning of relations between keypoints. OnOCHuman dataset, which consists of images with highly occluded people, ourMDPose achieves state-of-the-art performance by successfully learning thehigh-dimensional joint distribution of human keypoints. Furthermore, our MDPoseshows significant improvement in inference speed with a competitive accuracy onMS COCO, a widely-used human keypoint dataset, thanks to the proposed muchsimpler single-stage pipeline.</description><author>Seunghyeon Seo, Jaeyoung Yoo, Jihye Hwang, Nojun Kwak</author><pubDate>Mon, 08 May 2023 13:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08751v2</guid></item><item><title>Spatio-temporal MLP-graph network for 3D human pose estimation</title><link>http://arxiv.org/abs/2308.15313v1</link><description>Graph convolutional networks and their variants have shown significantpromise in 3D human pose estimation. Despite their success, most of thesemethods only consider spatial correlations between body joints and do not takeinto account temporal correlations, thereby limiting their ability to capturerelationships in the presence of occlusions and inherent ambiguity. To addressthis potential weakness, we propose a spatio-temporal network architecturecomposed of a joint-mixing multi-layer perceptron block that facilitatescommunication among different joints and a graph weighted Jacobi network blockthat enables communication among various feature channels. The major novelty ofour approach lies in a new weighted Jacobi feature propagation rule obtainedthrough graph filtering with implicit fairing. We leverage temporal informationfrom the 2D pose sequences, and integrate weight modulation into the model toenable untangling of the feature transformations of distinct nodes. We alsoemploy adjacency modulation with the aim of learning meaningful correlationsbeyond defined linkages between body joints by altering the graph topologythrough a learnable modulation matrix. Extensive experiments on two benchmarkdatasets demonstrate the effectiveness of our model, outperforming recentstate-of-the-art methods for 3D human pose estimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 29 Aug 2023 15:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15313v1</guid></item><item><title>POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities</title><link>http://arxiv.org/abs/2307.10387v1</link><description>The surgical usage of Mixed Reality (MR) has received growing attention inareas such as surgical navigation systems, skill assessment, and robot-assistedsurgeries. For such applications, pose estimation for hand and surgicalinstruments from an egocentric perspective is a fundamental task and has beenstudied extensively in the computer vision field in recent years. However, thedevelopment of this field has been impeded by a lack of datasets, especially inthe surgical field, where bloody gloves and reflective metallic tools make ithard to obtain 3D pose annotations for hands and objects using conventionalmethods. To address this issue, we propose POV-Surgery, a large-scale,synthetic, egocentric dataset focusing on pose estimation for hands withdifferent surgical gloves and three orthopedic surgical instruments, namelyscalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329frames, featuring high-resolution RGB-D video streams with activityannotations, accurate 3D and 2D annotations for hand-object pose, and 2Dhand-object segmentation masks. We fine-tune the current SOTA methods onPOV-Surgery and further show the generalizability when applying to real-lifecases with surgical gloves and tools by extensive evaluations. The code and thedataset are publicly available at batfacewayne.github.io/POV_Surgery_io/.</description><author>Rui Wang, Sophokles Ktistakis, Siwei Zhang, Mirko Meboldt, Quentin Lohmeyer</author><pubDate>Wed, 19 Jul 2023 19:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10387v1</guid></item><item><title>Self-supervised Optimization of Hand Pose Estimation using Anatomical Features and Iterative Learning</title><link>http://arxiv.org/abs/2307.03007v1</link><description>Manual assembly workers face increasing complexity in their work.Human-centered assistance systems could help, but object recognition as anenabling technology hinders sophisticated human-centered design of thesesystems. At the same time, activity recognition based on hand poses suffersfrom poor pose estimation in complex usage scenarios, such as wearing gloves.This paper presents a self-supervised pipeline for adapting hand poseestimation to specific use cases with minimal human interaction. This enablescheap and robust hand posebased activity recognition. The pipeline consists ofa general machine learning model for hand pose estimation trained on ageneralized dataset, spatial and temporal filtering to account for anatomicalconstraints of the hand, and a retraining step to improve the model. Differentparameter combinations are evaluated on a publicly available and annotateddataset. The best parameter and model combination is then applied to unlabelledvideos from a manual assembly scenario. The effectiveness of the pipeline isdemonstrated by training an activity recognition as a downstream task in themanual assembly scenario.</description><author>Christian Jauch, Timo Leitritz, Marco F. Huber</author><pubDate>Thu, 06 Jul 2023 15:13:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03007v1</guid></item><item><title>ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation</title><link>http://arxiv.org/abs/2305.01618v1</link><description>We propose a new dataset and a novel approach to learning hand-objectinteraction priors for hand and articulated object pose estimation. We firstcollect a dataset using visual teleoperation, where the human operator candirectly play within a physical simulator to manipulate the articulatedobjects. We record the data and obtain free and accurate annotations on objectposes and contact information from the simulator. Our system only requires aniPhone to record human hand motion, which can be easily scaled up and largelylower the costs of data and annotation collection. With this data, we learn 3Dinteraction priors including a discriminator (in a GAN) capturing thedistribution of how object parts are arranged, and a diffusion model whichgenerates the contact regions on articulated objects, guiding the hand poseestimation. Such structural and contact priors can easily transfer toreal-world data with barely any domain gap. By using our data and learnedpriors, our method significantly improves the performance on joint hand andarticulated object poses estimation over the existing state-of-the-art methods.The project is available at https://zehaozhu.github.io/ContactArt/ .</description><author>Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang</author><pubDate>Tue, 02 May 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01618v1</guid></item><item><title>Vision-based Target Pose Estimation with Multiple Markers for the Perching of UAVs</title><link>http://arxiv.org/abs/2304.14838v1</link><description>Autonomous Nano Aerial Vehicles have been increasingly popular insurveillance and monitoring operations due to their efficiency andmaneuverability. Once a target location has been reached, drones do not have toremain active during the mission. It is possible for the vehicle to perch andstop its motors in such situations to conserve energy, as well as maintain astatic position in unfavorable flying conditions. In the perching targetestimation phase, the steady and accuracy of a visual camera with markers is asignificant challenge. It is rapidly detectable from afar when using a largemarker, but when the drone approaches, it quickly disappears as out of cameraview. In this paper, a vision-based target poses estimation method usingmultiple markers is proposed to deal with the above-mentioned problems. First,a perching target with a small marker inside a larger one is designed toimprove detection capability at wide and close ranges. Second, the relativeposes of the flying vehicle are calculated from detected markers using amonocular camera. Next, a Kalman filter is applied to provide a more stable andreliable pose estimation, especially when the measurement data is missing dueto unexpected reasons. Finally, we introduced an algorithm for merging theposes data from multi markers. The poses are then sent to the positioncontroller to align the drone and the marker's center and steer it to perch onthe target. The experimental results demonstrated the effectiveness andfeasibility of the adopted approach. The drone can perch successfully onto thecenter of the markers with the attached 25mm-diameter rounded magnet.</description><author>Truong-Dong Do, Nguyen Xuan-Mung, Sung-Kyung Hong</author><pubDate>Tue, 25 Apr 2023 17:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14838v1</guid></item><item><title>Human Body Pose Estimation for Gait Identification: A Comprehensive Survey of Datasets and Models</title><link>http://arxiv.org/abs/2305.13765v1</link><description>Person identification is a problem that has received substantial attention,particularly in security domains. Gait recognition is one of the mostconvenient approaches enabling person identification at a distance without theneed of high-quality images. There are several review studies addressing personidentification such as the utilization of facial images, silhouette images, andwearable sensor. Despite skeleton-based person identification gainingpopularity while overcoming the challenges of traditional approaches, existingsurvey studies lack the comprehensive review of skeleton-based approaches togait identification. We present a detailed review of the human pose estimationand gait analysis that make the skeleton-based approaches possible. The studycovers various types of related datasets, tools, methodologies, and evaluationmetrics with associated challenges, limitations, and application domains.Detailed comparisons are presented for each of these aspects withrecommendations for potential research and alternatives. A common trendthroughout this paper is the positive impact that deep learning techniques arebeginning to have on topics such as human pose estimation and gaitidentification. The survey outcomes might be useful for the related researchcommunity and other stakeholders in terms of performance analysis of existingmethodologies, potential research gaps, application domains, and possiblecontributions in the future.</description><author>Luke K. Topham, Wasiq Khan, Dhiya Al-Jumeily, Abir Hussain</author><pubDate>Tue, 23 May 2023 08:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13765v1</guid></item><item><title>DTF-Net: Category-Level Pose Estimation and Shape Reconstruction via Deformable Template Field</title><link>http://arxiv.org/abs/2308.02239v1</link><description>Estimating 6D poses and reconstructing 3D shapes of objects in open-worldscenes from RGB-depth image pairs is challenging. Many existing methods rely onlearning geometric features that correspond to specific templates whiledisregarding shape variations and pose differences among objects in the samecategory. As a result, these methods underperform when handling unseen objectinstances in complex environments. In contrast, other approaches aim to achievecategory-level estimation and reconstruction by leveraging normalized geometricstructure priors, but the static prior-based reconstruction struggles withsubstantial intra-class variations. To solve these problems, we propose theDTF-Net, a novel framework for pose estimation and shape reconstruction basedon implicit neural fields of object categories. In DTF-Net, we design adeformable template field to represent the general category-wise shape latentfeatures and intra-category geometric deformation features. The fieldestablishes continuous shape correspondences, deforming the category templateinto arbitrary observed instances to accomplish shape reconstruction. Weintroduce a pose regression module that shares the deformation features andtemplate codes from the fields to estimate the accurate 6D pose of each objectin the scene. We integrate a multi-modal representation extraction module toextract object features and semantic masks, enabling end-to-end inference.Moreover, during training, we implement a shape-invariant training strategy anda viewpoint sampling method to further enhance the model's capability toextract object pose features. Extensive experiments on the REAL275 and CAMERA25datasets demonstrate the superiority of DTF-Net in both synthetic and realscenes. Furthermore, we show that DTF-Net effectively supports grasping taskswith a real robot arm.</description><author>Haowen Wang, Zhipeng Fan, Zhen Zhao, Zhengping Che, Zhiyuan Xu, Dong Liu, Feifei Feng, Yakun Huang, Xiuquan Qiao, Jian Tang</author><pubDate>Fri, 04 Aug 2023 11:35:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02239v1</guid></item><item><title>EgoCOL: Egocentric Camera pose estimation for Open-world 3D object Localization @Ego4D challenge 2023</title><link>http://arxiv.org/abs/2306.16606v1</link><description>We present EgoCOL, an egocentric camera pose estimation method for open-world3D object localization. Our method leverages sparse camera pose reconstructionsin a two-fold manner, video and scan independently, to estimate the camera poseof egocentric frames in 3D renders with high recall and precision. Weextensively evaluate our method on the Visual Query (VQ) 3D object localizationEgo4D benchmark. EgoCOL can estimate 62% and 59% more camera poses than theEgo4D baseline in the Ego4D Visual Queries 3D Localization challenge at CVPR2023 in the val and test sets, respectively. Our code is publicly available athttps://github.com/BCV-Uniandes/EgoCOL</description><author>Cristhian Forigua, Maria Escobar, Jordi Pont-Tuset, Kevis-Kokitsi Maninis, Pablo Arbeláez</author><pubDate>Thu, 29 Jun 2023 01:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16606v1</guid></item><item><title>Source-free Domain Adaptive Human Pose Estimation</title><link>http://arxiv.org/abs/2308.03202v1</link><description>Human Pose Estimation (HPE) is widely used in various fields, includingmotion analysis, healthcare, and virtual reality. However, the great expensesof labeled real-world datasets present a significant challenge for HPE. Toovercome this, one approach is to train HPE models on synthetic datasets andthen perform domain adaptation (DA) on real-world data. Unfortunately, existingDA methods for HPE neglect data privacy and security by using both source andtarget data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE,which aims to address the challenges of cross-domain learning of HPE withoutaccess to source data during the adaptation process. We further propose a novelframework that consists of three models: source model, intermediate model, andtarget model, which explores the task from both source-protect andtarget-relevant perspectives. The source-protect module preserves sourceinformation more effectively while resisting noise, and the target-relevantmodule reduces the sparsity of spatial representations by building a novelspatial probability space, and pose-specific contrastive learning andinformation maximization are proposed on the basis of this space. Comprehensiveexperiments on several domain adaptive HPE benchmarks show that the proposedmethod outperforms existing approaches by a considerable margin.</description><author>Qucheng Peng, Ce Zheng, Chen Chen</author><pubDate>Sun, 06 Aug 2023 21:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03202v1</guid></item><item><title>Source-free Domain Adaptive Human Pose Estimation</title><link>http://arxiv.org/abs/2308.03202v2</link><description>Human Pose Estimation (HPE) is widely used in various fields, includingmotion analysis, healthcare, and virtual reality. However, the great expensesof labeled real-world datasets present a significant challenge for HPE. Toovercome this, one approach is to train HPE models on synthetic datasets andthen perform domain adaptation (DA) on real-world data. Unfortunately, existingDA methods for HPE neglect data privacy and security by using both source andtarget data in the adaptation process. To this end, we propose a new task,named source-free domain adaptive HPE, which aims to address the challenges ofcross-domain learning of HPE without access to source data during theadaptation process. We further propose a novel framework that consists of threemodels: source model, intermediate model, and target model, which explores thetask from both source-protect and target-relevant perspectives. Thesource-protect module preserves source information more effectively whileresisting noise, and the target-relevant module reduces the sparsity of spatialrepresentations by building a novel spatial probability space, andpose-specific contrastive learning and information maximization are proposed onthe basis of this space. Comprehensive experiments on several domain adaptiveHPE benchmarks show that the proposed method outperforms existing approaches bya considerable margin.</description><author>Qucheng Peng, Ce Zheng, Chen Chen</author><pubDate>Mon, 14 Aug 2023 17:33:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03202v2</guid></item><item><title>Source-free Domain Adaptive Human Pose Estimation</title><link>http://arxiv.org/abs/2308.03202v3</link><description>Human Pose Estimation (HPE) is widely used in various fields, includingmotion analysis, healthcare, and virtual reality. However, the great expensesof labeled real-world datasets present a significant challenge for HPE. Toovercome this, one approach is to train HPE models on synthetic datasets andthen perform domain adaptation (DA) on real-world data. Unfortunately, existingDA methods for HPE neglect data privacy and security by using both source andtarget data in the adaptation process. To this end, we propose a new task,named source-free domain adaptive HPE, which aims to address the challenges ofcross-domain learning of HPE without access to source data during theadaptation process. We further propose a novel framework that consists of threemodels: source model, intermediate model, and target model, which explores thetask from both source-protect and target-relevant perspectives. Thesource-protect module preserves source information more effectively whileresisting noise, and the target-relevant module reduces the sparsity of spatialrepresentations by building a novel spatial probability space, andpose-specific contrastive learning and information maximization are proposed onthe basis of this space. Comprehensive experiments on several domain adaptiveHPE benchmarks show that the proposed method outperforms existing approaches bya considerable margin. The codes are available athttps://github.com/davidpengucf/SFDAHPE.</description><author>Qucheng Peng, Ce Zheng, Chen Chen</author><pubDate>Tue, 15 Aug 2023 16:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03202v3</guid></item><item><title>Of Mice and Pose: 2D Mouse Pose Estimation from Unlabelled Data and Synthetic Prior</title><link>http://arxiv.org/abs/2307.13361v1</link><description>Numerous fields, such as ecology, biology, and neuroscience, use animalrecordings to track and measure animal behaviour. Over time, a significantvolume of such data has been produced, but some computer vision techniquescannot explore it due to the lack of annotations. To address this, we proposean approach for estimating 2D mouse body pose from unlabelled images using asynthetically generated empirical pose prior. Our proposal is based on a recentself-supervised method for estimating 2D human pose that uses single images anda set of unpaired typical 2D poses within a GAN framework. We adapt this methodto the limb structure of the mouse and generate the empirical prior of 2D posesfrom a synthetic 3D mouse model, thereby avoiding manual annotation. Inexperiments on a new mouse video dataset, we evaluate the performance of theapproach by comparing pose predictions to a manually obtained ground truth. Wealso compare predictions with those from a supervised state-of-the-art methodfor animal pose estimation. The latter evaluation indicates promising resultsdespite the lack of paired training data. Finally, qualitative results using adataset of horse images show the potential of the setting to adapt to otheranimal species.</description><author>Jose Sosa, Sharn Perry, Jane Alty, David Hogg</author><pubDate>Tue, 25 Jul 2023 10:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13361v1</guid></item><item><title>MELON: NeRF with Unposed Images in SO(3)</title><link>http://arxiv.org/abs/2303.08096v2</link><description>Neural radiance fields enable novel-view synthesis and scene reconstructionwith photorealistic quality from a few images, but require known and accuratecamera poses. Conventional pose estimation algorithms fail on smooth orself-similar scenes, while methods performing inverse rendering from unposedviews require a rough initialization of the camera orientations. The maindifficulty of pose estimation lies in real-life objects being almost invariantunder certain transformations, making the photometric distance between renderedviews non-convex with respect to the camera parameters. Using an equivalencerelation that matches the distribution of local minima in camera space, wereduce this space to its quotient set, in which pose estimation becomes a moreconvex problem. Using a neural-network to regularize pose estimation, wedemonstrate that our method - MELON - can reconstruct a neural radiance fieldfrom unposed images with state-of-the-art accuracy while requiring ten timesfewer views than adversarial approaches.</description><author>Axel Levy, Mark Matthews, Matan Sela, Gordon Wetzstein, Dmitry Lagun</author><pubDate>Wed, 19 Jul 2023 09:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08096v2</guid></item><item><title>IST-Net: Prior-free Category-level Pose Estimation with Implicit Space Transformation</title><link>http://arxiv.org/abs/2303.13479v2</link><description>Category-level 6D pose estimation aims to predict the poses and sizes ofunseen objects from a specific category. Thanks to prior deformation, whichexplicitly adapts a category-specific 3D prior (i.e., a 3D template) to a givenobject instance, prior-based methods attained great success and have become amajor research stream. However, obtaining category-specific priors requirescollecting a large amount of 3D models, which is labor-consuming and often notaccessible in practice. This motivates us to investigate whether priors arenecessary to make prior-based methods effective. Our empirical study shows thatthe 3D prior itself is not the credit to the high performance. The keypointactually is the explicit deformation process, which aligns camera and worldcoordinates supervised by world-space 3D models (also called canonical space).Inspired by these observations, we introduce a simple prior-free implicit spacetransformation network, namely IST-Net, to transform camera-space features toworld-space counterparts and build correspondence between them in an implicitmanner without relying on 3D priors. Besides, we design camera- and world-spaceenhancers to enrich the features with pose-sensitive information andgeometrical constraints, respectively. Albeit simple, IST-Net achievesstate-of-the-art performance based-on prior-free design, with top inferencespeed on the REAL275 benchmark. Our code and models are available athttps://github.com/CVMI-Lab/IST-Net.</description><author>Jianhui Liu, Yukang Chen, Xiaoqing Ye, Xiaojuan Qi</author><pubDate>Wed, 19 Jul 2023 17:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13479v2</guid></item><item><title>A Probabilistic Relaxation of the Two-Stage Object Pose Estimation Paradigm</title><link>http://arxiv.org/abs/2306.00892v1</link><description>Existing object pose estimation methods commonly require a one-to-one pointmatching step that forces them to be separated into two consecutive stages:visual correspondence detection (e.g., by matching feature descriptors as partof a perception front-end) followed by geometric alignment (e.g., by optimizinga robust estimation objective for pointcloud registration orperspective-n-point). Instead, we propose a matching-free probabilisticformulation with two main benefits: i) it enables unified and concurrentoptimization of both visual correspondence and geometric alignment, and ii) itcan represent different plausible modes of the entire distribution of likelyposes. This in turn allows for a more graceful treatment of geometricperception scenarios where establishing one-to-one matches between points isconceptually ill-defined, such as textureless, symmetrical and/or occludedobjects and scenes where the correct pose is uncertain or there are multipleequally valid solutions.</description><author>Onur Beker</author><pubDate>Thu, 01 Jun 2023 17:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00892v1</guid></item><item><title>Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation</title><link>http://arxiv.org/abs/2303.04991v2</link><description>Accurately estimating 3D hand pose is crucial for understanding how humansinteract with the world. Despite remarkable progress, existing methods oftenstruggle to generate plausible hand poses when the hand is heavily occluded orblurred. In videos, the movements of the hand allow us to observe various partsof the hand that may be occluded or blurred in a single frame. To adaptivelyleverage the visual clue before and after the occlusion or blurring for robusthand pose estimation, we propose the Deformer: a framework that implicitlyreasons about the relationship between hand parts within the same image(spatial dimension) and different timesteps (temporal dimension). We show thata naive application of the transformer self-attention mechanism is notsufficient because motion blur or occlusions in certain frames can lead toheavily distorted hand features and generate imprecise keys and queries. Toaddress this challenge, we incorporate a Dynamic Fusion Module into Deformer,which predicts the deformation of the hand and warps the hand mesh predictionsfrom nearby frames to explicitly support the current frame estimation.Furthermore, we have observed that errors are unevenly distributed acrossdifferent hand parts, with vertices around fingertips having disproportionatelyhigher errors than those around the palm. We mitigate this issue by introducinga new loss function called maxMSE that automatically adjusts the weight ofevery vertex to focus the model on critical hand parts. Extensive experimentsshow that our method significantly outperforms state-of-the-art methods by 10%,and is more robust to occlusions (over 14%).</description><author>Qichen Fu, Xingyu Liu, Ran Xu, Juan Carlos Niebles, Kris M. Kitani</author><pubDate>Fri, 18 Aug 2023 02:20:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04991v2</guid></item><item><title>HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.14581v2</link><description>2D-to-3D human pose lifting is fundamental for 3D human pose estimation(HPE). Graph Convolutional Network (GCN) has been proven inherently suitable tomodel the human skeletal topology. However, current GCN-based 3D HPE methodsupdate the node features by aggregating their neighbors' information withoutconsidering the interaction of joints in different motion patterns. Althoughsome studies import limb information to learn the movement patterns, the latentsynergies among joints, such as maintaining balance in the motion are seldominvestigated. We propose a hop-wise GraphFormer with intragroup jointrefinement (HopFIR) to tackle the 3D HPE problem. The HopFIR mainly consists ofa novel Hop-wise GraphFormer(HGF) module and an Intragroup JointRefinement(IJR) module which leverages the prior limb information forperipheral joints refinement. The HGF module groups the joints by $k$-hopneighbors and utilizes a hop-wise transformer-like attention mechanism amongthese groups to discover latent joint synergy. Extensive experimental resultsshow that HopFIR outperforms the SOTA methods with a large margin (on theHuman3.6M dataset, the mean per joint position error (MPJPE) is 32.67mm).Furthermore, it is also demonstrated that previous SOTA GCN-based methods canbenefit from the proposed hop-wise attention mechanism efficiently withsignificant performance promotion, such as SemGCN and MGCN are improved by 8.9%and 4.5%, respectively.</description><author>Kai Zhai, Qiang Nie, Bo Ouyang, Xiang Li, ShanLin Yang</author><pubDate>Tue, 18 Jul 2023 17:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14581v2</guid></item></channel></rss>