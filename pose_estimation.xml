<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivpose estimation</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 21 Jun 2023 06:00:48 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training</title><link>http://arxiv.org/abs/2206.11215v4</link><description>We consider a certifiable object pose estimation problem, where -- given apartial point cloud of an object -- the goal is to not only estimate the objectpose, but also to provide a certificate of correctness for the resultingestimate. Our first contribution is a general theory of certification forend-to-end perception models. In particular, we introduce the notion of$\zeta$-correctness, which bounds the distance between an estimate and theground truth. We show that $\zeta$-correctness can be assessed by implementingtwo certificates: (i) a certificate of observable correctness, that asserts ifthe model output is consistent with the input data and prior information, (ii)a certificate of non-degeneracy, that asserts whether the input data issufficient to compute a unique estimate. Our second contribution is to applythis theory and design a new learning-based certifiable pose estimator. Wepropose C-3PO, a semantic-keypoint-based pose estimation model, augmented withthe two certificates, to solve the certifiable pose estimation problem. C-3POalso includes a keypoint corrector, implemented as a differentiableoptimization layer, that can correct large detection errors (e.g. due to thesim-to-real gap). Our third contribution is a novel self-supervised trainingapproach that uses our certificate of observable correctness to provide thesupervisory signal to C-3PO during training. In it, the model trains only onthe observably correct input-output pairs, in each training iteration. Astraining progresses, we see that the observably correct input-output pairsgrow, eventually reaching near 100% in many cases. Our experiments show that(i) standard semantic-keypoint-based methods outperform more recentalternatives, (ii) C-3PO further improves performance and significantlyoutperforms all the baselines, and (iii) C-3PO's certificates are able todiscern correct pose estimates.</description><author>Rajat Talak, Lisa Peng, Luca Carlone</author><pubDate>Fri, 28 Apr 2023 20:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11215v4</guid></item><item><title>A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training</title><link>http://arxiv.org/abs/2302.06019v2</link><description>Real-world robotics applications demand object pose estimation methods thatwork reliably across a variety of scenarios. Modern learning-based approachesrequire large labeled datasets and tend to perform poorly outside the trainingdomain. Our first contribution is to develop a robust corrector module thatcorrects pose estimates using depth information, thus enabling existing methodsto better generalize to new test domains; the corrector operates on semantickeypoints (but is also applicable to other pose estimators) and is fullydifferentiable. Our second contribution is an ensemble self-training approachthat simultaneously trains multiple pose estimators in a self-supervisedmanner. Our ensemble self-training architecture uses the robust corrector torefine the output of each pose estimator; then, it evaluates the quality of theoutputs using observable correctness certificates; finally, it uses theobservably correct outputs for further training, without requiring externalsupervision. As an additional contribution, we propose small improvements to aregression-based keypoint detection architecture, to enhance its robustness tooutliers; these improvements include a robust pooling scheme and a robustcentroid computation. Experiments on the YCBV and TLESS datasets show theproposed ensemble self-training outperforms fully supervised baselines whilenot requiring 3D annotations on real data.</description><author>Jingnan Shi, Rajat Talak, Dominic Maggio, Luca Carlone</author><pubDate>Thu, 11 May 2023 19:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06019v2</guid></item><item><title>Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video</title><link>http://arxiv.org/abs/2303.08475v2</link><description>Temporal modeling is crucial for multi-frame human pose estimation. Mostexisting methods directly employ optical flow or deformable convolution topredict full-spectrum motion fields, which might incur numerous irrelevantcues, such as a nearby person or background. Without further efforts toexcavate meaningful motion priors, their results are suboptimal, especially incomplicated spatiotemporal interactions. On the other hand, the temporaldifference has the ability to encode representative motion information whichcan potentially be valuable for pose estimation but has not been fullyexploited. In this paper, we present a novel multi-frame human pose estimationframework, which employs temporal differences across frames to model dynamiccontexts and engages mutual information objectively to facilitate useful motioninformation disentanglement. To be specific, we design a multi-stage TemporalDifference Encoder that performs incremental cascaded learning conditioned onmulti-stage feature difference sequences to derive informative motionrepresentation. We further propose a Representation Disentanglement module fromthe mutual information perspective, which can grasp discriminativetask-relevant motion signals by explicitly defining useful and noisyconstituents of the raw motion features and minimizing their mutualinformation. These place us to rank No.1 in the Crowd Pose Estimation inComplex Events Challenge on benchmark dataset HiEve, and achievestate-of-the-art performance on three benchmarks PoseTrack2017, PoseTrack2018,and PoseTrack21.</description><author>Runyang Feng, Yixing Gao, Xueqing Ma, Tze Ho Elden Tse, Hyung Jin Chang</author><pubDate>Mon, 08 May 2023 14:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08475v2</guid></item><item><title>Perspective-1-Ellipsoid: Formulation, Analysis and Solutions of the Camera Pose Estimation Problem from One Ellipse-Ellipsoid Correspondence</title><link>http://arxiv.org/abs/2208.12513v3</link><description>In computer vision, camera pose estimation from correspondences between 3Dgeometric entities and their projections into the image has been a widelyinvestigated problem. Although most state-of-the-art methods exploit low-levelprimitives such as points or lines, the emergence of very effective CNN-basedobject detectors in the recent years has paved the way to the use ofhigher-level features carrying semantically meaningful information. Pioneeringworks in that direction have shown that modelling 3D objects by ellipsoids and2D detections by ellipses offers a convenient manner to link 2D and 3D data.However, the mathematical formalism most often used in the related litteraturedoes not enable to easily distinguish ellipsoids and ellipses from otherquadrics and conics, leading to a loss of specificity potentially detrimentalin some developments. Moreover, the linearization process of the projectionequation creates an over-representation of the camera parameters, also possiblycausing an efficiency loss. In this paper, we therefore introduce anellipsoid-specific theoretical framework and demonstrate its beneficialproperties in the context of pose estimation. More precisely, we first showthat the proposed formalism enables to reduce the pose estimation problem to aposition or orientation-only estimation problem in which the remaining unknownscan be derived in closed-form. Then, we demonstrate that it can be furtherreduced to a 1 Degree-of-Freedom (1DoF) problem and provide the analyticalderivations of the pose as a function of that unique scalar unknown. Weillustrate our theoretical considerations by visual examples and include adiscussion on the practical aspects. Finally, we release this paper along withthe corresponding source code in order to contribute towards more efficientresolutions of ellipsoid-related pose estimation problems.</description><author>Vincent Gaudilli√®re, Gilles Simon, Marie-Odile Berger</author><pubDate>Wed, 14 Jun 2023 13:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12513v3</guid></item><item><title>GenPose: Generative Category-level Object Pose Estimation via Diffusion Models</title><link>http://arxiv.org/abs/2306.10531v1</link><description>Object pose estimation plays a vital role in embodied AI and computer vision,enabling intelligent agents to comprehend and interact with their surroundings.Despite the practicality of category-level pose estimation, current approachesencounter challenges with partially observed point clouds, known as themultihypothesis issue. In this study, we propose a novel solution by reframingcategorylevel object pose estimation as conditional generative modeling,departing from traditional point-to-point regression. Leveraging score-baseddiffusion models, we estimate object poses by sampling candidates from thediffusion model and aggregating them through a two-step process: filtering outoutliers via likelihood estimation and subsequently mean-pooling the remainingcandidates. To avoid the costly integration process when estimating thelikelihood, we introduce an alternative method that trains an energy-basedmodel from the original score-based model, enabling end-to-end likelihoodestimation. Our approach achieves state-of-the-art performance on the REAL275dataset, surpassing 50% and 60% on strict 5d2cm and 5d5cm metrics,respectively. Furthermore, our method demonstrates strong generalizability tonovel categories sharing similar symmetric properties without fine-tuning andcan readily adapt to object pose tracking tasks, yielding comparable results tothe current state-of-the-art baselines.</description><author>Jiyao Zhang, Mingdong Wu, Hao Dong</author><pubDate>Sun, 18 Jun 2023 12:45:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10531v1</guid></item><item><title>SPAC-Net: Synthetic Pose-aware Animal ControlNet for Enhanced Pose Estimation</title><link>http://arxiv.org/abs/2305.17845v1</link><description>Animal pose estimation has become a crucial area of research, but thescarcity of annotated data is a significant challenge in developing accuratemodels. Synthetic data has emerged as a promising alternative, but itfrequently exhibits domain discrepancies with real data. Style transferalgorithms have been proposed to address this issue, but they suffer frominsufficient spatial correspondence, leading to the loss of label information.In this work, we present a new approach called Synthetic Pose-aware AnimalControlNet (SPAC-Net), which incorporates ControlNet into the previouslyproposed Prior-Aware Synthetic animal data generation (PASyn) pipeline. Weleverage the plausible pose data generated by the Variational Auto-Encoder(VAE)-based data generation pipeline as input for the ControlNetHolistically-nested Edge Detection (HED) boundary task model to generatesynthetic data with pose labels that are closer to real data, making itpossible to train a high-precision pose estimation network without the need forreal data. In addition, we propose the Bi-ControlNet structure to separatelydetect the HED boundary of animals and backgrounds, improving the precision andstability of the generated data. Using the SPAC-Net pipeline, we generatesynthetic zebra and rhino images and test them on the AP10K real dataset,demonstrating superior performance compared to using only real images orsynthetic data generated by other methods. Our work demonstrates the potentialfor synthetic data to overcome the challenge of limited annotated data inanimal pose estimation.</description><author>Le Jiang, Sarah Ostadabbas</author><pubDate>Mon, 29 May 2023 02:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17845v1</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v3</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Wed, 17 May 2023 09:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v3</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v1</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Fri, 12 May 2023 10:52:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v1</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v2</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Mon, 15 May 2023 16:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v2</guid></item><item><title>CHSEL: Producing Diverse Plausible Pose Estimates from Contact and Free Space Data</title><link>http://arxiv.org/abs/2305.08042v1</link><description>This paper proposes a novel method for estimating the set of plausible posesof a rigid object from a set of points with volumetric information, such aswhether each point is in free space or on the surface of the object. Inparticular, we study how pose can be estimated from force and tactile dataarising from contact. Using data derived from contact is challenging because itis inherently less information-dense than visual data, and thus the poseestimation problem is severely under-constrained when there are few contacts.Rather than attempting to estimate the true pose of the object, which is nottractable without a large number of contacts, we seek to estimate a plausibleset of poses which obey the constraints imposed by the sensor data. Existingmethods struggle to estimate this set because they are either designed forsingle pose estimates or require informative priors to be effective. Ourapproach to this problem, Constrained pose Hypothesis Set Elimination (CHSEL),has three key attributes: 1) It considers volumetric information, which allowsus to account for known free space; 2) It uses a novel differentiablevolumetric cost function to take advantage of powerful gradient-basedoptimization tools; and 3) It uses methods from the Quality Diversity (QD)optimization literature to produce a diverse set of high-quality poses. To ourknowledge, QD methods have not been used previously for pose registration. Wealso show how to update our plausible pose estimates online as more data isgathered by the robot. Our experiments suggest that CHSEL shows largeperformance improvements over several baseline methods for both simulated andreal-world data.</description><author>Sheng Zhong, Nima Fazeli, Dmitry Berenson</author><pubDate>Sun, 14 May 2023 02:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08042v1</guid></item><item><title>Neural Voting Field for Camera-Space 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2305.04328v1</link><description>We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.</description><author>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</author><pubDate>Sun, 07 May 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04328v1</guid></item><item><title>POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference</title><link>http://arxiv.org/abs/2305.15727v1</link><description>Despite the significant progress in six degrees-of-freedom (6DoF) object poseestimation, existing methods have limited applicability in real-world scenariosinvolving embodied agents and downstream 3D vision tasks. These limitationsmainly come from the necessity of 3D models, closed-category detection, and alarge number of densely annotated support views. To mitigate this issue, wepropose a general paradigm for object pose estimation, called Promptable ObjectPose Estimation (POPE). The proposed approach POPE enables zero-shot 6DoFobject pose estimation for any target object in any scene, while only a singlereference is adopted as the support view. To achieve this, POPE leverages thepower of the pre-trained large-scale 2D foundation model, employs a frameworkwith hierarchical feature representation and 3D geometry principles. Moreover,it estimates the relative camera pose between object prompts and the targetobject in new views, enabling both two-view and multi-view 6DoF pose estimationtasks. Comprehensive experimental results demonstrate that POPE exhibitsunrivaled robust performance in zero-shot settings, by achieving a significantreduction in the averaged Median Pose Error by 52.38% and 50.47% on the LINEMODand OnePose datasets, respectively. We also conduct more challenging testingsin causally captured images (see Figure 1), which further demonstrates therobustness of POPE. Project page can be found withhttps://paulpanwang.github.io/POPE/.</description><author>Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Dejia Xu, Hanwen Jiang, Zhangyang Wang</author><pubDate>Thu, 25 May 2023 06:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15727v1</guid></item><item><title>Generalizable Pose Estimation Using Implicit Scene Representations</title><link>http://arxiv.org/abs/2305.17252v1</link><description>6-DoF pose estimation is an essential component of robotic manipulationpipelines. However, it usually suffers from a lack of generalization to newinstances and object types. Most widely used methods learn to infer the objectpose in a discriminative setup where the model filters useful information toinfer the exact pose of the object. While such methods offer accurate poses,the model does not store enough information to generalize to new objects. Inthis work, we address the generalization capability of pose estimation usingmodels that contain enough information about the object to render it indifferent poses. We follow the line of work that inverts neural renderers toinfer the pose. We propose i-$\sigma$SRN to maximize the information flowingfrom the input pose to the rendered scene and invert them to infer the posegiven an input image. Specifically, we extend Scene Representation Networks(SRNs) by incorporating a separate network for density estimation and introducea new way of obtaining a weighted scene representation. We investigate severalways of initial pose estimates and losses for the neural renderer. Our finalevaluation shows a significant improvement in inference performance and speedcompared to existing approaches.</description><author>Vaibhav Saxena, Kamal Rahimi Malekshan, Linh Tran, Yotto Koga</author><pubDate>Fri, 26 May 2023 21:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17252v1</guid></item><item><title>Depth-based 6DoF Object Pose Estimation using Swin Transformer</title><link>http://arxiv.org/abs/2303.02133v2</link><description>Accurately estimating the 6D pose of objects is crucial for manyapplications, such as robotic grasping, autonomous driving, and augmentedreality. However, this task becomes more challenging in poor lightingconditions or when dealing with textureless objects. To address this issue,depth images are becoming an increasingly popular choice due to theirinvariance to a scene's appearance and the implicit incorporation of essentialgeometric characteristics. However, fully leveraging depth information toimprove the performance of pose estimation remains a difficult andunder-investigated problem. To tackle this challenge, we propose a novelframework called SwinDePose, that uses only geometric information from depthimages to achieve accurate 6D pose estimation. SwinDePose first calculates theangles between each normal vector defined in a depth image and the threecoordinate axes in the camera coordinate system. The resulting angles are thenformed into an image, which is encoded using Swin Transformer. Additionally, weapply RandLA-Net to learn the representations from point clouds. The resultingimage and point clouds embeddings are concatenated and fed into a semanticsegmentation module and a 3D keypoints localization module. Finally, weestimate 6D poses using a least-square fitting approach based on the targetobject's predicted semantic mask and 3D keypoints. In experiments on theLineMod and Occlusion LineMod datasets, SwinDePose outperforms existingstate-of-the-art methods for 6D object pose estimation using depth images. Thisdemonstrates the effectiveness of our approach and highlights its potential forimproving performance in real-world scenarios. Our code is athttps://github.com/zhujunli1993/SwinDePose.</description><author>Zhujun Li, Ioannis Stamos</author><pubDate>Thu, 27 Apr 2023 19:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02133v2</guid></item><item><title>You Only Look at One: Category-Level Object Representations for Pose Estimation From a Single Example</title><link>http://arxiv.org/abs/2305.12626v1</link><description>In order to meaningfully interact with the world, robot manipulators must beable to interpret objects they encounter. A critical aspect of thisinterpretation is pose estimation: inferring quantities that describe theposition and orientation of an object in 3D space. Most existing approaches topose estimation make limiting assumptions, often working only for specific,known object instances, or at best generalising to an object category usinglarge pose-labelled datasets. In this work, we present a method for achievingcategory-level pose estimation by inspection of just a single object from adesired category. We show that we can subsequently perform accurate poseestimation for unseen objects from an inspected category, and considerablyoutperform prior work by exploiting multi-view correspondences. We demonstratethat our method runs in real-time, enabling a robot manipulator equipped withan RGBD sensor to perform online 6D pose estimation for novel objects. Finally,we showcase our method in a continual learning setting, with a robot able todetermine whether objects belong to known categories, and if not, use activeperception to produce a one-shot category representation for subsequent poseestimation.</description><author>Walter Goodwin, Ioannis Havoutis, Ingmar Posner</author><pubDate>Mon, 22 May 2023 02:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12626v1</guid></item><item><title>IMP: Iterative Matching and Pose Estimation with Adaptive Pooling</title><link>http://arxiv.org/abs/2304.14837v1</link><description>Previous methods solve feature matching and pose estimation using a two-stageprocess by first finding matches and then estimating the pose. As they ignorethe geometric relationships between the two tasks, they focus on eitherimproving the quality of matches or filtering potential outliers, leading tolimited efficiency or accuracy. In contrast, we propose an iterative matchingand pose estimation framework (IMP) leveraging the geometric connectionsbetween the two tasks: a few good matches are enough for a roughly accuratepose estimation; a roughly accurate pose can be used to guide the matching byproviding geometric constraints. To this end, we implement a geometry-awarerecurrent attention-based module which jointly outputs sparse matches andcamera poses. Specifically, for each iteration, we first implicitly embedgeometric information into the module via a pose-consistency loss, allowing itto predict geometry-aware matches progressively. Second, we introduce an\textbf{e}fficient IMP, called EIMP, to dynamically discard keypoints withoutpotential matches, avoiding redundant updating and significantly reducing thequadratic time complexity of attention computation in transformers. Experimentson YFCC100m, Scannet, and Aachen Day-Night datasets demonstrate that theproposed method outperforms previous approaches in terms of accuracy andefficiency.</description><author>Fei Xue, Ignas Budvytis, Roberto Cipolla</author><pubDate>Fri, 28 Apr 2023 14:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14837v1</guid></item><item><title>IMP: Iterative Matching and Pose Estimation with Adaptive Pooling</title><link>http://arxiv.org/abs/2304.14837v2</link><description>Previous methods solve feature matching and pose estimation using a two-stageprocess by first finding matches and then estimating the pose. As they ignorethe geometric relationships between the two tasks, they focus on eitherimproving the quality of matches or filtering potential outliers, leading tolimited efficiency or accuracy. In contrast, we propose an iterative matchingand pose estimation framework (IMP) leveraging the geometric connectionsbetween the two tasks: a few good matches are enough for a roughly accuratepose estimation; a roughly accurate pose can be used to guide the matching byproviding geometric constraints. To this end, we implement a geometry-awarerecurrent attention-based module which jointly outputs sparse matches andcamera poses. Specifically, for each iteration, we first implicitly embedgeometric information into the module via a pose-consistency loss, allowing itto predict geometry-aware matches progressively. Second, we introduce an\textbf{e}fficient IMP, called EIMP, to dynamically discard keypoints withoutpotential matches, avoiding redundant updating and significantly reducing thequadratic time complexity of attention computation in transformers. Experimentson YFCC100m, Scannet, and Aachen Day-Night datasets demonstrate that theproposed method outperforms previous approaches in terms of accuracy andefficiency.</description><author>Fei Xue, Ignas Budvytis, Roberto Cipolla</author><pubDate>Sun, 11 Jun 2023 17:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14837v2</guid></item><item><title>HDFormer: High-order Directed Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.01825v2</link><description>Human pose estimation is a challenging task due to its structured datasequence nature. Existing methods primarily focus on pair-wise interaction ofbody joints, which is insufficient for scenarios involving overlapping jointsand rapidly changing poses. To overcome these issues, we introduce a novelapproach, the High-order Directed Transformer (HDFormer), which leverageshigh-order bone and joint relationships for improved pose estimation.Specifically, HDFormer incorporates both self-attention and high-orderattention to formulate a multi-order attention module. This module facilitatesfirst-order "joint$\leftrightarrow$joint", second-order"bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint"interactions, effectively addressing issues in complex and occlusion-heavysituations. In addition, modern CNN techniques are integrated into thetransformer-based architecture, balancing the trade-off between performance andefficiency. HDFormer significantly outperforms state-of-the-art (SOTA) modelson Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parametersand significantly lower computational costs. Moreover, HDFormer demonstratesbroad real-world applicability, enabling real-time, accurate 3D poseestimation. The source code is in https://github.com/hyer/HDFormer</description><author>Hanyuan Chen, Jun-Yan He, Wangmeng Xiang, Zhi-Qi Cheng, Wei Liu, Hanbing Liu, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 22 May 2023 07:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01825v2</guid></item><item><title>Regular Splitting Graph Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2305.05785v1</link><description>In human pose estimation methods based on graph convolutional architectures,the human skeleton is usually modeled as an undirected graph whose nodes arebody joints and edges are connections between neighboring joints. However, mostof these methods tend to focus on learning relationships between body joints ofthe skeleton using first-order neighbors, ignoring higher-order neighbors andhence limiting their ability to exploit relationships between distant joints.In this paper, we introduce a higher-order regular splitting graph network(RS-Net) for 2D-to-3D human pose estimation using matrix splitting inconjunction with weight and adjacency modulation. The core idea is to capturelong-range dependencies between body joints using multi-hop neighborhoods andalso to learn different modulation vectors for different body joints as well asa modulation matrix added to the adjacency matrix associated to the skeleton.This learnable modulation matrix helps adjust the graph structure by addingextra graph edges in an effort to learn additional connections between bodyjoints. Instead of using a shared weight matrix for all neighboring bodyjoints, the proposed RS-Net model applies weight unsharing before aggregatingthe feature vectors associated to the joints in order to capture the differentrelations between them. Experiments and ablations studies performed on twobenchmark datasets demonstrate the effectiveness of our model, achievingsuperior performance over recent state-of-the-art methods for 3D human poseestimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 09 May 2023 23:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05785v1</guid></item><item><title>EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With Kinematic Structure Priors</title><link>http://arxiv.org/abs/2306.09615v1</link><description>Transformer is popular in recent 3D human pose estimation, which utilizeslong-term modeling to lift 2D keypoints into the 3D space. However, currenttransformer-based methods do not fully exploit the prior knowledge of the humanskeleton provided by the kinematic structure. In this paper, we propose a noveltransformer-based model EvoPose to introduce the human body prior knowledge for3D human pose estimation effectively. Specifically, a Structural PriorsRepresentation (SPR) module represents human priors as structural featurescarrying rich body patterns, e.g. joint relationships. The structural featuresare interacted with 2D pose sequences and help the model to achieve moreinformative spatiotemporal features. Moreover, a Recursive Refinement (RR)module is applied to refine the 3D pose outputs by utilizing estimated resultsand further injects human priors simultaneously. Extensive experimentsdemonstrate the effectiveness of EvoPose which achieves a new state of the arton two most popular benchmarks, Human3.6M and MPI-INF-3DHP.</description><author>Yaqi Zhang, Yan Lu, Bin Liu, Zhiwei Zhao, Qi Chu, Nenghai Yu</author><pubDate>Fri, 16 Jun 2023 05:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09615v1</guid></item><item><title>Hybrid model for Single-Stage Multi-Person Pose Estimation</title><link>http://arxiv.org/abs/2305.01167v1</link><description>In general, human pose estimation methods are categorized into two approachesaccording to their architectures: regression (i.e., heatmap-free) andheatmap-based methods. The former one directly estimates precise coordinates ofeach keypoint using convolutional and fully-connected layers. Although thisapproach is able to detect overlapped and dense keypoints, unexpected resultscan be obtained by non-existent keypoints in a scene. On the other hand, thelatter one is able to filter the non-existent ones out by utilizing predictedheatmaps for each keypoint. Nevertheless, it suffers from quantization errorwhen obtaining the keypoint coordinates from its heatmaps. In addition, unlikethe regression one, it is difficult to distinguish densely placed keypoints inan image. To this end, we propose a hybrid model for single-stage multi-personpose estimation, named HybridPose, which mutually overcomes each drawback ofboth approaches by maximizing their strengths. Furthermore, we introduceself-correlation loss to inject spatial dependencies between keypointcoordinates and their visibility. Therefore, HybridPose is capable of not onlydetecting densely placed keypoints, but also filtering the non-existentkeypoints in an image. Experimental results demonstrate that proposedHybridPose exhibits the keypoints visibility without performance degradation interms of the pose estimation accuracy.</description><author>Jonghyun Kim, Bosang Kim, Hyotae Lee, Jungpyo Kim, Wonhyeok Im, Lanying Jin, Dowoo Kwon, Jungho Lee</author><pubDate>Tue, 02 May 2023 03:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01167v1</guid></item><item><title>Hybrid model for Single-Stage Multi-Person Pose Estimation</title><link>http://arxiv.org/abs/2305.01167v2</link><description>In general, human pose estimation methods are categorized into two approachesaccording to their architectures: regression (i.e., heatmap-free) andheatmap-based methods. The former one directly estimates precise coordinates ofeach keypoint using convolutional and fully-connected layers. Although thisapproach is able to detect overlapped and dense keypoints, unexpected resultscan be obtained by non-existent keypoints in a scene. On the other hand, thelatter one is able to filter the non-existent ones out by utilizing predictedheatmaps for each keypoint. Nevertheless, it suffers from quantization errorwhen obtaining the keypoint coordinates from its heatmaps. In addition, unlikethe regression one, it is difficult to distinguish densely placed keypoints inan image. To this end, we propose a hybrid model for single-stage multi-personpose estimation, named HybridPose, which mutually overcomes each drawback ofboth approaches by maximizing their strengths. Furthermore, we introduceself-correlation loss to inject spatial dependencies between keypointcoordinates and their visibility. Therefore, HybridPose is capable of not onlydetecting densely placed keypoints, but also filtering the non-existentkeypoints in an image. Experimental results demonstrate that proposedHybridPose exhibits the keypoints visibility without performance degradation interms of the pose estimation accuracy.</description><author>Jonghyun Kim, Bosang Kim, Hyotae Lee, Jungpyo Kim, Wonhyeok Im, Lanying Jin, Dowoo Kwon, Jungho Lee</author><pubDate>Mon, 19 Jun 2023 01:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01167v2</guid></item><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)</title><link>http://arxiv.org/abs/2305.15873v1</link><description>Addressing accuracy limitations and pose ambiguity in 6D object poseestimation from single RGB images presents a significant challenge,particularly due to object symmetries or occlusions. In response, we introducea novel score-based diffusion method applied to the $SE(3)$ group, marking thefirst application of diffusion models to $SE(3)$ within the image domain,specifically tailored for pose estimation tasks. Extensive evaluationsdemonstrate the method's efficacy in handling pose ambiguity, mitigatingperspective-induced ambiguity, and showcasing the robustness of our surrogateStein score formulation on $SE(3)$. This formulation not only improves theconvergence of Langevin dynamics but also enhances computational efficiency.Thus, we pioneer a promising strategy for 6D object pose estimation.</description><author>Tsu-Ching Hsiao, Hao-Wei Chen, Hsuan-Kung Yang, Chun-Yi Lee</author><pubDate>Thu, 25 May 2023 10:09:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15873v1</guid></item><item><title>Graph-CoVis: GNN-based Multi-view Panorama Global Pose Estimation</title><link>http://arxiv.org/abs/2304.13201v1</link><description>In this paper, we address the problem of wide-baseline camera pose estimationfrom a group of 360$^\circ$ panoramas under upright-camera assumption. Recentwork has demonstrated the merit of deep-learning for end-to-end direct relativepose regression in 360$^\circ$ panorama pairs [11]. To exploit the benefits ofmulti-view logic in a learning-based framework, we introduce Graph-CoVis, whichnon-trivially extends CoVisPose [11] from relative two-view to globalmulti-view spherical camera pose estimation. Graph-CoVis is a novel GraphNeural Network based architecture that jointly learns the co-visible structureand global motion in an end-to-end and fully-supervised approach. Using theZInD [4] dataset, which features real homes presenting wide-baselines,occlusion, and limited visual overlap, we show that our model performscompetitively to state-of-the-art approaches.</description><author>Negar Nejatishahidin, Will Hutchcroft, Manjunath Narayana, Ivaylo Boyadzhiev, Yuguang Li, Naji Khosravan, Jana Kosecka, Sing Bing Kang</author><pubDate>Wed, 26 Apr 2023 01:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13201v1</guid></item><item><title>MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model</title><link>http://arxiv.org/abs/2302.08751v2</link><description>One of the major challenges in multi-person pose estimation is instance-awarekeypoint estimation. Previous methods address this problem by leveraging anoff-the-shelf detector, heuristic post-grouping process or explicit instanceidentification process, hindering further improvements in the inference speedwhich is an important factor for practical applications. From the statisticalpoint of view, those additional processes for identifying instances arenecessary to bypass learning the high-dimensional joint distribution of humankeypoints, which is a critical factor for another major challenge, theocclusion scenario. In this work, we propose a novel framework of single-stageinstance-aware pose estimation by modeling the joint distribution of humankeypoints with a mixture density model, termed as MDPose. Our MDPose estimatesthe distribution of human keypoints' coordinates using a mixture density modelwith an instance-aware keypoint head consisting simply of 8 convolutionallayers. It is trained by minimizing the negative log-likelihood of the groundtruth keypoints. Also, we propose a simple yet effective training strategy,Random Keypoint Grouping (RKG), which significantly alleviates the underflowproblem leading to successful learning of relations between keypoints. OnOCHuman dataset, which consists of images with highly occluded people, ourMDPose achieves state-of-the-art performance by successfully learning thehigh-dimensional joint distribution of human keypoints. Furthermore, our MDPoseshows significant improvement in inference speed with a competitive accuracy onMS COCO, a widely-used human keypoint dataset, thanks to the proposed muchsimpler single-stage pipeline.</description><author>Seunghyeon Seo, Jaeyoung Yoo, Jihye Hwang, Nojun Kwak</author><pubDate>Mon, 08 May 2023 13:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08751v2</guid></item><item><title>ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation</title><link>http://arxiv.org/abs/2305.01618v1</link><description>We propose a new dataset and a novel approach to learning hand-objectinteraction priors for hand and articulated object pose estimation. We firstcollect a dataset using visual teleoperation, where the human operator candirectly play within a physical simulator to manipulate the articulatedobjects. We record the data and obtain free and accurate annotations on objectposes and contact information from the simulator. Our system only requires aniPhone to record human hand motion, which can be easily scaled up and largelylower the costs of data and annotation collection. With this data, we learn 3Dinteraction priors including a discriminator (in a GAN) capturing thedistribution of how object parts are arranged, and a diffusion model whichgenerates the contact regions on articulated objects, guiding the hand poseestimation. Such structural and contact priors can easily transfer toreal-world data with barely any domain gap. By using our data and learnedpriors, our method significantly improves the performance on joint hand andarticulated object poses estimation over the existing state-of-the-art methods.The project is available at https://zehaozhu.github.io/ContactArt/ .</description><author>Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang</author><pubDate>Tue, 02 May 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01618v1</guid></item><item><title>Vision-based Target Pose Estimation with Multiple Markers for the Perching of UAVs</title><link>http://arxiv.org/abs/2304.14838v1</link><description>Autonomous Nano Aerial Vehicles have been increasingly popular insurveillance and monitoring operations due to their efficiency andmaneuverability. Once a target location has been reached, drones do not have toremain active during the mission. It is possible for the vehicle to perch andstop its motors in such situations to conserve energy, as well as maintain astatic position in unfavorable flying conditions. In the perching targetestimation phase, the steady and accuracy of a visual camera with markers is asignificant challenge. It is rapidly detectable from afar when using a largemarker, but when the drone approaches, it quickly disappears as out of cameraview. In this paper, a vision-based target poses estimation method usingmultiple markers is proposed to deal with the above-mentioned problems. First,a perching target with a small marker inside a larger one is designed toimprove detection capability at wide and close ranges. Second, the relativeposes of the flying vehicle are calculated from detected markers using amonocular camera. Next, a Kalman filter is applied to provide a more stable andreliable pose estimation, especially when the measurement data is missing dueto unexpected reasons. Finally, we introduced an algorithm for merging theposes data from multi markers. The poses are then sent to the positioncontroller to align the drone and the marker's center and steer it to perch onthe target. The experimental results demonstrated the effectiveness andfeasibility of the adopted approach. The drone can perch successfully onto thecenter of the markers with the attached 25mm-diameter rounded magnet.</description><author>Truong-Dong Do, Nguyen Xuan-Mung, Sung-Kyung Hong</author><pubDate>Tue, 25 Apr 2023 17:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14838v1</guid></item><item><title>Human Body Pose Estimation for Gait Identification: A Comprehensive Survey of Datasets and Models</title><link>http://arxiv.org/abs/2305.13765v1</link><description>Person identification is a problem that has received substantial attention,particularly in security domains. Gait recognition is one of the mostconvenient approaches enabling person identification at a distance without theneed of high-quality images. There are several review studies addressing personidentification such as the utilization of facial images, silhouette images, andwearable sensor. Despite skeleton-based person identification gainingpopularity while overcoming the challenges of traditional approaches, existingsurvey studies lack the comprehensive review of skeleton-based approaches togait identification. We present a detailed review of the human pose estimationand gait analysis that make the skeleton-based approaches possible. The studycovers various types of related datasets, tools, methodologies, and evaluationmetrics with associated challenges, limitations, and application domains.Detailed comparisons are presented for each of these aspects withrecommendations for potential research and alternatives. A common trendthroughout this paper is the positive impact that deep learning techniques arebeginning to have on topics such as human pose estimation and gaitidentification. The survey outcomes might be useful for the related researchcommunity and other stakeholders in terms of performance analysis of existingmethodologies, potential research gaps, application domains, and possiblecontributions in the future.</description><author>Luke K. Topham, Wasiq Khan, Dhiya Al-Jumeily, Abir Hussain</author><pubDate>Tue, 23 May 2023 08:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13765v1</guid></item><item><title>A Probabilistic Relaxation of the Two-Stage Object Pose Estimation Paradigm</title><link>http://arxiv.org/abs/2306.00892v1</link><description>Existing object pose estimation methods commonly require a one-to-one pointmatching step that forces them to be separated into two consecutive stages:visual correspondence detection (e.g., by matching feature descriptors as partof a perception front-end) followed by geometric alignment (e.g., by optimizinga robust estimation objective for pointcloud registration orperspective-n-point). Instead, we propose a matching-free probabilisticformulation with two main benefits: i) it enables unified and concurrentoptimization of both visual correspondence and geometric alignment, and ii) itcan represent different plausible modes of the entire distribution of likelyposes. This in turn allows for a more graceful treatment of geometricperception scenarios where establishing one-to-one matches between points isconceptually ill-defined, such as textureless, symmetrical and/or occludedobjects and scenes where the correct pose is uncertain or there are multipleequally valid solutions.</description><author>Onur Beker</author><pubDate>Thu, 01 Jun 2023 17:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00892v1</guid></item><item><title>Interweaved Graph and Attention Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2304.14045v1</link><description>Despite substantial progress in 3D human pose estimation from a single-viewimage, prior works rarely explore global and local correlations, leading toinsufficient learning of human skeleton representations. To address this issue,we propose a novel Interweaved Graph and Attention Network (IGANet) that allowsbidirectional communications between graph convolutional networks (GCNs) andattentions. Specifically, we introduce an IGA module, where attentions areprovided with local information from GCNs and GCNs are injected with globalinformation from attentions. Additionally, we design a simple yet effectiveU-shaped multi-layer perceptron (uMLP), which can capture multi-granularityinformation for body joints. Extensive experiments on two popular benchmarkdatasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate ourproposed method.The results show that IGANet achieves state-of-the-artperformance on both datasets. Code is available athttps://github.com/xiu-cs/IGANet.</description><author>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li</author><pubDate>Thu, 27 Apr 2023 10:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14045v1</guid></item><item><title>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</title><link>http://arxiv.org/abs/2206.02257v3</link><description>In this survey, we present a systematic review of 3D hand pose estimationfrom the perspective of efficient annotation and learning. 3D hand poseestimation has been an important research area owing to its potential to enablevarious applications, such as video understanding, AR/VR, and robotics.However, the performance of models is tied to the quality and quantity ofannotated 3D hand poses. Under the status quo, acquiring such annotated 3D handposes is challenging, e.g., due to the difficulty of 3D annotation and thepresence of occlusion. To reveal this problem, we review the pros and cons ofexisting annotation methods classified as manual, synthetic-model-based,hand-sensor-based, and computational approaches. Additionally, we examinemethods for learning 3D hand poses when annotated data are scarce, includingself-supervised pretraining, semi-supervised learning, and domain adaptation.Based on the study of efficient annotation and learning, we further discusslimitations and possible future directions in this field.</description><author>Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato</author><pubDate>Wed, 26 Apr 2023 07:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02257v3</guid></item><item><title>LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</title><link>http://arxiv.org/abs/2306.05410v1</link><description>A critical obstacle preventing NeRF models from being deployed broadly in thewild is their reliance on accurate camera poses. Consequently, there is growinginterest in extending NeRF models to jointly optimize camera poses and scenerepresentation, which offers an alternative to off-the-shelf SfM pipelineswhich have well-understood failure modes. Existing approaches for unposed NeRFoperate under limited assumptions, such as a prior pose distribution or coarsepose initialization, making them less effective in a general setting. In thiswork, we propose a novel approach, LU-NeRF, that jointly estimates camera posesand neural radiance fields with relaxed assumptions on pose configuration. Ourapproach operates in a local-to-global manner, where we first optimize overlocal subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose andgeometry for this challenging few-shot task. The mini-scene poses are broughtinto a global reference frame through a robust pose synchronization step, wherea final global optimization of pose and scene can be performed. We show ourLU-NeRF pipeline outperforms prior attempts at unposed NeRF without makingrestrictive assumptions on the pose prior. This allows us to operate in thegeneral SE(3) pose setting, unlike the baselines. Our results also indicate ourmodel can be complementary to feature-based SfM pipelines as it comparesfavorably to COLMAP on low-texture and low-resolution images.</description><author>Zezhou Cheng, Carlos Esteves, Varun Jampani, Abhishek Kar, Subhransu Maji, Ameesh Makadia</author><pubDate>Thu, 08 Jun 2023 18:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05410v1</guid></item><item><title>Occlusion Robust 3D Human Pose Estimation with StridedPoseGraphFormer and Data Augmentation</title><link>http://arxiv.org/abs/2304.12069v1</link><description>Occlusion is an omnipresent challenge in 3D human pose estimation (HPE). Inspite of the large amount of research dedicated to 3D HPE, only a limitednumber of studies address the problem of occlusion explicitly. To fill thisgap, we propose to combine exploitation of spatio-temporal features withsynthetic occlusion augmentation during training to deal with occlusion. Tothis end, we build a spatio-temporal 3D HPE model, StridedPoseGraphFormer basedon graph convolution and transformers, and train it using occlusionaugmentation. Unlike the existing occlusion-aware methods, that are only testedfor limited occlusion, we extensively evaluate our method for varying degreesof occlusion. We show that our proposed method compares favorably with thestate-of-the-art (SoA). Our experimental results also reveal that in theabsence of any occlusion handling mechanism, the performance of SoA 3D HPEmethods degrades significantly when they encounter occlusion.</description><author>Soubarna Banik, Patricia Gscho√ümann, Alejandro Mendoza Garcia, Alois Knoll</author><pubDate>Mon, 24 Apr 2023 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12069v1</guid></item><item><title>Next-generation Surgical Navigation: Multi-view Marker-less 6DoF Pose Estimation of Surgical Instruments</title><link>http://arxiv.org/abs/2305.03535v1</link><description>State-of-the-art research of traditional computer vision is increasinglyleveraged in the surgical domain. A particular focus in computer-assistedsurgery is to replace marker-based tracking systems for instrument localizationwith pure image-based 6DoF pose estimation. However, the state of the art hasnot yet met the accuracy required for surgical navigation. In this context, wepropose a high-fidelity marker-less optical tracking system for surgicalinstrument localization. We developed a multi-view camera setup consisting ofstatic and mobile cameras and collected a large-scale RGB-D video dataset withdedicated synchronization and data fusions methods. Different state-of-the-artpose estimation methods were integrated into a deep learning pipeline andevaluated on multiple camera configurations. Furthermore, the performanceimpacts of different input modalities and camera positions, as well as trainingon purely synthetic data, were compared. The best model achieved an averageposition and orientation error of 1.3 mm and 1.0{\deg} for a surgical drill aswell as 3.8 mm and 5.2{\deg} for a screwdriver. These results significantlyoutperform related methods in the literature and are close to clinical-gradeaccuracy, demonstrating that marker-less tracking of surgical instruments isbecoming a feasible alternative to existing marker-based systems.</description><author>Jonas Hein, Nicola Cavalcanti, Daniel Suter, Lukas Zingg, Fabio Carrillo, Mazda Farshad, Marc Pollefeys, Nassir Navab, Philipp F√ºrnstahl</author><pubDate>Fri, 05 May 2023 14:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03535v1</guid></item><item><title>IMUPoser: Full-Body Pose Estimation using IMUs in Phones, Watches, and Earbuds</title><link>http://arxiv.org/abs/2304.12518v1</link><description>Tracking body pose on-the-go could have powerful uses in fitness, mobilegaming, context-aware virtual assistants, and rehabilitation. However, usersare unlikely to buy and wear special suits or sensor arrays to achieve thisend. Instead, in this work, we explore the feasibility of estimating body poseusing IMUs already in devices that many users own -- namely smartphones,smartwatches, and earbuds. This approach has several challenges, includingnoisy data from low-cost commodity IMUs, and the fact that the number ofinstrumentation points on a users body is both sparse and in flux. Our pipelinereceives whatever subset of IMU data is available, potentially from just asingle device, and produces a best-guess pose. To evaluate our model, wecreated the IMUPoser Dataset, collected from 10 participants wearing or holdingoff-the-shelf consumer devices and across a variety of activity contexts. Weprovide a comprehensive evaluation of our system, benchmarking it on both ourown and existing IMU datasets.</description><author>Vimal Mollyn, Riku Arakawa, Mayank Goel, Chris Harrison, Karan Ahuja</author><pubDate>Tue, 25 Apr 2023 03:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12518v1</guid></item><item><title>D3L: Decomposition of 3D Rotation and Lift from 2D Joint to 3D for Human Mesh Recovery</title><link>http://arxiv.org/abs/2306.06406v1</link><description>Existing methods for 3D human mesh recovery always directly estimate SMPLparameters, which involve both joint rotations and shape parameters. However,these methods present rotation semantic ambiguity, rotation error accumulation,and shape estimation overfitting, which also leads to errors in the estimatedpose. Additionally, these methods have not efficiently leveraged theadvancements in another hot topic, human pose estimation. To address theseissues, we propose a novel approach, Decomposition of 3D Rotation and Lift from2D Joint to 3D mesh (D3L). We disentangle 3D joint rotation into bone directionand bone twist direction so that the human mesh recovery task is broken downinto estimation of pose, twist, and shape, which can be handled independently.Then we design a 2D-to-3D lifting network for estimating twist direction and 3Djoint position from 2D joint position sequences and introduce a nonlinearoptimization method for fitting shape parameters and bone directions. Ourapproach can leverage human pose estimation methods, and avoid pose errorsintroduced by shape estimation overfitting. We conduct experiments on theHuman3.6M dataset and demonstrate improved performance compared to existingmethods by a large margin.</description><author>Xiaoyang Hao, Han Li, Jun Cheng, Lei Wang</author><pubDate>Sat, 10 Jun 2023 11:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06406v1</guid></item><item><title>Digital Twin-Based 3D Map Management for Edge-Assisted Mobile Augmented Reality</title><link>http://arxiv.org/abs/2305.16571v1</link><description>In this paper, we design a 3D map management scheme for edge-assisted mobileaugmented reality (MAR) to support the pose estimation of individual MARdevice, which uploads camera frames to an edge server. Our objective is tominimize the pose estimation uncertainty of the MAR device by periodicallyselecting a proper set of camera frames for uploading to update the 3D map. Toaddress the challenges of the dynamic uplink data rate and the time-varyingpose of the MAR device, we propose a digital twin (DT)-based approach to 3D mapmanagement. First, a DT is created for the MAR device, which emulates 3D mapmanagement based on predicting subsequent camera frames. Second, a model-basedreinforcement learning (MBRL) algorithm is developed, utilizing the datacollected from both the actual and the emulated data to manage the 3D map. Withextensive emulated data provided by the DT, the MBRL algorithm can quicklyprovide an adaptive map management policy in a highly dynamic environment.Simulation results demonstrate that the proposed DT-based 3D map managementoutperforms benchmark schemes by achieving lower pose estimation uncertaintyand higher data efficiency in dynamic environments.</description><author>Conghao Zhou, Jie Gao, Mushu Li, Nan Cheng, Xuemin Shen, Weihua Zhuang</author><pubDate>Fri, 26 May 2023 02:38:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16571v1</guid></item><item><title>Decomposed Human Motion Prior for Video Pose Estimation via Adversarial Training</title><link>http://arxiv.org/abs/2305.18743v1</link><description>Estimating human pose from video is a task that receives considerableattention due to its applicability in numerous 3D fields. The complexity ofprior knowledge of human body movements poses a challenge to neural networkmodels in the task of regressing keypoints. In this paper, we address thisproblem by incorporating motion prior in an adversarial way. Different fromprevious methods, we propose to decompose holistic motion prior to joint motionprior, making it easier for neural networks to learn from prior knowledgethereby boosting the performance on the task. We also utilize a novelregularization loss to balance accuracy and smoothness introduced by motionprior. Our method achieves 9\% lower PA-MPJPE and 29\% lower acceleration errorthan previous methods tested on 3DPW. The estimator proves its robustness byachieving impressive performance on in-the-wild dataset.</description><author>Wenshuo Chen, Xiang Zhou, Zhengdi Yu, Zhaoyu Zheng, Weixi Gu, Kai Zhang</author><pubDate>Tue, 30 May 2023 05:53:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18743v1</guid></item><item><title>Full-Body Articulated Human-Object Interaction</title><link>http://arxiv.org/abs/2212.10621v2</link><description>Fine-grained capturing of 3D HOI boosts human activity understanding andfacilitates downstream visual tasks, including action recognition, holisticscene reconstruction, and human motion synthesis. Despite its significance,existing works mostly assume that humans interact with rigid objects using onlya few body parts, limiting their scope. In this paper, we address thechallenging problem of f-AHOI, wherein the whole human bodies interact witharticulated objects, whose parts are connected by movable joints. We presentCHAIRS, a large-scale motion-captured f-AHOI dataset, consisting of 16.2 hoursof versatile interactions between 46 participants and 81 articulated and rigidsittable objects. CHAIRS provides 3D meshes of both humans and articulatedobjects during the entire interactive process, as well as realistic andphysically plausible full-body interactions. We show the value of CHAIRS withobject pose estimation. By learning the geometrical relationships in HOI, wedevise the very first model that leverage human pose estimation to tackle theestimation of articulated object poses and shapes during whole-bodyinteractions. Given an image and an estimated human pose, our model firstreconstructs the pose and shape of the object, then optimizes thereconstruction according to a learned interaction prior. Under both evaluationsettings (e.g., with or without the knowledge of objects'geometries/structures), our model significantly outperforms baselines. We hopeCHAIRS will promote the community towards finer-grained interactionunderstanding. We will make the data/code publicly available.</description><author>Nan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Zhiyuan zhang, Yixin Chen, He Wang, Yixin Zhu, Siyuan Huang</author><pubDate>Tue, 16 May 2023 20:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10621v2</guid></item><item><title>HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</title><link>http://arxiv.org/abs/2212.10428v4</link><description>Estimating the 6D pose of objects is a major 3D computer vision problem.Since the promising outcomes from instance-level approaches, research headsalso move towards category-level pose estimation for more practical applicationscenarios. However, unlike well-established instance-level pose datasets,available category-level datasets lack annotation quality and provided posequantity. We propose the new category-level 6D pose dataset HouseCat6Dfeaturing 1) Multi-modality of Polarimetric RGB and Depth (RGBD+P), 2) Highlydiverse 194 objects of 10 household object categories including 2photometrically challenging categories, 3) High-quality pose annotation with anerror range of only 1.35 mm to 1.74 mm, 4) 41 large-scale scenes with extensiveviewpoint coverage and occlusions, 5) Checkerboard-free environment throughoutthe entire scene, and 6) Additionally annotated dense 6D parallel-jaw grasps.Furthermore, we also provide benchmark results of state-of-the-artcategory-level pose estimation networks.</description><author>HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp, Guangyao Zhai, Hannah Schieber, Giulia Rizzoli, Pengyuan Wang, Hongcheng Zhao, Lorenzo Garattoni, Sven Meier, Daniel Roth, Nassir Navab, Benjamin Busam</author><pubDate>Wed, 26 Apr 2023 11:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10428v4</guid></item><item><title>Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction</title><link>http://arxiv.org/abs/2305.09510v1</link><description>Robotic manipulation systems operating in complex environments rely onperception systems that provide information about the geometry (pose and 3Dshape) of the objects in the scene along with other semantic information suchas object labels. This information is then used for choosing the feasiblegrasps on relevant objects. In this paper, we present a novel method to providethis geometric and semantic information of all objects in the scene as well asfeasible grasps on those objects simultaneously. The main advantage of ourmethod is its speed as it avoids sequential perception and grasp planningsteps. With detailed quantitative analysis, we show that our method deliverscompetitive performance compared to the state-of-the-art dedicated methods forobject shape, pose, and grasp predictions while providing fast inference at 30frames per second speed.</description><author>Shubham Agrawal, Nikhil Chavan-Dafle, Isaac Kasahara, Selim Engin, Jinwook Huh, Volkan Isler</author><pubDate>Tue, 16 May 2023 16:03:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09510v1</guid></item><item><title>Event Camera-based Visual Odometry for Dynamic Motion Tracking of a Legged Robot Using Adaptive Time Surface</title><link>http://arxiv.org/abs/2305.08962v1</link><description>Our paper proposes a direct sparse visual odometry method that combines eventand RGB-D data to estimate the pose of agile-legged robots during dynamiclocomotion and acrobatic behaviors. Event cameras offer high temporalresolution and dynamic range, which can eliminate the issue of blurred RGBimages during fast movements. This unique strength holds a potential foraccurate pose estimation of agile-legged robots, which has been a challengingproblem to tackle. Our framework leverages the benefits of both RGB-D and eventcameras to achieve robust and accurate pose estimation, even during dynamicmaneuvers such as jumping and landing a quadruped robot, the Mini-Cheetah. Ourmajor contributions are threefold: Firstly, we introduce an adaptive timesurface (ATS) method that addresses the whiteout and blackout issue inconventional time surfaces by formulating pixel-wise decay rates based on scenecomplexity and motion speed. Secondly, we develop an effective pixel selectionmethod that directly samples from event data and applies sample filteringthrough ATS, enabling us to pick pixels on distinct features. Lastly, wepropose a nonlinear pose optimization formula that simultaneously performs3D-2D alignment on both RGB-based and event-based maps and images, allowing thealgorithm to fully exploit the benefits of both data streams. We extensivelyevaluate the performance of our framework on both public datasets and our ownquadruped robot dataset, demonstrating its effectiveness in accuratelyestimating the pose of agile robots during dynamic movements.</description><author>Shifan Zhu, Zhipeng Tang, Michael Yang, Erik Learned-Miller, Donghyun Kim</author><pubDate>Mon, 15 May 2023 20:03:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08962v1</guid></item><item><title>Shape-based pose estimation for automatic standard views of the knee</title><link>http://arxiv.org/abs/2305.16717v1</link><description>Surgical treatment of complicated knee fractures is guided by real-timeimaging using a mobile C-arm. Immediate and continuous control is achieved via2D anatomy-specific standard views that correspond to a specific C-arm poserelative to the patient positioning, which is currently determined manually,following a trial-and-error approach at the cost of time and radiation dose.The characteristics of the standard views of the knee suggests that the shapeinformation of individual bones could guide an automatic positioning procedure,reducing time and the amount of unnecessary radiation during C-arm positioning.To fully automate the C-arm positioning task during knee surgeries, we proposea complete framework that enables (1) automatic laterality and standard viewclassification and (2) automatic shape-based pose regression toward the desiredstandard view based on a single initial X-ray. A suitable shape representationis proposed to incorporate semantic information into the pose regressionpipeline. The pipeline is designed to handle two distinct standard viewssimultaneously. Experiments were conducted to assess the performance of theproposed system on 3528 synthetic and 1386 real X-rays for the a.-p. andlateral standard. The view/laterality classificator resulted in an accuracy of100\%/98\% on the simulated and 99\%/98\% on the real X-rays. The poseregression performance was$d\theta_{a.-p}=5.8\pm3.3\degree,\,d\theta_{lateral}=3.7\pm2.0\degree$ on thesimulated data and$d\theta_{a.-p}=7.4\pm5.0\degree,\,d\theta_{lateral}=8.4\pm5.4\degree$ on thereal data outperforming intensity-based pose regression.</description><author>Lisa Kausch, Sarina Thomas, Holger Kunze, Jan Siad El Barbari, Klaus Maier-Hein</author><pubDate>Fri, 26 May 2023 09:03:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16717v1</guid></item><item><title>Optimal and Robust Category-level Perception: Object Pose and Shape Estimation from 2D and 3D Semantic Keypoints</title><link>http://arxiv.org/abs/2206.12498v2</link><description>We consider a category-level perception problem, where one is given 2D or 3Dsensor data picturing an object of a given category (e.g., a car), and has toreconstruct the 3D pose and shape of the object despite intra-class variability(i.e., different car models have different shapes). We consider an active shapemodel, where -for an object category- we are given a library of potential CADmodels describing objects in that category, and we adopt a standard formulationwhere pose and shape are estimated from 2D or 3D keypoints via non-convexoptimization. Our first contribution is to develop PACE3D* and PACE2D*, thefirst certifiably optimal solvers for pose and shape estimation using 3D and 2Dkeypoints, respectively. Both solvers rely on the design of tight (i.e., exact)semidefinite relaxations. Our second contribution is to develop outlier-robustversions of both solvers, named PACE3D# and PACE2D#. Towards this goal, wepropose ROBIN, a general graph-theoretic framework to prune outliers, whichuses compatibility hypergraphs to model measurements' compatibility. We showthat in category-level perception problems these hypergraphs can be built fromthe winding orders of the keypoints (in 2D) or their convex hulls (in 3D), andmany outliers can be filtered out via maximum hyperclique computation. The lastcontribution is an extensive experimental evaluation. Besides providing anablation study on simulated datasets and on the PASCAL3D+ dataset, we combineour solver with a deep keypoint detector, and show that PACE3D# improves overthe state of the art in vehicle pose estimation in the ApolloScape datasets,and its runtime is compatible with practical applications. We release our codeat https://github.com/MIT-SPARK/PACE.</description><author>Jingnan Shi, Heng Yang, Luca Carlone</author><pubDate>Mon, 15 May 2023 04:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12498v2</guid></item><item><title>Learning to Estimate 6DoF Pose from Limited Data: A Few-Shot, Generalizable Approach using RGB Images</title><link>http://arxiv.org/abs/2306.07598v1</link><description>The accurate estimation of six degrees-of-freedom (6DoF) object poses isessential for many applications in robotics and augmented reality. However,existing methods for 6DoF pose estimation often depend on CAD templates ordense support views, restricting their usefulness in realworld situations. Inthis study, we present a new cascade framework named Cas6D for few-shot 6DoFpose estimation that is generalizable and uses only RGB images. To address thefalse positives of target object detection in the extreme few-shot setting, ourframework utilizes a selfsupervised pre-trained ViT to learn robust featurerepresentations. Then, we initialize the nearest top-K pose candidates based onsimilarity score and refine the initial poses using feature pyramids toformulate and update the cascade warped feature volume, which encodes contextat increasingly finer scales. By discretizing the pose search range usingmultiple pose bins and progressively narrowing the pose search range in eachstage using predictions from the previous stage, Cas6D can overcome the largegap between pose candidates and ground truth poses, which is a common failuremode in sparse-view scenarios. Experimental results on the LINEMOD and GenMOPdatasets demonstrate that Cas6D outperforms state-of-the-art methods by 9.2%and 3.8% accuracy (Proj-5) under the 32-shot setting compared to OnePose++ andGen6D.</description><author>Panwang Pan, Zhiwen Fan, Brandon Y. Feng, Peihao Wang, Chenxin Li, Zhangyang Wang</author><pubDate>Tue, 13 Jun 2023 08:45:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07598v1</guid></item><item><title>NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation</title><link>http://arxiv.org/abs/2305.08590v1</link><description>With the progress of 3D human pose and shape estimation, state-of-the-artmethods can either be robust to occlusions or obtain pixel-aligned accuracy innon-occlusion cases. However, they cannot obtain robustness and mesh-imagealignment at the same time. In this work, we present NIKI (Neural InverseKinematics with Invertible Neural Network), which models bi-directional errorsto improve the robustness to occlusions and obtain pixel-aligned accuracy. NIKIcan learn from both the forward and inverse processes with invertible networks.In the inverse process, the model separates the error from the plausible 3Dpose manifold for a robust 3D human pose estimation. In the forward process, weenforce the zero-error boundary conditions to improve the sensitivity toreliable joint positions for better mesh-image alignment. Furthermore, NIKIemulates the analytical inverse kinematics algorithms with the twist-and-swingdecomposition for better interpretability. Experiments on standard andocclusion-specific benchmarks demonstrate the effectiveness of NIKI, where weexhibit robust and well-aligned results simultaneously. Code is available athttps://github.com/Jeff-sjtu/NIKI</description><author>Jiefeng Li, Siyuan Bian, Qi Liu, Jiasheng Tang, Fan Wang, Cewu Lu</author><pubDate>Mon, 15 May 2023 13:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08590v1</guid></item><item><title>Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares</title><link>http://arxiv.org/abs/2305.09527v1</link><description>We propose a differentiable nonlinear least squares framework to account foruncertainty in relative pose estimation from feature correspondences.Specifically, we introduce a symmetric version of the probabilistic normalepipolar constraint, and an approach to estimate the covariance of featurepositions by differentiating through the camera pose estimation procedure. Weevaluate our approach on synthetic, as well as the KITTI and EuRoC real-worlddatasets. On the synthetic dataset, we confirm that our learned covariancesaccurately approximate the true noise distribution. In real world experiments,we find that our approach consistently outperforms state-of-the-artnon-probabilistic and probabilistic approaches, regardless of the featureextraction algorithm of choice.</description><author>Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallabhula, Daniel Cremers</author><pubDate>Tue, 16 May 2023 16:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09527v1</guid></item><item><title>Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares</title><link>http://arxiv.org/abs/2305.09527v2</link><description>We propose a differentiable nonlinear least squares framework to account foruncertainty in relative pose estimation from feature correspondences.Specifically, we introduce a symmetric version of the probabilistic normalepipolar constraint, and an approach to estimate the covariance of featurepositions by differentiating through the camera pose estimation procedure. Weevaluate our approach on synthetic, as well as the KITTI and EuRoC real-worlddatasets. On the synthetic dataset, we confirm that our learned covariancesaccurately approximate the true noise distribution. In real world experiments,we find that our approach consistently outperforms state-of-the-artnon-probabilistic and probabilistic approaches, regardless of the featureextraction algorithm of choice.</description><author>Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallabhula, Daniel Cremers</author><pubDate>Thu, 18 May 2023 19:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09527v2</guid></item><item><title>Poses as Queries: Image-to-LiDAR Map Localization with Transformers</title><link>http://arxiv.org/abs/2305.04298v1</link><description>High-precision vehicle localization with commercial setups is a crucialtechnique for high-level autonomous driving tasks. Localization with amonocular camera in LiDAR map is a newly emerged approach that achievespromising balance between cost and accuracy, but estimating pose by findingcorrespondences between such cross-modal sensor data is challenging, therebydamaging the localization accuracy. In this paper, we address the problem byproposing a novel Transformer-based neural network to register 2D images into3D LiDAR map in an end-to-end manner. Poses are implicitly represented ashigh-dimensional feature vectors called pose queries and can be iterativelyupdated by interacting with the retrieved relevant information from cross-modelfeatures using attention mechanism in a proposed POse Estimator Transformer(POET) module. Moreover, we apply a multiple hypotheses aggregation method thatestimates the final poses by performing parallel optimization on multiplerandomly initialized pose queries to reduce the network uncertainty.Comprehensive analysis and experimental results on public benchmark concludethat the proposed image-to-LiDAR map localization network could achievestate-of-the-art performances in challenging cross-modal localization tasks.</description><author>Jinyu Miao, Kun Jiang, Yunlong Wang, Tuopu Wen, Zhongyang Xiao, Zheng Fu, Mengmeng Yang, Maolin Liu, Diange Yang</author><pubDate>Sun, 07 May 2023 15:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04298v1</guid></item><item><title>BU-CVKit: Extendable Computer Vision Framework for Species Independent Tracking and Analysis</title><link>http://arxiv.org/abs/2306.04736v1</link><description>A major bottleneck of interdisciplinary computer vision (CV) research is thelack of a framework that eases the reuse and abstraction of state-of-the-art CVmodels by CV and non-CV researchers alike. We present here BU-CVKit, a computervision framework that allows the creation of research pipelines with chainableProcessors. The community can create plugins of their work for the framework,hence improving the re-usability, accessibility, and exposure of their workwith minimal overhead. Furthermore, we provide MuSeqPose Kit, a user interfacefor the pose estimation package of BU-CVKit, which automatically scans forinstalled plugins and programmatically generates an interface for them based onthe metadata provided by the user. It also provides software support forstandard pose estimation features such as annotations, 3D reconstruction,reprojection, and camera calibration. Finally, we show examples of behavioralneuroscience pipelines created through the sample plugins created for ourframework.</description><author>Mahir Patel, Lucas Carstensen, Yiwen Gu, Michael E. Hasselmo, Margrit Betke</author><pubDate>Wed, 07 Jun 2023 20:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04736v1</guid></item><item><title>DeepRM: Deep Recurrent Matching for 6D Pose Refinement</title><link>http://arxiv.org/abs/2205.14474v5</link><description>Precise 6D pose estimation of rigid objects from RGB images is a critical butchallenging task in robotics, augmented reality and human-computer interaction.To address this problem, we propose DeepRM, a novel recurrent networkarchitecture for 6D pose refinement. DeepRM leverages initial coarse poseestimates to render synthetic images of target objects. The rendered images arethen matched with the observed images to predict a rigid transform for updatingthe previous pose estimate. This process is repeated to incrementally refinethe estimate at each iteration. The DeepRM architecture incorporates LSTM unitsto propagate information through each refinement step, significantly improvingoverall performance. In contrast to current 2-stage Perspective-n-Point basedsolutions, DeepRM is trained end-to-end, and uses a scalable backbone that canbe tuned via a single parameter for accuracy and efficiency. During training, amulti-scale optical flow head is added to predict the optical flow between theobserved and synthetic images. Optical flow prediction stabilizes the trainingprocess, and enforces the learning of features that are relevant to the task ofpose estimation. Our results demonstrate that DeepRM achieves state-of-the-artperformance on two widely accepted challenging datasets.</description><author>Alexander Avery, Andreas Savakis</author><pubDate>Fri, 16 Jun 2023 21:26:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.14474v5</guid></item><item><title>EgoHumans: An Egocentric 3D Multi-Human Benchmark</title><link>http://arxiv.org/abs/2305.16487v1</link><description>We present EgoHumans, a new multi-view multi-human video benchmark to advancethe state-of-the-art of egocentric human 3D pose estimation and tracking.Existing egocentric benchmarks either capture single subject or indoor-onlyscenarios, which limit the generalization of computer vision algorithms forreal-world applications. We propose a novel 3D capture setup to construct acomprehensive egocentric multi-human benchmark in the wild with annotations tosupport diverse tasks such as human detection, tracking, 2D/3D pose estimation,and mesh recovery. We leverage consumer-grade wearable camera-equipped glassesfor the egocentric view, which enables us to capture dynamic activities likeplaying soccer, fencing, volleyball, etc. Furthermore, our multi-view setupgenerates accurate 3D ground truth even under severe or complete occlusion. Thedataset consists of more than 125k egocentric images, spanning diverse sceneswith a particular focus on challenging and unchoreographed multi-humanactivities and fast-moving egocentric views. We rigorously evaluate existingstate-of-the-art methods and highlight their limitations in the egocentricscenario, specifically on multi-human tracking. To address such limitations, wepropose EgoFormer, a novel approach with a multi-stream transformerarchitecture and explicit 3D spatial reasoning to estimate and track the humanpose. EgoFormer significantly outperforms prior art by 13.6% IDF1 and 9.3 HOTAon the EgoHumans dataset.</description><author>Rawal Khirodkar, Aayush Bansal, Lingni Ma, Richard Newcombe, Minh Vo, Kris Kitani</author><pubDate>Thu, 25 May 2023 22:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16487v1</guid></item><item><title>BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives</title><link>http://arxiv.org/abs/2306.04166v2</link><description>Implicit neural representation has emerged as a powerful method forreconstructing 3D scenes from 2D images. Given a set of camera poses andassociated images, the models can be trained to synthesize novel, unseen views.In order to expand the use cases for implicit neural representations, we needto incorporate camera pose estimation capabilities as part of therepresentation learning, as this is necessary for reconstructing scenes fromreal-world video sequences where cameras are generally not being tracked.Existing approaches like COLMAP and, most recently, bundle-adjusting neuralradiance field methods often suffer from lengthy processing times. These delaysranging from hours to days, arise from laborious feature matching, hardwarelimitations, dense point sampling, and long training times required by amulti-layer perceptron structure with a large number of parameters. To addressthese challenges, we propose a framework called bundle-adjusting acceleratedneural graphics primitives (BAA-NGP). Our approach leverages acceleratedsampling and hash encoding to expedite both pose refinement/estimation and 3Dscene reconstruction. Experimental results demonstrate that our method achievesa more than 10 to 20 $\times$ speed improvement in novel view synthesiscompared to other bundle-adjusting neural radiance field methods withoutsacrificing the quality of pose estimation.</description><author>Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey Supikov, Michael Yip</author><pubDate>Fri, 09 Jun 2023 19:10:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04166v2</guid></item><item><title>BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives</title><link>http://arxiv.org/abs/2306.04166v1</link><description>Implicit neural representation has emerged as a powerful method forreconstructing 3D scenes from 2D images. Given a set of camera poses andassociated images, the models can be trained to synthesize novel, unseen views.In order to expand the use cases for implicit neural representations, we needto incorporate camera pose estimation capabilities as part of therepresentation learning, as this is necessary for reconstructing scenes fromreal-world video sequences where cameras are generally not being tracked.Existing approaches like COLMAP and, most recently, bundle-adjusting neuralradiance field methods often suffer from lengthy processing times. These delaysranging from hours to days, arise from laborious feature matching, hardwarelimitations, dense point sampling, and long training times required by amulti-layer perceptron structure with a large number of parameters. To addressthese challenges, we propose a framework called bundle-adjusting acceleratedneural graphics primitives (BAA-NGP). Our approach leverages acceleratedsampling and hash encoding to expedite both pose refinement/estimation and 3Dscene reconstruction. Experimental results demonstrate that our method achievesa more than 10 to 20 $\times$ speed improvement in novel view synthesiscompared to other bundle-adjusting neural radiance field methods withoutsacrificing the quality of pose estimation.</description><author>Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey Supikov, Michael Yip</author><pubDate>Wed, 07 Jun 2023 06:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04166v1</guid></item><item><title>Perpetual Humanoid Control for Real-time Simulated Avatars</title><link>http://arxiv.org/abs/2305.06456v1</link><description>We present a physics-based humanoid controller that achieves high-fidelitymotion imitation and fault-tolerant behavior in the presence of noisy input(e.g. pose estimates from video or generated from language) and unexpectedfalls. Our controller scales up to learning ten thousand motion clips withoutusing any external stabilizing forces and learns to naturally recover fromfail-state. Given reference motion, our controller can perpetually controlsimulated avatars without requiring resets. At its core, we propose theprogressive multiplicative control policy (PMCP), which dynamically allocatesnew network capacity to learn harder and harder motion sequences. PMCP allowsefficient scaling for learning from large-scale motion databases and adding newtasks, such as fail-state recovery, without catastrophic forgetting. Wedemonstrate the effectiveness of our controller by using it to imitate noisyposes from video-based pose estimators and language-based motion generators ina live and real-time multi-person avatar use case.</description><author>Zhengyi Luo, Jinkun Cao, Alexander Winkler, Kris Kitani, Weipeng Xu</author><pubDate>Wed, 10 May 2023 21:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06456v1</guid></item><item><title>Perpetual Humanoid Control for Real-time Simulated Avatars</title><link>http://arxiv.org/abs/2305.06456v2</link><description>We present a physics-based humanoid controller that achieves high-fidelitymotion imitation and fault-tolerant behavior in the presence of noisy input(e.g. pose estimates from video or generated from language) and unexpectedfalls. Our controller scales up to learning ten thousand motion clips withoutusing any external stabilizing forces and learns to naturally recover fromfail-state. Given reference motion, our controller can perpetually controlsimulated avatars without requiring resets. At its core, we propose theprogressive multiplicative control policy (PMCP), which dynamically allocatesnew network capacity to learn harder and harder motion sequences. PMCP allowsefficient scaling for learning from large-scale motion databases and adding newtasks, such as fail-state recovery, without catastrophic forgetting. Wedemonstrate the effectiveness of our controller by using it to imitate noisyposes from video-based pose estimators and language-based motion generators ina live and real-time multi-person avatar use case.</description><author>Zhengyi Luo, Jinkun Cao, Alexander Winkler, Kris Kitani, Weipeng Xu</author><pubDate>Wed, 24 May 2023 23:05:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06456v2</guid></item><item><title>On Correlated Knowledge Distillation for Monitoring Human Pose with Radios</title><link>http://arxiv.org/abs/2305.14829v1</link><description>In this work, we propose and develop a simple experimental testbed to studythe feasibility of a novel idea by coupling radio frequency (RF) sensingtechnology with Correlated Knowledge Distillation (CKD) theory towardsdesigning lightweight, near real-time and precise human pose monitoringsystems. The proposed CKD framework transfers and fuses pose knowledge from arobust "Teacher" model to a parameterized "Student" model, which can be apromising technique for obtaining accurate yet lightweight pose estimates. Toassure its efficacy, we implemented CKD for distilling logits in our integratedSoftware Defined Radio (SDR)-based experimental setup and investigated theRF-visual signal correlation. Our CKD-RF sensing technique is characterized bytwo modes -- a camera-fed Teacher Class Network (e.g., images, videos) with anSDR-fed Student Class Network (e.g., RF signals). Specifically, our CKD modeltrains a dual multi-branch teacher and student network by distilling and fusingknowledge bases. The resulting CKD models are then subsequently used toidentify the multimodal correlation and teach the student branch in reverse.Instead of simply aggregating their learnings, CKD training comprised multipleparallel transformations with the two domains, i.e., visual images and RFsignals. Once trained, our CKD model can efficiently preserve privacy andutilize the multimodal correlated logits from the two different neural networksfor estimating poses without using visual signals/video frames (by using onlythe RF signals).</description><author>Shiva Raj Pokhrel, Jonathan Kua, Deol Satish, Phil Williams, Arkady Zaslavsky, Seng W. Loke, Jinho Choi</author><pubDate>Wed, 24 May 2023 08:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14829v1</guid></item><item><title>On Correlated Knowledge Distillation for Monitoring Human Pose with Radios</title><link>http://arxiv.org/abs/2305.14829v2</link><description>In this work, we propose and develop a simple experimental testbed to studythe feasibility of a novel idea by coupling radio frequency (RF) sensingtechnology with Correlated Knowledge Distillation (CKD) theory towardsdesigning lightweight, near real-time and precise human pose monitoringsystems. The proposed CKD framework transfers and fuses pose knowledge from arobust "Teacher" model to a parameterized "Student" model, which can be apromising technique for obtaining accurate yet lightweight pose estimates. Toassure its efficacy, we implemented CKD for distilling logits in our integratedSoftware Defined Radio (SDR)-based experimental setup and investigated theRF-visual signal correlation. Our CKD-RF sensing technique is characterized bytwo modes -- a camera-fed Teacher Class Network (e.g., images, videos) with anSDR-fed Student Class Network (e.g., RF signals). Specifically, our CKD modeltrains a dual multi-branch teacher and student network by distilling and fusingknowledge bases. The resulting CKD models are then subsequently used toidentify the multimodal correlation and teach the student branch in reverse.Instead of simply aggregating their learnings, CKD training comprised multipleparallel transformations with the two domains, i.e., visual images and RFsignals. Once trained, our CKD model can efficiently preserve privacy andutilize the multimodal correlated logits from the two different neural networksfor estimating poses without using visual signals/video frames (by using onlythe RF signals).</description><author>Shiva Raj Pokhrel, Jonathan Kua, Deol Satish, Phil Williams, Arkady Zaslavsky, Seng W. Loke, Jinho Choi</author><pubDate>Tue, 30 May 2023 14:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14829v2</guid></item><item><title>MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Pose</title><link>http://arxiv.org/abs/2210.07181v2</link><description>We propose a generalizable neural radiance fields - MonoNeRF, that can betrained on large-scale monocular videos of moving in static scenes without anyground-truth annotations of depth and camera poses. MonoNeRF follows anAutoencoder-based architecture, where the encoder estimates the monocular depthand the camera pose, and the decoder constructs a Multiplane NeRFrepresentation based on the depth encoder feature, and renders the input frameswith the estimated camera. The learning is supervised by the reconstructionerror. Once the model is learned, it can be applied to multiple applicationsincluding depth estimation, camera pose estimation, and single-image novel viewsynthesis. More qualitative results are available at:https://oasisyang.github.io/mononerf .</description><author>Yang Fu, Ishan Misra, Xiaolong Wang</author><pubDate>Sun, 04 Jun 2023 08:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07181v2</guid></item><item><title>TAPE: Temporal Attention-based Probabilistic human pose and shape Estimation</title><link>http://arxiv.org/abs/2305.00181v1</link><description>Reconstructing 3D human pose and shape from monocular videos is awell-studied but challenging problem. Common challenges include occlusions, theinherent ambiguities in the 2D to 3D mapping and the computational complexityof video processing. Existing methods ignore the ambiguities of thereconstruction and provide a single deterministic estimate for the 3D pose. Inorder to address these issues, we present a Temporal Attention basedProbabilistic human pose and shape Estimation method (TAPE) that operates on anRGB video. More specifically, we propose to use a neural network to encodevideo frames to temporal features using an attention-based neural network.Given these features, we output a per-frame but temporally-informed probabilitydistribution for the human pose using Normalizing Flows. We show that TAPEoutperforms state-of-the-art methods in standard benchmarks and serves as aneffective video-based prior for optimization-based human pose and shapeestimation. Code is available at: https: //github.com/nikosvasilik/TAPE</description><author>Nikolaos Vasilikopoulos, Nikos Kolotouros, Aggeliki Tsoli, Antonis Argyros</author><pubDate>Sat, 29 Apr 2023 07:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00181v1</guid></item><item><title>HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation</title><link>http://arxiv.org/abs/2305.06968v1</link><description>Monocular 3D human pose and shape estimation is an ill-posed problem sincemultiple 3D solutions can explain a 2D image of a subject. Recent approachespredict a probability distribution over plausible 3D pose and shape parametersconditioned on the image. We show that these approaches exhibit a trade-offbetween three key properties: (i) accuracy - the likelihood of the ground-truth3D solution under the predicted distribution, (ii) sample-input consistency -the extent to which 3D samples from the predicted distribution match thevisible 2D image evidence, and (iii) sample diversity - the range of plausible3D solutions modelled by the predicted distribution. Our method, HuManiFlow,predicts simultaneously accurate, consistent and diverse distributions. We usethe human kinematic tree to factorise full body pose into ancestor-conditionedper-body-part pose distributions in an autoregressive manner. Per-body-partdistributions are implemented using normalising flows that respect the manifoldstructure of SO(3), the Lie group of per-body-part poses. We show thatill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,and employ only probabilistic training losses. Code is available at:https://github.com/akashsengupta1997/HuManiFlow.</description><author>Akash Sengupta, Ignas Budvytis, Roberto Cipolla</author><pubDate>Thu, 11 May 2023 17:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06968v1</guid></item><item><title>RelPose++: Recovering 6D Poses from Sparse-view Observations</title><link>http://arxiv.org/abs/2305.04926v1</link><description>We address the task of estimating 6D camera poses from sparse-view image sets(2-8 images). This task is a vital pre-processing stage for nearly allcontemporary (neural) reconstruction algorithms but remains challenging givensparse views, especially for objects with visual symmetries and texture-lesssurfaces. We build on the recent RelPose framework which learns a network thatinfers distributions over relative rotations over image pairs. We extend thisapproach in two key ways; first, we use attentional transformer layers toprocess multiple images jointly, since additional views of an object mayresolve ambiguous symmetries in any given image pair (such as the handle of amug that becomes visible in a third view). Second, we augment this network toalso report camera translations by defining an appropriate coordinate systemthat decouples the ambiguity in rotation estimation from translationprediction. Our final system results in large improvements in 6D poseprediction over prior art on both seen and unseen object categories and alsoenables pose estimation and 3D reconstruction for in-the-wild objects.</description><author>Amy Lin, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Mon, 08 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04926v1</guid></item><item><title>OmniDet: Surround View Cameras based Multi-task Visual Perception Network for Autonomous Driving</title><link>http://arxiv.org/abs/2102.07448v3</link><description>Surround View fisheye cameras are commonly deployed in automated driving for360\deg{} near-field sensing around the vehicle. This work presents amulti-task visual perception network on unrectified fisheye images to enablethe vehicle to sense its surrounding environment. It consists of six primarytasks necessary for an autonomous driving system: depth estimation, visualodometry, semantic segmentation, motion segmentation, object detection, andlens soiling detection. We demonstrate that the jointly trained model performsbetter than the respective single task versions. Our multi-task model has ashared encoder providing a significant computational advantage and hassynergized decoders where tasks support each other. We propose a novel camerageometry based adaptation mechanism to encode the fisheye distortion model bothat training and inference. This was crucial to enable training on the WoodScapedataset, comprised of data from different parts of the world collected by 12different cameras mounted on three different cars with different intrinsics andviewpoints. Given that bounding boxes is not a good representation fordistorted fisheye images, we also extend object detection to use a polygon withnon-uniformly sampled vertices. We additionally evaluate our model on standardautomotive datasets, namely KITTI and Cityscapes. We obtain thestate-of-the-art results on KITTI for depth estimation and pose estimationtasks and competitive performance on the other tasks. We perform extensiveablation studies on various architecture choices and task weightingmethodologies. A short video at https://youtu.be/xbSjZ5OfPes providesqualitative results.</description><author>Varun Ravi Kumar, Senthil Yogamani, Hazem Rashed, Ganesh Sistu, Christian Witt, Isabelle Leang, Stefan Milz, Patrick M√§der</author><pubDate>Tue, 06 Jun 2023 15:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.07448v3</guid></item><item><title>S-Graphs+: Real-time Localization and Mapping leveraging Hierarchical Representations</title><link>http://arxiv.org/abs/2212.11770v3</link><description>In this paper, we present an evolved version of Situational Graphs, whichjointly models in a single optimizable factor graph (1) a pose graph, as a setof robot keyframes comprising associated measurements and robot poses, and (2)a 3D scene graph, as a high-level representation of the environment thatencodes its different geometric elements with semantic attributes and therelational information between them. Specifically, our S-Graphs+ is a novel four-layered factor graph thatincludes: (1) a keyframes layer with robot pose estimates, (2) a walls layerrepresenting wall surfaces, (3) a rooms layer encompassing sets of wall planes,and (4) a floors layer gathering the rooms within a given floor level. Theabove graph is optimized in real-time to obtain a robust and accurate estimateof the robots pose and its map, simultaneously constructing and leveraginghigh-level information of the environment. To extract this high-levelinformation, we present novel room and floor segmentation algorithms utilizingthe mapped wall planes and free-space clusters. We tested S-Graphs+ on multiple datasets, including simulated and real dataof indoor environments from varying construction sites, and on a real publicdataset of several indoor office areas. On average over our datasets, S-Graphs+outperforms the accuracy of the second-best method by a margin of 10.67%, whileextending the robot situational awareness by a richer scene model. Moreover, wemake the software available as a docker file.</description><author>Hriday Bavle, Jose Luis Sanchez-Lopez, Muhammad Shaheer, Javier Civera, Holger Voos</author><pubDate>Fri, 26 May 2023 10:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.11770v3</guid></item><item><title>Robust 3D-aware Object Classification via Discriminative Render-and-Compare</title><link>http://arxiv.org/abs/2305.14668v1</link><description>In real-world applications, it is essential to jointly estimate the 3D objectpose and class label of objects, i.e., to perform 3D-aware classification.Whilecurrent approaches for either image classification or pose estimation can beextended to 3D-aware classification, we observe that they are inherentlylimited: 1) Their performance is much lower compared to the respectivesingle-task models, and 2) they are not robust in out-of-distribution (OOD)scenarios. Our main contribution is a novel architecture for 3D-awareclassification, which builds upon a recent work and performs comparably tosingle-task models while being highly robust. In our method, an object categoryis represented as a 3D cuboid mesh composed of feature vectors at each meshvertex. Using differentiable rendering, we estimate the 3D object pose byminimizing the reconstruction error between the mesh and the featurerepresentation of the target image. Object classification is then performed bycomparing the reconstruction losses across object categories. Notably, theneural texture of the mesh is trained in a discriminative manner to enhance theclassification performance while also avoiding local optima in thereconstruction loss. Furthermore, we show how our method and feed-forwardneural networks can be combined to scale the render-and-compare approach tolarger numbers of categories. Our experiments on PASCAL3D+, occluded-PASCAL3D+,and OOD-CV show that our method outperforms all baselines at 3D-awareclassification by a wide margin in terms of performance and robustness.</description><author>Artur Jesslen, Guofeng Zhang, Angtian Wang, Alan Yuille, Adam Kortylewski</author><pubDate>Wed, 24 May 2023 04:20:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14668v1</guid></item><item><title>Robust 3D-aware Object Classification via Discriminative Render-and-Compare</title><link>http://arxiv.org/abs/2305.14668v2</link><description>In real-world applications, it is essential to jointly estimate the 3D objectpose and class label of objects, i.e., to perform 3D-aware classification.Whilecurrent approaches for either image classification or pose estimation can beextended to 3D-aware classification, we observe that they are inherentlylimited: 1) Their performance is much lower compared to the respectivesingle-task models, and 2) they are not robust in out-of-distribution (OOD)scenarios. Our main contribution is a novel architecture for 3D-awareclassification, which builds upon a recent work and performs comparably tosingle-task models while being highly robust. In our method, an object categoryis represented as a 3D cuboid mesh composed of feature vectors at each meshvertex. Using differentiable rendering, we estimate the 3D object pose byminimizing the reconstruction error between the mesh and the featurerepresentation of the target image. Object classification is then performed bycomparing the reconstruction losses across object categories. Notably, theneural texture of the mesh is trained in a discriminative manner to enhance theclassification performance while also avoiding local optima in thereconstruction loss. Furthermore, we show how our method and feed-forwardneural networks can be combined to scale the render-and-compare approach tolarger numbers of categories. Our experiments on PASCAL3D+, occluded-PASCAL3D+,and OOD-CV show that our method outperforms all baselines at 3D-awareclassification by a wide margin in terms of performance and robustness.</description><author>Artur Jesslen, Guofeng Zhang, Angtian Wang, Alan Yuille, Adam Kortylewski</author><pubDate>Mon, 05 Jun 2023 18:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14668v2</guid></item><item><title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer</title><link>http://arxiv.org/abs/2303.09681v3</link><description>Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available athttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be releasedupon paper acceptance.</description><author>Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng</author><pubDate>Thu, 11 May 2023 00:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09681v3</guid></item><item><title>Towards 3D Face Reconstruction in Perspective Projection: Estimating 6DoF Face Pose from Monocular Image</title><link>http://arxiv.org/abs/2205.04126v2</link><description>In 3D face reconstruction, orthogonal projection has been widely employed tosubstitute perspective projection to simplify the fitting process. Thisapproximation performs well when the distance between camera and face is farenough. However, in some scenarios that the face is very close to camera ormoving along the camera axis, the methods suffer from the inaccuratereconstruction and unstable temporal fitting due to the distortion under theperspective projection. In this paper, we aim to address the problem ofsingle-image 3D face reconstruction under perspective projection. Specifically,a deep neural network, Perspective Network (PerspNet), is proposed tosimultaneously reconstruct 3D face shape in canonical space and learn thecorrespondence between 2D pixels and 3D points, by which the 6DoF (6 Degrees ofFreedom) face pose can be estimated to represent perspective projection.Besides, we contribute a large ARKitFace dataset to enable the training andevaluation of 3D face reconstruction solutions under the scenarios ofperspective projection, which has 902,724 2D facial images with ground-truth 3Dface mesh and annotated 6DoF pose parameters. Experimental results show thatour approach outperforms current state-of-the-art methods by a significantmargin. The code and data are available athttps://github.com/cbsropenproject/6dof_face.</description><author>Yueying Kao, Bowen Pan, Miao Xu, Jiangjing Lyu, Xiangyu Zhu, Yuanzhang Chang, Xiaobo Li, Zhen Lei</author><pubDate>Wed, 17 May 2023 12:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.04126v2</guid></item><item><title>HypLiLoc: Towards Effective LiDAR Pose Regression with Hyperbolic Fusion</title><link>http://arxiv.org/abs/2304.00932v2</link><description>LiDAR relocalization plays a crucial role in many fields, including robotics,autonomous driving, and computer vision. LiDAR-based retrieval from a databasetypically incurs high computation storage costs and can lead to globallyinaccurate pose estimations if the database is too sparse. On the other hand,pose regression methods take images or point clouds as inputs and directlyregress global poses in an end-to-end manner. They do not perform databasematching and are more computationally efficient than retrieval techniques. Wepropose HypLiLoc, a new model for LiDAR pose regression. We use two branchedbackbones to extract 3D features and 2D projection features, respectively. Weconsider multi-modal feature fusion in both Euclidean and hyperbolic spaces toobtain more effective feature representations. Experimental results indicatethat HypLiLoc achieves state-of-the-art performance in both outdoor and indoordatasets. We also conduct extensive ablation studies on the framework design,which demonstrate the effectiveness of multi-modal feature extraction andmulti-space embedding. Our code is released at:https://github.com/sijieaaa/HypLiLoc</description><author>Sijie Wang, Qiyu Kang, Rui She, Wei Wang, Kai Zhao, Yang Song, Wee Peng Tay</author><pubDate>Thu, 25 May 2023 13:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00932v2</guid></item><item><title>Adaptive Graphical Model Network for 2D Handpose Estimation</title><link>http://arxiv.org/abs/1909.08205v2</link><description>In this paper, we propose a new architecture called Adaptive Graphical ModelNetwork (AGMN) to tackle the task of 2D hand pose estimation from a monocularRGB image. The AGMN consists of two branches of deep convolutional neuralnetworks for calculating unary and pairwise potential functions, followed by agraphical model inference module for integrating unary and pairwise potentials.Unlike existing architectures proposed to combine DCNNs with graphical models,our AGMN is novel in that the parameters of its graphical model are conditionedon and fully adaptive to individual input images. Experiments show that ourapproach outperforms the state-of-the-art method used in 2D hand keypointsestimation by a notable margin on two public datasets. Code can be found athttps://github.com/deyingk/agmn.</description><author>Deying Kong, Yifei Chen, Haoyu Ma, Xiangyi Yan, Xiaohui Xie</author><pubDate>Fri, 28 Apr 2023 05:08:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/1909.08205v2</guid></item><item><title>Bullying10K: A Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition</title><link>http://arxiv.org/abs/2306.11546v1</link><description>The prevalence of violence in daily life poses significant threats toindividuals' physical and mental well-being. Using surveillance cameras inpublic spaces has proven effective in proactively deterring and preventing suchincidents. However, concerns regarding privacy invasion have emerged due totheir widespread deployment. To address the problem, we leverage Dynamic VisionSensors (DVS) cameras to detect violent incidents and preserve privacy since itcaptures pixel brightness variations instead of static imagery. We introducethe Bullying10K dataset, encompassing various actions, complex movements, andocclusions from real-life scenarios. It provides three benchmarks forevaluating different tasks: action recognition, temporal action localization,and pose estimation. With 10,000 event segments, totaling 12 billion events and255 GB of data, Bullying10K contributes significantly by balancing violencedetection and personal privacy persevering. And it also poses a challenge tothe neuromorphic dataset. It will serve as a valuable resource for training anddeveloping privacy-protecting video systems. The Bullying10K opens newpossibilities for innovative approaches in these domains.</description><author>Yiting Dong, Yang Li, Dongcheng Zhao, Guobin Shen, Yi Zeng</author><pubDate>Tue, 20 Jun 2023 14:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11546v1</guid></item><item><title>Error Estimation for Single-Image Human Body Mesh Reconstruction</title><link>http://arxiv.org/abs/2305.17245v1</link><description>Human pose and shape estimation methods continue to suffer in situationswhere one or more parts of the body are occluded. More importantly, thesemethods cannot express when their predicted pose is incorrect. This has seriousconsequences when these methods are used in human-robot interaction scenarios,where we need methods that can evaluate their predictions and flag situationswhere they might be wrong. This work studies this problem. We propose a methodthat combines information from OpenPose and SPIN -- two popular human pose andshape estimation methods -- to highlight regions on the predicted mesh that areleast reliable. We have evaluated the proposed approach on 3DPW, 3DOH, andHuman3.6M datasets, and the results demonstrate our model's effectiveness inidentifying inaccurate regions of the human body mesh. Our code is available athttps://github.com/Hamoon1987/meshConfidence.</description><author>Hamoon Jafarian, Faisal Qureshi</author><pubDate>Fri, 26 May 2023 21:18:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17245v1</guid></item><item><title>Tame a Wild Camera: In-the-Wild Monocular Camera Calibration</title><link>http://arxiv.org/abs/2306.10988v1</link><description>3D sensing for monocular in-the-wild images, e.g., depth estimation and 3Dobject detection, has become increasingly important. However, the unknownintrinsic parameter hinders their development and deployment. Previous methodsfor the monocular camera calibration rely on specific 3D objects or stronggeometry prior, such as using a checkerboard or imposing a Manhattan Worldassumption. This work solves the problem from the other perspective byexploiting the monocular 3D prior. Our method is assumption-free and calibratesthe complete $4$ Degree-of-Freedom (DoF) intrinsic parameters. First, wedemonstrate intrinsic is solved from two well-studied monocular priors, i.e.,monocular depthmap, and surface normal map. However, this solution imposes alow-bias and low-variance requirement for depth estimation. Alternatively, weintroduce a novel monocular 3D prior, the incidence field, defined as theincidence rays between points in 3D space and pixels in the 2D imaging plane.The incidence field is a pixel-wise parametrization of the intrinsic invariantto image cropping and resizing. With the estimated incidence field, a robustRANSAC algorithm recovers intrinsic. We demonstrate the effectiveness of ourmethod by showing superior performance on synthetic and zero-shot testingdatasets. Beyond calibration, we demonstrate downstream applications in imagemanipulation detection &amp; restoration, uncalibrated two-view pose estimation,and 3D sensing. Codes, models, and data will be held inhttps://github.com/ShngJZ/WildCamera.</description><author>Shengjie Zhu, Abhinav Kumar, Masa Hu, Xiaoming Liu</author><pubDate>Mon, 19 Jun 2023 15:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10988v1</guid></item><item><title>Edge-aware Consistent Stereo Video Depth Estimation</title><link>http://arxiv.org/abs/2305.02645v1</link><description>Video depth estimation is crucial in various applications, such as scenereconstruction and augmented reality. In contrast to the naive method ofestimating depths from images, a more sophisticated approach uses temporalinformation, thereby eliminating flickering and geometrical inconsistencies. Wepropose a consistent method for dense video depth estimation; however, unlikethe existing monocular methods, ours relates to stereo videos. This techniqueovercomes the limitations arising from the monocular input. As a benefit ofusing stereo inputs, a left-right consistency loss is introduced to improve theperformance. Besides, we use SLAM-based camera pose estimation in the process.To address the problem of depth blurriness during test-time training (TTT), wepresent an edge-preserving loss function that improves the visibility of finedetails while preserving geometrical consistency. We show that our edge-awarestereo video model can accurately estimate the dense depth maps.</description><author>Elena Kosheleva, Sunil Jaiswal, Faranak Shamsafar, Noshaba Cheema, Klaus Illgner-Fehns, Philipp Slusallek</author><pubDate>Thu, 04 May 2023 09:30:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02645v1</guid></item><item><title>Indoor Localization and Multi-person Tracking Using Privacy Preserving Distributed Camera Network with Edge Computing</title><link>http://arxiv.org/abs/2305.05062v1</link><description>Localization of individuals in a built environment is a growing researchtopic. Estimating the positions, face orientation (or gaze direction) andtrajectories of people through space has many uses, such as in crowdmanagement, security, and healthcare. In this work, we present an open-source,low-cost, scalable and privacy-preserving edge computing framework formulti-person localization, i.e. estimating the positions, orientations, andtrajectories of multiple people in an indoor space. Our computing frameworkconsists of 38 Tensor Processing Unit (TPU)-enabled edge computing camerasystems placed in the ceiling of the indoor therapeutic space. The edge computesystems are connected to an on-premise fog server through a secure and privatenetwork. A multi-person detection algorithm and a pose estimation model run onthe edge TPU in real-time to collect features which are used, instead of rawimages, for downstream computations. This ensures the privacy of individuals inthe space, reduces data transmission/storage and improves scalability. Weimplemented a Kalman filter-based multi-person tracking method and astate-of-the-art body orientation estimation method to determine the positionsand facing orientations of multiple people simultaneously in the indoor space.For our study site with size of 18,000 square feet, our system demonstrated anaverage localization error of 1.41 meters, a multiple-object tracking accuracyscore of 62%, and a mean absolute body orientation error of 29{\deg}, which issufficient for understanding group activity behaviors in indoor environments.Additionally, our study provides practical guidance for deploying the proposedsystem by analyzing various elements of the camera installation with respect totracking accuracy.</description><author>Hyeokhyen Kwon, Chaitra Hedge, Yashar Kiarashi, Venkata Siva Krishna Madala, Ratan Singh, ArjunSinh Nakum, Robert Tweedy, Leandro Miletto Tonetto, Craig M. Zimring, Gari D. Clifford</author><pubDate>Mon, 08 May 2023 22:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05062v1</guid></item><item><title>Reuse your features: unifying retrieval and feature-metric alignment</title><link>http://arxiv.org/abs/2204.06292v2</link><description>We propose a compact pipeline to unify all the steps of Visual Localization:image retrieval, candidate re-ranking and initial pose estimation, and camerapose refinement. Our key assumption is that the deep features used for theseindividual tasks share common characteristics, so we should reuse them in allthe procedures of the pipeline. Our DRAN (Deep Retrieval and image AlignmentNetwork) is able to extract global descriptors for efficient image retrieval,use intermediate hierarchical features to re-rank the retrieval list andproduce an initial pose guess, which is finally refined by means of afeature-metric optimization based on learned deep multi-scale dense features.DRAN is the first single network able to produce the features for the threesteps of visual localization. DRAN achieves competitive performance in terms ofrobustness and accuracy under challenging conditions in public benchmarks,outperforming other unified approaches and consuming lower computational andmemory cost than its counterparts using multiple networks. Code and models willbe publicly available at https://github.com/jmorlana/DRAN.</description><author>Javier Morlana, J. M. M. Montiel</author><pubDate>Mon, 08 May 2023 13:10:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.06292v2</guid></item><item><title>Synthetic Data-based Detection of Zebras in Drone Imagery</title><link>http://arxiv.org/abs/2305.00432v1</link><description>Datasets that allow the training of common objects or human detectors arewidely available. These come in the form of labelled real-world images andrequire either a significant amount of human effort, with a high probability oferrors such as missing labels, or very constrained scenarios, e.g. VICONsystems. Likewise, uncommon scenarios, like aerial views, animals, like wildzebras, or difficult-to-obtain information as human shapes, are hardlyavailable. To overcome this, usage of synthetic data generation with realisticrendering technologies has recently gained traction and advanced tasks liketarget tracking and human pose estimation. However, subjects such as wildanimals are still usually not well represented in such datasets. In this work,we first show that a pre-trained YOLO detector can not identify zebras in realimages recorded from aerial viewpoints. To solve this, we present an approachfor training an animal detector using only synthetic data. We start bygenerating a novel synthetic zebra dataset using GRADE, a state-of-the-artframework for data generation. The dataset includes RGB, depth, skeletal jointlocations, pose, shape and instance segmentations for each subject. We use thisto train a YOLO detector from scratch. Through extensive evaluations of ourmodel with real-world data from i) limited datasets available on the internetand ii) a new one collected and manually labelled by us, we show that we candetect zebras by using only synthetic data during training. The code, results,trained models, and both the generated and training data are provided asopen-source at https://keeper.mpdl.mpg.de/d/12abb3bb6b12491480d5/.</description><author>Elia Bonetto, Aamir Ahmad</author><pubDate>Sun, 30 Apr 2023 10:24:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00432v1</guid></item><item><title>Two-View Geometry Scoring Without Correspondences</title><link>http://arxiv.org/abs/2306.01596v1</link><description>Camera pose estimation for two-view geometry traditionally relies on RANSAC.Normally, a multitude of image correspondences leads to a pool of proposedhypotheses, which are then scored to find a winning model. The inlier count isgenerally regarded as a reliable indicator of "consensus". We examine thisscoring heuristic, and find that it favors disappointing models under certaincircumstances. As a remedy, we propose the Fundamental Scoring Network (FSNet),which infers a score for a pair of overlapping images and any proposedfundamental matrix. It does not rely on sparse correspondences, but ratherembodies a two-view geometry model through an epipolar attention mechanism thatpredicts the pose error of the two images. FSNet can be incorporated intotraditional RANSAC loops. We evaluate FSNet on fundamental and essential matrixestimation on indoor and outdoor datasets, and establish that FSNet cansuccessfully identify good poses for pairs of images with few or unreliablecorrespondences. Besides, we show that naively combining FSNet with MAGSAC++scoring approach achieves state of the art results.</description><author>Axel Barroso-Laguna, Eric Brachmann, Victor Adrian Prisacariu, Gabriel J. Brostow, Daniyar Turmukhambetov</author><pubDate>Fri, 02 Jun 2023 16:07:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01596v1</guid></item><item><title>Embedded Feature Correlation Optimization with Specific Parameter Initialization for 2D/3D Registration</title><link>http://arxiv.org/abs/2305.06252v1</link><description>We present a novel deep learning-based framework: Embedded FeatureCorrelation Optimization with Specific Parameter Initialization (COSPI) for2D/3D registration which is a most challenging problem due to the difficultysuch as dimensional mismatch, heavy computation load and lack of goldenevaluating standard. The framework we designed includes a parameterspecification module to efficiently choose initialization pose parameter and afine-registration network to align images. The proposed framework takesextracting multi-scale features into consideration using a novel compositeconnection encoder with special training techniques. The method is comparedwith both learning-based methods and optimization-based methods to furtherevaluate the performance. Our experiments demonstrate that the method in thispaper has improved the registration performance, and thereby outperforms theexisting methods in terms of accuracy and running time. We also show thepotential of the proposed method as an initial pose estimator.</description><author>Minheng Chen, Zhirun Zhang, Shuheng Gu, Youyong Kong</author><pubDate>Wed, 10 May 2023 16:33:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06252v1</guid></item><item><title>Learning from synthetic data generated with GRADE</title><link>http://arxiv.org/abs/2305.04282v2</link><description>Recently, synthetic data generation and realistic rendering has advancedtasks like target tracking and human pose estimation. Simulations for mostrobotics applications are obtained in (semi)static environments, with specificsensors and low visual fidelity. To solve this, we present a fully customizableframework for generating realistic animated dynamic environments (GRADE) forrobotics research, first introduced in [1]. GRADE supports full simulationcontrol, ROS integration, realistic physics, while being in an engine thatproduces high visual fidelity images and ground truth data. We use GRADE togenerate a dataset focused on indoor dynamic scenes with people and flyingobjects. Using this, we evaluate the performance of YOLO and Mask R-CNN on thetasks of segmenting and detecting people. Our results provide evidence thatusing data generated with GRADE can improve the model performance when used fora pre-training step. We also show that, even training using only syntheticdata, can generalize well to real-world images in the same application domainsuch as the ones from the TUM-RGBD dataset. The code, results, trained models,and the generated data are provided as open-source athttps://eliabntt.github.io/grade-rr.</description><author>Elia Bonetto, Chenghao Xu, Aamir Ahmad</author><pubDate>Fri, 26 May 2023 10:26:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04282v2</guid></item><item><title>Learning from synthetic data generated with GRADE</title><link>http://arxiv.org/abs/2305.04282v1</link><description>Recently, synthetic data generation and realistic rendering has advancedtasks like target tracking and human pose estimation. Simulations for mostrobotics applications are obtained in (semi)static environments, with specificsensors and low visual fidelity. To solve this, we present a fully customizableframework for generating realistic animated dynamic environments (GRADE) forrobotics research, first introduced in [1]. GRADE supports full simulationcontrol, ROS integration, realistic physics, while being in an engine thatproduces high visual fidelity images and ground truth data. We use GRADE togenerate a dataset focused on indoor dynamic scenes with people and flyingobjects. Using this, we evaluate the performance of YOLO and Mask R-CNN on thetasks of segmenting and detecting people. Our results provide evidence thatusing data generated with GRADE can improve the model performance when used fora pre-training step. We also show that, even training using only syntheticdata, can generalize well to real-world images in the same application domainsuch as the ones from the TUM-RGBD dataset. The code, results, trained models,and the generated data are provided as open-source athttps://eliabntt.github.io/grade-rr.</description><author>Elia Bonetto, Chenghao Xu, Aamir Ahmad</author><pubDate>Sun, 07 May 2023 15:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04282v1</guid></item><item><title>Learning Probabilistic Coordinate Fields for Robust Correspondences</title><link>http://arxiv.org/abs/2306.04231v1</link><description>We introduce Probabilistic Coordinate Fields (PCFs), a novelgeometric-invariant coordinate representation for image correspondenceproblems. In contrast to standard Cartesian coordinates, PCFs encodecoordinates in correspondence-specific barycentric coordinate systems (BCS)with affine invariance. To know \textit{when and where to trust} the encodedcoordinates, we implement PCFs in a probabilistic network termed PCF-Net, whichparameterizes the distribution of coordinate fields as Gaussian mixture models.By jointly optimizing coordinate fields and their confidence conditioned ondense flows, PCF-Net can work with various feature descriptors when quantifyingthe reliability of PCFs by confidence maps. An interesting observation of thiswork is that the learned confidence map converges to geometrically coherent andsemantically consistent regions, which facilitates robust coordinaterepresentation. By delivering the confident coordinates to keypoint/featuredescriptors, we show that PCF-Net can be used as a plug-in to existingcorrespondence-dependent approaches. Extensive experiments on both indoor andoutdoor datasets suggest that accurate geometric invariant coordinates help toachieve the state of the art in several correspondence problems, such as sparsefeature matching, dense image registration, camera pose estimation, andconsistency filtering. Further, the interpretable confidence map predicted byPCF-Net can also be leveraged to other novel applications from texture transferto multi-homography classification.</description><author>Weiyue Zhao, Hao Lu, Xinyi Ye, Zhiguo Cao, Xin Li</author><pubDate>Wed, 07 Jun 2023 09:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04231v1</guid></item><item><title>Does Image Anonymization Impact Computer Vision Training?</title><link>http://arxiv.org/abs/2306.05135v1</link><description>Image anonymization is widely adapted in practice to comply with privacyregulations in many regions. However, anonymization often degrades the qualityof the data, reducing its utility for computer vision development. In thispaper, we investigate the impact of image anonymization for training computervision models on key computer vision tasks (detection, instance segmentation,and pose estimation). Specifically, we benchmark the recognition drop on commondetection datasets, where we evaluate both traditional and realisticanonymization for faces and full bodies. Our comprehensive experiments reflectthat traditional image anonymization substantially impacts final modelperformance, particularly when anonymizing the full body. Furthermore, we findthat realistic anonymization can mitigate this decrease in performance, whereour experiments reflect a minimal performance drop for face anonymization. Ourstudy demonstrates that realistic anonymization can enable privacy-preservingcomputer vision development with minimal performance degradation across a rangeof important computer vision benchmarks.</description><author>H√•kon Hukkel√•s, Frank Lindseth</author><pubDate>Thu, 08 Jun 2023 13:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05135v1</guid></item><item><title>The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose</title><link>http://arxiv.org/abs/2007.00394v2</link><description>The availability of a large labeled dataset is a key requirement for applyingdeep learning methods to solve various computer vision tasks. In the context ofunderstanding human activities, existing public datasets, while large in size,are often limited to a single RGB camera and provide only per-frame or per-clipaction annotations. To enable richer analysis and understanding of humanactivities, we introduce IKEA ASM -- a three million frame, multi-view,furniture assembly video dataset that includes depth, atomic actions, objectsegmentation, and human pose. Additionally, we benchmark prominent methods forvideo action recognition, object segmentation and human pose estimation taskson this challenging dataset. The dataset enables the development of holisticmethods, which integrate multi-modal and multi-view data to better perform onthese tasks.</description><author>Yizhak Ben-Shabat, Xin Yu, Fatemeh Sadat Saleh, Dylan Campbell, Cristian Rodriguez-Opazo, Hongdong Li, Stephen Gould</author><pubDate>Wed, 17 May 2023 08:56:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.00394v2</guid></item><item><title>CARLA-BSP: a simulated dataset with pedestrians</title><link>http://arxiv.org/abs/2305.00204v1</link><description>We present a sample dataset featuring pedestrians generated using the ARCANEframework, a new framework for generating datasets in CARLA (0.9.13). Weprovide use cases for pedestrian detection, autoencoding, pose estimation, andpose lifting. We also showcase baseline results. For more information, visithttps://project-arcane.eu/.</description><author>Maciej Wielgosz, Antonio M. L√≥pez, Muhammad Naveed Riaz</author><pubDate>Sat, 29 Apr 2023 10:10:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00204v1</guid></item><item><title>Is Generative Modeling-based Stylization Necessary for Domain Adaptation in Regression Tasks?</title><link>http://arxiv.org/abs/2306.01706v1</link><description>Unsupervised domain adaptation (UDA) aims to bridge the gap between sourceand target domains in the absence of target domain labels using two maintechniques: input-level alignment (such as generative modeling and stylization)and feature-level alignment (which matches the distribution of the featuremaps, e.g. gradient reversal layers). Motivated from the success of generativemodeling for image classification, stylization-based methods were recentlyproposed for regression tasks, such as pose estimation. However, use ofinput-level alignment via generative modeling and stylization incur additionaloverhead and computational complexity which limit their use in real-world DAtasks. To investigate the role of input-level alignment for DA, we ask thefollowing question: Is generative modeling-based stylization necessary forvisual domain adaptation in regression? Surprisingly, we find thatinput-alignment has little effect on regression tasks as compared toclassification. Based on these insights, we develop a non-parametricfeature-level domain alignment method -- Implicit Stylization (ImSty) -- whichresults in consistent improvements over SOTA regression task, without the needfor computationally intensive stylization and generative modeling. Our workconducts a critical evaluation of the role of generative modeling andstylization, at a time when these are also gaining popularity for domaingeneralization.</description><author>Jinman Park, Francois Barnard, Saad Hossain, Sirisha Rambhatla, Paul Fieguth</author><pubDate>Fri, 02 Jun 2023 18:28:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01706v1</guid></item><item><title>COVINS-G: A Generic Back-end for Collaborative Visual-Inertial SLAM</title><link>http://arxiv.org/abs/2301.07147v3</link><description>Collaborative SLAM is at the core of perception in multi-robot systems as itenables the co-localization of the team of robots in a common reference frame,which is of vital importance for any coordination amongst them. The paradigm ofa centralized architecture is well established, with the robots (i.e. agents)running Visual-Inertial Odometry (VIO) onboard while communicating relevantdata, such as e.g. Keyframes (KFs), to a central back-end (i.e. server), whichthen merges and optimizes the joint maps of the agents. While these frameworkshave proven to be successful, their capability and performance are highlydependent on the choice of the VIO front-end, thus limiting their flexibility.In this work, we present COVINS-G, a generalized back-end building upon theCOVINS framework, enabling the compatibility of the server-back-end with anyarbitrary VIO front-end, including, for example, off-the-shelf cameras withodometry capabilities, such as the Realsense T265. The COVINS-G back-enddeploys a multi-camera relative pose estimation algorithm for computing theloop-closure constraints allowing the system to work purely on 2D image data.In the experimental evaluation, we show on-par accuracy with state-of-the-artmulti-session and collaborative SLAM systems, while demonstrating theflexibility and generality of our approach by employing different front-endsonboard collaborating agents within the same mission. The COVINS-G codebasealong with a generalized front-end wrapper to allow any existing VIO front-endto be readily used in combination with the proposed collaborative back-end isopen-sourced. Video: https://youtu.be/FoJfXCfaYDw</description><author>Manthan Patel, Marco Karrer, Philipp B√§nninger, Margarita Chli</author><pubDate>Fri, 05 May 2023 09:17:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07147v3</guid></item><item><title>Searching from Area to Point: A Hierarchical Framework for Semantic-Geometric Combined Feature Matching</title><link>http://arxiv.org/abs/2305.00194v3</link><description>Feature matching is a crucial technique in computer vision. Essentially, itcan be considered as a searching problem to establish correspondences betweenimages. The key challenge in this task lies in the lack of a well-definedsearch space, leading to inaccurate point matching of current methods. Inpursuit of a reasonable matching search space, this paper introduces ahierarchical feature matching framework: Area to Point Matching (A2PM), tofirst find semantic area matches between images, and then perform pointmatching on area matches, thus setting the search space as the area matcheswith salient features to achieve high matching precision. This proper searchspace of A2PM framework also alleviates the accuracy limitation instate-of-the-art Transformer-based matching methods. To realize this framework,we further propose Semantic and Geometry Area Matching (SGAM) method, whichutilizes semantic prior and geometry consistency to establish accurate areamatches between images. By integrating SGAM with off-the-shelfTransformer-based matchers, our feature matching methods, adopting the A2PMframework, achieve encouraging precision improvements in massive point matchingand pose estimation experiments for present arts.</description><author>Yesheng Zhang, Xu Zhao, Dahong Qian</author><pubDate>Fri, 05 May 2023 10:04:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00194v3</guid></item><item><title>MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with Bird's Eye View based Appearance and Motion Features</title><link>http://arxiv.org/abs/2305.07336v1</link><description>Identifying moving objects is an essential capability for autonomous systems,as it provides critical information for pose estimation, navigation, collisionavoidance and static map construction. In this paper, we present MotionBEV, afast and accurate framework for LiDAR moving object segmentation, whichsegments moving objects with appearance and motion features in bird's eye view(BEV) domain. Our approach converts 3D LiDAR scans into 2D polar BEVrepresentation to achieve real-time performance. Specifically, we learnappearance features with a simplified PointNet, and compute motion featuresthrough the height differences of consecutive frames of point clouds projectedonto vertical columns in the polar BEV coordinate system. We employ adual-branch network bridged by the Appearance-Motion Co-attention Module (AMCM)to adaptively fuse the spatio-temporal information from appearance and motionfeatures. Our approach achieves state-of-the-art performance on theSemanticKITTI-MOS benchmark, with an average inference time of 23ms on an RTX3090 GPU. Furthermore, to demonstrate the practical effectiveness of ourmethod, we provide a LiDAR-MOS dataset recorded by a solid-state LiDAR, whichfeatures non-repetitive scanning patterns and small field of view.</description><author>Bo Zhou, Jiapeng Xie, Yan Pan, Jiajie Wu, Chuanzhao Lu</author><pubDate>Fri, 12 May 2023 10:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07336v1</guid></item><item><title>gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction</title><link>http://arxiv.org/abs/2304.11970v1</link><description>Signed distance functions (SDFs) is an attractive framework that has recentlyshown promising results for 3D shape reconstruction from images. SDFsseamlessly generalize to different shape resolutions and topologies but lackexplicit modelling of the underlying 3D geometry. In this work, we exploit thehand structure and use it as guidance for SDF-based shape reconstruction. Inparticular, we address reconstruction of hands and manipulated objects frommonocular RGB images. To this end, we estimate poses of hands and objects anduse them to guide 3D reconstruction. More specifically, we predict kinematicchains of pose transformations and align SDFs with highly-articulated handposes. We improve the visual features of 3D points with geometry alignment andfurther leverage temporal information to enhance the robustness to occlusionand motion blurs. We conduct extensive experiments on the challenging ObMan andDexYCB benchmarks and demonstrate significant improvements of the proposedmethod over the state of the art.</description><author>Zerui Chen, Shizhe Chen, Cordelia Schmid, Ivan Laptev</author><pubDate>Mon, 24 Apr 2023 11:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11970v1</guid></item><item><title>Meta Omnium: A Benchmark for General-Purpose Learning-to-Learn</title><link>http://arxiv.org/abs/2305.07625v1</link><description>Meta-learning and other approaches to few-shot learning are widely studiedfor image recognition, and are increasingly applied to other vision tasks suchas pose estimation and dense prediction. This naturally raises the question ofwhether there is any few-shot meta-learning algorithm capable of generalizingacross these diverse task types? To support the community in answering thisquestion, we introduce Meta Omnium, a dataset-of-datasets spanning multiplevision tasks including recognition, keypoint localization, semanticsegmentation and regression. We experiment with popular few-shot meta-learningbaselines and analyze their ability to generalize across tasks and to transferknowledge between them. Meta Omnium enables meta-learning researchers toevaluate model generalization to a much wider array of tasks than previouslypossible, and provides a single framework for evaluating meta-learners across awide suite of vision applications in a consistent manner.</description><author>Ondrej Bohdal, Yinbing Tian, Yongshuo Zong, Ruchika Chavhan, Da Li, Henry Gouk, Li Guo, Timothy Hospedales</author><pubDate>Fri, 12 May 2023 18:25:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07625v1</guid></item></channel></rss>