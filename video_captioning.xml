<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo captioning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 20 Feb 2024 06:01:18 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</title><link>http://arxiv.org/abs/2402.10698v1</link><description>We present Q-ViD, a simple approach for video question answering (video QA),that unlike prior methods, which are based on complex architectures,computationally expensive pipelines or use closed models like GPTs, Q-ViDrelies on a single instruction-aware open vision-language model (InstructBLIP)to tackle videoQA using frame descriptions. Specifically, we create captioninginstruction prompts that rely on the target questions about the videos andleverage InstructBLIP to obtain video frame captions that are useful to thetask at hand. Subsequently, we form descriptions of the whole video using thequestion-dependent frame captions, and feed that information, along with aquestion-answering prompt, to a large language model (LLM). The LLM is ourreasoning module, and performs the final step of multiple-choice QA. Our simpleQ-ViD framework achieves competitive or even higher performances than currentstate of the art models on a diverse range of videoQA benchmarks, includingNExT-QA, STAR, How2QA, TVQA and IntentQA.</description><author>David Romero, Thamar Solorio</author><pubDate>Fri, 16 Feb 2024 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10698v1</guid></item><item><title>LVCHAT: Facilitating Long Video Comprehension</title><link>http://arxiv.org/abs/2402.12079v1</link><description>Enabling large language models (LLMs) to read videos is vital for multimodalLLMs. Existing works show promise on short videos whereas long video (longerthan e.g.~1 minute) comprehension remains challenging. The major problem liesin the over-compression of videos, i.e., the encoded video representations arenot enough to represent the whole video. To address this issue, we propose LongVideo Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced todynamically adjust the number of embeddings in alignment with the duration ofthe video to ensure long videos are not overly compressed into a fewembeddings. To deal with long videos whose length is beyond videos seen duringtraining, we propose Interleaved Frame Encoding (IFE), repeating positionalembedding and interleaving multiple groups of videos to enable long videoinput, avoiding performance degradation due to overly long videos. Experimentalresults show that LVChat significantly outperforms existing methods by up to27\% in accuracy on long-video QA datasets and long-video captioningbenchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.</description><author>Yu Wang, Zeyuan Zhang, Julian McAuley, Zexue He</author><pubDate>Mon, 19 Feb 2024 11:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12079v1</guid></item><item><title>PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter</title><link>http://arxiv.org/abs/2402.10896v1</link><description>This paper demonstrates that a progressively aligned language model caneffectively bridge frozen vision encoders and large language models (LLMs).While the fundamental architecture and pre-training methods of vision encodersand LLMs have been extensively studied, the architecture and training strategyof vision-language adapters vary significantly across recent works. Ourresearch undertakes a thorough exploration of the state-of-the-art perceiverresampler architecture and builds a strong baseline. However, we observe thatthe vision-language alignment with perceiver resampler exhibits slowconvergence and limited scalability with a lack of direct supervision. Toaddress this issue, we propose PaLM2-VAdapter, employing a progressivelyaligned language model as the vision-language adapter. Compared to the strongbaseline with perceiver resampler, our method empirically shows fasterconvergence, higher performance, and stronger scalability. Extensiveexperiments across various Visual Question Answering (VQA) and captioning taskson both images and videos demonstrate that our model exhibits state-of-the-artvisual understanding and multi-modal reasoning capabilities. Notably, ourmethod achieves these advancements with 30~70% fewer parameters than thestate-of-the-art large vision-language models, marking a significant efficiencyimprovement.</description><author>Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang</author><pubDate>Fri, 16 Feb 2024 18:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10896v1</guid></item></channel></rss>