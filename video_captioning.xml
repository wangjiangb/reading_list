<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo captioning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 21 Feb 2024 06:00:47 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v1</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Tue, 20 Feb 2024 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v1</guid></item><item><title>VideoPrism: A Foundational Visual Encoder for Video Understanding</title><link>http://arxiv.org/abs/2402.13217v1</link><description>We introduce VideoPrism, a general-purpose video encoder that tackles diversevideo understanding tasks with a single frozen model. We pretrain VideoPrism ona heterogeneous corpus containing 36M high-quality video-caption pairs and 582Mvideo clips with noisy parallel text (e.g., ASR transcripts). The pretrainingapproach improves upon masked autoencoding by global-local distillation ofsemantic video embeddings and a token shuffling scheme, enabling VideoPrism tofocus primarily on the video modality while leveraging the invaluable textassociated with videos. We extensively test VideoPrism on four broad groups ofvideo understanding tasks, from web video question answering to CV for science,achieving state-of-the-art performance on 30 out of 33 video understandingbenchmarks.</description><author>Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, Boqing Gong</author><pubDate>Tue, 20 Feb 2024 18:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13217v1</guid></item><item><title>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</title><link>http://arxiv.org/abs/2402.10698v1</link><description>We present Q-ViD, a simple approach for video question answering (video QA),that unlike prior methods, which are based on complex architectures,computationally expensive pipelines or use closed models like GPTs, Q-ViDrelies on a single instruction-aware open vision-language model (InstructBLIP)to tackle videoQA using frame descriptions. Specifically, we create captioninginstruction prompts that rely on the target questions about the videos andleverage InstructBLIP to obtain video frame captions that are useful to thetask at hand. Subsequently, we form descriptions of the whole video using thequestion-dependent frame captions, and feed that information, along with aquestion-answering prompt, to a large language model (LLM). The LLM is ourreasoning module, and performs the final step of multiple-choice QA. Our simpleQ-ViD framework achieves competitive or even higher performances than currentstate of the art models on a diverse range of videoQA benchmarks, includingNExT-QA, STAR, How2QA, TVQA and IntentQA.</description><author>David Romero, Thamar Solorio</author><pubDate>Fri, 16 Feb 2024 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10698v1</guid></item><item><title>LVCHAT: Facilitating Long Video Comprehension</title><link>http://arxiv.org/abs/2402.12079v1</link><description>Enabling large language models (LLMs) to read videos is vital for multimodalLLMs. Existing works show promise on short videos whereas long video (longerthan e.g.~1 minute) comprehension remains challenging. The major problem liesin the over-compression of videos, i.e., the encoded video representations arenot enough to represent the whole video. To address this issue, we propose LongVideo Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced todynamically adjust the number of embeddings in alignment with the duration ofthe video to ensure long videos are not overly compressed into a fewembeddings. To deal with long videos whose length is beyond videos seen duringtraining, we propose Interleaved Frame Encoding (IFE), repeating positionalembedding and interleaving multiple groups of videos to enable long videoinput, avoiding performance degradation due to overly long videos. Experimentalresults show that LVChat significantly outperforms existing methods by up to27\% in accuracy on long-video QA datasets and long-video captioningbenchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.</description><author>Yu Wang, Zeyuan Zhang, Julian McAuley, Zexue He</author><pubDate>Mon, 19 Feb 2024 11:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12079v1</guid></item><item><title>PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter</title><link>http://arxiv.org/abs/2402.10896v1</link><description>This paper demonstrates that a progressively aligned language model caneffectively bridge frozen vision encoders and large language models (LLMs).While the fundamental architecture and pre-training methods of vision encodersand LLMs have been extensively studied, the architecture and training strategyof vision-language adapters vary significantly across recent works. Ourresearch undertakes a thorough exploration of the state-of-the-art perceiverresampler architecture and builds a strong baseline. However, we observe thatthe vision-language alignment with perceiver resampler exhibits slowconvergence and limited scalability with a lack of direct supervision. Toaddress this issue, we propose PaLM2-VAdapter, employing a progressivelyaligned language model as the vision-language adapter. Compared to the strongbaseline with perceiver resampler, our method empirically shows fasterconvergence, higher performance, and stronger scalability. Extensiveexperiments across various Visual Question Answering (VQA) and captioning taskson both images and videos demonstrate that our model exhibits state-of-the-artvisual understanding and multi-modal reasoning capabilities. Notably, ourmethod achieves these advancements with 30~70% fewer parameters than thestate-of-the-art large vision-language models, marking a significant efficiencyimprovement.</description><author>Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang</author><pubDate>Fri, 16 Feb 2024 18:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10896v1</guid></item></channel></rss>