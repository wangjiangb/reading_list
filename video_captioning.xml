<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo captioning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 16 Jul 2024 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models</title><link>http://arxiv.org/abs/2407.09012v1</link><description>Pose-driven human-image animation diffusion models have shown remarkablecapabilities in realistic human video synthesis. Despite the promising resultsachieved by previous approaches, challenges persist in achieving temporallyconsistent animation and ensuring robustness with off-the-shelf pose detectors.In this paper, we present TCAN, a pose-driven human image animation method thatis robust to erroneous poses and consistent over time. In contrast to previousmethods, we utilize the pre-trained ControlNet without fine-tuning to leverageits extensive pre-acquired knowledge from numerous pose-image-caption pairs. Tokeep the ControlNet frozen, we adapt LoRA to the UNet layers, enabling thenetwork to align the latent space between the pose and appearance features.Additionally, by introducing an additional temporal layer to the ControlNet, weenhance robustness against outliers of the pose detector. Through the analysisof attention maps over the temporal axis, we also designed a novel temperaturemap leveraging pose information, allowing for a more static background.Extensive experiments demonstrate that the proposed method can achievepromising results in video synthesis tasks encompassing various poses, likechibi. Project Page: https://eccv2024tcan.github.io/</description><author>Jeongho Kim, Min-Jung Kim, Junsoo Lee, Jaegul Choo</author><pubDate>Fri, 12 Jul 2024 06:02:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09012v1</guid></item></channel></rss>