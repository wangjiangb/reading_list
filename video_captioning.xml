<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo captioning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 08 May 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Streaming Dense Video Captioning</title><link>http://arxiv.org/abs/2404.01297v1</link><description>An ideal model for dense video captioning -- predicting captions localizedtemporally in a video -- should be able to handle long input videos, predictrich, detailed textual descriptions, and be able to produce outputs beforeprocessing the entire video. Current state-of-the-art models, however, processa fixed number of downsampled frames, and make a single full prediction afterseeing the whole video. We propose a streaming dense video captioning modelthat consists of two novel components: First, we propose a new memory module,based on clustering incoming tokens, which can handle arbitrarily long videosas the memory is of a fixed size. Second, we develop a streaming decodingalgorithm that enables our model to make predictions before the entire videohas been processed. Our model achieves this streaming ability, andsignificantly improves the state-of-the-art on three dense video captioningbenchmarks: ActivityNet, YouCook2 and ViTT. Our code is released athttps://github.com/google-research/scenic.</description><author>Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, Cordelia Schmid</author><pubDate>Mon, 01 Apr 2024 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01297v1</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v3</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Wed, 28 Feb 2024 13:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v3</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v1</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Tue, 20 Feb 2024 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v1</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v2</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Wed, 21 Feb 2024 22:19:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v2</guid></item><item><title>MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning</title><link>http://arxiv.org/abs/2402.17680v1</link><description>To address the problem of catastrophic forgetting due to the invisibility ofold categories in sequential input, existing work based on relatively simplecategorization tasks has made some progress. In contrast, video captioning is amore complex task in multimodal scenario, which has not been explored in thefield of incremental learning. After identifying this stability-plasticityproblem when analyzing video with sequential input, we originally propose amethod to Mitigate Catastrophic Forgetting in class-incremental learning formultimodal Video Captioning (MCF-VC). As for effectively maintaining goodperformance on old tasks at the macro level, we design Fine-grained SensitivitySelection (FgSS) based on the Mask of Linear's Parameters and FisherSensitivity to pick useful knowledge from old tasks. Further, in order tobetter constrain the knowledge characteristics of old and new tasks at thespecific feature level, we have created the Two-stage Knowledge Distillation(TsKD), which is able to learn the new task well while weighing the old task.Specifically, we design two distillation losses, which constrain the crossmodal semantic information of semantic attention feature map and the textualinformation of the final outputs respectively, so that the inter-model andintra-model stylized knowledge of the old class is retained while learning thenew class. In order to illustrate the ability of our model to resistforgetting, we designed a metric CIDER_t to detect the stage forgetting rate.Our experiments on the public dataset MSR-VTT show that the proposed methodsignificantly resists the forgetting of previous tasks without replaying oldsamples, and performs well on the new task.</description><author>Huiyu Xiong, Lanxiao Wang, Heqian Qiu, Taijin Zhao, Benliu Qiu, Hongliang Li</author><pubDate>Tue, 27 Feb 2024 16:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17680v1</guid></item><item><title>DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement</title><link>http://arxiv.org/abs/2404.02755v1</link><description>We present Dive Into the BoundarieS (DIBS), a novel pretraining framework fordense video captioning (DVC), that elaborates on improving the quality of thegenerated event captions and their associated pseudo event boundaries fromunlabeled videos. By leveraging the capabilities of diverse large languagemodels (LLMs), we generate rich DVC-oriented caption candidates and optimizethe corresponding pseudo boundaries under several meticulously designedobjectives, considering diversity, event-centricity, temporal ordering, andcoherence. Moreover, we further introduce a novel online boundary refinementstrategy that iteratively improves the quality of pseudo boundaries duringtraining. Comprehensive experiments have been conducted to examine theeffectiveness of the proposed technique components. By leveraging a substantialamount of unlabeled video data, such as HowTo100M, we achieve a remarkableadvancement on standard DVC datasets like YouCook2 and ActivityNet. Weoutperform the previous state-of-the-art Vid2Seq across a majority of metrics,achieving this with just 0.4% of the unlabeled video data used for pre-trainingby Vid2Seq.</description><author>Hao Wu, Huabin Liu, Yu Qiao, Xiao Sun</author><pubDate>Wed, 03 Apr 2024 14:57:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02755v1</guid></item><item><title>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</title><link>http://arxiv.org/abs/2402.19479v1</link><description>The quality of the data and annotation upper-bounds the quality of adownstream model. While there exist large text corpora and image-text pairs,high-quality video-text data is much harder to collect. First of all, manuallabeling is more time-consuming, as it requires an annotator to watch an entirevideo. Second, videos have a temporal dimension, consisting of several scenesstacked together, and showing multiple actions. Accordingly, to establish avideo dataset with high-quality captions, we propose an automatic approachleveraging multimodal inputs, such as textual video description, subtitles, andindividual video frames. Specifically, we curate 3.8M high-resolution videosfrom the publicly available HD-VILA-100M dataset. We then split them intosemantically consistent video clips, and apply multiple cross-modality teachermodels to obtain captions for each video. Next, we finetune a retrieval modelon a small subset where the best caption of each video is manually selected andthen employ the model in the whole dataset to select the best caption as theannotation. In this way, we get 70M videos paired with high-quality textcaptions. We dub the dataset as Panda-70M. We show the value of the proposeddataset on three downstream tasks: video captioning, video and text retrieval,and text-driven video generation. The models trained on the proposed data scoresubstantially better on the majority of metrics across all the tasks.</description><author>Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, Sergey Tulyakov</author><pubDate>Thu, 29 Feb 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19479v1</guid></item><item><title>TrafficVLM: A Controllable Visual Language Model for Traffic Video Captioning</title><link>http://arxiv.org/abs/2404.09275v1</link><description>Traffic video description and analysis have received much attention recentlydue to the growing demand for efficient and reliable urban surveillancesystems. Most existing methods only focus on locating traffic event segments,which severely lack descriptive details related to the behaviour and context ofall the subjects of interest in the events. In this paper, we presentTrafficVLM, a novel multi-modal dense video captioning model for vehicle egocamera view. TrafficVLM models traffic video events at different levels ofanalysis, both spatially and temporally, and generates long fine-graineddescriptions for the vehicle and pedestrian at different phases of the event.We also propose a conditional component for TrafficVLM to control thegeneration outputs and a multi-task fine-tuning paradigm to enhanceTrafficVLM's learning capability. Experiments show that TrafficVLM performswell on both vehicle and overhead camera views. Our solution achievedoutstanding results in Track 2 of the AI City Challenge 2024, ranking us thirdin the challenge standings. Our code is publicly available athttps://github.com/quangminhdinh/TrafficVLM.</description><author>Quang Minh Dinh, Minh Khoi Ho, Anh Quan Dang, Hung Phong Tran</author><pubDate>Sun, 14 Apr 2024 15:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09275v1</guid></item><item><title>The MSR-Video to Text Dataset with Clean Annotations</title><link>http://arxiv.org/abs/2102.06448v4</link><description>Video captioning automatically generates short descriptions of the videocontent, usually in form of a single sentence. Many methods have been proposedfor solving this task. A large dataset called MSR Video to Text (MSR-VTT) isoften used as the benchmark dataset for testing the performance of the methods.However, we found that the human annotations, i.e., the descriptions of videocontents in the dataset are quite noisy, e.g., there are many duplicatecaptions and many captions contain grammatical problems. These problems maypose difficulties to video captioning models for learning underlying patterns.We cleaned the MSR-VTT annotations by removing these problems, then testedseveral typical video captioning models on the cleaned dataset. Experimentalresults showed that data cleaning boosted the performances of the modelsmeasured by popular quantitative metrics. We recruited subjects to evaluate theresults of a model trained on the original and cleaned datasets. The humanbehavior experiment demonstrated that trained on the cleaned dataset, the modelgenerated captions that were more coherent and more relevant to the contents ofthe video clips.</description><author>Haoran Chen, Jianmin Li, Simone Frintrop, Xiaolin Hu</author><pubDate>Sun, 25 Feb 2024 09:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.06448v4</guid></item><item><title>Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval</title><link>http://arxiv.org/abs/2404.07610v1</link><description>There has been significant attention to the research on dense videocaptioning, which aims to automatically localize and caption all events withinuntrimmed video. Several studies introduce methods by designing dense videocaptioning as a multitasking problem of event localization and event captioningto consider inter-task relations. However, addressing both tasks using onlyvisual input is challenging due to the lack of semantic content. In this study,we address this by proposing a novel framework inspired by the cognitiveinformation processing of humans. Our model utilizes external memory toincorporate prior knowledge. The memory retrieval method is proposed withcross-modal video-to-text matching. To effectively incorporate retrieved textfeatures, the versatile encoder and the decoder with visual and textualcross-attention modules are designed. Comparative experiments have beenconducted to show the effectiveness of the proposed method on ActivityNetCaptions and YouCook2 datasets. Experimental results show promising performanceof our model without extensive pretraining from a large video dataset.</description><author>Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, Seong Tae Kim</author><pubDate>Thu, 11 Apr 2024 10:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07610v1</guid></item><item><title>OW-VISCap: Open-World Video Instance Segmentation and Captioning</title><link>http://arxiv.org/abs/2404.03657v1</link><description>Open-world video instance segmentation is an important video understandingtask. Yet most methods either operate in a closed-world setting, require anadditional user-input, or use classic region-based proposals to identify neverbefore seen objects. Further, these methods only assign a one-word label todetected objects, and don't generate rich object-centric descriptions. Theyalso often suffer from highly overlapping predictions. To address these issues,we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap),an approach to jointly segment, track, and caption previously seen or unseenobjects in a video. For this, we introduce open-world object queries todiscover never before seen objects without additional user-input. We generaterich and descriptive object-centric captions for each detected object via amasked attention augmented LLM input. We introduce an inter-query contrastiveloss to ensure that the object queries differ from one another. Our generalizedapproach matches or surpasses state-of-the-art on three tasks: open-world videoinstance segmentation on the BURST dataset, dense video object captioning onthe VidSTG dataset, and closed-world video instance segmentation on the OVISdataset.</description><author>Anwesa Choudhuri, Girish Chowdhary, Alexander G. Schwing</author><pubDate>Thu, 04 Apr 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03657v1</guid></item><item><title>Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality</title><link>http://arxiv.org/abs/2403.19221v1</link><description>Video paragraph captioning (VPC) involves generating detailed narratives forlong videos, utilizing supportive modalities such as speech and eventboundaries. However, the existing models are constrained by the assumption ofconstant availability of a single auxiliary modality, which is impracticalgiven the diversity and unpredictable nature of real-world scenarios. To thisend, we propose a Missing-Resistant framework MR-VPC that effectively harnessesall available auxiliary inputs and maintains resilience even in the absence ofcertain modalities. Under this framework, we propose the Multimodal VPC (MVPC)architecture integrating video, speech, and event boundary inputs in a unifiedmanner to process various auxiliary inputs. Moreover, to fortify the modelagainst incomplete data, we introduce DropAM, a data augmentation strategy thatrandomly omits auxiliary inputs, paired with DistillAM, a regularization targetthat distills knowledge from teacher models trained on modality-complete data,enabling efficient learning in modality-deficient environments. Throughexhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC hasproven to deliver superior performance on modality-complete andmodality-missing test data. This work highlights the significance of developingresilient VPC models and paves the way for more adaptive, robust multimodalvideo understanding.</description><author>Sishuo Chen, Lei Li, Shuhuai Ren, Rundong Gao, Yuanxin Liu, Xiaohan Bi, Xu Sun, Lu Hou</author><pubDate>Thu, 28 Mar 2024 09:35:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19221v1</guid></item><item><title>Dense Video Object Captioning from Disjoint Supervision</title><link>http://arxiv.org/abs/2306.11729v2</link><description>We propose a new task and model for dense video object captioning --detecting, tracking and captioning trajectories of objects in a video. Thistask unifies spatial and temporal localization in video, whilst also requiringfine-grained visual understanding that is best described by natural language.We propose a unified model, and demonstrate how our end-to-end approach is moreaccurate and temporally coherent than a multi-stage pipeline combiningstate-of-the-art detection, tracking, and captioning models. Moreover, wepropose a training strategy based on a mixture of disjoint tasks, which allowsus to leverage diverse, large-scale datasets which supervise different parts ofour model. Although each pretraining task only provides weak supervision, theyare complementary and, when combined, result in noteworthy zero-shot abilityand serve as strong initialization for additional finetuning to further improveaccuracy. We carefully design new metrics capturing all components of our task,and show how we can repurpose existing video grounding datasets (e.g. VidSTGand VLN) for our new task. We show that our model improves upon a number ofstrong baselines for this new task. Furthermore, we can apply our model to thetask of spatial grounding, outperforming prior state-of-the-art on VidSTG andVLN, without explicitly training for it. Code is available athttps://github.com/google-research/scenic/tree/main/scenic/projects/densevoc.</description><author>Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid</author><pubDate>Tue, 09 Apr 2024 06:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11729v2</guid></item><item><title>Beyond MOT: Semantic Multi-Object Tracking</title><link>http://arxiv.org/abs/2403.05021v1</link><description>Current multi-object tracking (MOT) aims to predict trajectories of targets(i.e.,"where") in videos. Yet, knowing merely "where" is insufficient in manycrucial applications. In comparison, semantic understanding such asfine-grained behaviors, interactions, and overall summarized captions (i.e.,"what") from videos, associated with "where", is highly-desired forcomprehensive video analysis. Thus motivated, we introduce SemanticMulti-Object Tracking (SMOT), that aims to estimate object trajectories andmeanwhile understand semantic details of associated trajectories includinginstance captions, instance interactions, and overall video captions,integrating "where" and "what" for tracking. In order to foster the explorationof SMOT, we propose BenSMOT, a large-scale Benchmark for Semantic MOT.Specifically, BenSMOT comprises 3,292 videos with 151K frames, covering variousscenarios for semantic tracking of humans. BenSMOT provides annotations for thetrajectories of targets, along with associated instance captions in naturallanguage, instance interactions, and overall caption for each video sequence.To our best knowledge, BenSMOT is the first publicly available benchmark forSMOT. Besides, to encourage future research, we present a novel tracker namedSMOTer, which is specially designed and end-to-end trained for SMOT, showingpromising performance. By releasing BenSMOT, we expect to go beyondconventional MOT by predicting "where" and "what" for SMOT, opening up a newdirection in tracking for video understanding. Our BenSMOT and SMOTer will bereleased.</description><author>Yunhao Li, Hao Wang, Qin Li, Xue Ma, Jiali Yao, Shaohua Dong, Heng Fan, Libo Zhang</author><pubDate>Fri, 08 Mar 2024 03:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05021v1</guid></item><item><title>OmniVid: A Generative Framework for Universal Video Understanding</title><link>http://arxiv.org/abs/2403.17935v1</link><description>The core of video understanding tasks, such as recognition, captioning, andtracking, is to automatically detect objects or actions in a video and analyzetheir temporal evolution. Despite sharing a common goal, different tasks oftenrely on distinct model architectures and annotation formats. In contrast,natural language processing benefits from a unified output space, i.e., textsequences, which simplifies the training of powerful foundational languagemodels, such as GPT-3, with extensive training corpora. Inspired by this, weseek to unify the output space of video understanding tasks by using languagesas labels and additionally introducing time and box tokens. In this way, avariety of video tasks could be formulated as video-grounded token generation.This enables us to address various types of video tasks, includingclassification (such as action recognition), captioning (covering clipcaptioning, video question answering, and dense video captioning), andlocalization tasks (such as visual object tracking) within a fully sharedencoder-decoder architecture, following a generative framework. Throughcomprehensive experiments, we demonstrate such a simple and straightforwardidea is quite effective and can achieve state-of-the-art or competitive resultson seven video benchmarks, providing a novel perspective for more universalvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.</description><author>Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Tue, 26 Mar 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17935v1</guid></item><item><title>Distilling Vision-Language Models on Millions of Videos</title><link>http://arxiv.org/abs/2401.06129v2</link><description>The recent advance in vision-language models is largely attributed to theabundance of image-text data. We aim to replicate this success forvideo-language models, but there simply is not enough human-curated video-textdata available. We thus resort to fine-tuning a video-language model from astrong image-language baseline with synthesized instructional data. Theresulting video model by video-instruction-tuning (VIIT) is then used toauto-label millions of videos to generate high-quality captions. We show theadapted video-language model performs well on a wide range of video-languagebenchmarks. For instance, it surpasses the best prior result on open-endedNExT-QA by 2.8%. Besides, our model generates detailed descriptions forpreviously unseen videos, which provide better textual supervision thanexisting methods. Experiments show that a video-language dual-encoder modelcontrastively trained on these auto-generated captions is 3.8% better than thestrongest baseline that also leverages vision-language models. Our best modeloutperforms state-of-the-art methods on MSR-VTT zero-shot text-to-videoretrieval by 6%. As a side product, we generate the largest video captiondataset to date.</description><author>Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing Gong, Philipp Krähenbühl, Liangzhe Yuan</author><pubDate>Mon, 15 Apr 2024 22:10:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06129v2</guid></item><item><title>PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning</title><link>http://arxiv.org/abs/2404.16994v2</link><description>Vision-language pre-training has significantly elevated performance across awide range of image-language applications. Yet, the pre-training process forvideo-related tasks demands exceptionally large computational and dataresources, which hinders the progress of video-language models. This paperinvestigates a straight-forward, highly efficient, and resource-light approachto adapting an existing image-language pre-trained model for dense videounderstanding. Our preliminary experiments reveal that directly fine-tuningpre-trained image-language models with multiple frames as inputs on videodatasets leads to performance saturation or even a drop. Our furtherinvestigation reveals that it is largely attributed to the bias of learnedhigh-norm visual features. Motivated by this finding, we propose a simple buteffective pooling strategy to smooth the feature distribution along thetemporal dimension and thus reduce the dominant impacts from the extremefeatures. The new model is termed Pooling LLaVA, or PLLaVA in short. PLLaVAachieves new state-of-the-art performance on modern benchmark datasets for bothvideo question-answer and captioning tasks. Notably, on the recent popularVideoChatGPT benchmark, PLLaVA achieves a score of 3.48 out of 5 on average offive evaluated dimensions, exceeding the previous SOTA results from GPT4V(IG-VLM) by 9%. On the latest multi-choice benchmark MVBench, PLLaVA achieves58.1% accuracy on average across 20 sub-tasks, 14.5% higher than GPT4V(IG-VLM). Code is available at https://pllava.github.io/</description><author>Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, Jiashi Feng</author><pubDate>Mon, 29 Apr 2024 15:52:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16994v2</guid></item><item><title>Learning text-to-video retrieval from image captioning</title><link>http://arxiv.org/abs/2404.17498v1</link><description>We describe a protocol to study text-to-video retrieval training withunlabeled videos, where we assume (i) no access to labels for any videos, i.e.,no access to the set of ground-truth captions, but (ii) access to labeledimages in the form of text. Using image expert models is a realistic scenariogiven that annotating images is cheaper therefore scalable, in contrast toexpensive video labeling schemes. Recently, zero-shot image experts such asCLIP have established a new strong baseline for video understanding tasks. Inthis paper, we make use of this progress and instantiate the image experts fromtwo types of models: a text-to-image retrieval model to provide an initialbackbone, and image captioning models to provide supervision signal intounlabeled videos. We show that automatically labeling video frames with imagecaptioning allows text-to-video retrieval training. This process adapts thefeatures to the target domain at no manual annotation cost, consequentlyoutperforming the strong zero-shot CLIP baseline. During training, we samplecaptions from multiple video frames that best match the visual content, andperform a temporal pooling over frame representations by scoring framesaccording to their relevance to each caption. We conduct extensive ablations toprovide insights and demonstrate the effectiveness of this simple framework byoutperforming the CLIP zero-shot baselines on text-to-video retrieval on threestandard datasets, namely ActivityNet, MSR-VTT, and MSVD.</description><author>Lucas Ventura, Cordelia Schmid, Gül Varol</author><pubDate>Fri, 26 Apr 2024 16:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17498v1</guid></item><item><title>Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation</title><link>http://arxiv.org/abs/2403.05131v1</link><description>Text-to-video generation marks a significant frontier in the rapidly evolvingdomain of generative AI, integrating advancements in text-to-image synthesis,video captioning, and text-guided editing. This survey critically examines theprogression of text-to-video technologies, focusing on the shift fromtraditional generative models to the cutting-edge Sora model, highlightingdevelopments in scalability and generalizability. Distinguishing our analysisfrom prior works, we offer an in-depth exploration of the technologicalframeworks and evolutionary pathways of these models. Additionally, we delveinto practical applications and address ethical and technological challengessuch as the inability to perform multiple entity handling, comprehendcausal-effect learning, understand physical interaction, perceive objectscaling and proportioning, and combat object hallucination which is also along-standing problem in generative models. Our comprehensive discussion coversthe topic of enablement of text-to-video generation models as human-assistivetools and world models, as well as eliciting model's shortcomings andsummarizing future improvement direction that mainly centers around trainingdatasets and evaluation metrics (both automatic and human-centered). Aimed atboth newcomers and seasoned researchers, this survey seeks to catalyze furtherinnovation and discussion in the growing field of text-to-video generation,paving the way for more reliable and practical generative artificialintelligence technologies.</description><author>Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao Zheng, Lik-Hang Lee, Tae-Ho Kim, Choong Seon Hong, Chaoning Zhang</author><pubDate>Fri, 08 Mar 2024 07:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05131v1</guid></item><item><title>VideoPrism: A Foundational Visual Encoder for Video Understanding</title><link>http://arxiv.org/abs/2402.13217v1</link><description>We introduce VideoPrism, a general-purpose video encoder that tackles diversevideo understanding tasks with a single frozen model. We pretrain VideoPrism ona heterogeneous corpus containing 36M high-quality video-caption pairs and 582Mvideo clips with noisy parallel text (e.g., ASR transcripts). The pretrainingapproach improves upon masked autoencoding by global-local distillation ofsemantic video embeddings and a token shuffling scheme, enabling VideoPrism tofocus primarily on the video modality while leveraging the invaluable textassociated with videos. We extensively test VideoPrism on four broad groups ofvideo understanding tasks, from web video question answering to CV for science,achieving state-of-the-art performance on 30 out of 33 video understandingbenchmarks.</description><author>Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, Boqing Gong</author><pubDate>Tue, 20 Feb 2024 18:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13217v1</guid></item><item><title>Pix2Gif: Motion-Guided Diffusion for GIF Generation</title><link>http://arxiv.org/abs/2403.04634v2</link><description>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)generation. We tackle this problem differently by formulating the task as animage translation problem steered by text and motion magnitude prompts, asshown in teaser fig. To ensure that the model adheres to motion guidance, wepropose a new motion-guided warping module to spatially transform the featuresof the source image conditioned on the two types of prompts. Furthermore, weintroduce a perceptual loss to ensure the transformed feature map remainswithin the same space as the target image, ensuring content consistency andcoherence. In preparation for the model training, we meticulously curated databy extracting coherent image frames from the TGIF video-caption dataset, whichprovides rich information about the temporal changes of subjects. Afterpretraining, we apply our model in a zero-shot manner to a number of videodatasets. Extensive qualitative and quantitative experiments demonstrate theeffectiveness of our model -- it not only captures the semantic prompt fromtext but also the spatial ones from motion guidance. We train all our modelsusing a single node of 16xV100 GPUs. Code, dataset and models are made publicat: https://hiteshk03.github.io/Pix2Gif/.</description><author>Hitesh Kandala, Jianfeng Gao, Jianwei Yang</author><pubDate>Fri, 08 Mar 2024 18:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04634v2</guid></item><item><title>Pix2Gif: Motion-Guided Diffusion for GIF Generation</title><link>http://arxiv.org/abs/2403.04634v1</link><description>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)generation. We tackle this problem differently by formulating the task as animage translation problem steered by text and motion magnitude prompts, asshown in teaser fig. To ensure that the model adheres to motion guidance, wepropose a new motion-guided warping module to spatially transform the featuresof the source image conditioned on the two types of prompts. Furthermore, weintroduce a perceptual loss to ensure the transformed feature map remainswithin the same space as the target image, ensuring content consistency andcoherence. In preparation for the model training, we meticulously curated databy extracting coherent image frames from the TGIF video-caption dataset, whichprovides rich information about the temporal changes of subjects. Afterpretraining, we apply our model in a zero-shot manner to a number of videodatasets. Extensive qualitative and quantitative experiments demonstrate theeffectiveness of our model -- it not only captures the semantic prompt fromtext but also the spatial ones from motion guidance. We train all our modelsusing a single node of 16xV100 GPUs. Code, dataset and models are made publicat: https://hiteshk03.github.io/Pix2Gif/.</description><author>Hitesh Kandala, Jianfeng Gao, Jianwei Yang</author><pubDate>Thu, 07 Mar 2024 16:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04634v1</guid></item><item><title>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</title><link>http://arxiv.org/abs/2402.10698v1</link><description>We present Q-ViD, a simple approach for video question answering (video QA),that unlike prior methods, which are based on complex architectures,computationally expensive pipelines or use closed models like GPTs, Q-ViDrelies on a single instruction-aware open vision-language model (InstructBLIP)to tackle videoQA using frame descriptions. Specifically, we create captioninginstruction prompts that rely on the target questions about the videos andleverage InstructBLIP to obtain video frame captions that are useful to thetask at hand. Subsequently, we form descriptions of the whole video using thequestion-dependent frame captions, and feed that information, along with aquestion-answering prompt, to a large language model (LLM). The LLM is ourreasoning module, and performs the final step of multiple-choice QA. Our simpleQ-ViD framework achieves competitive or even higher performances than currentstate of the art models on a diverse range of videoQA benchmarks, includingNExT-QA, STAR, How2QA, TVQA and IntentQA.</description><author>David Romero, Thamar Solorio</author><pubDate>Fri, 16 Feb 2024 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10698v1</guid></item><item><title>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</title><link>http://arxiv.org/abs/2404.05726v1</link><description>With the success of large language models (LLMs), integrating the visionmodel into LLMs to build vision-language foundation models has gained much moreinterest recently. However, existing LLM-based large multimodal models (e.g.,Video-LLaMA, VideoChat) can only take in a limited number of frames for shortvideo understanding. In this study, we mainly focus on designing an efficientand effective model for long-term video understanding. Instead of trying toprocess more frames simultaneously like most existing work, we propose toprocess videos in an online manner and store past video information in a memorybank. This allows our model to reference historical video content for long-termanalysis without exceeding LLMs' context length constraints or GPU memorylimits. Our memory bank can be seamlessly integrated into current multimodalLLMs in an off-the-shelf manner. We conduct extensive experiments on variousvideo understanding tasks, such as long-video understanding, video questionanswering, and video captioning, and our model can achieve state-of-the-artperformances across multiple datasets. Code available athttps://boheumd.github.io/MA-LMM/.</description><author>Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim</author><pubDate>Mon, 08 Apr 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05726v1</guid></item><item><title>The 8th AI City Challenge</title><link>http://arxiv.org/abs/2404.09432v1</link><description>The eighth AI City Challenge highlighted the convergence of computer visionand artificial intelligence in areas like retail, warehouse settings, andIntelligent Traffic Systems (ITS), presenting significant researchopportunities. The 2024 edition featured five tracks, attracting unprecedentedinterest from 726 teams in 47 countries and regions. Track 1 dealt withmulti-target multi-camera (MTMC) people tracking, highlighting significantenhancements in camera count, character number, 3D annotation, and cameramatrices, alongside new rules for 3D tracking and online tracking algorithmencouragement. Track 2 introduced dense video captioning for traffic safety,focusing on pedestrian accidents using multi-camera feeds to improve insightsfor insurance and prevention. Track 3 required teams to classify driver actionsin a naturalistic driving analysis. Track 4 explored fish-eye camera analyticsusing the FishEye8K dataset. Track 5 focused on motorcycle helmet ruleviolation detection. The challenge utilized two leaderboards to showcasemethods, with participants setting new benchmarks, some surpassing existingstate-of-the-art achievements.</description><author>Shuo Wang, David C. Anastasiu, Zheng Tang, Ming-Ching Chang, Yue Yao, Liang Zheng, Mohammed Shaiqur Rahman, Meenakshi S. Arya, Anuj Sharma, Pranamesh Chakraborty, Sanjita Prajapati, Quan Kong, Norimasa Kobori, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Fady Alnajjar, Ganzorig Batnasan, Ping-Yang Chen, Jun-Wei Hsieh, Xunlei Wu, Sameer Satish Pusegaonkar, Yizhou Wang, Sujit Biswas, Rama Chellappa</author><pubDate>Mon, 15 Apr 2024 04:12:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09432v1</guid></item><item><title>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</title><link>http://arxiv.org/abs/2404.14471v1</link><description>In this paper, we investigate a new problem called narrative actionevaluation (NAE). NAE aims to generate professional commentary that evaluatesthe execution of an action. Unlike traditional tasks such as score-based actionquality assessment and video captioning involving superficial sentences, NAEfocuses on creating detailed narratives in natural language. These narrativesprovide intricate descriptions of actions along with objective evaluations. NAEis a more challenging task because it requires both narrative flexibility andevaluation rigor. One existing possible solution is to use multi-task learning,where narrative language and evaluative information are predicted separately.However, this approach results in reduced performance for individual tasksbecause of variations between tasks and differences in modality betweenlanguage information and evaluation information. To address this, we propose aprompt-guided multimodal interaction framework. This framework utilizes a pairof transformers to facilitate the interaction between different modalities ofinformation. It also uses prompts to transform the score regression task into avideo-text matching task, thus enabling task interactivity. To support furtherresearch in this field, we re-annotate the MTL-AQA and FineGym datasets withhigh-quality and comprehensive action narration. Additionally, we establishbenchmarks for NAE. Extensive experiment results prove that our methodoutperforms separate learning methods and naive multi-task learning methods.Data and code are released at\href{https://github.com/shiyi-zh0408/NAE_CVPR2024 }{here}.</description><author>Shiyi Zhang, Sule Bai, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang</author><pubDate>Mon, 22 Apr 2024 18:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14471v1</guid></item><item><title>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</title><link>http://arxiv.org/abs/2404.05726v2</link><description>With the success of large language models (LLMs), integrating the visionmodel into LLMs to build vision-language foundation models has gained much moreinterest recently. However, existing LLM-based large multimodal models (e.g.,Video-LLaMA, VideoChat) can only take in a limited number of frames for shortvideo understanding. In this study, we mainly focus on designing an efficientand effective model for long-term video understanding. Instead of trying toprocess more frames simultaneously like most existing work, we propose toprocess videos in an online manner and store past video information in a memorybank. This allows our model to reference historical video content for long-termanalysis without exceeding LLMs' context length constraints or GPU memorylimits. Our memory bank can be seamlessly integrated into current multimodalLLMs in an off-the-shelf manner. We conduct extensive experiments on variousvideo understanding tasks, such as long-video understanding, video questionanswering, and video captioning, and our model can achieve state-of-the-artperformances across multiple datasets. Code available athttps://boheumd.github.io/MA-LMM/.</description><author>Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim</author><pubDate>Wed, 24 Apr 2024 16:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05726v2</guid></item><item><title>LVCHAT: Facilitating Long Video Comprehension</title><link>http://arxiv.org/abs/2402.12079v1</link><description>Enabling large language models (LLMs) to read videos is vital for multimodalLLMs. Existing works show promise on short videos whereas long video (longerthan e.g.~1 minute) comprehension remains challenging. The major problem liesin the over-compression of videos, i.e., the encoded video representations arenot enough to represent the whole video. To address this issue, we propose LongVideo Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced todynamically adjust the number of embeddings in alignment with the duration ofthe video to ensure long videos are not overly compressed into a fewembeddings. To deal with long videos whose length is beyond videos seen duringtraining, we propose Interleaved Frame Encoding (IFE), repeating positionalembedding and interleaving multiple groups of videos to enable long videoinput, avoiding performance degradation due to overly long videos. Experimentalresults show that LVChat significantly outperforms existing methods by up to27\% in accuracy on long-video QA datasets and long-video captioningbenchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.</description><author>Yu Wang, Zeyuan Zhang, Julian McAuley, Zexue He</author><pubDate>Mon, 19 Feb 2024 11:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12079v1</guid></item><item><title>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</title><link>http://arxiv.org/abs/2404.14471v2</link><description>In this paper, we investigate a new problem called narrative actionevaluation (NAE). NAE aims to generate professional commentary that evaluatesthe execution of an action. Unlike traditional tasks such as score-based actionquality assessment and video captioning involving superficial sentences, NAEfocuses on creating detailed narratives in natural language. These narrativesprovide intricate descriptions of actions along with objective evaluations. NAEis a more challenging task because it requires both narrative flexibility andevaluation rigor. One existing possible solution is to use multi-task learning,where narrative language and evaluative information are predicted separately.However, this approach results in reduced performance for individual tasksbecause of variations between tasks and differences in modality betweenlanguage information and evaluation information. To address this, we propose aprompt-guided multimodal interaction framework. This framework utilizes a pairof transformers to facilitate the interaction between different modalities ofinformation. It also uses prompts to transform the score regression task into avideo-text matching task, thus enabling task interactivity. To support furtherresearch in this field, we re-annotate the MTL-AQA and FineGym datasets withhigh-quality and comprehensive action narration. Additionally, we establishbenchmarks for NAE. Extensive experiment results prove that our methodoutperforms separate learning methods and naive multi-task learning methods.Data and code are released at https://github.com/shiyi-zh0408/NAE_CVPR2024.</description><author>Shiyi Zhang, Sule Bai, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang</author><pubDate>Fri, 26 Apr 2024 15:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14471v2</guid></item><item><title>Improving Interpretable Embeddings for Ad-hoc Video Search with Generative Captions and Multi-word Concept Bank</title><link>http://arxiv.org/abs/2404.06173v1</link><description>Aligning a user query and video clips in cross-modal latent space and thatwith semantic concepts are two mainstream approaches for ad-hoc video search(AVS). However, the effectiveness of existing approaches is bottlenecked by thesmall sizes of available video-text datasets and the low quality of conceptbanks, which results in the failures of unseen queries and theout-of-vocabulary problem. This paper addresses these two problems byconstructing a new dataset and developing a multi-word concept bank.Specifically, capitalizing on a generative model, we construct a new datasetconsisting of 7 million generated text and video pairs for pre-training. Totackle the out-of-vocabulary problem, we develop a multi-word concept bankbased on syntax analysis to enhance the capability of a state-of-the-artinterpretable AVS method in modeling relationships between query words. We alsostudy the impact of current advanced features on the method. Experimentalresults show that the integration of the above-proposed elements doubles theR@1 performance of the AVS method on the MSRVTT dataset and improves the xinfAPon the TRECVid AVS query sets for 2016-2023 (eight years) by a margin from 2%to 77%, with an average about 20%.</description><author>Jiaxin Wu, Chong-Wah Ngo, Wing-Kwong Chan</author><pubDate>Tue, 09 Apr 2024 10:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06173v1</guid></item><item><title>Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT</title><link>http://arxiv.org/abs/2402.15746v1</link><description>With the rise of short video platforms represented by TikTok, the trend ofusers expressing their creativity through photos and videos has increaseddramatically. However, ordinary users lack the professional skills to producehigh-quality videos using professional creation software. To meet the demandfor intelligent and user-friendly video creation tools, we propose the DynamicVisual Composition (DVC) task, an interesting and challenging task that aims toautomatically integrate various media elements based on user requirements andcreate storytelling videos. We propose an Intelligent Director framework,utilizing LENS to generate descriptions for images and video frames andcombining ChatGPT to generate coherent captions while recommending appropriatemusic names. Then, the best-matched music is obtained through music retrieval.Then, materials such as captions, images, videos, and music are integrated toseamlessly synthesize the video. Finally, we apply AnimeGANv2 for styletransfer. We construct UCF101-DVC and Personal Album datasets and verified theeffectiveness of our framework in solving DVC through qualitative andquantitative comparisons, along with user studies, demonstrating itssubstantial potential.</description><author>Sixiao Zheng, Jingyang Huo, Yu Wang, Yanwei Fu</author><pubDate>Sat, 24 Feb 2024 06:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15746v1</guid></item><item><title>Improved Baselines for Data-efficient Perceptual Augmentation of LLMs</title><link>http://arxiv.org/abs/2403.13499v1</link><description>The abilities of large language models (LLMs) have recently progressed tounprecedented levels, paving the way to novel applications in a wide variety ofareas. In computer vision, LLMs can be used to prime vision-language tasks suchimage captioning and visual question answering when coupled with pre-trainedvision backbones. While different approaches have been explored to interfaceLLMs with ``perceptual backbones'' that process, e.g., visual or audio data,they are often explored for different tasks, different datasets, and usingdifferent perceptual backbones and language models, hindering direct comparisonof the interfacing mechanisms. To remedy this lack of comparability betweenmethods, we present an extensive experimental evaluation of differentinterfacing mechanisms, across multiple tasks (including image, video, andaudio captioning as well as visual question answering), datasets and backbones,paying special attention to low-data settings. We find improved performanceusing existing mechanisms over state-of-the-art results, and identify a newinterfacing mechanism that yields (near) optimal results across differenttasks, while obtaining a 4x reduction in training time.</description><author>Théophane Vallaeys, Mustafa Shukor, Matthieu Cord, Jakob Verbeek</author><pubDate>Wed, 20 Mar 2024 11:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13499v1</guid></item><item><title>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</title><link>http://arxiv.org/abs/2403.15377v1</link><description>We introduce InternVideo2, a new video foundation model (ViFM) that achievesthe state-of-the-art performance in action recognition, video-text tasks, andvideo-centric dialogue. Our approach employs a progressive training paradigmthat unifies the different self- or weakly-supervised learning frameworks ofmasked video token reconstruction, cross-modal contrastive learning, and nexttoken prediction. Different training stages would guide our model to capturedifferent levels of structure and semantic information through differentpretext tasks. At the data level, we prioritize the spatiotemporal consistencyby semantically segmenting videos and generating video-audio-speech captions.This improves the alignment between video and text. We scale both data andmodel size for our InternVideo2. Through extensive experiments, we validate ourdesigns and demonstrate the state-of-the-art performance on over 60 video andaudio tasks. Notably, our model outperforms others on various video-relatedcaptioning, dialogue, and long video understanding benchmarks, highlighting itsability to reason and comprehend long temporal contexts. Code and models areavailable at https://github.com/OpenGVLab/InternVideo2/.</description><author>Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang</author><pubDate>Fri, 22 Mar 2024 18:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15377v1</guid></item><item><title>MoReVQA: Exploring Modular Reasoning Models for Video Question Answering</title><link>http://arxiv.org/abs/2404.06511v1</link><description>This paper addresses the task of video question answering (videoQA) via adecomposed multi-stage, modular reasoning framework. Previous modular methodshave shown promise with a single planning stage ungrounded in visual content.However, through a simple and effective baseline, we find that such systems canlead to brittle behavior in practice for challenging videoQA settings. Thus,unlike traditional single-stage planning methods, we propose a multi-stagesystem consisting of an event parser, a grounding stage, and a final reasoningstage in conjunction with an external memory. All stages are training-free, andperformed using few-shot prompting of large models, creating interpretableintermediate outputs at each stage. By decomposing the underlying planning andtask complexity, our method, MoReVQA, improves over prior work on standardvideoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) withstate-of-the-art results, and extensions to related tasks (grounded videoQA,paragraph captioning).</description><author>Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid</author><pubDate>Tue, 09 Apr 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06511v1</guid></item><item><title>Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward</title><link>http://arxiv.org/abs/2404.01258v2</link><description>Preference modeling techniques, such as direct preference optimization (DPO),has shown effective in enhancing the generalization abilities of large languagemodel (LLM). However, in tasks involving video instruction-following, providinginformative feedback, especially for detecting hallucinations in generatedresponses, remains a significant challenge. Previous studies have exploredusing large large multimodal models (LMMs) as reward models to guide preferencemodeling, but their ability to accurately assess the factuality of generatedresponses compared to corresponding videos has not been conclusivelyestablished. This paper introduces a novel framework that utilizes detailedvideo captions as a proxy of video content, enabling language models toincorporate this information as supporting evidence for scoring video QuestionAnswering (QA) predictions. Our approach demonstrates robust alignment withOpenAI GPT-4V model's reward mechanism, which directly takes video frames asinput. Furthermore, we show that applying this tailored reward through DPOsignificantly improves the performance of video LMMs on video QA tasks.</description><author>Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, Yiming Yang</author><pubDate>Tue, 02 Apr 2024 13:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01258v2</guid></item><item><title>HawkEye: Training Video-Text LLMs for Grounding Text in Videos</title><link>http://arxiv.org/abs/2403.10228v1</link><description>Video-text Large Language Models (video-text LLMs) have shown remarkableperformance in answering questions and holding conversations on simple videos.However, they perform almost the same as random on grounding text queries inlong and complicated videos, having little ability to understand and reasonabout temporal information, which is the most fundamental difference betweenvideos and images. In this paper, we propose HawkEye, one of the firstvideo-text LLMs that can perform temporal video grounding in a fullytext-to-text manner. To collect training data that is applicable for temporalvideo grounding, we construct InternVid-G, a large-scale video-text corpus withsegment-level captions and negative spans, with which we introduce two newtime-aware training objectives to video-text LLMs. We also propose acoarse-grained method of representing segments in videos, which is more robustand easier for LLMs to learn and follow than other alternatives. Extensiveexperiments show that HawkEye is better at temporal video grounding andcomparable on other video-text tasks with existing video-text LLMs, whichverifies its superior video-text multi-modal understanding abilities.</description><author>Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, Dongyan Zhao</author><pubDate>Fri, 15 Mar 2024 12:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10228v1</guid></item><item><title>Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection</title><link>http://arxiv.org/abs/2403.01169v1</link><description>Most models for weakly supervised video anomaly detection (WS-VAD) rely onmultiple instance learning, aiming to distinguish normal and abnormal snippetswithout specifying the type of anomaly. The ambiguous nature of anomalydefinitions across contexts introduces bias in detecting abnormal and normalsnippets within the abnormal bag. Taking the first step to show the model whyit is anomalous, a novel framework is proposed to guide the learning ofsuspected anomalies from event prompts. Given a textual prompt dictionary ofpotential anomaly events and the captions generated from anomaly videos, thesemantic anomaly similarity between them could be calculated to identify thesuspected anomalous events for each video snippet. It enables a newmulti-prompt learning process to constrain the visual-semantic features acrossall videos, as well as provides a new way to label pseudo anomalies forself-training. To demonstrate effectiveness, comprehensive experiments anddetailed ablation studies are conducted on four datasets, namely XD-Violence,UCF-Crime, TAD, and ShanghaiTech. Our proposed model outperforms moststate-of-the-art methods in terms of AP or AUC (82.6\%, 87.7\%, 93.1\%, and97.4\%). Furthermore, it shows promising performance in open-set andcross-dataset cases.</description><author>Chenchen Tao, Chong Wang, Yuexian Zou, Xiaohao Peng, Jiafei Wu, Jiangbo Qian</author><pubDate>Sat, 02 Mar 2024 10:42:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01169v1</guid></item><item><title>End-to-End Dense Video Grounding via Parallel Regression</title><link>http://arxiv.org/abs/2109.11265v5</link><description>Video grounding aims to localize the corresponding video moment in anuntrimmed video given a language query. Existing methods often address thistask in an indirect way, by casting it as a proposal-and-match orfusion-and-detection problem. Solving these surrogate problems often requiressophisticated label assignment during training and hand-crafted removal ofnear-duplicate results. Meanwhile, existing works typically focus on sparsevideo grounding with a single sentence as input, which could result inambiguous localization due to its unclear description. In this paper, we tacklea new problem of dense video grounding, by simultaneously localizing multiplemoments with a paragraph as input. From a perspective on video grounding aslanguage conditioned regression, we present an end-to-end parallel decodingparadigm by re-purposing a Transformer-alike architecture (PRVG). The keydesign in our PRVG is to use languages as queries, and directly regress themoment boundaries based on language-modulated visual representations. Thanks toits simplicity in design, our PRVG framework can be applied in differenttesting schemes (sparse or dense grounding) and allows for efficient inferencewithout any post-processing technique. In addition, we devise a robustproposal-level attention loss to guide the training of PRVG, which is invariantto moment duration and contributes to model convergence. We perform experimentson two video grounding benchmarks of ActivityNet Captions and TACoS,demonstrating that our PRVG can significantly outperform previous methods. Wealso perform in-depth studies to investigate the effectiveness of parallelregression paradigm on video grounding.</description><author>Fengyuan Shi, Weilin Huang, Limin Wang</author><pubDate>Wed, 28 Feb 2024 13:04:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.11265v5</guid></item><item><title>PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter</title><link>http://arxiv.org/abs/2402.10896v1</link><description>This paper demonstrates that a progressively aligned language model caneffectively bridge frozen vision encoders and large language models (LLMs).While the fundamental architecture and pre-training methods of vision encodersand LLMs have been extensively studied, the architecture and training strategyof vision-language adapters vary significantly across recent works. Ourresearch undertakes a thorough exploration of the state-of-the-art perceiverresampler architecture and builds a strong baseline. However, we observe thatthe vision-language alignment with perceiver resampler exhibits slowconvergence and limited scalability with a lack of direct supervision. Toaddress this issue, we propose PaLM2-VAdapter, employing a progressivelyaligned language model as the vision-language adapter. Compared to the strongbaseline with perceiver resampler, our method empirically shows fasterconvergence, higher performance, and stronger scalability. Extensiveexperiments across various Visual Question Answering (VQA) and captioning taskson both images and videos demonstrate that our model exhibits state-of-the-artvisual understanding and multi-modal reasoning capabilities. Notably, ourmethod achieves these advancements with 30~70% fewer parameters than thestate-of-the-art large vision-language models, marking a significant efficiencyimprovement.</description><author>Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang</author><pubDate>Fri, 16 Feb 2024 18:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10896v1</guid></item><item><title>ICSVR: Investigating Compositional and Syntactic Understanding in Video Retrieval Models</title><link>http://arxiv.org/abs/2306.16533v2</link><description>Video retrieval (VR) involves retrieving the ground truth video from thevideo database given a text caption or vice-versa. The two important componentsof compositionality: objects &amp; attributes and actions are joined using correctsyntax to form a proper text query. These components (objects &amp; attributes,actions and syntax) each play an important role to help distinguish amongvideos and retrieve the correct ground truth video. However, it is unclear whatis the effect of these components on the video retrieval performance. Wetherefore, conduct a systematic study to evaluate the compositional andsyntactic understanding of video retrieval models on standard benchmarks suchas MSRVTT, MSVD and DIDEMO. The study is performed on two categories of videoretrieval models: (i) which are pre-trained on video-text pairs and fine-tunedon downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)(ii) which adapt pre-trained image-text representations like CLIP for videoretrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal thatactions and syntax play a minor role compared to objects &amp; attributes in videounderstanding. Moreover, video retrieval models that use pre-trained image-textrepresentations (CLIP) have better syntactic and compositional understanding ascompared to models pre-trained on video-text data. The code is available athttps://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/ICSVR</description><author>Avinash Madasu, Vasudev Lal</author><pubDate>Wed, 17 Apr 2024 12:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16533v2</guid></item><item><title>Self-Explainable Affordance Learning with Embodied Caption</title><link>http://arxiv.org/abs/2404.05603v1</link><description>In the field of visual affordance learning, previous methods mainly usedabundant images or videos that delineate human behavior patterns to identifyaction possibility regions for object manipulation, with a variety ofapplications in robotic tasks. However, they encounter a main challenge ofaction ambiguity, illustrated by the vagueness like whether to beat or carry adrum, and the complexities involved in processing intricate scenes. Moreover,it is important for human intervention to rectify robot errors in time. Toaddress these issues, we introduce Self-Explainable Affordance learning (SEA)with embodied caption. This innovation enables robots to articulate theirintentions and bridge the gap between explainable vision-language caption andvisual affordance learning. Due to a lack of appropriate dataset, we unveil apioneering dataset and metrics tailored for this task, which integrates images,heatmaps, and embodied captions. Furthermore, we propose a novel model toeffectively combine affordance grounding with self-explanation in a simple butefficient manner. Extensive quantitative and qualitative experimentsdemonstrate our method's effectiveness.</description><author>Zhipeng Zhang, Zhimin Wei, Guolei Sun, Peng Wang, Luc Van Gool</author><pubDate>Mon, 08 Apr 2024 16:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05603v1</guid></item><item><title>VidCoM: Fast Video Comprehension through Large Language Models with Multimodal Tools</title><link>http://arxiv.org/abs/2310.10586v2</link><description>Building models that comprehends videos and responds specific userinstructions is a practical and challenging topic, as it requires mastery ofboth vision understanding and knowledge reasoning. Compared to language andimage modalities, training efficiency remains a serious problem as existingstudies train models on massive sparse videos paired with brief descriptions.In this paper, we introduce \textbf{VidCoM}, a fast adaptive framework thatleverages Large Language Models (LLMs) to reason about videos using lightweightvisual tools. Specifically, we reveal that the key to responding to specificinstructions is focusing on relevant video events, and utilize two visualtools, structured scene graph generation and descriptive image captiongeneration, to gather and represent the event information. Thus, a LLM enrichedwith world knowledge is adopted as the reasoning agent to achieve the responsesby performing multiple reasoning steps on specific video events. To address thedifficulty of LLMs identifying video events, we further propose anInstruction-oriented Video Events Recognition (InsOVER) algorithm. Thisalgorithm locates the corresponding video events based on an efficientHungarian matching between decompositions of linguistic instructions and videoevents, thereby enabling LLMs to interact effectively with extended videos.Extensive experiments on two typical video comprehension tasks show that theproposed tuning-free framework outperforms the pre-trained models includingFlamingo-80B, to achieve the state-of-the-art performance. Our source code andsystem will be publicly available.</description><author>Ji Qi, Kaixuan Ji, Jifan Yu, Duokang Wang, Bin Xu, Lei Hou, Juanzi Li</author><pubDate>Sat, 27 Apr 2024 09:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10586v2</guid></item><item><title>VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT</title><link>http://arxiv.org/abs/2403.02076v1</link><description>Video temporal grounding (VTG) aims to locate specific temporal segments froman untrimmed video based on a linguistic query. Most existing VTG models aretrained on extensive annotated video-text pairs, a process that not onlyintroduces human biases from the queries but also incurs significantcomputational costs. To tackle these challenges, we propose VTG-GPT, aGPT-based method for zero-shot VTG without training or fine-tuning. To reduceprejudice in the original query, we employ Baichuan2 to generate debiasedqueries. To lessen redundant information in videos, we apply MiniGPT-v2 totransform visual content into more precise captions. Finally, we devise theproposal generator and post-processing to produce accurate segments fromdebiased queries and image captions. Extensive experiments demonstrate thatVTG-GPT significantly outperforms SOTA methods in zero-shot settings andsurpasses unsupervised approaches. More notably, it achieves competitiveperformance comparable to supervised methods. The code is available onhttps://github.com/YoucanBaby/VTG-GPT</description><author>Yifang Xu, Yunzhuo Sun, Zien Xie, Benxiang Zhai, Sidan Du</author><pubDate>Mon, 04 Mar 2024 14:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02076v1</guid></item><item><title>Mozart's Touch: A Lightweight Multi-modal Music Generation Framework Based on Pre-Trained Large Models</title><link>http://arxiv.org/abs/2405.02801v2</link><description>In recent years, AI-Generated Content (AIGC) has witnessed rapidadvancements, facilitating the generation of music, images, and other forms ofartistic expression across various industries. However, researches on generalmulti-modal music generation model remain scarce. To fill this gap, we proposea multi-modal music generation framework Mozart's Touch. It could generatealigned music with the cross-modality inputs, such as images, videos and text.Mozart's Touch is composed of three main components: Multi-modal CaptioningModule, Large Language Model (LLM) Understanding &amp; Bridging Module, and MusicGeneration Module. Unlike traditional approaches, Mozart's Touch requires notraining or fine-tuning pre-trained models, offering efficiency andtransparency through clear, interpretable prompts. We also introduce"LLM-Bridge" method to resolve the heterogeneous representation problemsbetween descriptive texts of different modalities. We conduct a series ofobjective and subjective evaluations on the proposed model, and resultsindicate that our model surpasses the performance of current state-of-the-artmodels. Our codes and examples is availble at:https://github.com/WangTooNaive/MozartsTouch</description><author>Tianze Xu, Jiajun Li, Xuesong Chen, Xinrui Yao, Shuchang Liu</author><pubDate>Tue, 07 May 2024 10:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02801v2</guid></item><item><title>OSCaR: Object State Captioning and State Change Representation</title><link>http://arxiv.org/abs/2402.17128v4</link><description>The capability of intelligent models to extrapolate and comprehend changes inobject states is a crucial yet demanding aspect of AI research, particularlythrough the lens of human interaction in real-world settings. This taskinvolves describing complex visual environments, identifying active objects,and interpreting their changes as conveyed through language. Traditionalmethods, which isolate object captioning and state change detection, offer alimited view of dynamic environments. Moreover, relying on a small set ofsymbolic words to represent changes has restricted the expressiveness of thelanguage. To address these challenges, in this paper, we introduce the ObjectState Captioning and State Change Representation (OSCaR) dataset and benchmark.OSCaR consists of 14,084 annotated video segments with nearly 1,000 uniqueobjects from various egocentric video collections. It sets a new testbed forevaluating multimodal large language models (MLLMs). Our experimentsdemonstrate that while MLLMs show some skill, they lack a full understanding ofobject state changes. The benchmark includes a fine-tuned model that, despiteinitial capabilities, requires significant improvements in accuracy andgeneralization ability for effective understanding of these changes. Our codeand dataset are available at https://github.com/nguyennm1024/OSCaR.</description><author>Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu</author><pubDate>Wed, 03 Apr 2024 00:14:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17128v4</guid></item><item><title>OSCaR: Object State Captioning and State Change Representation</title><link>http://arxiv.org/abs/2402.17128v3</link><description>The capability of intelligent models to extrapolate and comprehend changes inobject states is a crucial yet demanding aspect of AI research, particularlythrough the lens of human interaction in real-world settings. This taskinvolves describing complex visual environments, identifying active objects,and interpreting their changes as conveyed through language. Traditionalmethods, which isolate object captioning and state change detection, offer alimited view of dynamic environments. Moreover, relying on a small set ofsymbolic words to represent changes has restricted the expressiveness of thelanguage. To address these challenges, in this paper, we introduce the ObjectState Captioning and State Change Representation (OSCaR) dataset and benchmark.OSCaR consists of 14,084 annotated video segments with nearly 1,000 uniqueobjects from various egocentric video collections. It sets a new testbed forevaluating multimodal large language models (MLLMs). Our experimentsdemonstrate that while MLLMs show some skill, they lack a full understanding ofobject state changes. The benchmark includes a fine-tuned model that, despiteinitial capabilities, requires significant improvements in accuracy andgeneralization ability for effective understanding of these changes. Our codeand dataset are available at https://github.com/nguyennm1024/OSCaR.</description><author>Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu</author><pubDate>Wed, 20 Mar 2024 15:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17128v3</guid></item><item><title>OSCaR: Object State Captioning and State Change Representation</title><link>http://arxiv.org/abs/2402.17128v2</link><description>The capability of intelligent models to extrapolate and comprehend changes inobject states is a crucial yet demanding aspect of AI research, particularlythrough the lens of human interaction in real-world settings. This taskinvolves describing complex visual environments, identifying active objects,and interpreting their changes as conveyed through language. Traditionalmethods, which isolate object captioning and state change detection, offer alimited view of dynamic environments. Moreover, relying on a small set ofsymbolic words to represent changes has restricted the expressiveness oflanguage. To address these challenges, in this paper, we introduce the ObjectState Captioning and State Change Representation (OSCaR) dataset and benchmark.OSCaR consists of 14,084 annotated video segments with nearly 1,000 uniqueobjects from various egocentric video collections. It sets a new testbed forevaluating multimodal large language models (MLLMs). Our experimentsdemonstrate that while MLLMs show some skill, they lack a full understanding ofobject state changes. The benchmark includes a fine-tuned model that, despiteinitial capabilities, requires significant improvements in accuracy andgeneralization ability for effective understanding of these changes. Our codeand dataset are available at https://github.com/nguyennm1024/OSCaR.</description><author>Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu</author><pubDate>Wed, 28 Feb 2024 02:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17128v2</guid></item><item><title>TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</title><link>http://arxiv.org/abs/2312.02051v2</link><description>This work proposes TimeChat, a time-sensitive multimodal large language modelspecifically designed for long video understanding. Our model incorporates twokey architectural contributions: (1) a timestamp-aware frame encoder that bindsvisual content with the timestamp of each frame, and (2) a sliding videoQ-Former that produces a video token sequence of varying lengths to accommodatevideos of various durations. Additionally, we construct an instruction-tuningdataset, encompassing 6 tasks and a total of 125K instances, to further enhanceTimeChat's instruction-following performance. Experiment results across variousvideo understanding tasks, such as dense captioning, temporal grounding, andhighlight detection, demonstrate TimeChat's strong zero-shot temporallocalization and reasoning capabilities. For example, it achieves +9.2 F1 scoreand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)on Charades-STA, compared to state-of-the-art video large language models,holding the potential to serve as a versatile video assistant for long-formvideo comprehension tasks and satisfy realistic user requirements.</description><author>Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou</author><pubDate>Thu, 28 Mar 2024 13:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02051v2</guid></item><item><title>EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition</title><link>http://arxiv.org/abs/2310.16640v2</link><description>Facial Expression Recognition (FER) is a crucial task in affective computing,but its conventional focus on the seven basic emotions limits its applicabilityto the complex and expanding emotional spectrum. To address the issue of newand unseen emotions present in dynamic in-the-wild FER, we propose a novelvision-language model that utilises sample-level text descriptions (i.e.captions of the context, expressions or emotional cues) as natural languagesupervision, aiming to enhance the learning of rich latent representations, forzero-shot classification. To test this, we evaluate using zero-shotclassification of the model trained on sample-level descriptions on fourpopular dynamic FER datasets. Our findings show that this approach yieldssignificant improvements when compared to baseline methods. Specifically, forzero-shot video FER, we outperform CLIP by over 10\% in terms of WeightedAverage Recall and 5\% in terms of Unweighted Average Recall on severaldatasets. Furthermore, we evaluate the representations obtained from thenetwork trained using sample-level descriptions on the downstream task ofmental health symptom estimation, achieving performance comparable or superiorto state-of-the-art methods and strong agreement with human experts. Namely, weachieve a Pearson's Correlation Coefficient of up to 0.85 on schizophreniasymptom severity estimation, which is comparable to human experts' agreement.The code is publicly available at: https://github.com/NickyFot/EmoCLIP.</description><author>Niki Maria Foteinopoulou, Ioannis Patras</author><pubDate>Mon, 18 Mar 2024 10:07:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16640v2</guid></item><item><title>Test-Time Zero-Shot Temporal Action Localization</title><link>http://arxiv.org/abs/2404.05426v1</link><description>Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locateactions in untrimmed videos unseen during training. Existing ZS-TAL methodsinvolve fine-tuning a model on a large amount of annotated training data. Whileeffective, training-based ZS-TAL approaches assume the availability of labeleddata for supervised learning, which can be impractical in some applications.Furthermore, the training process naturally induces a domain bias into thelearned model, which may adversely affect the model's generalization ability toarbitrary videos. These considerations prompt us to approach the ZS-TAL problemfrom a radically novel perspective, relaxing the requirement for training data.To this aim, we introduce a novel method that performs Test-Time adaptation forTemporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trainedVision and Language Model (VLM). T3AL operates in three steps. First, avideo-level pseudo-label of the action category is computed by aggregatinginformation from the entire video. Then, action localization is performedadopting a novel procedure inspired by self-supervised learning. Finally,frame-level textual descriptions extracted with a state-of-the-art captioningmodel are employed for refining the action region proposals. We validate theeffectiveness of T3AL by conducting experiments on the THUMOS14 and theActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantlyoutperforms zero-shot baselines based on state-of-the-art VLMs, confirming thebenefit of a test-time adaptation approach.</description><author>Benedetta Liberatori, Alessandro Conti, Paolo Rota, Yiming Wang, Elisa Ricci</author><pubDate>Mon, 08 Apr 2024 12:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05426v1</guid></item><item><title>Test-Time Zero-Shot Temporal Action Localization</title><link>http://arxiv.org/abs/2404.05426v2</link><description>Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locateactions in untrimmed videos unseen during training. Existing ZS-TAL methodsinvolve fine-tuning a model on a large amount of annotated training data. Whileeffective, training-based ZS-TAL approaches assume the availability of labeleddata for supervised learning, which can be impractical in some applications.Furthermore, the training process naturally induces a domain bias into thelearned model, which may adversely affect the model's generalization ability toarbitrary videos. These considerations prompt us to approach the ZS-TAL problemfrom a radically novel perspective, relaxing the requirement for training data.To this aim, we introduce a novel method that performs Test-Time adaptation forTemporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trainedVision and Language Model (VLM). T3AL operates in three steps. First, avideo-level pseudo-label of the action category is computed by aggregatinginformation from the entire video. Then, action localization is performedadopting a novel procedure inspired by self-supervised learning. Finally,frame-level textual descriptions extracted with a state-of-the-art captioningmodel are employed for refining the action region proposals. We validate theeffectiveness of T3AL by conducting experiments on the THUMOS14 and theActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantlyoutperforms zero-shot baselines based on state-of-the-art VLMs, confirming thebenefit of a test-time adaptation approach.</description><author>Benedetta Liberatori, Alessandro Conti, Paolo Rota, Yiming Wang, Elisa Ricci</author><pubDate>Thu, 11 Apr 2024 08:12:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05426v2</guid></item><item><title>LLM-AD: Large Language Model based Audio Description System</title><link>http://arxiv.org/abs/2405.00983v1</link><description>The development of Audio Description (AD) has been a pivotal step forward inmaking video content more accessible and inclusive. Traditionally, ADproduction has demanded a considerable amount of skilled labor, while existingautomated approaches still necessitate extensive training to integratemultimodal inputs and tailor the output from a captioning style to an AD style.In this paper, we introduce an automated AD generation pipeline that harnessesthe potent multimodal and instruction-following capacities of GPT-4V(ision).Notably, our methodology employs readily available components, eliminating theneed for additional training. It produces ADs that not only comply withestablished natural language AD production standards but also maintaincontextually consistent character information across frames, courtesy of atracking-based character recognition module. A thorough analysis on the MADdataset reveals that our approach achieves a performance on par withlearning-based methods in automated AD production, as substantiated by a CIDErscore of 20.5.</description><author>Peng Chu, Jiang Wang, Andre Abrantes</author><pubDate>Thu, 02 May 2024 04:38:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00983v1</guid></item><item><title>Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding</title><link>http://arxiv.org/abs/2403.14174v1</link><description>Inspired by the activity-silent and persistent activity mechanisms in humanvisual perception biology, we design a Unified Static and Dynamic Network(UniSDNet), to learn the semantic association between the video and text/audioqueries in a cross-modal environment for efficient video grounding. For staticmodeling, we devise a novel residual structure (ResMLP) to boost the globalcomprehensive interaction between the video segments and queries, achievingmore effective semantic enhancement/supplement. For dynamic modeling, weeffectively exploit three characteristics of the persistent activity mechanismin our network design for a better video context comprehension. Specifically,we construct a diffusely connected video clip graph on the basis of 2D sparsetemporal masking to reflect the "short-term effect" relationship. Weinnovatively consider the temporal distance and relevance as the joint"auxiliary evidence clues" and design a multi-kernel Temporal Gaussian Filterto expand the context clue into high-dimensional space, simulating the "complexvisual perception", and then conduct element level filtering convolutionoperations on neighbour clip nodes in message passing stage for finallygenerating and ranking the candidate proposals. Our UniSDNet is applicable toboth Natural Language Video Grounding (NLVG) and Spoken Language VideoGrounding (SLVG) tasks. Our UniSDNet achieves SOTA performance on three widelyused datasets for NLVG, as well as three datasets for SLVG, e.g., reporting newrecords at 38.88% R@1,IoU@0.7 on ActivityNet Captions and 40.26% R@1,IoU@0.5 onTACoS. To facilitate this field, we collect two new datasets (Charades-STASpeech and TACoS Speech) for SLVG task. Meanwhile, the inference speed of ourUniSDNet is 1.56$\times$ faster than the strong multi-query benchmark. Code isavailable at: https://github.com/xian-sh/UniSDNet.</description><author>Jingjing Hu, Dan Guo, Kun Li, Zhan Si, Xun Yang, Xiaojun Chang, Meng Wang</author><pubDate>Thu, 21 Mar 2024 07:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14174v1</guid></item><item><title>Enhancing Interactive Image Retrieval With Query Rewriting Using Large Language Models and Vision Language Models</title><link>http://arxiv.org/abs/2404.18746v1</link><description>Image search stands as a pivotal task in multimedia and computer vision,finding applications across diverse domains, ranging from internet search tomedical diagnostics. Conventional image search systems operate by acceptingtextual or visual queries, retrieving the top-relevant candidate results fromthe database. However, prevalent methods often rely on single-turn procedures,introducing potential inaccuracies and limited recall. These methods also facethe challenges, such as vocabulary mismatch and the semantic gap, constrainingtheir overall effectiveness. To address these issues, we propose an interactiveimage retrieval system capable of refining queries based on user relevancefeedback in a multi-turn setting. This system incorporates a vision languagemodel (VLM) based image captioner to enhance the quality of text-based queries,resulting in more informative queries with each iteration. Moreover, weintroduce a large language model (LLM) based denoiser to refine text-basedquery expansions, mitigating inaccuracies in image descriptions generated bycaptioning models. To evaluate our system, we curate a new dataset by adaptingthe MSR-VTT video retrieval dataset to the image retrieval task, offeringmultiple relevant ground truth images for each query. Through comprehensiveexperiments, we validate the effectiveness of our proposed system againstbaseline methods, achieving state-of-the-art performance with a notable 10\%improvement in terms of recall. Our contributions encompass the development ofan innovative interactive image retrieval system, the integration of anLLM-based denoiser, the curation of a meticulously designed evaluation dataset,and thorough experimental validation.</description><author>Hongyi Zhu, Jia-Hong Huang, Stevan Rudinac, Evangelos Kanoulas</author><pubDate>Mon, 29 Apr 2024 15:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18746v1</guid></item><item><title>Semi-supervised Text-based Person Search</title><link>http://arxiv.org/abs/2404.18106v1</link><description>Text-based person search (TBPS) aims to retrieve images of a specific personfrom a large image gallery based on a natural language description. Existingmethods rely on massive annotated image-text data to achieve satisfactoryperformance in fully-supervised learning. It poses a significant challenge inpractice, as acquiring person images from surveillance videos is relativelyeasy, while obtaining annotated texts is challenging. The paper undertakes apioneering initiative to explore TBPS under the semi-supervised setting, whereonly a limited number of person images are annotated with textual descriptionswhile the majority of images lack annotations. We present a two-stage basicsolution based on generation-then-retrieval for semi-supervised TBPS. Thegeneration stage enriches annotated data by applying an image captioning modelto generate pseudo-texts for unannotated images. Later, the retrieval stageperforms fully-supervised retrieval learning using the augmented data.Significantly, considering the noise interference of the pseudo-texts onretrieval learning, we propose a noise-robust retrieval framework that enhancesthe ability of the retrieval model to handle noisy data. The frameworkintegrates two key strategies: Hybrid Patch-Channel Masking (PC-Mask) to refinethe model architecture, and Noise-Guided Progressive Training (NP-Train) toenhance the training process. PC-Mask performs masking on the input data atboth the patch-level and the channel-level to prevent overfitting noisysupervision. NP-Train introduces a progressive training schedule based on thenoise level of pseudo-texts to facilitate noise-robust learning. Extensiveexperiments on multiple TBPS benchmarks show that the proposed frameworkachieves promising performance under the semi-supervised setting.</description><author>Daming Gao, Yang Bai, Min Cao, Hao Dou, Mang Ye, Min Zhang</author><pubDate>Sun, 28 Apr 2024 08:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18106v1</guid></item><item><title>CI w/o TN: Context Injection without Task Name for Procedure Planning</title><link>http://arxiv.org/abs/2402.15579v1</link><description>This paper explores the challenge of procedure planning in instructionalvideos, which involves creating goal-directed plans based on visual start andgoal observations from videos. Previous research has tackled this problem withgradually weaker training supervision, from heavy intermediate visualobservations or language instructions to task class supervision. However, withthe advent of large language models, even given only the task name, thesemodels can produce a detailed plan. In this study, we propose a much weakersetting without task name as supervision, which is not currently solvable byexisting large language models since they require good prompts with sufficientinformation. Specifically, we hypothesize that previous intermediatesupervisions can serve as context information, and we use captions of visualstart and goal observations as a much cheaper form of supervision. Thisapproach greatly reduces the labeling cost since the captions can be easilyobtained by large pre-trained vision-language foundation models. Technically,we apply BLIP to generate captions as supervision to train the context featurewith contrastive learning loss. Afterward, the context feature is fed into thegenerator to aid in plan generation. Our experiments on two datasets withvarying scales demonstrate that our model can achieve comparable performance onmultiple metrics, which validates our hypothesis.</description><author>Xinjie Li</author><pubDate>Fri, 23 Feb 2024 19:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15579v1</guid></item></channel></rss>