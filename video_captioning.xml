<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo captioning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 03 Jul 2024 06:00:14 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Streaming Dense Video Captioning</title><link>http://arxiv.org/abs/2404.01297v1</link><description>An ideal model for dense video captioning -- predicting captions localizedtemporally in a video -- should be able to handle long input videos, predictrich, detailed textual descriptions, and be able to produce outputs beforeprocessing the entire video. Current state-of-the-art models, however, processa fixed number of downsampled frames, and make a single full prediction afterseeing the whole video. We propose a streaming dense video captioning modelthat consists of two novel components: First, we propose a new memory module,based on clustering incoming tokens, which can handle arbitrarily long videosas the memory is of a fixed size. Second, we develop a streaming decodingalgorithm that enables our model to make predictions before the entire videohas been processed. Our model achieves this streaming ability, andsignificantly improves the state-of-the-art on three dense video captioningbenchmarks: ActivityNet, YouCook2 and ViTT. Our code is released athttps://github.com/google-research/scenic.</description><author>Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, Cordelia Schmid</author><pubDate>Mon, 01 Apr 2024 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01297v1</guid></item><item><title>Retrieval-Augmented Egocentric Video Captioning</title><link>http://arxiv.org/abs/2401.00789v3</link><description>Understanding human actions from videos of first-person view posessignificant challenges. Most prior approaches explore representation learningon egocentric videos only, while overlooking the potential benefit ofexploiting existing large-scale third-person videos. In this paper, (1) wedevelop EgoInstructor, a retrieval-augmented multimodal captioning model thatautomatically retrieves semantically relevant third-person instructional videosto enhance the video captioning of egocentric videos. (2) For training thecross-view retrieval module, we devise an automatic pipeline to discoverego-exo video pairs from distinct large-scale egocentric and exocentricdatasets. (3) We train the cross-view retrieval module with a novel EgoExoNCEloss that pulls egocentric and exocentric video features closer by aligningthem to shared text features that describe similar actions. (4) Throughextensive experiments, our cross-view retrieval module demonstrates superiorperformance across seven benchmarks. Regarding egocentric video captioning,EgoInstructor exhibits significant improvements by leveraging third-personvideos as references.</description><author>Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie</author><pubDate>Thu, 16 May 2024 03:05:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00789v3</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v6</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Thu, 16 May 2024 13:23:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v6</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v1</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Tue, 20 Feb 2024 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v1</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v2</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Wed, 21 Feb 2024 22:19:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v2</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v3</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Wed, 28 Feb 2024 13:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v3</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v5</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Fri, 10 May 2024 18:47:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v5</guid></item><item><title>NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative</title><link>http://arxiv.org/abs/2406.06499v1</link><description>Existing video captioning benchmarks and models lack coherent representationsof causal-temporal narrative, which is sequences of events linked through causeand effect, unfolding over time and driven by characters or agents. This lackof narrative restricts models' ability to generate text descriptions thatcapture the causal and temporal dynamics inherent in video content. To addressthis gap, we propose NarrativeBridge, an approach comprising of: (1) a novelCausal-Temporal Narrative (CTN) captions benchmark generated using a largelanguage model and few-shot prompting, explicitly encoding cause-effecttemporal relationships in video descriptions, evaluated automatically to ensurecaption quality and relevance; and (2) a dedicated Cause-Effect Network (CEN)architecture with separate encoders for capturing cause and effect dynamicsindependently, enabling effective learning and generation of captions withcausal-temporal narrative. Extensive experiments demonstrate that CEN is moreaccurate in articulating the causal and temporal aspects of video content thanthe second best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTTdatasets, respectively. The proposed framework understands and generatesnuanced text descriptions with intricate causal-temporal narrative structurespresent in videos, addressing a critical limitation in video captioning. Forproject details, visit https://narrativebridge.github.io/.</description><author>Asmar Nadeem, Faegheh Sardari, Robert Dawes, Syed Sameed Husain, Adrian Hilton, Armin Mustafa</author><pubDate>Mon, 10 Jun 2024 18:34:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06499v1</guid></item><item><title>ShareGPT4Video: Improving Video Understanding and Generation with Better Captions</title><link>http://arxiv.org/abs/2406.04325v1</link><description>We present the ShareGPT4Video series, aiming to facilitate the videounderstanding of large video-language models (LVLMs) and the video generationof text-to-video models (T2VMs) via dense and precise captions. The seriescomprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos withvarious lengths and sources, developed through carefully designed datafiltering and annotating strategy. 2) ShareCaptioner-Video, an efficient andcapable captioning model for arbitrary videos, with 4.8M high-quality aestheticvideos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM thatreached SOTA performance on three advancing video benchmarks. To achieve this,taking aside the non-scalable costly human annotators, we find using GPT4V tocaption video with a naive multi-frame or frame-concatenation input strategyleads to less detailed and sometimes temporal-confused results. We argue thechallenge of designing a high-quality video captioning strategy lies in threeaspects: 1) Inter-frame precise temporal change understanding. 2) Intra-framedetailed content description. 3) Frame-number scalability for arbitrary-lengthvideos. To this end, we meticulously designed a differential video captioningstrategy, which is stable, scalable, and efficient for generating captions forvideos with arbitrary resolution, aspect ratios, and length. Based on it, weconstruct ShareGPT4Video, which contains 40K high-quality videos spanning awide range of categories, and the resulting captions encompass rich worldknowledge, object attributes, camera movements, and crucially, detailed andprecise temporal descriptions of events. Based on ShareGPT4Video, we furtherdevelop ShareCaptioner-Video, a superior captioner capable of efficientlygenerating high-quality captions for arbitrary videos...</description><author>Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, Jiaqi Wang</author><pubDate>Thu, 06 Jun 2024 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04325v1</guid></item><item><title>MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning</title><link>http://arxiv.org/abs/2402.17680v1</link><description>To address the problem of catastrophic forgetting due to the invisibility ofold categories in sequential input, existing work based on relatively simplecategorization tasks has made some progress. In contrast, video captioning is amore complex task in multimodal scenario, which has not been explored in thefield of incremental learning. After identifying this stability-plasticityproblem when analyzing video with sequential input, we originally propose amethod to Mitigate Catastrophic Forgetting in class-incremental learning formultimodal Video Captioning (MCF-VC). As for effectively maintaining goodperformance on old tasks at the macro level, we design Fine-grained SensitivitySelection (FgSS) based on the Mask of Linear's Parameters and FisherSensitivity to pick useful knowledge from old tasks. Further, in order tobetter constrain the knowledge characteristics of old and new tasks at thespecific feature level, we have created the Two-stage Knowledge Distillation(TsKD), which is able to learn the new task well while weighing the old task.Specifically, we design two distillation losses, which constrain the crossmodal semantic information of semantic attention feature map and the textualinformation of the final outputs respectively, so that the inter-model andintra-model stylized knowledge of the old class is retained while learning thenew class. In order to illustrate the ability of our model to resistforgetting, we designed a metric CIDER_t to detect the stage forgetting rate.Our experiments on the public dataset MSR-VTT show that the proposed methodsignificantly resists the forgetting of previous tasks without replaying oldsamples, and performs well on the new task.</description><author>Huiyu Xiong, Lanxiao Wang, Heqian Qiu, Taijin Zhao, Benliu Qiu, Hongliang Li</author><pubDate>Tue, 27 Feb 2024 16:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17680v1</guid></item><item><title>DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement</title><link>http://arxiv.org/abs/2404.02755v1</link><description>We present Dive Into the BoundarieS (DIBS), a novel pretraining framework fordense video captioning (DVC), that elaborates on improving the quality of thegenerated event captions and their associated pseudo event boundaries fromunlabeled videos. By leveraging the capabilities of diverse large languagemodels (LLMs), we generate rich DVC-oriented caption candidates and optimizethe corresponding pseudo boundaries under several meticulously designedobjectives, considering diversity, event-centricity, temporal ordering, andcoherence. Moreover, we further introduce a novel online boundary refinementstrategy that iteratively improves the quality of pseudo boundaries duringtraining. Comprehensive experiments have been conducted to examine theeffectiveness of the proposed technique components. By leveraging a substantialamount of unlabeled video data, such as HowTo100M, we achieve a remarkableadvancement on standard DVC datasets like YouCook2 and ActivityNet. Weoutperform the previous state-of-the-art Vid2Seq across a majority of metrics,achieving this with just 0.4% of the unlabeled video data used for pre-trainingby Vid2Seq.</description><author>Hao Wu, Huabin Liu, Yu Qiao, Xiao Sun</author><pubDate>Wed, 03 Apr 2024 14:57:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02755v1</guid></item><item><title>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</title><link>http://arxiv.org/abs/2402.19479v1</link><description>The quality of the data and annotation upper-bounds the quality of adownstream model. While there exist large text corpora and image-text pairs,high-quality video-text data is much harder to collect. First of all, manuallabeling is more time-consuming, as it requires an annotator to watch an entirevideo. Second, videos have a temporal dimension, consisting of several scenesstacked together, and showing multiple actions. Accordingly, to establish avideo dataset with high-quality captions, we propose an automatic approachleveraging multimodal inputs, such as textual video description, subtitles, andindividual video frames. Specifically, we curate 3.8M high-resolution videosfrom the publicly available HD-VILA-100M dataset. We then split them intosemantically consistent video clips, and apply multiple cross-modality teachermodels to obtain captions for each video. Next, we finetune a retrieval modelon a small subset where the best caption of each video is manually selected andthen employ the model in the whole dataset to select the best caption as theannotation. In this way, we get 70M videos paired with high-quality textcaptions. We dub the dataset as Panda-70M. We show the value of the proposeddataset on three downstream tasks: video captioning, video and text retrieval,and text-driven video generation. The models trained on the proposed data scoresubstantially better on the majority of metrics across all the tasks.</description><author>Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, Sergey Tulyakov</author><pubDate>Thu, 29 Feb 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19479v1</guid></item><item><title>Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision Models For Video Captioning and Summarization</title><link>http://arxiv.org/abs/2405.20648v1</link><description>Video is an increasingly prominent and information-dense medium, yet it posessubstantial challenges for language models. A typical video consists of asequence of shorter segments, or shots, that collectively form a coherentnarrative. Each shot is analogous to a word in a sentence where multiple datastreams of information (such as visual and auditory data) must be processedsimultaneously. Comprehension of the entire video requires not onlyunderstanding the visual-audio information of each shot but also requires thatthe model links the ideas between each shot to generate a larger,all-encompassing story. Despite significant progress in the field, currentworks often overlook videos' more granular shot-by-shot semantic information.In this project, we propose a family of efficient large language vision models(LLVMs) to boost video summarization and captioning called Shotluck Holmes. Byleveraging better pretraining and data collection strategies, we extend theabilities of existing small LLVMs from being able to understand a picture tobeing able to understand a sequence of frames. Specifically, we show thatShotluck Holmes achieves better performance than state-of-the-art results onthe Shot2Story video captioning and summary task with significantly smaller andmore computationally efficient models.</description><author>Richard Luo, Austin Peng, Adithya Vasudev, Rishabh Jain</author><pubDate>Fri, 31 May 2024 08:30:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20648v1</guid></item><item><title>TrafficVLM: A Controllable Visual Language Model for Traffic Video Captioning</title><link>http://arxiv.org/abs/2404.09275v1</link><description>Traffic video description and analysis have received much attention recentlydue to the growing demand for efficient and reliable urban surveillancesystems. Most existing methods only focus on locating traffic event segments,which severely lack descriptive details related to the behaviour and context ofall the subjects of interest in the events. In this paper, we presentTrafficVLM, a novel multi-modal dense video captioning model for vehicle egocamera view. TrafficVLM models traffic video events at different levels ofanalysis, both spatially and temporally, and generates long fine-graineddescriptions for the vehicle and pedestrian at different phases of the event.We also propose a conditional component for TrafficVLM to control thegeneration outputs and a multi-task fine-tuning paradigm to enhanceTrafficVLM's learning capability. Experiments show that TrafficVLM performswell on both vehicle and overhead camera views. Our solution achievedoutstanding results in Track 2 of the AI City Challenge 2024, ranking us thirdin the challenge standings. Our code is publicly available athttps://github.com/quangminhdinh/TrafficVLM.</description><author>Quang Minh Dinh, Minh Khoi Ho, Anh Quan Dang, Hung Phong Tran</author><pubDate>Sun, 14 Apr 2024 15:51:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09275v1</guid></item><item><title>CoVR: Learning Composed Video Retrieval from Web Video Captions</title><link>http://arxiv.org/abs/2308.14746v2</link><description>Composed Image Retrieval (CoIR) has recently gained popularity as a task thatconsiders both text and image queries together, to search for relevant imagesin a database. Most CoIR approaches require manually annotated datasets,comprising image-text-image triplets, where the text describes a modificationfrom the query image to the target image. However, manual curation of CoIRtriplets is expensive and prevents scalability. In this work, we insteadpropose a scalable automatic dataset creation methodology that generatestriplets given video-caption pairs, while also expanding the scope of the taskto include composed video retrieval (CoVR). To this end, we mine paired videoswith a similar caption from a large database, and leverage a large languagemodel to generate the corresponding modification text. Applying thismethodology to the extensive WebVid2M collection, we automatically constructour WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, weintroduce a new benchmark for CoVR with a manually annotated evaluation set,along with baseline results. Our experiments further demonstrate that traininga CoVR model on our dataset effectively transfers to CoIR, leading to improvedstate-of-the-art performance in the zero-shot setup on both the CIRR andFashionIQ benchmarks. Our code, datasets, and models are publicly available athttps://imagine.enpc.fr/~ventural/covr.</description><author>Lucas Ventura, Antoine Yang, Cordelia Schmid, Gül Varol</author><pubDate>Tue, 21 May 2024 15:44:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14746v2</guid></item><item><title>Retrieval Enhanced Zero-Shot Video Captioning</title><link>http://arxiv.org/abs/2405.07046v1</link><description>Despite the significant progress of fully-supervised video captioning,zero-shot methods remain much less explored. In this paper, we propose to takeadvantage of existing pre-trained large-scale vision and language models todirectly generate captions with test time adaptation. Specifically, we bridgevideo and text using three key models: a general video understanding modelXCLIP, a general image understanding model CLIP, and a text generation modelGPT-2, due to their source-code availability. The main challenge is how toenable the text generation model to be sufficiently aware of the content in agiven video so as to generate corresponding captions. To address this problem,we propose using learnable tokens as a communication medium between frozenGPT-2 and frozen XCLIP as well as frozen CLIP. Differing from the conventionalway to train these tokens with training data, we update these tokens withpseudo-targets of the inference data under several carefully crafted lossfunctions which enable the tokens to absorb video information catered forGPT-2. This procedure can be done in just a few iterations (we use 16iterations in the experiments) and does not require ground truth data.Extensive experimental results on three widely used datasets, MSR-VTT, MSVD,and VATEX, show 4% to 20% improvements in terms of the main metric CIDErcompared to the existing state-of-the-art methods.</description><author>Yunchuan Ma, Laiyun Qing, Guorong Li, Yuankai Qi, Quan Z. Sheng, Qingming Huang</author><pubDate>Sat, 11 May 2024 17:22:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07046v1</guid></item><item><title>Edit As You Wish: Video Caption Editing with Multi-grained User Control</title><link>http://arxiv.org/abs/2305.08389v2</link><description>Automatically narrating videos in natural language complying with userrequests, i.e. Controllable Video Captioning task, can help people managemassive videos with desired intentions. However, existing works suffer from twoshortcomings: 1) the control signal is single-grained which can not satisfydiverse user intentions; 2) the video description is generated in a singleround which can not be further edited to meet dynamic needs. In this paper, wepropose a novel \textbf{V}ideo \textbf{C}aption \textbf{E}diting \textbf{(VCE)}task to automatically revise an existing video description guided bymulti-grained user requests. Inspired by human writing-revision habits, wedesign the user command as a pivotal triplet \{\textit{operation, position,attribute}\} to cover diverse user needs from coarse-grained to fine-grained.To facilitate the VCE task, we \textit{automatically} construct an open-domainbenchmark dataset named VATEX-EDIT and \textit{manually} collect an e-commercedataset called EMMAD-EDIT. We further propose a specialized small-scale model(i.e., OPA) compared with two generalist Large Multi-modal Models to perform anexhaustive analysis of the novel task. For evaluation, we adopt comprehensivemetrics considering caption fluency, command-caption consistency, andvideo-caption alignment. Experiments reveal the task challenges of fine-grainedmulti-modal semantics understanding and processing. Our datasets, codes, andevaluation tools are ready to be open-sourced.</description><author>Linli Yao, Yuanmeng Zhang, Ziheng Wang, Xinglin Hou, Tiezheng Ge, Yuning Jiang, Xu Sun, Qin Jin</author><pubDate>Mon, 03 Jun 2024 08:47:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08389v2</guid></item><item><title>The MSR-Video to Text Dataset with Clean Annotations</title><link>http://arxiv.org/abs/2102.06448v4</link><description>Video captioning automatically generates short descriptions of the videocontent, usually in form of a single sentence. Many methods have been proposedfor solving this task. A large dataset called MSR Video to Text (MSR-VTT) isoften used as the benchmark dataset for testing the performance of the methods.However, we found that the human annotations, i.e., the descriptions of videocontents in the dataset are quite noisy, e.g., there are many duplicatecaptions and many captions contain grammatical problems. These problems maypose difficulties to video captioning models for learning underlying patterns.We cleaned the MSR-VTT annotations by removing these problems, then testedseveral typical video captioning models on the cleaned dataset. Experimentalresults showed that data cleaning boosted the performances of the modelsmeasured by popular quantitative metrics. We recruited subjects to evaluate theresults of a model trained on the original and cleaned datasets. The humanbehavior experiment demonstrated that trained on the cleaned dataset, the modelgenerated captions that were more coherent and more relevant to the contents ofthe video clips.</description><author>Haoran Chen, Jianmin Li, Simone Frintrop, Xiaolin Hu</author><pubDate>Sun, 25 Feb 2024 09:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.06448v4</guid></item><item><title>Vript: A Video Is Worth Thousands of Words</title><link>http://arxiv.org/abs/2406.06040v1</link><description>Advancements in multimodal learning, particularly in video understanding andgeneration, require high-quality video-text datasets for improved modelperformance. Vript addresses this issue with a meticulously annotated corpus of12K high-resolution videos, offering detailed, dense, and script-like captionsfor over 420K clips. Each clip has a caption of ~145 words, which is over 10xlonger than most video-text datasets. Unlike captions only documenting staticcontent in previous datasets, we enhance video captioning to video scripting bydocumenting not just the content, but also the camera operations, which includethe shot types (medium shot, close-up, etc) and camera movements (panning,tilting, etc). By utilizing the Vript, we explore three training paradigms ofaligning more text with the video modality rather than clip-caption pairs. Thisresults in Vriptor, a top-performing video captioning model among open-sourcemodels, comparable to GPT-4V in performance. Vriptor is also a powerful modelcapable of end-to-end generation of dense and detailed captions for longvideos. Moreover, we introduce Vript-Hard, a benchmark consisting of threevideo understanding tasks that are more challenging than existing benchmarks:Vript-HAL is the first benchmark evaluating action and object hallucinations invideo LLMs, Vript-RR combines reasoning with retrieval resolving questionambiguity in long-video QAs, and Vript-ERO is a new task to evaluate thetemporal understanding of events in long videos rather than actions in shortvideos in previous works. All code, models, and datasets are available inhttps://github.com/mutonix/Vript.</description><author>Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, Hai Zhao</author><pubDate>Mon, 10 Jun 2024 07:17:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06040v1</guid></item><item><title>Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval</title><link>http://arxiv.org/abs/2404.07610v1</link><description>There has been significant attention to the research on dense videocaptioning, which aims to automatically localize and caption all events withinuntrimmed video. Several studies introduce methods by designing dense videocaptioning as a multitasking problem of event localization and event captioningto consider inter-task relations. However, addressing both tasks using onlyvisual input is challenging due to the lack of semantic content. In this study,we address this by proposing a novel framework inspired by the cognitiveinformation processing of humans. Our model utilizes external memory toincorporate prior knowledge. The memory retrieval method is proposed withcross-modal video-to-text matching. To effectively incorporate retrieved textfeatures, the versatile encoder and the decoder with visual and textualcross-attention modules are designed. Comparative experiments have beenconducted to show the effectiveness of the proposed method on ActivityNetCaptions and YouCook2 datasets. Experimental results show promising performanceof our model without extensive pretraining from a large video dataset.</description><author>Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, Seong Tae Kim</author><pubDate>Thu, 11 Apr 2024 10:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07610v1</guid></item><item><title>OW-VISCap: Open-World Video Instance Segmentation and Captioning</title><link>http://arxiv.org/abs/2404.03657v1</link><description>Open-world video instance segmentation is an important video understandingtask. Yet most methods either operate in a closed-world setting, require anadditional user-input, or use classic region-based proposals to identify neverbefore seen objects. Further, these methods only assign a one-word label todetected objects, and don't generate rich object-centric descriptions. Theyalso often suffer from highly overlapping predictions. To address these issues,we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap),an approach to jointly segment, track, and caption previously seen or unseenobjects in a video. For this, we introduce open-world object queries todiscover never before seen objects without additional user-input. We generaterich and descriptive object-centric captions for each detected object via amasked attention augmented LLM input. We introduce an inter-query contrastiveloss to ensure that the object queries differ from one another. Our generalizedapproach matches or surpasses state-of-the-art on three tasks: open-world videoinstance segmentation on the BURST dataset, dense video object captioning onthe VidSTG dataset, and closed-world video instance segmentation on the OVISdataset.</description><author>Anwesa Choudhuri, Girish Chowdhary, Alexander G. Schwing</author><pubDate>Thu, 04 Apr 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03657v1</guid></item><item><title>Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality</title><link>http://arxiv.org/abs/2403.19221v1</link><description>Video paragraph captioning (VPC) involves generating detailed narratives forlong videos, utilizing supportive modalities such as speech and eventboundaries. However, the existing models are constrained by the assumption ofconstant availability of a single auxiliary modality, which is impracticalgiven the diversity and unpredictable nature of real-world scenarios. To thisend, we propose a Missing-Resistant framework MR-VPC that effectively harnessesall available auxiliary inputs and maintains resilience even in the absence ofcertain modalities. Under this framework, we propose the Multimodal VPC (MVPC)architecture integrating video, speech, and event boundary inputs in a unifiedmanner to process various auxiliary inputs. Moreover, to fortify the modelagainst incomplete data, we introduce DropAM, a data augmentation strategy thatrandomly omits auxiliary inputs, paired with DistillAM, a regularization targetthat distills knowledge from teacher models trained on modality-complete data,enabling efficient learning in modality-deficient environments. Throughexhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC hasproven to deliver superior performance on modality-complete andmodality-missing test data. This work highlights the significance of developingresilient VPC models and paves the way for more adaptive, robust multimodalvideo understanding.</description><author>Sishuo Chen, Lei Li, Shuhuai Ren, Rundong Gao, Yuanxin Liu, Xiaohan Bi, Xu Sun, Lu Hou</author><pubDate>Thu, 28 Mar 2024 09:35:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19221v1</guid></item><item><title>ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation</title><link>http://arxiv.org/abs/2303.06458v3</link><description>Natural Language Generation (NLG) accepts input data in the form of images,videos, or text and generates corresponding natural language text as output.Existing NLG methods mainly adopt a supervised approach and rely heavily oncoupled data-to-text pairs. However, for many targeted scenarios and fornon-English languages, sufficient quantities of labeled data are often notavailable. To relax the dependency on labeled data of downstream tasks, wepropose an intuitive and effective zero-shot learning framework, ZeroNLG, whichcan deal with multiple NLG tasks, including image-to-text (image captioning),video-to-text (video captioning), and text-to-text (neural machinetranslation), across English, Chinese, German, and French within a unifiedframework. ZeroNLG does not require any labeled downstream pairs for training.During training, ZeroNLG (i) projects different domains (across modalities andlanguages) to corresponding coordinates in a shared common latent space; (ii)bridges different domains by aligning their corresponding coordinates in thisspace; and (iii) builds an unsupervised multilingual auto-encoder to learn togenerate text by reconstructing the input text given its coordinate in sharedlatent space. Consequently, during inference, based on the data-to-textpipeline, ZeroNLG can generate target sentences across different languagesgiven the coordinate of input data in the common space. Within this unifiedframework, given visual (imaging or video) data as input, ZeroNLG can performzero-shot visual captioning; given textual sentences as input, ZeroNLG canperform zero-shot machine translation. We present the results of extensiveexperiments on twelve NLG tasks, showing that, without using any labeleddownstream pairs for training, ZeroNLG generates high-quality and believableoutputs and significantly outperforms existing zero-shot methods.</description><author>Bang Yang, Fenglin Liu, Yuexian Zou, Xian Wu, Yaowei Wang, David A. Clifton</author><pubDate>Mon, 03 Jun 2024 13:47:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06458v3</guid></item><item><title>Dense Video Object Captioning from Disjoint Supervision</title><link>http://arxiv.org/abs/2306.11729v2</link><description>We propose a new task and model for dense video object captioning --detecting, tracking and captioning trajectories of objects in a video. Thistask unifies spatial and temporal localization in video, whilst also requiringfine-grained visual understanding that is best described by natural language.We propose a unified model, and demonstrate how our end-to-end approach is moreaccurate and temporally coherent than a multi-stage pipeline combiningstate-of-the-art detection, tracking, and captioning models. Moreover, wepropose a training strategy based on a mixture of disjoint tasks, which allowsus to leverage diverse, large-scale datasets which supervise different parts ofour model. Although each pretraining task only provides weak supervision, theyare complementary and, when combined, result in noteworthy zero-shot abilityand serve as strong initialization for additional finetuning to further improveaccuracy. We carefully design new metrics capturing all components of our task,and show how we can repurpose existing video grounding datasets (e.g. VidSTGand VLN) for our new task. We show that our model improves upon a number ofstrong baselines for this new task. Furthermore, we can apply our model to thetask of spatial grounding, outperforming prior state-of-the-art on VidSTG andVLN, without explicitly training for it. Code is available athttps://github.com/google-research/scenic/tree/main/scenic/projects/densevoc.</description><author>Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid</author><pubDate>Tue, 09 Apr 2024 06:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11729v2</guid></item><item><title>VTG-LLM: Integrating Timestamp Knowledge into Video LLMs for Enhanced Video Temporal Grounding</title><link>http://arxiv.org/abs/2405.13382v1</link><description>Video Temporal Grounding (VTG) focuses on accurately identifying eventtimestamps within a particular video based on a linguistic query, playing avital role in downstream tasks such as video browsing and editing. While VideoLarge Language Models (video LLMs) have made significant progress inunderstanding video content, they often face challenges in accuratelypinpointing timestamps within videos, which limits their performance on VTGtasks. Therefore, to improve video LLMs' ability to effectively locatetimestamps, we argue that two critical aspects need to be enhanced. First, itis essential to have high-quality instructional tuning datasets that encompassmainstream VTG tasks. Second, directly incorporating timestamp knowledge intovideo LLMs is crucial, as it enables models to efficiently comprehend timestampinformation. To address these needs, we first introduce VTG-IT-120K, ahigh-quality and comprehensive instruction tuning dataset that covers VTG taskssuch as moment retrieval, dense video captioning, video summarization, andvideo highlight detection. Furthermore, we propose a specially designed videoLLM model for VTG tasks, VTG-LLM, which (1) effectively integrates timestampknowledge into visual tokens; (2) incorporates absolute-time tokens thatspecifically handle timestamp knowledge, thereby avoiding concept shifts; and(3) introduces a lightweight, high-performance slot-based token compressionmethod to facilitate the sampling of more video frames. Comprehensiveexperiments showcase the superior performance of VTG-LLM in comparison to othervideo LLM methods across various VTG tasks. Our code and datasets are availableat \url{https://github.com/gyxxyg/VTG-LLM}.</description><author>Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Xi Chen, Bo Zhao</author><pubDate>Wed, 22 May 2024 07:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13382v1</guid></item><item><title>VTG-LLM: Integrating Timestamp Knowledge into Video LLMs for Enhanced Video Temporal Grounding</title><link>http://arxiv.org/abs/2405.13382v2</link><description>Video Temporal Grounding (VTG) focuses on accurately identifying eventtimestamps within a particular video based on a linguistic query, playing avital role in downstream tasks such as video browsing and editing. While VideoLarge Language Models (video LLMs) have made significant progress inunderstanding video content, they often face challenges in accuratelypinpointing timestamps within videos, which limits their performance on VTGtasks. Therefore, to improve video LLMs' ability to effectively locatetimestamps, we argue that two critical aspects need to be enhanced. First, itis essential to have high-quality instructional tuning datasets that encompassmainstream VTG tasks. Second, directly incorporating timestamp knowledge intovideo LLMs is crucial, as it enables models to efficiently comprehend timestampinformation. To address these needs, we first introduce VTG-IT-120K, ahigh-quality and comprehensive instruction tuning dataset that covers VTG taskssuch as moment retrieval, dense video captioning, video summarization, andvideo highlight detection. Furthermore, we propose a specially designed videoLLM model for VTG tasks, VTG-LLM, which (1) effectively integrates timestampknowledge into visual tokens; (2) incorporates absolute-time tokens thatspecifically handle timestamp knowledge, thereby avoiding concept shifts; and(3) introduces a lightweight, high-performance slot-based token compressionmethod to facilitate the sampling of more video frames. Comprehensiveexperiments showcase the superior performance of VTG-LLM in comparison to othervideo LLM methods across various VTG tasks. Our code and datasets are availableat \url{https://github.com/gyxxyg/VTG-LLM}.</description><author>Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Xi Chen, Bo Zhao</author><pubDate>Mon, 01 Jul 2024 07:14:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13382v2</guid></item><item><title>Beyond MOT: Semantic Multi-Object Tracking</title><link>http://arxiv.org/abs/2403.05021v1</link><description>Current multi-object tracking (MOT) aims to predict trajectories of targets(i.e.,"where") in videos. Yet, knowing merely "where" is insufficient in manycrucial applications. In comparison, semantic understanding such asfine-grained behaviors, interactions, and overall summarized captions (i.e.,"what") from videos, associated with "where", is highly-desired forcomprehensive video analysis. Thus motivated, we introduce SemanticMulti-Object Tracking (SMOT), that aims to estimate object trajectories andmeanwhile understand semantic details of associated trajectories includinginstance captions, instance interactions, and overall video captions,integrating "where" and "what" for tracking. In order to foster the explorationof SMOT, we propose BenSMOT, a large-scale Benchmark for Semantic MOT.Specifically, BenSMOT comprises 3,292 videos with 151K frames, covering variousscenarios for semantic tracking of humans. BenSMOT provides annotations for thetrajectories of targets, along with associated instance captions in naturallanguage, instance interactions, and overall caption for each video sequence.To our best knowledge, BenSMOT is the first publicly available benchmark forSMOT. Besides, to encourage future research, we present a novel tracker namedSMOTer, which is specially designed and end-to-end trained for SMOT, showingpromising performance. By releasing BenSMOT, we expect to go beyondconventional MOT by predicting "where" and "what" for SMOT, opening up a newdirection in tracking for video understanding. Our BenSMOT and SMOTer will bereleased.</description><author>Yunhao Li, Hao Wang, Qin Li, Xue Ma, Jiali Yao, Shaohua Dong, Heng Fan, Libo Zhang</author><pubDate>Fri, 08 Mar 2024 03:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05021v1</guid></item><item><title>OmniVid: A Generative Framework for Universal Video Understanding</title><link>http://arxiv.org/abs/2403.17935v1</link><description>The core of video understanding tasks, such as recognition, captioning, andtracking, is to automatically detect objects or actions in a video and analyzetheir temporal evolution. Despite sharing a common goal, different tasks oftenrely on distinct model architectures and annotation formats. In contrast,natural language processing benefits from a unified output space, i.e., textsequences, which simplifies the training of powerful foundational languagemodels, such as GPT-3, with extensive training corpora. Inspired by this, weseek to unify the output space of video understanding tasks by using languagesas labels and additionally introducing time and box tokens. In this way, avariety of video tasks could be formulated as video-grounded token generation.This enables us to address various types of video tasks, includingclassification (such as action recognition), captioning (covering clipcaptioning, video question answering, and dense video captioning), andlocalization tasks (such as visual object tracking) within a fully sharedencoder-decoder architecture, following a generative framework. Throughcomprehensive experiments, we demonstrate such a simple and straightforwardidea is quite effective and can achieve state-of-the-art or competitive resultson seven video benchmarks, providing a novel perspective for more universalvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.</description><author>Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Tue, 26 Mar 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17935v1</guid></item><item><title>Distilling Vision-Language Models on Millions of Videos</title><link>http://arxiv.org/abs/2401.06129v2</link><description>The recent advance in vision-language models is largely attributed to theabundance of image-text data. We aim to replicate this success forvideo-language models, but there simply is not enough human-curated video-textdata available. We thus resort to fine-tuning a video-language model from astrong image-language baseline with synthesized instructional data. Theresulting video model by video-instruction-tuning (VIIT) is then used toauto-label millions of videos to generate high-quality captions. We show theadapted video-language model performs well on a wide range of video-languagebenchmarks. For instance, it surpasses the best prior result on open-endedNExT-QA by 2.8%. Besides, our model generates detailed descriptions forpreviously unseen videos, which provide better textual supervision thanexisting methods. Experiments show that a video-language dual-encoder modelcontrastively trained on these auto-generated captions is 3.8% better than thestrongest baseline that also leverages vision-language models. Our best modeloutperforms state-of-the-art methods on MSR-VTT zero-shot text-to-videoretrieval by 6%. As a side product, we generate the largest video captiondataset to date.</description><author>Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing Gong, Philipp Krähenbühl, Liangzhe Yuan</author><pubDate>Mon, 15 Apr 2024 22:10:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06129v2</guid></item><item><title>Seeing the Unseen: Visual Metaphor Captioning for Videos</title><link>http://arxiv.org/abs/2406.04886v1</link><description>Metaphors are a common communication tool used in our day-to-day life. Thedetection and generation of metaphors in textual form have been studiedextensively but metaphors in other forms have been under-explored. Recentstudies have shown that Vision-Language (VL) models cannot understand visualmetaphors in memes and adverts. As of now, no probing studies have been donethat involve complex language phenomena like metaphors with videos. Hence, weintroduce a new VL task of describing the metaphors present in the videos inour work. To facilitate this novel task, we construct and release a manuallycreated dataset with 705 videos and 2115 human-written captions, along with anew metric called Average Concept Distance (ACD), to automatically evaluate thecreativity of the metaphors generated. We also propose a novel low-resourcevideo metaphor captioning system: GIT-LLaVA, which obtains comparableperformance to SoTA video language models on the proposed task. We perform acomprehensive analysis of existing video language models on this task andpublish our dataset, models, and benchmark results to enable further research.</description><author>Abisek Rajakumar Kalarani, Pushpak Bhattacharyya, Sumit Shekhar</author><pubDate>Fri, 07 Jun 2024 13:32:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04886v1</guid></item><item><title>PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning</title><link>http://arxiv.org/abs/2404.16994v2</link><description>Vision-language pre-training has significantly elevated performance across awide range of image-language applications. Yet, the pre-training process forvideo-related tasks demands exceptionally large computational and dataresources, which hinders the progress of video-language models. This paperinvestigates a straight-forward, highly efficient, and resource-light approachto adapting an existing image-language pre-trained model for dense videounderstanding. Our preliminary experiments reveal that directly fine-tuningpre-trained image-language models with multiple frames as inputs on videodatasets leads to performance saturation or even a drop. Our furtherinvestigation reveals that it is largely attributed to the bias of learnedhigh-norm visual features. Motivated by this finding, we propose a simple buteffective pooling strategy to smooth the feature distribution along thetemporal dimension and thus reduce the dominant impacts from the extremefeatures. The new model is termed Pooling LLaVA, or PLLaVA in short. PLLaVAachieves new state-of-the-art performance on modern benchmark datasets for bothvideo question-answer and captioning tasks. Notably, on the recent popularVideoChatGPT benchmark, PLLaVA achieves a score of 3.48 out of 5 on average offive evaluated dimensions, exceeding the previous SOTA results from GPT4V(IG-VLM) by 9%. On the latest multi-choice benchmark MVBench, PLLaVA achieves58.1% accuracy on average across 20 sub-tasks, 14.5% higher than GPT4V(IG-VLM). Code is available at https://pllava.github.io/</description><author>Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, Jiashi Feng</author><pubDate>Mon, 29 Apr 2024 15:52:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16994v2</guid></item><item><title>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</title><link>http://arxiv.org/abs/2406.07476v2</link><description>In this paper, we present the VideoLLaMA 2, a set of Video Large LanguageModels (Video-LLMs) designed to enhance spatial-temporal modeling and audiounderstanding in video and audio-oriented tasks. Building upon its predecessor,VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC)connector, which effectively captures the intricate spatial and temporaldynamics of video data. Additionally, we integrate an Audio Branch into themodel through joint training, thereby enriching the multimodal understandingcapabilities of the model by seamlessly incorporating audio cues. Comprehensiveevaluations on multiple-choice video question answering (MC-VQA), open-endedvideo question answering (OE-VQA), and video captioning (VC) tasks demonstratethat VideoLLaMA 2 consistently achieves competitive results among open-sourcemodels and even gets close to some proprietary models on several benchmarks.Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only andaudio-video question-answering (AQA &amp; OE-AVQA) benchmarks over existing models.These advancements underline VideoLLaMA 2's superior performance in multimodalcomprehension, setting a new standard for intelligent video analysis systems.All models are public to facilitate further research.</description><author>Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, Lidong Bing</author><pubDate>Mon, 17 Jun 2024 17:40:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07476v2</guid></item><item><title>VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs</title><link>http://arxiv.org/abs/2406.07476v1</link><description>In this paper, we present the VideoLLaMA 2, a set of Video Large LanguageModels (Video-LLMs) designed to enhance spatial-temporal modeling and audiounderstanding in video and audio-oriented tasks. Building upon its predecessor,VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC)connector, which effectively captures the intricate spatial and temporaldynamics of video data. Additionally, we integrate an Audio Branch into themodel through joint training, thereby enriching the multimodal understandingcapabilities of the model by seamlessly incorporating audio cues. Comprehensiveevaluations on multiple-choice video question answering (MC-VQA), open-endedvideo question answering (OE-VQA), and video captioning (VC) tasks demonstratethat VideoLLaMA 2 consistently achieves competitive results among open-sourcemodels and even gets close to some proprietary models on several benchmarks.Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only andaudio-video question-answering (AQA &amp; OE-AVQA) benchmarks over existing models.These advancements underline VideoLLaMA 2's superior performance in multimodalcomprehension, setting a new standard for intelligent video analysis systems.All models are public to facilitate further research.</description><author>Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, Lidong Bing</author><pubDate>Tue, 11 Jun 2024 18:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.07476v1</guid></item><item><title>Learning text-to-video retrieval from image captioning</title><link>http://arxiv.org/abs/2404.17498v1</link><description>We describe a protocol to study text-to-video retrieval training withunlabeled videos, where we assume (i) no access to labels for any videos, i.e.,no access to the set of ground-truth captions, but (ii) access to labeledimages in the form of text. Using image expert models is a realistic scenariogiven that annotating images is cheaper therefore scalable, in contrast toexpensive video labeling schemes. Recently, zero-shot image experts such asCLIP have established a new strong baseline for video understanding tasks. Inthis paper, we make use of this progress and instantiate the image experts fromtwo types of models: a text-to-image retrieval model to provide an initialbackbone, and image captioning models to provide supervision signal intounlabeled videos. We show that automatically labeling video frames with imagecaptioning allows text-to-video retrieval training. This process adapts thefeatures to the target domain at no manual annotation cost, consequentlyoutperforming the strong zero-shot CLIP baseline. During training, we samplecaptions from multiple video frames that best match the visual content, andperform a temporal pooling over frame representations by scoring framesaccording to their relevance to each caption. We conduct extensive ablations toprovide insights and demonstrate the effectiveness of this simple framework byoutperforming the CLIP zero-shot baselines on text-to-video retrieval on threestandard datasets, namely ActivityNet, MSR-VTT, and MSVD.</description><author>Lucas Ventura, Cordelia Schmid, Gül Varol</author><pubDate>Fri, 26 Apr 2024 16:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.17498v1</guid></item><item><title>AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding</title><link>http://arxiv.org/abs/2406.13807v2</link><description>AI personal assistants deployed via robots or wearables require embodiedunderstanding to collaborate with humans effectively. However, currentVision-Language Models (VLMs) primarily focus on third-person view videos,neglecting the richness of egocentric perceptual experience. To address thisgap, we propose three key contributions. First, we introduce the EgocentricVideo Understanding Dataset (EVUD) for training VLMs on video captioning andquestion answering tasks specific to egocentric videos. Second, we presentAlanaVLM, a 7B parameter VLM trained using parameter-efficient methods on EVUD.Finally, we evaluate AlanaVLM's capabilities on OpenEQA, a challengingbenchmark for embodied video question answering. Our model achievesstate-of-the-art performance, outperforming open-source models including strongSocratic models using GPT-4 as a planner by 3.6%. Additionally, we outperformClaude 3 and Gemini Pro Vision 1.0 and showcase competitive results compared toGemini Pro 1.5 and GPT-4V, even surpassing the latter in spatial reasoning.This research paves the way for building efficient VLMs that can be deployed inrobots or wearables, leveraging embodied video understanding to collaborateseamlessly with humans in everyday tasks, contributing to the next generationof Embodied AI.</description><author>Alessandro Suglia, Claudio Greco, Katie Baker, Jose L. Part, Ioannis Papaioannou, Arash Eshghi, Ioannis Konstas, Oliver Lemon</author><pubDate>Fri, 21 Jun 2024 10:53:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13807v2</guid></item><item><title>Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation</title><link>http://arxiv.org/abs/2403.05131v1</link><description>Text-to-video generation marks a significant frontier in the rapidly evolvingdomain of generative AI, integrating advancements in text-to-image synthesis,video captioning, and text-guided editing. This survey critically examines theprogression of text-to-video technologies, focusing on the shift fromtraditional generative models to the cutting-edge Sora model, highlightingdevelopments in scalability and generalizability. Distinguishing our analysisfrom prior works, we offer an in-depth exploration of the technologicalframeworks and evolutionary pathways of these models. Additionally, we delveinto practical applications and address ethical and technological challengessuch as the inability to perform multiple entity handling, comprehendcausal-effect learning, understand physical interaction, perceive objectscaling and proportioning, and combat object hallucination which is also along-standing problem in generative models. Our comprehensive discussion coversthe topic of enablement of text-to-video generation models as human-assistivetools and world models, as well as eliciting model's shortcomings andsummarizing future improvement direction that mainly centers around trainingdatasets and evaluation metrics (both automatic and human-centered). Aimed atboth newcomers and seasoned researchers, this survey seeks to catalyze furtherinnovation and discussion in the growing field of text-to-video generation,paving the way for more reliable and practical generative artificialintelligence technologies.</description><author>Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao Zheng, Lik-Hang Lee, Tae-Ho Kim, Choong Seon Hong, Chaoning Zhang</author><pubDate>Fri, 08 Mar 2024 07:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05131v1</guid></item><item><title>VideoPrism: A Foundational Visual Encoder for Video Understanding</title><link>http://arxiv.org/abs/2402.13217v1</link><description>We introduce VideoPrism, a general-purpose video encoder that tackles diversevideo understanding tasks with a single frozen model. We pretrain VideoPrism ona heterogeneous corpus containing 36M high-quality video-caption pairs and 582Mvideo clips with noisy parallel text (e.g., ASR transcripts). The pretrainingapproach improves upon masked autoencoding by global-local distillation ofsemantic video embeddings and a token shuffling scheme, enabling VideoPrism tofocus primarily on the video modality while leveraging the invaluable textassociated with videos. We extensively test VideoPrism on four broad groups ofvideo understanding tasks, from web video question answering to CV for science,achieving state-of-the-art performance on 30 out of 33 video understandingbenchmarks.</description><author>Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, Boqing Gong</author><pubDate>Tue, 20 Feb 2024 18:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13217v1</guid></item><item><title>Pix2Gif: Motion-Guided Diffusion for GIF Generation</title><link>http://arxiv.org/abs/2403.04634v1</link><description>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)generation. We tackle this problem differently by formulating the task as animage translation problem steered by text and motion magnitude prompts, asshown in teaser fig. To ensure that the model adheres to motion guidance, wepropose a new motion-guided warping module to spatially transform the featuresof the source image conditioned on the two types of prompts. Furthermore, weintroduce a perceptual loss to ensure the transformed feature map remainswithin the same space as the target image, ensuring content consistency andcoherence. In preparation for the model training, we meticulously curated databy extracting coherent image frames from the TGIF video-caption dataset, whichprovides rich information about the temporal changes of subjects. Afterpretraining, we apply our model in a zero-shot manner to a number of videodatasets. Extensive qualitative and quantitative experiments demonstrate theeffectiveness of our model -- it not only captures the semantic prompt fromtext but also the spatial ones from motion guidance. We train all our modelsusing a single node of 16xV100 GPUs. Code, dataset and models are made publicat: https://hiteshk03.github.io/Pix2Gif/.</description><author>Hitesh Kandala, Jianfeng Gao, Jianwei Yang</author><pubDate>Thu, 07 Mar 2024 16:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04634v1</guid></item><item><title>Pix2Gif: Motion-Guided Diffusion for GIF Generation</title><link>http://arxiv.org/abs/2403.04634v2</link><description>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)generation. We tackle this problem differently by formulating the task as animage translation problem steered by text and motion magnitude prompts, asshown in teaser fig. To ensure that the model adheres to motion guidance, wepropose a new motion-guided warping module to spatially transform the featuresof the source image conditioned on the two types of prompts. Furthermore, weintroduce a perceptual loss to ensure the transformed feature map remainswithin the same space as the target image, ensuring content consistency andcoherence. In preparation for the model training, we meticulously curated databy extracting coherent image frames from the TGIF video-caption dataset, whichprovides rich information about the temporal changes of subjects. Afterpretraining, we apply our model in a zero-shot manner to a number of videodatasets. Extensive qualitative and quantitative experiments demonstrate theeffectiveness of our model -- it not only captures the semantic prompt fromtext but also the spatial ones from motion guidance. We train all our modelsusing a single node of 16xV100 GPUs. Code, dataset and models are made publicat: https://hiteshk03.github.io/Pix2Gif/.</description><author>Hitesh Kandala, Jianfeng Gao, Jianwei Yang</author><pubDate>Fri, 08 Mar 2024 18:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04634v2</guid></item><item><title>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</title><link>http://arxiv.org/abs/2402.10698v1</link><description>We present Q-ViD, a simple approach for video question answering (video QA),that unlike prior methods, which are based on complex architectures,computationally expensive pipelines or use closed models like GPTs, Q-ViDrelies on a single instruction-aware open vision-language model (InstructBLIP)to tackle videoQA using frame descriptions. Specifically, we create captioninginstruction prompts that rely on the target questions about the videos andleverage InstructBLIP to obtain video frame captions that are useful to thetask at hand. Subsequently, we form descriptions of the whole video using thequestion-dependent frame captions, and feed that information, along with aquestion-answering prompt, to a large language model (LLM). The LLM is ourreasoning module, and performs the final step of multiple-choice QA. Our simpleQ-ViD framework achieves competitive or even higher performances than currentstate of the art models on a diverse range of videoQA benchmarks, includingNExT-QA, STAR, How2QA, TVQA and IntentQA.</description><author>David Romero, Thamar Solorio</author><pubDate>Fri, 16 Feb 2024 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10698v1</guid></item><item><title>Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM</title><link>http://arxiv.org/abs/2406.12235v2</link><description>Towards open-ended Video Anomaly Detection (VAD), existing methods oftenexhibit biased detection when faced with challenging or unseen events and lackinterpretability. To address these drawbacks, we propose Holmes-VAD, a novelframework that leverages precise temporal supervision and rich multimodalinstructions to enable accurate anomaly localization and comprehensiveexplanations. Firstly, towards unbiased and explainable VAD system, weconstruct the first large-scale multimodal VAD instruction-tuning benchmark,i.e., VAD-Instruct50k. This dataset is created using a carefully designedsemi-automatic labeling paradigm. Efficient single-frame annotations areapplied to the collected untrimmed videos, which are then synthesized intohigh-quality analyses of both abnormal and normal video clips using a robustoff-the-shelf video captioner and a large language model (LLM). Building uponthe VAD-Instruct50k dataset, we develop a customized solution for interpretablevideo anomaly detection. We train a lightweight temporal sampler to selectframes with high anomaly response and fine-tune a multimodal large languagemodel (LLM) to generate explanatory content. Extensive experimental resultsvalidate the generality and interpretability of the proposed Holmes-VAD,establishing it as a novel interpretable technique for real-world video anomalyanalysis. To support the community, our benchmark and model will be publiclyavailable at https://holmesvad.github.io.</description><author>Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Chuchu Han, Xiaonan Huang, Changxin Gao, Yuehuan Wang, Nong Sang</author><pubDate>Sat, 29 Jun 2024 09:15:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12235v2</guid></item><item><title>Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks</title><link>http://arxiv.org/abs/2406.16346v1</link><description>Large language models (LLMs) and large visual language models (LVLMs) havebeen at the forefront of the artificial intelligence field, particularly fortasks like text generation, video captioning, and question-answering.Typically, it is more applicable to train these models on broader knowledgebases or datasets to increase generalizability, learn relationships betweentopics, and recognize patterns. Instead, we propose to provide instructionaldatasets specific to the task of each modality within a distinct domain andthen fine-tune the parameters of the model using LORA. With our approach, wecan eliminate all noise irrelevant to the given task while also ensuring thatthe model generates with enhanced precision. For this work, we use Video-LLaVAto generate recipes given cooking videos without transcripts. Video-LLaVA'smultimodal architecture allows us to provide cooking images to its imageencoder, cooking videos to its video encoder, and general cooking questions toits text encoder. Thus, we aim to remove all noise unrelated to cooking whileimproving our model's capabilities to generate specific ingredient lists anddetailed instructions. As a result, our approach to fine-tuning Video-LLaVAleads to gains over the baseline Video-LLaVA by 2% on the YouCook2 dataset.While this may seem like a marginal increase, our model trains on an imageinstruction dataset 2.5% the size of Video-LLaVA's and a video instructiondataset 23.76% of Video-LLaVA's.</description><author>Daniel Wen, Nafisa Hussain</author><pubDate>Mon, 24 Jun 2024 07:39:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.16346v1</guid></item><item><title>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</title><link>http://arxiv.org/abs/2404.05726v2</link><description>With the success of large language models (LLMs), integrating the visionmodel into LLMs to build vision-language foundation models has gained much moreinterest recently. However, existing LLM-based large multimodal models (e.g.,Video-LLaMA, VideoChat) can only take in a limited number of frames for shortvideo understanding. In this study, we mainly focus on designing an efficientand effective model for long-term video understanding. Instead of trying toprocess more frames simultaneously like most existing work, we propose toprocess videos in an online manner and store past video information in a memorybank. This allows our model to reference historical video content for long-termanalysis without exceeding LLMs' context length constraints or GPU memorylimits. Our memory bank can be seamlessly integrated into current multimodalLLMs in an off-the-shelf manner. We conduct extensive experiments on variousvideo understanding tasks, such as long-video understanding, video questionanswering, and video captioning, and our model can achieve state-of-the-artperformances across multiple datasets. Code available athttps://boheumd.github.io/MA-LMM/.</description><author>Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim</author><pubDate>Wed, 24 Apr 2024 16:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05726v2</guid></item><item><title>LVCHAT: Facilitating Long Video Comprehension</title><link>http://arxiv.org/abs/2402.12079v1</link><description>Enabling large language models (LLMs) to read videos is vital for multimodalLLMs. Existing works show promise on short videos whereas long video (longerthan e.g.~1 minute) comprehension remains challenging. The major problem liesin the over-compression of videos, i.e., the encoded video representations arenot enough to represent the whole video. To address this issue, we propose LongVideo Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced todynamically adjust the number of embeddings in alignment with the duration ofthe video to ensure long videos are not overly compressed into a fewembeddings. To deal with long videos whose length is beyond videos seen duringtraining, we propose Interleaved Frame Encoding (IFE), repeating positionalembedding and interleaving multiple groups of videos to enable long videoinput, avoiding performance degradation due to overly long videos. Experimentalresults show that LVChat significantly outperforms existing methods by up to27\% in accuracy on long-video QA datasets and long-video captioningbenchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.</description><author>Yu Wang, Zeyuan Zhang, Julian McAuley, Zexue He</author><pubDate>Mon, 19 Feb 2024 11:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12079v1</guid></item><item><title>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</title><link>http://arxiv.org/abs/2404.05726v1</link><description>With the success of large language models (LLMs), integrating the visionmodel into LLMs to build vision-language foundation models has gained much moreinterest recently. However, existing LLM-based large multimodal models (e.g.,Video-LLaMA, VideoChat) can only take in a limited number of frames for shortvideo understanding. In this study, we mainly focus on designing an efficientand effective model for long-term video understanding. Instead of trying toprocess more frames simultaneously like most existing work, we propose toprocess videos in an online manner and store past video information in a memorybank. This allows our model to reference historical video content for long-termanalysis without exceeding LLMs' context length constraints or GPU memorylimits. Our memory bank can be seamlessly integrated into current multimodalLLMs in an off-the-shelf manner. We conduct extensive experiments on variousvideo understanding tasks, such as long-video understanding, video questionanswering, and video captioning, and our model can achieve state-of-the-artperformances across multiple datasets. Code available athttps://boheumd.github.io/MA-LMM/.</description><author>Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim</author><pubDate>Mon, 08 Apr 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05726v1</guid></item><item><title>The 8th AI City Challenge</title><link>http://arxiv.org/abs/2404.09432v1</link><description>The eighth AI City Challenge highlighted the convergence of computer visionand artificial intelligence in areas like retail, warehouse settings, andIntelligent Traffic Systems (ITS), presenting significant researchopportunities. The 2024 edition featured five tracks, attracting unprecedentedinterest from 726 teams in 47 countries and regions. Track 1 dealt withmulti-target multi-camera (MTMC) people tracking, highlighting significantenhancements in camera count, character number, 3D annotation, and cameramatrices, alongside new rules for 3D tracking and online tracking algorithmencouragement. Track 2 introduced dense video captioning for traffic safety,focusing on pedestrian accidents using multi-camera feeds to improve insightsfor insurance and prevention. Track 3 required teams to classify driver actionsin a naturalistic driving analysis. Track 4 explored fish-eye camera analyticsusing the FishEye8K dataset. Track 5 focused on motorcycle helmet ruleviolation detection. The challenge utilized two leaderboards to showcasemethods, with participants setting new benchmarks, some surpassing existingstate-of-the-art achievements.</description><author>Shuo Wang, David C. Anastasiu, Zheng Tang, Ming-Ching Chang, Yue Yao, Liang Zheng, Mohammed Shaiqur Rahman, Meenakshi S. Arya, Anuj Sharma, Pranamesh Chakraborty, Sanjita Prajapati, Quan Kong, Norimasa Kobori, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Fady Alnajjar, Ganzorig Batnasan, Ping-Yang Chen, Jun-Wei Hsieh, Xunlei Wu, Sameer Satish Pusegaonkar, Yizhou Wang, Sujit Biswas, Rama Chellappa</author><pubDate>Mon, 15 Apr 2024 04:12:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09432v1</guid></item><item><title>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</title><link>http://arxiv.org/abs/2404.14471v1</link><description>In this paper, we investigate a new problem called narrative actionevaluation (NAE). NAE aims to generate professional commentary that evaluatesthe execution of an action. Unlike traditional tasks such as score-based actionquality assessment and video captioning involving superficial sentences, NAEfocuses on creating detailed narratives in natural language. These narrativesprovide intricate descriptions of actions along with objective evaluations. NAEis a more challenging task because it requires both narrative flexibility andevaluation rigor. One existing possible solution is to use multi-task learning,where narrative language and evaluative information are predicted separately.However, this approach results in reduced performance for individual tasksbecause of variations between tasks and differences in modality betweenlanguage information and evaluation information. To address this, we propose aprompt-guided multimodal interaction framework. This framework utilizes a pairof transformers to facilitate the interaction between different modalities ofinformation. It also uses prompts to transform the score regression task into avideo-text matching task, thus enabling task interactivity. To support furtherresearch in this field, we re-annotate the MTL-AQA and FineGym datasets withhigh-quality and comprehensive action narration. Additionally, we establishbenchmarks for NAE. Extensive experiment results prove that our methodoutperforms separate learning methods and naive multi-task learning methods.Data and code are released at\href{https://github.com/shiyi-zh0408/NAE_CVPR2024 }{here}.</description><author>Shiyi Zhang, Sule Bai, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang</author><pubDate>Mon, 22 Apr 2024 18:55:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14471v1</guid></item><item><title>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</title><link>http://arxiv.org/abs/2404.14471v2</link><description>In this paper, we investigate a new problem called narrative actionevaluation (NAE). NAE aims to generate professional commentary that evaluatesthe execution of an action. Unlike traditional tasks such as score-based actionquality assessment and video captioning involving superficial sentences, NAEfocuses on creating detailed narratives in natural language. These narrativesprovide intricate descriptions of actions along with objective evaluations. NAEis a more challenging task because it requires both narrative flexibility andevaluation rigor. One existing possible solution is to use multi-task learning,where narrative language and evaluative information are predicted separately.However, this approach results in reduced performance for individual tasksbecause of variations between tasks and differences in modality betweenlanguage information and evaluation information. To address this, we propose aprompt-guided multimodal interaction framework. This framework utilizes a pairof transformers to facilitate the interaction between different modalities ofinformation. It also uses prompts to transform the score regression task into avideo-text matching task, thus enabling task interactivity. To support furtherresearch in this field, we re-annotate the MTL-AQA and FineGym datasets withhigh-quality and comprehensive action narration. Additionally, we establishbenchmarks for NAE. Extensive experiment results prove that our methodoutperforms separate learning methods and naive multi-task learning methods.Data and code are released at https://github.com/shiyi-zh0408/NAE_CVPR2024.</description><author>Shiyi Zhang, Sule Bai, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang</author><pubDate>Fri, 26 Apr 2024 15:35:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.14471v2</guid></item><item><title>Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications</title><link>http://arxiv.org/abs/2306.04539v2</link><description>In many machine learning systems that jointly learn from multiple modalities,a core research question is to understand the nature of multimodalinteractions: how modalities combine to provide new task-relevant informationthat was not present in either alone. We study this challenge of interactionquantification in a semi-supervised setting with only labeled unimodal data andnaturally co-occurring multimodal data (e.g., unlabeled images and captions,video and corresponding audio) but when labeling them is time-consuming. Usinga precise information-theoretic definition of interactions, our keycontribution is the derivation of lower and upper bounds to quantify the amountof multimodal interactions in this semi-supervised setting. We propose twolower bounds: one based on the shared information between modalities and theother based on disagreement between separately trained unimodal classifiers,and derive an upper bound through connections to approximate algorithms formin-entropy couplings. We validate these estimated bounds and show how theyaccurately track true interactions. Finally, we show how these theoreticalresults can be used to estimate multimodal model performance, guide datacollection, and select appropriate multimodal models for various tasks.</description><author>Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov</author><pubDate>Thu, 13 Jun 2024 18:05:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04539v2</guid></item><item><title>VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos</title><link>http://arxiv.org/abs/2405.19209v1</link><description>Video-language understanding tasks have focused on short video clips, oftenstruggling with long-form video understanding tasks. Recently, many longvideo-language understanding approaches have leveraged the reasoningcapabilities of Large Language Models (LLMs) to perform long video QA,transforming videos into densely sampled frame captions, and asking LLMs torespond to text queries over captions. However, the frames used for captioningare often redundant and contain irrelevant information, making dense samplinginefficient, and ignoring the fact that video QA requires varying levels ofgranularity, with some video segments being highly relevant to the question(needing more fine-grained detail) while others being less relevant. Thus,these LLM-based approaches are prone to missing information and operate onlarge numbers of irrelevant captions, lowering both performance and efficiency.To address these issues, we introduce VideoTree, a query-adaptive andhierarchical framework for long-video understanding with LLMs. VideoTreedynamically extracts query-related information from a video and builds atree-based representation for LLM reasoning. First, VideoTree adaptivelyselects frames for captioning by iteratively clustering frames based on theirvisual features and scoring clusters using their relevance to the query.Second, it organizes visual clusters into a query-adaptive and hierarchicaltree structure; the tree encodes varying levels of granularity, with higherresolution on relevant segments. Finally, VideoTree produces an answer bytraversing the tree's keyframes and passing their captions to an LLM answerer.Our method improves both reasoning accuracy and efficiency compared to existingmethods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselineson the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, whilereducing inference time by 40%.</description><author>Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, Mohit Bansal</author><pubDate>Wed, 29 May 2024 16:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19209v1</guid></item><item><title>Improving Interpretable Embeddings for Ad-hoc Video Search with Generative Captions and Multi-word Concept Bank</title><link>http://arxiv.org/abs/2404.06173v1</link><description>Aligning a user query and video clips in cross-modal latent space and thatwith semantic concepts are two mainstream approaches for ad-hoc video search(AVS). However, the effectiveness of existing approaches is bottlenecked by thesmall sizes of available video-text datasets and the low quality of conceptbanks, which results in the failures of unseen queries and theout-of-vocabulary problem. This paper addresses these two problems byconstructing a new dataset and developing a multi-word concept bank.Specifically, capitalizing on a generative model, we construct a new datasetconsisting of 7 million generated text and video pairs for pre-training. Totackle the out-of-vocabulary problem, we develop a multi-word concept bankbased on syntax analysis to enhance the capability of a state-of-the-artinterpretable AVS method in modeling relationships between query words. We alsostudy the impact of current advanced features on the method. Experimentalresults show that the integration of the above-proposed elements doubles theR@1 performance of the AVS method on the MSRVTT dataset and improves the xinfAPon the TRECVid AVS query sets for 2016-2023 (eight years) by a margin from 2%to 77%, with an average about 20%.</description><author>Jiaxin Wu, Chong-Wah Ngo, Wing-Kwong Chan</author><pubDate>Tue, 09 Apr 2024 10:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06173v1</guid></item><item><title>MotionLLM: Understanding Human Behaviors from Human Motions and Videos</title><link>http://arxiv.org/abs/2405.20340v1</link><description>This study delves into the realm of multi-modality (i.e., video and motionmodalities) human behavior understanding by leveraging the powerfulcapabilities of Large Language Models (LLMs). Diverging from recent LLMsdesigned for video-only or motion-only understanding, we argue thatunderstanding human behavior necessitates joint modeling from both videos andmotion sequences (e.g., SMPL sequences) to capture nuanced body part dynamicsand semantics effectively. In light of this, we present MotionLLM, astraightforward yet effective framework for human motion understanding,captioning, and reasoning. Specifically, MotionLLM adopts a unifiedvideo-motion training strategy that leverages the complementary advantages ofexisting coarse video-text data and fine-grained motion-text data to glean richspatial-temporal insights. Furthermore, we collect a substantial dataset,MoVid, comprising diverse videos, motions, captions, and instructions.Additionally, we propose the MoVid-Bench, with carefully manual annotations,for better evaluation of human behavior understanding on video and motion.Extensive experiments show the superiority of MotionLLM in the caption,spatial-temporal comprehension, and reasoning ability.</description><author>Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, Lei Zhang</author><pubDate>Thu, 30 May 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.20340v1</guid></item><item><title>Language-Guided Self-Supervised Video Summarization Using Text Semantic Matching Considering the Diversity of the Video</title><link>http://arxiv.org/abs/2405.08890v1</link><description>Current video summarization methods primarily depend on supervised computervision techniques, which demands time-consuming manual annotations. Further,the annotations are always subjective which make this task more challenging. Toaddress these issues, we analyzed the feasibility in transforming the videosummarization into a text summary task and leverage Large Language Models(LLMs) to boost video summarization. This paper proposes a novelself-supervised framework for video summarization guided by LLMs. Our methodbegins by generating captions for video frames, which are then synthesized intotext summaries by LLMs. Subsequently, we measure semantic distance between theframe captions and the text summary. It's worth noting that we propose a novelloss function to optimize our model according to the diversity of the video.Finally, the summarized video can be generated by selecting the frames whosecaptions are similar with the text summary. Our model achieves competitiveresults against other state-of-the-art methods and paves a novel pathway invideo summarization.</description><author>Tomoya Sugihara, Shuntaro Masuda, Ling Xiao, Toshihiko Yamasaki</author><pubDate>Tue, 14 May 2024 19:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08890v1</guid></item><item><title>Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT</title><link>http://arxiv.org/abs/2402.15746v1</link><description>With the rise of short video platforms represented by TikTok, the trend ofusers expressing their creativity through photos and videos has increaseddramatically. However, ordinary users lack the professional skills to producehigh-quality videos using professional creation software. To meet the demandfor intelligent and user-friendly video creation tools, we propose the DynamicVisual Composition (DVC) task, an interesting and challenging task that aims toautomatically integrate various media elements based on user requirements andcreate storytelling videos. We propose an Intelligent Director framework,utilizing LENS to generate descriptions for images and video frames andcombining ChatGPT to generate coherent captions while recommending appropriatemusic names. Then, the best-matched music is obtained through music retrieval.Then, materials such as captions, images, videos, and music are integrated toseamlessly synthesize the video. Finally, we apply AnimeGANv2 for styletransfer. We construct UCF101-DVC and Personal Album datasets and verified theeffectiveness of our framework in solving DVC through qualitative andquantitative comparisons, along with user studies, demonstrating itssubstantial potential.</description><author>Sixiao Zheng, Jingyang Huo, Yu Wang, Yanwei Fu</author><pubDate>Sat, 24 Feb 2024 06:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15746v1</guid></item><item><title>360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model</title><link>http://arxiv.org/abs/2401.06578v2</link><description>Panorama video recently attracts more interest in both study and application,courtesy of its immersive experience. Due to the expensive cost of capturing360-degree panoramic videos, generating desirable panorama videos by prompts isurgently required. Lately, the emerging text-to-video (T2V) diffusion methodsdemonstrate notable effectiveness in standard video generation. However, due tothe significant gap in content and motion patterns between panoramic andstandard videos, these methods encounter challenges in yielding satisfactory360-degree panoramic videos. In this paper, we propose a pipeline named360-Degree Video Diffusion model (360DVD) for generating 360-degree panoramicvideos based on the given prompts and motion conditions. Specifically, weintroduce a lightweight 360-Adapter accompanied by 360 Enhancement Techniquesto transform pre-trained T2V models for panorama video generation. We furtherpropose a new panorama dataset named WEB360 consisting of panoramic video-textpairs for training 360DVD, addressing the absence of captioned panoramic videodatasets. Extensive experiments demonstrate the superiority and effectivenessof 360DVD for panorama video generation. Our project page is athttps://akaneqwq.github.io/360DVD/.</description><author>Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, Jian Zhang</author><pubDate>Fri, 10 May 2024 13:11:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06578v2</guid></item><item><title>TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation</title><link>http://arxiv.org/abs/2405.04682v2</link><description>Recent advances in diffusion-based generative modeling have led to thedevelopment of text-to-video (T2V) models that can generate high-quality videosconditioned on a text prompt. Most of these T2V models often producesingle-scene video clips that depict an entity performing a particular action(e.g., `a red panda climbing a tree'). However, it is pertinent to generatemulti-scene videos since they are ubiquitous in the real-world (e.g., `a redpanda climbing a tree' followed by `the red panda sleeps on the top of thetree'). To generate multi-scene videos from the pretrained T2V model, weintroduce Time-Aligned Captions (TALC) framework. Specifically, we enhance thetext-conditioning mechanism in the T2V architecture to recognize the temporalalignment between the video scenes and scene descriptions. For instance, wecondition the visual features of the earlier and later scenes of the generatedvideo with the representations of the first scene description (e.g., `a redpanda climbing a tree') and second scene description (e.g., `the red pandasleeps on the top of the tree'), respectively. As a result, we show that theT2V model can generate multi-scene videos that adhere to the multi-scene textdescriptions and be visually consistent (e.g., entity and background). Further,we finetune the pretrained T2V model with multi-scene video-text data using theTALC framework. We show that the TALC-finetuned model outperforms the baselinemethods by 15.5 points in the overall score, which averages visual consistencyand text adherence using human evaluation. The project website ishttps://talc-mst2v.github.io/.</description><author>Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, Kai-Wei Chang</author><pubDate>Wed, 15 May 2024 22:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.04682v2</guid></item><item><title>Improved Baselines for Data-efficient Perceptual Augmentation of LLMs</title><link>http://arxiv.org/abs/2403.13499v1</link><description>The abilities of large language models (LLMs) have recently progressed tounprecedented levels, paving the way to novel applications in a wide variety ofareas. In computer vision, LLMs can be used to prime vision-language tasks suchimage captioning and visual question answering when coupled with pre-trainedvision backbones. While different approaches have been explored to interfaceLLMs with ``perceptual backbones'' that process, e.g., visual or audio data,they are often explored for different tasks, different datasets, and usingdifferent perceptual backbones and language models, hindering direct comparisonof the interfacing mechanisms. To remedy this lack of comparability betweenmethods, we present an extensive experimental evaluation of differentinterfacing mechanisms, across multiple tasks (including image, video, andaudio captioning as well as visual question answering), datasets and backbones,paying special attention to low-data settings. We find improved performanceusing existing mechanisms over state-of-the-art results, and identify a newinterfacing mechanism that yields (near) optimal results across differenttasks, while obtaining a 4x reduction in training time.</description><author>Théophane Vallaeys, Mustafa Shukor, Matthieu Cord, Jakob Verbeek</author><pubDate>Wed, 20 Mar 2024 11:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13499v1</guid></item><item><title>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</title><link>http://arxiv.org/abs/2403.15377v1</link><description>We introduce InternVideo2, a new video foundation model (ViFM) that achievesthe state-of-the-art performance in action recognition, video-text tasks, andvideo-centric dialogue. Our approach employs a progressive training paradigmthat unifies the different self- or weakly-supervised learning frameworks ofmasked video token reconstruction, cross-modal contrastive learning, and nexttoken prediction. Different training stages would guide our model to capturedifferent levels of structure and semantic information through differentpretext tasks. At the data level, we prioritize the spatiotemporal consistencyby semantically segmenting videos and generating video-audio-speech captions.This improves the alignment between video and text. We scale both data andmodel size for our InternVideo2. Through extensive experiments, we validate ourdesigns and demonstrate the state-of-the-art performance on over 60 video andaudio tasks. Notably, our model outperforms others on various video-relatedcaptioning, dialogue, and long video understanding benchmarks, highlighting itsability to reason and comprehend long temporal contexts. Code and models areavailable at https://github.com/OpenGVLab/InternVideo2/.</description><author>Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang</author><pubDate>Fri, 22 Mar 2024 18:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15377v1</guid></item><item><title>MoReVQA: Exploring Modular Reasoning Models for Video Question Answering</title><link>http://arxiv.org/abs/2404.06511v1</link><description>This paper addresses the task of video question answering (videoQA) via adecomposed multi-stage, modular reasoning framework. Previous modular methodshave shown promise with a single planning stage ungrounded in visual content.However, through a simple and effective baseline, we find that such systems canlead to brittle behavior in practice for challenging videoQA settings. Thus,unlike traditional single-stage planning methods, we propose a multi-stagesystem consisting of an event parser, a grounding stage, and a final reasoningstage in conjunction with an external memory. All stages are training-free, andperformed using few-shot prompting of large models, creating interpretableintermediate outputs at each stage. By decomposing the underlying planning andtask complexity, our method, MoReVQA, improves over prior work on standardvideoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) withstate-of-the-art results, and extensions to related tasks (grounded videoQA,paragraph captioning).</description><author>Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid</author><pubDate>Tue, 09 Apr 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.06511v1</guid></item><item><title>ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation</title><link>http://arxiv.org/abs/2406.18522v1</link><description>We propose a novel text-to-video (T2V) generation benchmark,ChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of theT2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrastto existing benchmarks that focus on the visual quality and textual relevanceof generated videos, ChronoMagic-Bench focuses on the model's ability togenerate time-lapse videos with significant metamorphic amplitude and temporalcoherence. The benchmark probes T2V models for their physics, biology, andchemistry capabilities, in a free-form text query. For these purposes,ChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,categorized into four major types of time-lapse videos: biological,human-created, meteorological, and physical phenomena, which are furtherdivided into 75 subcategories. This categorization comprehensively evaluatesthe model's capacity to handle diverse and complex transformations. Toaccurately align human preference with the benchmark, we introduce two newautomatic metrics, MTScore and CHScore, to evaluate the videos' metamorphicattributes and temporal coherence. MTScore measures the metamorphic amplitude,reflecting the degree of change over time, while CHScore assesses the temporalcoherence, ensuring the generated videos maintain logical progression andcontinuity. Based on the ChronoMagic-Bench, we conduct comprehensive manualevaluations of ten representative T2V models, revealing their strengths andweaknesses across different categories of prompts, and providing a thoroughevaluation framework that addresses current gaps in video generation research.Moreover, we create a large-scale ChronoMagic-Pro dataset, containing 460khigh-quality pairs of 720p time-lapse videos and detailed captions ensuringhigh physical pertinence and large metamorphic amplitude.</description><author>Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, Li Yuan</author><pubDate>Wed, 26 Jun 2024 18:50:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18522v1</guid></item><item><title>GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension</title><link>http://arxiv.org/abs/2406.18227v1</link><description>There are substantial instructional videos on the Internet, which provide ustutorials for completing various tasks. Existing instructional video datasetsonly focus on specific steps at the video level, lacking experientialguidelines at the task level, which can lead to beginners struggling to learnnew tasks due to the lack of relevant experience. Moreover, the specific stepswithout guidelines are trivial and unsystematic, making it difficult to providea clear tutorial. To address these problems, we present the GUIDE(Guideline-Guided) dataset, which contains 3.5K videos of 560 instructionaltasks in 8 domains related to our daily life. Specifically, we annotate eachinstructional task with a guideline, representing a common pattern shared byall task-related videos. On this basis, we annotate systematic specific steps,including their associated guideline steps, specific step descriptions andtimestamps. Our proposed benchmark consists of three sub-tasks to evaluatecomprehension ability of models: (1) Step Captioning: models have to generatecaptions for specific steps from videos. (2) Guideline Summarization: modelshave to mine the common pattern in task-related videos and summarize aguideline from them. (3) Guideline-Guided Captioning: models have to generatecaptions for specific steps under the guide of guideline. We evaluate plenty offoundation models with GUIDE and perform in-depth analysis. Given the diversityand practicality of GUIDE, we believe that it can be used as a better benchmarkfor instructional video comprehension.</description><author>Jiafeng Liang, Shixin Jiang, Zekun Wang, Haojie Pan, Zerui Chen, Zheng Chu, Ming Liu, Ruiji Fu, Zhongyuan Wang, Bing Qin</author><pubDate>Wed, 26 Jun 2024 11:24:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18227v1</guid></item><item><title>Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward</title><link>http://arxiv.org/abs/2404.01258v2</link><description>Preference modeling techniques, such as direct preference optimization (DPO),has shown effective in enhancing the generalization abilities of large languagemodel (LLM). However, in tasks involving video instruction-following, providinginformative feedback, especially for detecting hallucinations in generatedresponses, remains a significant challenge. Previous studies have exploredusing large large multimodal models (LMMs) as reward models to guide preferencemodeling, but their ability to accurately assess the factuality of generatedresponses compared to corresponding videos has not been conclusivelyestablished. This paper introduces a novel framework that utilizes detailedvideo captions as a proxy of video content, enabling language models toincorporate this information as supporting evidence for scoring video QuestionAnswering (QA) predictions. Our approach demonstrates robust alignment withOpenAI GPT-4V model's reward mechanism, which directly takes video frames asinput. Furthermore, we show that applying this tailored reward through DPOsignificantly improves the performance of video LMMs on video QA tasks.</description><author>Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, Yiming Yang</author><pubDate>Tue, 02 Apr 2024 13:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01258v2</guid></item><item><title>HawkEye: Training Video-Text LLMs for Grounding Text in Videos</title><link>http://arxiv.org/abs/2403.10228v1</link><description>Video-text Large Language Models (video-text LLMs) have shown remarkableperformance in answering questions and holding conversations on simple videos.However, they perform almost the same as random on grounding text queries inlong and complicated videos, having little ability to understand and reasonabout temporal information, which is the most fundamental difference betweenvideos and images. In this paper, we propose HawkEye, one of the firstvideo-text LLMs that can perform temporal video grounding in a fullytext-to-text manner. To collect training data that is applicable for temporalvideo grounding, we construct InternVid-G, a large-scale video-text corpus withsegment-level captions and negative spans, with which we introduce two newtime-aware training objectives to video-text LLMs. We also propose acoarse-grained method of representing segments in videos, which is more robustand easier for LLMs to learn and follow than other alternatives. Extensiveexperiments show that HawkEye is better at temporal video grounding andcomparable on other video-text tasks with existing video-text LLMs, whichverifies its superior video-text multi-modal understanding abilities.</description><author>Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, Dongyan Zhao</author><pubDate>Fri, 15 Mar 2024 12:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10228v1</guid></item><item><title>Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection</title><link>http://arxiv.org/abs/2403.01169v1</link><description>Most models for weakly supervised video anomaly detection (WS-VAD) rely onmultiple instance learning, aiming to distinguish normal and abnormal snippetswithout specifying the type of anomaly. The ambiguous nature of anomalydefinitions across contexts introduces bias in detecting abnormal and normalsnippets within the abnormal bag. Taking the first step to show the model whyit is anomalous, a novel framework is proposed to guide the learning ofsuspected anomalies from event prompts. Given a textual prompt dictionary ofpotential anomaly events and the captions generated from anomaly videos, thesemantic anomaly similarity between them could be calculated to identify thesuspected anomalous events for each video snippet. It enables a newmulti-prompt learning process to constrain the visual-semantic features acrossall videos, as well as provides a new way to label pseudo anomalies forself-training. To demonstrate effectiveness, comprehensive experiments anddetailed ablation studies are conducted on four datasets, namely XD-Violence,UCF-Crime, TAD, and ShanghaiTech. Our proposed model outperforms moststate-of-the-art methods in terms of AP or AUC (82.6\%, 87.7\%, 93.1\%, and97.4\%). Furthermore, it shows promising performance in open-set andcross-dataset cases.</description><author>Chenchen Tao, Chong Wang, Yuexian Zou, Xiaohao Peng, Jiafei Wu, Jiangbo Qian</author><pubDate>Sat, 02 Mar 2024 10:42:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01169v1</guid></item><item><title>End-to-End Dense Video Grounding via Parallel Regression</title><link>http://arxiv.org/abs/2109.11265v5</link><description>Video grounding aims to localize the corresponding video moment in anuntrimmed video given a language query. Existing methods often address thistask in an indirect way, by casting it as a proposal-and-match orfusion-and-detection problem. Solving these surrogate problems often requiressophisticated label assignment during training and hand-crafted removal ofnear-duplicate results. Meanwhile, existing works typically focus on sparsevideo grounding with a single sentence as input, which could result inambiguous localization due to its unclear description. In this paper, we tacklea new problem of dense video grounding, by simultaneously localizing multiplemoments with a paragraph as input. From a perspective on video grounding aslanguage conditioned regression, we present an end-to-end parallel decodingparadigm by re-purposing a Transformer-alike architecture (PRVG). The keydesign in our PRVG is to use languages as queries, and directly regress themoment boundaries based on language-modulated visual representations. Thanks toits simplicity in design, our PRVG framework can be applied in differenttesting schemes (sparse or dense grounding) and allows for efficient inferencewithout any post-processing technique. In addition, we devise a robustproposal-level attention loss to guide the training of PRVG, which is invariantto moment duration and contributes to model convergence. We perform experimentson two video grounding benchmarks of ActivityNet Captions and TACoS,demonstrating that our PRVG can significantly outperform previous methods. Wealso perform in-depth studies to investigate the effectiveness of parallelregression paradigm on video grounding.</description><author>Fengyuan Shi, Weilin Huang, Limin Wang</author><pubDate>Wed, 28 Feb 2024 13:04:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.11265v5</guid></item><item><title>VideoLLM-online: Online Video Large Language Model for Streaming Video</title><link>http://arxiv.org/abs/2406.11816v1</link><description>Recent Large Language Models have been enhanced with vision capabilities,enabling them to comprehend images, videos, and interleaved vision-languagecontent. However, the learning methods of these large multimodal modelstypically treat videos as predetermined clips, making them less effective andefficient at handling streaming video inputs. In this paper, we propose a novelLearning-In-Video-Stream (LIVE) framework, which enables temporally aligned,long-context, and real-time conversation within a continuous video stream. OurLIVE framework comprises comprehensive approaches to achieve video streamingdialogue, encompassing: (1) a training objective designed to perform languagemodeling for continuous streaming inputs, (2) a data generation scheme thatconverts offline temporal annotations into a streaming dialogue format, and (3)an optimized inference pipeline to speed up the model responses in real-worldvideo streams. With our LIVE framework, we built VideoLLM-online model uponLlama-2/Llama-3 and demonstrate its significant advantages in processingstreaming videos. For instance, on average, our model can support streamingdialogue in a 5-minute video clip at over 10 FPS on an A100 GPU. Moreover, italso showcases state-of-the-art performance on public offline video benchmarks,such as recognition, captioning, and forecasting. The code, model, data, anddemo have been made available at https://showlab.github.io/videollm-online.</description><author>Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Zheng Shou</author><pubDate>Mon, 17 Jun 2024 18:55:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11816v1</guid></item><item><title>PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter</title><link>http://arxiv.org/abs/2402.10896v1</link><description>This paper demonstrates that a progressively aligned language model caneffectively bridge frozen vision encoders and large language models (LLMs).While the fundamental architecture and pre-training methods of vision encodersand LLMs have been extensively studied, the architecture and training strategyof vision-language adapters vary significantly across recent works. Ourresearch undertakes a thorough exploration of the state-of-the-art perceiverresampler architecture and builds a strong baseline. However, we observe thatthe vision-language alignment with perceiver resampler exhibits slowconvergence and limited scalability with a lack of direct supervision. Toaddress this issue, we propose PaLM2-VAdapter, employing a progressivelyaligned language model as the vision-language adapter. Compared to the strongbaseline with perceiver resampler, our method empirically shows fasterconvergence, higher performance, and stronger scalability. Extensiveexperiments across various Visual Question Answering (VQA) and captioning taskson both images and videos demonstrate that our model exhibits state-of-the-artvisual understanding and multi-modal reasoning capabilities. Notably, ourmethod achieves these advancements with 30~70% fewer parameters than thestate-of-the-art large vision-language models, marking a significant efficiencyimprovement.</description><author>Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang</author><pubDate>Fri, 16 Feb 2024 18:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10896v1</guid></item><item><title>ICSVR: Investigating Compositional and Syntactic Understanding in Video Retrieval Models</title><link>http://arxiv.org/abs/2306.16533v2</link><description>Video retrieval (VR) involves retrieving the ground truth video from thevideo database given a text caption or vice-versa. The two important componentsof compositionality: objects &amp; attributes and actions are joined using correctsyntax to form a proper text query. These components (objects &amp; attributes,actions and syntax) each play an important role to help distinguish amongvideos and retrieve the correct ground truth video. However, it is unclear whatis the effect of these components on the video retrieval performance. Wetherefore, conduct a systematic study to evaluate the compositional andsyntactic understanding of video retrieval models on standard benchmarks suchas MSRVTT, MSVD and DIDEMO. The study is performed on two categories of videoretrieval models: (i) which are pre-trained on video-text pairs and fine-tunedon downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)(ii) which adapt pre-trained image-text representations like CLIP for videoretrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal thatactions and syntax play a minor role compared to objects &amp; attributes in videounderstanding. Moreover, video retrieval models that use pre-trained image-textrepresentations (CLIP) have better syntactic and compositional understanding ascompared to models pre-trained on video-text data. The code is available athttps://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/ICSVR</description><author>Avinash Madasu, Vasudev Lal</author><pubDate>Wed, 17 Apr 2024 12:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16533v2</guid></item><item><title>Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA</title><link>http://arxiv.org/abs/2406.09396v2</link><description>Long-form videos that span across wide temporal intervals are highlyinformation redundant and contain multiple distinct events or entities that areoften loosely-related. Therefore, when performing long-form video questionanswering (LVQA),all information necessary to generate a correct response canoften be contained within a small subset of frames. Recent literature explorethe use of large language models (LLMs) in LVQA benchmarks, achievingexceptional performance, while relying on vision language models (VLMs) toconvert all visual content within videos into natural language. Such VLMs oftenindependently caption a large number of frames uniformly sampled from longvideos, which is not efficient and can mostly be redundant. Questioning thesedecision choices, we explore optimal strategies for key-frame selection andsequence-aware captioning, that can significantly reduce these redundancies. Wepropose two novel approaches that improve each of aspects, namely HierarchicalKeyframe Selector and Sequential Visual LLM. Our resulting framework termedLVNet achieves state-of-the-art performance across three benchmark LVQAdatasets. Our code will be released publicly.</description><author>Jongwoo Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Wonjeong Ryoo, Donghyun Kim, Michael S. Ryoo</author><pubDate>Mon, 17 Jun 2024 18:50:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09396v2</guid></item><item><title>Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA</title><link>http://arxiv.org/abs/2406.09396v1</link><description>Long-form videos that span across wide temporal intervals are highlyinformation redundant and contain multiple distinct events or entities that areoften loosely-related. Therefore, when performing long-form video questionanswering (LVQA),all information necessary to generate a correct response canoften be contained within a small subset of frames. Recent literature explorethe use of large language models (LLMs) in LVQA benchmarks, achievingexceptional performance, while relying on vision language models (VLMs) toconvert all visual content within videos into natural language. Such VLMs oftenindependently caption a large number of frames uniformly sampled from longvideos, which is not efficient and can mostly be redundant. Questioning thesedecision choices, we explore optimal strategies for key-frame selection andsequence-aware captioning, that can significantly reduce these redundancies. Wepropose two novel approaches that improve each of aspects, namely HierarchicalKeyframe Selector and Sequential Visual LLM. Our resulting framework termedLVNet achieves state-of-the-art performance across three benchmark LVQAdatasets. Our code will be released publicly.</description><author>Jongwoo Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Wonjeong Ryoo, Donghyun Kim, Michael S. Ryoo</author><pubDate>Thu, 13 Jun 2024 18:59:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09396v1</guid></item><item><title>VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding</title><link>http://arxiv.org/abs/2406.09418v1</link><description>Building on the advances of language models, Large Multimodal Models (LMMs)have contributed significant improvements in video understanding. While thecurrent video LMMs utilize advanced Large Language Models (LLMs), they rely oneither image or video encoders to process visual inputs, each of which has itsown limitations. Image encoders excel at capturing rich spatial details fromframe sequences but lack explicit temporal context, which can be important invideos with intricate action sequences. On the other hand, video encodersprovide temporal context but are often limited by computational constraintsthat lead to processing only sparse frames at lower resolutions, resulting inreduced contextual and spatial understanding. To this end, we introduceVideoGPT+, which combines the complementary benefits of the image encoder (fordetailed spatial understanding) and the video encoder (for global temporalcontext modeling). The model processes videos by dividing them into smallersegments and applies an adaptive pooling strategy on features extracted by bothimage and video encoders. Our architecture showcases improved performanceacross multiple video benchmarks, including VCGBench, MVBench and Zero-shotquestion-answering. Further, we develop 112K video-instruction set using anovel semi-automatic annotation pipeline which further improves the modelperformance. Additionally, to comprehensively evaluate video LMMs, we presentVCGBench-Diverse, covering 18 broad video categories such as lifestyle, sports,science, gaming, and surveillance videos. This benchmark with 4,354question-answer pairs evaluates the generalization of existing LMMs on densevideo captioning, spatial and temporal understanding, and complex reasoning,ensuring comprehensive assessment across diverse video types and dynamics.Code: https://github.com/mbzuai-oryx/VideoGPT-plus.</description><author>Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Khan</author><pubDate>Thu, 13 Jun 2024 18:59:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.09418v1</guid></item><item><title>EgoNCE++: Do Egocentric Video-Language Models Really Understand Hand-Object Interactions?</title><link>http://arxiv.org/abs/2405.17719v2</link><description>Egocentric video-language pretraining is a crucial paradigm to advance thelearning of egocentric hand-object interactions (EgoHOI). Despite the greatsuccess on existing testbeds, these benchmarks focus more on closed-set visualconcepts or limited scenarios. Due to the occurrence of diverse EgoHOIs in thereal world, we propose an open-vocabulary benchmark named EgoHOIBench to revealthe diminished performance of current egocentric video-language models (EgoVLM)on fined-grained concepts, indicating that these models still lack a fullspectrum of egocentric understanding. We attribute this performance gap toinsufficient fine-grained supervision and strong bias towards understandingobjects rather than temporal dynamics in current methods. To tackle theseissues, we introduce a novel asymmetric contrastive objective for EgoHOI namedEgoNCE++. For video-to-text loss, we enhance text supervision through thegeneration of negative captions by leveraging the in-context learning of largelanguage models to perform HOI-related word substitution. For text-to-videoloss, we propose an object-centric positive video sampling strategy thataggregates video representations by the same nouns. Our extensive experimentsdemonstrate that EgoNCE++ significantly boosts open-vocabulary HOI recognition,multi-instance retrieval, and action recognition tasks across variousegocentric models, with improvements of up to +26.55%. Our code is available athttps://github.com/xuboshen/EgoNCEpp.</description><author>Boshen Xu, Ziheng Wang, Yang Du, Zhinan Song, Sipeng Zheng, Qin Jin</author><pubDate>Mon, 03 Jun 2024 08:29:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.17719v2</guid></item><item><title>Self-Explainable Affordance Learning with Embodied Caption</title><link>http://arxiv.org/abs/2404.05603v1</link><description>In the field of visual affordance learning, previous methods mainly usedabundant images or videos that delineate human behavior patterns to identifyaction possibility regions for object manipulation, with a variety ofapplications in robotic tasks. However, they encounter a main challenge ofaction ambiguity, illustrated by the vagueness like whether to beat or carry adrum, and the complexities involved in processing intricate scenes. Moreover,it is important for human intervention to rectify robot errors in time. Toaddress these issues, we introduce Self-Explainable Affordance learning (SEA)with embodied caption. This innovation enables robots to articulate theirintentions and bridge the gap between explainable vision-language caption andvisual affordance learning. Due to a lack of appropriate dataset, we unveil apioneering dataset and metrics tailored for this task, which integrates images,heatmaps, and embodied captions. Furthermore, we propose a novel model toeffectively combine affordance grounding with self-explanation in a simple butefficient manner. Extensive quantitative and qualitative experimentsdemonstrate our method's effectiveness.</description><author>Zhipeng Zhang, Zhimin Wei, Guolei Sun, Peng Wang, Luc Van Gool</author><pubDate>Mon, 08 Apr 2024 16:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05603v1</guid></item><item><title>VidCoM: Fast Video Comprehension through Large Language Models with Multimodal Tools</title><link>http://arxiv.org/abs/2310.10586v2</link><description>Building models that comprehends videos and responds specific userinstructions is a practical and challenging topic, as it requires mastery ofboth vision understanding and knowledge reasoning. Compared to language andimage modalities, training efficiency remains a serious problem as existingstudies train models on massive sparse videos paired with brief descriptions.In this paper, we introduce \textbf{VidCoM}, a fast adaptive framework thatleverages Large Language Models (LLMs) to reason about videos using lightweightvisual tools. Specifically, we reveal that the key to responding to specificinstructions is focusing on relevant video events, and utilize two visualtools, structured scene graph generation and descriptive image captiongeneration, to gather and represent the event information. Thus, a LLM enrichedwith world knowledge is adopted as the reasoning agent to achieve the responsesby performing multiple reasoning steps on specific video events. To address thedifficulty of LLMs identifying video events, we further propose anInstruction-oriented Video Events Recognition (InsOVER) algorithm. Thisalgorithm locates the corresponding video events based on an efficientHungarian matching between decompositions of linguistic instructions and videoevents, thereby enabling LLMs to interact effectively with extended videos.Extensive experiments on two typical video comprehension tasks show that theproposed tuning-free framework outperforms the pre-trained models includingFlamingo-80B, to achieve the state-of-the-art performance. Our source code andsystem will be publicly available.</description><author>Ji Qi, Kaixuan Ji, Jifan Yu, Duokang Wang, Bin Xu, Lei Hou, Juanzi Li</author><pubDate>Sat, 27 Apr 2024 09:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10586v2</guid></item><item><title>ID-Animator: Zero-Shot Identity-Preserving Human Video Generation</title><link>http://arxiv.org/abs/2404.15275v3</link><description>Generating high-fidelity human video with specified identities has attractedsignificant attention in the content generation community. However, existingtechniques struggle to strike a balance between training efficiency andidentity preservation, either requiring tedious case-by-case fine-tuning orusually missing identity details in the video generation process. In thisstudy, we present \textbf{ID-Animator}, a zero-shot human-video generationapproach that can perform personalized video generation given a singlereference facial image without further training. ID-Animator inherits existingdiffusion-based video generation backbones with a face adapter to encode theID-relevant embeddings from learnable facial latent queries. To facilitate theextraction of identity information in video generation, we introduce anID-oriented dataset construction pipeline that incorporates unified humanattributes and action captioning techniques from a constructed facial imagepool. Based on this pipeline, a random reference training strategy is furtherdevised to precisely capture the ID-relevant embeddings with an ID-preservingloss, thus improving the fidelity and generalization capacity of our model forID-specific video generation. Extensive experiments demonstrate the superiorityof ID-Animator to generate personalized human videos over previous models.Moreover, our method is highly compatible with popular pre-trained T2V modelslike animatediff and various community backbone models, showing highextendability in real-world applications for video generation where identitypreservation is highly desired. Our codes and checkpoints are released athttps://github.com/ID-Animator/ID-Animator.</description><author>Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Jie Zhang</author><pubDate>Tue, 25 Jun 2024 17:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15275v3</guid></item><item><title>VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT</title><link>http://arxiv.org/abs/2403.02076v1</link><description>Video temporal grounding (VTG) aims to locate specific temporal segments froman untrimmed video based on a linguistic query. Most existing VTG models aretrained on extensive annotated video-text pairs, a process that not onlyintroduces human biases from the queries but also incurs significantcomputational costs. To tackle these challenges, we propose VTG-GPT, aGPT-based method for zero-shot VTG without training or fine-tuning. To reduceprejudice in the original query, we employ Baichuan2 to generate debiasedqueries. To lessen redundant information in videos, we apply MiniGPT-v2 totransform visual content into more precise captions. Finally, we devise theproposal generator and post-processing to produce accurate segments fromdebiased queries and image captions. Extensive experiments demonstrate thatVTG-GPT significantly outperforms SOTA methods in zero-shot settings andsurpasses unsupervised approaches. More notably, it achieves competitiveperformance comparable to supervised methods. The code is available onhttps://github.com/YoucanBaby/VTG-GPT</description><author>Yifang Xu, Yunzhuo Sun, Zien Xie, Benxiang Zhai, Sidan Du</author><pubDate>Mon, 04 Mar 2024 14:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02076v1</guid></item><item><title>Diving Deep into the Motion Representation of Video-Text Models</title><link>http://arxiv.org/abs/2406.05075v1</link><description>Videos are more informative than images because they capture the dynamics ofthe scene. By representing motion in videos, we can capture dynamic activities.In this work, we introduce GPT-4 generated motion descriptions that capturefine-grained motion descriptions of activities and apply them to three actiondatasets. We evaluated several video-text models on the task of retrieval ofmotion descriptions. We found that they fall far behind human expertperformance on two action datasets, raising the question of whether video-textmodels understand motion in videos. To address it, we introduce a method ofimproving motion understanding in video-text models by utilizing motiondescriptions. This method proves to be effective on two action datasets for themotion description retrieval task. The results draw attention to the need forquality captions involving fine-grained motion information in existing datasetsand demonstrate the effectiveness of the proposed pipeline in understandingfine-grained motion during video-text retrieval.</description><author>Chinmaya Devaraj, Cornelia Fermuller, Yiannis Aloimonos</author><pubDate>Fri, 07 Jun 2024 17:46:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05075v1</guid></item><item><title>MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos</title><link>http://arxiv.org/abs/2406.08407v1</link><description>Multimodal Language Language Models (MLLMs) demonstrate the emergingabilities of "world models" -- interpreting and reasoning about complexreal-world dynamics. To assess these abilities, we posit videos are the idealmedium, as they encapsulate rich representations of real-world dynamics andcausalities. To this end, we introduce MMWorld, a new benchmark formulti-discipline, multi-faceted multimodal video understanding. MMWorlddistinguishes itself from previous video understanding benchmarks with twounique advantages: (1) multi-discipline, covering various disciplines thatoften require domain expertise for comprehensive understanding; (2)multi-faceted reasoning, including explanation, counterfactual thinking, futureprediction, etc. MMWorld consists of a human-annotated dataset to evaluateMLLMs with questions about the whole videos and a synthetic dataset to analyzeMLLMs within a single modality of perception. Together, MMWorld encompasses1,910 videos across seven broad disciplines and 69 subdisciplines, completewith 6,627 question-answer pairs and associated captions. The evaluationincludes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld(e.g., GPT-4V performs the best with only 52.3\% accuracy), showing large roomfor improvement. Further ablation studies reveal other interesting findingssuch as models' different skill sets from humans. We hope MMWorld can serve asan essential step towards world model evaluation in videos.</description><author>Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Kevin Lin, William Yang Wang, Lijuan Wang, Xin Eric Wang</author><pubDate>Wed, 12 Jun 2024 17:54:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08407v1</guid></item><item><title>Mozart's Touch: A Lightweight Multi-modal Music Generation Framework Based on Pre-Trained Large Models</title><link>http://arxiv.org/abs/2405.02801v2</link><description>In recent years, AI-Generated Content (AIGC) has witnessed rapidadvancements, facilitating the generation of music, images, and other forms ofartistic expression across various industries. However, researches on generalmulti-modal music generation model remain scarce. To fill this gap, we proposea multi-modal music generation framework Mozart's Touch. It could generatealigned music with the cross-modality inputs, such as images, videos and text.Mozart's Touch is composed of three main components: Multi-modal CaptioningModule, Large Language Model (LLM) Understanding &amp; Bridging Module, and MusicGeneration Module. Unlike traditional approaches, Mozart's Touch requires notraining or fine-tuning pre-trained models, offering efficiency andtransparency through clear, interpretable prompts. We also introduce"LLM-Bridge" method to resolve the heterogeneous representation problemsbetween descriptive texts of different modalities. We conduct a series ofobjective and subjective evaluations on the proposed model, and resultsindicate that our model surpasses the performance of current state-of-the-artmodels. Our codes and examples is availble at:https://github.com/WangTooNaive/MozartsTouch</description><author>Tianze Xu, Jiajun Li, Xuesong Chen, Xinrui Yao, Shuchang Liu</author><pubDate>Tue, 07 May 2024 10:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.02801v2</guid></item><item><title>LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision</title><link>http://arxiv.org/abs/2304.07647v4</link><description>We propose LASER, a neuro-symbolic approach to learn semantic videorepresentations that capture rich spatial and temporal properties in video databy leveraging high-level logic specifications. In particular, we formulate theproblem in terms of alignment between raw videos and spatio-temporal logicspecifications. The alignment algorithm leverages a differentiable symbolicreasoner and a combination of contrastive, temporal, and semantics losses. Iteffectively and efficiently trains low-level perception models to extract afine-grained video representation in the form of a spatio-temporal scene graphthat conforms to the desired high-level specification. To practically reducethe manual effort of obtaining ground truth labels, we derive logicspecifications from captions by employing a large language model with a genericprompting template. In doing so, we explore a novel methodology that weaklysupervises the learning of spatio-temporal scene graphs with widely accessiblevideo-caption data. We evaluate our method on three datasets with rich spatialand temporal specifications: 20BN-Something-Something, MUGEN, and OpenPVSG. Wedemonstrate that our method learns better fine-grained video semantics thanexisting baselines.</description><author>Jiani Huang, Ziyang Li, Mayur Naik, Ser-Nam Lim</author><pubDate>Wed, 12 Jun 2024 18:16:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07647v4</guid></item><item><title>OSCaR: Object State Captioning and State Change Representation</title><link>http://arxiv.org/abs/2402.17128v3</link><description>The capability of intelligent models to extrapolate and comprehend changes inobject states is a crucial yet demanding aspect of AI research, particularlythrough the lens of human interaction in real-world settings. This taskinvolves describing complex visual environments, identifying active objects,and interpreting their changes as conveyed through language. Traditionalmethods, which isolate object captioning and state change detection, offer alimited view of dynamic environments. Moreover, relying on a small set ofsymbolic words to represent changes has restricted the expressiveness of thelanguage. To address these challenges, in this paper, we introduce the ObjectState Captioning and State Change Representation (OSCaR) dataset and benchmark.OSCaR consists of 14,084 annotated video segments with nearly 1,000 uniqueobjects from various egocentric video collections. It sets a new testbed forevaluating multimodal large language models (MLLMs). Our experimentsdemonstrate that while MLLMs show some skill, they lack a full understanding ofobject state changes. The benchmark includes a fine-tuned model that, despiteinitial capabilities, requires significant improvements in accuracy andgeneralization ability for effective understanding of these changes. Our codeand dataset are available at https://github.com/nguyennm1024/OSCaR.</description><author>Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu</author><pubDate>Wed, 20 Mar 2024 15:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17128v3</guid></item><item><title>OSCaR: Object State Captioning and State Change Representation</title><link>http://arxiv.org/abs/2402.17128v2</link><description>The capability of intelligent models to extrapolate and comprehend changes inobject states is a crucial yet demanding aspect of AI research, particularlythrough the lens of human interaction in real-world settings. This taskinvolves describing complex visual environments, identifying active objects,and interpreting their changes as conveyed through language. Traditionalmethods, which isolate object captioning and state change detection, offer alimited view of dynamic environments. Moreover, relying on a small set ofsymbolic words to represent changes has restricted the expressiveness oflanguage. To address these challenges, in this paper, we introduce the ObjectState Captioning and State Change Representation (OSCaR) dataset and benchmark.OSCaR consists of 14,084 annotated video segments with nearly 1,000 uniqueobjects from various egocentric video collections. It sets a new testbed forevaluating multimodal large language models (MLLMs). Our experimentsdemonstrate that while MLLMs show some skill, they lack a full understanding ofobject state changes. The benchmark includes a fine-tuned model that, despiteinitial capabilities, requires significant improvements in accuracy andgeneralization ability for effective understanding of these changes. Our codeand dataset are available at https://github.com/nguyennm1024/OSCaR.</description><author>Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu</author><pubDate>Wed, 28 Feb 2024 02:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17128v2</guid></item><item><title>OSCaR: Object State Captioning and State Change Representation</title><link>http://arxiv.org/abs/2402.17128v4</link><description>The capability of intelligent models to extrapolate and comprehend changes inobject states is a crucial yet demanding aspect of AI research, particularlythrough the lens of human interaction in real-world settings. This taskinvolves describing complex visual environments, identifying active objects,and interpreting their changes as conveyed through language. Traditionalmethods, which isolate object captioning and state change detection, offer alimited view of dynamic environments. Moreover, relying on a small set ofsymbolic words to represent changes has restricted the expressiveness of thelanguage. To address these challenges, in this paper, we introduce the ObjectState Captioning and State Change Representation (OSCaR) dataset and benchmark.OSCaR consists of 14,084 annotated video segments with nearly 1,000 uniqueobjects from various egocentric video collections. It sets a new testbed forevaluating multimodal large language models (MLLMs). Our experimentsdemonstrate that while MLLMs show some skill, they lack a full understanding ofobject state changes. The benchmark includes a fine-tuned model that, despiteinitial capabilities, requires significant improvements in accuracy andgeneralization ability for effective understanding of these changes. Our codeand dataset are available at https://github.com/nguyennm1024/OSCaR.</description><author>Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu</author><pubDate>Wed, 03 Apr 2024 00:14:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17128v4</guid></item><item><title>The Surprising Effectiveness of Multimodal Large Language Models for Video Moment Retrieval</title><link>http://arxiv.org/abs/2406.18113v1</link><description>Recent studies have shown promising results in utilizing multimodal largelanguage models (MLLMs) for computer vision tasks such as object detection andsemantic segmentation. However, many challenging video tasks remainunder-explored. Video-language tasks necessitate spatial and temporalcomprehension and require significant compute. Therefore, prior works havedeveloped complex, highly specialized architectures or leveraged additionalinput signals such as video transcripts to best encode contextual and temporalinformation, which limits their generality and can be impractical. Oneparticularly challenging task is video moment retrieval, which requires precisetemporal and contextual grounding. This work demonstrates the surprisingeffectiveness of leveraging image-text pretrained MLLMs for moment retrieval.We introduce Mr. BLIP (Mr. as in Moment Retrieval), a multimodal, single-stagemodel that requires no expensive video-language pretraining, no additionalinput signal (e.g., no transcript or audio), and has a simpler and moreversatile design than prior state-of-the-art methods. We achieve a newstate-of-the-art in moment retrieval on the widely used benchmarksCharades-STA, QVHighlights, and ActivityNet Captions and illustrate ourmethod's versatility with a new state-of-the-art in temporal actionlocalization on ActivityNet. Notably, we attain over 9% (absolute) higherRecall (at 0.5 and 0.7 IoU) on the challenging long-video multi-momentQVHighlights benchmark. Our code is publicly available.</description><author>Meinardus Boris, Batra Anil, Rohrbach Anna, Rohrbach Marcus</author><pubDate>Wed, 26 Jun 2024 07:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18113v1</guid></item><item><title>MatchTime: Towards Automatic Soccer Game Commentary Generation</title><link>http://arxiv.org/abs/2406.18530v1</link><description>Soccer is a globally popular sport with a vast audience, in this paper, weconsider constructing an automatic soccer game commentary model to improve theaudiences' viewing experience. In general, we make the following contributions:First, observing the prevalent video-text misalignment in existing datasets, wemanually annotate timestamps for 49 matches, establishing a more robustbenchmark for soccer game commentary generation, termed asSN-Caption-test-align; Second, we propose a multi-modal temporal alignmentpipeline to automatically correct and filter the existing dataset at scale,creating a higher-quality soccer game commentary dataset for training, denotedas MatchTime; Third, based on our curated dataset, we train an automaticcommentary generation model, named MatchVoice. Extensive experiments andablation studies have demonstrated the effectiveness of our alignment pipeline,and training model on the curated datasets achieves state-of-the-artperformance for commentary generation, showcasing that better alignment canlead to significant performance improvements in downstream tasks.</description><author>Jiayuan Rao, Haoning Wu, Chang Liu, Yanfeng Wang, Weidi Xie</author><pubDate>Wed, 26 Jun 2024 18:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18530v1</guid></item><item><title>TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</title><link>http://arxiv.org/abs/2312.02051v2</link><description>This work proposes TimeChat, a time-sensitive multimodal large language modelspecifically designed for long video understanding. Our model incorporates twokey architectural contributions: (1) a timestamp-aware frame encoder that bindsvisual content with the timestamp of each frame, and (2) a sliding videoQ-Former that produces a video token sequence of varying lengths to accommodatevideos of various durations. Additionally, we construct an instruction-tuningdataset, encompassing 6 tasks and a total of 125K instances, to further enhanceTimeChat's instruction-following performance. Experiment results across variousvideo understanding tasks, such as dense captioning, temporal grounding, andhighlight detection, demonstrate TimeChat's strong zero-shot temporallocalization and reasoning capabilities. For example, it achieves +9.2 F1 scoreand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)on Charades-STA, compared to state-of-the-art video large language models,holding the potential to serve as a versatile video assistant for long-formvideo comprehension tasks and satisfy realistic user requirements.</description><author>Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou</author><pubDate>Thu, 28 Mar 2024 13:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02051v2</guid></item><item><title>EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition</title><link>http://arxiv.org/abs/2310.16640v2</link><description>Facial Expression Recognition (FER) is a crucial task in affective computing,but its conventional focus on the seven basic emotions limits its applicabilityto the complex and expanding emotional spectrum. To address the issue of newand unseen emotions present in dynamic in-the-wild FER, we propose a novelvision-language model that utilises sample-level text descriptions (i.e.captions of the context, expressions or emotional cues) as natural languagesupervision, aiming to enhance the learning of rich latent representations, forzero-shot classification. To test this, we evaluate using zero-shotclassification of the model trained on sample-level descriptions on fourpopular dynamic FER datasets. Our findings show that this approach yieldssignificant improvements when compared to baseline methods. Specifically, forzero-shot video FER, we outperform CLIP by over 10\% in terms of WeightedAverage Recall and 5\% in terms of Unweighted Average Recall on severaldatasets. Furthermore, we evaluate the representations obtained from thenetwork trained using sample-level descriptions on the downstream task ofmental health symptom estimation, achieving performance comparable or superiorto state-of-the-art methods and strong agreement with human experts. Namely, weachieve a Pearson's Correlation Coefficient of up to 0.85 on schizophreniasymptom severity estimation, which is comparable to human experts' agreement.The code is publicly available at: https://github.com/NickyFot/EmoCLIP.</description><author>Niki Maria Foteinopoulou, Ioannis Patras</author><pubDate>Mon, 18 Mar 2024 10:07:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16640v2</guid></item><item><title>Test-Time Zero-Shot Temporal Action Localization</title><link>http://arxiv.org/abs/2404.05426v1</link><description>Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locateactions in untrimmed videos unseen during training. Existing ZS-TAL methodsinvolve fine-tuning a model on a large amount of annotated training data. Whileeffective, training-based ZS-TAL approaches assume the availability of labeleddata for supervised learning, which can be impractical in some applications.Furthermore, the training process naturally induces a domain bias into thelearned model, which may adversely affect the model's generalization ability toarbitrary videos. These considerations prompt us to approach the ZS-TAL problemfrom a radically novel perspective, relaxing the requirement for training data.To this aim, we introduce a novel method that performs Test-Time adaptation forTemporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trainedVision and Language Model (VLM). T3AL operates in three steps. First, avideo-level pseudo-label of the action category is computed by aggregatinginformation from the entire video. Then, action localization is performedadopting a novel procedure inspired by self-supervised learning. Finally,frame-level textual descriptions extracted with a state-of-the-art captioningmodel are employed for refining the action region proposals. We validate theeffectiveness of T3AL by conducting experiments on the THUMOS14 and theActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantlyoutperforms zero-shot baselines based on state-of-the-art VLMs, confirming thebenefit of a test-time adaptation approach.</description><author>Benedetta Liberatori, Alessandro Conti, Paolo Rota, Yiming Wang, Elisa Ricci</author><pubDate>Mon, 08 Apr 2024 12:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05426v1</guid></item><item><title>Test-Time Zero-Shot Temporal Action Localization</title><link>http://arxiv.org/abs/2404.05426v2</link><description>Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locateactions in untrimmed videos unseen during training. Existing ZS-TAL methodsinvolve fine-tuning a model on a large amount of annotated training data. Whileeffective, training-based ZS-TAL approaches assume the availability of labeleddata for supervised learning, which can be impractical in some applications.Furthermore, the training process naturally induces a domain bias into thelearned model, which may adversely affect the model's generalization ability toarbitrary videos. These considerations prompt us to approach the ZS-TAL problemfrom a radically novel perspective, relaxing the requirement for training data.To this aim, we introduce a novel method that performs Test-Time adaptation forTemporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trainedVision and Language Model (VLM). T3AL operates in three steps. First, avideo-level pseudo-label of the action category is computed by aggregatinginformation from the entire video. Then, action localization is performedadopting a novel procedure inspired by self-supervised learning. Finally,frame-level textual descriptions extracted with a state-of-the-art captioningmodel are employed for refining the action region proposals. We validate theeffectiveness of T3AL by conducting experiments on the THUMOS14 and theActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantlyoutperforms zero-shot baselines based on state-of-the-art VLMs, confirming thebenefit of a test-time adaptation approach.</description><author>Benedetta Liberatori, Alessandro Conti, Paolo Rota, Yiming Wang, Elisa Ricci</author><pubDate>Thu, 11 Apr 2024 08:12:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05426v2</guid></item><item><title>LLM-AD: Large Language Model based Audio Description System</title><link>http://arxiv.org/abs/2405.00983v1</link><description>The development of Audio Description (AD) has been a pivotal step forward inmaking video content more accessible and inclusive. Traditionally, ADproduction has demanded a considerable amount of skilled labor, while existingautomated approaches still necessitate extensive training to integratemultimodal inputs and tailor the output from a captioning style to an AD style.In this paper, we introduce an automated AD generation pipeline that harnessesthe potent multimodal and instruction-following capacities of GPT-4V(ision).Notably, our methodology employs readily available components, eliminating theneed for additional training. It produces ADs that not only comply withestablished natural language AD production standards but also maintaincontextually consistent character information across frames, courtesy of atracking-based character recognition module. A thorough analysis on the MADdataset reveals that our approach achieves a performance on par withlearning-based methods in automated AD production, as substantiated by a CIDErscore of 20.5.</description><author>Peng Chu, Jiang Wang, Andre Abrantes</author><pubDate>Thu, 02 May 2024 04:38:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.00983v1</guid></item><item><title>Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding</title><link>http://arxiv.org/abs/2403.14174v1</link><description>Inspired by the activity-silent and persistent activity mechanisms in humanvisual perception biology, we design a Unified Static and Dynamic Network(UniSDNet), to learn the semantic association between the video and text/audioqueries in a cross-modal environment for efficient video grounding. For staticmodeling, we devise a novel residual structure (ResMLP) to boost the globalcomprehensive interaction between the video segments and queries, achievingmore effective semantic enhancement/supplement. For dynamic modeling, weeffectively exploit three characteristics of the persistent activity mechanismin our network design for a better video context comprehension. Specifically,we construct a diffusely connected video clip graph on the basis of 2D sparsetemporal masking to reflect the "short-term effect" relationship. Weinnovatively consider the temporal distance and relevance as the joint"auxiliary evidence clues" and design a multi-kernel Temporal Gaussian Filterto expand the context clue into high-dimensional space, simulating the "complexvisual perception", and then conduct element level filtering convolutionoperations on neighbour clip nodes in message passing stage for finallygenerating and ranking the candidate proposals. Our UniSDNet is applicable toboth Natural Language Video Grounding (NLVG) and Spoken Language VideoGrounding (SLVG) tasks. Our UniSDNet achieves SOTA performance on three widelyused datasets for NLVG, as well as three datasets for SLVG, e.g., reporting newrecords at 38.88% R@1,IoU@0.7 on ActivityNet Captions and 40.26% R@1,IoU@0.5 onTACoS. To facilitate this field, we collect two new datasets (Charades-STASpeech and TACoS Speech) for SLVG task. Meanwhile, the inference speed of ourUniSDNet is 1.56$\times$ faster than the strong multi-query benchmark. Code isavailable at: https://github.com/xian-sh/UniSDNet.</description><author>Jingjing Hu, Dan Guo, Kun Li, Zhan Si, Xun Yang, Xiaojun Chang, Meng Wang</author><pubDate>Thu, 21 Mar 2024 07:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14174v1</guid></item><item><title>SoccerNet-Echoes: A Soccer Game Audio Commentary Dataset</title><link>http://arxiv.org/abs/2405.07354v1</link><description>The application of Automatic Speech Recognition (ASR) technology in socceroffers numerous opportunities for sports analytics. Specifically, extractingaudio commentaries with ASR provides valuable insights into the events of thegame, and opens the door to several downstream applications such as automatichighlight generation. This paper presents SoccerNet-Echoes, an augmentation ofthe SoccerNet dataset with automatically generated transcriptions of audiocommentaries from soccer game broadcasts, enhancing video content with richlayers of textual information derived from the game audio using ASR. Thesetextual commentaries, generated using the Whisper model and translated withGoogle Translate, extend the usefulness of the SoccerNet dataset in diverseapplications such as enhanced action spotting, automatic caption generation,and game summarization. By incorporating textual data alongside visual andauditory content, SoccerNet-Echoes aims to serve as a comprehensive resourcefor the development of algorithms specialized in capturing the dynamics ofsoccer games. We detail the methods involved in the curation of this datasetand the integration of ASR. We also highlight the implications of a multimodalapproach in sports analytics, and how the enriched dataset can support diverseapplications, thus broadening the scope of research and development in thefield of sports analytics.</description><author>Sushant Gautam, Mehdi Houshmand Sarkhoosh, Jan Held, Cise Midoglu, Anthony Cioppa, Silvio Giancola, Vajira Thambawita, Michael A. Riegler, Pål Halvorsen, Mubarak Shah</author><pubDate>Sun, 12 May 2024 19:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.07354v1</guid></item><item><title>Enhancing Interactive Image Retrieval With Query Rewriting Using Large Language Models and Vision Language Models</title><link>http://arxiv.org/abs/2404.18746v1</link><description>Image search stands as a pivotal task in multimedia and computer vision,finding applications across diverse domains, ranging from internet search tomedical diagnostics. Conventional image search systems operate by acceptingtextual or visual queries, retrieving the top-relevant candidate results fromthe database. However, prevalent methods often rely on single-turn procedures,introducing potential inaccuracies and limited recall. These methods also facethe challenges, such as vocabulary mismatch and the semantic gap, constrainingtheir overall effectiveness. To address these issues, we propose an interactiveimage retrieval system capable of refining queries based on user relevancefeedback in a multi-turn setting. This system incorporates a vision languagemodel (VLM) based image captioner to enhance the quality of text-based queries,resulting in more informative queries with each iteration. Moreover, weintroduce a large language model (LLM) based denoiser to refine text-basedquery expansions, mitigating inaccuracies in image descriptions generated bycaptioning models. To evaluate our system, we curate a new dataset by adaptingthe MSR-VTT video retrieval dataset to the image retrieval task, offeringmultiple relevant ground truth images for each query. Through comprehensiveexperiments, we validate the effectiveness of our proposed system againstbaseline methods, achieving state-of-the-art performance with a notable 10\%improvement in terms of recall. Our contributions encompass the development ofan innovative interactive image retrieval system, the integration of anLLM-based denoiser, the curation of a meticulously designed evaluation dataset,and thorough experimental validation.</description><author>Hongyi Zhu, Jia-Hong Huang, Stevan Rudinac, Evangelos Kanoulas</author><pubDate>Mon, 29 Apr 2024 15:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18746v1</guid></item><item><title>Semi-supervised Text-based Person Search</title><link>http://arxiv.org/abs/2404.18106v1</link><description>Text-based person search (TBPS) aims to retrieve images of a specific personfrom a large image gallery based on a natural language description. Existingmethods rely on massive annotated image-text data to achieve satisfactoryperformance in fully-supervised learning. It poses a significant challenge inpractice, as acquiring person images from surveillance videos is relativelyeasy, while obtaining annotated texts is challenging. The paper undertakes apioneering initiative to explore TBPS under the semi-supervised setting, whereonly a limited number of person images are annotated with textual descriptionswhile the majority of images lack annotations. We present a two-stage basicsolution based on generation-then-retrieval for semi-supervised TBPS. Thegeneration stage enriches annotated data by applying an image captioning modelto generate pseudo-texts for unannotated images. Later, the retrieval stageperforms fully-supervised retrieval learning using the augmented data.Significantly, considering the noise interference of the pseudo-texts onretrieval learning, we propose a noise-robust retrieval framework that enhancesthe ability of the retrieval model to handle noisy data. The frameworkintegrates two key strategies: Hybrid Patch-Channel Masking (PC-Mask) to refinethe model architecture, and Noise-Guided Progressive Training (NP-Train) toenhance the training process. PC-Mask performs masking on the input data atboth the patch-level and the channel-level to prevent overfitting noisysupervision. NP-Train introduces a progressive training schedule based on thenoise level of pseudo-texts to facilitate noise-robust learning. Extensiveexperiments on multiple TBPS benchmarks show that the proposed frameworkachieves promising performance under the semi-supervised setting.</description><author>Daming Gao, Yang Bai, Min Cao, Hao Dou, Mang Ye, Min Zhang</author><pubDate>Sun, 28 Apr 2024 08:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.18106v1</guid></item><item><title>CI w/o TN: Context Injection without Task Name for Procedure Planning</title><link>http://arxiv.org/abs/2402.15579v1</link><description>This paper explores the challenge of procedure planning in instructionalvideos, which involves creating goal-directed plans based on visual start andgoal observations from videos. Previous research has tackled this problem withgradually weaker training supervision, from heavy intermediate visualobservations or language instructions to task class supervision. However, withthe advent of large language models, even given only the task name, thesemodels can produce a detailed plan. In this study, we propose a much weakersetting without task name as supervision, which is not currently solvable byexisting large language models since they require good prompts with sufficientinformation. Specifically, we hypothesize that previous intermediatesupervisions can serve as context information, and we use captions of visualstart and goal observations as a much cheaper form of supervision. Thisapproach greatly reduces the labeling cost since the captions can be easilyobtained by large pre-trained vision-language foundation models. Technically,we apply BLIP to generate captions as supervision to train the context featurewith contrastive learning loss. Afterward, the context feature is fed into thegenerator to aid in plan generation. Our experiments on two datasets withvarying scales demonstrate that our model can achieve comparable performance onmultiple metrics, which validates our hypothesis.</description><author>Xinjie Li</author><pubDate>Fri, 23 Feb 2024 19:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15579v1</guid></item></channel></rss>