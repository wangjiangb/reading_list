<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo retrieval</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 18 Jan 2024 14:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models</title><link>http://arxiv.org/abs/2306.16533v1</link><description>Video retrieval (VR) involves retrieving the ground truth video from thevideo database given a text caption or vice-versa. The two important componentsof compositionality: objects \&amp; attributes and actions are joined using correctsemantics to form a proper text query. These components (objects \&amp; attributes,actions and semantics) each play an important role to help distinguish amongvideos and retrieve the correct ground truth video. However, it is unclear whatis the effect of these components on the video retrieval performance. Wetherefore, conduct a systematic study to evaluate the compositional andsemantic understanding of video retrieval models on standard benchmarks such asMSRVTT, MSVD and DIDEMO. The study is performed on two categories of videoretrieval models: (i) which are pre-trained on video-text pairs and fine-tunedon downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)(ii) which adapt pre-trained image-text representations like CLIP for videoretrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal thatactions and semantics play a minor role compared to objects \&amp; attributes invideo understanding. Moreover, video retrieval models that use pre-trainedimage-text representations (CLIP) have better semantic and compositionalunderstanding as compared to models pre-trained on video-text data.</description><author>Avinash Madasu, Vasudev Lal</author><pubDate>Wed, 28 Jun 2023 21:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16533v1</guid></item><item><title>A Large Cross-Modal Video Retrieval Dataset with Reading Comprehension</title><link>http://arxiv.org/abs/2305.03347v1</link><description>Most existing cross-modal language-to-video retrieval (VR) research focuseson single-modal input from video, i.e., visual representation, while the textis omnipresent in human environments and frequently critical to understandvideo. To study how to retrieve video with both modal inputs, i.e., visual andtext semantic representations, we first introduce a large-scale and cross-modalVideo Retrieval dataset with text reading comprehension, TextVR, which contains42.2k sentence queries for 10.5k videos of 8 scenario domains, i.e., StreetView (indoor), Street View (outdoor), Games, Sports, Driving, Activity, TVShow, and Cooking. The proposed TextVR requires one unified cross-modal modelto recognize and comprehend texts, relate them to the visual context, anddecide what text semantic information is vital for the video retrieval task.Besides, we present a detailed analysis of TextVR compared to the existingdatasets and design a novel multimodal video retrieval baseline for thetext-based video retrieval task. The dataset analysis and extensive experimentsshow that our TextVR benchmark provides many new technical challenges andinsights from previous datasets for the video-and-language community. Theproject website and GitHub repo can be found athttps://sites.google.com/view/loveucvpr23/guest-track andhttps://github.com/callsys/TextVR, respectively.</description><author>Weijia Wu, Yuzhong Zhao, Zhuang Li, Jiahong Li, Hong Zhou, Mike Zheng Shou, Xiang Bai</author><pubDate>Fri, 05 May 2023 09:00:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03347v1</guid></item><item><title>Towards Video Anomaly Retrieval from Video Anomaly Detection: New Benchmarks and Model</title><link>http://arxiv.org/abs/2307.12545v1</link><description>Video anomaly detection (VAD) has been paid increasing attention due to itspotential applications, its current dominant tasks focus on online detectinganomalies% at the frame level, which can be roughly interpreted as the binaryor multiple event classification. However, such a setup that buildsrelationships between complicated anomalous events and single labels, e.g.,``vandalism'', is superficial, since single labels are deficient tocharacterize anomalous events. In reality, users tend to search a specificvideo rather than a series of approximate videos. Therefore, retrievinganomalous events using detailed descriptions is practical and positive but fewresearches focus on this. In this context, we propose a novel task called VideoAnomaly Retrieval (VAR), which aims to pragmatically retrieve relevantanomalous videos by cross-modalities, e.g., language descriptions andsynchronous audios. Unlike the current video retrieval where videos are assumedto be temporally well-trimmed with short duration, VAR is devised to retrievelong untrimmed videos which may be partially relevant to the given query. Toachieve this, we present two large-scale VAR benchmarks, UCFCrime-AR andXDViolence-AR, constructed on top of prevalent anomaly datasets. Meanwhile, wedesign a model called Anomaly-Led Alignment Network (ALAN) for VAR. In ALAN, wepropose an anomaly-led sampling to focus on key segments in long untrimmedvideos. Then, we introduce an efficient pretext task to enhance semanticassociations between video-text fine-grained representations. Besides, weleverage two complementary alignments to further match cross-modal contents.Experimental results on two benchmarks reveal the challenges of VAR task andalso demonstrate the advantages of our tailored method.</description><author>Peng Wu, Jing Liu, Xiangteng He, Yuxin Peng, Peng Wang, Yanning Zhang</author><pubDate>Mon, 24 Jul 2023 07:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12545v1</guid></item><item><title>C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval</title><link>http://arxiv.org/abs/2210.03625v2</link><description>Multilingual text-video retrieval methods have improved significantly inrecent years, but the performance for other languages lags behind English. Wepropose a Cross-Lingual Cross-Modal Knowledge Distillation method to improvemultilingual text-video retrieval. Inspired by the fact that English text-videoretrieval outperforms other languages, we train a student model using inputtext in different languages to match the cross-modal predictions from teachermodels using input text in English. We propose a cross entropy based objectivewhich forces the distribution over the student's text-video similarity scoresto be similar to those of the teacher models. We introduce a new multilingualvideo dataset, Multi-YouCook2, by translating the English captions in theYouCook2 video dataset to 8 other languages. Our method improves multilingualtext-video retrieval performance on Multi-YouCook2 and several other datasetssuch as Multi-MSRVTT and VATEX. We also conducted an analysis on theeffectiveness of different multilingual text models as teachers. The code,models, and dataset are available at https://github.com/roudimit/c2kd.</description><author>Andrew Rouditchenko, Yung-Sung Chuang, Nina Shvetsova, Samuel Thomas, Rogerio Feris, Brian Kingsbury, Leonid Karlinsky, David Harwath, Hilde Kuehne, James Glass</author><pubDate>Tue, 09 May 2023 20:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03625v2</guid></item><item><title>Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment</title><link>http://arxiv.org/abs/2307.12964v1</link><description>Text-to-video retrieval systems have recently made significant progress byutilizing pre-trained models trained on large-scale image-text pairs. However,most of the latest methods primarily focus on the video modality whiledisregarding the audio signal for this task. Nevertheless, a recent advancementby ECLIPSE has improved long-range text-to-video retrieval by developing anaudiovisual video representation. Nonetheless, the objective of thetext-to-video retrieval task is to capture the complementary audio and videoinformation that is pertinent to the text query rather than simply achievingbetter audio and video alignment. To address this issue, we introduce TEFAL, aTExt-conditioned Feature ALignment method that produces both audio and videorepresentations conditioned on the text query. Instead of using only anaudiovisual attention block, which could suppress the audio informationrelevant to the text query, our approach employs two independent cross-modalattention blocks that enable the text to attend to the audio and videorepresentations separately. Our proposed method's efficacy is demonstrated onfour benchmark datasets that include audio: MSR-VTT, LSMDC, VATEX, andCharades, and achieves better than state-of-the-art performance consistentlyacross the four datasets. This is attributed to the additionaltext-query-conditioned audio representation and the complementary informationit adds to the text-query-conditioned video representation.</description><author>Sarah Ibrahimi, Xiaohang Sun, Pichao Wang, Amanmeet Garg, Ashutosh Sanan, Mohamed Omar</author><pubDate>Mon, 24 Jul 2023 18:43:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12964v1</guid></item><item><title>MarineVRS: Marine Video Retrieval System with Explainability via Semantic Understanding</title><link>http://arxiv.org/abs/2306.04593v1</link><description>Building a video retrieval system that is robust and reliable, especially forthe marine environment, is a challenging task due to several factors such asdealing with massive amounts of dense and repetitive data, occlusion,blurriness, low lighting conditions, and abstract queries. To address thesechallenges, we present MarineVRS, a novel and flexible video retrieval systemdesigned explicitly for the marine domain. MarineVRS integratesstate-of-the-art methods for visual and linguistic object representation toenable efficient and accurate search and analysis of vast volumes of underwatervideo data. In addition, unlike the conventional video retrieval system, whichonly permits users to index a collection of images or videos and search using afree-form natural language sentence, our retrieval system includes anadditional Explainability module that outputs the segmentation masks of theobjects that the input query referred to. This feature allows users to identifyand isolate specific objects in the video footage, leading to more detailedanalysis and understanding of their behavior and movements. Finally, with itsadaptability, explainability, accuracy, and scalability, MarineVRS is apowerful tool for marine researchers and scientists to efficiently andaccurately process vast amounts of data and gain deeper insights into thebehavior and movements of marine species.</description><author>Tan-Sang Ha, Hai Nguyen-Truong, Tuan-Anh Vu, Sai-Kit Yeung</author><pubDate>Wed, 07 Jun 2023 17:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04593v1</guid></item><item><title>Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning</title><link>http://arxiv.org/abs/2309.11082v1</link><description>In recent years, the explosion of web videos makes text-video retrievalincreasingly essential and popular for video filtering, recommendation, andsearch. Text-video retrieval aims to rank relevant text/video higher thanirrelevant ones. The core of this task is to precisely measure the cross-modalsimilarity between texts and videos. Recently, contrastive learning methodshave shown promising results for text-video retrieval, most of which focus onthe construction of positive and negative pairs to learn text and videorepresentations. Nevertheless, they do not pay enough attention to hardnegative pairs and lack the ability to model different levels of semanticsimilarity. To address these two issues, this paper improves contrastivelearning using two novel techniques. First, to exploit hard examples for robustdiscriminative power, we propose a novel Dual-Modal Attention-Enhanced Module(DMAE) to mine hard negative pairs from textual and visual clues. By furtherintroducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptivelyidentify all these hard negatives and explicitly highlight their impacts in thetraining loss. Second, our work argues that triplet samples can better modelfine-grained semantic similarity compared to pairwise samples. We therebypresent a new Triplet Partial Margin Contrastive Learning (TPM-CL) module toconstruct partial order triplet samples by automatically generatingfine-grained hard negatives for matched text-video pairs. The proposed TPM-CLdesigns an adaptive token masking strategy with cross-modal interaction tomodel subtle semantic differences. Extensive experiments demonstrate that theproposed approach outperforms existing methods on four widely-used text-videoretrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.</description><author>Chen Jiang, Hong Liu, Xuzheng Yu, Qing Wang, Yuan Cheng, Jia Xu, Zhongyi Liu, Qingpei Guo, Wei Chu, Ming Yang, Yuan Qi</author><pubDate>Wed, 20 Sep 2023 07:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11082v1</guid></item><item><title>MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in Indonesian</title><link>http://arxiv.org/abs/2306.11341v1</link><description>Multimodal learning on video and text data has been receiving growingattention from many researchers in various research tasks, includingtext-to-video retrieval, video-to-text retrieval, and video captioning.Although many algorithms have been proposed for those challenging tasks, mostof them are developed on English language datasets. Despite Indonesian beingone of the most spoken languages in the world, the research progress on themultimodal video-text with Indonesian sentences is still under-explored, likelydue to the absence of the public benchmark dataset. To address this issue, weconstruct the first public Indonesian video-text dataset by translating Englishsentences from the MSVD dataset to Indonesian sentences. Using our dataset, wethen train neural network models which were developed for the Englishvideo-text dataset on three tasks, i.e., text-to-video retrieval, video-to-textretrieval, and video captioning. The recent neural network-based approaches tovideo-text tasks often utilized a feature extractor that is primarilypretrained on an English vision-language dataset. Since the availability of thepretraining resources with Indonesian sentences is relatively limited, theapplicability of those approaches to our dataset is still questionable. Toovercome the lack of pretraining resources, we apply cross-lingual transferlearning by utilizing the feature extractors pretrained on the English dataset,and we then fine-tune the models on our Indonesian dataset. Our experimentalresults show that this approach can help to improve the performance for thethree tasks on all metrics. Finally, we discuss potential future works usingour dataset, inspiring further research in the Indonesian multimodal video-texttasks. We believe that our dataset and our experimental results could providevaluable contributions to the community. Our dataset is available on GitHub.</description><author>Willy Fitra Hendria</author><pubDate>Tue, 20 Jun 2023 08:19:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11341v1</guid></item><item><title>CoVR: Learning Composed Video Retrieval from Web Video Captions</title><link>http://arxiv.org/abs/2308.14746v1</link><description>Composed Image Retrieval (CoIR) has recently gained popularity as a task thatconsiders both text and image queries together, to search for relevant imagesin a database. Most CoIR approaches require manually annotated datasets,comprising image-text-image triplets, where the text describes a modificationfrom the query image to the target image. However, manual curation of CoIRtriplets is expensive and prevents scalability. In this work, we insteadpropose a scalable automatic dataset creation methodology that generatestriplets given video-caption pairs, while also expanding the scope of the taskto include composed video retrieval (CoVR). To this end, we mine paired videoswith a similar caption from a large database, and leverage a large languagemodel to generate the corresponding modification text. Applying thismethodology to the extensive WebVid2M collection, we automatically constructour WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, weintroduce a new benchmark for CoVR with a manually annotated evaluation set,along with baseline results. Our experiments further demonstrate that traininga CoVR model on our dataset effectively transfers to CoIR, leading to improvedstate-of-the-art performance in the zero-shot setup on both the CIRR andFashionIQ benchmarks. Our code, datasets, and models are publicly available athttps://imagine.enpc.fr/~ventural/covr.</description><author>Lucas Ventura, Antoine Yang, Cordelia Schmid, Gül Varol</author><pubDate>Mon, 28 Aug 2023 18:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14746v1</guid></item><item><title>MuMUR : Multilingual Multimodal Universal Retrieval</title><link>http://arxiv.org/abs/2208.11553v6</link><description>Multi-modal retrieval has seen tremendous progress with the development ofvision-language models. However, further improving these models requireadditional labelled data which is a huge manual effort. In this paper, wepropose a framework MuMUR, that utilizes knowledge transfer from a multilingualmodel to boost the performance of multi-modal (image and video) retrieval. Wefirst use state-of-the-art machine translation models to construct pseudoground-truth multilingual visual-text pairs. We then use this data to learn ajoint vision-text representation where English and non-English text queries arerepresented in a common embedding space based on pretrained multilingualmodels. We evaluate our proposed approach on a diverse set of retrievaldatasets: five video retrieval datasets such as MSRVTT, MSVD, DiDeMo, Charadesand MSRVTT multilingual, two image retrieval datasets such as Flickr30k andMulti30k . Experimental results demonstrate that our approach achievesstate-of-the-art results on all video retrieval datasets outperforming previousmodels. Additionally, our framework MuMUR significantly beats othermultilingual video retrieval dataset. We also observe that MuMUR exhibitsstrong performance on image retrieval. This demonstrates the universal abilityof MuMUR to perform retrieval across all visual inputs (image and video) andtext inputs (monolingual and multilingual).</description><author>Avinash Madasu, Estelle Aflalo, Gabriela Ben Melech Stan, Shachar Rosenman, Shao-Yen Tseng, Gedas Bertasius, Vasudev Lal</author><pubDate>Mon, 18 Sep 2023 16:33:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.11553v6</guid></item><item><title>Live Laparoscopic Video Retrieval with Compressed Uncertainty</title><link>http://arxiv.org/abs/2203.04301v2</link><description>Searching through large volumes of medical data to retrieve relevantinformation is a challenging yet crucial task for clinical care. However theprimitive and most common approach to retrieval, involving text in the form ofkeywords, is severely limited when dealing with complex media formats.Content-based retrieval offers a way to overcome this limitation, by using richmedia as the query itself. Surgical video-to-video retrieval in particular is anew and largely unexplored research problem with high clinical value,especially in the real-time case: using real-time video hashing, search can beachieved directly inside of the operating room. Indeed, the process of hashingconverts large data entries into compact binary arrays or hashes, enablinglarge-scale search operations at a very fast rate. However, due to fluctuationsover the course of a video, not all bits in a given hash are equally reliable.In this work, we propose a method capable of mitigating this uncertainty whilemaintaining a light computational footprint. We present superior retrievalresults (3-4 % top 10 mean average precision) on a multi-task evaluationprotocol for surgery, using cholecystectomy phases, bypass phases, and comingfrom an entirely new dataset introduced here, critical events across sixdifferent surgery types. Success on this multi-task benchmark shows thegeneralizability of our approach for surgical video retrieval.</description><author>Tong Yu, Pietro Mascagni, Juan Verde, Jacques Marescaux, Didier Mutter, Nicolas Padoy</author><pubDate>Mon, 12 Jun 2023 13:51:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.04301v2</guid></item><item><title>TeachCLIP: Multi-Grained Teaching for Efficient Text-to-Video Retrieval</title><link>http://arxiv.org/abs/2308.01217v1</link><description>For text-to-video retrieval (T2VR), which aims to retrieve unlabeled videosby ad-hoc textual queries, CLIP-based methods are dominating. Compared toCLIP4Clip which is efficient and compact, the state-of-the-art models tend tocompute video-text similarity by fine-grained cross-modal feature interactionand matching, putting their scalability for large-scale T2VR into doubt. Forefficient T2VR, we propose TeachCLIP with multi-grained teaching to let aCLIP4Clip based student network learn from more advanced yet computationallyheavy models such as X-CLIP, TS2-Net and X-Pool . To improve the student'slearning capability, we add an Attentional frame-Feature Aggregation (AFA)block, which by design adds no extra storage/computation overhead at theretrieval stage. While attentive weights produced by AFA are commonly used forcombining frame-level features, we propose a novel use of the weights to letthem imitate frame-text relevance estimated by the teacher network. As such,AFA provides a fine-grained learning (teaching) channel for the student(teacher). Extensive experiments on multiple public datasets justify theviability of the proposed method.</description><author>Kaibin Tian, Ruixiang Zhao, Hu Hu, Runquan Xie, Fengzong Lian, Zhanhui Kang, Xirong Li</author><pubDate>Wed, 02 Aug 2023 16:22:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.01217v1</guid></item><item><title>An Overview of Challenges in Egocentric Text-Video Retrieval</title><link>http://arxiv.org/abs/2306.04345v1</link><description>Text-video retrieval contains various challenges, including biases comingfrom diverse sources. We highlight some of them supported by illustrations toopen a discussion. Besides, we address one of the biases, frame length bias,with a simple method which brings a very incremental but promising increase. Weconclude with future directions.</description><author>Burak Satar, Hongyuan Zhu, Hanwang Zhang, Joo Hwee Lim</author><pubDate>Wed, 07 Jun 2023 12:20:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04345v1</guid></item><item><title>Large-scale Vision-Language Models Learn Super Images for Efficient and High-Performance Partially Relevant Video Retrieval</title><link>http://arxiv.org/abs/2312.00414v1</link><description>In this paper, we propose an efficient and high-performance method forpartially relevant video retrieval (PRVR), which aims to retrieve untrimmedlong videos that contain at least one relevant moment to the input text query.In terms of both efficiency and performance, the overlooked bottleneck ofprevious studies is the visual encoding of dense frames. This guidesresearchers to choose lightweight visual backbones, yielding sub-optimalretrieval performance due to their limited capabilities of learned visualrepresentations. However, it is undesirable to simply replace them withhigh-performance large-scale vision-and-language models (VLMs) due to their lowefficiency. To address these issues, instead of dense frames, we focus on superimages, which are created by rearranging the video frames in a $N \times N$grid layout. This reduces the number of visual encodings to $\frac{1}{N^2}$ andcompensates for the low efficiency of large-scale VLMs, allowing us to adoptthem as powerful encoders. Surprisingly, we discover that with a simplequery-image attention trick, VLMs generalize well to super images effectivelyand demonstrate promising zero-shot performance against SOTA methodsefficiently. In addition, we propose a fine-tuning approach by incorporating afew trainable modules into the VLM backbones. The experimental resultsdemonstrate that our approaches efficiently achieve the best performance onActivityNet Captions and TVR.</description><author>Taichi Nishimura, Shota Nakada, Masayoshi Kondo</author><pubDate>Fri, 01 Dec 2023 08:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.00414v1</guid></item><item><title>Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval</title><link>http://arxiv.org/abs/2308.07648v1</link><description>In text-video retrieval, recent works have benefited from the powerfullearning capabilities of pre-trained text-image foundation models (e.g., CLIP)by adapting them to the video domain. A critical problem for them is how toeffectively capture the rich semantics inside the video using the image encoderof CLIP. To tackle this, state-of-the-art methods adopt complex cross-modalmodeling techniques to fuse the text information into video framerepresentations, which, however, incurs severe efficiency issues in large-scaleretrieval systems as the video representations must be recomputed online forevery text query. In this paper, we discard this problematic cross-modal fusionprocess and aim to learn semantically-enhanced representations purely from thevideo, so that the video representations can be computed offline and reused fordifferent texts. Concretely, we first introduce a spatial-temporal "PromptCube" into the CLIP image encoder and iteratively switch it within the encoderlayers to efficiently incorporate the global video semantics into framerepresentations. We then propose to apply an auxiliary video captioningobjective to train the frame representations, which facilitates the learning ofdetailed video semantics by providing fine-grained guidance in the semanticspace. With a naive temporal fusion strategy (i.e., mean-pooling) on theenhanced frame representations, we obtain state-of-the-art performances onthree benchmark datasets, i.e., MSR-VTT, MSVD, and LSMDC.</description><author>Chaorui Deng, Qi Chen, Pengda Qin, Da Chen, Qi Wu</author><pubDate>Tue, 15 Aug 2023 09:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07648v1</guid></item><item><title>VVS: Video-to-Video Retrieval with Irrelevant Frame Suppression</title><link>http://arxiv.org/abs/2303.08906v2</link><description>In content-based video retrieval (CBVR), dealing with large-scalecollections, efficiency is as important as accuracy; thus, several video-levelfeature-based studies have actively been conducted. Nevertheless, owing to thesevere difficulty of embedding a lengthy and untrimmed video into a singlefeature, these studies have been insufficient for accurate retrieval comparedto frame-level feature-based studies. In this paper, we show that appropriatesuppression of irrelevant frames can provide insight into the current obstaclesof the video-level approaches. Furthermore, we propose a Video-to-VideoSuppression network (VVS) as a solution. VVS is an end-to-end framework thatconsists of an easy distractor elimination stage to identify which frames toremove and a suppression weight generation stage to determine the extent tosuppress the remaining frames. This structure is intended to effectivelydescribe an untrimmed video with varying content and meaningless information.Its efficacy is proved via extensive experiments, and we show that our approachis not only state-of-the-art in video-level approaches but also has a fastinference time despite possessing retrieval capabilities close to those offrame-level approaches. Code is available at https://github.com/sejong-rcv/VVS</description><author>Won Jo, Geuntaek Lim, Gwangjin Lee, Hyunwoo Kim, Byungsoo Ko, Yukyung Choi</author><pubDate>Tue, 19 Dec 2023 09:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08906v2</guid></item><item><title>Towards Efficient and Effective Text-to-Video Retrieval with Coarse-to-Fine Visual Representation Learning</title><link>http://arxiv.org/abs/2401.00701v1</link><description>In recent years, text-to-video retrieval methods based on CLIP haveexperienced rapid development. The primary direction of evolution is to exploitthe much wider gamut of visual and textual cues to achieve alignment.Concretely, those methods with impressive performance often design a heavyfusion block for sentence (words)-video (frames) interaction, regardless of theprohibitive computation complexity. Nevertheless, these approaches are notoptimal in terms of feature utilization and retrieval efficiency. To addressthis issue, we adopt multi-granularity visual feature learning, ensuring themodel's comprehensiveness in capturing visual content features spanning fromabstract to detailed levels during the training phase. To better leverage themulti-granularity features, we devise a two-stage retrieval architecture in theretrieval phase. This solution ingeniously balances the coarse and finegranularity of retrieval content. Moreover, it also strikes a harmoniousequilibrium between retrieval effectiveness and efficiency. Specifically, intraining phase, we design a parameter-free text-gated interaction block (TIB)for fine-grained video representation learning and embed an extra PearsonConstraint to optimize cross-modal representation learning. In retrieval phase,we use coarse-grained video representations for fast recall of top-kcandidates, which are then reranked by fine-grained video representations.Extensive experiments on four benchmarks demonstrate the efficiency andeffectiveness. Notably, our method achieves comparable performance with thecurrent state-of-the-art methods while being nearly 50 times faster.</description><author>Kaibin Tian, Yanhua Cheng, Yi Liu, Xinglin Hou, Quan Chen, Han Li</author><pubDate>Mon, 01 Jan 2024 08:54:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00701v1</guid></item><item><title>GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient Partially Relevant Video Retrieval</title><link>http://arxiv.org/abs/2310.05195v2</link><description>Given a text query, partially relevant video retrieval (PRVR) seeks to finduntrimmed videos containing pertinent moments in a database. For PRVR, clipmodeling is essential to capture the partial relationship between texts andvideos. Current PRVR methods adopt scanning-based clip construction to achieveexplicit clip modeling, which is information-redundant and requires a largestorage overhead. To solve the efficiency problem of PRVR methods, this paperproposes GMMFormer, a Gaussian-Mixture-Model based Transformer which modelsclip representations implicitly. During frame interactions, we incorporateGaussian-Mixture-Model constraints to focus each frame on its adjacent framesinstead of the whole video. Then generated representations will containmulti-scale clip information, achieving implicit clip modeling. In addition,PRVR methods ignore semantic differences between text queries relevant to thesame video, leading to a sparse embedding space. We propose a query diverseloss to distinguish these text queries, making the embedding space moreintensive and contain more semantic information. Extensive experiments on threelarge-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)demonstrate the superiority and efficiency of GMMFormer. Code is available at\url{https://github.com/huangmozhi9527/GMMFormer}.</description><author>Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia</author><pubDate>Wed, 03 Jan 2024 07:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05195v2</guid></item><item><title>GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient Partially Relevant Video Retrieval</title><link>http://arxiv.org/abs/2310.05195v1</link><description>Given a text query, partially relevant video retrieval (PRVR) seeks to finduntrimmed videos containing pertinent moments in a database. For PRVR, clipmodeling is essential to capture the partial relationship between texts andvideos. Current PRVR methods adopt scanning-based clip construction to achieveexplicit clip modeling, which is information-redundant and requires a largestorage overhead. To solve the efficiency problem of PRVR methods, this paperproposes GMMFormer, a \textbf{G}aussian-\textbf{M}ixture-\textbf{M}odel basedTrans\textbf{former} which models clip representations implicitly. During frameinteractions, we incorporate Gaussian-Mixture-Model constraints to focus eachframe on its adjacent frames instead of the whole video. Then generatedrepresentations will contain multi-scale clip information, achieving implicitclip modeling. In addition, PRVR methods ignore semantic differences betweentext queries relevant to the same video, leading to a sparse embedding space.We propose a query diverse loss to distinguish these text queries, making theembedding space more intensive and contain more semantic information. Extensiveexperiments on three large-scale video datasets (\ie, TVR, ActivityNetCaptions, and Charades-STA) demonstrate the superiority and efficiency ofGMMFormer.</description><author>Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia</author><pubDate>Sun, 08 Oct 2023 16:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05195v1</guid></item><item><title>Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval</title><link>http://arxiv.org/abs/2309.11091v1</link><description>With the explosive growth of web videos in recent years, large-scaleContent-Based Video Retrieval (CBVR) becomes increasingly essential in videofiltering, recommendation, and copyright protection. Segment-level CBVR(S-CBVR) locates the start and end time of similar segments in finergranularity, which is beneficial for user browsing efficiency and infringementdetection especially in long video scenarios. The challenge of S-CBVR task ishow to achieve high temporal alignment accuracy with efficient computation andlow storage consumption. In this paper, we propose a Segment Similarity andAlignment Network (SSAN) in dealing with the challenge which is firstly trainedend-to-end in S-CBVR. SSAN is based on two newly proposed modules in videoretrieval: (1) An efficient Self-supervised Keyframe Extraction (SKE) module toreduce redundant frame features, (2) A robust Similarity Pattern Detection(SPD) module for temporal alignment. In comparison with uniform frameextraction, SKE not only saves feature storage and search time, but alsointroduces comparable accuracy and limited extra computation time. In terms oftemporal alignment, SPD localizes similar segments with higher accuracy andefficiency than existing deep learning methods. Furthermore, we jointly trainSSAN with SKE and SPD and achieve an end-to-end improvement. Meanwhile, the twokey modules SKE and SPD can also be effectively inserted into other videoretrieval pipelines and gain considerable performance improvements.Experimental results on public datasets show that SSAN can obtain higheralignment accuracy while saving storage and online query computational costcompared to existing methods.</description><author>Chen Jiang, Kaiming Huang, Sifeng He, Xudong Yang, Wei Zhang, Xiaobo Zhang, Yuan Cheng, Lei Yang, Qing Wang, Furong Xu, Tan Pan, Wei Chu</author><pubDate>Wed, 20 Sep 2023 07:41:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11091v1</guid></item><item><title>Simple Baselines for Interactive Video Retrieval with Questions and Answers</title><link>http://arxiv.org/abs/2308.10402v1</link><description>To date, the majority of video retrieval systems have been optimized for a"single-shot" scenario in which the user submits a query in isolation, ignoringprevious interactions with the system. Recently, there has been renewedinterest in interactive systems to enhance retrieval, but existing approachesare complex and deliver limited gains in performance. In this work, we revisitthis topic and propose several simple yet effective baselines for interactivevideo retrieval via question-answering. We employ a VideoQA model to simulateuser interactions and show that this enables the productive study of theinteractive retrieval task without access to ground truth dialogue data.Experiments on MSR-VTT, MSVD, and AVSD show that our framework usingquestion-based interaction significantly improves the performance of text-basedvideo retrieval systems.</description><author>Kaiqu Liang, Samuel Albanie</author><pubDate>Mon, 21 Aug 2023 01:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10402v1</guid></item><item><title>SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval</title><link>http://arxiv.org/abs/2310.05241v1</link><description>Video moment retrieval aims to localize moments in video corresponding to agiven language query. To avoid the expensive cost of annotating the temporalmoments, weakly-supervised VMR (wsVMR) systems have been studied. For suchsystems, generating a number of proposals as moment candidates and thenselecting the most appropriate proposal has been a popular approach. Theseproposals are assumed to contain many distinguishable scenes in a video ascandidates. However, existing proposals of wsVMR systems do not respect thevarying numbers of scenes in each video, where the proposals are heuristicallydetermined irrespective of the video. We argue that the retrieval system shouldbe able to counter the complexities caused by varying numbers of scenes in eachvideo. To this end, we present a novel concept of a retrieval system referredto as Scene Complexity Aware Network (SCANet), which measures the `scenecomplexity' of multiple scenes in each video and generates adaptive proposalsresponding to variable complexities of scenes in each video. Experimentalresults on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR)achieve state-of-the-art performances and demonstrate the effectiveness ofincorporating the scene complexity.</description><author>Sunjae Yoon, Gwanhyeong Koo, Dahyun Kim, Chang D. Yoo</author><pubDate>Sun, 08 Oct 2023 18:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05241v1</guid></item><item><title>WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge</title><link>http://arxiv.org/abs/2312.09507v3</link><description>Text-video retrieval, a prominent sub-field within the domain of multimodalinformation retrieval, has witnessed remarkable growth in recent years.However, existing methods assume video scenes are consistent with unbiaseddescriptions. These limitations fail to align with real-world scenarios sincedescriptions can be influenced by annotator biases, diverse writing styles, andvarying textual perspectives. To overcome the aforementioned problems, weintroduce $\texttt{WAVER}$, a cross-domain knowledge distillation framework viavision-language models through open-vocabulary knowledge designed to tackle thechallenge of handling different writing styles in video descriptions.$\texttt{WAVER}$ capitalizes on the open-vocabulary properties that lie inpre-trained vision-language models and employs an implicit knowledgedistillation approach to transfer text-based knowledge from a teacher model toa vision-based student. Empirical studies conducted across four standardbenchmark datasets, encompassing various settings, provide compelling evidencethat $\texttt{WAVER}$ can achieve state-of-the-art performance in text-videoretrieval task while handling writing-style variations. The code is availableat: https://github.com/Fsoft-AIC/WAVER</description><author>Huy Le, Tung Kieu, Anh Nguyen, Ngan Le</author><pubDate>Wed, 10 Jan 2024 21:40:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09507v3</guid></item><item><title>Multi-event Video-Text Retrieval</title><link>http://arxiv.org/abs/2308.11551v1</link><description>Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massivevideo-text data on the Internet. A plethora of work characterized by using atwo-stream Vision-Language model architecture that learns a jointrepresentation of video-text pairs has become a prominent approach for the VTRtask. However, these models operate under the assumption of bijectivevideo-text correspondences and neglect a more practical scenario where videocontent usually encompasses multiple events, while texts like user queries orwebpage metadata tend to be specific and correspond to single events. Thisestablishes a gap between the previous training objective and real-worldapplications, leading to the potential performance degradation of earliermodels during inference. In this study, we introduce the Multi-event Video-TextRetrieval (MeVTR) task, addressing scenarios in which each video containsmultiple different events, as a niche scenario of the conventional Video-TextRetrieval Task. We present a simple model, Me-Retriever, which incorporates keyevent video representation and a new MeVTR loss for the MeVTR task.Comprehensive experiments show that this straightforward framework outperformsother models in the Video-to-Text and Text-to-Video tasks, effectivelyestablishing a robust baseline for the MeVTR task. We believe this work servesas a strong foundation for future studies. Code is available athttps://github.com/gengyuanmax/MeVTR.</description><author>Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</author><pubDate>Tue, 22 Aug 2023 17:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11551v1</guid></item><item><title>Multi-event Video-Text Retrieval</title><link>http://arxiv.org/abs/2308.11551v2</link><description>Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massivevideo-text data on the Internet. A plethora of work characterized by using atwo-stream Vision-Language model architecture that learns a jointrepresentation of video-text pairs has become a prominent approach for the VTRtask. However, these models operate under the assumption of bijectivevideo-text correspondences and neglect a more practical scenario where videocontent usually encompasses multiple events, while texts like user queries orwebpage metadata tend to be specific and correspond to single events. Thisestablishes a gap between the previous training objective and real-worldapplications, leading to the potential performance degradation of earliermodels during inference. In this study, we introduce the Multi-event Video-TextRetrieval (MeVTR) task, addressing scenarios in which each video containsmultiple different events, as a niche scenario of the conventional Video-TextRetrieval Task. We present a simple model, Me-Retriever, which incorporates keyevent video representation and a new MeVTR loss for the MeVTR task.Comprehensive experiments show that this straightforward framework outperformsother models in the Video-to-Text and Text-to-Video tasks, effectivelyestablishing a robust baseline for the MeVTR task. We believe this work servesas a strong foundation for future studies. Code is available athttps://github.com/gengyuanmax/MeVTR.</description><author>Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</author><pubDate>Mon, 25 Sep 2023 14:04:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11551v2</guid></item><item><title>Sinkhorn Transformations for Single-Query Postprocessing in Text-Video Retrieval</title><link>http://arxiv.org/abs/2311.08143v1</link><description>A recent trend in multimodal retrieval is related to postprocessing test setresults via the dual-softmax loss (DSL). While this approach can bringsignificant improvements, it usually presumes that an entire matrix of testsamples is available as DSL input. This work introduces a new postprocessingapproach based on Sinkhorn transformations that outperforms DSL. Further, wepropose a new postprocessing setting that does not require access to multipletest queries. We show that our approach can significantly improve the resultsof state of the art models such as CLIP4Clip, BLIP, X-CLIP, and DRL, thusachieving a new state-of-the-art on several standard text-video retrievaldatasets both with access to the entire test set and in the single-querysetting.</description><author>Konstantin Yakovlev, Gregory Polyakov, Ilseyar Alimova, Alexander Podolskiy, Andrey Bout, Sergey Nikolenko, Irina Piontkovskaya</author><pubDate>Tue, 14 Nov 2023 13:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.08143v1</guid></item><item><title>DiffusionVMR: Diffusion Model for Video Moment Retrieval</title><link>http://arxiv.org/abs/2308.15109v1</link><description>Video moment retrieval is a fundamental visual-language task that aims toretrieve target moments from an untrimmed video based on a language query.Existing methods typically generate numerous proposals manually or viagenerative networks in advance as the support set for retrieval, which is notonly inflexible but also time-consuming. Inspired by the success of diffusionmodels on object detection, this work aims at reformulating video momentretrieval as a denoising generation process to get rid of the inflexible andtime-consuming proposal generation. To this end, we propose a novelproposal-free framework, namely DiffusionVMR, which directly samples randomspans from noise as candidates and introduces denoising learning to groundtarget moments. During training, Gaussian noise is added to the real moments,and the model is trained to learn how to reverse this process. In inference, aset of time spans is progressively refined from the initial noise to the finaloutput. Notably, the training and inference of DiffusionVMR are decoupled, andan arbitrary number of random spans can be used in inference without beingconsistent with the training phase. Extensive experiments conducted on threewidely-used benchmarks (i.e., QVHighlight, Charades-STA, and TACoS) demonstratethe effectiveness of the proposed DiffusionVMR by comparing it withstate-of-the-art methods.</description><author>Henghao Zhao, Kevin Qinghong Lin, Rui Yan, Zechao Li</author><pubDate>Tue, 29 Aug 2023 09:20:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15109v1</guid></item><item><title>Cross-modal Contrastive Learning with Asymmetric Co-attention Network for Video Moment Retrieval</title><link>http://arxiv.org/abs/2312.07435v1</link><description>Video moment retrieval is a challenging task requiring fine-grainedinteractions between video and text modalities. Recent work in image-textpretraining has demonstrated that most existing pretrained models suffer frominformation asymmetry due to the difference in length between visual andtextual sequences. We question whether the same problem also exists in thevideo-text domain with an auxiliary need to preserve both spatial and temporalinformation. Thus, we evaluate a recently proposed solution involving theaddition of an asymmetric co-attention network for video grounding tasks.Additionally, we incorporate momentum contrastive loss for robust,discriminative representation learning in both modalities. We note that theintegration of these supplementary modules yields better performance comparedto state-of-the-art models on the TACoS dataset and comparable results onActivityNet Captions, all while utilizing significantly fewer parameters withrespect to baseline.</description><author>Love Panta, Prashant Shrestha, Brabeem Sapkota, Amrita Bhattarai, Suresh Manandhar, Anand Kumar Sah</author><pubDate>Tue, 12 Dec 2023 17:00:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07435v1</guid></item><item><title>Overcoming Weak Visual-Textual Alignment for Video Moment Retrieval</title><link>http://arxiv.org/abs/2306.02728v1</link><description>Video moment retrieval (VMR) aims to identify the specific moment in anuntrimmed video for a given natural language query. However, this task is proneto suffer the weak visual-textual alignment problem from query ambiguity,potentially limiting further performance gains and generalization capability.Due to the complex multimodal interactions in videos, a query may not fullycover the relevant details of the corresponding moment, and the moment maycontain misaligned and irrelevant frames. To tackle this problem, we propose astraightforward yet effective model, called Background-aware Moment DEtectionTRansformer (BM-DETR). Given a target query and its moment, BM-DETR also takesnegative queries corresponding to different moments. Specifically, our modellearns to predict the target moment from the joint probability of the givenquery and the complement of negative queries for each candidate frame. In thisway, it leverages the surrounding background to consider relative importance,improving moment sensitivity. Extensive experiments on Charades-STA andQVHighlights demonstrate the effectiveness of our model. Moreover, we show thatBM-DETR can perform robustly in three challenging VMR scenarios, such asseveral out-of-distribution test cases, demonstrating superior generalizationability.</description><author>Minjoon Jung, Youwon Jang, Seongho Choi, Joochan Kim, Jin-Hwa Kim, Byoung-Tak Zhang</author><pubDate>Mon, 05 Jun 2023 10:26:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02728v1</guid></item><item><title>Overcoming Weak Visual-Textual Alignment for Video Moment Retrieval</title><link>http://arxiv.org/abs/2306.02728v2</link><description>Video moment retrieval (VMR) identifies a specific moment in an untrimmedvideo for a given natural language query. This task is prone to suffer the weakvisual-textual alignment problem innate in video datasets. Due to theambiguity, a query does not fully cover the relevant details of thecorresponding moment, or the moment may contain misaligned and irrelevantframes, potentially limiting further performance gains. To tackle this problem,we propose a background-aware moment detection transformer (BM-DETR). Our modeladopts a contrastive approach, carefully utilizing the negative queries matchedto other moments in the video. Specifically, our model learns to predict thetarget moment from the joint probability of each frame given the positive queryand the complement of negative queries. This leads to effective use of thesurrounding background, improving moment sensitivity and enhancing overallalignments in videos. Extensive experiments on four benchmarks demonstrate theeffectiveness of our approach.</description><author>Minjoon Jung, Youwon Jang, Seongho Choi, Joochan Kim, Jin-Hwa Kim, Byoung-Tak Zhang</author><pubDate>Mon, 20 Nov 2023 02:22:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02728v2</guid></item><item><title>Towards Balanced Alignment: Modal-Enhanced Semantic Modeling for Video Moment Retrieval</title><link>http://arxiv.org/abs/2312.12155v1</link><description>Video Moment Retrieval (VMR) aims to retrieve temporal segments in untrimmedvideos corresponding to a given language query by constructing cross-modalalignment strategies. However, these existing strategies are often sub-optimalsince they ignore the modality imbalance problem, \textit{i.e.}, the semanticrichness inherent in videos far exceeds that of a given limited-lengthsentence. Therefore, in pursuit of better alignment, a natural idea isenhancing the video modality to filter out query-irrelevant semantics, andenhancing the text modality to capture more segment-relevant knowledge. In thispaper, we introduce Modal-Enhanced Semantic Modeling (MESM), a novel frameworkfor more balanced alignment through enhancing features at two levels. First, weenhance the video modality at the frame-word level through word reconstruction.This strategy emphasizes the portions associated with query words inframe-level features while suppressing irrelevant parts. Therefore, theenhanced video contains less redundant semantics and is more balanced with thetextual modality. Second, we enhance the textual modality at thesegment-sentence level by learning complementary knowledge from contextsentences and ground-truth segments. With the knowledge added to the query, thetextual modality thus maintains more meaningful semantics and is more balancedwith the video modality. By implementing two levels of MESM, the semanticinformation from both modalities is more balanced to align, thereby bridgingthe modality gap. Experiments on three widely used benchmarks, including theout-of-distribution settings, show that the proposed framework achieves a newstart-of-the-art performance with notable generalization ability (e.g., 4.42%and 7.69% average gains of R1@0.7 on Charades-STA and Charades-CG). The codewill be available at https://github.com/lntzm/MESM.</description><author>Zhihang Liu, Jun Li, Hongtao Xie, Pandeng Li, Jiannan Ge, Sun-Ao Liu, Guoqing Jin</author><pubDate>Tue, 19 Dec 2023 13:38:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12155v1</guid></item><item><title>Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation</title><link>http://arxiv.org/abs/2307.06940v1</link><description>Generating videos for visual storytelling can be a tedious and complexprocess that typically requires either live-action filming or graphicsanimation rendering. To bypass these challenges, our key idea is to utilize theabundance of existing video clips and synthesize a coherent storytelling videoby customizing their appearances. We achieve this by developing a frameworkcomprised of two functional modules: (i) Motion Structure Retrieval, whichprovides video candidates with desired scene or motion context described byquery texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generatesplot-aligned videos under the guidance of motion structure and text prompts.For the first module, we leverage an off-the-shelf video retrieval system andextract video depths as motion structure. For the second module, we propose acontrollable video generation model that offers flexible controls overstructure and characters. The videos are synthesized by following thestructural guidance and appearance instruction. To ensure visual consistencyacross clips, we propose an effective concept personalization approach, whichallows the specification of the desired character identities through textprompts. Extensive experiments demonstrate that our approach exhibitssignificant advantages over various existing baselines.</description><author>Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, Qifeng Chen</author><pubDate>Thu, 13 Jul 2023 18:57:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06940v1</guid></item><item><title>Video-adverb retrieval with compositional adverb-action embeddings</title><link>http://arxiv.org/abs/2309.15086v1</link><description>Retrieving adverbs that describe an action in a video poses a crucial steptowards fine-grained video understanding. We propose a framework forvideo-to-adverb retrieval (and vice versa) that aligns video embeddings withtheir matching compositional adverb-action text embedding in a joint embeddingspace. The compositional adverb-action text embedding is learned using aresidual gating mechanism, along with a novel training objective consisting oftriplet losses and a regression target. Our method achieves state-of-the-artperformance on five recent benchmarks for video-adverb retrieval. Furthermore,we introduce dataset splits to benchmark video-adverb retrieval for unseenadverb-action compositions on subsets of the MSR-VTT Adverbs and ActivityNetAdverbs datasets. Our proposed framework outperforms all prior works for thegeneralisation task of retrieving adverbs from videos for unseen adverb-actioncompositions. Code and dataset splits are available athttps://hummelth.github.io/ReGaDa/.</description><author>Thomas Hummel, Otniel-Bogdan Mercea, A. Sophia Koepke, Zeynep Akata</author><pubDate>Tue, 26 Sep 2023 18:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15086v1</guid></item><item><title>Video-Text Retrieval by Supervised Sparse Multi-Grained Learning</title><link>http://arxiv.org/abs/2302.09473v2</link><description>While recent progress in video-text retrieval has been advanced by theexploration of better representation learning, in this paper, we present anovel multi-grained sparse learning framework, S3MA, to learn an aligned sparsespace shared between the video and the text for video-text retrieval. Theshared sparse space is initialized with a finite number of sparse concepts,each of which refers to a number of words. With the text data at hand, we learnand update the shared sparse space in a supervised manner using the proposedsimilarity and alignment losses. Moreover, to enable multi-grained alignment,we incorporate frame representations for better modeling the video modality andcalculating fine-grained and coarse-grained similarities. Benefiting from thelearned shared sparse space and multi-grained similarities, extensiveexperiments on several video-text retrieval benchmarks demonstrate thesuperiority of S3MA over existing methods. Our code is available athttps://github.com/yimuwangcs/Better_Cross_Modal_Retrieval.</description><author>Yimu Wang, Peng Shi</author><pubDate>Tue, 17 Oct 2023 23:01:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09473v2</guid></item><item><title>Differentiable Resolution Compression and Alignment for Efficient Video Classification and Retrieval</title><link>http://arxiv.org/abs/2309.08167v1</link><description>Optimizing video inference efficiency has become increasingly important withthe growing demand for video analysis in various fields. Some existing methodsachieve high efficiency by explicit discard of spatial or temporal information,which poses challenges in fast-changing and fine-grained scenarios. To addressthese issues, we propose an efficient video representation network withDifferentiable Resolution Compression and Alignment mechanism, which compressesnon-essential information in the early stage of the network to reducecomputational costs while maintaining consistent temporal correlations.Specifically, we leverage a Differentiable Context-aware Compression Module toencode the saliency and non-saliency frame features, refining and updating thefeatures into a high-low resolution video sequence. To process the newsequence, we introduce a new Resolution-Align Transformer Layer to captureglobal temporal correlations among frame features with different resolutions,while reducing spatial computation costs quadratically by utilizing fewerspatial tokens in low-resolution non-saliency frames. The entire network can beend-to-end optimized via the integration of the differentiable compressionmodule. Experimental results show that our method achieves the best trade-offbetween efficiency and performance on near-duplicate video retrieval andcompetitive results on dynamic video classification compared tostate-of-the-art methods. Code:https://github.com/dun-research/DRCA</description><author>Rui Deng, Qian Wu, Yuke Li, Haoran Fu</author><pubDate>Fri, 15 Sep 2023 06:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08167v1</guid></item><item><title>Exploiting Semantic Role Contextualized Video Features for Multi-Instance Text-Video Retrieval EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022</title><link>http://arxiv.org/abs/2206.14381v2</link><description>In this report, we present our approach for EPIC-KITCHENS-100 Multi-InstanceRetrieval Challenge 2022. We first parse sentences into semantic rolescorresponding to verbs and nouns; then utilize self-attentions to exploitsemantic role contextualized video features along with textual features viatriplet losses in multiple embedding spaces. Our method overpasses the strongbaseline in normalized Discounted Cumulative Gain (nDCG), which is morevaluable for semantic similarity. Our submission is ranked 3rd for nDCG andranked 4th for mAP.</description><author>Burak Satar, Hongyuan Zhu, Hanwang Zhang, Joo Hwee Lim</author><pubDate>Tue, 26 Sep 2023 15:27:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.14381v2</guid></item><item><title>Faster Video Moment Retrieval with Point-Level Supervision</title><link>http://arxiv.org/abs/2305.14017v1</link><description>Video Moment Retrieval (VMR) aims at retrieving the most relevant events froman untrimmed video with natural language queries. Existing VMR methods sufferfrom two defects: (1) massive expensive temporal annotations are required toobtain satisfying performance; (2) complicated cross-modal interaction modulesare deployed, which lead to high computational cost and low efficiency for theretrieval process. To address these issues, we propose a novel method termedCheaper and Faster Moment Retrieval (CFMR), which well balances the retrievalaccuracy, efficiency, and annotation cost for VMR. Specifically, our proposedCFMR method learns from point-level supervision where each annotation is asingle frame randomly located within the target moment. It is 6 times cheaperthan the conventional annotations of event boundaries. Furthermore, we alsodesign a concept-based multimodal alignment mechanism to bypass the usage ofcross-modal interaction modules during the inference process, remarkablyimproving retrieval efficiency. The experimental results on three widely usedVMR benchmarks demonstrate the proposed CFMR method establishes newstate-of-the-art with point-level supervision. Moreover, it significantlyaccelerates the retrieval speed with more than 100 times FLOPs compared toexisting approaches with point-level supervision.</description><author>Xun Jiang, Zailei Zhou, Xing Xu, Yang Yang, Guoqing Wang, Heng Tao Shen</author><pubDate>Tue, 23 May 2023 13:53:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14017v1</guid></item><item><title>MomentDiff: Generative Video Moment Retrieval from Random to Real</title><link>http://arxiv.org/abs/2307.02869v1</link><description>Video moment retrieval pursues an efficient and generalized solution toidentify the specific temporal segments within an untrimmed video thatcorrespond to a given language description. To achieve this goal, we provide agenerative diffusion-based framework called MomentDiff, which simulates atypical human retrieval process from random browsing to gradual localization.Specifically, we first diffuse the real span to random noise, and learn todenoise the random noise to the original span with the guidance of similaritybetween text and video. This allows the model to learn a mapping from arbitraryrandom locations to real moments, enabling the ability to locate segments fromrandom initialization. Once trained, MomentDiff could sample random temporalsegments as initial guesses and iteratively refine them to generate an accuratetemporal boundary. Different from discriminative works (e.g., based onlearnable proposals or queries), MomentDiff with random initialized spans couldresist the temporal location biases from datasets. To evaluate the influence ofthe temporal location biases, we propose two anti-bias datasets with locationdistribution shifts, named Charades-STA-Len and Charades-STA-Mom. Theexperimental results demonstrate that our efficient framework consistentlyoutperforms state-of-the-art methods on three public benchmarks, and exhibitsbetter generalization and robustness on the proposed anti-bias datasets. Thecode, model, and anti-bias evaluation datasets are available athttps://github.com/IMCCretrieval/MomentDiff.</description><author>Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, Lei Zhang, Yun Zheng, Deli Zhao, Yongdong Zhang</author><pubDate>Thu, 06 Jul 2023 10:12:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02869v1</guid></item><item><title>MomentDiff: Generative Video Moment Retrieval from Random to Real</title><link>http://arxiv.org/abs/2307.02869v2</link><description>Video moment retrieval pursues an efficient and generalized solution toidentify the specific temporal segments within an untrimmed video thatcorrespond to a given language description. To achieve this goal, we provide agenerative diffusion-based framework called MomentDiff, which simulates atypical human retrieval process from random browsing to gradual localization.Specifically, we first diffuse the real span to random noise, and learn todenoise the random noise to the original span with the guidance of similaritybetween text and video. This allows the model to learn a mapping from arbitraryrandom locations to real moments, enabling the ability to locate segments fromrandom initialization. Once trained, MomentDiff could sample random temporalsegments as initial guesses and iteratively refine them to generate an accuratetemporal boundary. Different from discriminative works (e.g., based onlearnable proposals or queries), MomentDiff with random initialized spans couldresist the temporal location biases from datasets. To evaluate the influence ofthe temporal location biases, we propose two anti-bias datasets with locationdistribution shifts, named Charades-STA-Len and Charades-STA-Mom. Theexperimental results demonstrate that our efficient framework consistentlyoutperforms state-of-the-art methods on three public benchmarks, and exhibitsbetter generalization and robustness on the proposed anti-bias datasets. Thecode, model, and anti-bias evaluation datasets are available athttps://github.com/IMCCretrieval/MomentDiff.</description><author>Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, Lei Zhang, Yun Zheng, Deli Zhao, Yongdong Zhang</author><pubDate>Wed, 11 Oct 2023 11:03:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02869v2</guid></item><item><title>Leveraging Generative Language Models for Weakly Supervised Sentence Component Analysis in Video-Language Joint Learning</title><link>http://arxiv.org/abs/2312.06699v1</link><description>A thorough comprehension of textual data is a fundamental element inmulti-modal video analysis tasks. However, recent works have shown that thecurrent models do not achieve a comprehensive understanding of the textual dataduring the training for the target downstream tasks. Orthogonal to the previousapproaches to this limitation, we postulate that understanding the significanceof the sentence components according to the target task can potentially enhancethe performance of the models. Hence, we utilize the knowledge of a pre-trainedlarge language model (LLM) to generate text samples from the original ones,targeting specific sentence components. We propose a weakly supervisedimportance estimation module to compute the relative importance of thecomponents and utilize them to improve different video-language tasks. Throughrigorous quantitative analysis, our proposed method exhibits significantimprovement across several video-language tasks. In particular, our approachnotably enhances video-text retrieval by a relative improvement of 8.3\% invideo-to-text and 1.4\% in text-to-video retrieval over the baselines, in termsof R@1. Additionally, in video moment retrieval, average mAP shows a relativeimprovement ranging from 2.0\% to 13.7 \% across different baselines.</description><author>Zaber Ibn Abdul Hakim, Najibul Haque Sarker, Rahul Pratap Singh, Bishmoy Paul, Ali Dabouei, Min Xu</author><pubDate>Sun, 10 Dec 2023 02:03:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06699v1</guid></item><item><title>Analyzing Zero-Shot Abilities of Vision-Language Models on Video Understanding Tasks</title><link>http://arxiv.org/abs/2310.04914v1</link><description>Foundational multimodal models pre-trained on large scale image-text pairs orvideo-text pairs or both have shown strong generalization abilities ondownstream tasks. However unlike image-text models, pretraining video-textmodels is always not feasible due to the difficulty in collecting large-scaleclean and aligned data, and exponential computational costs involved in thepretraining phase. Therefore, the pertinent question to ask is: Can image-textmodels be adapted to video tasks and is there any benefit to using these modelsover pretraining directly on videos? In this work, we focus on this question byproposing a detailed study on the generalization abilities of image-text modelswhen evaluated on video understanding tasks in a zero-shot setting. Weinvestigate 9 foundational image-text models on a diverse set of video tasksthat include video action recognition (video AR), video retrieval (video RT),video question answering (video QA), video multiple choice (video MC) and videocaptioning (video CP). Our experiments show that image-text models exhibitimpressive performance on video AR, video RT and video MC. Furthermore, theyperform moderately on video captioning and poorly on video QA. These findingsshed a light on the benefits of adapting foundational image-text models to anarray of video tasks while avoiding the costly pretraining step.</description><author>Avinash Madasu, Anahita Bhiwandiwalla, Vasudev Lal</author><pubDate>Sat, 07 Oct 2023 21:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04914v1</guid></item><item><title>TVPR: Text-to-Video Person Retrieval and a New Benchmark</title><link>http://arxiv.org/abs/2307.07184v1</link><description>Most existing methods for text-based person retrieval focus on text-to-imageperson retrieval. Nevertheless, due to the lack of dynamic information providedby isolated frames, the performance is hampered when the person is obscured inisolated frames or variable motion details are given in the textualdescription. In this paper, we propose a new task called Text-to-Video PersonRetrieval(TVPR) which aims to effectively overcome the limitations of isolatedframes. Since there is no dataset or benchmark that describes person videoswith natural language, we construct a large-scale cross-modal person videodataset containing detailed natural language annotations, such as person'sappearance, actions and interactions with environment, etc., termed asText-to-Video Person Re-identification (TVPReid) dataset, which will bepublicly available. To this end, a Text-to-Video Person Retrieval Network(TVPRN) is proposed. Specifically, TVPRN acquires video representations byfusing visual and motion representations of person videos, which can deal withtemporal occlusion and the absence of variable motion details in isolatedframes. Meanwhile, we employ the pre-trained BERT to obtain captionrepresentations and the relationship between caption and video representationsto reveal the most relevant person videos. To evaluate the effectiveness of theproposed TVPRN, extensive experiments have been conducted on TVPReid dataset.To the best of our knowledge, TVPRN is the first successful attempt to usevideo for text-based person retrieval task and has achieved state-of-the-artperformance on TVPReid dataset. The TVPReid dataset will be publicly availableto benefit future research.</description><author>Fan Ni, Xu Zhang, Jianhui Wu, Guan-Nan Dong, Aichun Zhu, Hui Liu, Yue Zhang</author><pubDate>Fri, 14 Jul 2023 07:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07184v1</guid></item><item><title>Mask to reconstruct: Cooperative Semantics Completion for Video-text Retrieval</title><link>http://arxiv.org/abs/2305.07910v1</link><description>Recently, masked video modeling has been widely explored and significantlyimproved the model's understanding ability of visual regions at a local level.However, existing methods usually adopt random masking and follow the samereconstruction paradigm to complete the masked regions, which do not leveragethe correlations between cross-modal content. In this paper, we present Maskfor Semantics Completion (MASCOT) based on semantic-based masked modeling.Specifically, after applying attention-based video masking to generatehigh-informed and low-informed masks, we propose Informed Semantics Completionto recover masked semantics information. The recovery mechanism is achieved byaligning the masked content with the unmasked visual regions and correspondingtextual context, which makes the model capture more text-related details at apatch level. Additionally, we shift the emphasis of reconstruction fromirrelevant backgrounds to discriminative parts to ignore regions withlow-informed masks. Furthermore, we design dual-mask co-learning to incorporatevideo cues under different masks and learn more aligned video representation.Our MASCOT performs state-of-the-art performance on four major text-videoretrieval benchmarks, including MSR-VTT, LSMDC, ActivityNet, and DiDeMo.Extensive ablation studies demonstrate the effectiveness of the proposedschemes.</description><author>Han Fang, Zhifei Yang, Xianghao Zang, Chao Ban, Hao Sun</author><pubDate>Sat, 13 May 2023 13:31:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07910v1</guid></item><item><title>Retrieval-Augmented Egocentric Video Captioning</title><link>http://arxiv.org/abs/2401.00789v2</link><description>Understanding human actions from videos of first-person view posessignificant challenges. Most prior approaches explore representation learningon egocentric videos only, while overlooking the potential benefit ofexploiting existing large-scale third-person videos. In this paper, (1) wedevelop EgoInstructor, a retrieval-augmented multimodal captioning model thatautomatically retrieves semantically relevant third-person instructional videosto enhance the video captioning of egocentric videos. (2) For training thecross-view retrieval module, we devise an automatic pipeline to discoverego-exo video pairs from distinct large-scale egocentric and exocentricdatasets. (3) We train the cross-view retrieval module with a novel EgoExoNCEloss that pulls egocentric and exocentric video features closer by aligningthem to shared text features that describe similar actions. (4) Throughextensive experiments, our cross-view retrieval module demonstrates superiorperformance across seven benchmarks. Regarding egocentric video captioning,EgoInstructor exhibits significant improvements by leveraging third-personvideos as references.</description><author>Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie</author><pubDate>Wed, 03 Jan 2024 05:08:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00789v2</guid></item><item><title>VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending</title><link>http://arxiv.org/abs/2305.13167v1</link><description>Large-scale image-text contrastive pre-training models, such as CLIP, havebeen demonstrated to effectively learn high-quality multimodal representations.However, there is limited research on learning video-text representations forgeneral video multimodal tasks based on these powerful features. Towards thisgoal, we propose a novel video-text pre-training method dubbed VLAB: VideoLanguage pre-training by feature Adapting and Blending, which transfers CLIPrepresentations to video pre-training tasks and develops unified videomultimodal models for a wide range of video-text tasks. Specifically, VLAB isfounded on two key strategies: feature adapting and feature blending. In theformer, we introduce a new video adapter module to address CLIP's deficiency inmodeling temporal information and extend the model's capability to encompassboth contrastive and generative tasks. In the latter, we propose an end-to-endtraining method that further enhances the model's performance by exploiting thecomplementarity of image and video features. We validate the effectiveness andversatility of VLAB through extensive experiments on highly competitive videomultimodal tasks, including video text retrieval, video captioning, and videoquestion answering. Remarkably, VLAB outperforms competing methodssignificantly and sets new records in video question answering on MSRVTT, MSVD,and TGIF datasets. It achieves an accuracy of 49.6, 61.0, and 79.0,respectively. Codes and models will be released.</description><author>Xingjian He, Sihan Chen, Fan Ma, Zhicheng Huang, Xiaojie Jin, Zikang Liu, Dongmei Fu, Yi Yang, Jing Liu, Jiashi Feng</author><pubDate>Mon, 22 May 2023 16:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13167v1</guid></item><item><title>UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory</title><link>http://arxiv.org/abs/2308.14316v1</link><description>Fine-tuning pre-trained models has emerged as a powerful technique innumerous domains, owing to its ability to leverage enormous pre-existingknowledge and achieve remarkable performance on downstream tasks. However,updating the parameters of entire networks is computationally intensive.Although state-of-the-art parameter-efficient transfer learning (PETL) methodssignificantly reduce the trainable parameters and storage demand, almost all ofthem still need to back-propagate the gradients through large pre-trainednetworks. This memory-extensive characteristic extremely limits theapplicability of PETL methods in real-world scenarios. To this end, we proposea new memory-efficient PETL strategy, dubbed Universal Parallel Tuning (UniPT).Specifically, we facilitate the transfer process via a lightweight learnableparallel network, which consists of two modules: 1) A parallel interactionmodule that decouples the inherently sequential connections and processes theintermediate activations detachedly of the pre-trained network. 2) A confidenceaggregation module that learns optimal strategies adaptively for integratingcross-layer features. We evaluate UniPT with different backbones (e.g.,VSE$\infty$, CLIP4Clip, Clip-ViL, and MDETR) on five challengingvision-and-language tasks (i.e., image-text retrieval, video-text retrieval,visual question answering, compositional question answering, and visualgrounding). Extensive ablations on ten datasets have validated that our UniPTcan not only dramatically reduce memory consumption and outperform the bestmemory-efficient competitor, but also achieve higher performance than existingPETL methods in a low-memory scenario on different architectures. Our code ispublicly available at: https://github.com/Paranioar/UniPT.</description><author>Haiwen Diao, Bo Wan, Ying Zhang, Xu Jia, Huchuan Lu, Long Chen</author><pubDate>Mon, 28 Aug 2023 06:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14316v1</guid></item><item><title>Retrieval-based Video Language Model for Efficient Long Video Question Answering</title><link>http://arxiv.org/abs/2312.04931v1</link><description>The remarkable natural language understanding, reasoning, and generationcapabilities of large language models (LLMs) have made them attractive forapplication to video question answering (Video QA) tasks, utilizing videotokens as contextual input. However, employing LLMs for long videounderstanding presents significant challenges and remains under-explored. Theextensive number of video tokens leads to considerable computational costs forLLMs while using aggregated tokens results in loss of vision details. Moreover,the presence of abundant question-irrelevant tokens introduces noise to thevideo QA process. To address these issues, we introduce a simple yet effectiveretrieval-based video language model (R-VLM) for efficient and interpretablelong video QA. Specifically, given a question (query) and a long video, ourmodel identifies and selects the most relevant $K$ video chunks and uses theirassociated visual tokens to serve as context for the LLM inference. Thiseffectively reduces the number of video tokens, eliminates noise interference,and enhances system performance. Our experimental results validate theeffectiveness of our framework for comprehending long videos. Furthermore,based on the retrieved chunks, our model is interpretable that provides thejustifications on where we get the answers.</description><author>Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu</author><pubDate>Fri, 08 Dec 2023 09:48:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04931v1</guid></item><item><title>Marking anything: application of point cloud in extracting video target features</title><link>http://arxiv.org/abs/2306.07559v1</link><description>Extracting retrievable features from video is of great significance forstructured video database construction, video copyright protection and fakevideo rumor refutation. Inspired by point cloud data processing, this paperproposes a method for marking anything (MA) in the video, which can extract thecontour features of any target in the video and convert it into a featurevector with a length of 256 that can be retrieved. The algorithm uses YOLO-v8algorithm, multi-object tracking algorithm and PointNet++ to extract contour ofthe video detection target to form spatial point cloud data. Then extract thepoint cloud feature vector and use it as the retrievable feature of the videodetection target. In order to verify the effectiveness and robustness ofcontour feature, some datasets are crawled from Dou Yin and Kinetics-700dataset as experimental data. For Dou Yin's homogenized videos, the proposedcontour features achieve retrieval accuracy higher than 97% in Top1 returnmode. For videos from Kinetics 700, the contour feature also showed goodrobustness for partial clip mode video tracing.</description><author>Xiangchun Xu</author><pubDate>Tue, 13 Jun 2023 07:16:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07559v1</guid></item><item><title>Bridging the Gap: A Unified Video Comprehension Framework for Moment Retrieval and Highlight Detection</title><link>http://arxiv.org/abs/2311.16464v1</link><description>Video Moment Retrieval (MR) and Highlight Detection (HD) have attractedsignificant attention due to the growing demand for video analysis. Recentapproaches treat MR and HD as similar video grounding problems and address themtogether with transformer-based architecture. However, we observe that theemphasis of MR and HD differs, with one necessitating the perception of localrelationships and the other prioritizing the understanding of global contexts.Consequently, the lack of task-specific design will inevitably lead tolimitations in associating the intrinsic specialty of two tasks. To tackle theissue, we propose a Unified Video COMprehension framework (UVCOM) to bridge thegap and jointly solve MR and HD effectively. By performing progressiveintegration on intra and inter-modality across multi-granularity, UVCOMachieves the comprehensive understanding in processing a video. Moreover, wepresent multi-aspect contrastive learning to consolidate the local relationmodeling and global knowledge accumulation via well aligned multi-modal space.Extensive experiments on QVHighlights, Charades-STA, TACoS , YouTube Highlightsand TVSum datasets demonstrate the effectiveness and rationality of UVCOM whichoutperforms the state-of-the-art methods by a remarkable margin.</description><author>Yicheng Xiao, Zhuoyan Luo, Yong Liu, Yue Ma, Hengwei Bian, Yatai Ji, Yujiu Yang, Xiu Li</author><pubDate>Tue, 28 Nov 2023 03:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16464v1</guid></item><item><title>TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding</title><link>http://arxiv.org/abs/2310.19060v1</link><description>Large-scale video-language pre-training has made remarkable strides inadvancing video-language understanding tasks. However, the heavy computationalburden of video encoding remains a formidable efficiency bottleneck,particularly for long-form videos. These videos contain massive visual tokensdue to their inherent 3D properties and spatiotemporal redundancy, making itchallenging to capture complex temporal and spatial relationships. To tacklethis issue, we propose an efficient method called TEmporal-Spatial TokenAggregation (TESTA). TESTA condenses video semantics by adaptively aggregatingsimilar frames, as well as similar patches within each frame. TESTA can reducethe number of visual tokens by 75% and thus accelerate video encoding. Buildingupon TESTA, we introduce a pre-trained video-language model equipped with adivided space-time token aggregation module in each video encoder block. Weevaluate our model on five datasets for paragraph-to-video retrieval andlong-form VideoQA tasks. Experimental results show that TESTA improvescomputing efficiency by 1.7 times, and achieves significant performance gainsfrom its scalability in processing longer input frames, e.g., +13.7 R@1 onQuerYD and +6.5 R@1 on Condensed Movie.</description><author>Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, Lu Hou</author><pubDate>Sun, 29 Oct 2023 17:25:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.19060v1</guid></item><item><title>UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling</title><link>http://arxiv.org/abs/2302.06605v2</link><description>Large-scale vision-language pre-trained models have shown promisingtransferability to various downstream tasks. As the size of these foundationmodels and the number of downstream tasks grow, the standard full fine-tuningparadigm becomes unsustainable due to heavy computational and storage costs.This paper proposes UniAdapter, which unifies unimodal and multimodal adaptersfor parameter-efficient cross-modal adaptation on pre-trained vision-languagemodels. Specifically, adapters are distributed to different modalities andtheir interactions, with the total number of tunable parameters reduced bypartial weight sharing. The unified and knowledge-sharing design enablespowerful cross-modal representations that can benefit various downstream tasks,requiring only 1.0%-2.0% tunable parameters of the pre-trained model. Extensiveexperiments on 6 cross-modal downstream benchmarks (including video-textretrieval, image-text retrieval, VideoQA, and VQA) show that in most cases,UniAdapter not only outperforms the state-of-the-arts, but even beats the fullfine-tuning strategy. Particularly, on the MSRVTT retrieval task, UniAdapterachieves 49.7% recall@1 with 2.2% model parameters, outperforming the latestcompetitors by 2.0%. The code and models are available athttps://github.com/RERV/UniAdapter.</description><author>Haoyu Lu, Yuqi Huo, Guoxing Yang, Zhiwu Lu, Wei Zhan, Masayoshi Tomizuka, Mingyu Ding</author><pubDate>Sun, 21 May 2023 18:50:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06605v2</guid></item><item><title>Detours for Navigating Instructional Videos</title><link>http://arxiv.org/abs/2401.01823v1</link><description>We introduce the video detours problem for navigating instructional videos.Given a source video and a natural language query asking to alter the how-tovideo's current path of execution in a certain way, the goal is to find arelated ''detour video'' that satisfies the requested alteration. To addressthis challenge, we propose VidDetours, a novel video-language approach thatlearns to retrieve the targeted temporal segments from a large repository ofhow-to's using video-and-text conditioned queries. Furthermore, we devise alanguage-based pipeline that exploits how-to video narration text to createweakly supervised training data. We demonstrate our idea applied to the domainof how-to cooking videos, where a user can detour from their current recipe tofind steps with alternate ingredients, tools, and techniques. Validating on aground truth annotated dataset of 16K samples, we show our model's significantimprovements over best available methods for video retrieval and questionanswering, with recall rates exceeding the state of the art by 35%.</description><author>Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, Kristen Grauman</author><pubDate>Wed, 03 Jan 2024 16:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01823v1</guid></item><item><title>Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries</title><link>http://arxiv.org/abs/2308.09089v1</link><description>Finding the right sound effects (SFX) to match moments in a video is adifficult and time-consuming task, and relies heavily on the quality andcompleteness of text metadata. Retrieving high-quality (HQ) SFX using a videoframe directly as the query is an attractive alternative, removing the relianceon text metadata and providing a low barrier to entry for non-experts. Due tothe lack of HQ audio-visual training data, previous work on audio-visualretrieval relies on YouTube (in-the-wild) videos of varied quality fortraining, where the audio is often noisy and the video of amateur quality. Assuch it is unclear whether these systems would generalize to the task ofmatching HQ audio to production-quality video. To address this, we propose amultimodal framework for recommending HQ SFX given a video frame by (1)leveraging large language models and foundational vision-language models tobridge HQ audio and video to create audio-visual pairs, resulting in a highlyscalable automatic audio-visual data curation pipeline; and (2) usingpre-trained audio and visual encoders to train a contrastive learning-basedretrieval system. We show that our system, trained using our automatic datacuration pipeline, significantly outperforms baselines trained on in-the-wilddata on the task of HQ SFX retrieval for video. Furthermore, while thebaselines fail to generalize to this task, our system generalizes well fromclean to in-the-wild data, outperforming the baselines on a dataset of YouTubevideos despite only being trained on the HQ audio-visual pairs. A user studyconfirms that people prefer SFX retrieved by our system over the baseline 67%of the time both for HQ and in-the-wild data. Finally, we present ablations todetermine the impact of model and data pipeline design choices on downstreamretrieval performance. Please visit our project website to listen to and viewour SFX retrieval results.</description><author>Julia Wilkins, Justin Salamon, Magdalena Fuentes, Juan Pablo Bello, Oriol Nieto</author><pubDate>Thu, 17 Aug 2023 17:38:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09089v1</guid></item><item><title>Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures</title><link>http://arxiv.org/abs/2307.15220v2</link><description>Recent advancements in surgical computer vision applications have been drivenby fully-supervised methods, primarily using only visual data. These methodsrely on manually annotated surgical videos to predict a fixed set of objectcategories, limiting their generalizability to unseen surgical procedures anddownstream tasks. In this work, we put forward the idea that the surgical videolectures available through open surgical e-learning platforms can provideeffective supervisory signals for multi-modal representation learning withoutrelying on manual annotations. We address the surgery-specific linguisticchallenges present in surgical video lectures by employing multiplecomplementary automatic speech recognition systems to generate texttranscriptions. We then present a novel method, SurgVLP - Surgical VisionLanguage Pre-training, for multi-modal representation learning. SurgVLPconstructs a new contrastive learning objective to align video clip embeddingswith the corresponding multiple text embeddings by bringing them togetherwithin a joint latent space. To effectively show the representation capabilityof the learned joint latent space, we introduce several vision-and-languagetasks for surgery, such as text-based video retrieval, temporal activitygrounding, and video captioning, as benchmarks for evaluation. We furtherdemonstrate that without using any labeled ground truth, our approach can beemployed for traditional vision-only surgical downstream tasks, such assurgical tool, phase, and triplet recognition. The code will be made availableat https://github.com/CAMMA-public/SurgVLP</description><author>Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Pietro Mascagni, Nassir Navab, Nicolas Padoy</author><pubDate>Sat, 13 Jan 2024 13:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15220v2</guid></item><item><title>RGNet: A Unified Retrieval and Grounding Network for Long Videos</title><link>http://arxiv.org/abs/2312.06729v1</link><description>We present a novel end-to-end method for long-form video temporal groundingto locate specific moments described by natural language queries. Priorlong-video methods for this task typically contain two stages: proposalselection and grounding regression. However, the proposal selection of thesemethods is disjoint from the grounding network and is not trained end-to-end,which limits the effectiveness of these methods. Moreover, these methodsoperate uniformly over the entire temporal window, which is suboptimal givenredundant and irrelevant features in long videos. In contrast to these priorapproaches, we introduce RGNet, a unified network designed for jointlyselecting proposals from hour-long videos and locating moments specified bynatural language queries within them. To achieve this, we redefine proposalselection as a video-text retrieval task, i.e., retrieving the correctcandidate videos given a text query. The core component of RGNet is a unifiedcross-modal RG-Encoder that bridges the two stages with shared features andmutual optimization. The encoder strategically focuses on relevant time framesusing a sparse sampling technique. RGNet outperforms previous methods,demonstrating state-of-the-art performance on long video temporal groundingdatasets MAD and Ego4D. The code is released athttps://github.com/Tanveer81/RGNet</description><author>Tanveer Hannan, Md Mohaiminul Islam, Thomas Seidl, Gedas Bertasius</author><pubDate>Mon, 11 Dec 2023 09:12:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06729v1</guid></item><item><title>Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures</title><link>http://arxiv.org/abs/2307.15220v1</link><description>Recent advancements in surgical computer vision applications have been drivenby fully-supervised methods, primarily using only visual data. These methodsrely on manually annotated surgical videos to predict a fixed set of objectcategories, limiting their generalizability to unseen surgical procedures anddownstream tasks. In this work, we put forward the idea that the surgical videolectures available through open surgical e-learning platforms can provideeffective supervisory signals for multi-modal representation learning withoutrelying on manual annotations. We address the surgery-specific linguisticchallenges present in surgical video lectures by employing multiplecomplementary automatic speech recognition systems to generate texttranscriptions. We then present a novel method, SurgVLP - Surgical VisionLanguage Pre-training, for multi-modal representation learning. SurgVLPconstructs a new contrastive learning objective to align video clip embeddingswith the corresponding multiple text embeddings by bringing them togetherwithin a joint latent space. To effectively show the representation capabilityof the learned joint latent space, we introduce several vision-and-languagetasks for surgery, such as text-based video retrieval, temporal activitygrounding, and video captioning, as benchmarks for evaluation. We furtherdemonstrate that without using any labeled ground truth, our approach can beemployed for traditional vision-only surgical downstream tasks, such assurgical tool, phase, and triplet recognition. The code will be made availableat https://github.com/CAMMA-public/SurgVLP</description><author>Kun Yuan, Vinkle Srivastav, Tong Yu, Joel Lavanchy, Pietro Mascagni, Nassir Navab, Nicolas Padoy</author><pubDate>Thu, 27 Jul 2023 23:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15220v1</guid></item><item><title>A Survey on Video Moment Localization</title><link>http://arxiv.org/abs/2306.07515v1</link><description>Video moment localization, also known as video moment retrieval, aiming tosearch a target segment within a video described by a given natural languagequery. Beyond the task of temporal action localization whereby the targetactions are pre-defined, video moment retrieval can query arbitrary complexactivities. In this survey paper, we aim to present a comprehensive review ofexisting video moment localization techniques, including supervised, weaklysupervised, and unsupervised ones. We also review the datasets available forvideo moment localization and group results of related work. In addition, wediscuss promising future directions for this field, in particular large-scaledatasets and interpretable video moment localization models.</description><author>Meng Liu, Liqiang Nie, Yunxiao Wang, Meng Wang, Yong Rui</author><pubDate>Tue, 13 Jun 2023 03:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07515v1</guid></item><item><title>HowToCaption: Prompting LLMs to Transform Video Annotations at Scale</title><link>http://arxiv.org/abs/2310.04900v1</link><description>Instructional videos are an excellent source for learning multimodalrepresentations by leveraging video-subtitle pairs extracted with automaticspeech recognition systems (ASR) from the audio signal in the videos. However,in contrast to human-annotated captions, both speech and subtitles naturallydiffer from the visual content of the videos and thus provide only noisysupervision for multimodal learning. As a result, large-scale annotation-freeweb video training data remains sub-optimal for training text-video models. Inthis work, we propose to leverage the capability of large language models(LLMs) to obtain fine-grained video descriptions aligned with videos.Specifically, we prompt an LLM to create plausible video descriptions based onASR narrations of the video for a large-scale instructional video dataset. Tothis end, we introduce a prompting method that is able to take into account alonger text of subtitles, allowing us to capture context beyond a singlesentence. To align the captions to the video temporally, we prompt the LLM togenerate timestamps for each produced caption based on the subtitles. In thisway, we obtain human-style video captions at scale without human supervision.We apply our method to the subtitles of the HowTo100M dataset, creating a newlarge-scale dataset, HowToCaption. Our evaluation shows that the resultingcaptions not only significantly improve the performance over many differentbenchmark datasets for text-video retrieval but also lead to a disentangling oftextual narration from the audio, boosting performance in text-video-audiotasks.</description><author>Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt Schiele, Hilde Kuehne</author><pubDate>Sat, 07 Oct 2023 20:32:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04900v1</guid></item><item><title>Global and Local Semantic Completion Learning for Vision-Language Pre-training</title><link>http://arxiv.org/abs/2306.07096v1</link><description>Cross-modal alignment plays a crucial role in vision-language pre-training(VLP) models, enabling them to capture meaningful associations across differentmodalities. For this purpose, inspired by the success of masked languagemodeling (MLM) tasks in the NLP pre-training area, numerous masked modelingtasks have been proposed for VLP to further promote cross-modal interactions.The core idea of previous masked modeling tasks is to focus on reconstructingthe masked tokens based on visible context for learning local-local alignment,i.e., associations between image patches and text tokens. However, most of thempay little attention to the global semantic features generated for the maskeddata, resulting in a limited cross-modal alignment ability of globalrepresentations to local features of the other modality. Therefore, in thispaper, we propose a novel Global and Local Semantic Completion Learning (GLSCL)task to facilitate global-local alignment and local-local alignmentsimultaneously. Specifically, the GLSCL task complements the missing semanticsof masked data and recovers global and local features by cross-modalinteractions. Our GLSCL consists of masked global semantic completion (MGSC)and masked local token completion (MLTC). MGSC promotes learning morerepresentative global features which have a great impact on the performance ofdownstream tasks, and MLTC can further enhance accurate comprehension onmultimodal data. Moreover, we present a flexible vision encoder, enabling ourmodel to simultaneously perform image-text and video-text multimodal tasks.Experimental results show that our proposed method obtains state-of-the-artperformance on various vision-language benchmarks, such as visual questionanswering, image-text retrieval, and video-text retrieval.</description><author>Rong-Cheng Tu, Yatai Ji, Jie Jiang, Weijie Kong, Chengfei Cai, Wenzhe Zhao, Hongfa Wang, Yujiu Yang, Wei Liu</author><pubDate>Mon, 12 Jun 2023 14:20:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07096v1</guid></item><item><title>MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks</title><link>http://arxiv.org/abs/2303.16839v3</link><description>The development of language models have moved from encoder-decoder todecoder-only designs. In addition, we observe that the two most popularmultimodal tasks, the generative and contrastive tasks, are nontrivial toaccommodate in one architecture, and further need adaptations for downstreamtasks. We propose a novel paradigm of training with a decoder-only model formultimodal tasks, which is surprisingly effective in jointly learning of thesedisparate vision-language tasks. This is done with a simple model, calledMaMMUT. It consists of a single vision encoder and a text decoder, and is ableto accommodate contrastive and generative learning by a novel two-pass approachon the text decoder. We demonstrate that joint learning of these diverseobjectives is simple, effective, and maximizes the weight-sharing of the modelacross these tasks. Furthermore, the same architecture enables straightforwardextensions to open-vocabulary object detection and video-language tasks. Themodel tackles a diverse range of tasks, while being modest in capacity. Ourmodel achieves the state of the art on image-text and text-image retrieval,video question answering and open-vocabulary detection tasks, outperformingmuch larger and more extensively trained foundational models. It shows verycompetitive results on VQA and Video Captioning, especially considering itscapacity. Ablations confirm the flexibility and advantages of our approach.</description><author>Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew Dai, Zhifeng Chen, Claire Cui, Anelia Angelova</author><pubDate>Wed, 09 Aug 2023 06:39:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16839v3</guid></item><item><title>Robustness Analysis of Video-Language Models Against Visual and Language Perturbations</title><link>http://arxiv.org/abs/2207.02159v4</link><description>Joint visual and language modeling on large-scale datasets has recently showngood progress in multi-modal tasks when compared to single modal learning.However, robustness of these approaches against real-world perturbations hasnot been studied. In this work, we perform the first extensive robustness studyof video-language models against various real-world perturbations. We focus ontext-to-video retrieval and propose two large-scale benchmark datasets,MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 differenttext perturbations. The study reveals some interesting initial findings fromthe studied models: 1) models are generally more susceptible when only video isperturbed as opposed to when only text is perturbed, 2) models that arepre-trained are more robust than those trained from scratch, 3) models attendmore to scene and objects rather than motion and action. We hope this studywill serve as a benchmark and guide future research in robust video-languagelearning. The benchmark introduced in this study along with the code anddatasets is available at https://bit.ly/3CNOly4.</description><author>Madeline C. Schiappa, Shruti Vyas, Hamid Palangi, Yogesh S. Rawat, Vibhav Vineet</author><pubDate>Tue, 18 Jul 2023 18:23:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.02159v4</guid></item><item><title>Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video Understanding in Novel Domains</title><link>http://arxiv.org/abs/2311.18773v1</link><description>Learning from videos is an emerging research area that enables robots toacquire skills from human demonstrations, such as procedural videos. To dothis, video-language models must be able to obtain structured understandings,such as the temporal segmentation of a demonstration into sequences of actionsand skills, and to generalize the understandings to novel domains. In pursuitof this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1)step recognition and (2) intra-video retrieval over a dataset of temporallysegmented and labeled tasks in International Space Station spacewalkrecordings. In tandem, the two tasks quantify a model's ability to make use of:(1) out-of-domain visual information; (2) a high temporal context window; and(3) multimodal (text + video) domains. This departs from existing benchmarksfor procedural video understanding, which typically deal with short contextlengths and can be solved with a single modality. Spacewalk-18, with itsinherent multimodal and long-form complexity, exposes the high difficulty oftask recognition and segmentation. We find that state-of-the-art methodsperform poorly on our benchmark, demonstrating that the goal of generalizableprocedural video understanding models is far out and underscoring the need todevelop new approaches to these tasks. Data, model, and code will be publiclyreleased.</description><author>Rohan Myer Krishnan, Zitian Tang, Zhiqiu Yu, Chen Sun</author><pubDate>Thu, 30 Nov 2023 18:19:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18773v1</guid></item><item><title>Enhanced Multimodal Representation Learning with Cross-modal KD</title><link>http://arxiv.org/abs/2306.07646v1</link><description>This paper explores the tasks of leveraging auxiliary modalities which areonly available at training to enhance multimodal representation learningthrough cross-modal Knowledge Distillation (KD). The widely adopted mutualinformation maximization-based objective leads to a short-cut solution of theweak teacher, i.e., achieving the maximum mutual information by simply makingthe teacher model as weak as the student model. To prevent such a weaksolution, we introduce an additional objective term, i.e., the mutualinformation between the teacher and the auxiliary modality model. Besides, tonarrow down the information gap between the student and teacher, we furtherpropose to minimize the conditional entropy of the teacher given the student.Novel training schemes based on contrastive learning and adversarial learningare designed to optimize the mutual information and the conditional entropy,respectively. Experimental results on three popular multimodal benchmarkdatasets have shown that the proposed method outperforms a range ofstate-of-the-art approaches for video recognition, video retrieval and emotionclassification.</description><author>Mengxi Chen, Linyu Xing, Yu Wang, Ya Zhang</author><pubDate>Tue, 13 Jun 2023 10:35:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07646v1</guid></item><item><title>Cross-modal Manifold Cutmix for Self-supervised Video Representation Learning</title><link>http://arxiv.org/abs/2112.03906v2</link><description>In this paper, we address the challenge of obtaining large-scale unlabelledvideo datasets for contrastive representation learning in real-worldapplications. We present a novel video augmentation technique forself-supervised learning, called Cross-Modal Manifold Cutmix (CMMC), whichgenerates augmented samples by combining different modalities in videos. Byembedding a video tesseract into another across two modalities in the featurespace, our method enhances the quality of learned video representations. Weperform extensive experiments on two small-scale video datasets, UCF101 andHMDB51, for action recognition and video retrieval tasks. Our approach is alsoshown to be effective on the NTU dataset with limited domain knowledge. OurCMMC achieves comparable performance to other self-supervised methods whileusing less training data for both downstream tasks.</description><author>Srijan Das, Michael S. Ryoo</author><pubDate>Wed, 26 Jul 2023 15:49:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.03906v2</guid></item><item><title>MultiVENT: Multilingual Videos of Events with Aligned Natural Text</title><link>http://arxiv.org/abs/2307.03153v1</link><description>Everyday news coverage has shifted from traditional broadcasts towards a widerange of presentation formats such as first-hand, unedited video footage.Datasets that reflect the diverse array of multimodal, multilingual newssources available online could be used to teach models to benefit from thisshift, but existing news video datasets focus on traditional news broadcastsproduced for English-speaking audiences. We address this limitation byconstructing MultiVENT, a dataset of multilingual, event-centric videosgrounded in text documents across five target languages. MultiVENT includesboth news broadcast videos and non-professional event footage, which we use toanalyze the state of online news videos and how they can be leveraged to buildrobust, factually accurate models. Finally, we provide a model for complex,multilingual video retrieval to serve as a baseline for information retrievalusing MultiVENT.</description><author>Kate Sanders, David Etter, Reno Kriz, Benjamin Van Durme</author><pubDate>Thu, 06 Jul 2023 18:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03153v1</guid></item><item><title>Cross-modal Manifold Cutmix for Self-supervised Video Representation Learning</title><link>http://arxiv.org/abs/2112.03906v3</link><description>Contrastive representation learning of videos highly relies on theavailability of millions of unlabelled videos. This is practical for videosavailable on web but acquiring such large scale of videos for real-worldapplications is very expensive and laborious. Therefore, in this paper we focus on designing video augmentation forself-supervised learning, we first analyze the best strategy to mix videos tocreate a new augmented video sample. Then, the question remains, can we makeuse of the other modalities in videos for data mixing? To this end, we proposeCross-Modal Manifold Cutmix (CMMC) that inserts a video tesseract into anothervideo tesseract in the feature space across two different modalities. We findthat our video mixing strategy STC-mix, i.e. preliminary mixing of videosfollowed by CMMC across different modalities in a video, improves the qualityof learned video representations. We conduct thorough experiments for twodownstream tasks: action recognition and video retrieval on two small scalevideo datasets UCF101, and HMDB51. We also demonstrate the effectiveness of ourSTC-mix on NTU dataset where domain knowledge is limited. We show that the performance of our STC-mix on both the downstream tasks ison par with the other self-supervised approaches while requiring less trainingdata.</description><author>Srijan Das, Michael S. Ryoo</author><pubDate>Thu, 27 Jul 2023 19:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.03906v3</guid></item><item><title>Distilling Vision-Language Models on Millions of Videos</title><link>http://arxiv.org/abs/2401.06129v1</link><description>The recent advance in vision-language models is largely attributed to theabundance of image-text data. We aim to replicate this success forvideo-language models, but there simply is not enough human-curated video-textdata available. We thus resort to fine-tuning a video-language model from astrong image-language baseline with synthesized instructional data. Theresulting video-language model is then used to auto-label millions of videos togenerate high-quality captions. We show the adapted video-language modelperforms well on a wide range of video-language benchmarks. For instance, itsurpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, ourmodel generates detailed descriptions for previously unseen videos, whichprovide better textual supervision than existing methods. Experiments show thata video-language dual-encoder model contrastively trained on theseauto-generated captions is 3.8% better than the strongest baseline that alsoleverages vision-language models. Our best model outperforms state-of-the-artmethods on MSR-VTT zero-shot text-to-video retrieval by 6%.</description><author>Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing Gong, Philipp Krähenbühl, Liangzhe Yuan</author><pubDate>Thu, 11 Jan 2024 18:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06129v1</guid></item><item><title>E-ViLM: Efficient Video-Language Model via Masked Video Modeling with Semantic Vector-Quantized Tokenizer</title><link>http://arxiv.org/abs/2311.17267v1</link><description>To build scalable models for challenging real-world tasks, it is important tolearn from diverse, multi-modal data in various forms (e.g., videos, text, andimages). Among the existing works, a plethora of them have focused onleveraging large but cumbersome cross-modal architectures. Regardless of theireffectiveness, larger architectures unavoidably prevent the models from beingextended to real-world applications, so building a lightweight VL architectureand an efficient learning schema is of great practical value. In this paper, wepropose an Efficient Video-Language Model (dubbed as E-ViLM) and a masked videomodeling (MVM) schema, assisted with a semantic vector-quantized tokenizer. Inparticular, our E-ViLM learns to reconstruct the semantic labels of maskedvideo regions, produced by the pre-trained vector-quantized tokenizer, whichdiscretizes the continuous visual signals into labels. We show that with oursimple MVM task and regular VL pre-training modelings, our E-ViLM, despite itscompactness, is able to learn expressive representations from Video-Languagecorpus and generalize well to extensive Video-Language tasks including videoquestion answering, text-to-video retrieval, etc. In particular, our E-ViLMobtains obvious efficiency improvements by reaching competing performances withfaster inference speed, i.e., our model reaches $39.3$% Top-$1$ accuracy on theMSRVTT benchmark, retaining $91.4$% of the accuracy of state-of-the-art largerVL architecture with only $15%$ parameters and $94.8%$ fewer GFLOPs. We alsoprovide extensive ablative studies that validate the effectiveness of ourproposed learning schema for E-ViLM.</description><author>Jacob Zhiyuan Fang, Skyler Zheng, Vasu Sharma, Robinson Piramuthu</author><pubDate>Tue, 28 Nov 2023 22:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17267v1</guid></item><item><title>Meta-Personalizing Vision-Language Models to Find Named Instances in Video</title><link>http://arxiv.org/abs/2306.10169v1</link><description>Large-scale vision-language models (VLM) have shown impressive results forlanguage-guided search applications. While these models allow category-levelqueries, they currently struggle with personalized searches for moments in avideo where a specific object instance such as ``My dog Biscuit'' appears. Wepresent the following three contributions to address this problem. First, wedescribe a method to meta-personalize a pre-trained VLM, i.e., learning how tolearn to personalize a VLM at test time to search in video. Our method extendsthe VLM's token vocabulary by learning novel word embeddings specific to eachinstance. To capture only instance-specific features, we represent eachinstance embedding as a combination of shared and learned global categoryfeatures. Second, we propose to learn such personalization without explicithuman supervision. Our approach automatically identifies moments of namedvisual instances in video using transcripts and vision-language similarity inthe VLM's embedding space. Finally, we introduce This-Is-My, a personal videoinstance retrieval benchmark. We evaluate our approach on This-Is-My andDeepFashion2 and show that we obtain a 15% relative improvement over the stateof the art on the latter dataset.</description><author>Chun-Hsiao Yeh, Bryan Russell, Josef Sivic, Fabian Caba Heilbron, Simon Jenni</author><pubDate>Fri, 16 Jun 2023 21:12:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10169v1</guid></item><item><title>A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings</title><link>http://arxiv.org/abs/2308.09202v1</link><description>Item representation holds significant importance in recommendation systems,which encompasses domains such as news, retail, and videos. Retrieval andranking models utilise item representation to capture the user-itemrelationship based on user behaviours. While existing representation learningmethods primarily focus on optimising item-based mechanisms, such as attentionand sequential modelling. However, these methods lack a modelling mechanism todirectly reflect user interests within the learned item representations.Consequently, these methods may be less effective in capturing user interestsindirectly. To address this challenge, we propose a novel Interest-awareCapsule network (IaCN) recommendation model, a model-agnostic framework thatdirectly learns interest-oriented item representations. IaCN serves as anauxiliary task, enabling the joint learning of both item-based andinterest-based representations. This framework adopts existing recommendationmodels without requiring substantial redesign. We evaluate the proposedapproach on benchmark datasets, exploring various scenarios involving differentdeep neural networks, behaviour sequence lengths, and joint learning ratios ofinterest-oriented item representations. Experimental results demonstratesignificant performance enhancements across diverse recommendation models,validating the effectiveness of our approach.</description><author>Amit Kumar Jaiswal, Yu Xiong</author><pubDate>Thu, 17 Aug 2023 23:40:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09202v1</guid></item><item><title>An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling</title><link>http://arxiv.org/abs/2209.01540v4</link><description>Masked visual modeling (MVM) has been recently proven effective for visualpre-training. While similar reconstructive objectives on video inputs (e.g.,masked frame modeling) have been explored in video-language (VidL)pre-training, previous studies fail to find a truly effective MVM strategy thatcan largely benefit the downstream performance. In this work, we systematicallyexamine the potential of MVM in the context of VidL learning. Specifically, webase our study on a fully end-to-end VIdeO-LanguagE Transformer (VIOLET), wherethe supervision from MVM training can be backpropagated to the video pixelspace. In total, eight different reconstructive targets of MVM are explored,from low-level pixel values and oriented gradients to high-level depth maps,optical flow, discrete visual tokens, and latent visual features. We conductcomprehensive experiments and provide insights into the factors leading toeffective MVM training, resulting in an enhanced model VIOLETv2. Empirically,we show VIOLETv2 pre-trained with MVM objective achieves notable improvementson 13 VidL benchmarks, ranging from video question answering, video captioning,to text-to-video retrieval.</description><author>Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, Zicheng Liu</author><pubDate>Tue, 30 May 2023 07:56:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01540v4</guid></item><item><title>TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection</title><link>http://arxiv.org/abs/2401.02309v1</link><description>Video moment retrieval (MR) and highlight detection (HD) based on naturallanguage queries are two highly related tasks, which aim to obtain relevantmoments within videos and highlight scores of each video clip. Recently,several methods have been devoted to building DETR-based networks to solve bothMR and HD jointly. These methods simply add two separate task heads aftermulti-modal feature extraction and feature interaction, achieving goodperformance. Nevertheless, these approaches underutilize the reciprocalrelationship between two tasks. In this paper, we propose a task-reciprocaltransformer based on DETR (TR-DETR) that focuses on exploring the inherentreciprocity between MR and HD. Specifically, a local-global multi-modalalignment module is first built to align features from diverse modalities intoa shared latent space. Subsequently, a visual feature refinement is designed toeliminate query-irrelevant information from visual features for modalinteraction. Finally, a task cooperation module is constructed to refine theretrieval pipeline and the highlight score prediction process by utilizing thereciprocity between MR and HD. Comprehensive experiments on QVHighlights,Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existingstate-of-the-art methods. Codes are available at\url{https://github.com/mingyao1120/TR-DETR}.</description><author>Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie</author><pubDate>Thu, 04 Jan 2024 14:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02309v1</guid></item><item><title>TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection</title><link>http://arxiv.org/abs/2401.02309v2</link><description>Video moment retrieval (MR) and highlight detection (HD) based on naturallanguage queries are two highly related tasks, which aim to obtain relevantmoments within videos and highlight scores of each video clip. Recently,several methods have been devoted to building DETR-based networks to solve bothMR and HD jointly. These methods simply add two separate task heads aftermulti-modal feature extraction and feature interaction, achieving goodperformance. Nevertheless, these approaches underutilize the reciprocalrelationship between two tasks. In this paper, we propose a task-reciprocaltransformer based on DETR (TR-DETR) that focuses on exploring the inherentreciprocity between MR and HD. Specifically, a local-global multi-modalalignment module is first built to align features from diverse modalities intoa shared latent space. Subsequently, a visual feature refinement is designed toeliminate query-irrelevant information from visual features for modalinteraction. Finally, a task cooperation module is constructed to refine theretrieval pipeline and the highlight score prediction process by utilizing thereciprocity between MR and HD. Comprehensive experiments on QVHighlights,Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existingstate-of-the-art methods. Codes are available at\url{https://github.com/mingyao1120/TR-DETR}.</description><author>Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie</author><pubDate>Fri, 05 Jan 2024 03:11:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.02309v2</guid></item><item><title>Building an Open-Vocabulary Video CLIP Model with Better Architectures, Optimization and Data</title><link>http://arxiv.org/abs/2310.05010v1</link><description>Despite significant results achieved by Contrastive Language-ImagePretraining (CLIP) in zero-shot image recognition, limited effort has been madeexploring its potential for zero-shot video recognition. This paper presentsOpen-VCLIP++, a simple yet effective framework that adapts CLIP to a strongzero-shot video classifier, capable of identifying novel actions and eventsduring testing. Open-VCLIP++ minimally modifies CLIP to capturespatial-temporal relationships in videos, thereby creating a specialized videoclassifier while striving for generalization. We formally demonstrate thattraining Open-VCLIP++ is tantamount to continual learning with zero historicaldata. To address this problem, we introduce Interpolated Weight Optimization, atechnique that leverages the advantages of weight interpolation during bothtraining and testing. Furthermore, we build upon large language models toproduce fine-grained video descriptions. These detailed descriptions arefurther aligned with video features, facilitating a better transfer of CLIP tothe video domain. Our approach is evaluated on three widely used actionrecognition datasets, following a variety of zero-shot evaluation protocols.The results demonstrate that our method surpasses existing state-of-the-arttechniques by significant margins. Specifically, we achieve zero-shot accuracyscores of 88.1%, 58.7%, and 81.2% on UCF, HMDB, and Kinetics-600 datasetsrespectively, outpacing the best-performing alternative methods by 8.5%, 8.2%,and 12.3%. We also evaluate our approach on the MSR-VTT video-text retrievaldataset, where it delivers competitive video-to-text and text-to-videoretrieval performance, while utilizing substantially less fine-tuning datacompared to other methods. Code is released athttps://github.com/wengzejia1/Open-VCLIP.</description><author>Zuxuan Wu, Zejia Weng, Wujian Peng, Xitong Yang, Ang Li, Larry S. Davis, Yu-Gang Jiang</author><pubDate>Sun, 08 Oct 2023 05:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05010v1</guid></item><item><title>Query by Activity Video in the Wild</title><link>http://arxiv.org/abs/2311.13895v1</link><description>This paper focuses on activity retrieval from a video query in an imbalancedscenario. In current query-by-activity-video literature, a common assumption isthat all activities have sufficient labelled examples when learning anembedding. This assumption does however practically not hold, as only a portionof activities have many examples, while other activities are only described byfew examples. In this paper, we propose a visual-semantic embedding networkthat explicitly deals with the imbalanced scenario for activity retrieval. Ournetwork contains two novel modules. The visual alignment module performs aglobal alignment between the input video and fixed-sized visual bankrepresentations for all activities. The semantic module performs an alignmentbetween the input video and fixed-sized semantic activity representations. Bymatching videos with both visual and semantic activity representations that areof equal size over all activities, we no longer ignore infrequent activitiesduring retrieval. Experiments on a new imbalanced activity retrieval benchmarkshow the effectiveness of our approach for all types of activities.</description><author>Tao Hu, William Thong, Pascal Mettes, Cees G. M. Snoek</author><pubDate>Thu, 23 Nov 2023 10:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13895v1</guid></item><item><title>Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception</title><link>http://arxiv.org/abs/2305.06324v1</link><description>We present Integrated Multimodal Perception (IMP), a simple and scalablemultimodal multi-task training and modeling approach. IMP integrates multimodalinputs including image, video, text, and audio into a single Transformerencoder with minimal modality-specific components. IMP makes use of a noveldesign that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts(MoE) for efficient model \&amp; task scaling. We conduct extensive empiricalstudies about IMP and reveal the following key insights: 1) performing gradientdescent updates by alternating on diverse heterogeneous modalities, lossfunctions, and tasks, while also varying input resolutions, efficientlyimproves multimodal understanding. 2) model sparsification with MoE on a singlemodality-agnostic encoder substantially improves the performance, outperformingdense models that use modality-specific encoders or additional fusion layersand greatly mitigating the conflicts between modalities. IMP achievescompetitive performance on a wide range of downstream tasks including imageclassification, video classification, image-text, and video-text retrieval.Most notably, we train a sparse IMP-MoE-L focusing on video tasks that achievesnew state-of-the-art in zero-shot video classification. Our model achieves77.0% on Kinetics-400, 76.8% on Kinetics-600, and 76.8% on Kinetics-700zero-shot classification accuracy, improving the previous state-of-the-art by+5%, +6.7%, and +5.8%, respectively, while using only 15% of their totaltraining computational cost.</description><author>Hassan Akbari, Dan Kondratyuk, Yin Cui, Rachel Hornung, Huisheng Wang, Hartwig Adam</author><pubDate>Wed, 10 May 2023 18:22:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06324v1</guid></item><item><title>InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</title><link>http://arxiv.org/abs/2307.06942v2</link><description>This paper introduces InternVid, a large-scale video-centric multimodaldataset that enables learning powerful and transferable video-textrepresentations for multimodal understanding and generation. The InternViddataset contains over 7 million videos lasting nearly 760K hours, yielding 234Mvideo clips accompanied by detailed descriptions of total 4.1B words. Our corecontribution is to develop a scalable approach to autonomously build ahigh-quality video-text dataset with large language models (LLM), therebyshowcasing its efficacy in learning video-language representation at scale.Specifically, we utilize a multi-scale approach to generate video-relateddescriptions. Furthermore, we introduce ViCLIP, a video-text representationlearning model based on ViT-L. Learned on InternVid via contrastive learning,this model demonstrates leading zero-shot action recognition and competitivevideo retrieval performance. Beyond basic video understanding tasks likerecognition and retrieval, our dataset and model have broad applications. Theyare particularly beneficial for generating interleaved video-text data forlearning a video-centric dialogue system, advancing video-to-text andtext-to-video generation research. These proposed resources provide a tool forresearchers and practitioners interested in multimodal video understanding andgeneration.</description><author>Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao</author><pubDate>Thu, 04 Jan 2024 05:00:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06942v2</guid></item><item><title>Zero-shot Audio Topic Reranking using Large Language Models</title><link>http://arxiv.org/abs/2309.07606v1</link><description>The Multimodal Video Search by Examples (MVSE) project investigates usingvideo clips as the query term for information retrieval, rather than the moretraditional text query. This enables far richer search modalities such asimages, speaker, content, topic, and emotion. A key element for this process ishighly rapid, flexible, search to support large archives, which in MVSE isfacilitated by representing video attributes by embeddings. This work aims tomitigate any performance loss from this rapid archive search by examiningreranking approaches. In particular, zero-shot reranking methods using largelanguage models are investigated as these are applicable to any video archiveaudio content. Performance is evaluated for topic-based retrieval on a publiclyavailable video archive, the BBC Rewind corpus. Results demonstrate thatreranking can achieve improved retrieval ranking without the need for anytask-specific training data.</description><author>Mengjie Qian, Rao Ma, Adian Liusie, Erfan Loweimi, Kate M. Knill, Mark J. F. Gales</author><pubDate>Thu, 14 Sep 2023 12:13:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07606v1</guid></item><item><title>ProS: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval</title><link>http://arxiv.org/abs/2312.12478v2</link><description>The goal of Universal Cross-Domain Retrieval (UCDR) is to achieve robustperformance in generalized test scenarios, wherein data may belong to strictlyunknown domains and categories during training. Recently, pre-trained modelswith prompt tuning have shown strong generalization capabilities and attainednoteworthy achievements in various downstream tasks, such as few-shot learningand video-text retrieval. However, applying them directly to UCDR may notsufficiently to handle both domain shift (i.e., adapting to unfamiliar domains)and semantic shift (i.e., transferring to unknown categories). To this end, wepropose Prompting-to-Simulate (ProS), the first method to apply prompt tuningfor UCDR. ProS employs a two-step process to simulate Content-aware DynamicPrompts (CaDP) which can impact models to produce generalized features forUCDR. Concretely, in Prompt Units Learning stage, we introduce two Prompt Unitsto individually capture domain and semantic knowledge in a mask-and-align way.Then, in Context-aware Simulator Learning stage, we train a Content-awarePrompt Simulator under a simulated test scenarios to produce the correspondingCaDP. Extensive experiments conducted on three benchmark datasets show that ourmethod achieves new state-of-the-art performance without bringing excessiveparameters. Our method is publicly available athttps://anonymous.4open.science/r/ProS</description><author>Kaipeng Fang, Jingkuan Song, Lianli Gao, Pengpeng Zeng, Zhi-Qi Cheng, Xiyao Li, Heng Tao Shen</author><pubDate>Sun, 07 Jan 2024 15:05:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12478v2</guid></item><item><title>Video-CSR: Complex Video Digest Creation for Visual-Language Models</title><link>http://arxiv.org/abs/2310.05060v1</link><description>We present a novel task and human annotated dataset for evaluating theability for visual-language models to generate captions and summaries forreal-world video clips, which we call Video-CSR (Captioning, Summarization andRetrieval). The dataset contains 4.8K YouTube video clips of 20-60 seconds induration and covers a wide range of topics and interests. Each video clipcorresponds to 5 independently annotated captions (1 sentence) and summaries(3-10 sentences). Given any video selected from the dataset and itscorresponding ASR information, we evaluate visual-language models on eithercaption or summary generation that is grounded in both the visual and auditorycontent of the video. Additionally, models are also evaluated on caption- andsummary-based retrieval tasks, where the summary-based retrieval task requiresthe identification of a target video given excerpts of a corresponding summary.Given the novel nature of the paragraph-length video summarization task, weperform extensive comparative analyses of different existing evaluation metricsand their alignment with human preferences. Finally, we propose a foundationmodel with competitive generation and retrieval capabilities that serves as abaseline for the Video-CSR task. We aim for Video-CSR to serve as a usefulevaluation set in the age of large language models and complex multi-modaltasks.</description><author>Tingkai Liu, Yunzhe Tao, Haogeng Liu, Qihang Fan, Ding Zhou, Huaibo Huang, Ran He, Hongxia Yang</author><pubDate>Sun, 08 Oct 2023 09:02:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05060v1</guid></item><item><title>Lightweight High-Speed Photography Built on Coded Exposure and Implicit Neural Representation of Videos</title><link>http://arxiv.org/abs/2311.13134v1</link><description>The compact cameras recording high-speed scenes with high resolution arehighly demanded, but the required high bandwidth often leads to bulky, heavysystems, which limits their applications on low-capacity platforms. Adopting acoded exposure setup to encode a frame sequence into a blurry snapshot andretrieve the latent sharp video afterward can serve as a lightweight solution.However, restoring motion from blur is quite challenging due to the highill-posedness of motion blur decomposition, intrinsic ambiguity in motiondirection, and diverse motions in natural videos. In this work, by leveragingclassical coded exposure imaging technique and emerging implicit neuralrepresentation for videos, we tactfully embed the motion direction cues intothe blurry image during the imaging process and develop a novel self-recursiveneural network to sequentially retrieve the latent video sequence from theblurry image utilizing the embedded motion direction cues. To validate theeffectiveness and efficiency of the proposed framework, we conduct extensiveexperiments on benchmark datasets and real-captured blurry images. The resultsdemonstrate that our proposed framework significantly outperforms existingmethods in quality and flexibility. The code for our work is available athttps://github.com/zhihongz/BDINR</description><author>Zhihong Zhang, Runzhao Yang, Jinli Suo, Yuxiao Cheng, Qionghai Dai</author><pubDate>Wed, 22 Nov 2023 03:41:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13134v1</guid></item><item><title>Video-based Person Re-identification with Long Short-Term Representation Learning</title><link>http://arxiv.org/abs/2308.03703v1</link><description>Video-based person Re-Identification (V-ReID) aims to retrieve specificpersons from raw videos captured by non-overlapped cameras. As a fundamentaltask, it spreads many multimedia and computer vision applications. However, dueto the variations of persons and scenes, there are still many obstacles thatmust be overcome for high performance. In this work, we notice that both thelong-term and short-term information of persons are important for robust videorepresentations. Thus, we propose a novel deep learning framework named LongShort-Term Representation Learning (LSTRL) for effective V-ReID. Morespecifically, to extract long-term representations, we propose aMulti-granularity Appearance Extractor (MAE), in which four granularityappearances are effectively captured across multiple frames. Meanwhile, toextract short-term representations, we propose a Bi-direction Motion Estimator(BME), in which reciprocal motion information is efficiently extracted fromconsecutive frames. The MAE and BME are plug-and-play and can be easilyinserted into existing networks for efficient feature learning. As a result,they significantly improve the feature representation ability for V-ReID.Extensive experiments on three widely used benchmarks show that our proposedapproach can deliver better performances than most state-of-the-arts.</description><author>Xuehu Liu, Pingping Zhang, Huchuan Lu</author><pubDate>Mon, 07 Aug 2023 17:22:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03703v1</guid></item><item><title>MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images</title><link>http://arxiv.org/abs/2306.07257v1</link><description>In this paper, we present MovieFactory, a powerful framework to generatecinematic-picture (3072$\times$1280), film-style (multi-scene), andmulti-modality (sounding) movies on the demand of natural languages. As thefirst fully automated movie generation model to the best of our knowledge, ourapproach empowers users to create captivating movies with smooth transitionsusing simple text inputs, surpassing existing methods that produce soundlessvideos limited to a single scene of modest quality. To facilitate thisdistinctive functionality, we leverage ChatGPT to expand user-provided textinto detailed sequential scripts for movie generation. Then we bring scripts tolife visually and acoustically through vision generation and audio retrieval.To generate videos, we extend the capabilities of a pretrained text-to-imagediffusion model through a two-stage process. Firstly, we employ spatialfinetuning to bridge the gap between the pretrained image model and the newvideo dataset. Subsequently, we introduce temporal learning to capture objectmotion. In terms of audio, we leverage sophisticated retrieval models to selectand align audio elements that correspond to the plot and visual content of themovie. Extensive experiments demonstrate that our MovieFactory produces movieswith realistic visuals, diverse scenes, and seamlessly fitting audio, offeringusers a novel and immersive experience. Generated samples can be found inYouTube or Bilibili (1080P).</description><author>Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi Tuo, Wen-Huang Cheng, Lianli Gao, Jingkuan Song, Jianlong Fu</author><pubDate>Mon, 12 Jun 2023 18:31:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07257v1</guid></item><item><title>Joint Moment Retrieval and Highlight Detection Via Natural Language Queries</title><link>http://arxiv.org/abs/2305.04961v1</link><description>Video summarization has become an increasingly important task in the field ofcomputer vision due to the vast amount of video content available on theinternet. In this project, we propose a new method for natural language querybased joint video summarization and highlight detection using multi-modaltransformers. This approach will use both visual and audio cues to match auser's natural language query to retrieve the most relevant and interestingmoments from a video. Our approach employs multiple recent techniques used inVision Transformers (ViTs) to create a transformer-like encoder-decoder model.We evaluated our approach on multiple datasets such as YouTube Highlights andTVSum to demonstrate the flexibility of our proposed method.</description><author>Richard Luo, Austin Peng, Heidi Yap, Koby Beard</author><pubDate>Mon, 08 May 2023 19:00:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04961v1</guid></item><item><title>SoccerNet 2023 Challenges Results</title><link>http://arxiv.org/abs/2309.06006v1</link><description>The SoccerNet 2023 challenges were the third annual video understandingchallenges organized by the SoccerNet team. For this third edition, thechallenges were composed of seven vision-based tasks split into three mainthemes. The first theme, broadcast video understanding, is composed of threehigh-level tasks related to describing events occurring in the videobroadcasts: (1) action spotting, focusing on retrieving all timestamps relatedto global actions in soccer, (2) ball action spotting, focusing on retrievingall timestamps related to the soccer ball change of state, and (3) dense videocaptioning, focusing on describing the broadcast with natural language andanchored timestamps. The second theme, field understanding, relates to thesingle task of (4) camera calibration, focusing on retrieving the intrinsic andextrinsic camera parameters from images. The third and last theme, playerunderstanding, is composed of three low-level tasks related to extractinginformation about the players: (5) re-identification, focusing on retrievingthe same players across multiple views, (6) multiple object tracking, focusingon tracking players and the ball through unedited video streams, and (7) jerseynumber recognition, focusing on recognizing the jersey number of players fromtracklets. Compared to the previous editions of the SoccerNet challenges, tasks(2-3-7) are novel, including new annotations and data, task (4) was enhancedwith more data and annotations, and task (6) now focuses on end-to-endapproaches. More information on the tasks, challenges, and leaderboards areavailable on https://www.soccer-net.org. Baselines and development kits can befound on https://github.com/SoccerNet.</description><author>Anthony Cioppa, Silvio Giancola, Vladimir Somers, Floriane Magera, Xin Zhou, Hassan Mkhallati, Adrien Deliège, Jan Held, Carlos Hinojosa, Amir M. Mansourian, Pierre Miralles, Olivier Barnich, Christophe De Vleeschouwer, Alexandre Alahi, Bernard Ghanem, Marc Van Droogenbroeck, Abdullah Kamal, Adrien Maglo, Albert Clapés, Amr Abdelaziz, Artur Xarles, Astrid Orcesi, Atom Scott, Bin Liu, Byoungkwon Lim, Chen Chen, Fabian Deuser, Feng Yan, Fufu Yu, Gal Shitrit, Guanshuo Wang, Gyusik Choi, Hankyul Kim, Hao Guo, Hasby Fahrudin, Hidenari Koguchi, Håkan Ardö, Ibrahim Salah, Ido Yerushalmy, Iftikar Muhammad, Ikuma Uchida, Ishay Be'ery, Jaonary Rabarisoa, Jeongae Lee, Jiajun Fu, Jianqin Yin, Jinghang Xu, Jongho Nang, Julien Denize, Junjie Li, Junpei Zhang, Juntae Kim, Kamil Synowiec, Kenji Kobayashi,</author><pubDate>Tue, 12 Sep 2023 08:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.06006v1</guid></item><item><title>VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset</title><link>http://arxiv.org/abs/2305.18500v2</link><description>Vision and text have been fully explored in contemporary video-textfoundational models, while other modalities such as audio and subtitles invideos have not received sufficient attention. In this paper, we resort toestablish connections between multi-modality video tracks, including Vision,Audio, and Subtitle, and Text by exploring an automatically generatedlarge-scale omni-modality video caption dataset called VAST-27M. Specifically,we first collect 27 million open-domain video clips and separately train avision and an audio captioner to generate vision and audio captions. Then, weemploy an off-the-shelf Large Language Model (LLM) to integrate the generatedcaptions, together with subtitles and instructional prompts into omni-modalitycaptions. Based on the proposed VAST-27M dataset, we train an omni-modalityvideo-text foundational model named VAST, which can perceive and processvision, audio, and subtitle modalities from video, and better support varioustasks including vision-text, audio-text, and multi-modal video-text tasks(retrieval, captioning and QA). Extensive experiments have been conducted todemonstrate the effectiveness of our proposed VAST-27M corpus and VASTfoundation model. VAST achieves 22 new state-of-the-art results on variouscross-modality benchmarks. Code, model and dataset will be released athttps://github.com/TXH-mercury/VAST.</description><author>Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu</author><pubDate>Sat, 07 Oct 2023 13:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18500v2</guid></item><item><title>Materialistic: Selecting Similar Materials in Images</title><link>http://arxiv.org/abs/2305.13291v1</link><description>Separating an image into meaningful underlying components is a crucial firststep for both editing and understanding images. We present a method capable ofselecting the regions of a photograph exhibiting the same material as anartist-chosen area. Our proposed approach is robust to shading, specularhighlights, and cast shadows, enabling selection in real images. As we do notrely on semantic segmentation (different woods or metal should not be selectedtogether), we formulate the problem as a similarity-based grouping problembased on a user-provided image location. In particular, we propose to leveragethe unsupervised DINO features coupled with a proposed Cross-Similarity moduleand an MLP head to extract material similarities in an image. We train ourmodel on a new synthetic image dataset, that we release. We show that ourmethod generalizes well to real-world images. We carefully analyze our model'sbehavior on varying material properties and lighting. Additionally, we evaluateit against a hand-annotated benchmark of 50 real photographs. We furtherdemonstrate our model on a set of applications, including material editing,in-video selection, and retrieval of object photographs with similar materials.</description><author>Prafull Sharma, Julien Philip, Michaël Gharbi, William T. Freeman, Fredo Durand, Valentin Deschaintre</author><pubDate>Mon, 22 May 2023 18:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13291v1</guid></item><item><title>VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset</title><link>http://arxiv.org/abs/2305.18500v1</link><description>Vision and text have been fully explored in contemporary video-textfoundational models, while other modalities such as audio and subtitles invideos have not received sufficient attention. In this paper, we resort toestablish connections between multi-modality video tracks, including Vision,Audio, and Subtitle, and Text by exploring an automatically generatedlarge-scale omni-modality video caption dataset called VAST-27M. Specifically,we first collect 27 million open-domain video clips and separately train avision and an audio captioner to generate vision and audio captions. Then, weemploy an off-the-shelf Large Language Model (LLM) to integrate the generatedcaptions, together with subtitles and instructional prompts into omni-modalitycaptions. Based on the proposed VAST-27M dataset, we train an omni-modalityvideo-text foundational model named VAST, which can perceive and processvision, audio, and subtitle modalities from video, and better support varioustasks including vision-text, audio-text, and multi-modal video-text tasks(retrieval, captioning and QA). Extensive experiments have been conducted todemonstrate the effectiveness of our proposed VAST-27M corpus and VASTfoundation model. VAST achieves 22 new state-of-the-art results on variouscross-modality benchmarks. Code, model and dataset will be released athttps://github.com/TXH-mercury/VAST.</description><author>Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu</author><pubDate>Mon, 29 May 2023 15:34:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18500v1</guid></item><item><title>No-frills Temporal Video Grounding: Multi-Scale Neighboring Attention and Zoom-in Boundary Detection</title><link>http://arxiv.org/abs/2307.10567v1</link><description>Temporal video grounding (TVG) aims to retrieve the time interval of alanguage query from an untrimmed video. A significant challenge in TVG is thelow "Semantic Noise Ratio (SNR)", which results in worse performance with lowerSNR. Prior works have addressed this challenge using sophisticated techniques.In this paper, we propose a no-frills TVG model that consists of two coremodules, namely multi-scale neighboring attention and zoom-in boundarydetection. The multi-scale neighboring attention restricts each video token toonly aggregate visual contexts from its neighbor, enabling the extraction ofthe most distinguishing information with multi-scale feature hierarchies fromhigh-ratio noises. The zoom-in boundary detection then focuses on local-wisediscrimination of the selected top candidates for fine-grained groundingadjustment. With an end-to-end training strategy, our model achievescompetitive performance on different TVG benchmarks, while also having theadvantage of faster inference speed and lighter model parameters, thanks to itslightweight architecture.</description><author>Qi Zhang, Sipeng Zheng, Qin Jin</author><pubDate>Thu, 20 Jul 2023 05:12:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10567v1</guid></item><item><title>UniVTG: Towards Unified Video-Language Temporal Grounding</title><link>http://arxiv.org/abs/2307.16715v1</link><description>Video Temporal Grounding (VTG), which aims to ground target clips from videos(such as consecutive intervals or disjoint shots) according to custom languagequeries (e.g., sentences or words), is key for video browsing on social media.Most methods in this direction develop taskspecific models that are trainedwith type-specific labels, such as moment retrieval (time interval) andhighlight detection (worthiness curve), which limits their abilities togeneralize to various VTG tasks and labels. In this paper, we propose to Unifythe diverse VTG labels and tasks, dubbed UniVTG, along three directions:Firstly, we revisit a wide range of VTG labels and tasks and define a unifiedformulation. Based on this, we develop data annotation schemes to createscalable pseudo supervision. Secondly, we develop an effective and flexiblegrounding model capable of addressing each task and making full use of eachlabel. Lastly, thanks to the unified framework, we are able to unlock temporalgrounding pretraining from large-scale diverse labels and develop strongergrounding abilities e.g., zero-shot grounding. Extensive experiments on threetasks (moment retrieval, highlight detection and video summarization) acrossseven datasets (QVHighlights, Charades-STA, TACoS, Ego4D, YouTube Highlights,TVSum, and QFVS) demonstrate the effectiveness and flexibility of our proposedframework. The codes are available at https://github.com/showlab/UniVTG.</description><author>Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, Mike Zheng Shou</author><pubDate>Mon, 31 Jul 2023 15:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16715v1</guid></item><item><title>UniVTG: Towards Unified Video-Language Temporal Grounding</title><link>http://arxiv.org/abs/2307.16715v2</link><description>Video Temporal Grounding (VTG), which aims to ground target clips from videos(such as consecutive intervals or disjoint shots) according to custom languagequeries (e.g., sentences or words), is key for video browsing on social media.Most methods in this direction develop taskspecific models that are trainedwith type-specific labels, such as moment retrieval (time interval) andhighlight detection (worthiness curve), which limits their abilities togeneralize to various VTG tasks and labels. In this paper, we propose to Unifythe diverse VTG labels and tasks, dubbed UniVTG, along three directions:Firstly, we revisit a wide range of VTG labels and tasks and define a unifiedformulation. Based on this, we develop data annotation schemes to createscalable pseudo supervision. Secondly, we develop an effective and flexiblegrounding model capable of addressing each task and making full use of eachlabel. Lastly, thanks to the unified framework, we are able to unlock temporalgrounding pretraining from large-scale diverse labels and develop strongergrounding abilities e.g., zero-shot grounding. Extensive experiments on threetasks (moment retrieval, highlight detection and video summarization) acrossseven datasets (QVHighlights, Charades-STA, TACoS, Ego4D, YouTube Highlights,TVSum, and QFVS) demonstrate the effectiveness and flexibility of our proposedframework. The codes are available at https://github.com/showlab/UniVTG.</description><author>Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, Mike Zheng Shou</author><pubDate>Fri, 18 Aug 2023 08:56:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16715v2</guid></item><item><title>Graph Convolution Based Efficient Re-Ranking for Visual Retrieval</title><link>http://arxiv.org/abs/2306.08792v1</link><description>Visual retrieval tasks such as image retrieval and person re-identification(Re-ID) aim at effectively and thoroughly searching images with similar contentor the same identity. After obtaining retrieved examples, re-ranking is awidely adopted post-processing step to reorder and improve the initialretrieval results by making use of the contextual information from semanticallyneighboring samples. Prevailing re-ranking approaches update distance metricsand mostly rely on inefficient crosscheck set comparison operations whilecomputing expanded neighbors based distances. In this work, we present anefficient re-ranking method which refines initial retrieval results by updatingfeatures. Specifically, we reformulate re-ranking based on Graph ConvolutionNetworks (GCN) and propose a novel Graph Convolution based Re-ranking (GCR) forvisual retrieval tasks via feature propagation. To accelerate computation forlarge-scale retrieval, a decentralized and synchronous feature propagationalgorithm which supports parallel or distributed computing is introduced. Inparticular, the plain GCR is extended for cross-camera retrieval and animproved feature propagation formulation is presented to leverage affinityrelationships across different cameras. It is also extended for video-basedretrieval, and Graph Convolution based Re-ranking for Video (GCRV) is proposedby mathematically deriving a novel profile vector generation method for thetracklet. Without bells and whistles, the proposed approaches achievestate-of-the-art performances on seven benchmark datasets from three differenttasks, i.e., image retrieval, person Re-ID and video-based person Re-ID.</description><author>Yuqi Zhang, Qi Qian, Hongsong Wang, Chong Liu, Weihua Chen, Fan Wang</author><pubDate>Thu, 15 Jun 2023 01:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08792v1</guid></item><item><title>Judging a video by its bitstream cover</title><link>http://arxiv.org/abs/2309.07361v1</link><description>Classifying videos into distinct categories, such as Sport and Music Video,is crucial for multimedia understanding and retrieval, especially in an agewhere an immense volume of video content is constantly being generated.Traditional methods require video decompression to extract pixel-level featureslike color, texture, and motion, thereby increasing computational and storagedemands. Moreover, these methods often suffer from performance degradation inlow-quality videos. We present a novel approach that examines only thepost-compression bitstream of a video to perform classification, eliminatingthe need for bitstream. We validate our approach using a custom-built data setcomprising over 29,000 YouTube video clips, totaling 6,000 hours and spanning11 distinct categories. Our preliminary evaluations indicate precision,accuracy, and recall rates well over 80%. The algorithm operates approximately15,000 times faster than real-time for 30fps videos, outperforming traditionalDynamic Time Warping (DTW) algorithm by six orders of magnitude.</description><author>Yuxing Han, Yunan Ding, Jiangtao Wen, Chen Ye Gan</author><pubDate>Thu, 14 Sep 2023 01:34:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07361v1</guid></item><item><title>Cross-Modal Retrieval for Motion and Text via DopTriple Loss</title><link>http://arxiv.org/abs/2305.04195v3</link><description>Cross-modal retrieval of image-text and video-text is a prominent researcharea in computer vision and natural language processing. However, there hasbeen insufficient attention given to cross-modal retrieval between human motionand text, despite its wide-ranging applicability. To address this gap, weutilize a concise yet effective dual-unimodal transformer encoder for tacklingthis task. Recognizing that overlapping atomic actions in different humanmotion sequences can lead to semantic conflicts between samples, we explore anovel triplet loss function called DropTriple Loss. This loss function discardsfalse negative samples from the negative sample set and focuses on miningremaining genuinely hard negative samples for triplet training, therebyreducing violations they cause. We evaluate our model and approach on theHumanML3D and KIT Motion-Language datasets. On the latest HumanML3D dataset, weachieve a recall of 62.9% for motion retrieval and 71.5% for text retrieval(both based on R@10). The source code for our approach is publicly available athttps://github.com/eanson023/rehamot.</description><author>Sheng Yan, Yang Liu, Haoqiang Wang, Xin Du, Mengyuan Liu, Hong Liu</author><pubDate>Tue, 03 Oct 2023 05:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04195v3</guid></item><item><title>Video Background Music Generation: Dataset, Method and Evaluation</title><link>http://arxiv.org/abs/2211.11248v2</link><description>Music is essential when editing videos, but selecting music manually isdifficult and time-consuming. Thus, we seek to automatically generatebackground music tracks given video input. This is a challenging task since itrequires music-video datasets, efficient architectures for video-to-musicgeneration, and reasonable metrics, none of which currently exist. To closethis gap, we introduce a complete recipe including dataset, benchmark model,and evaluation metric for video background music generation. We present SymMV,a video and symbolic music dataset with various musical annotations. To thebest of our knowledge, it is the first video-music dataset with rich musicalannotations. We also propose a benchmark video background music generationframework named V-MusProd, which utilizes music priors of chords, melody, andaccompaniment along with video-music relations of semantic, color, and motionfeatures. To address the lack of objective metrics for video-musiccorrespondence, we design a retrieval-based metric VMCP built upon a powerfulvideo-music representation learning model. Experiments show that with ourdataset, V-MusProd outperforms the state-of-the-art method in both musicquality and correspondence with videos. We believe our dataset, benchmarkmodel, and evaluation metric will boost the development of video backgroundmusic generation. Our dataset and code are available athttps://github.com/zhuole1025/SymMV.</description><author>Le Zhuo, Zhaokai Wang, Baisen Wang, Yue Liao, Chenxi Bao, Stanley Peng, Songhao Han, Aixi Zhang, Fei Fang, Si Liu</author><pubDate>Fri, 04 Aug 2023 16:57:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11248v2</guid></item><item><title>Counterfactual Cross-modality Reasoning for Weakly Supervised Video Moment Localization</title><link>http://arxiv.org/abs/2308.05648v1</link><description>Video moment localization aims to retrieve the target segment of an untrimmedvideo according to the natural language query. Weakly supervised methods gainsattention recently, as the precise temporal location of the target segment isnot always available. However, one of the greatest challenges encountered bythe weakly supervised method is implied in the mismatch between the video andlanguage induced by the coarse temporal annotations. To refine thevision-language alignment, recent works contrast the cross-modalitysimilarities driven by reconstructing masked queries between positive andnegative video proposals. However, the reconstruction may be influenced by thelatent spurious correlation between the unmasked and the masked parts, whichdistorts the restoring process and further degrades the efficacy of contrastivelearning since the masked words are not completely reconstructed from thecross-modality knowledge. In this paper, we discover and mitigate this spuriouscorrelation through a novel proposed counterfactual cross-modality reasoningmethod. Specifically, we first formulate query reconstruction as an aggregatedcausal effect of cross-modality and query knowledge. Then by introducingcounterfactual cross-modality knowledge into this aggregation, the spuriousimpact of the unmasked part contributing to the reconstruction is explicitlymodeled. Finally, by suppressing the unimodal effect of masked query, we canrectify the reconstructions of video proposals to perform reasonablecontrastive learning. Extensive experimental evaluations demonstrate theeffectiveness of our proposed method. The code is available at\href{https://github.com/sLdZ0306/CCR}{https://github.com/sLdZ0306/CCR}.</description><author>Zezhong Lv, Bing Su, Ji-Rong Wen</author><pubDate>Thu, 10 Aug 2023 16:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05648v1</guid></item><item><title>Transcript to Video: Efficient Clip Sequencing from Texts</title><link>http://arxiv.org/abs/2107.11851v2</link><description>Among numerous videos shared on the web, well-edited ones always attract moreattention. However, it is difficult for inexperienced users to make well-editedvideos because it requires professional expertise and immense manual labor. Tomeet the demands for non-experts, we present Transcript-to-Video -- aweakly-supervised framework that uses texts as input to automatically createvideo sequences from an extensive collection of shots. Specifically, we proposea Content Retrieval Module and a Temporal Coherent Module to learnvisual-language representations and model shot sequencing styles, respectively.For fast inference, we introduce an efficient search strategy for real-timevideo clip sequencing. Quantitative results and user studies demonstrateempirically that the proposed learning framework can retrieve content-relevantshots while creating plausible video sequences in terms of style. Besides, therun-time performance analysis shows that our framework can support real-worldapplications.</description><author>Yu Xiong, Fabian Caba Heilbron, Dahua Lin</author><pubDate>Mon, 20 Nov 2023 02:14:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.11851v2</guid></item><item><title>Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception</title><link>http://arxiv.org/abs/2305.06324v2</link><description>We present Integrated Multimodal Perception (IMP), a simple and scalablemultimodal multi-task training and modeling approach. IMP integrates multimodalinputs including image, video, text, and audio into a single Transformerencoder with minimal modality-specific components. IMP makes use of a noveldesign that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts(MoE) for efficient model and task scaling. We conduct extensive empiricalstudies and reveal the following key insights: 1) Performing gradient descentupdates by alternating on diverse modalities, loss functions, and tasks, withvarying input resolutions, efficiently improves the model. 2) Sparsificationwith MoE on a single modality-agnostic encoder substantially improves theperformance, outperforming dense models that use modality-specific encoders oradditional fusion layers and greatly mitigates the conflicts betweenmodalities. IMP achieves competitive performance on a wide range of downstreamtasks including video classification, image classification, image-text, andvideo-text retrieval. Most notably, we train a sparse IMP-MoE-L variantfocusing on video tasks that achieves new state-of-the-art in zero-shot videoclassification: 77.0% on Kinetics-400, 76.8% on Kinetics-600, and 68.3% onKinetics-700, improving the previous state-of-the-art by +5%, +6.7%, and +5.8%,respectively, while using only 15% of their total training computational cost.</description><author>Hassan Akbari, Dan Kondratyuk, Yin Cui, Rachel Hornung, Huisheng Wang, Hartwig Adam</author><pubDate>Mon, 11 Dec 2023 18:52:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06324v2</guid></item><item><title>Self-Supervised Video Similarity Learning</title><link>http://arxiv.org/abs/2304.03378v2</link><description>We introduce S$^2$VS, a video similarity learning approach withself-supervision. Self-Supervised Learning (SSL) is typically used to traindeep models on a proxy task so as to have strong transferability on targettasks after fine-tuning. Here, in contrast to prior work, SSL is used toperform video similarity learning and address multiple retrieval and detectiontasks at once with no use of labeled data. This is achieved by learning viainstance-discrimination with task-tailored augmentations and the widely usedInfoNCE loss together with an additional loss operating jointly onself-similarity and hard-negative similarity. We benchmark our method on taskswhere video relevance is defined with varying granularity, ranging from videocopies to videos depicting the same incident or event. We learn a singleuniversal model that achieves state-of-the-art performance on all tasks,surpassing previously proposed methods that use labeled data. The code andpretrained models are publicly available at: https://github.com/gkordo/s2vs</description><author>Giorgos Kordopatis-Zilos, Giorgos Tolias, Christos Tzelepis, Ioannis Kompatsiaris, Ioannis Patras, Symeon Papadopoulos</author><pubDate>Fri, 16 Jun 2023 15:11:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.03378v2</guid></item></channel></rss>