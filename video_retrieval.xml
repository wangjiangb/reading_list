<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo retrieval</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 26 Feb 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Event-aware Video Corpus Moment Retrieval</title><link>http://arxiv.org/abs/2402.13566v1</link><description>Video Corpus Moment Retrieval (VCMR) is a practical video retrieval taskfocused on identifying a specific moment within a vast corpus of untrimmedvideos using the natural language query. Existing methods for VCMR typicallyrely on frame-aware video retrieval, calculating similarities between the queryand video frames to rank videos based on maximum frame similarity.However, thisapproach overlooks the semantic structure embedded within the informationbetween frames, namely, the event, a crucial element for human comprehension ofvideos. Motivated by this, we propose EventFormer, a model that explicitlyutilizes events within videos as fundamental units for video retrieval. Themodel extracts event representations through event reasoning and hierarchicalevent encoding. The event reasoning module groups consecutive and visuallysimilar frame representations into events, while the hierarchical eventencoding encodes information at both the frame and event levels. We alsointroduce anchor multi-head self-attenion to encourage Transformer to capturethe relevance of adjacent content in the video. The training of EventFormer isconducted by two-branch contrastive learning and dual optimization for twosub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMobenchmarks show the effectiveness and efficiency of EventFormer in VCMR,achieving new state-of-the-art results. Additionally, the effectiveness ofEventFormer is also validated on partially relevant video retrieval task.</description><author>Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng</author><pubDate>Wed, 21 Feb 2024 06:55:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13566v1</guid></item><item><title>Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement</title><link>http://arxiv.org/abs/2402.13576v1</link><description>Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed atretrieving a relevant moment from a large corpus of untrimmed videos using anatural language text as query. The relevance between the video and query ispartial, mainly evident in two aspects: (1) Scope: The untrimmed video containsinformation-rich frames, and not all are relevant to the query. Strongcorrelation is typically observed only within the relevant moment, emphasizingthe importance of capturing key content. (2) Modality: The relevance of queryto different modalities varies; action descriptions align more with the visualelements, while character conversations are more related to textualinformation. Recognizing and addressing these modality-specific nuances iscrucial for effective retrieval in VCMR. However, existing methods often treatall video contents equally, leading to sub-optimal moment retrieval. We arguethat effectively capturing the partial relevance between the query and video isessential for the VCMR task. To this end, we propose a Partial RelevanceEnhanced Model~(PREM) to improve VCMR. VCMR involves two sub-tasks: videoretrieval and moment localization. To align with their distinct objectives, weimplement specialized partial relevance enhancement strategies. For videoretrieval, we introduce a multi-modal collaborative video retriever, generatingdistinct query representations tailored for different modalities bymodality-specific pooling, ensuring a more effective match. For momentlocalization, we propose the focus-then-fuse moment localizer, utilizingmodality-specific gates to capture essential content, followed by fusingmulti-modal information for moment localization. Experimental results on TVRand DiDeMo datasets show that the proposed model outperforms the baselines,achieving a new state-of-the-art of VCMR.</description><author>Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng</author><pubDate>Wed, 21 Feb 2024 07:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13576v1</guid></item><item><title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</title><link>http://arxiv.org/abs/2401.01505v3</link><description>Reasoning over sports videos for question answering is an important task withnumerous applications, such as player training and information retrieval.However, this task has not been explored due to the lack of relevant datasetsand the challenging nature it presents. Most datasets for video questionanswering (VideoQA) focus mainly on general and coarse-grained understanding ofdaily-life videos, which is not applicable to sports scenarios requiringprofessional action understanding and fine-grained motion analysis. In thispaper, we introduce the first dataset, named Sports-QA, specifically designedfor the sports VideoQA task. The Sports-QA dataset includes various types ofquestions, such as descriptions, chronologies, causalities, and counterfactualconditions, covering multiple sports. Furthermore, to address thecharacteristics of the sports VideoQA task, we propose a new Auto-FocusTransformer (AFT) capable of automatically focusing on particular scales oftemporal information for question answering. We conduct extensive experimentson Sports-QA, including baseline studies and the evaluation of differentmethods. The results demonstrate that our AFT achieves state-of-the-artperformance.</description><author>Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen</author><pubDate>Wed, 14 Feb 2024 23:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01505v3</guid></item><item><title>Tactile-based Object Retrieval From Granular Media</title><link>http://arxiv.org/abs/2402.04536v2</link><description>We introduce GEOTACT, a robotic manipulation method capable of retrievingobjects buried in granular media. This is a challenging task due to the need tointeract with granular media, and doing so based exclusively on tactilefeedback, since a buried object can be completely hidden from vision. Tactilefeedback is in itself challenging in this context, due to ubiquitous contactwith the surrounding media, and the inherent noise level induced by the tactilereadings. To address these challenges, we use a learning method trainedend-to-end with simulated sensor noise. We show that our problem formulationleads to the natural emergence of learned pushing behaviors that themanipulator uses to reduce uncertainty and funnel the object to a stable graspdespite spurious and noisy tactile readings. We also introduce a trainingcurriculum that enables learning these behaviors in simulation, followed byzero-shot transfer to real hardware. To the best of our knowledge, GEOTACT isthe first method to reliably retrieve a number of different objects from agranular environment, doing so on real hardware and with integrated tactilesensing. Videos and additional information can be found athttps://jxu.ai/geotact.</description><author>Jingxi Xu, Yinsen Jia, Dongxiao Yang, Patrick Meng, Xinyue Zhu, Zihan Guo, Shuran Song, Matei Ciocarlie</author><pubDate>Wed, 21 Feb 2024 17:31:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04536v2</guid></item><item><title>DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models</title><link>http://arxiv.org/abs/2402.13181v1</link><description>We propose DINOBot, a novel imitation learning framework for robotmanipulation, which leverages the image-level and pixel-level capabilities offeatures extracted from Vision Transformers trained with DINO. When interactingwith a novel object, DINOBot first uses these features to retrieve the mostvisually similar object experienced during human demonstrations, and then usesthis object to align its end-effector with the novel object to enable effectiveinteraction. Through a series of real-world experiments on everyday tasks, weshow that exploiting both the image-level and pixel-level properties of visionfoundation models enables unprecedented learning efficiency and generalisation.Videos and code are available at https://www.robot-learning.uk/dinobot.</description><author>Norman Di Palo, Edward Johns</author><pubDate>Tue, 20 Feb 2024 17:48:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13181v1</guid></item><item><title>Generative AI in the Construction Industry: A State-of-the-art Analysis</title><link>http://arxiv.org/abs/2402.09939v1</link><description>The construction industry is a vital sector of the global economy, but itfaces many productivity challenges in various processes, such as design,planning, procurement, inspection, and maintenance. Generative artificialintelligence (AI), which can create novel and realistic data or content, suchas text, image, video, or code, based on some input or prior knowledge, offersinnovative and disruptive solutions to address these challenges. However, thereis a gap in the literature on the current state, opportunities, and challengesof generative AI in the construction industry. This study aims to fill this gapby providing a state-of-the-art analysis of generative AI in construction, withthree objectives: (1) to review and categorize the existing and emerginggenerative AI opportunities and challenges in the construction industry; (2) topropose a framework for construction firms to build customized generative AIsolutions using their own data, comprising steps such as data collection,dataset curation, training custom large language model (LLM), model evaluation,and deployment; and (3) to demonstrate the framework via a case study ofdeveloping a generative model for querying contract documents. The results showthat retrieval augmented generation (RAG) improves the baseline LLM by 5.2,9.4, and 4.8% in terms of quality, relevance, and reproducibility. This studyprovides academics and construction professionals with a comprehensive analysisand practical framework to guide the adoption of generative AI techniques toenhance productivity, quality, safety, and sustainability across theconstruction industry.</description><author>Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Tarek Zayed</author><pubDate>Thu, 15 Feb 2024 13:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09939v1</guid></item><item><title>Adversarial Illusions in Multi-Modal Embeddings</title><link>http://arxiv.org/abs/2308.11804v3</link><description>Multi-modal embeddings encode texts, images, sounds, videos, etc., into asingle embedding space, aligning representations across different modalities(e.g., associate an image of a dog with a barking sound). In this paper, weshow that multi-modal embeddings can be vulnerable to an attack we call"adversarial illusions." Given an image or a sound, an adversary can perturb itto make its embedding close to an arbitrary, adversary-chosen input in anothermodality. These attacks are cross-modal and targeted: the adversary is free to alignany image and any sound with any target of his choice. Adversarial illusionsexploit proximity in the embedding space and are thus agnostic to downstreamtasks and modalities, enabling a wholesale compromise of current and futuredownstream tasks and modalities not available to the adversary. Using ImageBindand AudioCLIP embeddings, we demonstrate how adversarially aligned inputs,generated without knowledge of specific downstream tasks, mislead imagegeneration, text generation, zero-shot classification, and audio retrieval. We investigate transferability of illusions across different embeddings anddevelop a black-box version of our method that we use to demonstrate the firstadversarial alignment attack on Amazon's commercial, proprietary Titanembedding. Finally, we analyze countermeasures and evasion attacks.</description><author>Tingwei Zhang, Rishi Jha, Eugene Bagdasaryan, Vitaly Shmatikov</author><pubDate>Sat, 17 Feb 2024 02:26:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11804v3</guid></item></channel></rss>