<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo retrieval</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 20 Feb 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</title><link>http://arxiv.org/abs/2401.01505v3</link><description>Reasoning over sports videos for question answering is an important task withnumerous applications, such as player training and information retrieval.However, this task has not been explored due to the lack of relevant datasetsand the challenging nature it presents. Most datasets for video questionanswering (VideoQA) focus mainly on general and coarse-grained understanding ofdaily-life videos, which is not applicable to sports scenarios requiringprofessional action understanding and fine-grained motion analysis. In thispaper, we introduce the first dataset, named Sports-QA, specifically designedfor the sports VideoQA task. The Sports-QA dataset includes various types ofquestions, such as descriptions, chronologies, causalities, and counterfactualconditions, covering multiple sports. Furthermore, to address thecharacteristics of the sports VideoQA task, we propose a new Auto-FocusTransformer (AFT) capable of automatically focusing on particular scales oftemporal information for question answering. We conduct extensive experimentson Sports-QA, including baseline studies and the evaluation of differentmethods. The results demonstrate that our AFT achieves state-of-the-artperformance.</description><author>Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen</author><pubDate>Wed, 14 Feb 2024 23:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01505v3</guid></item><item><title>Generative AI in the Construction Industry: A State-of-the-art Analysis</title><link>http://arxiv.org/abs/2402.09939v1</link><description>The construction industry is a vital sector of the global economy, but itfaces many productivity challenges in various processes, such as design,planning, procurement, inspection, and maintenance. Generative artificialintelligence (AI), which can create novel and realistic data or content, suchas text, image, video, or code, based on some input or prior knowledge, offersinnovative and disruptive solutions to address these challenges. However, thereis a gap in the literature on the current state, opportunities, and challengesof generative AI in the construction industry. This study aims to fill this gapby providing a state-of-the-art analysis of generative AI in construction, withthree objectives: (1) to review and categorize the existing and emerginggenerative AI opportunities and challenges in the construction industry; (2) topropose a framework for construction firms to build customized generative AIsolutions using their own data, comprising steps such as data collection,dataset curation, training custom large language model (LLM), model evaluation,and deployment; and (3) to demonstrate the framework via a case study ofdeveloping a generative model for querying contract documents. The results showthat retrieval augmented generation (RAG) improves the baseline LLM by 5.2,9.4, and 4.8% in terms of quality, relevance, and reproducibility. This studyprovides academics and construction professionals with a comprehensive analysisand practical framework to guide the adoption of generative AI techniques toenhance productivity, quality, safety, and sustainability across theconstruction industry.</description><author>Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Tarek Zayed</author><pubDate>Thu, 15 Feb 2024 13:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09939v1</guid></item><item><title>Adversarial Illusions in Multi-Modal Embeddings</title><link>http://arxiv.org/abs/2308.11804v3</link><description>Multi-modal embeddings encode texts, images, sounds, videos, etc., into asingle embedding space, aligning representations across different modalities(e.g., associate an image of a dog with a barking sound). In this paper, weshow that multi-modal embeddings can be vulnerable to an attack we call"adversarial illusions." Given an image or a sound, an adversary can perturb itto make its embedding close to an arbitrary, adversary-chosen input in anothermodality. These attacks are cross-modal and targeted: the adversary is free to alignany image and any sound with any target of his choice. Adversarial illusionsexploit proximity in the embedding space and are thus agnostic to downstreamtasks and modalities, enabling a wholesale compromise of current and futuredownstream tasks and modalities not available to the adversary. Using ImageBindand AudioCLIP embeddings, we demonstrate how adversarially aligned inputs,generated without knowledge of specific downstream tasks, mislead imagegeneration, text generation, zero-shot classification, and audio retrieval. We investigate transferability of illusions across different embeddings anddevelop a black-box version of our method that we use to demonstrate the firstadversarial alignment attack on Amazon's commercial, proprietary Titanembedding. Finally, we analyze countermeasures and evasion attacks.</description><author>Tingwei Zhang, Rishi Jha, Eugene Bagdasaryan, Vitaly Shmatikov</author><pubDate>Sat, 17 Feb 2024 02:26:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11804v3</guid></item></channel></rss>