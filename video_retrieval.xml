<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo retrieval</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 15 Mar 2024 06:00:28 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Event-aware Video Corpus Moment Retrieval</title><link>http://arxiv.org/abs/2402.13566v1</link><description>Video Corpus Moment Retrieval (VCMR) is a practical video retrieval taskfocused on identifying a specific moment within a vast corpus of untrimmedvideos using the natural language query. Existing methods for VCMR typicallyrely on frame-aware video retrieval, calculating similarities between the queryand video frames to rank videos based on maximum frame similarity.However, thisapproach overlooks the semantic structure embedded within the informationbetween frames, namely, the event, a crucial element for human comprehension ofvideos. Motivated by this, we propose EventFormer, a model that explicitlyutilizes events within videos as fundamental units for video retrieval. Themodel extracts event representations through event reasoning and hierarchicalevent encoding. The event reasoning module groups consecutive and visuallysimilar frame representations into events, while the hierarchical eventencoding encodes information at both the frame and event levels. We alsointroduce anchor multi-head self-attenion to encourage Transformer to capturethe relevance of adjacent content in the video. The training of EventFormer isconducted by two-branch contrastive learning and dual optimization for twosub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMobenchmarks show the effectiveness and efficiency of EventFormer in VCMR,achieving new state-of-the-art results. Additionally, the effectiveness ofEventFormer is also validated on partially relevant video retrieval task.</description><author>Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng</author><pubDate>Wed, 21 Feb 2024 06:55:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13566v1</guid></item><item><title>Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement</title><link>http://arxiv.org/abs/2402.13576v1</link><description>Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed atretrieving a relevant moment from a large corpus of untrimmed videos using anatural language text as query. The relevance between the video and query ispartial, mainly evident in two aspects: (1) Scope: The untrimmed video containsinformation-rich frames, and not all are relevant to the query. Strongcorrelation is typically observed only within the relevant moment, emphasizingthe importance of capturing key content. (2) Modality: The relevance of queryto different modalities varies; action descriptions align more with the visualelements, while character conversations are more related to textualinformation. Recognizing and addressing these modality-specific nuances iscrucial for effective retrieval in VCMR. However, existing methods often treatall video contents equally, leading to sub-optimal moment retrieval. We arguethat effectively capturing the partial relevance between the query and video isessential for the VCMR task. To this end, we propose a Partial RelevanceEnhanced Model~(PREM) to improve VCMR. VCMR involves two sub-tasks: videoretrieval and moment localization. To align with their distinct objectives, weimplement specialized partial relevance enhancement strategies. For videoretrieval, we introduce a multi-modal collaborative video retriever, generatingdistinct query representations tailored for different modalities bymodality-specific pooling, ensuring a more effective match. For momentlocalization, we propose the focus-then-fuse moment localizer, utilizingmodality-specific gates to capture essential content, followed by fusingmulti-modal information for moment localization. Experimental results on TVRand DiDeMo datasets show that the proposed model outperforms the baselines,achieving a new state-of-the-art of VCMR.</description><author>Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng</author><pubDate>Wed, 21 Feb 2024 07:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13576v1</guid></item><item><title>DiffusionVMR: Diffusion Model for Joint Video Moment Retrieval and Highlight Detection</title><link>http://arxiv.org/abs/2308.15109v2</link><description>Video moment retrieval and highlight detection have received attention in thecurrent era of video content proliferation, aiming to localize moments andestimate clip relevances based on user-specific queries. Given that the videocontent is continuous in time, there is often a lack of clear boundariesbetween temporal events in a video. This boundary ambiguity makes itchallenging for the model to learn text-video clip correspondences, resultingin the subpar performance of existing methods in predicting target segments. Toalleviate this problem, we propose to solve the two tasks jointly from theperspective of denoising generation. Moreover, the target boundary can belocalized clearly by iterative refinement from coarse to fine. Specifically, anovel framework, DiffusionVMR, is proposed to redefine the two tasks as aunified conditional denoising generation process by combining the diffusionmodel. During training, Gaussian noise is added to corrupt the ground truth,with noisy candidates produced as input. The model is trained to reverse thisnoise addition process. In the inference phase, DiffusionVMR initiates directlyfrom Gaussian noise and progressively refines the proposals from the noise tothe meaningful output. Notably, the proposed DiffusionVMR inherits theadvantages of diffusion models that allow for iteratively refined resultsduring inference, enhancing the boundary transition from coarse to fine.Furthermore, the training and inference of DiffusionVMR are decoupled. Anarbitrary setting can be used in DiffusionVMR during inference withoutconsistency with the training phase. Extensive experiments conducted on fivewidely-used benchmarks (i.e., QVHighlight, Charades-STA, TACoS,YouTubeHighlights and TVSum) across two tasks (moment retrieval and/orhighlight detection) demonstrate the effectiveness and flexibility of theproposed DiffusionVMR.</description><author>Henghao Zhao, Kevin Qinghong Lin, Rui Yan, Zechao Li</author><pubDate>Sat, 02 Mar 2024 12:34:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15109v2</guid></item><item><title>MuLTI: Efficient Video-and-Language Understanding with Text-Guided MultiWay-Sampler and Multiple Choice Modeling</title><link>http://arxiv.org/abs/2303.05707v2</link><description>Video-and-language understanding has a variety of applications in theindustry, such as video question answering, text-video retrieval, andmulti-label classification. Existing video-and-language understanding methodsgenerally adopt heavy multi-modal encoders and feature fusion modules, whichconsume high computational costs. Specially, they have difficulty dealing withdense video frames or long text prevalent in industrial applications. Thispaper proposes MuLTI, a highly accurate and efficient video-and-languageunderstanding model that achieves efficient and effective feature fusion andrapid adaptation to downstream tasks. Specifically, we design a Text-GuidedMultiWay-Sampler based on adapt-pooling residual mapping and self-attentionmodules to sample long sequences and fuse multi-modal features, which reducesthe computational costs and addresses performance degradation caused byprevious samplers. Therefore, MuLTI can handle longer sequences with limitedcomputational costs. Then, to further enhance the model's performance and fillin the lack of pretraining tasks in the video question answering, we propose anew pretraining task named Multiple Choice Modeling. This task bridges the gapbetween pretraining and downstream tasks and improves the model's ability toalign video and text features. Benefiting from the efficient feature fusionmodule and the new pretraining task, MuLTI achieves state-of-the-artperformance on multiple datasets. Implementation and pretrained models will bereleased.</description><author>Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi</author><pubDate>Fri, 01 Mar 2024 02:32:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.05707v2</guid></item><item><title>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</title><link>http://arxiv.org/abs/2402.19479v1</link><description>The quality of the data and annotation upper-bounds the quality of adownstream model. While there exist large text corpora and image-text pairs,high-quality video-text data is much harder to collect. First of all, manuallabeling is more time-consuming, as it requires an annotator to watch an entirevideo. Second, videos have a temporal dimension, consisting of several scenesstacked together, and showing multiple actions. Accordingly, to establish avideo dataset with high-quality captions, we propose an automatic approachleveraging multimodal inputs, such as textual video description, subtitles, andindividual video frames. Specifically, we curate 3.8M high-resolution videosfrom the publicly available HD-VILA-100M dataset. We then split them intosemantically consistent video clips, and apply multiple cross-modality teachermodels to obtain captions for each video. Next, we finetune a retrieval modelon a small subset where the best caption of each video is manually selected andthen employ the model in the whole dataset to select the best caption as theannotation. In this way, we get 70M videos paired with high-quality textcaptions. We dub the dataset as Panda-70M. We show the value of the proposeddataset on three downstream tasks: video captioning, video and text retrieval,and text-driven video generation. The models trained on the proposed data scoresubstantially better on the majority of metrics across all the tasks.</description><author>Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, Sergey Tulyakov</author><pubDate>Thu, 29 Feb 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19479v1</guid></item><item><title>ProS: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval</title><link>http://arxiv.org/abs/2312.12478v3</link><description>The goal of Universal Cross-Domain Retrieval (UCDR) is to achieve robustperformance in generalized test scenarios, wherein data may belong to strictlyunknown domains and categories during training. Recently, pre-trained modelswith prompt tuning have shown strong generalization capabilities and attainednoteworthy achievements in various downstream tasks, such as few-shot learningand video-text retrieval. However, applying them directly to UCDR may notsufficiently to handle both domain shift (i.e., adapting to unfamiliar domains)and semantic shift (i.e., transferring to unknown categories). To this end, wepropose \textbf{Pro}mpting-to-\textbf{S}imulate (ProS), the first method toapply prompt tuning for UCDR. ProS employs a two-step process to simulateContent-aware Dynamic Prompts (CaDP) which can impact models to producegeneralized features for UCDR. Concretely, in Prompt Units Learning stage, weintroduce two Prompt Units to individually capture domain and semanticknowledge in a mask-and-align way. Then, in Context-aware Simulator Learningstage, we train a Content-aware Prompt Simulator under a simulated testscenarios to produce the corresponding CaDP. Extensive experiments conducted onthree benchmark datasets show that our method achieves new state-of-the-artperformance without bringing excessive parameters. Our method is publiclyavailable at https://github.com/fangkaipeng/ProS.</description><author>Kaipeng Fang, Jingkuan Song, Lianli Gao, Pengpeng Zeng, Zhi-Qi Cheng, Xiyao Li, Heng Tao Shen</author><pubDate>Thu, 29 Feb 2024 12:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.12478v3</guid></item><item><title>World Model on Million-Length Video And Language With Blockwise RingAttention</title><link>http://arxiv.org/abs/2402.08268v2</link><description>Current language models fall short in understanding aspects of the world noteasily described in words, and struggle with complex, long-form tasks. Videosequences offer valuable temporal information absent in language and staticimages, making them attractive for joint modeling with language. Such modelscould develop a understanding of both human textual knowledge and the physicalworld, enabling broader AI capabilities for assisting humans. However, learningfrom millions of tokens of video and language sequences poses challenges due tomemory constraints, computational complexity, and limited datasets. To addressthese challenges, we curate a large dataset of diverse videos and books,utilize the Blockwise RingAttention technique to scalably train on longsequences, and gradually increase context size from 4K to 1M tokens. This papermakes the following contributions: (a) Largest context size neural network: Wetrain one of the largest context size transformers on long video and languagesequences, setting new benchmarks in difficult retrieval tasks and long videounderstanding. (b) Solutions for overcoming vision-language trainingchallenges, including using masked sequence packing for mixing differentsequence lengths, loss weighting to balance language and vision, andmodel-generated QA dataset for long sequence chat. (c) A highly-optimizedimplementation with RingAttention, Blockwise Transformers, masked sequencepacking, and other key features for training on millions-length multimodalsequences. (d) Fully open-sourced a family of 7B parameter models capable ofprocessing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM,LWM-Chat) of over 1M tokens. This work paves the way for training on massivedatasets of long video and language to develop understanding of both humanknowledge and the multimodal world, and broader capabilities.</description><author>Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel</author><pubDate>Thu, 14 Mar 2024 08:17:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08268v2</guid></item><item><title>Leveraging Compressed Frame Sizes For Ultra-Fast Video Classification</title><link>http://arxiv.org/abs/2403.08580v1</link><description>Classifying videos into distinct categories, such as Sport and Music Video,is crucial for multimedia understanding and retrieval, especially when animmense volume of video content is being constantly generated. Traditionalmethods require video decompression to extract pixel-level features like color,texture, and motion, thereby increasing computational and storage demands.Moreover, these methods often suffer from performance degradation inlow-quality videos. We present a novel approach that examines only thepost-compression bitstream of a video to perform classification, eliminatingthe need for bitstream decoding. To validate our approach, we built acomprehensive data set comprising over 29,000 YouTube video clips, totaling6,000 hours and spanning 11 distinct categories. Our evaluations indicateprecision, accuracy, and recall rates consistently above 80%, many exceeding90%, and some reaching 99%. The algorithm operates approximately 15,000 timesfaster than real-time for 30fps videos, outperforming traditional Dynamic TimeWarping (DTW) algorithm by seven orders of magnitude.</description><author>Yuxing Han, Yunan Ding, Chen Ye Gan, Jiangtao Wen</author><pubDate>Wed, 13 Mar 2024 15:35:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08580v1</guid></item><item><title>Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT</title><link>http://arxiv.org/abs/2402.15746v1</link><description>With the rise of short video platforms represented by TikTok, the trend ofusers expressing their creativity through photos and videos has increaseddramatically. However, ordinary users lack the professional skills to producehigh-quality videos using professional creation software. To meet the demandfor intelligent and user-friendly video creation tools, we propose the DynamicVisual Composition (DVC) task, an interesting and challenging task that aims toautomatically integrate various media elements based on user requirements andcreate storytelling videos. We propose an Intelligent Director framework,utilizing LENS to generate descriptions for images and video frames andcombining ChatGPT to generate coherent captions while recommending appropriatemusic names. Then, the best-matched music is obtained through music retrieval.Then, materials such as captions, images, videos, and music are integrated toseamlessly synthesize the video. Finally, we apply AnimeGANv2 for styletransfer. We construct UCF101-DVC and Personal Album datasets and verified theeffectiveness of our framework in solving DVC through qualitative andquantitative comparisons, along with user studies, demonstrating itssubstantial potential.</description><author>Sixiao Zheng, Jingyang Huo, Yu Wang, Yanwei Fu</author><pubDate>Sat, 24 Feb 2024 06:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15746v1</guid></item><item><title>Tri-Modal Motion Retrieval by Learning a Joint Embedding Space</title><link>http://arxiv.org/abs/2403.00691v1</link><description>Information retrieval is an ever-evolving and crucial research domain. Thesubstantial demand for high-quality human motion data especially in onlineacquirement has led to a surge in human motion research works. Prior works havemainly concentrated on dual-modality learning, such as text and motion tasks,but three-modality learning has been rarely explored. Intuitively, an extraintroduced modality can enrich a model's application scenario, and moreimportantly, an adequate choice of the extra modality can also act as anintermediary and enhance the alignment between the other two disparatemodalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtionalignment), a novel framework for three-modality learning integratinghuman-centric videos as an additional modality, thereby effectively bridgingthe gap between text and motion. Moreover, our approach leverages a speciallydesigned attention mechanism to foster enhanced alignment and synergisticeffects among text, video, and motion modalities. Empirically, our results onthe HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-artperformance in various motion-related cross-modal retrieval tasks, includingtext-to-motion, motion-to-text, video-to-motion and motion-to-video.</description><author>Kangning Yin, Shihao Zou, Yuxuan Ge, Zheng Tian</author><pubDate>Fri, 01 Mar 2024 17:23:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00691v1</guid></item><item><title>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title><link>http://arxiv.org/abs/2403.05530v1</link><description>In this report, we present the latest model of the Gemini family, Gemini 1.5Pro, a highly compute-efficient multimodal mixture-of-experts model capable ofrecalling and reasoning over fine-grained information from millions of tokensof context, including multiple long documents and hours of video and audio.Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasksacross modalities, improves the state-of-the-art in long-document QA,long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra'sstate-of-the-art performance across a broad set of benchmarks. Studying thelimits of Gemini 1.5 Pro's long-context ability, we find continued improvementin next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10Mtokens, a generational leap over existing models such as Claude 2.1 (200k) andGPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of largelanguage models at the frontier; when given a grammar manual for Kalamang, alanguage with fewer than 200 speakers worldwide, the model learns to translateEnglish to Kalamang at a similar level to a person who learned from the samecontent.</description><author>Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezene</author><pubDate>Fri, 08 Mar 2024 18:54:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05530v1</guid></item><item><title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</title><link>http://arxiv.org/abs/2401.01505v3</link><description>Reasoning over sports videos for question answering is an important task withnumerous applications, such as player training and information retrieval.However, this task has not been explored due to the lack of relevant datasetsand the challenging nature it presents. Most datasets for video questionanswering (VideoQA) focus mainly on general and coarse-grained understanding ofdaily-life videos, which is not applicable to sports scenarios requiringprofessional action understanding and fine-grained motion analysis. In thispaper, we introduce the first dataset, named Sports-QA, specifically designedfor the sports VideoQA task. The Sports-QA dataset includes various types ofquestions, such as descriptions, chronologies, causalities, and counterfactualconditions, covering multiple sports. Furthermore, to address thecharacteristics of the sports VideoQA task, we propose a new Auto-FocusTransformer (AFT) capable of automatically focusing on particular scales oftemporal information for question answering. We conduct extensive experimentson Sports-QA, including baseline studies and the evaluation of differentmethods. The results demonstrate that our AFT achieves state-of-the-artperformance.</description><author>Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen</author><pubDate>Wed, 14 Feb 2024 23:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01505v3</guid></item><item><title>Tactile-based Object Retrieval From Granular Media</title><link>http://arxiv.org/abs/2402.04536v2</link><description>We introduce GEOTACT, a robotic manipulation method capable of retrievingobjects buried in granular media. This is a challenging task due to the need tointeract with granular media, and doing so based exclusively on tactilefeedback, since a buried object can be completely hidden from vision. Tactilefeedback is in itself challenging in this context, due to ubiquitous contactwith the surrounding media, and the inherent noise level induced by the tactilereadings. To address these challenges, we use a learning method trainedend-to-end with simulated sensor noise. We show that our problem formulationleads to the natural emergence of learned pushing behaviors that themanipulator uses to reduce uncertainty and funnel the object to a stable graspdespite spurious and noisy tactile readings. We also introduce a trainingcurriculum that enables learning these behaviors in simulation, followed byzero-shot transfer to real hardware. To the best of our knowledge, GEOTACT isthe first method to reliably retrieve a number of different objects from agranular environment, doing so on real hardware and with integrated tactilesensing. Videos and additional information can be found athttps://jxu.ai/geotact.</description><author>Jingxi Xu, Yinsen Jia, Dongxiao Yang, Patrick Meng, Xinyue Zhu, Zihan Guo, Shuran Song, Matei Ciocarlie</author><pubDate>Wed, 21 Feb 2024 17:31:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04536v2</guid></item><item><title>Text-based Person Search in Full Images via Semantic-Driven Proposal Generation</title><link>http://arxiv.org/abs/2109.12965v3</link><description>Finding target persons in full scene images with a query of text descriptionhas important practical applications in intelligent video surveillance.However,different from the real-world scenarios where the bounding boxes are notavailable, existing text-based person retrieval methods mainly focus on thecross modal matching between the query text descriptions and the gallery ofcropped pedestrian images. To close the gap, we study the problem of text-basedperson search in full images by proposing a new end-to-end learning frameworkwhich jointly optimize the pedestrian detection, identification andvisual-semantic feature embedding tasks. To take full advantage of the querytext, the semantic features are leveraged to instruct the Region ProposalNetwork to pay more attention to the text-described proposals. Besides, across-scale visual-semantic embedding mechanism is utilized to improve theperformance. To validate the proposed method, we collect and annotate twolarge-scale benchmark datasets based on the widely adopted image-based personsearch datasets CUHK-SYSU and PRW. Comprehensive experiments are conducted onthe two datasets and compared with the baseline methods, our method achievesthe state-of-the-art performance.</description><author>Shizhou Zhang, De Cheng, Wenlong Luo, Yinghui Xing, Duo Long, Hao Li, Kai Niu, Guoqiang Liang, Yanning Zhang</author><pubDate>Sun, 25 Feb 2024 10:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.12965v3</guid></item><item><title>DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models</title><link>http://arxiv.org/abs/2402.13181v1</link><description>We propose DINOBot, a novel imitation learning framework for robotmanipulation, which leverages the image-level and pixel-level capabilities offeatures extracted from Vision Transformers trained with DINO. When interactingwith a novel object, DINOBot first uses these features to retrieve the mostvisually similar object experienced during human demonstrations, and then usesthis object to align its end-effector with the novel object to enable effectiveinteraction. Through a series of real-world experiments on everyday tasks, weshow that exploiting both the image-level and pixel-level properties of visionfoundation models enables unprecedented learning efficiency and generalisation.Videos and code are available at https://www.robot-learning.uk/dinobot.</description><author>Norman Di Palo, Edward Johns</author><pubDate>Tue, 20 Feb 2024 17:48:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13181v1</guid></item><item><title>Generative AI in the Construction Industry: A State-of-the-art Analysis</title><link>http://arxiv.org/abs/2402.09939v1</link><description>The construction industry is a vital sector of the global economy, but itfaces many productivity challenges in various processes, such as design,planning, procurement, inspection, and maintenance. Generative artificialintelligence (AI), which can create novel and realistic data or content, suchas text, image, video, or code, based on some input or prior knowledge, offersinnovative and disruptive solutions to address these challenges. However, thereis a gap in the literature on the current state, opportunities, and challengesof generative AI in the construction industry. This study aims to fill this gapby providing a state-of-the-art analysis of generative AI in construction, withthree objectives: (1) to review and categorize the existing and emerginggenerative AI opportunities and challenges in the construction industry; (2) topropose a framework for construction firms to build customized generative AIsolutions using their own data, comprising steps such as data collection,dataset curation, training custom large language model (LLM), model evaluation,and deployment; and (3) to demonstrate the framework via a case study ofdeveloping a generative model for querying contract documents. The results showthat retrieval augmented generation (RAG) improves the baseline LLM by 5.2,9.4, and 4.8% in terms of quality, relevance, and reproducibility. This studyprovides academics and construction professionals with a comprehensive analysisand practical framework to guide the adoption of generative AI techniques toenhance productivity, quality, safety, and sustainability across theconstruction industry.</description><author>Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Tarek Zayed</author><pubDate>Thu, 15 Feb 2024 13:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09939v1</guid></item><item><title>Adversarial Illusions in Multi-Modal Embeddings</title><link>http://arxiv.org/abs/2308.11804v3</link><description>Multi-modal embeddings encode texts, images, sounds, videos, etc., into asingle embedding space, aligning representations across different modalities(e.g., associate an image of a dog with a barking sound). In this paper, weshow that multi-modal embeddings can be vulnerable to an attack we call"adversarial illusions." Given an image or a sound, an adversary can perturb itto make its embedding close to an arbitrary, adversary-chosen input in anothermodality. These attacks are cross-modal and targeted: the adversary is free to alignany image and any sound with any target of his choice. Adversarial illusionsexploit proximity in the embedding space and are thus agnostic to downstreamtasks and modalities, enabling a wholesale compromise of current and futuredownstream tasks and modalities not available to the adversary. Using ImageBindand AudioCLIP embeddings, we demonstrate how adversarially aligned inputs,generated without knowledge of specific downstream tasks, mislead imagegeneration, text generation, zero-shot classification, and audio retrieval. We investigate transferability of illusions across different embeddings anddevelop a black-box version of our method that we use to demonstrate the firstadversarial alignment attack on Amazon's commercial, proprietary Titanembedding. Finally, we analyze countermeasures and evasion attacks.</description><author>Tingwei Zhang, Rishi Jha, Eugene Bagdasaryan, Vitaly Shmatikov</author><pubDate>Sat, 17 Feb 2024 02:26:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11804v3</guid></item></channel></rss>