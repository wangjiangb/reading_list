<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo segmentation</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 27 Jul 2023 06:00:24 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation</title><link>http://arxiv.org/abs/2307.07812v1</link><description>Few-shot video segmentation is the task of delineating a specific novel classin a query video using few labelled support images. Typical approaches comparesupport and query features while limiting comparisons to a single feature layerand thereby ignore potentially valuable information. We present a meta-learnedMultiscale Memory Comparator (MMC) for few-shot video segmentation thatcombines information across scales within a transformer decoder. Typicalmultiscale transformer decoders for segmentation tasks learn a compressedrepresentation, their queries, through information exchange across scales.Unlike previous work, we instead preserve the detailed feature maps duringacross scale information exchange via a multiscale memory transformer decodingto reduce confusion between the background and novel class. Integral to theapproach, we investigate multiple forms of information exchange across scalesin different tasks and provide insights with empirical evidence on which to usein each task. The overall comparisons among query and support features benefitfrom both rich semantics and precise localization. We demonstrate our approachprimarily on few-shot video object segmentation and an adapted version on thefully supervised counterpart. In all cases, our approach outperforms thebaseline and yields state-of-the-art performance. Our code is publiclyavailable at https://github.com/MSiam/MMC-MultiscaleMemory.</description><author>Mennatullah Siam, Rezaul Karim, He Zhao, Richard Wildes</author><pubDate>Sat, 15 Jul 2023 15:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07812v1</guid></item><item><title>3rd Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.06753v1</link><description>In order to deal with the task of video panoptic segmentation in the wild, wepropose a robust integrated video panoptic segmentation solution. In oursolution, we regard the video panoptic segmentation task as a segmentationtarget querying task, represent both semantic and instance targets as a set ofqueries, and then combine these queries with video features extracted by neuralnetworks to predict segmentation masks. In order to improve the learningaccuracy and convergence speed of the solution, we add additional tasks ofvideo semantic segmentation and video instance segmentation for joint training.In addition, we also add an additional image semantic segmentation model tofurther improve the performance of semantic classes. In addition, we also addsome additional operations to improve the robustness of the model. Extensiveexperiments on the VIPSeg dataset show that the proposed solution achievesstate-of-the-art performance with 50.04\% VPQ on the VIPSeg test set, which is3rd place on the video panoptic segmentation track of the PVUW Challenge 2023.</description><author>Jinming Su, Wangwang Yang, Junfeng Luo, Xiaolin Wei</author><pubDate>Sun, 11 Jun 2023 20:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06753v1</guid></item><item><title>Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal Feature Affinity Learning for Robust Video Segmentation</title><link>http://arxiv.org/abs/2307.05898v1</link><description>Noisy label problems are inevitably in existence within medical imagesegmentation causing severe performance degradation. Previous segmentationmethods for noisy label problems only utilize a single image while thepotential of leveraging the correlation between images has been overlooked.Especially for video segmentation, adjacent frames contain rich contextualinformation beneficial in cognizing noisy labels. Based on two insights, wepropose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework toresolve noisy-labeled medical video segmentation issues. First, we argue thesequential prior of videos is an effective reference, i.e., pixel-levelfeatures from adjacent frames are close in distance for the same class and farin distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) isdevised to indicate possible noisy labels by evaluating the affinity betweenpixels in two adjacent frames. We also notice that the noise distributionexhibits considerable variations across video, image, and pixel levels. In thisway, we introduce Multi-Scale Supervision (MSS) to supervise the network fromthree different perspectives by re-weighting and refining the samples. Thisdesign enables the network to concentrate on clean samples in a coarse-to-finemanner. Experiments with both synthetic and real-world label noise demonstratethat our method outperforms recent state-of-the-art robust segmentationapproaches. Code is available at https://github.com/BeileiCui/MS-TFAL.</description><author>Beilei Cui, Minqing Zhang, Mengya Xu, An Wang, Wu Yuan, Hongliang Ren</author><pubDate>Wed, 12 Jul 2023 05:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05898v1</guid></item><item><title>Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation</title><link>http://arxiv.org/abs/2305.16318v1</link><description>Recently, video object segmentation (VOS) referred by multi-modal signals,e.g., language and audio, has evoked increasing attention in both industry andacademia. It is challenging for exploring the semantic alignment withinmodalities and the visual correspondence across frames. However, existingmethods adopt separate network architectures for different modalities, andneglect the inter-frame temporal interaction with references. In this paper, wepropose MUTR, a Multi-modal Unified Temporal transformer for Referring videoobject segmentation. With a unified framework for the first time, MUTR adopts aDETR-style transformer and is capable of segmenting video objects designated byeither text or audio reference. Specifically, we introduce two strategies tofully explore the temporal relations between videos and multi-modal signals.Firstly, for low-level temporal aggregation before the transformer, we enablethe multi-modal references to capture multi-scale visual cues from consecutivevideo frames. This effectively endows the text or audio signals with temporalknowledge and boosts the semantic alignment between modalities. Secondly, forhigh-level temporal interaction after the transformer, we conduct inter-framefeature communication for different object embeddings, contributing to betterobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS andAVSBench datasets with respective text and audio references, MUTR achieves+4.2% and +4.2% J&amp;F improvements to state-of-the-art methods, demonstrating oursignificance for unified multi-modal VOS. Code is released athttps://github.com/OpenGVLab/MUTR.</description><author>Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Zhongjiang He, Peng Gao</author><pubDate>Thu, 25 May 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16318v1</guid></item><item><title>A spatio-temporal network for video semantic segmentation in surgical videos</title><link>http://arxiv.org/abs/2306.11052v1</link><description>Semantic segmentation in surgical videos has applications in intra-operativeguidance, post-operative analytics and surgical education. Segmentation modelsneed to provide accurate and consistent predictions since temporallyinconsistent identification of anatomical structures can impair usability andhinder patient safety. Video information can alleviate these challenges leadingto reliable models suitable for clinical use. We propose a novel architecturefor modelling temporal relationships in videos. The proposed model includes aspatio-temporal decoder to enable video semantic segmentation by improvingtemporal consistency across frames. The encoder processes individual frameswhilst the decoder processes a temporal batch of adjacent frames. The proposeddecoder can be used on top of any segmentation encoder to improve temporalconsistency. Model performance was evaluated on the CholecSeg8k dataset and aprivate dataset of robotic Partial Nephrectomy procedures. Segmentationperformance was improved when the temporal decoder was applied across bothdatasets. The proposed model also displayed improvements in temporalconsistency.</description><author>Maria Grammatikopoulou, Ricardo Sanchez-Matilla, Felix Bragman, David Owen, Lucy Culshaw, Karen Kerr, Danail Stoyanov, Imanol Luengo</author><pubDate>Mon, 19 Jun 2023 17:36:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11052v1</guid></item><item><title>1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.04091v1</link><description>Video panoptic segmentation is a challenging task that serves as thecornerstone of numerous downstream applications, including video editing andautonomous driving. We believe that the decoupling strategy proposed by DVISenables more effective utilization of temporal information for both "thing" and"stuff" objects. In this report, we successfully validated the effectiveness ofthe decoupling strategy in video panoptic segmentation. Finally, our methodachieved a VPQ score of 51.4 and 53.7 in the development and test phases,respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUWChallenge. The code is available at https://github.com/zhang-tao-whu/DVIS</description><author>Tao Zhang, Xingye Tian, Haoran Wei, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Wed, 07 Jun 2023 02:24:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04091v1</guid></item><item><title>1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.04091v2</link><description>Video panoptic segmentation is a challenging task that serves as thecornerstone of numerous downstream applications, including video editing andautonomous driving. We believe that the decoupling strategy proposed by DVISenables more effective utilization of temporal information for both "thing" and"stuff" objects. In this report, we successfully validated the effectiveness ofthe decoupling strategy in video panoptic segmentation. Finally, our methodachieved a VPQ score of 51.4 and 53.7 in the development and test phases,respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUWChallenge. The code is available at https://github.com/zhang-tao-whu/DVIS</description><author>Tao Zhang, Xingye Tian, Haoran Wei, Yu Wu, Shunping Ji, Xuebo Wang, Xin Tao, Yuan Zhang, Pengfei Wan</author><pubDate>Thu, 08 Jun 2023 09:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04091v2</guid></item><item><title>Tamed Warping Network for High-Resolution Semantic Video Segmentation</title><link>http://arxiv.org/abs/2005.01344v4</link><description>Recent approaches for fast semantic video segmentation have reducedredundancy by warping feature maps across adjacent frames, greatly speeding upthe inference phase. However, the accuracy drops seriously owing to the errorsincurred by warping. In this paper, we propose a novel framework and design asimple and effective correction stage after warping. Specifically, we build anon-key-frame CNN, fusing warped context features with current spatial details.Based on the feature fusion, our Context Feature Rectification~(CFR) modulelearns the model's difference from a per-frame model to correct the warpedfeatures. Furthermore, our Residual-Guided Attention~(RGA) module utilizes theresidual maps in the compressed domain to help CRF focus on error-proneregions. Results on Cityscapes show that the accuracy significantly increasesfrom $67.3\%$ to $71.6\%$, and the speed edges down from $65.5$ FPS to $61.8$FPS at a resolution of $1024\times 2048$. For non-rigid categories, e.g.,``human'' and ``object'', the improvements are even higher than 18 percentagepoints.</description><author>Songyuan Li, Junyi Feng, Xi Li</author><pubDate>Tue, 11 Jul 2023 09:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.01344v4</guid></item><item><title>UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model</title><link>http://arxiv.org/abs/2305.12659v1</link><description>Unsupervised video object segmentation has made significant progress inrecent years, but the manual annotation of video mask datasets is expensive andlimits the diversity of available datasets. The Segment Anything Model (SAM)has introduced a new prompt-driven paradigm for image segmentation, unlocking arange of previously unexplored capabilities. In this paper, we propose a novelparadigm called UVOSAM, which leverages SAM for unsupervised video objectsegmentation without requiring video mask labels. To address SAM's limitationsin instance discovery and identity association, we introduce a video salientobject tracking network that automatically generates trajectories for prominentforeground objects. These trajectories then serve as prompts for SAM to producevideo masks on a frame-by-frame basis. Our experimental results demonstratethat UVOSAM significantly outperforms current mask-supervised methods. Thesefindings suggest that UVOSAM has the potential to improve unsupervised videoobject segmentation and reduce the cost of manual annotation.</description><author>Zhenghao Zhang, Zhichao Wei, Shengfan Zhang, Zuozhuo Dai, Siyu Zhu</author><pubDate>Mon, 22 May 2023 04:03:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12659v1</guid></item><item><title>Temporal Transductive Inference for Few-Shot Video Object Segmentation</title><link>http://arxiv.org/abs/2203.14308v2</link><description>Few-shot video object segmentation (FS-VOS) aims at segmenting video framesusing a few labelled examples of classes not seen during initial training. Inthis paper, we present a simple but effective temporal transductive inference(TTI) approach that leverages temporal consistency in the unlabelled videoframes during few-shot inference. Key to our approach is the use of both globaland local temporal constraints. The objective of the global constraint is tolearn consistent linear classifiers for novel classes across the imagesequence, whereas the local constraint enforces the proportion offoreground/background regions in each frame to be coherent across a localtemporal window. These constraints act as spatiotemporal regularizers duringthe transductive inference to increase temporal coherence and reduceoverfitting on the few-shot support set. Empirically, our model outperformsstate-of-the-art meta-learning approaches in terms of mean intersection overunion on YouTube-VIS by 2.8%. In addition, we introduce improved benchmarksthat are exhaustively labelled (i.e. all object occurrences are labelled,unlike the currently available), and present a more realistic evaluationparadigm that targets data distribution shift between training and testingsets. Our empirical results and in-depth analysis confirm the added benefits ofthe proposed spatiotemporal regularizers to improve temporal coherence andovercome certain overfitting scenarios.</description><author>Mennatullah Siam, Konstantinos G. Derpanis, Richard P. Wildes</author><pubDate>Sun, 16 Jul 2023 14:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.14308v2</guid></item><item><title>Measuring Student Behavioral Engagement using Histogram of Actions</title><link>http://arxiv.org/abs/2307.09420v1</link><description>In this paper, we propose a novel technique for measuring behavioralengagement through students' actions recognition. The proposed approachrecognizes student actions then predicts the student behavioral engagementlevel. For student action recognition, we use human skeletons to model studentpostures and upper body movements. To learn the dynamics of student upper body,a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actionswithin every 2minute video segment then these actions are used to build ahistogram of actions which encodes the student actions and their frequencies.This histogram is utilized as an input to SVM classifier to classify whetherthe student is engaged or disengaged. To evaluate the proposed framework, webuild a dataset consisting of 1414 2-minute video segments annotated with 13actions and 112 video segments annotated with two engagement levels.Experimental results indicate that student actions can be recognized with top 1accuracy 83.63% and the proposed framework can capture the average engagementof the class.</description><author>Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Tue, 18 Jul 2023 17:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09420v1</guid></item><item><title>Scalable Video Object Segmentation with Identification Mechanism</title><link>http://arxiv.org/abs/2203.11442v6</link><description>This paper delves into the challenges of achieving scalable and effectivemulti-object modeling for semi-supervised Video Object Segmentation (VOS).Previous VOS methods decode features with a single positive object, limitingthe learning of multi-object representation as they must match and segment eachtarget separately under multi-object scenarios. Additionally, earliertechniques catered to specific application objectives and lacked theflexibility to fulfill different speed-accuracy requirements. To address theseproblems, we present two innovative approaches, Associating Objects withTransformers (AOT) and Associating Objects with Scalable Transformers (AOST).In pursuing effective multi-object modeling, AOT introduces the IDentification(ID) mechanism to allocate each object a unique identity. This approach enablesthe network to model the associations among all objects simultaneously, thusfacilitating the tracking and segmentation of objects in a single network pass.To address the challenge of inflexible deployment, AOST further integratesscalable long short-term transformers that incorporate layer-wise ID-basedattention and scalable supervision. This overcomes ID embeddings'representation limitations and enables online architecture scalability in VOSfor the first time. Given the absence of a benchmark for VOS involving denselymulti-object annotations, we propose a challenging Video Object Segmentation inthe Wild (VOSW) benchmark to validate our approaches. We evaluated various AOTand AOST variants using extensive experiments across VOSW and fivecommonly-used VOS benchmarks. Our approaches surpass the state-of-the-artcompetitors and display exceptional efficiency and scalability consistentlyacross all six benchmarks. Moreover, we notably achieved the 1st position inthe 3rd Large-scale Video Object Segmentation Challenge.</description><author>Zongxin Yang, Xiaohan Wang, Jiaxu Miao, Yunchao Wei, Wenguan Wang, Yi Yang</author><pubDate>Mon, 03 Jul 2023 05:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11442v6</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v2</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Thu, 08 Jun 2023 09:15:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v2</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v1</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Tue, 06 Jun 2023 06:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v1</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v3</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Fri, 14 Jul 2023 09:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v3</guid></item><item><title>LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2306.08736v1</link><description>Referring video object segmentation (RVOS) aims to segment the targetinstance referred by a given text expression in a video clip. The textexpression normally contains sophisticated descriptions of the instance'sappearance, actions, and relations with others. It is therefore ratherdifficult for an RVOS model to capture all these attributes correspondingly inthe video; in fact, the model often favours more on the action- andrelation-related visual attribute of the instance. This can end up withincomplete or even incorrect mask prediction of the target instance. In thispaper, we tackle this problem by taking a subject-centric short text expressionfrom the original long text expression. The short one retains only theappearance-related information of the target instance so that we can use it tofocus the model's attention on the instance's appearance. We let the model makejoint predictions using both long and short text expressions and introduce along-short predictions intersection loss to align the joint predictions.Besides the improvement on the linguistic part, we also introduce aforward-backward visual consistency loss, which utilizes optical flows to warpvisual features between the annotated frames and their temporal neighbors forconsistency. We build our method on top of two state of the arttransformer-based pipelines for end-to-end training. Extensive experiments onA2D-Sentences and JHMDB-Sentences datasets show impressive improvements of ourmethod.</description><author>Linfeng Yuan, Miaojing Shi, Zijie Yue</author><pubDate>Wed, 14 Jun 2023 21:40:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08736v1</guid></item><item><title>Hierarchical Spatiotemporal Transformers for Video Object Segmentation</title><link>http://arxiv.org/abs/2307.08263v1</link><description>This paper presents a novel framework called HST for semi-supervised videoobject segmentation (VOS). HST extracts image and video features using thelatest Swin Transformer and Video Swin Transformer to inherit their inductivebias for the spatiotemporal locality, which is essential for temporallycoherent VOS. To take full advantage of the image and video features, HST castsimage and video features as a query and memory, respectively. By applyingefficient memory read operations at multiple scales, HST produces hierarchicalfeatures for the precise reconstruction of object masks. HST showseffectiveness and robustness in handling challenging scenarios with occludedand fast-moving objects under cluttered backgrounds. In particular, HST-Boutperforms the state-of-the-art competitors on multiple popular benchmarks,i.e., YouTube-VOS (85.0%), DAVIS 2017 (85.9%), and DAVIS 2016 (94.0%).</description><author>Jun-Sang Yoo, Hongjae Lee, Seung-Won Jung</author><pubDate>Mon, 17 Jul 2023 07:12:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08263v1</guid></item><item><title>RefineVIS: Video Instance Segmentation with Temporal Attention Refinement</title><link>http://arxiv.org/abs/2306.04774v1</link><description>We introduce a novel framework called RefineVIS for Video InstanceSegmentation (VIS) that achieves good object association between frames andaccurate segmentation masks by iteratively refining the representations usingsequence context. RefineVIS learns two separate representations on top of anoff-the-shelf frame-level image instance segmentation model: an associationrepresentation responsible for associating objects across frames and asegmentation representation that produces accurate segmentation masks.Contrastive learning is utilized to learn temporally stable associationrepresentations. A Temporal Attention Refinement (TAR) module learnsdiscriminative segmentation representations by exploiting temporalrelationships and a novel temporal contrastive denoising technique. Our methodsupports both online and offline inference. It achieves state-of-the-art videoinstance segmentation accuracy on YouTube-VIS 2019 (64.4 AP), Youtube-VIS 2021(61.4 AP), and OVIS (46.1 AP) datasets. The visualization shows that the TARmodule can generate more accurate instance segmentation masks, particularly forchallenging cases such as highly occluded objects.</description><author>Andre Abrantes, Jiang Wang, Peng Chu, Quanzeng You, Zicheng Liu</author><pubDate>Wed, 07 Jun 2023 21:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04774v1</guid></item><item><title>BoxVIS: Video Instance Segmentation with Box Annotations</title><link>http://arxiv.org/abs/2303.14618v2</link><description>It is expensive and labour-extensive to label the pixel-wise object masks ina video. As a result, the amount of pixel-wise annotations in existing videoinstance segmentation (VIS) datasets is small, limiting the generalizationcapability of trained VIS models. An alternative but much cheaper solution isto use bounding boxes to label instances in videos. Inspired by the recentsuccess of box-supervised image instance segmentation, we adapt thestate-of-the-art pixel-supervised VIS models to a box-supervised VIS (BoxVIS)baseline, and observe slight performance degradation. We consequently proposeto improve the BoxVIS performance from two aspects. First, we propose abox-center guided spatial-temporal pairwise affinity (STPA) loss to predictinstance masks for better spatial and temporal consistency. Second, we collecta larger scale box-annotated VIS dataset (BVISD) by consolidating the videosfrom current VIS benchmarks and converting images from the COCO dataset toshort pseudo video clips. With the proposed BVISD and the STPA loss, ourtrained BoxVIS model achieves 43.2\% and 29.0\% mask AP on the YouTube-VIS 2021and OVIS valid sets, respectively. It exhibits comparable instance maskprediction performance and better generalization ability than state-of-the-artpixel-supervised VIS models by using only 16\% of their annotation time andcost. Codes and data can be found at \url{https://github.com/MinghanLi/BoxVIS}.</description><author>Minghan Li, Lei Zhang</author><pubDate>Wed, 12 Jul 2023 11:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14618v2</guid></item><item><title>OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2307.09356v1</link><description>Referring video object segmentation (RVOS) aims at segmenting an object in avideo following human instruction. Current state-of-the-art methods fall intoan offline pattern, in which each clip independently interacts with textembedding for cross-modal understanding. They usually present that the offlinepattern is necessary for RVOS, yet model limited temporal association withineach clip. In this work, we break up the previous offline belief and propose asimple yet effective online model using explicit query propagation, namedOnlineRefer. Specifically, our approach leverages target cues that gathersemantic information and position prior to improve the accuracy and ease ofreferring predictions for the current frame. Furthermore, we generalize ouronline model into a semi-online framework to be compatible with video-basedbackbones. To show the effectiveness of our method, we evaluate it on fourbenchmarks, \ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, andJHMDB-Sentences. Without bells and whistles, our OnlineRefer with a Swin-Lbackbone achieves 63.5 J&amp;F and 64.8 J&amp;F on Refer-Youtube-VOS and Refer-DAVIS17,outperforming all other offline methods.</description><author>Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, Jianbing Shen</author><pubDate>Tue, 18 Jul 2023 16:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09356v1</guid></item><item><title>Spectrum-guided Multi-granularity Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2307.13537v1</link><description>Current referring video object segmentation (R-VOS) techniques extractconditional kernels from encoded (low-resolution) vision-language features tosegment the decoded high-resolution features. We discovered that this causessignificant feature drift, which the segmentation kernels struggle to perceiveduring the forward computation. This negatively affects the ability ofsegmentation kernels. To address the drift problem, we propose aSpectrum-guided Multi-granularity (SgMg) approach, which performs directsegmentation on the encoded features and employs visual details to furtheroptimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion(SCF) to perform intra-frame global interactions in the spectral domain foreffective multimodal representation. Finally, we extend SgMg to performmulti-object R-VOS, a new paradigm that enables simultaneous segmentation ofmultiple referred objects in a video. This not only makes R-VOS faster, butalso more practical. Extensive experiments show that SgMg achievesstate-of-the-art performance on four video benchmark datasets, outperformingthe nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMgenables multi-object R-VOS, runs about 3 times faster while maintainingsatisfactory performance. Code is available at https://github.com/bo-miao/SgMg.</description><author>Bo Miao, Mohammed Bennamoun, Yongsheng Gao, Ajmal Mian</author><pubDate>Tue, 25 Jul 2023 15:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13537v1</guid></item><item><title>Track Anything: Segment Anything Meets Videos</title><link>http://arxiv.org/abs/2304.11968v2</link><description>Recently, the Segment Anything Model (SAM) gains lots of attention rapidlydue to its impressive segmentation performance on images. Regarding its strongability on image segmentation and high interactivity with different prompts, wefound that it performs poorly on consistent segmentation in videos. Therefore,in this report, we propose Track Anything Model (TAM), which achieveshigh-performance interactive tracking and segmentation in videos. To bedetailed, given a video sequence, only with very little human participation,i.e., several clicks, people can track anything they are interested in, and getsatisfactory results in one-pass inference. Without additional training, suchan interactive design performs impressively on video object tracking andsegmentation. All resources are available on{https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitaterelated research.</description><author>Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng</author><pubDate>Fri, 28 Apr 2023 04:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11968v2</guid></item><item><title>Undercover Deepfakes: Detecting Fake Segments in Videos</title><link>http://arxiv.org/abs/2305.06564v2</link><description>The recent renaissance in generative models, driven primarily by the adventof diffusion models and iterative improvement in GAN methods, has enabled manycreative applications. However, each advancement is also accompanied by a risein the potential for misuse. In the arena of deepfake generation this is a keysocietal issue. In particular, the ability to modify segments of videos usingsuch generative techniques creates a new paradigm of deepfakes which are mostlyreal videos altered slightly to distort the truth. Current deepfake detectionmethods in the academic literature are not evaluated on this paradigm. In thispaper, we present a deepfake detection method able to address this issue byperforming both frame and video level deepfake prediction. To facilitatetesting our method we create a new benchmark dataset where videos have bothreal and fake frame sequences. Our method utilizes the Vision Transformer,Scaling and Shifting pretraining and Timeseries Transformer to temporallysegment videos to help facilitate the interpretation of possible deepfakes.Extensive experiments on a variety of deepfake generation methods showexcellent results on temporal segmentation and classical video levelpredictions as well. In particular, the paradigm we introduce will form apowerful tool for the moderation of deepfakes, where human oversight can bebetter targeted to the parts of videos suspected of being deepfakes. Allexperiments can be reproduced at:https://github.com/sanjaysaha1311/temporal-deepfake-segmentation.</description><author>Sanjay Saha, Rashindrie Perera, Sachith Seneviratne, Tamasha Malepathirana, Sanka Rasnayaka, Deshani Geethika, Terence Sim, Saman Halgamuge</author><pubDate>Tue, 16 May 2023 16:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06564v2</guid></item><item><title>GRAtt-VIS: Gated Residual Attention for Auto Rectifying Video Instance Segmentation</title><link>http://arxiv.org/abs/2305.17096v1</link><description>Recent trends in Video Instance Segmentation (VIS) have seen a growingreliance on online methods to model complex and lengthy video sequences.However, the degradation of representation and noise accumulation of the onlinemethods, especially during occlusion and abrupt changes, pose substantialchallenges. Transformer-based query propagation provides promising directionsat the cost of quadratic memory attention. However, they are susceptible to thedegradation of instance features due to the above-mentioned challenges andsuffer from cascading effects. The detection and rectification of such errorsremain largely underexplored. To this end, we introduce \textbf{GRAtt-VIS},\textbf{G}ated \textbf{R}esidual \textbf{Att}ention for \textbf{V}ideo\textbf{I}nstance \textbf{S}egmentation. Firstly, we leverage aGumbel-Softmax-based gate to detect possible errors in the current frame. Next,based on the gate activation, we rectify degraded features from its pastrepresentation. Such a residual configuration alleviates the need for dedicatedmemory and provides a continuous stream of relevant instance features.Secondly, we propose a novel inter-instance interaction using gate activationas a mask for self-attention. This masking strategy dynamically restricts theunrepresentative instance queries in the self-attention and preserves vitalinformation for long-term tracking. We refer to this novel combination of GatedResidual Connection and Masked Self-Attention as \textbf{GRAtt} block, whichcan easily be integrated into the existing propagation-based framework.Further, GRAtt blocks significantly reduce the attention overhead and simplifydynamic temporal modeling. GRAtt-VIS achieves state-of-the-art performance onYouTube-VIS and the highly challenging OVIS dataset, significantly improvingover previous methods. Code is available at\url{https://github.com/Tanveer81/GRAttVIS}.</description><author>Tanveer Hannan, Rajat Koner, Maximilian Bernhard, Suprosanna Shit, Bjoern Menze, Volker Tresp, Matthias Schubert, Thomas Seidl</author><pubDate>Fri, 26 May 2023 18:10:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17096v1</guid></item><item><title>FODVid: Flow-guided Object Discovery in Videos</title><link>http://arxiv.org/abs/2307.04392v1</link><description>Segmentation of objects in a video is challenging due to the nuances such asmotion blurring, parallax, occlusions, changes in illumination, etc. Instead ofaddressing these nuances separately, we focus on building a generalizablesolution that avoids overfitting to the individual intricacies. Such a solutionwould also help us save enormous resources involved in human annotation ofvideo corpora. To solve Video Object Segmentation (VOS) in an unsupervisedsetting, we propose a new pipeline (FODVid) based on the idea of guidingsegmentation outputs using flow-guided graph-cut and temporal consistency.Basically, we design a segmentation model incorporating intra-frame appearanceand flow similarities, and inter-frame temporal continuation of the objectsunder consideration. We perform an extensive experimental analysis of ourstraightforward methodology on the standard DAVIS16 video benchmark. Thoughsimple, our approach produces results comparable (within a range of ~2 mIoU) tothe existing top approaches in unsupervised VOS. The simplicity andeffectiveness of our technique opens up new avenues for research in the videodomain.</description><author>Silky Singh, Shripad Deshmukh, Mausoom Sarkar, Rishabh Jain, Mayur Hemani, Balaji Krishnamurthy</author><pubDate>Mon, 10 Jul 2023 08:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04392v1</guid></item><item><title>SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2305.17011v1</link><description>This paper studies referring video object segmentation (RVOS) by boostingvideo-level visual-linguistic alignment. Recent approaches model the RVOS taskas a sequence prediction problem and perform multi-modal interaction as well assegmentation for each frame separately. However, the lack of a global view ofvideo content leads to difficulties in effectively utilizing inter-framerelationships and understanding textual descriptions of object temporalvariations. To address this issue, we propose Semantic-assisted Object Cluster(SOC), which aggregates video content and textual guidance for unified temporalmodeling and cross-modal alignment. By associating a group of frame-levelobject embeddings with language tokens, SOC facilitates joint space learningacross modalities and time steps. Moreover, we present multi-modal contrastivesupervision to help construct well-aligned joint space at the video level. Weconduct extensive experiments on popular RVOS benchmarks, and our methodoutperforms state-of-the-art competitors on all benchmarks by a remarkablemargin. Besides, the emphasis on temporal coherence enhances the segmentationstability and adaptability of our method in processing text expressions withtemporal variations. Code will be available.</description><author>Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang</author><pubDate>Fri, 26 May 2023 16:13:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17011v1</guid></item><item><title>READMem: Robust Embedding Association for a Diverse Memory in Unconstrained Video Object Segmentation</title><link>http://arxiv.org/abs/2305.12823v1</link><description>We present READMem (Robust Embedding Association for a Diverse Memory), amodular framework for semi-automatic video object segmentation (sVOS) methodsdesigned to handle unconstrained videos. Contemporary sVOS works typicallyaggregate video frames in an ever-expanding memory, demanding high hardwareresources for long-term applications. To mitigate memory requirements andprevent near object duplicates (caused by information of adjacent frames),previous methods introduce a hyper-parameter that controls the frequency offrames eligible to be stored. This parameter has to be adjusted according toconcrete video properties (such as rapidity of appearance changes and videolength) and does not generalize well. Instead, we integrate the embedding of anew frame into the memory only if it increases the diversity of the memorycontent. Furthermore, we propose a robust association of the embeddings storedin the memory with query embeddings during the update process. Our approachavoids the accumulation of redundant data, allowing us in return, to restrictthe memory size and prevent extreme memory demands in long videos. We extendpopular sVOS baselines with READMem, which previously showed limitedperformance on long videos. Our approach achieves competitive results on theLong-time Video dataset (LV1) while not hindering performance on shortsequences. Our code is publicly available.</description><author>Stéphane Vujasinović, Sebastian Bullinger, Stefan Becker, Norbert Scherer-Negenborn, Michael Arens, Rainer Stiefelhagen</author><pubDate>Mon, 22 May 2023 09:31:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12823v1</guid></item><item><title>Online Unsupervised Video Object Segmentation via Contrastive Motion Clustering</title><link>http://arxiv.org/abs/2306.12048v1</link><description>Online unsupervised video object segmentation (UVOS) uses the previous framesas its input to automatically separate the primary object(s) from a streamingvideo without using any further manual annotation. A major challenge is thatthe model has no access to the future and must rely solely on the history,i.e., the segmentation mask is predicted from the current frame as soon as itis captured. In this work, a novel contrastive motion clustering algorithm withan optical flow as its input is proposed for the online UVOS by exploiting thecommon fate principle that visual elements tend to be perceived as a group ifthey possess the same motion pattern. We build a simple and effectiveauto-encoder to iteratively summarize non-learnable prototypical bases for themotion pattern, while the bases in turn help learn the representation of theembedding network. Further, a contrastive learning strategy based on a boundaryprior is developed to improve foreground and background feature discriminationin the representation learning stage. The proposed algorithm can be optimizedon arbitrarily-scale data i.e., frame, clip, dataset) and performed in anonline fashion. Experiments on $\textit{DAVIS}_{\textit{16}}$, $\textit{FBMS}$,and $\textit{SegTrackV2}$ datasets show that the accuracy of our methodsurpasses the previous state-of-the-art (SoTA) online UVOS method by a marginof 0.8%, 2.9%, and 1.1%, respectively. Furthermore, by using an online deepsubspace clustering to tackle the motion grouping, our method is able toachieve higher accuracy at $3\times$ faster inference time compared to SoTAonline UVOS method, and making a good trade-off between effectiveness andefficiency.</description><author>Lin Xi, Weihai Chen, Xingming Wu, Zhong Liu, Zhengguo Li</author><pubDate>Wed, 21 Jun 2023 07:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12048v1</guid></item><item><title>Segment Anything Meets Point Tracking</title><link>http://arxiv.org/abs/2307.01197v1</link><description>The Segment Anything Model (SAM) has established itself as a powerfulzero-shot image segmentation model, employing interactive prompts such aspoints to generate masks. This paper presents SAM-PT, a method extending SAM'scapability to tracking and segmenting anything in dynamic videos. SAM-PTleverages robust and sparse point selection and propagation techniques for maskgeneration, demonstrating that a SAM-based segmentation tracker can yieldstrong zero-shot performance across popular video object segmentationbenchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditionalobject-centric mask propagation strategies, we uniquely use point propagationto exploit local structure information that is agnostic to object semantics. Wehighlight the merits of point-based tracking through direct evaluation on thezero-shot open-world Unidentified Video Objects (UVO) benchmark. To furtherenhance our approach, we utilize K-Medoids clustering for point initializationand track both positive and negative points to clearly distinguish the targetobject. We also employ multiple mask decoding passes for mask refinement anddevise a point re-initialization strategy to improve tracking accuracy. Ourcode integrates different point trackers and video segmentation benchmarks andwill be released at https://github.com/SysCV/sam-pt.</description><author>Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu</author><pubDate>Mon, 03 Jul 2023 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01197v1</guid></item><item><title>A Similarity Alignment Model for Video Copy Segment Matching</title><link>http://arxiv.org/abs/2305.15679v1</link><description>With the development of multimedia technology, Video Copy Detection has beena crucial problem for social media platforms. Meta AI hold Video SimilarityChallenge on CVPR 2023 to push the technology forward. In this report, we shareour winner solutions on Matching Track. We propose a Similarity AlignmentModel(SAM) for video copy segment matching. Our SAM exhibits superiorperformance compared to other competitors, with a 0.108 / 0.144 absoluteimprovement over the second-place competitor in Phase 1 / Phase 2. Code isavailable athttps://github.com/FeipengMa6/VSC22-Submission/tree/main/VSC22-Matching-Track-1st.</description><author>Zhenhua Liu, Feipeng Ma, Tianyi Wang, Fengyun Rao</author><pubDate>Thu, 25 May 2023 04:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15679v1</guid></item><item><title>Video Object Segmentation in Panoptic Wild Scenes</title><link>http://arxiv.org/abs/2305.04470v2</link><description>In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.</description><author>Yuanyou Xu, Zongxin Yang, Yi Yang</author><pubDate>Sun, 25 Jun 2023 07:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04470v2</guid></item><item><title>Video Object Segmentation in Panoptic Wild Scenes</title><link>http://arxiv.org/abs/2305.04470v1</link><description>In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.</description><author>Yuanyou Xu, Zongxin Yang, Yi Yang</author><pubDate>Mon, 08 May 2023 06:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04470v1</guid></item><item><title>CTVIS: Consistent Training for Online Video Instance Segmentation</title><link>http://arxiv.org/abs/2307.12616v1</link><description>The discrimination of instance embeddings plays a vital role in associatinginstances across time for online video instance segmentation (VIS). Instanceembedding learning is directly supervised by the contrastive loss computed uponthe contrastive items (CIs), which are sets of anchor/positive/negativeembeddings. Recent online VIS methods leverage CIs sourced from one referenceframe only, which we argue is insufficient for learning highly discriminativeembeddings. Intuitively, a possible strategy to enhance CIs is replicating theinference phase during training. To this end, we propose a simple yet effectivetraining strategy, called Consistent Training for Online VIS (CTVIS), whichdevotes to aligning the training and inference pipelines in terms of buildingCIs. Specifically, CTVIS constructs CIs by referring inference themomentum-averaged embedding and the memory bank storage mechanisms, and addingnoise to the relevant embeddings. Such an extension allows a reliablecomparison between embeddings of current instances and the stablerepresentations of historical instances, thereby conferring an advantage inmodeling VIS challenges such as occlusion, re-identification, and deformation.Empirically, CTVIS outstrips the SOTA VIS models by up to +5.0 points on threeVIS benchmarks, including YTVIS19 (55.1% AP), YTVIS21 (50.1% AP) and OVIS(35.5% AP). Furthermore, we find that pseudo-videos transformed from images cantrain robust models surpassing fully-supervised ones.</description><author>Kaining Ying, Qing Zhong, Weian Mao, Zhenhua Wang, Hao Chen, Lin Yuanbo Wu, Yifan Liu, Chengxiang Fan, Yunzhi Zhuge, Chunhua Shen</author><pubDate>Mon, 24 Jul 2023 09:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12616v1</guid></item><item><title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation</title><link>http://arxiv.org/abs/2307.02010v1</link><description>The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objectsegmentation. In this study, we introduce MSDeAOT, a variant of the AOT seriesthat incorporates transformers at multiple feature scales. Leveraging thehierarchical Gated Propagation Module (GPM), MSDeAOT efficiently propagatesobject masks from previous frames to the current frame using a feature scalewith a stride of 16. Additionally, we employ GPM in a more refined featurescale with a stride of 8, leading to improved accuracy in detecting andtracking small objects. Through the implementation of test-time augmentationsand model ensemble techniques, we achieve the top-ranking position in theEPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge.</description><author>Jiahao Li, Yuanyou Xu, Zongxin Yang, Yi Yang, Yueting Zhuang</author><pubDate>Wed, 05 Jul 2023 04:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02010v1</guid></item><item><title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation</title><link>http://arxiv.org/abs/2307.02010v2</link><description>The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objectsegmentation. In this study, we introduce MSDeAOT, a variant of the AOT seriesthat incorporates transformers at multiple feature scales. Leveraging thehierarchical Gated Propagation Module (GPM), MSDeAOT efficiently propagatesobject masks from previous frames to the current frame using a feature scalewith a stride of 16. Additionally, we employ GPM in a more refined featurescale with a stride of 8, leading to improved accuracy in detecting andtracking small objects. Through the implementation of test-time augmentationsand model ensemble techniques, we achieve the top-ranking position in theEPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge.</description><author>Jiahao Li, Yuanyou Xu, Zongxin Yang, Yi Yang, Yueting Zhuang</author><pubDate>Mon, 10 Jul 2023 10:20:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02010v2</guid></item><item><title>Self-Supervised Video Representation Learning via Latent Time Navigation</title><link>http://arxiv.org/abs/2305.06437v1</link><description>Self-supervised video representation learning aimed at maximizing similaritybetween different temporal segments of one video, in order to enforce featurepersistence over time. This leads to loss of pertinent information related totemporal relationships, rendering actions such as `enter' and `leave' to beindistinguishable. To mitigate this limitation, we propose Latent TimeNavigation (LTN), a time-parameterized contrastive learning strategy that isstreamlined to capture fine-grained motions. Specifically, we maximize therepresentation similarity between different video segments from one video,while maintaining their representations time-aware along a subspace of thelatent representation code including an orthogonal basis to represent temporalchanges. Our extensive experimental analysis suggests that learning videorepresentations by LTN consistently improves performance of actionclassification in fine-grained and human-oriented tasks (e.g., on ToyotaSmarthome dataset). In addition, we demonstrate that our proposed model, whenpre-trained on Kinetics-400, generalizes well onto the unseen real world videobenchmark datasets UCF101 and HMDB51, achieving state-of-the-art performance inaction recognition.</description><author>Di Yang, Yaohui Wang, Quan Kong, Antitza Dantcheva, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond</author><pubDate>Wed, 10 May 2023 21:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06437v1</guid></item><item><title>Enhancing Transformer Backbone for Egocentric Video Action Segmentation</title><link>http://arxiv.org/abs/2305.11365v1</link><description>Egocentric temporal action segmentation in videos is a crucial task incomputer vision with applications in various fields such as mixed reality,human behavior analysis, and robotics. Although recent research has utilizedadvanced visual-language frameworks, transformers remain the backbone of actionsegmentation models. Therefore, it is necessary to improve transformers toenhance the robustness of action segmentation models. In this work, we proposetwo novel ideas to enhance the state-of-the-art transformer for actionsegmentation. First, we introduce a dual dilated attention mechanism toadaptively capture hierarchical representations in both local-to-global andglobal-to-local contexts. Second, we incorporate cross-connections between theencoder and decoder blocks to prevent the loss of local context by the decoder.Additionally, we utilize state-of-the-art visual-language representationlearning techniques to extract richer and more compact features for ourtransformer. Our proposed approach outperforms other state-of-the-art methodson the Georgia Tech Egocentric Activities (GTEA) and HOI4D Office Toolsdatasets, and we validate our introduced components with ablation studies. Thesource code and supplementary materials are publicly available onhttps://www.sail-nu.com/dxformer.</description><author>Sakib Reza, Balaji Sundareshan, Mohsen Moghaddam, Octavia Camps</author><pubDate>Fri, 19 May 2023 02:00:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11365v1</guid></item><item><title>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</title><link>http://arxiv.org/abs/2306.08640v1</link><description>Recent research on Large Language Models (LLMs) has led to remarkableadvancements in general NLP AI assistants. Some studies have further exploredthe use of LLMs for planning and invoking models or APIs to address moregeneral multi-modal user queries. Despite this progress, complex visual-basedtasks still remain challenging due to the diverse nature of visual tasks. Thisdiversity is reflected in two aspects: 1) Reasoning paths. For many real-lifeapplications, it is hard to accurately decompose a query simply by examiningthe query itself. Planning based on the specific visual content and the resultsof each step is usually required. 2) Flexible inputs and intermediate results.Input forms could be flexible for in-the-wild cases, and involves not only asingle image or video but a mixture of videos and images, e.g., a user-viewimage with some reference videos. Besides, a complex reasoning process willalso generate diverse multimodal intermediate results, e.g., video narrations,segmented video clips, etc. To address such general cases, we propose amulti-modal AI assistant, AssistGPT, with an interleaved code and languagereasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrateLLMs with various tools. Specifically, the Planner is capable of using naturallanguage to plan which tool in Executor should do next based on the currentreasoning progress. Inspector is an efficient memory manager to assist thePlanner to feed proper visual information into a specific tool. Finally, sincethe entire reasoning process is complex and flexible, a Learner is designed toenable the model to autonomously explore and discover the optimal solution. Weconducted experiments on A-OKVQA and NExT-QA benchmarks, achievingstate-of-the-art results. Moreover, showcases demonstrate the ability of oursystem to handle questions far more complex than those found in the benchmarks.</description><author>Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, Mike Zheng Shou</author><pubDate>Wed, 14 Jun 2023 18:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08640v1</guid></item><item><title>MultiSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos</title><link>http://arxiv.org/abs/2306.04216v1</link><description>Multimodal summarization with multimodal output (MSMO) has emerged as apromising research direction. Nonetheless, numerous limitations exist withinexisting public MSMO datasets, including insufficient upkeep, datainaccessibility, limited size, and the absence of proper categorization, whichpose significant challenges to effective research. To address these challengesand provide a comprehensive dataset for this new direction, we havemeticulously curated the MultiSum dataset. Our new dataset features (1)Human-validated summaries for both video and textual content, providingsuperior human instruction and labels for multimodal learning. (2)Comprehensively and meticulously arranged categorization, spanning 17 principalcategories and 170 subcategories to encapsulate a diverse array of real-worldscenarios. (3) Benchmark tests performed on the proposed dataset to assessvaried tasks and methods, including video temporal segmentation, videosummarization, text summarization, and multimodal summarization. To championaccessibility and collaboration, we release the MultiSum dataset and the datacollection tool as fully open-source resources, fostering transparency andaccelerating future developments. Our project website can be found athttps://multisum-dataset.github.io/.</description><author>Jielin Qiu, Jiacheng Zhu, William Han, Aditesh Kumar, Karthik Mittal, Claire Jin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Bo Li, Ding Zhao, Lijuan Wang</author><pubDate>Wed, 07 Jun 2023 08:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04216v1</guid></item><item><title>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</title><link>http://arxiv.org/abs/2306.08640v2</link><description>Recent research on Large Language Models (LLMs) has led to remarkableadvancements in general NLP AI assistants. Some studies have further exploredthe use of LLMs for planning and invoking models or APIs to address moregeneral multi-modal user queries. Despite this progress, complex visual-basedtasks still remain challenging due to the diverse nature of visual tasks. Thisdiversity is reflected in two aspects: 1) Reasoning paths. For many real-lifeapplications, it is hard to accurately decompose a query simply by examiningthe query itself. Planning based on the specific visual content and the resultsof each step is usually required. 2) Flexible inputs and intermediate results.Input forms could be flexible for in-the-wild cases, and involves not only asingle image or video but a mixture of videos and images, e.g., a user-viewimage with some reference videos. Besides, a complex reasoning process willalso generate diverse multimodal intermediate results, e.g., video narrations,segmented video clips, etc. To address such general cases, we propose amulti-modal AI assistant, AssistGPT, with an interleaved code and languagereasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrateLLMs with various tools. Specifically, the Planner is capable of using naturallanguage to plan which tool in Executor should do next based on the currentreasoning progress. Inspector is an efficient memory manager to assist thePlanner to feed proper visual information into a specific tool. Finally, sincethe entire reasoning process is complex and flexible, a Learner is designed toenable the model to autonomously explore and discover the optimal solution. Weconducted experiments on A-OKVQA and NExT-QA benchmarks, achievingstate-of-the-art results. Moreover, showcases demonstrate the ability of oursystem to handle questions far more complex than those found in the benchmarks.</description><author>Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, Mike Zheng Shou</author><pubDate>Wed, 28 Jun 2023 06:00:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08640v2</guid></item><item><title>Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation</title><link>http://arxiv.org/abs/2302.11325v2</link><description>This paper presents a deep learning framework for medical video segmentation.Convolution neural network (CNN) and transformer-based methods have achievedgreat milestones in medical image segmentation tasks due to their incrediblesemantic feature encoding and global information comprehension abilities.However, most existing approaches ignore a salient aspect of medical video data- the temporal dimension. Our proposed framework explicitly extracts featuresfrom neighbouring frames across the temporal dimension and incorporates themwith a temporal feature blender, which then tokenises the high-levelspatio-temporal feature to form a strong global feature encoded via a SwinTransformer. The final segmentation results are produced via a UNet-likeencoder-decoder architecture. Our model outperforms other approaches by asignificant margin and improves the segmentation benchmarks on the VFSS2022dataset, achieving a dice coefficient of 0.8986 and 0.8186 for the two datasetstested. Our studies also show the efficacy of the temporal feature blendingscheme and cross-dataset transferability of learned capabilities. Code andmodels are fully available at https://github.com/SimonZeng7108/Video-SwinUNet.</description><author>Chengxi Zeng, Xinyu Yang, David Smithard, Majid Mirmehdi, Alberto M Gambaruto, Tilo Burghardt</author><pubDate>Tue, 04 Jul 2023 16:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11325v2</guid></item><item><title>Tracking Anything in High Quality</title><link>http://arxiv.org/abs/2307.13974v1</link><description>Visual object tracking is a fundamental video task in computer vision.Recently, the notably increasing power of perception algorithms allows theunification of single/multiobject and box/mask-based tracking. Among them, theSegment Anything Model (SAM) attracts much attention. In this report, wepropose HQTrack, a framework for High Quality Tracking anything in videos.HQTrack mainly consists of a video multi-object segmenter (VMOS) and a maskrefiner (MR). Given the object to be tracked in the initial frame of a video,VMOS propagates the object masks to the current frame. The mask results at thisstage are not accurate enough since VMOS is trained on several closeset videoobject segmentation (VOS) datasets, which has limited ability to generalize tocomplex and corner scenes. To further improve the quality of tracking masks, apretrained MR model is employed to refine the tracking results. As a compellingtestament to the effectiveness of our paradigm, without employing any trickssuch as test-time data augmentations and model ensemble, HQTrack ranks the 2ndplace in the Visual Object Tracking and Segmentation (VOTS2023) challenge. Codeand models are available at https://github.com/jiawen-zhu/HQTrack.</description><author>Jiawen Zhu, Zhenyu Chen, Zeqi Hao, Shijie Chang, Lu Zhang, Dong Wang, Huchuan Lu, Bin Luo, Jun-Yan He, Jin-Peng Lan, Hanyuan Chen, Chenyang Li</author><pubDate>Wed, 26 Jul 2023 07:19:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13974v1</guid></item><item><title>Detect Any Shadow: Segment Anything for Video Shadow Detection</title><link>http://arxiv.org/abs/2305.16698v1</link><description>Segment anything model (SAM) has achieved great success in the field ofnatural image segmentation. Nevertheless, SAM tends to classify shadows asbackground, resulting in poor segmentation performance for shadow detectiontask. In this paper, we propose an simple but effective approach for finetuning SAM to detect shadows. Additionally, we also combine it with longshort-term attention mechanism to extend its capabilities to video shadowdetection. Specifically, we first fine tune SAM by utilizing shadow datacombined with sparse prompts and apply the fine-tuned model to detect aspecific frame (e.g., first frame) in the video with a little user assistance.Subsequently, using the detected frame as a reference, we employ a longshort-term network to learn spatial correlations between distant frames andtemporal consistency between contiguous frames, thereby achieving shadowinformation propagation across frames. Extensive experimental resultsdemonstrate that our method outperforms the state-of-the-art techniques, withimprovements of 17.2% and 3.3% in terms of MAE and IoU, respectively,validating the effectiveness of our method.</description><author>Yonghui Wang, Wengang Zhou, Yunyao Mao, Houqiang Li</author><pubDate>Fri, 26 May 2023 08:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16698v1</guid></item><item><title>Transform-Equivariant Consistency Learning for Temporal Sentence Grounding</title><link>http://arxiv.org/abs/2305.04123v1</link><description>This paper addresses the temporal sentence grounding (TSG). Although existingmethods have made decent achievements in this task, they not only severely relyon abundant video-query paired data for training, but also easily fail into thedataset distribution bias. To alleviate these limitations, we introduce a novelEquivariant Consistency Regulation Learning (ECRL) framework to learn morediscriminative query-related frame-wise representations for each video, in aself-supervised manner. Our motivation comes from that the temporal boundary ofthe query-guided activity should be consistently predicted under variousvideo-level transformations. Concretely, we first design a series ofspatio-temporal augmentations on both foreground and background video segmentsto generate a set of synthetic video samples. In particular, we devise aself-refine module to enhance the completeness and smoothness of the augmentedvideo. Then, we present a novel self-supervised consistency loss (SSCL) appliedon the original and augmented videos to capture their invariant query-relatedsemantic by minimizing the KL-divergence between the sequence similarity of twovideos and a prior Gaussian distribution of timestamp distance. At last, ashared grounding head is introduced to predict the transform-equivariantquery-guided segment boundaries for both the original and augmented videos.Extensive experiments on three challenging datasets (ActivityNet, TACoS, andCharades-STA) demonstrate both effectiveness and efficiency of our proposedECRL framework.</description><author>Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Zichuan Xu, Haozhao Wang, Xing Di, Weining Lu, Yu Cheng</author><pubDate>Sat, 06 May 2023 20:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04123v1</guid></item><item><title>Multi-object Video Generation from Single Frame Layouts</title><link>http://arxiv.org/abs/2305.03983v1</link><description>In this paper, we study video synthesis with emphasis on simplifying thegeneration conditions. Most existing video synthesis models or datasets aredesigned to address complex motions of a single object, lacking the ability ofcomprehensively understanding the spatio-temporal relationships among multipleobjects. Besides, current methods are usually conditioned on intricateannotations (e.g. video segmentations) to generate new videos, beingfundamentally less practical. These motivate us to generate multi-object videosconditioning exclusively on object layouts from a single frame. To solve abovechallenges and inspired by recent research on image generation from layouts, wehave proposed a novel video generative framework capable of synthesizing globalscenes with local objects, via implicit neural representations and layoutmotion self-inference. Our framework is a non-trivial adaptation from imagegeneration methods, and is new to this field. In addition, our model has beenevaluated on two widely-used video recognition benchmarks, demonstratingeffectiveness compared to the baseline model.</description><author>Yang Wu, Zhibin Liu, Hefeng Wu, Liang Lin</author><pubDate>Sat, 06 May 2023 10:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03983v1</guid></item><item><title>TrickVOS: A Bag of Tricks for Video Object Segmentation</title><link>http://arxiv.org/abs/2306.15377v2</link><description>Space-time memory (STM) network methods have been dominant in semi-supervisedvideo object segmentation (SVOS) due to their remarkable performance. In thiswork, we identify three key aspects where we can improve such methods; i)supervisory signal, ii) pretraining and iii) spatial awareness. We then proposeTrickVOS; a generic, method-agnostic bag of tricks addressing each aspect withi) a structure-aware hybrid loss, ii) a simple decoder pretraining regime andiii) a cheap tracker that imposes spatial constraints in model predictions.Finally, we propose a lightweight network and show that when trained withTrickVOS, it achieves competitive results to state-of-the-art methods on DAVISand YouTube benchmarks, while being one of the first STM-based SVOS methodsthat can run in real-time on a mobile device.</description><author>Evangelos Skartados, Konstantinos Georgiadis, Mehmet Kerim Yucel, Koskinas Ioannis, Armando Domi, Anastasios Drosou, Bruno Manganelli, Albert Saa-Garriga</author><pubDate>Wed, 28 Jun 2023 13:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15377v2</guid></item><item><title>Multi-object Video Generation from Single Frame Layouts</title><link>http://arxiv.org/abs/2305.03983v2</link><description>In this paper, we study video synthesis with emphasis on simplifying thegeneration conditions. Most existing video synthesis models or datasets aredesigned to address complex motions of a single object, lacking the ability ofcomprehensively understanding the spatio-temporal relationships among multipleobjects. Besides, current methods are usually conditioned on intricateannotations (e.g. video segmentations) to generate new videos, beingfundamentally less practical. These motivate us to generate multi-object videosconditioning exclusively on object layouts from a single frame. To solve abovechallenges and inspired by recent research on image generation from layouts, wehave proposed a novel video generative framework capable of synthesizing globalscenes with local objects, via implicit neural representations and layoutmotion self-inference. Our framework is a non-trivial adaptation from imagegeneration methods, and is new to this field. In addition, our model has beenevaluated on two widely-used video recognition benchmarks, demonstratingeffectiveness compared to the baseline model.</description><author>Yang Wu, Zhibin Liu, Hefeng Wu, Liang Lin</author><pubDate>Tue, 23 May 2023 16:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03983v2</guid></item><item><title>TrickVOS: A Bag of Tricks for Video Object Segmentation</title><link>http://arxiv.org/abs/2306.15377v1</link><description>Space-time memory (STM) network methods have been dominant in semi-supervisedvideo object segmentation (SVOS) due to their remarkable performance. In thiswork, we identify three key aspects where we can improve such methods; i)supervisory signal, ii) pretraining and iii) spatial awareness. We then proposeTrickVOS; a generic, method-agnostic bag of tricks addressing each aspect withi) a structure-aware hybrid loss, ii) a simple decoder pretraining regime andiii) a cheap tracker that imposes spatial constraints in model predictions.Finally, we propose a lightweight network and show that when trained withTrickVOS, it achieves competitive results to state-of-the-art methods on DAVISand YouTube benchmarks, while being one of the first STM-based SVOS methodsthat can run in real-time on a mobile device.</description><author>Evangelos Skartados, Konstantinos Georgiadis, Mehmet Kerim Yucel, Koskinas Ioannis, Armando Domi, Anastasios Drosou, Bruno Manganelli, Albert Sa`a-Garriga</author><pubDate>Tue, 27 Jun 2023 11:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15377v1</guid></item><item><title>Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment</title><link>http://arxiv.org/abs/2307.02682v1</link><description>Dense video captioning, a task of localizing meaningful moments andgenerating relevant captions for videos, often requires a large, expensivecorpus of annotated video segments paired with text. In an effort to minimizethe annotation cost, we propose ZeroTA, a novel method for dense videocaptioning in a zero-shot manner. Our method does not require any videos orannotations for training; instead, it localizes and describes events withineach input video at test time by optimizing solely on the input. This isaccomplished by introducing a soft moment mask that represents a temporalsegment in the video and jointly optimizing it with the prefix parameters of alanguage model. This joint optimization aligns a frozen language generationmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,CLIP) by maximizing the matching score between the generated text and a momentwithin the video. We also introduce a pairwise temporal IoU loss to let a setof soft moment masks capture multiple distinct events within the video. Ourmethod effectively discovers diverse significant events within the video, withthe resulting captions appropriately describing these events. The empiricalresults demonstrate that ZeroTA surpasses zero-shot baselines and evenoutperforms the state-of-the-art few-shot method on the widely-used benchmarkActivityNet Captions. Moreover, our method shows greater robustness compared tosupervised methods when evaluated in out-of-domain scenarios. This researchprovides insight into the potential of aligning widely-used models, such aslanguage generation models and vision-language models, to unlock a newcapability: understanding temporal aspects of videos.</description><author>Yongrae Jo, Seongyun Lee, Aiden SJ Lee, Hyunji Lee, Hanseok Oh, Minjoon Seo</author><pubDate>Thu, 06 Jul 2023 00:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02682v1</guid></item><item><title>Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment</title><link>http://arxiv.org/abs/2307.02682v2</link><description>Dense video captioning, a task of localizing meaningful moments andgenerating relevant captions for videos, often requires a large, expensivecorpus of annotated video segments paired with text. In an effort to minimizethe annotation cost, we propose ZeroTA, a novel method for dense videocaptioning in a zero-shot manner. Our method does not require any videos orannotations for training; instead, it localizes and describes events withineach input video at test time by optimizing solely on the input. This isaccomplished by introducing a soft moment mask that represents a temporalsegment in the video and jointly optimizing it with the prefix parameters of alanguage model. This joint optimization aligns a frozen language generationmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,CLIP) by maximizing the matching score between the generated text and a momentwithin the video. We also introduce a pairwise temporal IoU loss to let a setof soft moment masks capture multiple distinct events within the video. Ourmethod effectively discovers diverse significant events within the video, withthe resulting captions appropriately describing these events. The empiricalresults demonstrate that ZeroTA surpasses zero-shot baselines and evenoutperforms the state-of-the-art few-shot method on the widely-used benchmarkActivityNet Captions. Moreover, our method shows greater robustness compared tosupervised methods when evaluated in out-of-domain scenarios. This researchprovides insight into the potential of aligning widely-used models, such aslanguage generation models and vision-language models, to unlock a newcapability: understanding temporal aspects of videos.</description><author>Yongrae Jo, Seongyun Lee, Aiden SJ Lee, Hyunji Lee, Hanseok Oh, Minjoon Seo</author><pubDate>Tue, 11 Jul 2023 05:10:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02682v2</guid></item><item><title>Autonomous Stabilization of Retinal Videos for Streamlining Assessment of Spontaneous Venous Pulsations</title><link>http://arxiv.org/abs/2305.06043v1</link><description>Spontaneous retinal Venous Pulsations (SVP) are rhythmic changes in thecaliber of the central retinal vein and are observed in the optic disc region(ODR) of the retina. Its absence is a critical indicator of various ocular orneurological abnormalities. Recent advances in imaging technology have enabledthe development of portable smartphone-based devices for observing the retinaand assessment of SVPs. However, the quality of smartphone-based retinal videosis often poor due to noise and image jitting, which in return, can severelyobstruct the observation of SVPs. In this work, we developed a fully automatedretinal video stabilization method that enables the examination of SVPscaptured by various mobile devices. Specifically, we first propose an ODRSpatio-Temporal Localization (ODR-STL) module to localize visible ODR andremove noisy and jittering frames. Then, we introduce a Noise-Aware TemplateMatching (NATM) module to stabilize high-quality video segments at a fixedposition in the field of view. After the processing, the SVPs can be easilyobserved in the stabilized videos, significantly facilitating userobservations. Furthermore, our method is cost-effective and has been tested inboth subjective and objective evaluations. Both of the evaluations support itseffectiveness in facilitating the observation of SVPs. This can improve thetimely diagnosis and treatment of associated diseases, making it a valuabletool for eye health professionals.</description><author>Hongwei Sheng, Xin Yu, Feiyu Wang, MD Wahiduzzaman Khan, Hexuan Weng, Sahar Shariflou, S. Mojtaba Golzan</author><pubDate>Wed, 10 May 2023 11:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06043v1</guid></item><item><title>Gloss Attention for Gloss-free Sign Language Translation</title><link>http://arxiv.org/abs/2307.07361v1</link><description>Most sign language translation (SLT) methods to date require the use of glossannotations to provide additional supervision information, however, theacquisition of gloss is not easy. To solve this problem, we first perform ananalysis of existing models to confirm how gloss annotations make SLT easier.We find that it can provide two aspects of information for the model, 1) it canhelp the model implicitly learn the location of semantic boundaries incontinuous sign language videos, 2) it can help the model understand the signlanguage video globally. We then propose \emph{gloss attention}, which enablesthe model to keep its attention within video segments that have the samesemantics locally, just as gloss helps existing models do. Furthermore, wetransfer the knowledge of sentence-to-sentence similarity from the naturallanguage model to our gloss attention SLT network (GASLT) to help it understandsign language videos at the sentence level. Experimental results on multiplelarge-scale sign language datasets show that our proposed GASLT modelsignificantly outperforms existing methods. Our code is provided in\url{https://github.com/YinAoXiong/GASLT}.</description><author>Aoxiong Yin, Tianyun Zhong, Li Tang, Weike Jin, Tao Jin, Zhou Zhao</author><pubDate>Fri, 14 Jul 2023 15:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07361v1</guid></item><item><title>Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations</title><link>http://arxiv.org/abs/2307.01533v2</link><description>This paper aims to address the unsupervised video anomaly detection (VAD)problem, which involves classifying each frame in a video as normal orabnormal, without any access to labels. To accomplish this, the proposed methodemploys conditional diffusion models, where the input data is thespatiotemporal features extracted from a pre-trained network, and the conditionis the features extracted from compact motion representations that summarize agiven video segment in terms of its motion and appearance. Our method utilizesa data-driven threshold and considers a high reconstruction error as anindicator of anomalous events. This study is the first to utilize compactmotion representations for VAD and the experiments conducted on two large-scaleVAD benchmarks demonstrate that they supply relevant information to thediffusion model, and consequently improve VAD performances w.r.t the prior art.Importantly, our method exhibits better generalization performance acrossdifferent datasets, notably outperforming both the state-of-the-art andbaseline methods. The code of our method is available athttps://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion</description><author>Anil Osman Tur, Nicola Dall'Asen, Cigdem Beyan, Elisa Ricci</author><pubDate>Wed, 19 Jul 2023 07:39:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01533v2</guid></item><item><title>CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition</title><link>http://arxiv.org/abs/2305.16049v1</link><description>Audio-visual person recognition (AVPR) has received extensive attention.However, most datasets used for AVPR research so far are collected inconstrained environments, and thus cannot reflect the true performance of AVPRsystems in real-world scenarios. To meet the request for research on AVPR inunconstrained conditions, this paper presents a multi-genre AVPR datasetcollected `in the wild', named CN-Celeb-AV. This dataset contains more than420k video segments from 1,136 persons from public media. In particular, we putmore emphasis on two real-world complexities: (1) data in multiple genres; (2)segments with partial information. A comprehensive study was conducted tocompare CN-Celeb-AV with two popular public AVPR benchmark datasets, and theresults demonstrated that CN-Celeb-AV is more in line with real-world scenariosand can be regarded as a new benchmark dataset for AVPR research. The datasetalso involves a development set that can be used to boost the performance ofAVPR systems in real-life situations. The dataset is free for researchers andcan be downloaded from http://cnceleb.org/.</description><author>Lantian Li, Xiaolou Li, Haoyu Jiang, Chen Chen, Ruihai Hou, Dong Wang</author><pubDate>Thu, 25 May 2023 14:31:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16049v1</guid></item><item><title>Automatic Interaction and Activity Recognition from Videos of Human Manual Demonstrations with Application to Anomaly Detection</title><link>http://arxiv.org/abs/2304.09789v2</link><description>This paper presents a new method to describe spatio-temporal relationsbetween objects and hands, to recognize both interactions and activities withinvideo demonstrations of manual tasks. The approach exploits Scene Graphs toextract key interaction features from image sequences while simultaneouslyencoding motion patterns and context. Additionally, the method introducesevent-based automatic video segmentation and clustering, which allow for thegrouping of similar events and detect if a monitored activity is executedcorrectly. The effectiveness of the approach was demonstrated in twomulti-subject experiments, showing the ability to recognize and clusterhand-object and object-object interactions without prior knowledge of theactivity, as well as matching the same activity performed by differentsubjects.</description><author>Elena Merlo, Marta Lagomarsino, Edoardo Lamon, Arash Ajoudani</author><pubDate>Fri, 07 Jul 2023 09:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09789v2</guid></item><item><title>TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical Phase Recognition</title><link>http://arxiv.org/abs/2307.09997v1</link><description>To enable context-aware computer assistance in the operating room of thefuture, cognitive systems need to understand automatically which surgical phaseis being performed by the medical team. The primary source of information forsurgical phase recognition is typically video, which presents two challenges:extracting meaningful features from the video stream and effectively modelingtemporal information in the sequence of visual features. For temporal modeling,attention mechanisms have gained popularity due to their ability to capturelong-range dependencies. In this paper, we explore design choices for attentionin existing temporal models for surgical phase recognition and propose a novelapproach that does not resort to local attention or regularization of attentionweights: TUNeS is an efficient and simple temporal model that incorporatesself-attention at the coarsest stage of a U-Net-like structure. In addition, wepropose to train the feature extractor, a standard CNN, together with an LSTMon preferably long video segments, i.e., with long temporal context. In ourexperiments, all temporal models performed better on top of feature extractorsthat were trained with longer temporal context. On top of these contextualizedfeatures, TUNeS achieves state-of-the-art results on Cholec80.</description><author>Isabel Funke, Dominik Rivoir, Stefanie Krell, Stefanie Speidel</author><pubDate>Wed, 19 Jul 2023 15:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09997v1</guid></item><item><title>Colonoscopy Coverage Revisited: Identifying Scanning Gaps in Real-Time</title><link>http://arxiv.org/abs/2305.10026v1</link><description>Colonoscopy is the most widely used medical technique for preventingColorectal Cancer, by detecting and removing polyps before they becomemalignant. Recent studies show that around one quarter of the existing polypsare routinely missed. While some of these do appear in the endoscopist's fieldof view, others are missed due to a partial coverage of the colon. The task ofdetecting and marking unseen regions of the colon has been addressed in recentwork, where the common approach is based on dense 3D reconstruction, whichproves to be challenging due to lack of 3D ground truth and periods with poorvisual content. In this paper we propose a novel and complementary method todetect deficient local coverage in real-time for video segments where areliable 3D reconstruction is impossible. Our method aims to identify skipsalong the colon caused by a drifted position of the endoscope during poorvisibility time intervals. The proposed solution consists of two phases. Duringthe first, time segments with good visibility of the colon and gaps betweenthem are identified. During the second phase, a trained model operates on eachgap, answering the question: Do you observe the same scene before and after thegap? If the answer is negative, the endoscopist is alerted and can be directedto the appropriate area in real-time. The second phase model is trained using acontrastive loss based on auto-generated examples. Our method evaluation on adataset of 250 procedures annotated by trained physicians provides sensitivityof 0.75 with specificity of 0.9.</description><author>G. Leifman, I. Kligvasser, R. Goldenberg, M. Elad, E. Rivlin</author><pubDate>Wed, 17 May 2023 09:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10026v1</guid></item><item><title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking</title><link>http://arxiv.org/abs/2307.02508v1</link><description>The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objecttracking and segmentation. In this study, we convert the bounding boxes tomasks in reference frames with the help of the Segment Anything Model (SAM) andAlpha-Refine, and then propagate the masks to the current frame, transformingthe task from Video Object Tracking (VOT) to video object segmentation (VOS).Furthermore, we introduce MSDeAOT, a variant of the AOT series thatincorporates transformers at multiple feature scales. MSDeAOT efficientlypropagates object masks from previous frames to the current frame using twofeature scales of 16 and 8. As a testament to the effectiveness of our design,we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object TrackingChallenge.</description><author>Yuanyou Xu, Jiahao Li, Zongxin Yang, Yi Yang, Yueting Zhuang</author><pubDate>Wed, 05 Jul 2023 04:50:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02508v1</guid></item><item><title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking</title><link>http://arxiv.org/abs/2307.02508v2</link><description>The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objecttracking and segmentation. In this study, we convert the bounding boxes tomasks in reference frames with the help of the Segment Anything Model (SAM) andAlpha-Refine, and then propagate the masks to the current frame, transformingthe task from Video Object Tracking (VOT) to video object segmentation (VOS).Furthermore, we introduce MSDeAOT, a variant of the AOT series thatincorporates transformers at multiple feature scales. MSDeAOT efficientlypropagates object masks from previous frames to the current frame using twofeature scales of 16 and 8. As a testament to the effectiveness of our design,we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object TrackingChallenge.</description><author>Yuanyou Xu, Jiahao Li, Zongxin Yang, Yi Yang, Yueting Zhuang</author><pubDate>Mon, 10 Jul 2023 10:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02508v2</guid></item><item><title>EPIC Fields: Marrying 3D Geometry and Video Understanding</title><link>http://arxiv.org/abs/2306.08731v1</link><description>Neural rendering is fuelling a unification of learning, 3D geometry and videounderstanding that has been waiting for more than two decades. Progress,however, is still hampered by a lack of suitable datasets and benchmarks. Toaddress this gap, we introduce EPIC Fields, an augmentation of EPIC-KITCHENSwith 3D camera information. Like other datasets for neural rendering, EPICFields removes the complex and expensive step of reconstructing cameras usingphotogrammetry, and allows researchers to focus on modelling problems. Weillustrate the challenge of photogrammetry in egocentric videos of dynamicactions and propose innovations to address them. Compared to other neuralrendering datasets, EPIC Fields is better tailored to video understandingbecause it is paired with labelled action segments and the recent VISOR segmentannotations. To further motivate the community, we also evaluate two benchmarktasks in neural rendering and segmenting dynamic objects, with strong baselinesthat showcase what is not possible today. We also highlight the advantage ofgeometry in semi-supervised video object segmentations on the VISORannotations. EPIC Fields reconstructs 96% of videos in EPICKITCHENS,registering 19M frames in 99 hours recorded in 45 kitchens.</description><author>Vadim Tschernezki, Ahmad Darkhalil, Zhifan Zhu, David Fouhey, Iro Laina, Diane Larlus, Dima Damen, Andrea Vedaldi</author><pubDate>Wed, 14 Jun 2023 21:33:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08731v1</guid></item><item><title>VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions</title><link>http://arxiv.org/abs/2305.18756v1</link><description>Video-grounded dialogue understanding is a challenging problem that requiresmachine to perceive, parse and reason over situated semantics extracted fromweakly aligned video and dialogues. Most existing benchmarks treat bothmodalities the same as a frame-independent visual understanding task, whileneglecting the intrinsic attributes in multimodal dialogues, such as scene andtopic transitions. In this paper, we present Video-grounded Scene&amp;Topic AwaRedialogue (VSTAR) dataset, a large scale video-grounded dialogue understandingdataset based on 395 TV series. Based on VSTAR, we propose two benchmarks forvideo-grounded dialogue understanding: scene segmentation and topicsegmentation, and one benchmark for video-grounded dialogue generation.Comprehensive experiments are performed on these benchmarks to demonstrate theimportance of multimodal information and segments in video-grounded dialogueunderstanding and generation.</description><author>Yuxuan Wang, Zilong Zheng, Xueliang Zhao, Jinpeng Li, Yueqian Wang, Dongyan Zhao</author><pubDate>Tue, 30 May 2023 06:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18756v1</guid></item><item><title>Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising</title><link>http://arxiv.org/abs/2305.18264v1</link><description>Leveraging large-scale image-text datasets and advancements in diffusionmodels, text-driven generative models have made remarkable strides in the fieldof image generation and editing. This study explores the potential of extendingthe text-driven ability to the generation and editing of multi-text conditionedlong videos. Current methodologies for video generation and editing, whileinnovative, are often confined to extremely short videos (typically less than24 frames) and are limited to a single text condition. These constraintssignificantly limit their applications given that real-world videos usuallyconsist of multiple segments, each bearing different semantic information. Toaddress this challenge, we introduce a novel paradigm dubbed as Gen-L-Video,capable of extending off-the-shelf short video diffusion models for generatingand editing videos comprising hundreds of frames with diverse semantic segmentswithout introducing additional training, all while preserving contentconsistency. We have implemented three mainstream text-driven video generationand editing methodologies and extended them to accommodate longer videos imbuedwith a variety of semantic segments with our proposed paradigm. Ourexperimental outcomes reveal that our approach significantly broadens thegenerative and editing capabilities of video diffusion models, offering newpossibilities for future research and applications. The code is available athttps://github.com/G-U-N/Gen-L-Video.</description><author>Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, Hongsheng Li</author><pubDate>Mon, 29 May 2023 18:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18264v1</guid></item><item><title>3rd Place Solution for PVUW2023 VSS Track: A Large Model for Semantic Segmentation on VSPW</title><link>http://arxiv.org/abs/2306.02291v2</link><description>In this paper, we introduce 3rd place solution for PVUW2023 VSS track.Semantic segmentation is a fundamental task in computer vision with numerousreal-world applications. We have explored various image-level visual backbonesand segmentation heads to tackle the problem of video semantic segmentation.Through our experimentation, we find that InternImage-H as the backbone andMask2former as the segmentation head achieves the best performance. Inaddition, we explore two post-precessing methods: CascadePSP and SegmentAnything Model (SAM). Ultimately, our approach obtains 62.60\% and 64.84\% mIoUon the VSPW test set1 and final test set, respectively, securing the thirdposition in the PVUW2023 VSS track.</description><author>Shijie Chang, Zeqi Hao, Ben Kang, Xiaoqi Zhao, Jiawen Zhu, Zhenyu Chen, Lihe Zhang, Lu Zhang, Huchuan Lu</author><pubDate>Tue, 06 Jun 2023 02:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02291v2</guid></item><item><title>Semantic Segmentation on VSPW Dataset through Contrastive Loss and Multi-dataset Training Approach</title><link>http://arxiv.org/abs/2306.03508v1</link><description>Video scene parsing incorporates temporal information, which can enhance theconsistency and accuracy of predictions compared to image scene parsing. Theadded temporal dimension enables a more comprehensive understanding of thescene, leading to more reliable results. This paper presents the winningsolution of the CVPR2023 workshop for video semantic segmentation, focusing onenhancing Spatial-Temporal correlations with contrastive loss. We also explorethe influence of multi-dataset training by utilizing a label-mapping technique.And the final result is aggregating the output of the above two models. Ourapproach achieves 65.95% mIoU performance on the VSPW dataset, ranked 1st placeon the VSPW challenge at CVPR 2023.</description><author>Min Yan, Qianxiong Ning, Qian Wang</author><pubDate>Tue, 06 Jun 2023 09:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03508v1</guid></item><item><title>Video-to-Music Recommendation using Temporal Alignment of Segments</title><link>http://arxiv.org/abs/2306.07187v1</link><description>We study cross-modal recommendation of music tracks to be used as soundtracksfor videos. This problem is known as the music supervision task. We build on aself-supervised system that learns a content association between music andvideo. In addition to the adequacy of content, adequacy of structure is crucialin music supervision to obtain relevant recommendations. We propose a novelapproach to significantly improve the system's performance usingstructure-aware recommendation. The core idea is to consider not only the fullaudio-video clips, but rather shorter segments for training and inference. Wefind that using semantic segments and ranking the tracks according to sequencealignment costs significantly improves the results. We investigate the impactof different ranking metrics and segmentation methods.</description><author>Laure Prétet, Gaël Richard, Clément Souchier, Geoffroy Peeters</author><pubDate>Mon, 12 Jun 2023 16:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07187v1</guid></item><item><title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</title><link>http://arxiv.org/abs/2306.00595v2</link><description>We focus on the weakly-supervised audio-visual video parsing task (AVVP),which aims to identify and locate all the events in audio/visual modalities.Previous works only concentrate on video-level overall label denoising acrossmodalities, but overlook the segment-level label noise, where adjacent videosegments (i.e., 1-second video clips) may contain different events. However,recognizing events in the segment is challenging because its label could be anycombination of events that occur in the video. To address this issue, weconsider tackling AVVP from the language perspective, since language couldfreely describe how various events appear in each segment beyond fixed labels.Specifically, we design language prompts to describe all cases of eventappearance for each video. Then, the similarity between language prompts andsegments is calculated, where the event of the most similar prompt is regardedas the segment-level label. In addition, to deal with the mislabeled segments,we propose to perform dynamic re-weighting on the unreliable segments to adjusttheir labels. Experiments show that our simple yet effective approachoutperforms state-of-the-art methods by a large margin.</description><author>Yingying Fan, Yu Wu, Yutian Lin, Bo Du</author><pubDate>Wed, 07 Jun 2023 07:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00595v2</guid></item><item><title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</title><link>http://arxiv.org/abs/2306.00595v3</link><description>We focus on the weakly-supervised audio-visual video parsing task (AVVP),which aims to identify and locate all the events in audio/visual modalities.Previous works only concentrate on video-level overall label denoising acrossmodalities, but overlook the segment-level label noise, where adjacent videosegments (i.e., 1-second video clips) may contain different events. However,recognizing events in the segment is challenging because its label could be anycombination of events that occur in the video. To address this issue, weconsider tackling AVVP from the language perspective, since language couldfreely describe how various events appear in each segment beyond fixed labels.Specifically, we design language prompts to describe all cases of eventappearance for each video. Then, the similarity between language prompts andsegments is calculated, where the event of the most similar prompt is regardedas the segment-level label. In addition, to deal with the mislabeled segments,we propose to perform dynamic re-weighting on the unreliable segments to adjusttheir labels. Experiments show that our simple yet effective approachoutperforms state-of-the-art methods by a large margin.</description><author>Yingying Fan, Yu Wu, Yutian Lin, Bo Du</author><pubDate>Wed, 14 Jun 2023 15:12:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00595v3</guid></item><item><title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</title><link>http://arxiv.org/abs/2306.00595v4</link><description>We focus on the weakly-supervised audio-visual video parsing task (AVVP),which aims to identify and locate all the events in audio/visual modalities.Previous works only concentrate on video-level overall label denoising acrossmodalities, but overlook the segment-level label noise, where adjacent videosegments (i.e., 1-second video clips) may contain different events. However,recognizing events in the segment is challenging because its label could be anycombination of events that occur in the video. To address this issue, weconsider tackling AVVP from the language perspective, since language couldfreely describe how various events appear in each segment beyond fixed labels.Specifically, we design language prompts to describe all cases of eventappearance for each video. Then, the similarity between language prompts andsegments is calculated, where the event of the most similar prompt is regardedas the segment-level label. In addition, to deal with the mislabeled segments,we propose to perform dynamic re-weighting on the unreliable segments to adjusttheir labels. Experiments show that our simple yet effective approachoutperforms state-of-the-art methods by a large margin.</description><author>Yingying Fan, Yu Wu, Yutian Lin, Bo Du</author><pubDate>Wed, 21 Jun 2023 14:49:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00595v4</guid></item><item><title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</title><link>http://arxiv.org/abs/2306.00595v1</link><description>We focus on the weakly-supervised audio-visual video parsing task (AVVP),which aims to identify and locate all the events in audio/visual modalities.Previous works only concentrate on video-level overall label denoising acrossmodalities, but overlook the segment-level label noise, where adjacent videosegments (i.e., 1-second video clips) may contain different events. However,recognizing events in the segment is challenging because its label could be anycombination of events that occur in the video. To address this issue, weconsider tackling AVVP from the language perspective, since language couldfreely describe how various events appear in each segment beyond fixed labels.Specifically, we design language prompts to describe all cases of eventappearance for each video. Then, the similarity between language prompts andsegments is calculated, where the event of the most similar prompt is regardedas the segment-level label. In addition, to deal with the mislabeled segments,we propose to perform dynamic re-weighting on the unreliable segments to adjusttheir labels. Experiments show that our simple yet effective approachoutperforms state-of-the-art methods by a large margin.</description><author>Yingying Fan, Yu Wu, Yutian Lin, Bo Du</author><pubDate>Thu, 01 Jun 2023 13:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00595v1</guid></item><item><title>Holistic Prototype Attention Network for Few-Shot VOS</title><link>http://arxiv.org/abs/2307.07933v1</link><description>Few-shot video object segmentation (FSVOS) aims to segment dynamic objects ofunseen classes by resorting to a small set of support images that containpixel-level object annotations. Existing methods have demonstrated that thedomain agent-based attention mechanism is effective in FSVOS by learning thecorrelation between support images and query frames. However, the agent framecontains redundant pixel information and background noise, resulting ininferior segmentation performance. Moreover, existing methods tend to ignoreinter-frame correlations in query videos. To alleviate the above dilemma, wepropose a holistic prototype attention network (HPAN) for advancing FSVOS.Specifically, HPAN introduces a prototype graph attention module (PGAM) and abidirectional prototype attention module (BPAM), transferring informativeknowledge from seen to unseen classes. PGAM generates local prototypes from allforeground features and then utilizes their internal correlations to enhancethe representation of the holistic prototypes. BPAM exploits the holisticinformation from support images and video frames by fusing co-attention andself-attention to achieve support-query semantic consistency and inner-frametemporal consistency. Extensive experiments on YouTube-FSVOS have been providedto demonstrate the effectiveness and superiority of our proposed HPAN method.</description><author>Yin Tang, Tao Chen, Xiruo Jiang, Yazhou Yao, Guo-Sen Xie, Heng-Tao Shen</author><pubDate>Sun, 16 Jul 2023 04:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07933v1</guid></item><item><title>Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization</title><link>http://arxiv.org/abs/2204.06228v2</link><description>Due to its high societal impact, deepfake detection is getting activeattention in the computer vision community. Most deepfake detection methodsrely on identity, facial attributes, and adversarial perturbation-basedspatio-temporal modifications at the whole video or random locations whilekeeping the meaning of the content intact. However, a sophisticated deepfakemay contain only a small segment of video/audio manipulation, through which themeaning of the content can be, for example, completely inverted from asentiment perspective. We introduce a content-driven audio-visual deepfakedataset, termed Localized Audio Visual DeepFake (LAV-DF), explicitly designedfor the task of learning temporal forgery localization. Specifically, thecontent-driven audio-visual manipulations are performed strategically to changethe sentiment polarity of the whole video. Our baseline method for benchmarkingthe proposed dataset is a 3DCNN model, termed as Boundary Aware TemporalForgery Detection (BA-TFD), which is guided via contrastive, boundary matching,and frame classification loss functions. Our extensive quantitative andqualitative analysis demonstrates the proposed method's strong performance fortemporal forgery localization and deepfake detection tasks.</description><author>Zhixi Cai, Kalin Stefanov, Abhinav Dhall, Munawar Hayat</author><pubDate>Thu, 04 May 2023 01:41:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.06228v2</guid></item><item><title>RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation</title><link>http://arxiv.org/abs/2307.06577v1</link><description>Retinal vessel segmentation is generally grounded in image-based datasetscollected with bench-top devices. The static images naturally lose the dynamiccharacteristics of retina fluctuation, resulting in diminished datasetrichness, and the usage of bench-top devices further restricts datasetscalability due to its limited accessibility. Considering these limitations, weintroduce the first video-based retinal dataset by employing handheld devicesfor data acquisition. The dataset comprises 635 smartphone-based fundus videoscollected from four different clinics, involving 415 patients from 50 to 75years old. It delivers comprehensive and precise annotations of retinalstructures in both spatial and temporal dimensions, aiming to advance thelandscape of vasculature segmentation. Specifically, the dataset provides threelevels of spatial annotations: binary vessel masks for overall retinalstructure delineation, general vein-artery masks for distinguishing the veinand artery, and fine-grained vein-artery masks for further characterizing thegranularities of each artery and vein. In addition, the dataset offers temporalannotations that capture the vessel pulsation characteristics, assisting indetecting ocular diseases that require fine-grained recognition of hemodynamicfluctuation. In application, our dataset exhibits a significant domain shiftwith respect to data captured by bench-top devices, thus posing greatchallenges to existing methods. In the experiments, we provide evaluationmetrics and benchmark results on our dataset, reflecting both the potential andchallenges it offers for vessel segmentation tasks. We hope this challengingdataset would significantly contribute to the development of eye diseasediagnosis and early prevention.</description><author>MD Wahiduzzaman Khan, Hongwei Sheng, Hu Zhang, Heming Du, Sen Wang, Minas Theodore Coroneo, Farshid Hajati, Sahar Shariflou, Michael Kalloniatis, Jack Phu, Ashish Agar, Zi Huang, Mojtaba Golzan, Xin Yu</author><pubDate>Thu, 13 Jul 2023 07:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06577v1</guid></item><item><title>Boosting Weakly-Supervised Temporal Action Localization with Text Information</title><link>http://arxiv.org/abs/2305.00607v1</link><description>Due to the lack of temporal annotation, current Weakly-supervised TemporalAction Localization (WTAL) methods are generally stuck into over-complete orincomplete localization. In this paper, we aim to leverage the text informationto boost WTAL from two aspects, i.e., (a) the discriminative objective toenlarge the inter-class difference, thus reducing the over-complete; (b) thegenerative objective to enhance the intra-class integrity, thus finding morecomplete temporal boundaries. For the discriminative objective, we propose aText-Segment Mining (TSM) mechanism, which constructs a text description basedon the action class label, and regards the text as the query to mine allclass-related segments. Without the temporal annotation of actions, TSMcompares the text query with the entire videos across the dataset to mine thebest matching segments while ignoring irrelevant ones. Due to the sharedsub-actions in different categories of videos, merely applying TSM is toostrict to neglect the semantic-related segments, which results in incompletelocalization. We further introduce a generative objective named Video-textLanguage Completion (VLC), which focuses on all semantic-related segments fromvideos to complete the text sentence. We achieve the state-of-the-artperformance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find ourproposed method can be seamlessly applied to existing methods, and improvetheir performances with a clear margin. The code is available athttps://github.com/lgzlIlIlI/Boosting-WTAL.</description><author>Guozhang Li, De Cheng, Xinpeng Ding, Nannan Wang, Xiaoyu Wang, Xinbo Gao</author><pubDate>Mon, 01 May 2023 01:07:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00607v1</guid></item><item><title>Tracking through Containers and Occluders in the Wild</title><link>http://arxiv.org/abs/2305.03052v1</link><description>Tracking objects with persistence in cluttered and dynamic environmentsremains a difficult challenge for computer vision systems. In this paper, weintroduce $\textbf{TCOW}$, a new benchmark and model for visual trackingthrough heavy occlusion and containment. We set up a task where the goal is to,given a video sequence, segment both the projected extent of the target object,as well as the surrounding container or occluder whenever one exists. To studythis task, we create a mixture of synthetic and annotated real datasets tosupport both supervised learning and structured evaluation of model performanceunder various forms of task variation, such as moving or nested containment. Weevaluate two recent transformer-based video models and find that while they canbe surprisingly capable of tracking targets under certain settings of taskvariation, there remains a considerable performance gap before we can claim atracking model to have acquired a true notion of object permanence.</description><author>Basile Van Hoorick, Pavel Tokmakov, Simon Stent, Jie Li, Carl Vondrick</author><pubDate>Thu, 04 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03052v1</guid></item><item><title>Mean Shift Mask Transformer for Unseen Object Instance Segmentation</title><link>http://arxiv.org/abs/2211.11679v2</link><description>Segmenting unseen objects from images is a critical perception skill that arobot needs to acquire. In robot manipulation, it can facilitate a robot tograsp and manipulate unseen objects. Mean shift clustering is a widely usedmethod for image segmentation tasks. However, the traditional mean shiftclustering algorithm is not differentiable, making it difficult to integrate itinto an end-to-end neural network training framework. In this work, we proposethe Mean Shift Mask Transformer (MSMFormer), a new transformer architecturethat simulates the von Mises-Fisher (vMF) mean shift clustering algorithm,allowing for the joint training and inference of both the feature extractor andthe clustering. Its central component is a hypersphere attention mechanism,which updates object queries on a hypersphere. To illustrate the effectivenessof our method, we apply MSMFormer to unseen object instance segmentation. Ourexperiments show that MSMFormer achieves competitive performance compared tostate-of-the-art methods for unseen object instance segmentation. The video andcode are available at https://irvlutd.github.io/MSMFormer</description><author>Yangxiao Lu, Yuqiao Chen, Nicholas Ruozzi, Yu Xiang</author><pubDate>Mon, 12 Jun 2023 18:05:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11679v2</guid></item><item><title>Siamese Masked Autoencoders</title><link>http://arxiv.org/abs/2305.14344v1</link><description>Establishing correspondence between images or scenes is a significantchallenge in computer vision, especially given occlusions, viewpoint changes,and varying object appearances. In this paper, we present Siamese MaskedAutoencoders (SiamMAE), a simple extension of Masked Autoencoders (MAE) forlearning visual correspondence from videos. SiamMAE operates on pairs ofrandomly sampled video frames and asymmetrically masks them. These frames areprocessed independently by an encoder network, and a decoder composed of asequence of cross-attention layers is tasked with predicting the missingpatches in the future frame. By masking a large fraction ($95\%$) of patches inthe future frame while leaving the past frame unchanged, SiamMAE encourages thenetwork to focus on object motion and learn object-centric representations.Despite its conceptual simplicity, features learned via SiamMAE outperformstate-of-the-art self-supervised methods on video object segmentation, posekeypoint propagation, and semantic part propagation tasks. SiamMAE achievescompetitive results without relying on data augmentation, handcraftedtracking-based pretext tasks, or other techniques to prevent representationalcollapse.</description><author>Agrim Gupta, Jiajun Wu, Jia Deng, Li Fei-Fei</author><pubDate>Tue, 23 May 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14344v1</guid></item><item><title>DQ-Det: Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation</title><link>http://arxiv.org/abs/2307.12239v1</link><description>Transformer-based detection and segmentation methods use a list of learneddetection queries to retrieve information from the transformer network andlearn to predict the location and category of one specific object from eachquery. We empirically find that random convex combinations of the learnedqueries are still good for the corresponding models. We then propose to learn aconvex combination with dynamic coefficients based on the high-level semanticsof the image. The generated dynamic queries, named modulated queries, bettercapture the prior of object locations and categories in the different images.Equipped with our modulated queries, a wide range of DETR-based models achieveconsistent and superior performance across multiple tasks including objectdetection, instance segmentation, panoptic segmentation, and video instancesegmentation.</description><author>Yiming Cui, Linjie Yang, Haichao Yu</author><pubDate>Sun, 23 Jul 2023 07:26:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12239v1</guid></item><item><title>A Survey on Video Moment Localization</title><link>http://arxiv.org/abs/2306.07515v1</link><description>Video moment localization, also known as video moment retrieval, aiming tosearch a target segment within a video described by a given natural languagequery. Beyond the task of temporal action localization whereby the targetactions are pre-defined, video moment retrieval can query arbitrary complexactivities. In this survey paper, we aim to present a comprehensive review ofexisting video moment localization techniques, including supervised, weaklysupervised, and unsupervised ones. We also review the datasets available forvideo moment localization and group results of related work. In addition, wediscuss promising future directions for this field, in particular large-scaledatasets and interpretable video moment localization models.</description><author>Meng Liu, Liqiang Nie, Yunxiao Wang, Meng Wang, Yong Rui</author><pubDate>Tue, 13 Jun 2023 03:57:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07515v1</guid></item><item><title>Multimodal Short Video Rumor Detection System Based on Contrastive Learning</title><link>http://arxiv.org/abs/2304.08401v3</link><description>With the rise of short video platforms as prominent channels for newsdissemination, major platforms in China have gradually evolved into fertilegrounds for the proliferation of fake news. However, distinguishing short videorumors poses a significant challenge due to the substantial amount ofinformation and shared features among videos, resulting in homogeneity. Toaddress the dissemination of short video rumors effectively, our research groupproposes a methodology encompassing multimodal feature fusion and theintegration of external knowledge, considering the merits and drawbacks of eachalgorithm. The proposed detection approach entails the following steps: (1)creation of a comprehensive dataset comprising multiple features extracted fromshort videos; (2) development of a multimodal rumor detection model: first, weemploy the Temporal Segment Networks (TSN) video coding model to extract videofeatures, followed by the utilization of Optical Character Recognition (OCR)and Automatic Speech Recognition (ASR) to extract textual features.Subsequently, the BERT model is employed to fuse textual and video features;(3) distinction is achieved through contrast learning: we acquire externalknowledge by crawling relevant sources and leverage a vector database toincorporate this knowledge into the classification output. Our research processis driven by practical considerations, and the knowledge derived from thisstudy will hold significant value in practical scenarios, such as short videorumor identification and the management of social opinions.</description><author>Yuxing Yang, Junhao Zhao, Siyi Wang, Xiangyu Min, Pengchao Wang, Haizhou Wang</author><pubDate>Wed, 17 May 2023 14:12:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08401v3</guid></item><item><title>Learning Grounded Vision-Language Representation for Versatile Understanding in Untrimmed Videos</title><link>http://arxiv.org/abs/2303.06378v2</link><description>Joint video-language learning has received increasing attention in recentyears. However, existing works mainly focus on single or multiple trimmed videoclips (events), which makes human-annotated event boundaries necessary duringinference. To break away from the ties, we propose a grounded vision-languagelearning framework for untrimmed videos, which automatically detectsinformative events and effectively excavates the alignments betweenmulti-sentence descriptions and corresponding event segments. Instead ofcoarse-level video-language alignments, we present two dual pretext tasks toencourage fine-grained segment-level alignments, i.e., text-to-event grounding(TEG) and event-to-text generation (ETG). TEG learns to adaptively ground thepossible event proposals given a set of sentences by estimating the cross-modaldistance in a joint semantic space. Meanwhile, ETG aims to reconstruct(generate) the matched texts given event proposals, encouraging the eventrepresentation to retain meaningful semantic information. To encourage accuratelabel assignment between the event set and the text set, we propose a novelsemantic-aware cost to mitigate the sub-optimal matching results caused byambiguous boundary annotations. Our framework is easily extensible to taskscovering visually-grounded language understanding and generation. We achievestate-of-the-art dense video captioning performance on ActivityNet Captions,YouCook2 and YouMakeup, and competitive performance on several other languagegeneration and understanding tasks. Our method also achieved 1st place in boththe MTVG and MDVC tasks of the PIC 4th Challenge. Our code is publiclyavailable at https://github.com/zjr2000/GVL.</description><author>Teng Wang, Jinrui Zhang, Feng Zheng, Wenhao Jiang, Ran Cheng, Ping Luo</author><pubDate>Wed, 17 May 2023 10:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06378v2</guid></item><item><title>Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization</title><link>http://arxiv.org/abs/2305.17861v1</link><description>Weakly-supervised temporal action localization aims to localize and recognizeactions in untrimmed videos with only video-level category labels duringtraining. Without instance-level annotations, most existing methods follow theSegment-based Multiple Instance Learning (S-MIL) framework, where thepredictions of segments are supervised by the labels of videos. However, theobjective for acquiring segment-level scores during training is not consistentwith the target for acquiring proposal-level scores during testing, leading tosuboptimal results. To deal with this problem, we propose a novelProposal-based Multiple Instance Learning (P-MIL) framework that directlyclassifies the candidate proposals in both the training and testing stages,which includes three key designs: 1) a surrounding contrastive featureextraction module to suppress the discriminative short proposals by consideringthe surrounding contrastive information, 2) a proposal completeness evaluationmodule to inhibit the low-quality proposals with the guidance of thecompleteness pseudo labels, and 3) an instance-level rank consistency loss toachieve robust detection by leveraging the complementarity of RGB and FLOWmodalities. Extensive experimental results on two challenging benchmarksincluding THUMOS14 and ActivityNet demonstrate the superior performance of ourmethod.</description><author>Huan Ren, Wenfei Yang, Tianzhu Zhang, Yongdong Zhang</author><pubDate>Mon, 29 May 2023 03:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17861v1</guid></item><item><title>MomentDiff: Generative Video Moment Retrieval from Random to Real</title><link>http://arxiv.org/abs/2307.02869v1</link><description>Video moment retrieval pursues an efficient and generalized solution toidentify the specific temporal segments within an untrimmed video thatcorrespond to a given language description. To achieve this goal, we provide agenerative diffusion-based framework called MomentDiff, which simulates atypical human retrieval process from random browsing to gradual localization.Specifically, we first diffuse the real span to random noise, and learn todenoise the random noise to the original span with the guidance of similaritybetween text and video. This allows the model to learn a mapping from arbitraryrandom locations to real moments, enabling the ability to locate segments fromrandom initialization. Once trained, MomentDiff could sample random temporalsegments as initial guesses and iteratively refine them to generate an accuratetemporal boundary. Different from discriminative works (e.g., based onlearnable proposals or queries), MomentDiff with random initialized spans couldresist the temporal location biases from datasets. To evaluate the influence ofthe temporal location biases, we propose two anti-bias datasets with locationdistribution shifts, named Charades-STA-Len and Charades-STA-Mom. Theexperimental results demonstrate that our efficient framework consistentlyoutperforms state-of-the-art methods on three public benchmarks, and exhibitsbetter generalization and robustness on the proposed anti-bias datasets. Thecode, model, and anti-bias evaluation datasets are available athttps://github.com/IMCCretrieval/MomentDiff.</description><author>Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, Lei Zhang, Yun Zheng, Deli Zhao, Yongdong Zhang</author><pubDate>Thu, 06 Jul 2023 10:12:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02869v1</guid></item><item><title>Echocardiography Segmentation Using Neural ODE-based Diffeomorphic Registration Field</title><link>http://arxiv.org/abs/2306.09687v1</link><description>Convolutional neural networks (CNNs) have recently proven their excellentability to segment 2D cardiac ultrasound images. However, the majority ofattempts to perform full-sequence segmentation of cardiac ultrasound videoseither rely on models trained only on keyframe images or fail to maintain thetopology over time. To address these issues, in this work, we considersegmentation of ultrasound video as a registration estimation problem andpresent a novel method for diffeomorphic image registration using neuralordinary differential equations (Neural ODE). In particular, we consider theregistration field vector field between frames as a continuous trajectory ODE.The estimated registration field is then applied to the segmentation mask ofthe first frame to obtain a segment for the whole cardiac cycle. The proposedmethod, Echo-ODE, introduces several key improvements compared to the previousstate-of-the-art. Firstly, by solving a continuous ODE, the proposed methodachieves smoother segmentation, preserving the topology of segmentation mapsover the whole sequence (Hausdorff distance: 3.7-4.4). Secondly, it maintainstemporal consistency between frames without explicitly optimizing for temporalconsistency attributes, achieving temporal consistency in 91% of the videos inthe dataset. Lastly, the proposed method is able to maintain the clinicalaccuracy of the segmentation maps (MAE of the LVEF: 2.7-3.1). The results showthat our method surpasses the previous state-of-the-art in multiple aspects,demonstrating the importance of spatial-temporal data processing for theimplementation of Neural ODEs in medical imaging applications. These findingsopen up new research directions for solving echocardiography segmentationtasks.</description><author>Phi Nguyen Van, Hieu Pham Huy, Long Tran Quoc</author><pubDate>Fri, 16 Jun 2023 09:37:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09687v1</guid></item><item><title>Domain Adaptive Sim-to-Real Segmentation of Oropharyngeal Organs</title><link>http://arxiv.org/abs/2305.10883v1</link><description>Video-assisted transoral tracheal intubation (TI) necessitates using anendoscope that helps the physician insert a tracheal tube into the glottisinstead of the esophagus. The growing trend of robotic-assisted TI wouldrequire a medical robot to distinguish anatomical features like an experiencedphysician which can be imitated by utilizing supervised deep-learningtechniques. However, the real datasets of oropharyngeal organs are ofteninaccessible due to limited open-source data and patient privacy. In this work,we propose a domain adaptive Sim-to-Real framework called IoU-RankingBlend-ArtFlow (IRB-AF) for image segmentation of oropharyngeal organs. Theframework includes an image blending strategy called IoU-Ranking Blend (IRB)and style-transfer method ArtFlow. Here, IRB alleviates the problem of poorsegmentation performance caused by significant datasets domain differences;while ArtFlow is introduced to reduce the discrepancies between datasetsfurther. A virtual oropharynx image dataset generated by the SOFA framework isused as the learning subject for semantic segmentation to deal with the limitedavailability of actual endoscopic images. We adapted IRB-AF with thestate-of-the-art domain adaptive segmentation models. The results demonstratethe superior performance of our approach in further improving the segmentationaccuracy and training stability.</description><author>Guankun Wang, Tian-Ao Ren, Jiewen Lai, Long Bai, Hongliang Ren</author><pubDate>Thu, 18 May 2023 12:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10883v1</guid></item><item><title>Transformer-Based Visual Segmentation: A Survey</title><link>http://arxiv.org/abs/2304.09854v2</link><description>Visual segmentation seeks to partition images, video frames, or point cloudsinto multiple segments or groups. This technique has numerous real-worldapplications, such as autonomous driving, image editing, robot sensing, andmedical analysis. Over the past decade, deep learning-based methods have maderemarkable strides in this area. Recently, transformers, a type of neuralnetwork based on self-attention originally designed for natural languageprocessing, have considerably surpassed previous convolutional or recurrentapproaches in various vision processing tasks. Specifically, visiontransformers offer robust, unified, and even simpler solutions for varioussegmentation tasks. This survey provides a thorough overview oftransformer-based visual segmentation, summarizing recent advancements. Wefirst review the background, encompassing problem definitions, datasets, andprior convolutional methods. Next, we summarize a meta-architecture thatunifies all recent transformer-based approaches. Based on thismeta-architecture, we examine various method designs, including modificationsto the meta-architecture and associated applications. We also present severalclosely related settings, including 3D point cloud segmentation, foundationmodel tuning, domain-aware segmentation, efficient segmentation, and medicalsegmentation. Additionally, we compile and re-evaluate the reviewed methods onseveral well-established datasets. Finally, we identify open challenges in thisfield and propose directions for future research. The project page can be foundat https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will alsocontinually monitor developments in this rapidly evolving field.</description><author>Xiangtai Li, Henghui Ding, Wenwei Zhang, Haobo Yuan, Jiangmiao Pang, Guangliang Cheng, Kai Chen, Ziwei Liu, Chen Change Loy</author><pubDate>Fri, 02 Jun 2023 08:33:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09854v2</guid></item><item><title>PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?</title><link>http://arxiv.org/abs/2202.05821v3</link><description>This paper presents the design and results of the "PEg TRAnsfert Workflowrecognition" (PETRAW) challenge whose objective was to develop surgicalworkflow recognition methods based on one or several modalities, among video,kinematic, and segmentation data, in order to study their added value. ThePETRAW challenge provided a data set of 150 peg transfer sequences performed ona virtual simulator. This data set was composed of videos, kinematics, semanticsegmentation, and workflow annotations which described the sequences at threedifferent granularity levels: phase, step, and activity. Five tasks wereproposed to the participants: three of them were related to the recognition ofall granularities with one of the available modalities, while the othersaddressed the recognition with a combination of modalities. Averageapplication-dependent balanced accuracy (AD-Accuracy) was used as evaluationmetric to take unbalanced classes into account and because it is moreclinically relevant than a frame-by-frame score. Seven teams participated in atleast one task and four of them in all tasks. Best results are obtained withthe use of the video and the kinematics data with an AD-Accuracy between 93%and 90% for the four teams who participated in all tasks. The improvementbetween video/kinematic-based methods and the uni-modality ones was significantfor all of the teams. However, the difference in testing execution time betweenthe video/kinematic-based and the kinematic-based methods has to be taken intoconsideration. Is it relevant to spend 20 to 200 times more computing time forless than 3% of improvement? The PETRAW data set is publicly available atwww.synapse.org/PETRAW to encourage further research in surgical workflowrecognition.</description><author>Arnaud Huaulmé, Kanako Harada, Quang-Minh Nguyen, Bogyu Park, Seungbum Hong, Min-Kook Choi, Michael Peven, Yunshuang Li, Yonghao Long, Qi Dou, Satyadwyoom Kumar, Seenivasan Lalithkumar, Ren Hongliang, Hiroki Matsuzaki, Yuto Ishikawa, Yuriko Harai, Satoshi Kondo, Mamoru Mitsuishi, Pierre Jannin</author><pubDate>Thu, 27 Apr 2023 14:27:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.05821v3</guid></item><item><title>Weakly-supervised Micro- and Macro-expression Spotting Based on Multi-level Consistency</title><link>http://arxiv.org/abs/2305.02734v1</link><description>Most micro- and macro-expression spotting methods in untrimmed videos sufferfrom the burden of video-wise collection and frame-wise annotation.Weakly-supervised expression spotting (WES) based on video-level labels canpotentially mitigate the complexity of frame-level annotation while achievingfine-grained frame-level spotting. However, we argue that existingweakly-supervised methods are based on multiple instance learning (MIL)involving inter-modality, inter-sample, and inter-task gaps. The inter-samplegap is primarily from the sample distribution and duration. Therefore, wepropose a novel and simple WES framework, MC-WES, using multi-consistencycollaborative mechanisms that include modal-level saliency, video-leveldistribution, label-level duration and segment-level feature consistencystrategies to implement fine frame-level spotting with only video-level labelsto alleviate the above gaps and merge prior knowledge. The modal-level saliencyconsistency strategy focuses on capturing key correlations between raw imagesand optical flow. The video-level distribution consistency strategy utilizesthe difference of sparsity in temporal distribution. The label-level durationconsistency strategy exploits the difference in the duration of facial muscles.The segment-level feature consistency strategy emphasizes that features underthe same labels maintain similarity. Experimental results on two challengingdatasets -- CAS(ME)$^2$ and SAMM-LV -- demonstrate that MC-WES is comparable tostate-of-the-art fully-supervised methods.</description><author>Wang-Wang Yu, Kai-Fu Yang, Hong-Mei Yan, Yong-Jie Li</author><pubDate>Thu, 04 May 2023 12:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02734v1</guid></item><item><title>MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning</title><link>http://arxiv.org/abs/2306.02252v1</link><description>We introduce MoviePuzzle, a novel challenge that targets visual narrativereasoning and holistic movie understanding. Despite the notable progress thathas been witnessed in the realm of video understanding, most prior works failto present tasks and models to address holistic video understanding and theinnate visual narrative structures existing in long-form videos. To tackle thisquandary, we put forth MoviePuzzle task that amplifies the temporal featurelearning and structure learning of video models by reshuffling the shot, frame,and clip layers of movie segments in the presence of video-dialogueinformation. We start by establishing a carefully refined dataset based onMovieNet by dissecting movies into hierarchical layers and randomly permutingthe orders. Besides benchmarking the MoviePuzzle with prior arts on movieunderstanding, we devise a Hierarchical Contrastive Movie Clustering (HCMC)model that considers the underlying structure and visual semantic orders formovie reordering. Specifically, through a pairwise and contrastive learningapproach, we train models to predict the correct order of each layer. Thisequips them with the knack for deciphering the visual narrative structure ofmovies and handling the disorder lurking in video data. Experiments show thatour approach outperforms existing state-of-the-art methods on the \MoviePuzzlebenchmark, underscoring its efficacy.</description><author>Jianghui Wang, Yuxuan Wang, Dongyan Zhao, Zilong Zheng</author><pubDate>Sun, 04 Jun 2023 04:51:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02252v1</guid></item><item><title>MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning</title><link>http://arxiv.org/abs/2306.02252v2</link><description>We introduce MoviePuzzle, a novel challenge that targets visual narrativereasoning and holistic movie understanding. Despite the notable progress thathas been witnessed in the realm of video understanding, most prior works failto present tasks and models to address holistic video understanding and theinnate visual narrative structures existing in long-form videos. To tackle thisquandary, we put forth MoviePuzzle task that amplifies the temporal featurelearning and structure learning of video models by reshuffling the shot, frame,and clip layers of movie segments in the presence of video-dialogueinformation. We start by establishing a carefully refined dataset based onMovieNet by dissecting movies into hierarchical layers and randomly permutingthe orders. Besides benchmarking the MoviePuzzle with prior arts on movieunderstanding, we devise a Hierarchical Contrastive Movie Clustering (HCMC)model that considers the underlying structure and visual semantic orders formovie reordering. Specifically, through a pairwise and contrastive learningapproach, we train models to predict the correct order of each layer. Thisequips them with the knack for deciphering the visual narrative structure ofmovies and handling the disorder lurking in video data. Experiments show thatour approach outperforms existing state-of-the-art methods on the \MoviePuzzlebenchmark, underscoring its efficacy.</description><author>Jianghui Wang, Yuxuan Wang, Dongyan Zhao, Zilong Zheng</author><pubDate>Wed, 14 Jun 2023 11:11:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02252v2</guid></item><item><title>Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos: A review of the state of the art</title><link>http://arxiv.org/abs/2304.13014v1</link><description>In the field of computer- and robot-assisted minimally invasive surgery,enormous progress has been made in recent years based on the recognition ofsurgical instruments in endoscopic images. Especially the determination of theposition and type of the instruments is of great interest here. Current workinvolves both spatial and temporal information with the idea, that theprediction of movement of surgical tools over time may improve the quality offinal segmentations. The provision of publicly available datasets has recentlyencouraged the development of new methods, mainly based on deep learning. Inthis review, we identify datasets used for method development and evaluation,as well as quantify their frequency of use in the literature. We furtherpresent an overview of the current state of research regarding the segmentationand tracking of minimally invasive surgical instruments in endoscopic images.The paper focuses on methods that work purely visually without attached markersof any kind on the instruments, taking into account both single-framesegmentation approaches as well as those involving temporal information. Adiscussion of the reviewed literature is provided, highlighting existingshortcomings and emphasizing available potential for future developments. Thepublications considered were identified through the platforms Google Scholar,Web of Science, and PubMed. The search terms used were "instrumentsegmentation", "instrument tracking", "surgical tool segmentation", and"surgical tool tracking" and result in 408 articles published between 2015 and2022 from which 109 were included using systematic selection criteria.</description><author>Tobias Rueckert, Daniel Rueckert, Christoph Palm</author><pubDate>Tue, 25 Apr 2023 18:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13014v1</guid></item><item><title>SF-TMN: SlowFast Temporal Modeling Network for Surgical Phase Recognition</title><link>http://arxiv.org/abs/2306.08859v1</link><description>Automatic surgical phase recognition is one of the key technologies tosupport Video-Based Assessment (VBA) systems for surgical education. Utilizingtemporal information is crucial for surgical phase recognition, hence variousrecent approaches extract frame-level features to conduct full video temporalmodeling. For better temporal modeling, we propose SlowFast Temporal ModelingNetwork (SF-TMN) for surgical phase recognition that can not only achieveframe-level full video temporal modeling but also achieve segment-level fullvideo temporal modeling. We employ a feature extraction network, pre-trained onthe target dataset, to extract features from video frames as the training datafor SF-TMN. The Slow Path in SF-TMN utilizes all frame features for frametemporal modeling. The Fast Path in SF-TMN utilizes segment-level featuressummarized from frame features for segment temporal modeling. The proposedparadigm is flexible regarding the choice of temporal modeling networks. Weexplore MS-TCN and ASFormer models as temporal modeling networks and experimentwith multiple combination strategies for Slow and Fast Paths. We evaluateSF-TMN on Cholec80 surgical phase recognition task and demonstrate that SF-TMNcan achieve state-of-the-art results on all considered metrics. SF-TMN withASFormer backbone outperforms the state-of-the-art Not End-to-End(TCN) methodby 2.6% in accuracy and 7.4% in the Jaccard score. We also evaluate SF-TMN onaction segmentation datasets including 50salads, GTEA, and Breakfast, andachieve state-of-the-art results. The improvement in the results shows thatcombining temporal information from both frame level and segment level byrefining outputs with temporal refinement stages is beneficial for the temporalmodeling of surgical phases.</description><author>Bokai Zhang, Mohammad Hasan Sarhan, Bharti Goel, Svetlana Petculescu, Amer Ghanem</author><pubDate>Thu, 15 Jun 2023 06:04:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08859v1</guid></item><item><title>Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation</title><link>http://arxiv.org/abs/2307.08779v1</link><description>Low-light conditions not only hamper human visual experience but also degradethe model's performance on downstream vision tasks. While existing works makeremarkable progress on day-night domain adaptation, they rely heavily on domainknowledge derived from the task-specific nighttime dataset. This paperchallenges a more complicated scenario with border applicability, i.e.,zero-shot day-night domain adaptation, which eliminates reliance on anynighttime data. Unlike prior zero-shot adaptation approaches emphasizing eitherimage-level translation or model-level adaptation, we propose a similaritymin-max paradigm that considers them under a unified framework. On the imagelevel, we darken images towards minimum feature similarity to enlarge thedomain gap. Then on the model level, we maximize the feature similarity betweenthe darkened images and their normal-light counterparts for better modeladaptation. To the best of our knowledge, this work represents the pioneeringeffort in jointly optimizing both aspects, resulting in a significantimprovement of model generalizability. Extensive experiments demonstrate ourmethod's effectiveness and broad applicability on various nighttime visiontasks, including classification, semantic segmentation, visual placerecognition, and video action recognition. Code and pre-trained models areavailable at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.</description><author>Rundong Luo, Wenjing Wang, Wenhan Yang, Jiaying Liu</author><pubDate>Mon, 17 Jul 2023 19:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08779v1</guid></item><item><title>Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation</title><link>http://arxiv.org/abs/2307.08779v2</link><description>Low-light conditions not only hamper human visual experience but also degradethe model's performance on downstream vision tasks. While existing works makeremarkable progress on day-night domain adaptation, they rely heavily on domainknowledge derived from the task-specific nighttime dataset. This paperchallenges a more complicated scenario with border applicability, i.e.,zero-shot day-night domain adaptation, which eliminates reliance on anynighttime data. Unlike prior zero-shot adaptation approaches emphasizing eitherimage-level translation or model-level adaptation, we propose a similaritymin-max paradigm that considers them under a unified framework. On the imagelevel, we darken images towards minimum feature similarity to enlarge thedomain gap. Then on the model level, we maximize the feature similarity betweenthe darkened images and their normal-light counterparts for better modeladaptation. To the best of our knowledge, this work represents the pioneeringeffort in jointly optimizing both aspects, resulting in a significantimprovement of model generalizability. Extensive experiments demonstrate ourmethod's effectiveness and broad applicability on various nighttime visiontasks, including classification, semantic segmentation, visual placerecognition, and video action recognition. Code and pre-trained models areavailable at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.</description><author>Rundong Luo, Wenjing Wang, Wenhan Yang, Jiaying Liu</author><pubDate>Wed, 19 Jul 2023 06:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08779v2</guid></item></channel></rss>