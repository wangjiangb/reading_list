<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo segmentation</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 18 Oct 2023 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Tube-Link: A Flexible Cross Tube Framework for Universal Video Segmentation</title><link>http://arxiv.org/abs/2303.12782v3</link><description>Video segmentation aims to segment and track every pixel in diverse scenariosaccurately. In this paper, we present Tube-Link, a versatile framework thataddresses multiple core tasks of video segmentation with a unifiedarchitecture. Our framework is a near-online approach that takes a shortsubclip as input and outputs the corresponding spatial-temporal tube masks. Toenhance the modeling of cross-tube relationships, we propose an effective wayto perform tube-level linking via attention along the queries. In addition, weintroduce temporal contrastive learning to instance-wise discriminativefeatures for tube-level association. Our approach offers flexibility andefficiency for both short and long video inputs, as the length of each subclipcan be varied according to the needs of datasets or scenarios. Tube-Linkoutperforms existing specialized architectures by a significant margin on fivevideo segmentation datasets. Specifically, it achieves almost 13% relativeimprovements on VIPSeg and 4% improvements on KITTI-STEP over the strongbaseline Video K-Net. When using a ResNet50 backbone on Youtube-VIS-2019 and2021, Tube-Link boosts IDOL by 3% and 4%, respectively.</description><author>Xiangtai Li, Haobo Yuan, Wenwei Zhang, Guangliang Cheng, Jiangmiao Pang, Chen Change Loy</author><pubDate>Mon, 21 Aug 2023 13:46:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12782v3</guid></item><item><title>GraphEcho: Graph-Driven Unsupervised Domain Adaptation for Echocardiogram Video Segmentation</title><link>http://arxiv.org/abs/2309.11145v1</link><description>Echocardiogram video segmentation plays an important role in cardiac diseasediagnosis. This paper studies the unsupervised domain adaption (UDA) forechocardiogram video segmentation, where the goal is to generalize the modeltrained on the source domain to other unlabelled target domains. Existing UDAsegmentation methods are not suitable for this task because they do not modellocal information and the cyclical consistency of heartbeat. In this paper, weintroduce a newly collected CardiacUDA dataset and a novel GraphEcho method forcardiac structure segmentation. Our GraphEcho comprises two innovative modules,the Spatial-wise Cross-domain Graph Matching (SCGM) and the Temporal CycleConsistency (TCC) module, which utilize prior knowledge of echocardiogramvideos, i.e., consistent cardiac structure across patients and centers and theheartbeat cyclical consistency, respectively. These two modules can betteralign global and local features from source and target domains, improving UDAsegmentation results. Experimental results showed that our GraphEchooutperforms existing state-of-the-art UDA segmentation methods. Our collecteddataset and code will be publicly released upon acceptance. This work will laya new and solid cornerstone for cardiac structure segmentation fromechocardiogram videos. Code and dataset are available at:https://github.com/xmed-lab/GraphEcho</description><author>Jiewen Yang, Xinpeng Ding, Ziyang Zheng, Xiaowei Xu, Xiaomeng Li</author><pubDate>Wed, 20 Sep 2023 09:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11145v1</guid></item><item><title>Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation</title><link>http://arxiv.org/abs/2307.07812v1</link><description>Few-shot video segmentation is the task of delineating a specific novel classin a query video using few labelled support images. Typical approaches comparesupport and query features while limiting comparisons to a single feature layerand thereby ignore potentially valuable information. We present a meta-learnedMultiscale Memory Comparator (MMC) for few-shot video segmentation thatcombines information across scales within a transformer decoder. Typicalmultiscale transformer decoders for segmentation tasks learn a compressedrepresentation, their queries, through information exchange across scales.Unlike previous work, we instead preserve the detailed feature maps duringacross scale information exchange via a multiscale memory transformer decodingto reduce confusion between the background and novel class. Integral to theapproach, we investigate multiple forms of information exchange across scalesin different tasks and provide insights with empirical evidence on which to usein each task. The overall comparisons among query and support features benefitfrom both rich semantics and precise localization. We demonstrate our approachprimarily on few-shot video object segmentation and an adapted version on thefully supervised counterpart. In all cases, our approach outperforms thebaseline and yields state-of-the-art performance. Our code is publiclyavailable at https://github.com/MSiam/MMC-MultiscaleMemory.</description><author>Mennatullah Siam, Rezaul Karim, He Zhao, Richard Wildes</author><pubDate>Sat, 15 Jul 2023 15:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07812v1</guid></item><item><title>MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions</title><link>http://arxiv.org/abs/2308.08544v1</link><description>This paper strives for motion expressions guided video segmentation, whichfocuses on segmenting objects in video content based on a sentence describingthe motion of the objects. Existing referring video object datasets typicallyfocus on salient objects and use language expressions that contain excessivestatic attributes that could potentially enable the target object to beidentified in a single frame. These datasets downplay the importance of motionin video content for language-guided video object segmentation. To investigatethe feasibility of using motion expressions to ground and segment objects invideos, we propose a large-scale dataset called MeViS, which contains numerousmotion expressions to indicate target objects in complex environments. Webenchmarked 5 existing referring video object segmentation (RVOS) methods andconducted a comprehensive comparison on the MeViS dataset. The results showthat current RVOS methods cannot effectively address motion expression-guidedvideo segmentation. We further analyze the challenges and propose a baselineapproach for the proposed MeViS dataset. The goal of our benchmark is toprovide a platform that enables the development of effective language-guidedvideo segmentation algorithms that leverage motion expressions as a primary cuefor object segmentation in complex video scenes. The proposed MeViS dataset hasbeen released at https://henghuiding.github.io/MeViS.</description><author>Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Chen Change Loy</author><pubDate>Wed, 16 Aug 2023 18:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08544v1</guid></item><item><title>CoralVOS: Dataset and Benchmark for Coral Video Segmentation</title><link>http://arxiv.org/abs/2310.01946v1</link><description>Coral reefs formulate the most valuable and productive marine ecosystems,providing habitat for many marine species. Coral reef surveying and analysisare currently confined to coral experts who invest substantial effort ingenerating comprehensive and dependable reports (\emph{e.g.}, coral coverage,population, spatial distribution, \textit{etc}), from the collected surveydata. However, performing dense coral analysis based on manual efforts issignificantly time-consuming, the existing coral analysis algorithms compromiseand opt for performing down-sampling and only conducting sparse point-basedcoral analysis within selected frames. However, such down-sampling will\textbf{inevitable} introduce the estimation bias or even lead to wrongresults. To address this issue, we propose to perform \textbf{dense coral videosegmentation}, with no down-sampling involved. Through video objectsegmentation, we could generate more \textit{reliable} and \textit{in-depth}coral analysis than the existing coral reef analysis algorithms. To boost suchdense coral analysis, we propose a large-scale coral video segmentationdataset: \textbf{CoralVOS} as demonstrated in Fig. 1. To the best of ourknowledge, our CoralVOS is the first dataset and benchmark supporting densecoral video segmentation. We perform experiments on our CoralVOS dataset,including 6 recent state-of-the-art video object segmentation (VOS) algorithms.We fine-tuned these VOS algorithms on our CoralVOS dataset and achievedobservable performance improvement. The results show that there is still greatpotential for further promoting the segmentation accuracy. The dataset andtrained models will be released with the acceptance of this work to foster thecoral reef research community.</description><author>Zheng Ziqiang, Xie Yaofeng, Liang Haixin, Yu Zhibin, Sai-Kit Yeung</author><pubDate>Tue, 03 Oct 2023 11:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01946v1</guid></item><item><title>3rd Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.06753v1</link><description>In order to deal with the task of video panoptic segmentation in the wild, wepropose a robust integrated video panoptic segmentation solution. In oursolution, we regard the video panoptic segmentation task as a segmentationtarget querying task, represent both semantic and instance targets as a set ofqueries, and then combine these queries with video features extracted by neuralnetworks to predict segmentation masks. In order to improve the learningaccuracy and convergence speed of the solution, we add additional tasks ofvideo semantic segmentation and video instance segmentation for joint training.In addition, we also add an additional image semantic segmentation model tofurther improve the performance of semantic classes. In addition, we also addsome additional operations to improve the robustness of the model. Extensiveexperiments on the VIPSeg dataset show that the proposed solution achievesstate-of-the-art performance with 50.04\% VPQ on the VIPSeg test set, which is3rd place on the video panoptic segmentation track of the PVUW Challenge 2023.</description><author>Jinming Su, Wangwang Yang, Junfeng Luo, Xiaolin Wei</author><pubDate>Sun, 11 Jun 2023 20:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06753v1</guid></item><item><title>PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation</title><link>http://arxiv.org/abs/2309.12303v1</link><description>Panoramic videos contain richer spatial information and have attractedtremendous amounts of attention due to their exceptional experience in somefields such as autonomous driving and virtual reality. However, existingdatasets for video segmentation only focus on conventional planar images. Toaddress the challenge, in this paper, we present a panoramic video dataset,PanoVOS. The dataset provides 150 videos with high video resolutions anddiverse motions. To quantify the domain gap between 2D planar videos andpanoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS)models on PanoVOS. Through error analysis, we found that all of them fail totackle pixel-level content discontinues of panoramic videos. Thus, we present aPanoramic Space Consistency Transformer (PSCFormer), which can effectivelyutilize the semantic boundary information of the previous frame for pixel-levelmatching with the current frame. Extensive experiments demonstrate thatcompared with the previous SOTA models, our PSCFormer network exhibits a greatadvantage in terms of segmentation results under the panoramic setting. Ourdataset poses new challenges in panoramic VOS and we hope that our PanoVOS canadvance the development of panoramic segmentation/tracking.</description><author>Shilin Yan, Xiaohao Xu, Lingyi Hong, Wenchao Chen, Wenqiang Zhang, Wei Zhang</author><pubDate>Thu, 21 Sep 2023 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12303v1</guid></item><item><title>PanoVOS: Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation</title><link>http://arxiv.org/abs/2309.12303v2</link><description>Panoramic videos contain richer spatial information and have attractedtremendous amounts of attention due to their exceptional experience in somefields such as autonomous driving and virtual reality. However, existingdatasets for video segmentation only focus on conventional planar images. Toaddress the challenge, in this paper, we present a panoramic video dataset,PanoVOS. The dataset provides 150 videos with high video resolutions anddiverse motions. To quantify the domain gap between 2D planar videos andpanoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS)models on PanoVOS. Through error analysis, we found that all of them fail totackle pixel-level content discontinues of panoramic videos. Thus, we present aPanoramic Space Consistency Transformer (PSCFormer), which can effectivelyutilize the semantic boundary information of the previous frame for pixel-levelmatching with the current frame. Extensive experiments demonstrate thatcompared with the previous SOTA models, our PSCFormer network exhibits a greatadvantage in terms of segmentation results under the panoramic setting. Ourdataset poses new challenges in panoramic VOS and we hope that our PanoVOS canadvance the development of panoramic segmentation/tracking.</description><author>Shilin Yan, Xiaohao Xu, Lingyi Hong, Wenchao Chen, Wenqiang Zhang, Wei Zhang</author><pubDate>Fri, 22 Sep 2023 05:39:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12303v2</guid></item><item><title>Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal Feature Affinity Learning for Robust Video Segmentation</title><link>http://arxiv.org/abs/2307.05898v1</link><description>Noisy label problems are inevitably in existence within medical imagesegmentation causing severe performance degradation. Previous segmentationmethods for noisy label problems only utilize a single image while thepotential of leveraging the correlation between images has been overlooked.Especially for video segmentation, adjacent frames contain rich contextualinformation beneficial in cognizing noisy labels. Based on two insights, wepropose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework toresolve noisy-labeled medical video segmentation issues. First, we argue thesequential prior of videos is an effective reference, i.e., pixel-levelfeatures from adjacent frames are close in distance for the same class and farin distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) isdevised to indicate possible noisy labels by evaluating the affinity betweenpixels in two adjacent frames. We also notice that the noise distributionexhibits considerable variations across video, image, and pixel levels. In thisway, we introduce Multi-Scale Supervision (MSS) to supervise the network fromthree different perspectives by re-weighting and refining the samples. Thisdesign enables the network to concentrate on clean samples in a coarse-to-finemanner. Experiments with both synthetic and real-world label noise demonstratethat our method outperforms recent state-of-the-art robust segmentationapproaches. Code is available at https://github.com/BeileiCui/MS-TFAL.</description><author>Beilei Cui, Minqing Zhang, Mengya Xu, An Wang, Wu Yuan, Hongliang Ren</author><pubDate>Wed, 12 Jul 2023 05:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05898v1</guid></item><item><title>Towards Unbalanced Motion: Part-Decoupling Network for Video Portrait Segmentation</title><link>http://arxiv.org/abs/2307.16565v1</link><description>Video portrait segmentation (VPS), aiming at segmenting prominent foregroundportraits from video frames, has received much attention in recent years.However, simplicity of existing VPS datasets leads to a limitation on extensiveresearch of the task. In this work, we propose a new intricate large-scaleMulti-scene Video Portrait Segmentation dataset MVPS consisting of 101 videoclips in 7 scenario categories, in which 10,843 sampled frames are finelyannotated at pixel level. The dataset has diverse scenes and complicatedbackground environments, which is the most complex dataset in VPS to our bestknowledge. Through the observation of a large number of videos with portraitsduring dataset construction, we find that due to the joint structure of humanbody, motion of portraits is part-associated, which leads that different partsare relatively independent in motion. That is, motion of different parts of theportraits is unbalanced. Towards this unbalance, an intuitive and reasonableidea is that different motion states in portraits can be better exploited bydecoupling the portraits into parts. To achieve this, we propose aPart-Decoupling Network (PDNet) for video portrait segmentation. Specifically,an Inter-frame Part-Discriminated Attention (IPDA) module is proposed whichunsupervisely segments portrait into parts and utilizes different attentivenesson discriminative features specified to each different part. In this way,appropriate attention can be imposed to portrait parts with unbalanced motionto extract part-discriminated correlations, so that the portraits can besegmented more accurately. Experimental results demonstrate that our methodachieves leading performance with the comparison to state-of-the-art methods.</description><author>Tianshu Yu, Changqun Xia, Jia Li</author><pubDate>Mon, 31 Jul 2023 11:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16565v1</guid></item><item><title>Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation</title><link>http://arxiv.org/abs/2305.16318v1</link><description>Recently, video object segmentation (VOS) referred by multi-modal signals,e.g., language and audio, has evoked increasing attention in both industry andacademia. It is challenging for exploring the semantic alignment withinmodalities and the visual correspondence across frames. However, existingmethods adopt separate network architectures for different modalities, andneglect the inter-frame temporal interaction with references. In this paper, wepropose MUTR, a Multi-modal Unified Temporal transformer for Referring videoobject segmentation. With a unified framework for the first time, MUTR adopts aDETR-style transformer and is capable of segmenting video objects designated byeither text or audio reference. Specifically, we introduce two strategies tofully explore the temporal relations between videos and multi-modal signals.Firstly, for low-level temporal aggregation before the transformer, we enablethe multi-modal references to capture multi-scale visual cues from consecutivevideo frames. This effectively endows the text or audio signals with temporalknowledge and boosts the semantic alignment between modalities. Secondly, forhigh-level temporal interaction after the transformer, we conduct inter-framefeature communication for different object embeddings, contributing to betterobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS andAVSBench datasets with respective text and audio references, MUTR achieves+4.2% and +4.2% J&amp;F improvements to state-of-the-art methods, demonstrating oursignificance for unified multi-modal VOS. Code is released athttps://github.com/OpenGVLab/MUTR.</description><author>Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Zhongjiang He, Peng Gao</author><pubDate>Thu, 25 May 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16318v1</guid></item><item><title>CATR: Combinatorial-Dependence Audio-Queried Transformer for Audio-Visual Video Segmentation</title><link>http://arxiv.org/abs/2309.09709v1</link><description>Audio-visual video segmentation~(AVVS) aims to generate pixel-level maps ofsound-producing objects within image frames and ensure the maps faithfullyadhere to the given audio, such as identifying and segmenting a singing personin a video. However, existing methods exhibit two limitations: 1) they addressvideo temporal features and audio-visual interactive features separately,disregarding the inherent spatial-temporal dependence of combined audio andvideo, and 2) they inadequately introduce audio constraints and object-levelinformation during the decoding stage, resulting in segmentation outcomes thatfail to comply with audio directives. To tackle these issues, we propose adecoupled audio-video transformer that combines audio and video features fromtheir respective temporal and spatial dimensions, capturing their combineddependence. To optimize memory consumption, we design a block, which, whenstacked, enables capturing audio-visual fine-grained combinatorial-dependencein a memory-efficient manner. Additionally, we introduce audio-constrainedqueries during the decoding phase. These queries contain rich object-levelinformation, ensuring the decoded mask adheres to the sounds. Experimentalresults confirm our approach's effectiveness, with our framework achieving anew SOTA performance on all three datasets using two backbones. The code isavailable at \url{https://github.com/aspirinone/CATR.github.io}</description><author>Kexin Li, Zongxin Yang, Lei Chen, Yi Yang, Jun Xun</author><pubDate>Mon, 18 Sep 2023 13:24:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09709v1</guid></item><item><title>CATR: Combinatorial-Dependence Audio-Queried Transformer for Audio-Visual Video Segmentation</title><link>http://arxiv.org/abs/2309.09709v2</link><description>Audio-visual video segmentation~(AVVS) aims to generate pixel-level maps ofsound-producing objects within image frames and ensure the maps faithfullyadhere to the given audio, such as identifying and segmenting a singing personin a video. However, existing methods exhibit two limitations: 1) they addressvideo temporal features and audio-visual interactive features separately,disregarding the inherent spatial-temporal dependence of combined audio andvideo, and 2) they inadequately introduce audio constraints and object-levelinformation during the decoding stage, resulting in segmentation outcomes thatfail to comply with audio directives. To tackle these issues, we propose adecoupled audio-video transformer that combines audio and video features fromtheir respective temporal and spatial dimensions, capturing their combineddependence. To optimize memory consumption, we design a block, which, whenstacked, enables capturing audio-visual fine-grained combinatorial-dependencein a memory-efficient manner. Additionally, we introduce audio-constrainedqueries during the decoding phase. These queries contain rich object-levelinformation, ensuring the decoded mask adheres to the sounds. Experimentalresults confirm our approach's effectiveness, with our framework achieving anew SOTA performance on all three datasets using two backbones. The code isavailable at \url{https://github.com/aspirinone/CATR.github.io}</description><author>Kexin Li, Zongxin Yang, Lei Chen, Yi Yang, Jun Xiao</author><pubDate>Wed, 20 Sep 2023 18:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09709v2</guid></item><item><title>MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation</title><link>http://arxiv.org/abs/2308.11185v1</link><description>Previous research has studied the task of segmenting cinematic videos intoscenes and into narrative acts. However, these studies have overlooked theessential task of multimodal alignment and fusion for effectively andefficiently processing long-form videos (&gt;60min). In this paper, we introduceMultimodal alignmEnt aGgregation and distillAtion (MEGA) for cinematiclong-video segmentation. MEGA tackles the challenge by leveraging multiplemedia modalities. The method coarsely aligns inputs of variable lengths anddifferent modalities with alignment positional encoding. To maintain temporalsynchronization while reducing computation, we further introduce an enhancedbottleneck fusion layer which uses temporal alignment. Additionally, MEGAemploys a novel contrastive loss to synchronize and transfer labels acrossmodalities, enabling act segmentation from labeled synopsis sentences on videoshots. Our experimental results show that MEGA outperforms state-of-the-artmethods on MovieNet dataset for scene segmentation (with an Average Precisionimprovement of +1.19%) and on TRIPOD dataset for act segmentation (with a TotalAgreement improvement of +5.51%)</description><author>Najmeh Sadoughi, Xinyu Li, Avijit Vajpayee, David Fan, Bing Shuai, Hector Santos-Villalobos, Vimal Bhat, Rohith MV</author><pubDate>Tue, 22 Aug 2023 05:23:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11185v1</guid></item><item><title>XMem++: Production-level Video Segmentation From Few Annotated Frames</title><link>http://arxiv.org/abs/2307.15958v2</link><description>Despite advancements in user-guided video segmentation, extracting complexobjects consistently for highly complex scenes is still a labor-intensive task,especially for production. It is not uncommon that a majority of frames need tobe annotated. We introduce a novel semi-supervised video object segmentation(SSVOS) model, XMem++, that improves existing memory-based models, with apermanent memory module. Most existing methods focus on single frameannotations, while our approach can effectively handle multiple user-selectedframes with varying appearances of the same object or region. Our method canextract highly consistent results while keeping the required number of frameannotations low. We further introduce an iterative and attention-based framesuggestion mechanism, which computes the next best frame for annotation. Ourmethod is real-time and does not require retraining after each user input. Wealso introduce a new dataset, PUMaVOS, which covers new challenging use casesnot found in previous benchmarks. We demonstrate SOTA performance onchallenging (partial and multi-class) segmentation scenarios as well as longvideos, while ensuring significantly fewer frame annotations than any existingmethod. Project page: https://max810.github.io/xmem2-project-page/</description><author>Maksym Bekuzarov, Ariana Bermudez, Joon-Young Lee, Hao Li</author><pubDate>Tue, 15 Aug 2023 12:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15958v2</guid></item><item><title>Towards Open-Vocabulary Video Instance Segmentation</title><link>http://arxiv.org/abs/2304.01715v2</link><description>Video Instance Segmentation (VIS) aims at segmenting and categorizing objectsin videos from a closed set of training categories, lacking the generalizationability to handle novel categories in real-world videos. To address thislimitation, we make the following three contributions. First, we introduce thenovel task of Open-Vocabulary Video Instance Segmentation, which aims tosimultaneously segment, track, and classify objects in videos from open-setcategories, including novel categories unseen during training. Second, tobenchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video InstanceSegmentation dataset (LV-VIS), that contains well-annotated objects from 1,196diverse categories, significantly surpassing the category size of existingdatasets by more than one order of magnitude. Third, we propose an efficientMemory-Induced Transformer architecture, OV2Seg, to first achieveOpen-Vocabulary VIS in an end-to-end manner with near real-time inferencespeed. Extensive experiments on LV-VIS and four existing VIS datasetsdemonstrate the strong zero-shot generalization ability of OV2Seg on novelcategories. The dataset and code are released herehttps://github.com/haochenheheda/LVVIS.</description><author>Haochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang, XU Tang, Yao Hu, Weidi Xie, Efstratios Gavves</author><pubDate>Sun, 06 Aug 2023 21:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01715v2</guid></item><item><title>GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation</title><link>http://arxiv.org/abs/2309.11144v1</link><description>Cardiac structure segmentation from echocardiogram videos plays a crucialrole in diagnosing heart disease. The combination of multi-view echocardiogramdata is essential to enhance the accuracy and robustness of automated methods.However, due to the visual disparity of the data, deriving cross-view contextinformation remains a challenging task, and unsophisticated fusion strategiescan even lower performance. In this study, we propose a novel Gobal-Localfusion (GL-Fusion) network to jointly utilize multi-view information globallyand locally that improve the accuracy of echocardiogram analysis. Specifically,a Multi-view Global-based Fusion Module (MGFM) is proposed to extract globalcontext information and to explore the cyclic relationship of differentheartbeat cycles in an echocardiogram video. Additionally, a Multi-viewLocal-based Fusion Module (MLFM) is designed to extract correlations of cardiacstructures from different views. Furthermore, we collect a multi-viewechocardiogram video dataset (MvEVD) to evaluate our method. Our methodachieves an 82.29% average dice score, which demonstrates a 7.83% improvementover the baseline method, and outperforms other existing state-of-the-artmethods. To our knowledge, this is the first exploration of a multi-view methodfor echocardiogram video segmentation. Code available at:https://github.com/xmed-lab/GL-Fusion</description><author>Ziyang Zheng, Jiewen Yang, Xinpeng Ding, Xiaowei Xu, Xiaomeng Li</author><pubDate>Wed, 20 Sep 2023 09:43:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11144v1</guid></item><item><title>A spatio-temporal network for video semantic segmentation in surgical videos</title><link>http://arxiv.org/abs/2306.11052v1</link><description>Semantic segmentation in surgical videos has applications in intra-operativeguidance, post-operative analytics and surgical education. Segmentation modelsneed to provide accurate and consistent predictions since temporallyinconsistent identification of anatomical structures can impair usability andhinder patient safety. Video information can alleviate these challenges leadingto reliable models suitable for clinical use. We propose a novel architecturefor modelling temporal relationships in videos. The proposed model includes aspatio-temporal decoder to enable video semantic segmentation by improvingtemporal consistency across frames. The encoder processes individual frameswhilst the decoder processes a temporal batch of adjacent frames. The proposeddecoder can be used on top of any segmentation encoder to improve temporalconsistency. Model performance was evaluated on the CholecSeg8k dataset and aprivate dataset of robotic Partial Nephrectomy procedures. Segmentationperformance was improved when the temporal decoder was applied across bothdatasets. The proposed model also displayed improvements in temporalconsistency.</description><author>Maria Grammatikopoulou, Ricardo Sanchez-Matilla, Felix Bragman, David Owen, Lucy Culshaw, Karen Kerr, Danail Stoyanov, Imanol Luengo</author><pubDate>Mon, 19 Jun 2023 17:36:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11052v1</guid></item><item><title>1st Place Solution for the 5th LSVOS Challenge: Video Instance Segmentation</title><link>http://arxiv.org/abs/2308.14392v1</link><description>Video instance segmentation is a challenging task that serves as thecornerstone of numerous downstream applications, including video editing andautonomous driving. In this report, we present further improvements to the SOTAVIS method, DVIS. First, we introduce a denoising training strategy for thetrainable tracker, allowing it to achieve more stable and accurate objecttracking in complex and long videos. Additionally, we explore the role ofvisual foundation models in video instance segmentation. By utilizing a frozenVIT-L model pre-trained by DINO v2, DVIS demonstrates remarkable performanceimprovements. With these enhancements, our method achieves 57.9 AP and 56.0 APin the development and test phases, respectively, and ultimately ranked 1st inthe VIS track of the 5th LSVOS Challenge. The code will be available athttps://github.com/zhang-tao-whu/DVIS.</description><author>Tao Zhang, Xingye Tian, Yikang Zhou, Yu Wu, Shunping Ji, Cilin Yan, Xuebo Wang, Xin Tao, Yuan Zhang, Pengfei Wan</author><pubDate>Mon, 28 Aug 2023 09:15:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14392v1</guid></item><item><title>1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.04091v1</link><description>Video panoptic segmentation is a challenging task that serves as thecornerstone of numerous downstream applications, including video editing andautonomous driving. We believe that the decoupling strategy proposed by DVISenables more effective utilization of temporal information for both "thing" and"stuff" objects. In this report, we successfully validated the effectiveness ofthe decoupling strategy in video panoptic segmentation. Finally, our methodachieved a VPQ score of 51.4 and 53.7 in the development and test phases,respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUWChallenge. The code is available at https://github.com/zhang-tao-whu/DVIS</description><author>Tao Zhang, Xingye Tian, Haoran Wei, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Wed, 07 Jun 2023 02:24:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04091v1</guid></item><item><title>1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.04091v2</link><description>Video panoptic segmentation is a challenging task that serves as thecornerstone of numerous downstream applications, including video editing andautonomous driving. We believe that the decoupling strategy proposed by DVISenables more effective utilization of temporal information for both "thing" and"stuff" objects. In this report, we successfully validated the effectiveness ofthe decoupling strategy in video panoptic segmentation. Finally, our methodachieved a VPQ score of 51.4 and 53.7 in the development and test phases,respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUWChallenge. The code is available at https://github.com/zhang-tao-whu/DVIS</description><author>Tao Zhang, Xingye Tian, Haoran Wei, Yu Wu, Shunping Ji, Xuebo Wang, Xin Tao, Yuan Zhang, Pengfei Wan</author><pubDate>Thu, 08 Jun 2023 09:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04091v2</guid></item><item><title>Tamed Warping Network for High-Resolution Semantic Video Segmentation</title><link>http://arxiv.org/abs/2005.01344v4</link><description>Recent approaches for fast semantic video segmentation have reducedredundancy by warping feature maps across adjacent frames, greatly speeding upthe inference phase. However, the accuracy drops seriously owing to the errorsincurred by warping. In this paper, we propose a novel framework and design asimple and effective correction stage after warping. Specifically, we build anon-key-frame CNN, fusing warped context features with current spatial details.Based on the feature fusion, our Context Feature Rectification~(CFR) modulelearns the model's difference from a per-frame model to correct the warpedfeatures. Furthermore, our Residual-Guided Attention~(RGA) module utilizes theresidual maps in the compressed domain to help CRF focus on error-proneregions. Results on Cityscapes show that the accuracy significantly increasesfrom $67.3\%$ to $71.6\%$, and the speed edges down from $65.5$ FPS to $61.8$FPS at a resolution of $1024\times 2048$. For non-rigid categories, e.g.,``human'' and ``object'', the improvements are even higher than 18 percentagepoints.</description><author>Songyuan Li, Junyi Feng, Xi Li</author><pubDate>Tue, 11 Jul 2023 09:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.01344v4</guid></item><item><title>UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model</title><link>http://arxiv.org/abs/2305.12659v1</link><description>Unsupervised video object segmentation has made significant progress inrecent years, but the manual annotation of video mask datasets is expensive andlimits the diversity of available datasets. The Segment Anything Model (SAM)has introduced a new prompt-driven paradigm for image segmentation, unlocking arange of previously unexplored capabilities. In this paper, we propose a novelparadigm called UVOSAM, which leverages SAM for unsupervised video objectsegmentation without requiring video mask labels. To address SAM's limitationsin instance discovery and identity association, we introduce a video salientobject tracking network that automatically generates trajectories for prominentforeground objects. These trajectories then serve as prompts for SAM to producevideo masks on a frame-by-frame basis. Our experimental results demonstratethat UVOSAM significantly outperforms current mask-supervised methods. Thesefindings suggest that UVOSAM has the potential to improve unsupervised videoobject segmentation and reduce the cost of manual annotation.</description><author>Zhenghao Zhang, Zhichao Wei, Shengfan Zhang, Zuozhuo Dai, Siyu Zhu</author><pubDate>Mon, 22 May 2023 04:03:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12659v1</guid></item><item><title>Temporal Transductive Inference for Few-Shot Video Object Segmentation</title><link>http://arxiv.org/abs/2203.14308v2</link><description>Few-shot video object segmentation (FS-VOS) aims at segmenting video framesusing a few labelled examples of classes not seen during initial training. Inthis paper, we present a simple but effective temporal transductive inference(TTI) approach that leverages temporal consistency in the unlabelled videoframes during few-shot inference. Key to our approach is the use of both globaland local temporal constraints. The objective of the global constraint is tolearn consistent linear classifiers for novel classes across the imagesequence, whereas the local constraint enforces the proportion offoreground/background regions in each frame to be coherent across a localtemporal window. These constraints act as spatiotemporal regularizers duringthe transductive inference to increase temporal coherence and reduceoverfitting on the few-shot support set. Empirically, our model outperformsstate-of-the-art meta-learning approaches in terms of mean intersection overunion on YouTube-VIS by 2.8%. In addition, we introduce improved benchmarksthat are exhaustively labelled (i.e. all object occurrences are labelled,unlike the currently available), and present a more realistic evaluationparadigm that targets data distribution shift between training and testingsets. Our empirical results and in-depth analysis confirm the added benefits ofthe proposed spatiotemporal regularizers to improve temporal coherence andovercome certain overfitting scenarios.</description><author>Mennatullah Siam, Konstantinos G. Derpanis, Richard P. Wildes</author><pubDate>Sun, 16 Jul 2023 14:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.14308v2</guid></item><item><title>Making a Case for 3D Convolutions for Object Segmentation in Videos</title><link>http://arxiv.org/abs/2008.11516v2</link><description>The task of object segmentation in videos is usually accomplished byprocessing appearance and motion information separately using standard 2Dconvolutional networks, followed by a learned fusion of the two sources ofinformation. On the other hand, 3D convolutional networks have beensuccessfully applied for video classification tasks, but have not beenleveraged as effectively to problems involving dense per-pixel interpretationof videos compared to their 2D convolutional counterparts and lag behind theaforementioned networks in terms of performance. In this work, we show that 3DCNNs can be effectively applied to dense video prediction tasks such as salientobject segmentation. We propose a simple yet effective encoder-decoder networkarchitecture consisting entirely of 3D convolutions that can be trainedend-to-end using a standard cross-entropy loss. To this end, we leverage anefficient 3D encoder, and propose a 3D decoder architecture, that comprisesnovel 3D Global Convolution layers and 3D Refinement modules. Our approachoutperforms existing state-of-the-arts by a large margin on the DAVIS'16Unsupervised, FBMS and ViSal dataset benchmarks in addition to being faster,thus showing that our architecture can efficiently learn expressivespatio-temporal features and produce high quality video segmentation masks. Wehave made our code and trained models publicly available athttps://github.com/sabarim/3DC-Seg.</description><author>Sabarinath Mahadevan, Ali Athar, Aljoša Ošep, Sebastian Hennen, Laura Leal-Taixé, Bastian Leibe</author><pubDate>Fri, 01 Sep 2023 15:02:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2008.11516v2</guid></item><item><title>Measuring Student Behavioral Engagement using Histogram of Actions</title><link>http://arxiv.org/abs/2307.09420v1</link><description>In this paper, we propose a novel technique for measuring behavioralengagement through students' actions recognition. The proposed approachrecognizes student actions then predicts the student behavioral engagementlevel. For student action recognition, we use human skeletons to model studentpostures and upper body movements. To learn the dynamics of student upper body,a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actionswithin every 2minute video segment then these actions are used to build ahistogram of actions which encodes the student actions and their frequencies.This histogram is utilized as an input to SVM classifier to classify whetherthe student is engaged or disengaged. To evaluate the proposed framework, webuild a dataset consisting of 1414 2-minute video segments annotated with 13actions and 112 video segments annotated with two engagement levels.Experimental results indicate that student actions can be recognized with top 1accuracy 83.63% and the proposed framework can capture the average engagementof the class.</description><author>Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Tue, 18 Jul 2023 17:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09420v1</guid></item><item><title>Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation</title><link>http://arxiv.org/abs/2309.13248v1</link><description>Video amodal segmentation is a particularly challenging task in computervision, which requires to deduce the full shape of an object from the visibleparts of it. Recently, some studies have achieved promising performance byusing motion flow to integrate information across frames under aself-supervised setting. However, motion flow has a clear limitation by the twofactors of moving cameras and object deformation. This paper presents arethinking to previous works. We particularly leverage the supervised signalswith object-centric representation in \textit{real-world scenarios}. Theunderlying idea is the supervision signal of the specific object and thefeatures from different views can mutually benefit the deduction of the fullmask in any specific frame. We thus propose an Efficient object-centricRepresentation amodal Segmentation (EoRaS). Specially, beyond solely relying onsupervision signals, we design a translation module to project image featuresinto the Bird's-Eye View (BEV), which introduces 3D information to improvecurrent feature quality. Furthermore, we propose a multi-view fusion layerbased temporal module which is equipped with a set of object slots andinteracts with features from different views by attention mechanism to fulfillsufficient object representation completion. As a result, the full mask of theobject can be decoded from image features updated by object slots. Extensiveexperiments on both real-world and synthetic benchmarks demonstrate thesuperiority of our proposed method, achieving state-of-the-art performance. Ourcode will be released at \url{https://github.com/kfan21/EoRaS}.</description><author>Ke Fan, Jingshi Lei, Xuelin Qian, Miaopeng Yu, Tianjun Xiao, Tong He, Zheng Zhang, Yanwei Fu</author><pubDate>Sat, 23 Sep 2023 05:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13248v1</guid></item><item><title>LVOS: A Benchmark for Long-term Video Object Segmentation</title><link>http://arxiv.org/abs/2211.10181v2</link><description>Existing video object segmentation (VOS) benchmarks focus on short-termvideos which just last about 3-5 seconds and where objects are visible most ofthe time. These videos are poorly representative of practical applications, andthe absence of long-term datasets restricts further investigation of VOS on theapplication in realistic scenarios. So, in this paper, we present a newbenchmark dataset named \textbf{LVOS}, which consists of 220 videos with atotal duration of 421 minutes. To the best of our knowledge, LVOS is the firstdensely annotated long-term VOS dataset. The videos in our LVOS last 1.59minutes on average, which is 20 times longer than videos in existing VOSdatasets. Each video includes various attributes, especially challengesderiving from the wild, such as long-term reappearing and cross-temporalsimilar objeccts.Based on LVOS, we assess existing video object segmentationalgorithms and propose a Diverse Dynamic Memory network (DDMemory) thatconsists of three complementary memory banks to exploit temporal informationadequately. The experimental results demonstrate the strength and weaknesses ofprior methods, pointing promising directions for further study. Data and codeare available at https://lingyihongfd.github.io/lvos.github.io/.</description><author>Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, Wenqiang Zhang</author><pubDate>Fri, 18 Aug 2023 13:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10181v2</guid></item><item><title>EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2308.04162v1</link><description>Audio-guided Video Object Segmentation (A-VOS) and Referring Video ObjectSegmentation (R-VOS) are two highly-related tasks, which both aim to segmentspecific objects from video sequences according to user-provided expressionprompts. However, due to the challenges in modeling representations fordifferent modalities, contemporary methods struggle to strike a balance betweeninteraction flexibility and high-precision localization and segmentation. Inthis paper, we address this problem from two perspectives: the alignmentrepresentation of audio and text and the deep interaction among audio, text,and visual features. First, we propose a universal architecture, the ExpressionPrompt Collaboration Transformer, herein EPCFormer. Next, we propose anExpression Alignment (EA) mechanism for audio and text expressions. Byintroducing contrastive learning for audio and text expressions, the proposedEPCFormer realizes comprehension of the semantic equivalence between audio andtext expressions denoting the same objects. Then, to facilitate deepinteractions among audio, text, and video features, we introduce anExpression-Visual Attention (EVA) mechanism. The knowledge of video objectsegmentation in terms of the expression prompts can seamlessly transfer betweenthe two tasks by deeply exploring complementary cues between text and audio.Experiments on well-recognized benchmarks demonstrate that our universalEPCFormer attains state-of-the-art results on both tasks. The source code ofEPCFormer will be made publicly available athttps://github.com/lab206/EPCFormer.</description><author>Jiajun Chen, Jiacheng Lin, Zhiqiang Xiao, Haolong Fu, Ke Nai, Kailun Yang, Zhiyong Li</author><pubDate>Tue, 08 Aug 2023 10:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04162v1</guid></item><item><title>Scalable Video Object Segmentation with Identification Mechanism</title><link>http://arxiv.org/abs/2203.11442v6</link><description>This paper delves into the challenges of achieving scalable and effectivemulti-object modeling for semi-supervised Video Object Segmentation (VOS).Previous VOS methods decode features with a single positive object, limitingthe learning of multi-object representation as they must match and segment eachtarget separately under multi-object scenarios. Additionally, earliertechniques catered to specific application objectives and lacked theflexibility to fulfill different speed-accuracy requirements. To address theseproblems, we present two innovative approaches, Associating Objects withTransformers (AOT) and Associating Objects with Scalable Transformers (AOST).In pursuing effective multi-object modeling, AOT introduces the IDentification(ID) mechanism to allocate each object a unique identity. This approach enablesthe network to model the associations among all objects simultaneously, thusfacilitating the tracking and segmentation of objects in a single network pass.To address the challenge of inflexible deployment, AOST further integratesscalable long short-term transformers that incorporate layer-wise ID-basedattention and scalable supervision. This overcomes ID embeddings'representation limitations and enables online architecture scalability in VOSfor the first time. Given the absence of a benchmark for VOS involving denselymulti-object annotations, we propose a challenging Video Object Segmentation inthe Wild (VOSW) benchmark to validate our approaches. We evaluated various AOTand AOST variants using extensive experiments across VOSW and fivecommonly-used VOS benchmarks. Our approaches surpass the state-of-the-artcompetitors and display exceptional efficiency and scalability consistentlyacross all six benchmarks. Moreover, we notably achieved the 1st position inthe 3rd Large-scale Video Object Segmentation Challenge.</description><author>Zongxin Yang, Xiaohan Wang, Jiaxu Miao, Yunchao Wei, Wenguan Wang, Yi Yang</author><pubDate>Mon, 03 Jul 2023 05:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11442v6</guid></item><item><title>STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos</title><link>http://arxiv.org/abs/2003.08429v4</link><description>Existing methods for instance segmentation in videos typically involvemulti-stage pipelines that follow the tracking-by-detection paradigm and modela video clip as a sequence of images. Multiple networks are used to detectobjects in individual frames, and then associate these detections over time.Hence, these methods are often non-end-to-end trainable and highly tailored tospecific tasks. In this paper, we propose a different approach that iswell-suited to a variety of tasks involving instance segmentation in videos. Inparticular, we model a video clip as a single 3D spatio-temporal volume, andpropose a novel approach that segments and tracks instances across space andtime in a single stage. Our problem formulation is centered around the idea ofspatio-temporal embeddings which are trained to cluster pixels belonging to aspecific object instance over an entire video clip. To this end, we introduce(i) novel mixing functions that enhance the feature representation ofspatio-temporal embeddings, and (ii) a single-stage, proposal-free network thatcan reason about temporal context. Our network is trained end-to-end to learnspatio-temporal embeddings as well as parameters required to cluster theseembeddings, thus simplifying inference. Our method achieves state-of-the-artresults across multiple datasets and tasks. Code and models are available athttps://github.com/sabarim/STEm-Seg.</description><author>Ali Athar, Sabarinath Mahadevan, Aljoša Ošep, Laura Leal-Taixé, Bastian Leibe</author><pubDate>Fri, 01 Sep 2023 14:25:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2003.08429v4</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v3</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Fri, 14 Jul 2023 09:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v3</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v2</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Thu, 08 Jun 2023 09:15:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v2</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v1</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Tue, 06 Jun 2023 06:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v1</guid></item><item><title>Treating Motion as Option with Output Selection for Unsupervised Video Object Segmentation</title><link>http://arxiv.org/abs/2309.14786v1</link><description>Unsupervised video object segmentation (VOS) is a task that aims to detectthe most salient object in a video without external guidance about the object.To leverage the property that salient objects usually have distinctivemovements compared to the background, recent methods collaboratively use motioncues extracted from optical flow maps with appearance cues extracted from RGBimages. However, as optical flow maps are usually very relevant to segmentationmasks, the network is easy to be learned overly dependent on the motion cuesduring network training. As a result, such two-stream approaches are vulnerableto confusing motion cues, making their prediction unstable. To relieve thisissue, we design a novel motion-as-option network by treating motion cues asoptional. During network training, RGB images are randomly provided to themotion encoder instead of optical flow maps, to implicitly reduce motiondependency of the network. As the learned motion encoder can deal with both RGBimages and optical flow maps, two different predictions can be generateddepending on which source information is used as motion input. In order tofully exploit this property, we also propose an adaptive output selectionalgorithm to adopt optimal prediction result at test time. Our proposedapproach affords state-of-the-art performance on all public benchmark datasets,even maintaining real-time inference speed.</description><author>Suhwan Cho, Minhyeok Lee, Jungho Lee, MyeongAh Cho, Sangyoun Lee</author><pubDate>Tue, 26 Sep 2023 10:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14786v1</guid></item><item><title>Efficient Long-Short Temporal Attention Network for Unsupervised Video Object Segmentation</title><link>http://arxiv.org/abs/2309.11707v1</link><description>Unsupervised Video Object Segmentation (VOS) aims at identifying the contoursof primary foreground objects in videos without any prior knowledge. However,previous methods do not fully use spatial-temporal context and fail to tacklethis challenging task in real-time. This motivates us to develop an efficientLong-Short Temporal Attention network (termed LSTA) for unsupervised VOS taskfrom a holistic view. Specifically, LSTA consists of two dominant modules,i.e., Long Temporal Memory and Short Temporal Attention. The former capturesthe long-term global pixel relations of the past frames and the current frame,which models constantly present objects by encoding appearance pattern.Meanwhile, the latter reveals the short-term local pixel relations of onenearby frame and the current frame, which models moving objects by encodingmotion pattern. To speedup the inference, the efficient projection and thelocality-based sliding window are adopted to achieve nearly linear timecomplexity for the two light modules, respectively. Extensive empirical studieson several benchmarks have demonstrated promising performances of the proposedmethod with high efficiency.</description><author>Ping Li, Yu Zhang, Li Yuan, Huaxin Xiao, Binbin Lin, Xianghua Xu</author><pubDate>Thu, 21 Sep 2023 02:09:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11707v1</guid></item><item><title>LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2306.08736v1</link><description>Referring video object segmentation (RVOS) aims to segment the targetinstance referred by a given text expression in a video clip. The textexpression normally contains sophisticated descriptions of the instance'sappearance, actions, and relations with others. It is therefore ratherdifficult for an RVOS model to capture all these attributes correspondingly inthe video; in fact, the model often favours more on the action- andrelation-related visual attribute of the instance. This can end up withincomplete or even incorrect mask prediction of the target instance. In thispaper, we tackle this problem by taking a subject-centric short text expressionfrom the original long text expression. The short one retains only theappearance-related information of the target instance so that we can use it tofocus the model's attention on the instance's appearance. We let the model makejoint predictions using both long and short text expressions and introduce along-short predictions intersection loss to align the joint predictions.Besides the improvement on the linguistic part, we also introduce aforward-backward visual consistency loss, which utilizes optical flows to warpvisual features between the annotated frames and their temporal neighbors forconsistency. We build our method on top of two state of the arttransformer-based pipelines for end-to-end training. Extensive experiments onA2D-Sentences and JHMDB-Sentences datasets show impressive improvements of ourmethod.</description><author>Linfeng Yuan, Miaojing Shi, Zijie Yue</author><pubDate>Wed, 14 Jun 2023 21:40:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08736v1</guid></item><item><title>Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples</title><link>http://arxiv.org/abs/2309.02041v1</link><description>Referring video object segmentation (RVOS), as a supervised learning task,relies on sufficient annotated data for a given scene. However, in morerealistic scenarios, only minimal annotations are available for a new scene,which poses significant challenges to existing RVOS methods. With this in mind,we propose a simple yet effective model with a newly designed cross-modalaffinity (CMA) module based on a Transformer architecture. The CMA modulebuilds multimodal affinity with a few samples, thus quickly learning newsemantic information, and enabling the model to adapt to different scenarios.Since the proposed method targets limited samples for new scenes, we generalizethe problem as - few-shot referring video object segmentation (FS-RVOS). Tofoster research in this direction, we build up a new FS-RVOS benchmark based oncurrently available datasets. The benchmark covers a wide range and includesmultiple situations, which can maximally simulate real-world scenarios.Extensive experiments show that our model adapts well to different scenarioswith only a few samples, reaching state-of-the-art performance on thebenchmark. On Mini-Ref-YouTube-VOS, our model achieves an average performanceof 53.1 J and 54.8 F, which are 10% better than the baselines. Furthermore, weshow impressive results of 77.7 J and 74.8 F on Mini-Ref-SAIL-VOS, which aresignificantly better than the baselines. Code is publicly available athttps://github.com/hengliusky/Few_shot_RVOS.</description><author>Guanghui Li, Mingqi Gao, Heng Liu, Xiantong Zhen, Feng Zheng</author><pubDate>Tue, 05 Sep 2023 09:34:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02041v1</guid></item><item><title>Hierarchical Spatiotemporal Transformers for Video Object Segmentation</title><link>http://arxiv.org/abs/2307.08263v1</link><description>This paper presents a novel framework called HST for semi-supervised videoobject segmentation (VOS). HST extracts image and video features using thelatest Swin Transformer and Video Swin Transformer to inherit their inductivebias for the spatiotemporal locality, which is essential for temporallycoherent VOS. To take full advantage of the image and video features, HST castsimage and video features as a query and memory, respectively. By applyingefficient memory read operations at multiple scales, HST produces hierarchicalfeatures for the precise reconstruction of object masks. HST showseffectiveness and robustness in handling challenging scenarios with occludedand fast-moving objects under cluttered backgrounds. In particular, HST-Boutperforms the state-of-the-art competitors on multiple popular benchmarks,i.e., YouTube-VOS (85.0%), DAVIS 2017 (85.9%), and DAVIS 2016 (94.0%).</description><author>Jun-Sang Yoo, Hongjae Lee, Seung-Won Jung</author><pubDate>Mon, 17 Jul 2023 07:12:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08263v1</guid></item><item><title>RefineVIS: Video Instance Segmentation with Temporal Attention Refinement</title><link>http://arxiv.org/abs/2306.04774v1</link><description>We introduce a novel framework called RefineVIS for Video InstanceSegmentation (VIS) that achieves good object association between frames andaccurate segmentation masks by iteratively refining the representations usingsequence context. RefineVIS learns two separate representations on top of anoff-the-shelf frame-level image instance segmentation model: an associationrepresentation responsible for associating objects across frames and asegmentation representation that produces accurate segmentation masks.Contrastive learning is utilized to learn temporally stable associationrepresentations. A Temporal Attention Refinement (TAR) module learnsdiscriminative segmentation representations by exploiting temporalrelationships and a novel temporal contrastive denoising technique. Our methodsupports both online and offline inference. It achieves state-of-the-art videoinstance segmentation accuracy on YouTube-VIS 2019 (64.4 AP), Youtube-VIS 2021(61.4 AP), and OVIS (46.1 AP) datasets. The visualization shows that the TARmodule can generate more accurate instance segmentation masks, particularly forchallenging cases such as highly occluded objects.</description><author>Andre Abrantes, Jiang Wang, Peng Chu, Quanzeng You, Zicheng Liu</author><pubDate>Wed, 07 Jun 2023 21:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04774v1</guid></item><item><title>Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos</title><link>http://arxiv.org/abs/2310.01861v1</link><description>Breast lesion segmentation in ultrasound (US) videos is essential fordiagnosing and treating axillary lymph node metastasis. However, the lack of awell-established and large-scale ultrasound video dataset with high-qualityannotations has posed a persistent challenge for the research community. Toovercome this issue, we meticulously curated a US video breast lesionsegmentation dataset comprising 572 videos and 34,300 annotated frames,covering a wide range of realistic clinical scenarios. Furthermore, we proposea novel frequency and localization feature aggregation network (FLA-Net) thatlearns temporal features from the frequency domain and predicts additionallesion location positions to assist with breast lesion segmentation. We alsodevise a localization-based contrastive loss to reduce the lesion locationdistance between neighboring video frames within the same video and enlarge thelocation distances between frames from different ultrasound videos. Ourexperiments on our annotated dataset and two public video polyp segmentationdatasets demonstrate that our proposed FLA-Net achieves state-of-the-artperformance in breast lesion segmentation in US videos and video polypsegmentation while significantly reducing time and space complexity. Our modeland dataset are available at https://github.com/jhl-Det/FLA-Net.</description><author>Junhao Lin, Qian Dai, Lei Zhu, Huazhu Fu, Qiong Wang, Weibin Li, Wenhao Rao, Xiaoyang Huang, Liansheng Wang</author><pubDate>Tue, 03 Oct 2023 08:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01861v1</guid></item><item><title>BoxVIS: Video Instance Segmentation with Box Annotations</title><link>http://arxiv.org/abs/2303.14618v2</link><description>It is expensive and labour-extensive to label the pixel-wise object masks ina video. As a result, the amount of pixel-wise annotations in existing videoinstance segmentation (VIS) datasets is small, limiting the generalizationcapability of trained VIS models. An alternative but much cheaper solution isto use bounding boxes to label instances in videos. Inspired by the recentsuccess of box-supervised image instance segmentation, we adapt thestate-of-the-art pixel-supervised VIS models to a box-supervised VIS (BoxVIS)baseline, and observe slight performance degradation. We consequently proposeto improve the BoxVIS performance from two aspects. First, we propose abox-center guided spatial-temporal pairwise affinity (STPA) loss to predictinstance masks for better spatial and temporal consistency. Second, we collecta larger scale box-annotated VIS dataset (BVISD) by consolidating the videosfrom current VIS benchmarks and converting images from the COCO dataset toshort pseudo video clips. With the proposed BVISD and the STPA loss, ourtrained BoxVIS model achieves 43.2\% and 29.0\% mask AP on the YouTube-VIS 2021and OVIS valid sets, respectively. It exhibits comparable instance maskprediction performance and better generalization ability than state-of-the-artpixel-supervised VIS models by using only 16\% of their annotation time andcost. Codes and data can be found at \url{https://github.com/MinghanLi/BoxVIS}.</description><author>Minghan Li, Lei Zhang</author><pubDate>Wed, 12 Jul 2023 11:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14618v2</guid></item><item><title>Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2309.11933v1</link><description>Referring Video Object Segmentation (RVOS) requires segmenting the object invideo referred by a natural language query. Existing methods mainly rely onsophisticated pipelines to tackle such cross-modal task, and do not explicitlymodel the object-level spatial context which plays an important role inlocating the referred object. Therefore, we propose an end-to-end RVOSframework completely built upon transformers, termed \textit{FullyTransformer-Equipped Architecture} (FTEA), which treats the RVOS task as a masksequence learning problem and regards all the objects in video as candidateobjects. Given a video clip with a text query, the visual-textual features areyielded by encoder, while the corresponding pixel-level and word-level featuresare aligned in terms of semantic similarity. To capture the object-levelspatial context, we have developed the Stacked Transformer, which individuallycharacterizes the visual appearance of each candidate object, whose feature mapis decoded to the binary mask sequence in order directly. Finally, the modelfinds the best matching between mask sequence and text query. In addition, todiversify the generated masks for candidate objects, we impose a diversity losson the model for capturing more accurate mask of the referred object. Empiricalstudies have shown the superiority of the proposed method on three benchmarks,e.g., FETA achieves 45.1% and 38.7% in terms of mAP on A2D Sentences (3782videos) and J-HMDB Sentences (928 videos), respectively; it achieves 56.6% interms of $\mathcal{J\&amp;F}$ on Ref-YouTube-VOS (3975 videos and 7451 objects).Particularly, compared to the best candidate method, it has a gain of 2.1% and3.2% in terms of P$@$0.5 on the former two, respectively, while it has a gainof 2.9% in terms of $\mathcal{J}$ on the latter one.</description><author>Ping Li, Yu Zhang, Li Yuan, Xianghua Xu</author><pubDate>Thu, 21 Sep 2023 10:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11933v1</guid></item><item><title>OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2307.09356v1</link><description>Referring video object segmentation (RVOS) aims at segmenting an object in avideo following human instruction. Current state-of-the-art methods fall intoan offline pattern, in which each clip independently interacts with textembedding for cross-modal understanding. They usually present that the offlinepattern is necessary for RVOS, yet model limited temporal association withineach clip. In this work, we break up the previous offline belief and propose asimple yet effective online model using explicit query propagation, namedOnlineRefer. Specifically, our approach leverages target cues that gathersemantic information and position prior to improve the accuracy and ease ofreferring predictions for the current frame. Furthermore, we generalize ouronline model into a semi-online framework to be compatible with video-basedbackbones. To show the effectiveness of our method, we evaluate it on fourbenchmarks, \ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, andJHMDB-Sentences. Without bells and whistles, our OnlineRefer with a Swin-Lbackbone achieves 63.5 J&amp;F and 64.8 J&amp;F on Refer-Youtube-VOS and Refer-DAVIS17,outperforming all other offline methods.</description><author>Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, Jianbing Shen</author><pubDate>Tue, 18 Jul 2023 16:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09356v1</guid></item><item><title>RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2307.00997v2</link><description>The Segment Anything Model (SAM) has gained significant attention for itsimpressive performance in image segmentation. However, it lacks proficiency inreferring video object segmentation (RVOS) due to the need for preciseuser-interactive prompts and a limited understanding of different modalities,such as language and vision. This paper presents the RefSAM model, whichexplores the potential of SAM for RVOS by incorporating multi-view informationfrom diverse modalities and successive frames at different timestamps in anonline manner. Our proposed approach adapts the original SAM model to enhancecross-modality learning by employing a lightweight Cross-Modal MLP thatprojects the text embedding of the referring expression into sparse and denseembeddings, serving as user-interactive prompts. Additionally, we haveintroduced the hierarchical dense attention module to fuse hierarchical visualsemantic information with sparse embeddings in order to obtain fine-graineddense embeddings, and an implicit tracking module to generate a track token andprovide historical information for the mask decoder. Furthermore, we employ aparameter-efficient tuning strategy to effectively align and fuse the languageand vision features. Through comprehensive ablation studies, we demonstrate thepractical and effective design choices of our model. Extensive experimentsconducted on Ref-Youtu-VOS, Ref-DAVIS17, and three referring image segmentationdatasets validate the superiority and effectiveness of our RefSAM model overexisting methods. The code and models will be made publicly at\href{https://github.com/LancasterLi/RefSAM}{github.com/LancasterLi/RefSAM}.</description><author>Yonglin Li, Jing Zhang, Xiao Teng, Long Lan</author><pubDate>Mon, 02 Oct 2023 03:32:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.00997v2</guid></item><item><title>Spectrum-guided Multi-granularity Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2307.13537v1</link><description>Current referring video object segmentation (R-VOS) techniques extractconditional kernels from encoded (low-resolution) vision-language features tosegment the decoded high-resolution features. We discovered that this causessignificant feature drift, which the segmentation kernels struggle to perceiveduring the forward computation. This negatively affects the ability ofsegmentation kernels. To address the drift problem, we propose aSpectrum-guided Multi-granularity (SgMg) approach, which performs directsegmentation on the encoded features and employs visual details to furtheroptimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion(SCF) to perform intra-frame global interactions in the spectral domain foreffective multimodal representation. Finally, we extend SgMg to performmulti-object R-VOS, a new paradigm that enables simultaneous segmentation ofmultiple referred objects in a video. This not only makes R-VOS faster, butalso more practical. Extensive experiments show that SgMg achievesstate-of-the-art performance on four video benchmark datasets, outperformingthe nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMgenables multi-object R-VOS, runs about 3 times faster while maintainingsatisfactory performance. Code is available at https://github.com/bo-miao/SgMg.</description><author>Bo Miao, Mohammed Bennamoun, Yongsheng Gao, Ajmal Mian</author><pubDate>Tue, 25 Jul 2023 15:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13537v1</guid></item><item><title>Track Anything: Segment Anything Meets Videos</title><link>http://arxiv.org/abs/2304.11968v2</link><description>Recently, the Segment Anything Model (SAM) gains lots of attention rapidlydue to its impressive segmentation performance on images. Regarding its strongability on image segmentation and high interactivity with different prompts, wefound that it performs poorly on consistent segmentation in videos. Therefore,in this report, we propose Track Anything Model (TAM), which achieveshigh-performance interactive tracking and segmentation in videos. To bedetailed, given a video sequence, only with very little human participation,i.e., several clicks, people can track anything they are interested in, and getsatisfactory results in one-pass inference. Without additional training, suchan interactive design performs impressively on video object tracking andsegmentation. All resources are available on{https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitaterelated research.</description><author>Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng</author><pubDate>Fri, 28 Apr 2023 04:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11968v2</guid></item><item><title>Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation</title><link>http://arxiv.org/abs/2309.11160v1</link><description>Few-Shot Video Object Segmentation (FSVOS) aims to segment objects in a queryvideo with the same category defined by a few annotated support images.However, this task was seldom explored. In this work, based on IPMT, astate-of-the-art few-shot image segmentation method that combines externalsupport guidance information with adaptive query guidance cues, we propose toleverage multi-grained temporal guidance information for handling the temporalcorrelation nature of video data. We decompose the query video information intoa clip prototype and a memory prototype for capturing local and long-terminternal temporal guidance, respectively. Frame prototypes are further used foreach frame independently to handle fine-grained adaptive guidance and enablebidirectional clip-frame prototype communication. To reduce the influence ofnoisy memory, we propose to leverage the structural similarity relation amongdifferent predicted regions and the support for selecting reliable memoryframes. Furthermore, a new segmentation loss is also proposed to enhance thecategory discriminability of the learned prototypes. Experimental resultsdemonstrate that our proposed video IPMT model significantly outperformsprevious models on two benchmark datasets. Code is available athttps://github.com/nankepan/VIPMT.</description><author>Nian Liu, Kepan Nan, Wangbo Zhao, Yuanwei Liu, Xiwen Yao, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Junwei Han, Fahad Shahbaz Khan</author><pubDate>Wed, 20 Sep 2023 10:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11160v1</guid></item><item><title>Learning Referring Video Object Segmentation from Weak Annotation</title><link>http://arxiv.org/abs/2308.02162v1</link><description>Referring video object segmentation (RVOS) is a task that aims to segment thetarget object in all video frames based on a sentence describing the object.Previous RVOS methods have achieved significant performance withdensely-annotated datasets, whose construction is expensive and time-consuming.To relieve the burden of data annotation while maintaining sufficientsupervision for segmentation, we propose a new annotation scheme, in which welabel the frame where the object first appears with a mask and use boundingboxes for the subsequent frames. Based on this scheme, we propose a method tolearn from this weak annotation. Specifically, we design a cross framesegmentation method, which uses the language-guided dynamic filters tothoroughly leverage the valuable mask annotation and bounding boxes. We furtherdevelop a bi-level contrastive learning method to encourage the model to learndiscriminative representation at the pixel level. Extensive experiments andablative analyses show that our method is able to achieve competitiveperformance without the demand of dense mask annotation. The code will beavailable at https://github.com/wangbo-zhao/WRVOS/.</description><author>Wangbo Zhao, Kepan Nan, Songyang Zhang, Kai Chen, Dahua Lin, Yang You</author><pubDate>Fri, 04 Aug 2023 07:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02162v1</guid></item><item><title>Adversarial Attacks on Video Object Segmentation with Hard Region Discovery</title><link>http://arxiv.org/abs/2309.13857v1</link><description>Video object segmentation has been applied to various computer vision tasks,such as video editing, autonomous driving, and human-robot interaction.However, the methods based on deep neural networks are vulnerable toadversarial examples, which are the inputs attacked by almosthuman-imperceptible perturbations, and the adversary (i.e., attacker) will foolthe segmentation model to make incorrect pixel-level predictions. This willrise the security issues in highly-demanding tasks because small perturbationsto the input video will result in potential attack risks. Though adversarialexamples have been extensively used for classification, it is rarely studied invideo object segmentation. Existing related methods in computer vision eitherrequire prior knowledge of categories or cannot be directly applied due to thespecial design for certain tasks, failing to consider the pixel-wise regionattack. Hence, this work develops an object-agnostic adversary that hasadversarial impacts on VOS by first-frame attacking via hard region discovery.Particularly, the gradients from the segmentation model are exploited todiscover the easily confused region, in which it is difficult to identify thepixel-wise objects from the background in a frame. This provides a hardness mapthat helps to generate perturbations with a stronger adversarial power forattacking the first frame. Empirical studies on three benchmarks indicate thatour attacker significantly degrades the performance of several state-of-the-artvideo object segmentation models.</description><author>Ping Li, Yu Zhang, Li Yuan, Jian Zhao, Xianghua Xu, Xiaoqin Zhang</author><pubDate>Mon, 25 Sep 2023 04:52:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13857v1</guid></item><item><title>Undercover Deepfakes: Detecting Fake Segments in Videos</title><link>http://arxiv.org/abs/2305.06564v2</link><description>The recent renaissance in generative models, driven primarily by the adventof diffusion models and iterative improvement in GAN methods, has enabled manycreative applications. However, each advancement is also accompanied by a risein the potential for misuse. In the arena of deepfake generation this is a keysocietal issue. In particular, the ability to modify segments of videos usingsuch generative techniques creates a new paradigm of deepfakes which are mostlyreal videos altered slightly to distort the truth. Current deepfake detectionmethods in the academic literature are not evaluated on this paradigm. In thispaper, we present a deepfake detection method able to address this issue byperforming both frame and video level deepfake prediction. To facilitatetesting our method we create a new benchmark dataset where videos have bothreal and fake frame sequences. Our method utilizes the Vision Transformer,Scaling and Shifting pretraining and Timeseries Transformer to temporallysegment videos to help facilitate the interpretation of possible deepfakes.Extensive experiments on a variety of deepfake generation methods showexcellent results on temporal segmentation and classical video levelpredictions as well. In particular, the paradigm we introduce will form apowerful tool for the moderation of deepfakes, where human oversight can bebetter targeted to the parts of videos suspected of being deepfakes. Allexperiments can be reproduced at:https://github.com/sanjaysaha1311/temporal-deepfake-segmentation.</description><author>Sanjay Saha, Rashindrie Perera, Sachith Seneviratne, Tamasha Malepathirana, Sanka Rasnayaka, Deshani Geethika, Terence Sim, Saman Halgamuge</author><pubDate>Tue, 16 May 2023 16:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06564v2</guid></item><item><title>Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation</title><link>http://arxiv.org/abs/2308.13505v1</link><description>Current prevailing Video Object Segmentation (VOS) methods usually performdense matching between the current and reference frames after extracting theirfeatures. One on hand, the decoupled modeling restricts the targets informationpropagation only at high-level feature space. On the other hand, the pixel-wisematching leads to a lack of holistic understanding of the targets. To overcomethese issues, we propose a unified VOS framework, coined as JointFormer, forjoint modeling the three elements of feature, correspondence, and a compressedmemory. The core design is the Joint Block, utilizing the flexibility ofattention to simultaneously extract feature and propagate the targetsinformation to the current tokens and the compressed memory token. This schemeallows to perform extensive information propagation and discriminative featurelearning. To incorporate the long-term temporal targets information, we alsodevise a customized online updating mechanism for the compressed memory token,which can prompt the information flow along the temporal dimension and thusimprove the global modeling capability. Under the design, our method achieves anew state-of-art performance on DAVIS 2017 val/test-dev (89.7% and 87.6%) andYouTube-VOS 2018/2019 val (87.0% and 87.0%) benchmarks, outperforming existingworks by a large margin.</description><author>Jiaming Zhang, Yutao Cui, Gangshan Wu, Limin Wang</author><pubDate>Fri, 25 Aug 2023 18:30:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13505v1</guid></item><item><title>Towards Robust Referring Video Object Segmentation with Cyclic Relational Consensus</title><link>http://arxiv.org/abs/2207.01203v2</link><description>Referring Video Object Segmentation (R-VOS) is a challenging task that aimsto segment an object in a video based on a linguistic expression. Most existingR-VOS methods have a critical assumption: the object referred to must appear inthe video. This assumption, which we refer to as semantic consensus, is oftenviolated in real-world scenarios, where the expression may be queried againstfalse videos. In this work, we highlight the need for a robust R-VOS model thatcan handle semantic mismatches. Accordingly, we propose an extended task calledRobust R-VOS, which accepts unpaired video-text inputs. We tackle this problemby jointly modeling the primary R-VOS problem and its dual (textreconstruction). A structural text-to-text cycle constraint is introduced todiscriminate semantic consensus between video-text pairs and impose it inpositive pairs, thereby achieving multi-modal alignment from both positive andnegative pairs. Our structural constraint effectively addresses the challengeposed by linguistic diversity, overcoming the limitations of previous methodsthat relied on the point-wise constraint. A new evaluation dataset,R\textsuperscript{2}-Youtube-VOSis constructed to measure the model robustness.Our model achieves state-of-the-art performance on R-VOS benchmarks,Ref-DAVIS17 and Ref-Youtube-VOS, and also ourR\textsuperscript{2}-Youtube-VOS~dataset.</description><author>Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Bhiksha Raj, Yan Lu</author><pubDate>Mon, 31 Jul 2023 19:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.01203v2</guid></item><item><title>GRAtt-VIS: Gated Residual Attention for Auto Rectifying Video Instance Segmentation</title><link>http://arxiv.org/abs/2305.17096v1</link><description>Recent trends in Video Instance Segmentation (VIS) have seen a growingreliance on online methods to model complex and lengthy video sequences.However, the degradation of representation and noise accumulation of the onlinemethods, especially during occlusion and abrupt changes, pose substantialchallenges. Transformer-based query propagation provides promising directionsat the cost of quadratic memory attention. However, they are susceptible to thedegradation of instance features due to the above-mentioned challenges andsuffer from cascading effects. The detection and rectification of such errorsremain largely underexplored. To this end, we introduce \textbf{GRAtt-VIS},\textbf{G}ated \textbf{R}esidual \textbf{Att}ention for \textbf{V}ideo\textbf{I}nstance \textbf{S}egmentation. Firstly, we leverage aGumbel-Softmax-based gate to detect possible errors in the current frame. Next,based on the gate activation, we rectify degraded features from its pastrepresentation. Such a residual configuration alleviates the need for dedicatedmemory and provides a continuous stream of relevant instance features.Secondly, we propose a novel inter-instance interaction using gate activationas a mask for self-attention. This masking strategy dynamically restricts theunrepresentative instance queries in the self-attention and preserves vitalinformation for long-term tracking. We refer to this novel combination of GatedResidual Connection and Masked Self-Attention as \textbf{GRAtt} block, whichcan easily be integrated into the existing propagation-based framework.Further, GRAtt blocks significantly reduce the attention overhead and simplifydynamic temporal modeling. GRAtt-VIS achieves state-of-the-art performance onYouTube-VIS and the highly challenging OVIS dataset, significantly improvingover previous methods. Code is available at\url{https://github.com/Tanveer81/GRAttVIS}.</description><author>Tanveer Hannan, Rajat Koner, Maximilian Bernhard, Suprosanna Shit, Bjoern Menze, Volker Tresp, Matthias Schubert, Thomas Seidl</author><pubDate>Fri, 26 May 2023 18:10:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17096v1</guid></item><item><title>FODVid: Flow-guided Object Discovery in Videos</title><link>http://arxiv.org/abs/2307.04392v1</link><description>Segmentation of objects in a video is challenging due to the nuances such asmotion blurring, parallax, occlusions, changes in illumination, etc. Instead ofaddressing these nuances separately, we focus on building a generalizablesolution that avoids overfitting to the individual intricacies. Such a solutionwould also help us save enormous resources involved in human annotation ofvideo corpora. To solve Video Object Segmentation (VOS) in an unsupervisedsetting, we propose a new pipeline (FODVid) based on the idea of guidingsegmentation outputs using flow-guided graph-cut and temporal consistency.Basically, we design a segmentation model incorporating intra-frame appearanceand flow similarities, and inter-frame temporal continuation of the objectsunder consideration. We perform an extensive experimental analysis of ourstraightforward methodology on the standard DAVIS16 video benchmark. Thoughsimple, our approach produces results comparable (within a range of ~2 mIoU) tothe existing top approaches in unsupervised VOS. The simplicity andeffectiveness of our technique opens up new avenues for research in the videodomain.</description><author>Silky Singh, Shripad Deshmukh, Mausoom Sarkar, Rishabh Jain, Mayur Hemani, Balaji Krishnamurthy</author><pubDate>Mon, 10 Jul 2023 08:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04392v1</guid></item><item><title>NOVIS: A Case for End-to-End Near-Online Video Instance Segmentation</title><link>http://arxiv.org/abs/2308.15266v2</link><description>Until recently, the Video Instance Segmentation (VIS) community operatedunder the common belief that offline methods are generally superior to a frameby frame online processing. However, the recent success of online methodsquestions this belief, in particular, for challenging and long video sequences.We understand this work as a rebuttal of those recent observations and anappeal to the community to focus on dedicated near-online VIS approaches. Tosupport our argument, we present a detailed analysis on different processingparadigms and the new end-to-end trainable NOVIS (Near-Online Video InstanceSegmentation) method. Our transformer-based model directly predictsspatio-temporal mask volumes for clips of frames and performs instance trackingbetween clips via overlap embeddings. NOVIS represents the first near-onlineVIS approach which avoids any handcrafted tracking heuristics. We outperformall existing VIS methods by large margins and provide new state-of-the-artresults on both YouTube-VIS (2019/2021) and the OVIS benchmarks.</description><author>Tim Meinhardt, Matt Feiszli, Yuchen Fan, Laura Leal-Taixe, Rakesh Ranjan</author><pubDate>Mon, 18 Sep 2023 15:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15266v2</guid></item><item><title>NOVIS: A Case for End-to-End Near-Online Video Instance Segmentation</title><link>http://arxiv.org/abs/2308.15266v1</link><description>Until recently, the Video Instance Segmentation (VIS) community operatedunder the common belief that offline methods are generally superior to a frameby frame online processing. However, the recent success of online methodsquestions this belief, in particular, for challenging and long video sequences.We understand this work as a rebuttal of those recent observations and anappeal to the community to focus on dedicated near-online VIS approaches. Tosupport our argument, we present a detailed analysis on different processingparadigms and the new end-to-end trainable NOVIS (Near-Online Video InstanceSegmentation) method. Our transformer-based model directly predictsspatio-temporal mask volumes for clips of frames and performs instance trackingbetween clips via overlap embeddings. NOVIS represents the first near-onlineVIS approach which avoids any handcrafted tracking heuristics. We outperformall existing VIS methods by large margins and provide new state-of-the-artresults on both YouTube-VIS (2019/2021) and the OVIS benchmarks.</description><author>Tim Meinhardt, Matt Feiszli, Yuchen Fan, Laura Leal-Taixe, Rakesh Ranjan</author><pubDate>Tue, 29 Aug 2023 13:51:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15266v1</guid></item><item><title>SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2305.17011v1</link><description>This paper studies referring video object segmentation (RVOS) by boostingvideo-level visual-linguistic alignment. Recent approaches model the RVOS taskas a sequence prediction problem and perform multi-modal interaction as well assegmentation for each frame separately. However, the lack of a global view ofvideo content leads to difficulties in effectively utilizing inter-framerelationships and understanding textual descriptions of object temporalvariations. To address this issue, we propose Semantic-assisted Object Cluster(SOC), which aggregates video content and textual guidance for unified temporalmodeling and cross-modal alignment. By associating a group of frame-levelobject embeddings with language tokens, SOC facilitates joint space learningacross modalities and time steps. Moreover, we present multi-modal contrastivesupervision to help construct well-aligned joint space at the video level. Weconduct extensive experiments on popular RVOS benchmarks, and our methodoutperforms state-of-the-art competitors on all benchmarks by a remarkablemargin. Besides, the emphasis on temporal coherence enhances the segmentationstability and adaptability of our method in processing text expressions withtemporal variations. Code will be available.</description><author>Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang</author><pubDate>Fri, 26 May 2023 16:13:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17011v1</guid></item><item><title>Knowledge-Guided Short-Context Action Anticipation in Human-Centric Videos</title><link>http://arxiv.org/abs/2309.05943v1</link><description>This work focuses on anticipating long-term human actions, particularly usingshort video segments, which can speed up editing workflows through improvedsuggestions while fostering creativity by suggesting narratives. To this end,we imbue a transformer network with a symbolic knowledge graph for actionanticipation in video segments by boosting certain aspects of the transformer'sattention mechanism at run-time. Demonstrated on two benchmark datasets,Breakfast and 50Salads, our approach outperforms current state-of-the-artmethods for long-term action anticipation using short video context by up to9%.</description><author>Sarthak Bhagat, Simon Stepputtis, Joseph Campbell, Katia Sycara</author><pubDate>Tue, 12 Sep 2023 04:48:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05943v1</guid></item><item><title>Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation</title><link>http://arxiv.org/abs/2308.06693v1</link><description>Recent leading zero-shot video object segmentation (ZVOS) works devote tointegrating appearance and motion information by elaborately designing featurefusion modules and identically applying them in multiple feature stages. Ourpreliminary experiments show that with the strong long-range dependencymodeling capacity of Transformer, simply concatenating the two modalityfeatures and feeding them to vanilla Transformers for feature fusion candistinctly benefit the performance but at a cost of heavy computation. Throughfurther empirical analysis, we find that attention dependencies learned inTransformer in different stages exhibit completely different properties: globalquery-independent dependency in the low-level stages and semantic-specificdependency in the high-level stages. Motivated by the observations, we proposetwo Transformer variants: i) Context-Sharing Transformer (CST) that learns theglobal-shared contextual information within image frames with a lightweightcomputation. ii) Semantic Gathering-Scattering Transformer (SGST) that modelsthe semantic correlation separately for the foreground and background andreduces the computation cost with a soft token merging mechanism. We apply CSTand SGST for low-level and high-level feature fusions, respectively,formulating a level-isomerous Transformer framework for ZVOS task. Comparedwith the baseline that uses vanilla Transformers for multi-stage fusion, ourssignificantly increase the speed by 13 times and achieves new state-of-the-artZVOS performance. Code is available at https://github.com/DLUT-yyc/Isomer.</description><author>Yichen Yuan, Yifan Wang, Lijun Wang, Xiaoqi Zhao, Huchuan Lu, Yu Wang, Weibo Su, Lei Zhang</author><pubDate>Sun, 13 Aug 2023 07:12:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06693v1</guid></item><item><title>READMem: Robust Embedding Association for a Diverse Memory in Unconstrained Video Object Segmentation</title><link>http://arxiv.org/abs/2305.12823v2</link><description>We present READMem (Robust Embedding Association for a Diverse Memory), amodular framework for semi-automatic video object segmentation (sVOS) methodsdesigned to handle unconstrained videos. Contemporary sVOS works typicallyaggregate video frames in an ever-expanding memory, demanding high hardwareresources for long-term applications. To mitigate memory requirements andprevent near object duplicates (caused by information of adjacent frames),previous methods introduce a hyper-parameter that controls the frequency offrames eligible to be stored. This parameter has to be adjusted according toconcrete video properties (such as rapidity of appearance changes and videolength) and does not generalize well. Instead, we integrate the embedding of anew frame into the memory only if it increases the diversity of the memorycontent. Furthermore, we propose a robust association of the embeddings storedin the memory with query embeddings during the update process. Our approachavoids the accumulation of redundant data, allowing us in return, to restrictthe memory size and prevent extreme memory demands in long videos. We extendpopular sVOS baselines with READMem, which previously showed limitedperformance on long videos. Our approach achieves competitive results on theLong-time Video dataset (LV1) while not hindering performance on shortsequences. Our code is publicly available.</description><author>Stéphane Vujasinović, Sebastian Bullinger, Stefan Becker, Norbert Scherer-Negenborn, Michael Arens, Rainer Stiefelhagen</author><pubDate>Mon, 25 Sep 2023 14:36:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12823v2</guid></item><item><title>READMem: Robust Embedding Association for a Diverse Memory in Unconstrained Video Object Segmentation</title><link>http://arxiv.org/abs/2305.12823v1</link><description>We present READMem (Robust Embedding Association for a Diverse Memory), amodular framework for semi-automatic video object segmentation (sVOS) methodsdesigned to handle unconstrained videos. Contemporary sVOS works typicallyaggregate video frames in an ever-expanding memory, demanding high hardwareresources for long-term applications. To mitigate memory requirements andprevent near object duplicates (caused by information of adjacent frames),previous methods introduce a hyper-parameter that controls the frequency offrames eligible to be stored. This parameter has to be adjusted according toconcrete video properties (such as rapidity of appearance changes and videolength) and does not generalize well. Instead, we integrate the embedding of anew frame into the memory only if it increases the diversity of the memorycontent. Furthermore, we propose a robust association of the embeddings storedin the memory with query embeddings during the update process. Our approachavoids the accumulation of redundant data, allowing us in return, to restrictthe memory size and prevent extreme memory demands in long videos. We extendpopular sVOS baselines with READMem, which previously showed limitedperformance on long videos. Our approach achieves competitive results on theLong-time Video dataset (LV1) while not hindering performance on shortsequences. Our code is publicly available.</description><author>Stéphane Vujasinović, Sebastian Bullinger, Stefan Becker, Norbert Scherer-Negenborn, Michael Arens, Rainer Stiefelhagen</author><pubDate>Mon, 22 May 2023 09:31:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12823v1</guid></item><item><title>TCOVIS: Temporally Consistent Online Video Instance Segmentation</title><link>http://arxiv.org/abs/2309.11857v1</link><description>In recent years, significant progress has been made in video instancesegmentation (VIS), with many offline and online methods achievingstate-of-the-art performance. While offline methods have the advantage ofproducing temporally consistent predictions, they are not suitable forreal-time scenarios. Conversely, online methods are more practical, butmaintaining temporal consistency remains a challenging task. In this paper, wepropose a novel online method for video instance segmentation, called TCOVIS,which fully exploits the temporal information in a video clip. The core of ourmethod consists of a global instance assignment strategy and a spatio-temporalenhancement module, which improve the temporal consistency of the features fromtwo aspects. Specifically, we perform global optimal matching between thepredictions and ground truth across the whole video clip, and supervise themodel with the global optimal objective. We also capture the spatial featureand aggregate it with the semantic feature between frames, thus realizing thespatio-temporal enhancement. We evaluate our method on four widely adopted VISbenchmarks, namely YouTube-VIS 2019/2021/2022 and OVIS, and achievestate-of-the-art performance on all benchmarks without bells-and-whistles. Forinstance, on YouTube-VIS 2021, TCOVIS achieves 49.5 AP and 61.3 AP withResNet-50 and Swin-L backbones, respectively. Code is available athttps://github.com/jun-long-li/TCOVIS.</description><author>Junlong Li, Bingyao Yu, Yongming Rao, Jie Zhou, Jiwen Lu</author><pubDate>Thu, 21 Sep 2023 08:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11857v1</guid></item><item><title>Temporal-aware Hierarchical Mask Classification for Video Semantic Segmentation</title><link>http://arxiv.org/abs/2309.08020v1</link><description>Modern approaches have proved the huge potential of addressing semanticsegmentation as a mask classification task which is widely used ininstance-level segmentation. This paradigm trains models by assigning part ofobject queries to ground truths via conventional one-to-one matching. However,we observe that the popular video semantic segmentation (VSS) dataset haslimited categories per video, meaning less than 10% of queries could be matchedto receive meaningful gradient updates during VSS training. This inefficiencylimits the full expressive potential of all queries.Thus, we present a novelsolution THE-Mask for VSS, which introduces temporal-aware hierarchical objectqueries for the first time. Specifically, we propose to use a simple two-roundmatching mechanism to involve more queries matched with minimal cost duringtraining while without any extra cost during inference. To support ourmore-to-one assignment, in terms of the matching results, we further design ahierarchical loss to train queries with their corresponding hierarchy ofprimary or secondary. Moreover, to effectively capture temporal informationacross frames, we propose a temporal aggregation decoder that fits seamlesslyinto the mask-classification paradigm for VSS. Utilizing temporal-sensitivemulti-level queries, our method achieves state-of-the-art performance on thelatest challenging VSS benchmark VSPW without bells and whistles.</description><author>Zhaochong An, Guolei Sun, Zongwei Wu, Hao Tang, Luc Van Gool</author><pubDate>Thu, 14 Sep 2023 21:31:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08020v1</guid></item><item><title>Online Unsupervised Video Object Segmentation via Contrastive Motion Clustering</title><link>http://arxiv.org/abs/2306.12048v1</link><description>Online unsupervised video object segmentation (UVOS) uses the previous framesas its input to automatically separate the primary object(s) from a streamingvideo without using any further manual annotation. A major challenge is thatthe model has no access to the future and must rely solely on the history,i.e., the segmentation mask is predicted from the current frame as soon as itis captured. In this work, a novel contrastive motion clustering algorithm withan optical flow as its input is proposed for the online UVOS by exploiting thecommon fate principle that visual elements tend to be perceived as a group ifthey possess the same motion pattern. We build a simple and effectiveauto-encoder to iteratively summarize non-learnable prototypical bases for themotion pattern, while the bases in turn help learn the representation of theembedding network. Further, a contrastive learning strategy based on a boundaryprior is developed to improve foreground and background feature discriminationin the representation learning stage. The proposed algorithm can be optimizedon arbitrarily-scale data i.e., frame, clip, dataset) and performed in anonline fashion. Experiments on $\textit{DAVIS}_{\textit{16}}$, $\textit{FBMS}$,and $\textit{SegTrackV2}$ datasets show that the accuracy of our methodsurpasses the previous state-of-the-art (SoTA) online UVOS method by a marginof 0.8%, 2.9%, and 1.1%, respectively. Furthermore, by using an online deepsubspace clustering to tackle the motion grouping, our method is able toachieve higher accuracy at $3\times$ faster inference time compared to SoTAonline UVOS method, and making a good trade-off between effectiveness andefficiency.</description><author>Lin Xi, Weihai Chen, Xingming Wu, Zhong Liu, Zhengguo Li</author><pubDate>Wed, 21 Jun 2023 07:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12048v1</guid></item><item><title>Segment Anything Meets Point Tracking</title><link>http://arxiv.org/abs/2307.01197v1</link><description>The Segment Anything Model (SAM) has established itself as a powerfulzero-shot image segmentation model, employing interactive prompts such aspoints to generate masks. This paper presents SAM-PT, a method extending SAM'scapability to tracking and segmenting anything in dynamic videos. SAM-PTleverages robust and sparse point selection and propagation techniques for maskgeneration, demonstrating that a SAM-based segmentation tracker can yieldstrong zero-shot performance across popular video object segmentationbenchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditionalobject-centric mask propagation strategies, we uniquely use point propagationto exploit local structure information that is agnostic to object semantics. Wehighlight the merits of point-based tracking through direct evaluation on thezero-shot open-world Unidentified Video Objects (UVO) benchmark. To furtherenhance our approach, we utilize K-Medoids clustering for point initializationand track both positive and negative points to clearly distinguish the targetobject. We also employ multiple mask decoding passes for mask refinement anddevise a point re-initialization strategy to improve tracking accuracy. Ourcode integrates different point trackers and video segmentation benchmarks andwill be released at https://github.com/SysCV/sam-pt.</description><author>Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu</author><pubDate>Mon, 03 Jul 2023 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01197v1</guid></item><item><title>A Similarity Alignment Model for Video Copy Segment Matching</title><link>http://arxiv.org/abs/2305.15679v1</link><description>With the development of multimedia technology, Video Copy Detection has beena crucial problem for social media platforms. Meta AI hold Video SimilarityChallenge on CVPR 2023 to push the technology forward. In this report, we shareour winner solutions on Matching Track. We propose a Similarity AlignmentModel(SAM) for video copy segment matching. Our SAM exhibits superiorperformance compared to other competitors, with a 0.108 / 0.144 absoluteimprovement over the second-place competitor in Phase 1 / Phase 2. Code isavailable athttps://github.com/FeipengMa6/VSC22-Submission/tree/main/VSC22-Matching-Track-1st.</description><author>Zhenhua Liu, Feipeng Ma, Tianyi Wang, Fengyun Rao</author><pubDate>Thu, 25 May 2023 04:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15679v1</guid></item><item><title>Undercover Deepfakes: Detecting Fake Segments in Videos</title><link>http://arxiv.org/abs/2305.06564v4</link><description>The recent renaissance in generative models, driven primarily by the adventof diffusion models and iterative improvement in GAN methods, has enabled manycreative applications. However, each advancement is also accompanied by a risein the potential for misuse. In the arena of the deepfake generation, this is akey societal issue. In particular, the ability to modify segments of videosusing such generative techniques creates a new paradigm of deepfakes which aremostly real videos altered slightly to distort the truth. This paradigm hasbeen under-explored by the current deepfake detection methods in the academicliterature. In this paper, we present a deepfake detection method that canaddress this issue by performing deepfake prediction at the frame and videolevels. To facilitate testing our method, we prepared a new benchmark datasetwhere videos have both real and fake frame sequences with very subtletransitions. We provide a benchmark on the proposed dataset with our detectionmethod which utilizes the Vision Transformer based on Scaling and Shifting tolearn spatial features, and a Timeseries Transformer to learn temporal featuresof the videos to help facilitate the interpretation of possible deepfakes.Extensive experiments on a variety of deepfake generation methods showexcellent results by the proposed method on temporal segmentation and classicalvideo-level predictions as well. In particular, the paradigm we address willform a powerful tool for the moderation of deepfakes, where human oversight canbe better targeted to the parts of videos suspected of being deepfakes. Allexperiments can be reproduced at:github.com/rgb91/temporal-deepfake-segmentation.</description><author>Sanjay Saha, Rashindrie Perera, Sachith Seneviratne, Tamasha Malepathirana, Sanka Rasnayaka, Deshani Geethika, Terence Sim, Saman Halgamuge</author><pubDate>Fri, 25 Aug 2023 04:12:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06564v4</guid></item><item><title>Video-Instrument Synergistic Network for Referring Video Instrument Segmentation in Robotic Surgery</title><link>http://arxiv.org/abs/2308.09475v1</link><description>Robot-assisted surgery has made significant progress, with instrumentsegmentation being a critical factor in surgical intervention quality. Itserves as the building block to facilitate surgical robot navigation andsurgical education for the next generation of operating intelligence. Althoughexisting methods have achieved accurate instrument segmentation results, theysimultaneously generate segmentation masks for all instruments, without thecapability to specify a target object and allow an interactive experience. Thiswork explores a new task of Referring Surgical Video Instrument Segmentation(RSVIS), which aims to automatically identify and segment the correspondingsurgical instruments based on the given language expression. To achieve this,we devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn bothvideo-level and instrument-level knowledge to boost performance, while previouswork only used video-level information. Meanwhile, we design a Graph-basedRelation-aware Module (GRM) to model the correlation between multi-modalinformation (i.e., textual description and video frame) to facilitate theextraction of instrument-level information. We are also the first to producetwo RSVIS datasets to promote related research. Our method is verified on thesedatasets, and experimental results exhibit that the VIS-Net can significantlyoutperform existing state-of-the-art referring segmentation methods. Our codeand our datasets will be released upon the publication of this work.</description><author>Hongqiu Wang, Lei Zhu, Guang Yang, Yike Guo, Shichen Zhang, Bo Xu, Yueming Jin</author><pubDate>Fri, 18 Aug 2023 12:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09475v1</guid></item><item><title>Video Object Segmentation in Panoptic Wild Scenes</title><link>http://arxiv.org/abs/2305.04470v2</link><description>In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.</description><author>Yuanyou Xu, Zongxin Yang, Yi Yang</author><pubDate>Sun, 25 Jun 2023 07:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04470v2</guid></item><item><title>Video Object Segmentation in Panoptic Wild Scenes</title><link>http://arxiv.org/abs/2305.04470v1</link><description>In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.</description><author>Yuanyou Xu, Zongxin Yang, Yi Yang</author><pubDate>Mon, 08 May 2023 06:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04470v1</guid></item><item><title>CTVIS: Consistent Training for Online Video Instance Segmentation</title><link>http://arxiv.org/abs/2307.12616v1</link><description>The discrimination of instance embeddings plays a vital role in associatinginstances across time for online video instance segmentation (VIS). Instanceembedding learning is directly supervised by the contrastive loss computed uponthe contrastive items (CIs), which are sets of anchor/positive/negativeembeddings. Recent online VIS methods leverage CIs sourced from one referenceframe only, which we argue is insufficient for learning highly discriminativeembeddings. Intuitively, a possible strategy to enhance CIs is replicating theinference phase during training. To this end, we propose a simple yet effectivetraining strategy, called Consistent Training for Online VIS (CTVIS), whichdevotes to aligning the training and inference pipelines in terms of buildingCIs. Specifically, CTVIS constructs CIs by referring inference themomentum-averaged embedding and the memory bank storage mechanisms, and addingnoise to the relevant embeddings. Such an extension allows a reliablecomparison between embeddings of current instances and the stablerepresentations of historical instances, thereby conferring an advantage inmodeling VIS challenges such as occlusion, re-identification, and deformation.Empirically, CTVIS outstrips the SOTA VIS models by up to +5.0 points on threeVIS benchmarks, including YTVIS19 (55.1% AP), YTVIS21 (50.1% AP) and OVIS(35.5% AP). Furthermore, we find that pseudo-videos transformed from images cantrain robust models surpassing fully-supervised ones.</description><author>Kaining Ying, Qing Zhong, Weian Mao, Zhenhua Wang, Hao Chen, Lin Yuanbo Wu, Yifan Liu, Chengxiang Fan, Yunzhi Zhuge, Chunhua Shen</author><pubDate>Mon, 24 Jul 2023 09:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12616v1</guid></item><item><title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation</title><link>http://arxiv.org/abs/2307.02010v2</link><description>The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objectsegmentation. In this study, we introduce MSDeAOT, a variant of the AOT seriesthat incorporates transformers at multiple feature scales. Leveraging thehierarchical Gated Propagation Module (GPM), MSDeAOT efficiently propagatesobject masks from previous frames to the current frame using a feature scalewith a stride of 16. Additionally, we employ GPM in a more refined featurescale with a stride of 8, leading to improved accuracy in detecting andtracking small objects. Through the implementation of test-time augmentationsand model ensemble techniques, we achieve the top-ranking position in theEPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge.</description><author>Jiahao Li, Yuanyou Xu, Zongxin Yang, Yi Yang, Yueting Zhuang</author><pubDate>Mon, 10 Jul 2023 10:20:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02010v2</guid></item><item><title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation</title><link>http://arxiv.org/abs/2307.02010v1</link><description>The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objectsegmentation. In this study, we introduce MSDeAOT, a variant of the AOT seriesthat incorporates transformers at multiple feature scales. Leveraging thehierarchical Gated Propagation Module (GPM), MSDeAOT efficiently propagatesobject masks from previous frames to the current frame using a feature scalewith a stride of 16. Additionally, we employ GPM in a more refined featurescale with a stride of 8, leading to improved accuracy in detecting andtracking small objects. Through the implementation of test-time augmentationsand model ensemble techniques, we achieve the top-ranking position in theEPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge.</description><author>Jiahao Li, Yuanyou Xu, Zongxin Yang, Yi Yang, Yueting Zhuang</author><pubDate>Wed, 05 Jul 2023 04:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02010v1</guid></item><item><title>Self-Supervised Video Representation Learning via Latent Time Navigation</title><link>http://arxiv.org/abs/2305.06437v1</link><description>Self-supervised video representation learning aimed at maximizing similaritybetween different temporal segments of one video, in order to enforce featurepersistence over time. This leads to loss of pertinent information related totemporal relationships, rendering actions such as `enter' and `leave' to beindistinguishable. To mitigate this limitation, we propose Latent TimeNavigation (LTN), a time-parameterized contrastive learning strategy that isstreamlined to capture fine-grained motions. Specifically, we maximize therepresentation similarity between different video segments from one video,while maintaining their representations time-aware along a subspace of thelatent representation code including an orthogonal basis to represent temporalchanges. Our extensive experimental analysis suggests that learning videorepresentations by LTN consistently improves performance of actionclassification in fine-grained and human-oriented tasks (e.g., on ToyotaSmarthome dataset). In addition, we demonstrate that our proposed model, whenpre-trained on Kinetics-400, generalizes well onto the unseen real world videobenchmark datasets UCF101 and HMDB51, achieving state-of-the-art performance inaction recognition.</description><author>Di Yang, Yaohui Wang, Quan Kong, Antitza Dantcheva, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond</author><pubDate>Wed, 10 May 2023 21:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06437v1</guid></item><item><title>Enhancing Transformer Backbone for Egocentric Video Action Segmentation</title><link>http://arxiv.org/abs/2305.11365v1</link><description>Egocentric temporal action segmentation in videos is a crucial task incomputer vision with applications in various fields such as mixed reality,human behavior analysis, and robotics. Although recent research has utilizedadvanced visual-language frameworks, transformers remain the backbone of actionsegmentation models. Therefore, it is necessary to improve transformers toenhance the robustness of action segmentation models. In this work, we proposetwo novel ideas to enhance the state-of-the-art transformer for actionsegmentation. First, we introduce a dual dilated attention mechanism toadaptively capture hierarchical representations in both local-to-global andglobal-to-local contexts. Second, we incorporate cross-connections between theencoder and decoder blocks to prevent the loss of local context by the decoder.Additionally, we utilize state-of-the-art visual-language representationlearning techniques to extract richer and more compact features for ourtransformer. Our proposed approach outperforms other state-of-the-art methodson the Georgia Tech Egocentric Activities (GTEA) and HOI4D Office Toolsdatasets, and we validate our introduced components with ablation studies. Thesource code and supplementary materials are publicly available onhttps://www.sail-nu.com/dxformer.</description><author>Sakib Reza, Balaji Sundareshan, Mohsen Moghaddam, Octavia Camps</author><pubDate>Fri, 19 May 2023 02:00:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11365v1</guid></item><item><title>Propagating Semantic Labels in Video Data</title><link>http://arxiv.org/abs/2310.00783v1</link><description>Semantic Segmentation combines two sub-tasks: the identification ofpixel-level image masks and the application of semantic labels to those masks.Recently, so-called Foundation Models have been introduced; general modelstrained on very large datasets which can be specialized and applied to morespecific tasks. One such model, the Segment Anything Model (SAM), performsimage segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNNare trained on datasets of paired segments and semantic labels. Manual labelingof custom data, however, is time-consuming. This work presents a method forperforming segmentation for objects in video. Once an object has been found ina frame of video, the segment can then be propagated to future frames; thusreducing manual annotation effort. The method works by combining SAM withStructure from Motion (SfM). The video input to the system is firstreconstructed into 3D geometry using SfM. A frame of video is then segmentedusing SAM. Segments identified by SAM are then projected onto the thereconstructed 3D geometry. In subsequent video frames, the labeled 3D geometryis reprojected into the new perspective, allowing SAM to be invoked fewertimes. System performance is evaluated, including the contributions of the SAMand SfM components. Performance is evaluated over three main metrics:computation time, mask IOU with manual labels, and the number of trackinglosses. Results demonstrate that the system has substantial computation timeimprovements over human performance for tracking objects over video frames, butsuffers in performance.</description><author>David Balaban, Justin Medich, Pranay Gosar, Justin Hart</author><pubDate>Sun, 01 Oct 2023 21:32:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00783v1</guid></item><item><title>End-to-End Streaming Video Temporal Action Segmentation with Reinforce Learning</title><link>http://arxiv.org/abs/2309.15683v1</link><description>Temporal Action Segmentation (TAS) from video is a kind of frame recognitiontask for long video with multiple action classes. As an video understandingtask for long videos, current methods typically combine multi-modality actionrecognition models with temporal models to convert feature sequences to labelsequences. This approach can only be applied to offline scenarios, whichseverely limits the TAS application. Therefore, this paper proposes anend-to-end Streaming Video Temporal Action Segmentation with Reinforce Learning(SVTAS-RL). The end-to-end SVTAS which regard TAS as an action segmentclustering task can expand the application scenarios of TAS; and RL is used toalleviate the problem of inconsistent optimization objective and direction.Through extensive experiments, the SVTAS-RL model achieves a competitiveperformance to the state-of-the-art model of TAS on multiple datasets, andshows greater advantages on the ultra-long video dataset EGTEA. This indicatesthat our method can replace all current TAS models end-to-end and SVTAS-RL ismore suitable for long video TAS. Code is availabel athttps://github.com/Thinksky5124/SVTAS.</description><author>Wujun Wen, Jinrong Zhang, Shenglan Liu, Yunheng Li, Qifeng Li, Lin Feng</author><pubDate>Wed, 27 Sep 2023 15:30:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15683v1</guid></item><item><title>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</title><link>http://arxiv.org/abs/2306.08640v2</link><description>Recent research on Large Language Models (LLMs) has led to remarkableadvancements in general NLP AI assistants. Some studies have further exploredthe use of LLMs for planning and invoking models or APIs to address moregeneral multi-modal user queries. Despite this progress, complex visual-basedtasks still remain challenging due to the diverse nature of visual tasks. Thisdiversity is reflected in two aspects: 1) Reasoning paths. For many real-lifeapplications, it is hard to accurately decompose a query simply by examiningthe query itself. Planning based on the specific visual content and the resultsof each step is usually required. 2) Flexible inputs and intermediate results.Input forms could be flexible for in-the-wild cases, and involves not only asingle image or video but a mixture of videos and images, e.g., a user-viewimage with some reference videos. Besides, a complex reasoning process willalso generate diverse multimodal intermediate results, e.g., video narrations,segmented video clips, etc. To address such general cases, we propose amulti-modal AI assistant, AssistGPT, with an interleaved code and languagereasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrateLLMs with various tools. Specifically, the Planner is capable of using naturallanguage to plan which tool in Executor should do next based on the currentreasoning progress. Inspector is an efficient memory manager to assist thePlanner to feed proper visual information into a specific tool. Finally, sincethe entire reasoning process is complex and flexible, a Learner is designed toenable the model to autonomously explore and discover the optimal solution. Weconducted experiments on A-OKVQA and NExT-QA benchmarks, achievingstate-of-the-art results. Moreover, showcases demonstrate the ability of oursystem to handle questions far more complex than those found in the benchmarks.</description><author>Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, Mike Zheng Shou</author><pubDate>Wed, 28 Jun 2023 06:00:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08640v2</guid></item><item><title>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</title><link>http://arxiv.org/abs/2306.08640v1</link><description>Recent research on Large Language Models (LLMs) has led to remarkableadvancements in general NLP AI assistants. Some studies have further exploredthe use of LLMs for planning and invoking models or APIs to address moregeneral multi-modal user queries. Despite this progress, complex visual-basedtasks still remain challenging due to the diverse nature of visual tasks. Thisdiversity is reflected in two aspects: 1) Reasoning paths. For many real-lifeapplications, it is hard to accurately decompose a query simply by examiningthe query itself. Planning based on the specific visual content and the resultsof each step is usually required. 2) Flexible inputs and intermediate results.Input forms could be flexible for in-the-wild cases, and involves not only asingle image or video but a mixture of videos and images, e.g., a user-viewimage with some reference videos. Besides, a complex reasoning process willalso generate diverse multimodal intermediate results, e.g., video narrations,segmented video clips, etc. To address such general cases, we propose amulti-modal AI assistant, AssistGPT, with an interleaved code and languagereasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrateLLMs with various tools. Specifically, the Planner is capable of using naturallanguage to plan which tool in Executor should do next based on the currentreasoning progress. Inspector is an efficient memory manager to assist thePlanner to feed proper visual information into a specific tool. Finally, sincethe entire reasoning process is complex and flexible, a Learner is designed toenable the model to autonomously explore and discover the optimal solution. Weconducted experiments on A-OKVQA and NExT-QA benchmarks, achievingstate-of-the-art results. Moreover, showcases demonstrate the ability of oursystem to handle questions far more complex than those found in the benchmarks.</description><author>Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, Mike Zheng Shou</author><pubDate>Wed, 14 Jun 2023 18:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08640v1</guid></item><item><title>MultiSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos</title><link>http://arxiv.org/abs/2306.04216v1</link><description>Multimodal summarization with multimodal output (MSMO) has emerged as apromising research direction. Nonetheless, numerous limitations exist withinexisting public MSMO datasets, including insufficient upkeep, datainaccessibility, limited size, and the absence of proper categorization, whichpose significant challenges to effective research. To address these challengesand provide a comprehensive dataset for this new direction, we havemeticulously curated the MultiSum dataset. Our new dataset features (1)Human-validated summaries for both video and textual content, providingsuperior human instruction and labels for multimodal learning. (2)Comprehensively and meticulously arranged categorization, spanning 17 principalcategories and 170 subcategories to encapsulate a diverse array of real-worldscenarios. (3) Benchmark tests performed on the proposed dataset to assessvaried tasks and methods, including video temporal segmentation, videosummarization, text summarization, and multimodal summarization. To championaccessibility and collaboration, we release the MultiSum dataset and the datacollection tool as fully open-source resources, fostering transparency andaccelerating future developments. Our project website can be found athttps://multisum-dataset.github.io/.</description><author>Jielin Qiu, Jiacheng Zhu, William Han, Aditesh Kumar, Karthik Mittal, Claire Jin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Bo Li, Ding Zhao, Lijuan Wang</author><pubDate>Wed, 07 Jun 2023 08:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04216v1</guid></item><item><title>Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation</title><link>http://arxiv.org/abs/2302.11325v2</link><description>This paper presents a deep learning framework for medical video segmentation.Convolution neural network (CNN) and transformer-based methods have achievedgreat milestones in medical image segmentation tasks due to their incrediblesemantic feature encoding and global information comprehension abilities.However, most existing approaches ignore a salient aspect of medical video data- the temporal dimension. Our proposed framework explicitly extracts featuresfrom neighbouring frames across the temporal dimension and incorporates themwith a temporal feature blender, which then tokenises the high-levelspatio-temporal feature to form a strong global feature encoded via a SwinTransformer. The final segmentation results are produced via a UNet-likeencoder-decoder architecture. Our model outperforms other approaches by asignificant margin and improves the segmentation benchmarks on the VFSS2022dataset, achieving a dice coefficient of 0.8986 and 0.8186 for the two datasetstested. Our studies also show the efficacy of the temporal feature blendingscheme and cross-dataset transferability of learned capabilities. Code andmodels are fully available at https://github.com/SimonZeng7108/Video-SwinUNet.</description><author>Chengxi Zeng, Xinyu Yang, David Smithard, Majid Mirmehdi, Alberto M Gambaruto, Tilo Burghardt</author><pubDate>Tue, 04 Jul 2023 16:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11325v2</guid></item><item><title>A Generalist Framework for Panoptic Segmentation of Images and Videos</title><link>http://arxiv.org/abs/2210.06366v3</link><description>Panoptic segmentation assigns semantic and instance ID labels to every pixelof an image. As permutations of instance IDs are also valid solutions, the taskrequires learning of high-dimensional one-to-many mapping. As a result,state-of-the-art approaches use customized architectures and task-specific lossfunctions. We formulate panoptic segmentation as a discrete data generationproblem, without relying on inductive bias of the task. A diffusion model isproposed to model panoptic masks, with a simple architecture and generic lossfunction. By simply adding past predictions as a conditioning signal, ourmethod is capable of modeling video (in a streaming setting) and thereby learnsto track object instances automatically. With extensive experiments, wedemonstrate that our simple approach can perform competitively tostate-of-the-art specialist methods in similar settings.</description><author>Ting Chen, Lala Li, Saurabh Saxena, Geoffrey Hinton, David J. Fleet</author><pubDate>Mon, 25 Sep 2023 01:42:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06366v3</guid></item><item><title>Memory-Efficient Continual Learning Object Segmentation for Long Video</title><link>http://arxiv.org/abs/2309.15274v1</link><description>Recent state-of-the-art semi-supervised Video Object Segmentation (VOS)methods have shown significant improvements in target object segmentationaccuracy when information from preceding frames is used in undertakingsegmentation on the current frame. In particular, such memory-based approachescan help a model to more effectively handle appearance changes (representationdrift) or occlusions. Ideally, for maximum performance, online VOS methodswould need all or most of the preceding frames (or their extracted information)to be stored in memory and be used for online learning in consecutive frames.Such a solution is not feasible for long videos, as the required memory sizewould grow without bound. On the other hand, these methods can fail when memoryis limited and a target object experiences repeated representation driftsthroughout a video. We propose two novel techniques to reduce the memory requirement of onlineVOS methods while improving modeling accuracy and generalization on longvideos. Motivated by the success of continual learning techniques in preservingpreviously-learned knowledge, here we propose Gated-Regularizer ContinualLearning (GRCL), which improves the performance of any online VOS subject tolimited memory, and a Reconstruction-based Memory Selection Continual Learning(RMSCL) which empowers online VOS methods to efficiently benefit from storedinformation in memory. Experimental results show that the proposed methods improve the performanceof online VOS models up to 10 %, and boosts their robustness on long-videodatasets while maintaining comparable performance on short-video datasetsDAVIS16 and DAVIS17.</description><author>Amir Nazemi, Mohammad Javad Shafiee, Zahra Gharaee, Paul Fieguth</author><pubDate>Tue, 26 Sep 2023 22:22:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15274v1</guid></item><item><title>Detect Any Shadow: Segment Anything for Video Shadow Detection</title><link>http://arxiv.org/abs/2305.16698v1</link><description>Segment anything model (SAM) has achieved great success in the field ofnatural image segmentation. Nevertheless, SAM tends to classify shadows asbackground, resulting in poor segmentation performance for shadow detectiontask. In this paper, we propose an simple but effective approach for finetuning SAM to detect shadows. Additionally, we also combine it with longshort-term attention mechanism to extend its capabilities to video shadowdetection. Specifically, we first fine tune SAM by utilizing shadow datacombined with sparse prompts and apply the fine-tuned model to detect aspecific frame (e.g., first frame) in the video with a little user assistance.Subsequently, using the detected frame as a reference, we employ a longshort-term network to learn spatial correlations between distant frames andtemporal consistency between contiguous frames, thereby achieving shadowinformation propagation across frames. Extensive experimental resultsdemonstrate that our method outperforms the state-of-the-art techniques, withimprovements of 17.2% and 3.3% in terms of MAE and IoU, respectively,validating the effectiveness of our method.</description><author>Yonghui Wang, Wengang Zhou, Yunyao Mao, Houqiang Li</author><pubDate>Fri, 26 May 2023 08:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16698v1</guid></item><item><title>Transform-Equivariant Consistency Learning for Temporal Sentence Grounding</title><link>http://arxiv.org/abs/2305.04123v1</link><description>This paper addresses the temporal sentence grounding (TSG). Although existingmethods have made decent achievements in this task, they not only severely relyon abundant video-query paired data for training, but also easily fail into thedataset distribution bias. To alleviate these limitations, we introduce a novelEquivariant Consistency Regulation Learning (ECRL) framework to learn morediscriminative query-related frame-wise representations for each video, in aself-supervised manner. Our motivation comes from that the temporal boundary ofthe query-guided activity should be consistently predicted under variousvideo-level transformations. Concretely, we first design a series ofspatio-temporal augmentations on both foreground and background video segmentsto generate a set of synthetic video samples. In particular, we devise aself-refine module to enhance the completeness and smoothness of the augmentedvideo. Then, we present a novel self-supervised consistency loss (SSCL) appliedon the original and augmented videos to capture their invariant query-relatedsemantic by minimizing the KL-divergence between the sequence similarity of twovideos and a prior Gaussian distribution of timestamp distance. At last, ashared grounding head is introduced to predict the transform-equivariantquery-guided segment boundaries for both the original and augmented videos.Extensive experiments on three challenging datasets (ActivityNet, TACoS, andCharades-STA) demonstrate both effectiveness and efficiency of our proposedECRL framework.</description><author>Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Zichuan Xu, Haozhao Wang, Xing Di, Weining Lu, Yu Cheng</author><pubDate>Sat, 06 May 2023 20:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04123v1</guid></item><item><title>SANPO: A Scene Understanding, Accessibility, Navigation, Pathfinding, Obstacle Avoidance Dataset</title><link>http://arxiv.org/abs/2309.12172v1</link><description>We introduce SANPO, a large-scale egocentric video dataset focused on denseprediction in outdoor environments. It contains stereo video sessions collectedacross diverse outdoor environments, as well as rendered synthetic videosessions. (Synthetic data was provided by Parallel Domain.) All sessions have(dense) depth and odometry labels. All synthetic sessions and a subset of realsessions have temporally consistent dense panoptic segmentation labels. To ourknowledge, this is the first human egocentric video dataset with both largescale dense panoptic segmentation and depth annotations. In addition to thedataset we also provide zero-shot baselines and SANPO benchmarks for futureresearch. We hope that the challenging nature of SANPO will help advance thestate-of-the-art in video segmentation, depth estimation, multi-task visualmodeling, and synthetic-to-real domain adaptation, while enabling humannavigation systems. SANPO is available here:https://google-research-datasets.github.io/sanpo_dataset/</description><author>Sagar M. Waghmare, Kimberly Wilber, Dave Hawkey, Xuan Yang, Matthew Wilson, Stephanie Debats, Cattalyya Nuengsigkapian, Astuti Sharma, Lars Pandikow, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko</author><pubDate>Thu, 21 Sep 2023 16:28:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12172v1</guid></item><item><title>BIT: Bi-Level Temporal Modeling for Efficient Supervised Action Segmentation</title><link>http://arxiv.org/abs/2308.14900v2</link><description>We address the task of supervised action segmentation which aims to partitiona video into non-overlapping segments, each representing a different action.Recent works apply transformers to perform temporal modeling at theframe-level, which suffer from high computational cost and cannot well captureaction dependencies over long temporal horizons. To address these issues, wepropose an efficient BI-level Temporal modeling (BIT) framework that learnsexplicit action tokens to represent action segments, in parallel performstemporal modeling on frame and action levels, while maintaining a lowcomputational cost. Our model contains (i) a frame branch that uses convolutionto learn frame-level relationships, (ii) an action branch that uses transformerto learn action-level dependencies with a small set of action tokens and (iii)cross-attentions to allow communication between the two branches. We apply andextend a set-prediction objective to allow each action token to represent oneor multiple action segments, thus can avoid learning a large number of tokensover long videos with many segments. Thanks to the design of our action branch,we can also seamlessly leverage textual transcripts of videos (when available)to help action segmentation by using them to initialize the action tokens. Weevaluate our model on four video datasets (two egocentric and two third-person)for action segmentation with and without transcripts, showing that BITsignificantly improves the state-of-the-art accuracy with much lowercomputational cost (30 times faster) compared to existing transformer-basedmethods.</description><author>Zijia Lu, Ehsan Elhamifar</author><pubDate>Sat, 07 Oct 2023 22:50:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14900v2</guid></item><item><title>BIT: Bi-Level Temporal Modeling for Efficient Supervised Action Segmentation</title><link>http://arxiv.org/abs/2308.14900v1</link><description>We address the task of supervised action segmentation which aims to partitiona video into non-overlapping segments, each representing a different action.Recent works apply transformers to perform temporal modeling at theframe-level, which suffer from high computational cost and cannot well captureaction dependencies over long temporal horizons. To address these issues, wepropose an efficient BI-level Temporal modeling (BIT) framework that learnsexplicit action tokens to represent action segments, in parallel performstemporal modeling on frame and action levels, while maintaining a lowcomputational cost. Our model contains (i) a frame branch that uses convolutionto learn frame-level relationships, (ii) an action branch that uses transformerto learn action-level dependencies with a small set of action tokens and (iii)cross-attentions to allow communication between the two branches. We apply andextend a set-prediction objective to allow each action token to represent oneor multiple action segments, thus can avoid learning a large number of tokensover long videos with many segments. Thanks to the design of our action branch,we can also seamlessly leverage textual transcripts of videos (when available)to help action segmentation by using them to initialize the action tokens. Weevaluate our model on four video datasets (two egocentric and two third-person)for action segmentation with and without transcripts, showing that BITsignificantly improves the state-of-the-art accuracy with much lowercomputational cost (30 times faster) compared to existing transformer-basedmethods.</description><author>Zijia Lu, Ehsan Elhamifar</author><pubDate>Mon, 28 Aug 2023 21:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14900v1</guid></item><item><title>TrickVOS: A Bag of Tricks for Video Object Segmentation</title><link>http://arxiv.org/abs/2306.15377v1</link><description>Space-time memory (STM) network methods have been dominant in semi-supervisedvideo object segmentation (SVOS) due to their remarkable performance. In thiswork, we identify three key aspects where we can improve such methods; i)supervisory signal, ii) pretraining and iii) spatial awareness. We then proposeTrickVOS; a generic, method-agnostic bag of tricks addressing each aspect withi) a structure-aware hybrid loss, ii) a simple decoder pretraining regime andiii) a cheap tracker that imposes spatial constraints in model predictions.Finally, we propose a lightweight network and show that when trained withTrickVOS, it achieves competitive results to state-of-the-art methods on DAVISand YouTube benchmarks, while being one of the first STM-based SVOS methodsthat can run in real-time on a mobile device.</description><author>Evangelos Skartados, Konstantinos Georgiadis, Mehmet Kerim Yucel, Koskinas Ioannis, Armando Domi, Anastasios Drosou, Bruno Manganelli, Albert Sa`a-Garriga</author><pubDate>Tue, 27 Jun 2023 11:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15377v1</guid></item></channel></rss>