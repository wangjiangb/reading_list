<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo segmentation</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 11 Jun 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation</title><link>http://arxiv.org/abs/2305.16318v1</link><description>Recently, video object segmentation (VOS) referred by multi-modal signals,e.g., language and audio, has evoked increasing attention in both industry andacademia. It is challenging for exploring the semantic alignment withinmodalities and the visual correspondence across frames. However, existingmethods adopt separate network architectures for different modalities, andneglect the inter-frame temporal interaction with references. In this paper, wepropose MUTR, a Multi-modal Unified Temporal transformer for Referring videoobject segmentation. With a unified framework for the first time, MUTR adopts aDETR-style transformer and is capable of segmenting video objects designated byeither text or audio reference. Specifically, we introduce two strategies tofully explore the temporal relations between videos and multi-modal signals.Firstly, for low-level temporal aggregation before the transformer, we enablethe multi-modal references to capture multi-scale visual cues from consecutivevideo frames. This effectively endows the text or audio signals with temporalknowledge and boosts the semantic alignment between modalities. Secondly, forhigh-level temporal interaction after the transformer, we conduct inter-framefeature communication for different object embeddings, contributing to betterobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS andAVSBench datasets with respective text and audio references, MUTR achieves+4.2% and +4.2% J&amp;F improvements to state-of-the-art methods, demonstrating oursignificance for unified multi-modal VOS. Code is released athttps://github.com/OpenGVLab/MUTR.</description><author>Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Zhongjiang He, Peng Gao</author><pubDate>Thu, 25 May 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16318v1</guid></item><item><title>1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.04091v1</link><description>Video panoptic segmentation is a challenging task that serves as thecornerstone of numerous downstream applications, including video editing andautonomous driving. We believe that the decoupling strategy proposed by DVISenables more effective utilization of temporal information for both "thing" and"stuff" objects. In this report, we successfully validated the effectiveness ofthe decoupling strategy in video panoptic segmentation. Finally, our methodachieved a VPQ score of 51.4 and 53.7 in the development and test phases,respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUWChallenge. The code is available at https://github.com/zhang-tao-whu/DVIS</description><author>Tao Zhang, Xingye Tian, Haoran Wei, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Wed, 07 Jun 2023 02:24:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04091v1</guid></item><item><title>1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.04091v2</link><description>Video panoptic segmentation is a challenging task that serves as thecornerstone of numerous downstream applications, including video editing andautonomous driving. We believe that the decoupling strategy proposed by DVISenables more effective utilization of temporal information for both "thing" and"stuff" objects. In this report, we successfully validated the effectiveness ofthe decoupling strategy in video panoptic segmentation. Finally, our methodachieved a VPQ score of 51.4 and 53.7 in the development and test phases,respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUWChallenge. The code is available at https://github.com/zhang-tao-whu/DVIS</description><author>Tao Zhang, Xingye Tian, Haoran Wei, Yu Wu, Shunping Ji, Xuebo Wang, Xin Tao, Yuan Zhang, Pengfei Wan</author><pubDate>Thu, 08 Jun 2023 09:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04091v2</guid></item><item><title>UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model</title><link>http://arxiv.org/abs/2305.12659v1</link><description>Unsupervised video object segmentation has made significant progress inrecent years, but the manual annotation of video mask datasets is expensive andlimits the diversity of available datasets. The Segment Anything Model (SAM)has introduced a new prompt-driven paradigm for image segmentation, unlocking arange of previously unexplored capabilities. In this paper, we propose a novelparadigm called UVOSAM, which leverages SAM for unsupervised video objectsegmentation without requiring video mask labels. To address SAM's limitationsin instance discovery and identity association, we introduce a video salientobject tracking network that automatically generates trajectories for prominentforeground objects. These trajectories then serve as prompts for SAM to producevideo masks on a frame-by-frame basis. Our experimental results demonstratethat UVOSAM significantly outperforms current mask-supervised methods. Thesefindings suggest that UVOSAM has the potential to improve unsupervised videoobject segmentation and reduce the cost of manual annotation.</description><author>Zhenghao Zhang, Zhichao Wei, Shengfan Zhang, Zuozhuo Dai, Siyu Zhu</author><pubDate>Mon, 22 May 2023 04:03:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12659v1</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v1</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Tue, 06 Jun 2023 06:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v1</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v2</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Thu, 08 Jun 2023 09:15:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v2</guid></item><item><title>RefineVIS: Video Instance Segmentation with Temporal Attention Refinement</title><link>http://arxiv.org/abs/2306.04774v1</link><description>We introduce a novel framework called RefineVIS for Video InstanceSegmentation (VIS) that achieves good object association between frames andaccurate segmentation masks by iteratively refining the representations usingsequence context. RefineVIS learns two separate representations on top of anoff-the-shelf frame-level image instance segmentation model: an associationrepresentation responsible for associating objects across frames and asegmentation representation that produces accurate segmentation masks.Contrastive learning is utilized to learn temporally stable associationrepresentations. A Temporal Attention Refinement (TAR) module learnsdiscriminative segmentation representations by exploiting temporalrelationships and a novel temporal contrastive denoising technique. Our methodsupports both online and offline inference. It achieves state-of-the-art videoinstance segmentation accuracy on YouTube-VIS 2019 (64.4 AP), Youtube-VIS 2021(61.4 AP), and OVIS (46.1 AP) datasets. The visualization shows that the TARmodule can generate more accurate instance segmentation masks, particularly forchallenging cases such as highly occluded objects.</description><author>Andre Abrantes, Jiang Wang, Peng Chu, Quanzeng You, Zicheng Liu</author><pubDate>Wed, 07 Jun 2023 21:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04774v1</guid></item><item><title>Track Anything: Segment Anything Meets Videos</title><link>http://arxiv.org/abs/2304.11968v2</link><description>Recently, the Segment Anything Model (SAM) gains lots of attention rapidlydue to its impressive segmentation performance on images. Regarding its strongability on image segmentation and high interactivity with different prompts, wefound that it performs poorly on consistent segmentation in videos. Therefore,in this report, we propose Track Anything Model (TAM), which achieveshigh-performance interactive tracking and segmentation in videos. To bedetailed, given a video sequence, only with very little human participation,i.e., several clicks, people can track anything they are interested in, and getsatisfactory results in one-pass inference. Without additional training, suchan interactive design performs impressively on video object tracking andsegmentation. All resources are available on{https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitaterelated research.</description><author>Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng</author><pubDate>Fri, 28 Apr 2023 04:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11968v2</guid></item><item><title>Undercover Deepfakes: Detecting Fake Segments in Videos</title><link>http://arxiv.org/abs/2305.06564v2</link><description>The recent renaissance in generative models, driven primarily by the adventof diffusion models and iterative improvement in GAN methods, has enabled manycreative applications. However, each advancement is also accompanied by a risein the potential for misuse. In the arena of deepfake generation this is a keysocietal issue. In particular, the ability to modify segments of videos usingsuch generative techniques creates a new paradigm of deepfakes which are mostlyreal videos altered slightly to distort the truth. Current deepfake detectionmethods in the academic literature are not evaluated on this paradigm. In thispaper, we present a deepfake detection method able to address this issue byperforming both frame and video level deepfake prediction. To facilitatetesting our method we create a new benchmark dataset where videos have bothreal and fake frame sequences. Our method utilizes the Vision Transformer,Scaling and Shifting pretraining and Timeseries Transformer to temporallysegment videos to help facilitate the interpretation of possible deepfakes.Extensive experiments on a variety of deepfake generation methods showexcellent results on temporal segmentation and classical video levelpredictions as well. In particular, the paradigm we introduce will form apowerful tool for the moderation of deepfakes, where human oversight can bebetter targeted to the parts of videos suspected of being deepfakes. Allexperiments can be reproduced at:https://github.com/sanjaysaha1311/temporal-deepfake-segmentation.</description><author>Sanjay Saha, Rashindrie Perera, Sachith Seneviratne, Tamasha Malepathirana, Sanka Rasnayaka, Deshani Geethika, Terence Sim, Saman Halgamuge</author><pubDate>Tue, 16 May 2023 16:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06564v2</guid></item><item><title>GRAtt-VIS: Gated Residual Attention for Auto Rectifying Video Instance Segmentation</title><link>http://arxiv.org/abs/2305.17096v1</link><description>Recent trends in Video Instance Segmentation (VIS) have seen a growingreliance on online methods to model complex and lengthy video sequences.However, the degradation of representation and noise accumulation of the onlinemethods, especially during occlusion and abrupt changes, pose substantialchallenges. Transformer-based query propagation provides promising directionsat the cost of quadratic memory attention. However, they are susceptible to thedegradation of instance features due to the above-mentioned challenges andsuffer from cascading effects. The detection and rectification of such errorsremain largely underexplored. To this end, we introduce \textbf{GRAtt-VIS},\textbf{G}ated \textbf{R}esidual \textbf{Att}ention for \textbf{V}ideo\textbf{I}nstance \textbf{S}egmentation. Firstly, we leverage aGumbel-Softmax-based gate to detect possible errors in the current frame. Next,based on the gate activation, we rectify degraded features from its pastrepresentation. Such a residual configuration alleviates the need for dedicatedmemory and provides a continuous stream of relevant instance features.Secondly, we propose a novel inter-instance interaction using gate activationas a mask for self-attention. This masking strategy dynamically restricts theunrepresentative instance queries in the self-attention and preserves vitalinformation for long-term tracking. We refer to this novel combination of GatedResidual Connection and Masked Self-Attention as \textbf{GRAtt} block, whichcan easily be integrated into the existing propagation-based framework.Further, GRAtt blocks significantly reduce the attention overhead and simplifydynamic temporal modeling. GRAtt-VIS achieves state-of-the-art performance onYouTube-VIS and the highly challenging OVIS dataset, significantly improvingover previous methods. Code is available at\url{https://github.com/Tanveer81/GRAttVIS}.</description><author>Tanveer Hannan, Rajat Koner, Maximilian Bernhard, Suprosanna Shit, Bjoern Menze, Volker Tresp, Matthias Schubert, Thomas Seidl</author><pubDate>Fri, 26 May 2023 18:10:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17096v1</guid></item><item><title>SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2305.17011v1</link><description>This paper studies referring video object segmentation (RVOS) by boostingvideo-level visual-linguistic alignment. Recent approaches model the RVOS taskas a sequence prediction problem and perform multi-modal interaction as well assegmentation for each frame separately. However, the lack of a global view ofvideo content leads to difficulties in effectively utilizing inter-framerelationships and understanding textual descriptions of object temporalvariations. To address this issue, we propose Semantic-assisted Object Cluster(SOC), which aggregates video content and textual guidance for unified temporalmodeling and cross-modal alignment. By associating a group of frame-levelobject embeddings with language tokens, SOC facilitates joint space learningacross modalities and time steps. Moreover, we present multi-modal contrastivesupervision to help construct well-aligned joint space at the video level. Weconduct extensive experiments on popular RVOS benchmarks, and our methodoutperforms state-of-the-art competitors on all benchmarks by a remarkablemargin. Besides, the emphasis on temporal coherence enhances the segmentationstability and adaptability of our method in processing text expressions withtemporal variations. Code will be available.</description><author>Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang</author><pubDate>Fri, 26 May 2023 16:13:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17011v1</guid></item><item><title>READMem: Robust Embedding Association for a Diverse Memory in Unconstrained Video Object Segmentation</title><link>http://arxiv.org/abs/2305.12823v1</link><description>We present READMem (Robust Embedding Association for a Diverse Memory), amodular framework for semi-automatic video object segmentation (sVOS) methodsdesigned to handle unconstrained videos. Contemporary sVOS works typicallyaggregate video frames in an ever-expanding memory, demanding high hardwareresources for long-term applications. To mitigate memory requirements andprevent near object duplicates (caused by information of adjacent frames),previous methods introduce a hyper-parameter that controls the frequency offrames eligible to be stored. This parameter has to be adjusted according toconcrete video properties (such as rapidity of appearance changes and videolength) and does not generalize well. Instead, we integrate the embedding of anew frame into the memory only if it increases the diversity of the memorycontent. Furthermore, we propose a robust association of the embeddings storedin the memory with query embeddings during the update process. Our approachavoids the accumulation of redundant data, allowing us in return, to restrictthe memory size and prevent extreme memory demands in long videos. We extendpopular sVOS baselines with READMem, which previously showed limitedperformance on long videos. Our approach achieves competitive results on theLong-time Video dataset (LV1) while not hindering performance on shortsequences. Our code is publicly available.</description><author>Stéphane Vujasinović, Sebastian Bullinger, Stefan Becker, Norbert Scherer-Negenborn, Michael Arens, Rainer Stiefelhagen</author><pubDate>Mon, 22 May 2023 09:31:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12823v1</guid></item><item><title>A Similarity Alignment Model for Video Copy Segment Matching</title><link>http://arxiv.org/abs/2305.15679v1</link><description>With the development of multimedia technology, Video Copy Detection has beena crucial problem for social media platforms. Meta AI hold Video SimilarityChallenge on CVPR 2023 to push the technology forward. In this report, we shareour winner solutions on Matching Track. We propose a Similarity AlignmentModel(SAM) for video copy segment matching. Our SAM exhibits superiorperformance compared to other competitors, with a 0.108 / 0.144 absoluteimprovement over the second-place competitor in Phase 1 / Phase 2. Code isavailable athttps://github.com/FeipengMa6/VSC22-Submission/tree/main/VSC22-Matching-Track-1st.</description><author>Zhenhua Liu, Feipeng Ma, Tianyi Wang, Fengyun Rao</author><pubDate>Thu, 25 May 2023 04:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15679v1</guid></item><item><title>Video Object Segmentation in Panoptic Wild Scenes</title><link>http://arxiv.org/abs/2305.04470v1</link><description>In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.</description><author>Yuanyou Xu, Zongxin Yang, Yi Yang</author><pubDate>Mon, 08 May 2023 06:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04470v1</guid></item><item><title>Self-Supervised Video Representation Learning via Latent Time Navigation</title><link>http://arxiv.org/abs/2305.06437v1</link><description>Self-supervised video representation learning aimed at maximizing similaritybetween different temporal segments of one video, in order to enforce featurepersistence over time. This leads to loss of pertinent information related totemporal relationships, rendering actions such as `enter' and `leave' to beindistinguishable. To mitigate this limitation, we propose Latent TimeNavigation (LTN), a time-parameterized contrastive learning strategy that isstreamlined to capture fine-grained motions. Specifically, we maximize therepresentation similarity between different video segments from one video,while maintaining their representations time-aware along a subspace of thelatent representation code including an orthogonal basis to represent temporalchanges. Our extensive experimental analysis suggests that learning videorepresentations by LTN consistently improves performance of actionclassification in fine-grained and human-oriented tasks (e.g., on ToyotaSmarthome dataset). In addition, we demonstrate that our proposed model, whenpre-trained on Kinetics-400, generalizes well onto the unseen real world videobenchmark datasets UCF101 and HMDB51, achieving state-of-the-art performance inaction recognition.</description><author>Di Yang, Yaohui Wang, Quan Kong, Antitza Dantcheva, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond</author><pubDate>Wed, 10 May 2023 21:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06437v1</guid></item><item><title>Enhancing Transformer Backbone for Egocentric Video Action Segmentation</title><link>http://arxiv.org/abs/2305.11365v1</link><description>Egocentric temporal action segmentation in videos is a crucial task incomputer vision with applications in various fields such as mixed reality,human behavior analysis, and robotics. Although recent research has utilizedadvanced visual-language frameworks, transformers remain the backbone of actionsegmentation models. Therefore, it is necessary to improve transformers toenhance the robustness of action segmentation models. In this work, we proposetwo novel ideas to enhance the state-of-the-art transformer for actionsegmentation. First, we introduce a dual dilated attention mechanism toadaptively capture hierarchical representations in both local-to-global andglobal-to-local contexts. Second, we incorporate cross-connections between theencoder and decoder blocks to prevent the loss of local context by the decoder.Additionally, we utilize state-of-the-art visual-language representationlearning techniques to extract richer and more compact features for ourtransformer. Our proposed approach outperforms other state-of-the-art methodson the Georgia Tech Egocentric Activities (GTEA) and HOI4D Office Toolsdatasets, and we validate our introduced components with ablation studies. Thesource code and supplementary materials are publicly available onhttps://www.sail-nu.com/dxformer.</description><author>Sakib Reza, Balaji Sundareshan, Mohsen Moghaddam, Octavia Camps</author><pubDate>Fri, 19 May 2023 02:00:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11365v1</guid></item><item><title>MultiSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos</title><link>http://arxiv.org/abs/2306.04216v1</link><description>Multimodal summarization with multimodal output (MSMO) has emerged as apromising research direction. Nonetheless, numerous limitations exist withinexisting public MSMO datasets, including insufficient upkeep, datainaccessibility, limited size, and the absence of proper categorization, whichpose significant challenges to effective research. To address these challengesand provide a comprehensive dataset for this new direction, we havemeticulously curated the MultiSum dataset. Our new dataset features (1)Human-validated summaries for both video and textual content, providingsuperior human instruction and labels for multimodal learning. (2)Comprehensively and meticulously arranged categorization, spanning 17 principalcategories and 170 subcategories to encapsulate a diverse array of real-worldscenarios. (3) Benchmark tests performed on the proposed dataset to assessvaried tasks and methods, including video temporal segmentation, videosummarization, text summarization, and multimodal summarization. To championaccessibility and collaboration, we release the MultiSum dataset and the datacollection tool as fully open-source resources, fostering transparency andaccelerating future developments. Our project website can be found athttps://multisum-dataset.github.io/.</description><author>Jielin Qiu, Jiacheng Zhu, William Han, Aditesh Kumar, Karthik Mittal, Claire Jin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Bo Li, Ding Zhao, Lijuan Wang</author><pubDate>Wed, 07 Jun 2023 08:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04216v1</guid></item><item><title>Detect Any Shadow: Segment Anything for Video Shadow Detection</title><link>http://arxiv.org/abs/2305.16698v1</link><description>Segment anything model (SAM) has achieved great success in the field ofnatural image segmentation. Nevertheless, SAM tends to classify shadows asbackground, resulting in poor segmentation performance for shadow detectiontask. In this paper, we propose an simple but effective approach for finetuning SAM to detect shadows. Additionally, we also combine it with longshort-term attention mechanism to extend its capabilities to video shadowdetection. Specifically, we first fine tune SAM by utilizing shadow datacombined with sparse prompts and apply the fine-tuned model to detect aspecific frame (e.g., first frame) in the video with a little user assistance.Subsequently, using the detected frame as a reference, we employ a longshort-term network to learn spatial correlations between distant frames andtemporal consistency between contiguous frames, thereby achieving shadowinformation propagation across frames. Extensive experimental resultsdemonstrate that our method outperforms the state-of-the-art techniques, withimprovements of 17.2% and 3.3% in terms of MAE and IoU, respectively,validating the effectiveness of our method.</description><author>Yonghui Wang, Wengang Zhou, Yunyao Mao, Houqiang Li</author><pubDate>Fri, 26 May 2023 08:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16698v1</guid></item><item><title>Transform-Equivariant Consistency Learning for Temporal Sentence Grounding</title><link>http://arxiv.org/abs/2305.04123v1</link><description>This paper addresses the temporal sentence grounding (TSG). Although existingmethods have made decent achievements in this task, they not only severely relyon abundant video-query paired data for training, but also easily fail into thedataset distribution bias. To alleviate these limitations, we introduce a novelEquivariant Consistency Regulation Learning (ECRL) framework to learn morediscriminative query-related frame-wise representations for each video, in aself-supervised manner. Our motivation comes from that the temporal boundary ofthe query-guided activity should be consistently predicted under variousvideo-level transformations. Concretely, we first design a series ofspatio-temporal augmentations on both foreground and background video segmentsto generate a set of synthetic video samples. In particular, we devise aself-refine module to enhance the completeness and smoothness of the augmentedvideo. Then, we present a novel self-supervised consistency loss (SSCL) appliedon the original and augmented videos to capture their invariant query-relatedsemantic by minimizing the KL-divergence between the sequence similarity of twovideos and a prior Gaussian distribution of timestamp distance. At last, ashared grounding head is introduced to predict the transform-equivariantquery-guided segment boundaries for both the original and augmented videos.Extensive experiments on three challenging datasets (ActivityNet, TACoS, andCharades-STA) demonstrate both effectiveness and efficiency of our proposedECRL framework.</description><author>Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Zichuan Xu, Haozhao Wang, Xing Di, Weining Lu, Yu Cheng</author><pubDate>Sat, 06 May 2023 20:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04123v1</guid></item><item><title>Multi-object Video Generation from Single Frame Layouts</title><link>http://arxiv.org/abs/2305.03983v1</link><description>In this paper, we study video synthesis with emphasis on simplifying thegeneration conditions. Most existing video synthesis models or datasets aredesigned to address complex motions of a single object, lacking the ability ofcomprehensively understanding the spatio-temporal relationships among multipleobjects. Besides, current methods are usually conditioned on intricateannotations (e.g. video segmentations) to generate new videos, beingfundamentally less practical. These motivate us to generate multi-object videosconditioning exclusively on object layouts from a single frame. To solve abovechallenges and inspired by recent research on image generation from layouts, wehave proposed a novel video generative framework capable of synthesizing globalscenes with local objects, via implicit neural representations and layoutmotion self-inference. Our framework is a non-trivial adaptation from imagegeneration methods, and is new to this field. In addition, our model has beenevaluated on two widely-used video recognition benchmarks, demonstratingeffectiveness compared to the baseline model.</description><author>Yang Wu, Zhibin Liu, Hefeng Wu, Liang Lin</author><pubDate>Sat, 06 May 2023 10:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03983v1</guid></item><item><title>Multi-object Video Generation from Single Frame Layouts</title><link>http://arxiv.org/abs/2305.03983v2</link><description>In this paper, we study video synthesis with emphasis on simplifying thegeneration conditions. Most existing video synthesis models or datasets aredesigned to address complex motions of a single object, lacking the ability ofcomprehensively understanding the spatio-temporal relationships among multipleobjects. Besides, current methods are usually conditioned on intricateannotations (e.g. video segmentations) to generate new videos, beingfundamentally less practical. These motivate us to generate multi-object videosconditioning exclusively on object layouts from a single frame. To solve abovechallenges and inspired by recent research on image generation from layouts, wehave proposed a novel video generative framework capable of synthesizing globalscenes with local objects, via implicit neural representations and layoutmotion self-inference. Our framework is a non-trivial adaptation from imagegeneration methods, and is new to this field. In addition, our model has beenevaluated on two widely-used video recognition benchmarks, demonstratingeffectiveness compared to the baseline model.</description><author>Yang Wu, Zhibin Liu, Hefeng Wu, Liang Lin</author><pubDate>Tue, 23 May 2023 16:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03983v2</guid></item><item><title>Autonomous Stabilization of Retinal Videos for Streamlining Assessment of Spontaneous Venous Pulsations</title><link>http://arxiv.org/abs/2305.06043v1</link><description>Spontaneous retinal Venous Pulsations (SVP) are rhythmic changes in thecaliber of the central retinal vein and are observed in the optic disc region(ODR) of the retina. Its absence is a critical indicator of various ocular orneurological abnormalities. Recent advances in imaging technology have enabledthe development of portable smartphone-based devices for observing the retinaand assessment of SVPs. However, the quality of smartphone-based retinal videosis often poor due to noise and image jitting, which in return, can severelyobstruct the observation of SVPs. In this work, we developed a fully automatedretinal video stabilization method that enables the examination of SVPscaptured by various mobile devices. Specifically, we first propose an ODRSpatio-Temporal Localization (ODR-STL) module to localize visible ODR andremove noisy and jittering frames. Then, we introduce a Noise-Aware TemplateMatching (NATM) module to stabilize high-quality video segments at a fixedposition in the field of view. After the processing, the SVPs can be easilyobserved in the stabilized videos, significantly facilitating userobservations. Furthermore, our method is cost-effective and has been tested inboth subjective and objective evaluations. Both of the evaluations support itseffectiveness in facilitating the observation of SVPs. This can improve thetimely diagnosis and treatment of associated diseases, making it a valuabletool for eye health professionals.</description><author>Hongwei Sheng, Xin Yu, Feiyu Wang, MD Wahiduzzaman Khan, Hexuan Weng, Sahar Shariflou, S. Mojtaba Golzan</author><pubDate>Wed, 10 May 2023 11:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06043v1</guid></item><item><title>CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition</title><link>http://arxiv.org/abs/2305.16049v1</link><description>Audio-visual person recognition (AVPR) has received extensive attention.However, most datasets used for AVPR research so far are collected inconstrained environments, and thus cannot reflect the true performance of AVPRsystems in real-world scenarios. To meet the request for research on AVPR inunconstrained conditions, this paper presents a multi-genre AVPR datasetcollected `in the wild', named CN-Celeb-AV. This dataset contains more than420k video segments from 1,136 persons from public media. In particular, we putmore emphasis on two real-world complexities: (1) data in multiple genres; (2)segments with partial information. A comprehensive study was conducted tocompare CN-Celeb-AV with two popular public AVPR benchmark datasets, and theresults demonstrated that CN-Celeb-AV is more in line with real-world scenariosand can be regarded as a new benchmark dataset for AVPR research. The datasetalso involves a development set that can be used to boost the performance ofAVPR systems in real-life situations. The dataset is free for researchers andcan be downloaded from http://cnceleb.org/.</description><author>Lantian Li, Xiaolou Li, Haoyu Jiang, Chen Chen, Ruihai Hou, Dong Wang</author><pubDate>Thu, 25 May 2023 14:31:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16049v1</guid></item><item><title>Colonoscopy Coverage Revisited: Identifying Scanning Gaps in Real-Time</title><link>http://arxiv.org/abs/2305.10026v1</link><description>Colonoscopy is the most widely used medical technique for preventingColorectal Cancer, by detecting and removing polyps before they becomemalignant. Recent studies show that around one quarter of the existing polypsare routinely missed. While some of these do appear in the endoscopist's fieldof view, others are missed due to a partial coverage of the colon. The task ofdetecting and marking unseen regions of the colon has been addressed in recentwork, where the common approach is based on dense 3D reconstruction, whichproves to be challenging due to lack of 3D ground truth and periods with poorvisual content. In this paper we propose a novel and complementary method todetect deficient local coverage in real-time for video segments where areliable 3D reconstruction is impossible. Our method aims to identify skipsalong the colon caused by a drifted position of the endoscope during poorvisibility time intervals. The proposed solution consists of two phases. Duringthe first, time segments with good visibility of the colon and gaps betweenthem are identified. During the second phase, a trained model operates on eachgap, answering the question: Do you observe the same scene before and after thegap? If the answer is negative, the endoscopist is alerted and can be directedto the appropriate area in real-time. The second phase model is trained using acontrastive loss based on auto-generated examples. Our method evaluation on adataset of 250 procedures annotated by trained physicians provides sensitivityof 0.75 with specificity of 0.9.</description><author>G. Leifman, I. Kligvasser, R. Goldenberg, M. Elad, E. Rivlin</author><pubDate>Wed, 17 May 2023 09:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10026v1</guid></item><item><title>VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions</title><link>http://arxiv.org/abs/2305.18756v1</link><description>Video-grounded dialogue understanding is a challenging problem that requiresmachine to perceive, parse and reason over situated semantics extracted fromweakly aligned video and dialogues. Most existing benchmarks treat bothmodalities the same as a frame-independent visual understanding task, whileneglecting the intrinsic attributes in multimodal dialogues, such as scene andtopic transitions. In this paper, we present Video-grounded Scene&amp;Topic AwaRedialogue (VSTAR) dataset, a large scale video-grounded dialogue understandingdataset based on 395 TV series. Based on VSTAR, we propose two benchmarks forvideo-grounded dialogue understanding: scene segmentation and topicsegmentation, and one benchmark for video-grounded dialogue generation.Comprehensive experiments are performed on these benchmarks to demonstrate theimportance of multimodal information and segments in video-grounded dialogueunderstanding and generation.</description><author>Yuxuan Wang, Zilong Zheng, Xueliang Zhao, Jinpeng Li, Yueqian Wang, Dongyan Zhao</author><pubDate>Tue, 30 May 2023 06:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18756v1</guid></item><item><title>Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising</title><link>http://arxiv.org/abs/2305.18264v1</link><description>Leveraging large-scale image-text datasets and advancements in diffusionmodels, text-driven generative models have made remarkable strides in the fieldof image generation and editing. This study explores the potential of extendingthe text-driven ability to the generation and editing of multi-text conditionedlong videos. Current methodologies for video generation and editing, whileinnovative, are often confined to extremely short videos (typically less than24 frames) and are limited to a single text condition. These constraintssignificantly limit their applications given that real-world videos usuallyconsist of multiple segments, each bearing different semantic information. Toaddress this challenge, we introduce a novel paradigm dubbed as Gen-L-Video,capable of extending off-the-shelf short video diffusion models for generatingand editing videos comprising hundreds of frames with diverse semantic segmentswithout introducing additional training, all while preserving contentconsistency. We have implemented three mainstream text-driven video generationand editing methodologies and extended them to accommodate longer videos imbuedwith a variety of semantic segments with our proposed paradigm. Ourexperimental outcomes reveal that our approach significantly broadens thegenerative and editing capabilities of video diffusion models, offering newpossibilities for future research and applications. The code is available athttps://github.com/G-U-N/Gen-L-Video.</description><author>Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, Hongsheng Li</author><pubDate>Mon, 29 May 2023 18:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18264v1</guid></item><item><title>3rd Place Solution for PVUW2023 VSS Track: A Large Model for Semantic Segmentation on VSPW</title><link>http://arxiv.org/abs/2306.02291v2</link><description>In this paper, we introduce 3rd place solution for PVUW2023 VSS track.Semantic segmentation is a fundamental task in computer vision with numerousreal-world applications. We have explored various image-level visual backbonesand segmentation heads to tackle the problem of video semantic segmentation.Through our experimentation, we find that InternImage-H as the backbone andMask2former as the segmentation head achieves the best performance. Inaddition, we explore two post-precessing methods: CascadePSP and SegmentAnything Model (SAM). Ultimately, our approach obtains 62.60\% and 64.84\% mIoUon the VSPW test set1 and final test set, respectively, securing the thirdposition in the PVUW2023 VSS track.</description><author>Shijie Chang, Zeqi Hao, Ben Kang, Xiaoqi Zhao, Jiawen Zhu, Zhenyu Chen, Lihe Zhang, Lu Zhang, Huchuan Lu</author><pubDate>Tue, 06 Jun 2023 02:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02291v2</guid></item><item><title>Semantic Segmentation on VSPW Dataset through Contrastive Loss and Multi-dataset Training Approach</title><link>http://arxiv.org/abs/2306.03508v1</link><description>Video scene parsing incorporates temporal information, which can enhance theconsistency and accuracy of predictions compared to image scene parsing. Theadded temporal dimension enables a more comprehensive understanding of thescene, leading to more reliable results. This paper presents the winningsolution of the CVPR2023 workshop for video semantic segmentation, focusing onenhancing Spatial-Temporal correlations with contrastive loss. We also explorethe influence of multi-dataset training by utilizing a label-mapping technique.And the final result is aggregating the output of the above two models. Ourapproach achieves 65.95% mIoU performance on the VSPW dataset, ranked 1st placeon the VSPW challenge at CVPR 2023.</description><author>Min Yan, Qianxiong Ning, Qian Wang</author><pubDate>Tue, 06 Jun 2023 09:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03508v1</guid></item><item><title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</title><link>http://arxiv.org/abs/2306.00595v2</link><description>We focus on the weakly-supervised audio-visual video parsing task (AVVP),which aims to identify and locate all the events in audio/visual modalities.Previous works only concentrate on video-level overall label denoising acrossmodalities, but overlook the segment-level label noise, where adjacent videosegments (i.e., 1-second video clips) may contain different events. However,recognizing events in the segment is challenging because its label could be anycombination of events that occur in the video. To address this issue, weconsider tackling AVVP from the language perspective, since language couldfreely describe how various events appear in each segment beyond fixed labels.Specifically, we design language prompts to describe all cases of eventappearance for each video. Then, the similarity between language prompts andsegments is calculated, where the event of the most similar prompt is regardedas the segment-level label. In addition, to deal with the mislabeled segments,we propose to perform dynamic re-weighting on the unreliable segments to adjusttheir labels. Experiments show that our simple yet effective approachoutperforms state-of-the-art methods by a large margin.</description><author>Yingying Fan, Yu Wu, Yutian Lin, Bo Du</author><pubDate>Wed, 07 Jun 2023 07:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00595v2</guid></item><item><title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</title><link>http://arxiv.org/abs/2306.00595v1</link><description>We focus on the weakly-supervised audio-visual video parsing task (AVVP),which aims to identify and locate all the events in audio/visual modalities.Previous works only concentrate on video-level overall label denoising acrossmodalities, but overlook the segment-level label noise, where adjacent videosegments (i.e., 1-second video clips) may contain different events. However,recognizing events in the segment is challenging because its label could be anycombination of events that occur in the video. To address this issue, weconsider tackling AVVP from the language perspective, since language couldfreely describe how various events appear in each segment beyond fixed labels.Specifically, we design language prompts to describe all cases of eventappearance for each video. Then, the similarity between language prompts andsegments is calculated, where the event of the most similar prompt is regardedas the segment-level label. In addition, to deal with the mislabeled segments,we propose to perform dynamic re-weighting on the unreliable segments to adjusttheir labels. Experiments show that our simple yet effective approachoutperforms state-of-the-art methods by a large margin.</description><author>Yingying Fan, Yu Wu, Yutian Lin, Bo Du</author><pubDate>Thu, 01 Jun 2023 13:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00595v1</guid></item><item><title>Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization</title><link>http://arxiv.org/abs/2204.06228v2</link><description>Due to its high societal impact, deepfake detection is getting activeattention in the computer vision community. Most deepfake detection methodsrely on identity, facial attributes, and adversarial perturbation-basedspatio-temporal modifications at the whole video or random locations whilekeeping the meaning of the content intact. However, a sophisticated deepfakemay contain only a small segment of video/audio manipulation, through which themeaning of the content can be, for example, completely inverted from asentiment perspective. We introduce a content-driven audio-visual deepfakedataset, termed Localized Audio Visual DeepFake (LAV-DF), explicitly designedfor the task of learning temporal forgery localization. Specifically, thecontent-driven audio-visual manipulations are performed strategically to changethe sentiment polarity of the whole video. Our baseline method for benchmarkingthe proposed dataset is a 3DCNN model, termed as Boundary Aware TemporalForgery Detection (BA-TFD), which is guided via contrastive, boundary matching,and frame classification loss functions. Our extensive quantitative andqualitative analysis demonstrates the proposed method's strong performance fortemporal forgery localization and deepfake detection tasks.</description><author>Zhixi Cai, Kalin Stefanov, Abhinav Dhall, Munawar Hayat</author><pubDate>Thu, 04 May 2023 01:41:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.06228v2</guid></item><item><title>Boosting Weakly-Supervised Temporal Action Localization with Text Information</title><link>http://arxiv.org/abs/2305.00607v1</link><description>Due to the lack of temporal annotation, current Weakly-supervised TemporalAction Localization (WTAL) methods are generally stuck into over-complete orincomplete localization. In this paper, we aim to leverage the text informationto boost WTAL from two aspects, i.e., (a) the discriminative objective toenlarge the inter-class difference, thus reducing the over-complete; (b) thegenerative objective to enhance the intra-class integrity, thus finding morecomplete temporal boundaries. For the discriminative objective, we propose aText-Segment Mining (TSM) mechanism, which constructs a text description basedon the action class label, and regards the text as the query to mine allclass-related segments. Without the temporal annotation of actions, TSMcompares the text query with the entire videos across the dataset to mine thebest matching segments while ignoring irrelevant ones. Due to the sharedsub-actions in different categories of videos, merely applying TSM is toostrict to neglect the semantic-related segments, which results in incompletelocalization. We further introduce a generative objective named Video-textLanguage Completion (VLC), which focuses on all semantic-related segments fromvideos to complete the text sentence. We achieve the state-of-the-artperformance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find ourproposed method can be seamlessly applied to existing methods, and improvetheir performances with a clear margin. The code is available athttps://github.com/lgzlIlIlI/Boosting-WTAL.</description><author>Guozhang Li, De Cheng, Xinpeng Ding, Nannan Wang, Xiaoyu Wang, Xinbo Gao</author><pubDate>Mon, 01 May 2023 01:07:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00607v1</guid></item><item><title>Tracking through Containers and Occluders in the Wild</title><link>http://arxiv.org/abs/2305.03052v1</link><description>Tracking objects with persistence in cluttered and dynamic environmentsremains a difficult challenge for computer vision systems. In this paper, weintroduce $\textbf{TCOW}$, a new benchmark and model for visual trackingthrough heavy occlusion and containment. We set up a task where the goal is to,given a video sequence, segment both the projected extent of the target object,as well as the surrounding container or occluder whenever one exists. To studythis task, we create a mixture of synthetic and annotated real datasets tosupport both supervised learning and structured evaluation of model performanceunder various forms of task variation, such as moving or nested containment. Weevaluate two recent transformer-based video models and find that while they canbe surprisingly capable of tracking targets under certain settings of taskvariation, there remains a considerable performance gap before we can claim atracking model to have acquired a true notion of object permanence.</description><author>Basile Van Hoorick, Pavel Tokmakov, Simon Stent, Jie Li, Carl Vondrick</author><pubDate>Thu, 04 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03052v1</guid></item><item><title>Siamese Masked Autoencoders</title><link>http://arxiv.org/abs/2305.14344v1</link><description>Establishing correspondence between images or scenes is a significantchallenge in computer vision, especially given occlusions, viewpoint changes,and varying object appearances. In this paper, we present Siamese MaskedAutoencoders (SiamMAE), a simple extension of Masked Autoencoders (MAE) forlearning visual correspondence from videos. SiamMAE operates on pairs ofrandomly sampled video frames and asymmetrically masks them. These frames areprocessed independently by an encoder network, and a decoder composed of asequence of cross-attention layers is tasked with predicting the missingpatches in the future frame. By masking a large fraction ($95\%$) of patches inthe future frame while leaving the past frame unchanged, SiamMAE encourages thenetwork to focus on object motion and learn object-centric representations.Despite its conceptual simplicity, features learned via SiamMAE outperformstate-of-the-art self-supervised methods on video object segmentation, posekeypoint propagation, and semantic part propagation tasks. SiamMAE achievescompetitive results without relying on data augmentation, handcraftedtracking-based pretext tasks, or other techniques to prevent representationalcollapse.</description><author>Agrim Gupta, Jiajun Wu, Jia Deng, Li Fei-Fei</author><pubDate>Tue, 23 May 2023 18:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14344v1</guid></item><item><title>Multimodal Short Video Rumor Detection System Based on Contrastive Learning</title><link>http://arxiv.org/abs/2304.08401v3</link><description>With the rise of short video platforms as prominent channels for newsdissemination, major platforms in China have gradually evolved into fertilegrounds for the proliferation of fake news. However, distinguishing short videorumors poses a significant challenge due to the substantial amount ofinformation and shared features among videos, resulting in homogeneity. Toaddress the dissemination of short video rumors effectively, our research groupproposes a methodology encompassing multimodal feature fusion and theintegration of external knowledge, considering the merits and drawbacks of eachalgorithm. The proposed detection approach entails the following steps: (1)creation of a comprehensive dataset comprising multiple features extracted fromshort videos; (2) development of a multimodal rumor detection model: first, weemploy the Temporal Segment Networks (TSN) video coding model to extract videofeatures, followed by the utilization of Optical Character Recognition (OCR)and Automatic Speech Recognition (ASR) to extract textual features.Subsequently, the BERT model is employed to fuse textual and video features;(3) distinction is achieved through contrast learning: we acquire externalknowledge by crawling relevant sources and leverage a vector database toincorporate this knowledge into the classification output. Our research processis driven by practical considerations, and the knowledge derived from thisstudy will hold significant value in practical scenarios, such as short videorumor identification and the management of social opinions.</description><author>Yuxing Yang, Junhao Zhao, Siyi Wang, Xiangyu Min, Pengchao Wang, Haizhou Wang</author><pubDate>Wed, 17 May 2023 14:12:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08401v3</guid></item><item><title>Learning Grounded Vision-Language Representation for Versatile Understanding in Untrimmed Videos</title><link>http://arxiv.org/abs/2303.06378v2</link><description>Joint video-language learning has received increasing attention in recentyears. However, existing works mainly focus on single or multiple trimmed videoclips (events), which makes human-annotated event boundaries necessary duringinference. To break away from the ties, we propose a grounded vision-languagelearning framework for untrimmed videos, which automatically detectsinformative events and effectively excavates the alignments betweenmulti-sentence descriptions and corresponding event segments. Instead ofcoarse-level video-language alignments, we present two dual pretext tasks toencourage fine-grained segment-level alignments, i.e., text-to-event grounding(TEG) and event-to-text generation (ETG). TEG learns to adaptively ground thepossible event proposals given a set of sentences by estimating the cross-modaldistance in a joint semantic space. Meanwhile, ETG aims to reconstruct(generate) the matched texts given event proposals, encouraging the eventrepresentation to retain meaningful semantic information. To encourage accuratelabel assignment between the event set and the text set, we propose a novelsemantic-aware cost to mitigate the sub-optimal matching results caused byambiguous boundary annotations. Our framework is easily extensible to taskscovering visually-grounded language understanding and generation. We achievestate-of-the-art dense video captioning performance on ActivityNet Captions,YouCook2 and YouMakeup, and competitive performance on several other languagegeneration and understanding tasks. Our method also achieved 1st place in boththe MTVG and MDVC tasks of the PIC 4th Challenge. Our code is publiclyavailable at https://github.com/zjr2000/GVL.</description><author>Teng Wang, Jinrui Zhang, Feng Zheng, Wenhao Jiang, Ran Cheng, Ping Luo</author><pubDate>Wed, 17 May 2023 10:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06378v2</guid></item><item><title>Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization</title><link>http://arxiv.org/abs/2305.17861v1</link><description>Weakly-supervised temporal action localization aims to localize and recognizeactions in untrimmed videos with only video-level category labels duringtraining. Without instance-level annotations, most existing methods follow theSegment-based Multiple Instance Learning (S-MIL) framework, where thepredictions of segments are supervised by the labels of videos. However, theobjective for acquiring segment-level scores during training is not consistentwith the target for acquiring proposal-level scores during testing, leading tosuboptimal results. To deal with this problem, we propose a novelProposal-based Multiple Instance Learning (P-MIL) framework that directlyclassifies the candidate proposals in both the training and testing stages,which includes three key designs: 1) a surrounding contrastive featureextraction module to suppress the discriminative short proposals by consideringthe surrounding contrastive information, 2) a proposal completeness evaluationmodule to inhibit the low-quality proposals with the guidance of thecompleteness pseudo labels, and 3) an instance-level rank consistency loss toachieve robust detection by leveraging the complementarity of RGB and FLOWmodalities. Extensive experimental results on two challenging benchmarksincluding THUMOS14 and ActivityNet demonstrate the superior performance of ourmethod.</description><author>Huan Ren, Wenfei Yang, Tianzhu Zhang, Yongdong Zhang</author><pubDate>Mon, 29 May 2023 03:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17861v1</guid></item><item><title>Domain Adaptive Sim-to-Real Segmentation of Oropharyngeal Organs</title><link>http://arxiv.org/abs/2305.10883v1</link><description>Video-assisted transoral tracheal intubation (TI) necessitates using anendoscope that helps the physician insert a tracheal tube into the glottisinstead of the esophagus. The growing trend of robotic-assisted TI wouldrequire a medical robot to distinguish anatomical features like an experiencedphysician which can be imitated by utilizing supervised deep-learningtechniques. However, the real datasets of oropharyngeal organs are ofteninaccessible due to limited open-source data and patient privacy. In this work,we propose a domain adaptive Sim-to-Real framework called IoU-RankingBlend-ArtFlow (IRB-AF) for image segmentation of oropharyngeal organs. Theframework includes an image blending strategy called IoU-Ranking Blend (IRB)and style-transfer method ArtFlow. Here, IRB alleviates the problem of poorsegmentation performance caused by significant datasets domain differences;while ArtFlow is introduced to reduce the discrepancies between datasetsfurther. A virtual oropharynx image dataset generated by the SOFA framework isused as the learning subject for semantic segmentation to deal with the limitedavailability of actual endoscopic images. We adapted IRB-AF with thestate-of-the-art domain adaptive segmentation models. The results demonstratethe superior performance of our approach in further improving the segmentationaccuracy and training stability.</description><author>Guankun Wang, Tian-Ao Ren, Jiewen Lai, Long Bai, Hongliang Ren</author><pubDate>Thu, 18 May 2023 12:25:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10883v1</guid></item><item><title>Transformer-Based Visual Segmentation: A Survey</title><link>http://arxiv.org/abs/2304.09854v2</link><description>Visual segmentation seeks to partition images, video frames, or point cloudsinto multiple segments or groups. This technique has numerous real-worldapplications, such as autonomous driving, image editing, robot sensing, andmedical analysis. Over the past decade, deep learning-based methods have maderemarkable strides in this area. Recently, transformers, a type of neuralnetwork based on self-attention originally designed for natural languageprocessing, have considerably surpassed previous convolutional or recurrentapproaches in various vision processing tasks. Specifically, visiontransformers offer robust, unified, and even simpler solutions for varioussegmentation tasks. This survey provides a thorough overview oftransformer-based visual segmentation, summarizing recent advancements. Wefirst review the background, encompassing problem definitions, datasets, andprior convolutional methods. Next, we summarize a meta-architecture thatunifies all recent transformer-based approaches. Based on thismeta-architecture, we examine various method designs, including modificationsto the meta-architecture and associated applications. We also present severalclosely related settings, including 3D point cloud segmentation, foundationmodel tuning, domain-aware segmentation, efficient segmentation, and medicalsegmentation. Additionally, we compile and re-evaluate the reviewed methods onseveral well-established datasets. Finally, we identify open challenges in thisfield and propose directions for future research. The project page can be foundat https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will alsocontinually monitor developments in this rapidly evolving field.</description><author>Xiangtai Li, Henghui Ding, Wenwei Zhang, Haobo Yuan, Jiangmiao Pang, Guangliang Cheng, Kai Chen, Ziwei Liu, Chen Change Loy</author><pubDate>Fri, 02 Jun 2023 08:33:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09854v2</guid></item><item><title>PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?</title><link>http://arxiv.org/abs/2202.05821v3</link><description>This paper presents the design and results of the "PEg TRAnsfert Workflowrecognition" (PETRAW) challenge whose objective was to develop surgicalworkflow recognition methods based on one or several modalities, among video,kinematic, and segmentation data, in order to study their added value. ThePETRAW challenge provided a data set of 150 peg transfer sequences performed ona virtual simulator. This data set was composed of videos, kinematics, semanticsegmentation, and workflow annotations which described the sequences at threedifferent granularity levels: phase, step, and activity. Five tasks wereproposed to the participants: three of them were related to the recognition ofall granularities with one of the available modalities, while the othersaddressed the recognition with a combination of modalities. Averageapplication-dependent balanced accuracy (AD-Accuracy) was used as evaluationmetric to take unbalanced classes into account and because it is moreclinically relevant than a frame-by-frame score. Seven teams participated in atleast one task and four of them in all tasks. Best results are obtained withthe use of the video and the kinematics data with an AD-Accuracy between 93%and 90% for the four teams who participated in all tasks. The improvementbetween video/kinematic-based methods and the uni-modality ones was significantfor all of the teams. However, the difference in testing execution time betweenthe video/kinematic-based and the kinematic-based methods has to be taken intoconsideration. Is it relevant to spend 20 to 200 times more computing time forless than 3% of improvement? The PETRAW data set is publicly available atwww.synapse.org/PETRAW to encourage further research in surgical workflowrecognition.</description><author>Arnaud Huaulmé, Kanako Harada, Quang-Minh Nguyen, Bogyu Park, Seungbum Hong, Min-Kook Choi, Michael Peven, Yunshuang Li, Yonghao Long, Qi Dou, Satyadwyoom Kumar, Seenivasan Lalithkumar, Ren Hongliang, Hiroki Matsuzaki, Yuto Ishikawa, Yuriko Harai, Satoshi Kondo, Mamoru Mitsuishi, Pierre Jannin</author><pubDate>Thu, 27 Apr 2023 14:27:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.05821v3</guid></item><item><title>Weakly-supervised Micro- and Macro-expression Spotting Based on Multi-level Consistency</title><link>http://arxiv.org/abs/2305.02734v1</link><description>Most micro- and macro-expression spotting methods in untrimmed videos sufferfrom the burden of video-wise collection and frame-wise annotation.Weakly-supervised expression spotting (WES) based on video-level labels canpotentially mitigate the complexity of frame-level annotation while achievingfine-grained frame-level spotting. However, we argue that existingweakly-supervised methods are based on multiple instance learning (MIL)involving inter-modality, inter-sample, and inter-task gaps. The inter-samplegap is primarily from the sample distribution and duration. Therefore, wepropose a novel and simple WES framework, MC-WES, using multi-consistencycollaborative mechanisms that include modal-level saliency, video-leveldistribution, label-level duration and segment-level feature consistencystrategies to implement fine frame-level spotting with only video-level labelsto alleviate the above gaps and merge prior knowledge. The modal-level saliencyconsistency strategy focuses on capturing key correlations between raw imagesand optical flow. The video-level distribution consistency strategy utilizesthe difference of sparsity in temporal distribution. The label-level durationconsistency strategy exploits the difference in the duration of facial muscles.The segment-level feature consistency strategy emphasizes that features underthe same labels maintain similarity. Experimental results on two challengingdatasets -- CAS(ME)$^2$ and SAMM-LV -- demonstrate that MC-WES is comparable tostate-of-the-art fully-supervised methods.</description><author>Wang-Wang Yu, Kai-Fu Yang, Hong-Mei Yan, Yong-Jie Li</author><pubDate>Thu, 04 May 2023 12:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02734v1</guid></item><item><title>MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning</title><link>http://arxiv.org/abs/2306.02252v1</link><description>We introduce MoviePuzzle, a novel challenge that targets visual narrativereasoning and holistic movie understanding. Despite the notable progress thathas been witnessed in the realm of video understanding, most prior works failto present tasks and models to address holistic video understanding and theinnate visual narrative structures existing in long-form videos. To tackle thisquandary, we put forth MoviePuzzle task that amplifies the temporal featurelearning and structure learning of video models by reshuffling the shot, frame,and clip layers of movie segments in the presence of video-dialogueinformation. We start by establishing a carefully refined dataset based onMovieNet by dissecting movies into hierarchical layers and randomly permutingthe orders. Besides benchmarking the MoviePuzzle with prior arts on movieunderstanding, we devise a Hierarchical Contrastive Movie Clustering (HCMC)model that considers the underlying structure and visual semantic orders formovie reordering. Specifically, through a pairwise and contrastive learningapproach, we train models to predict the correct order of each layer. Thisequips them with the knack for deciphering the visual narrative structure ofmovies and handling the disorder lurking in video data. Experiments show thatour approach outperforms existing state-of-the-art methods on the \MoviePuzzlebenchmark, underscoring its efficacy.</description><author>Jianghui Wang, Yuxuan Wang, Dongyan Zhao, Zilong Zheng</author><pubDate>Sun, 04 Jun 2023 04:51:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02252v1</guid></item><item><title>Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos: A review of the state of the art</title><link>http://arxiv.org/abs/2304.13014v1</link><description>In the field of computer- and robot-assisted minimally invasive surgery,enormous progress has been made in recent years based on the recognition ofsurgical instruments in endoscopic images. Especially the determination of theposition and type of the instruments is of great interest here. Current workinvolves both spatial and temporal information with the idea, that theprediction of movement of surgical tools over time may improve the quality offinal segmentations. The provision of publicly available datasets has recentlyencouraged the development of new methods, mainly based on deep learning. Inthis review, we identify datasets used for method development and evaluation,as well as quantify their frequency of use in the literature. We furtherpresent an overview of the current state of research regarding the segmentationand tracking of minimally invasive surgical instruments in endoscopic images.The paper focuses on methods that work purely visually without attached markersof any kind on the instruments, taking into account both single-framesegmentation approaches as well as those involving temporal information. Adiscussion of the reviewed literature is provided, highlighting existingshortcomings and emphasizing available potential for future developments. Thepublications considered were identified through the platforms Google Scholar,Web of Science, and PubMed. The search terms used were "instrumentsegmentation", "instrument tracking", "surgical tool segmentation", and"surgical tool tracking" and result in 408 articles published between 2015 and2022 from which 109 were included using systematic selection criteria.</description><author>Tobias Rueckert, Daniel Rueckert, Christoph Palm</author><pubDate>Tue, 25 Apr 2023 18:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13014v1</guid></item><item><title>Learning Weakly Supervised Audio-Visual Violence Detection in Hyperbolic Space</title><link>http://arxiv.org/abs/2305.18797v2</link><description>In recent years, the task of weakly supervised audio-visual violencedetection has gained considerable attention. The goal of this task is toidentify violent segments within multimodal data based on video-level labels.Despite advances in this field, traditional Euclidean neural networks, whichhave been used in prior research, encounter difficulties in capturing highlydiscriminative representations due to limitations of the feature space. Toovercome this, we propose HyperVD, a novel framework that learns snippetembeddings in hyperbolic space to improve model discrimination. Our frameworkcomprises a detour fusion module for multimodal fusion, effectively alleviatingmodality inconsistency between audio and visual signals. Additionally, wecontribute two branches of fully hyperbolic graph convolutional networks thatexcavate feature similarities and temporal relationships among snippets inhyperbolic space. By learning snippet representations in this space, theframework effectively learns semantic discrepancies between violent and normalevents. Extensive experiments on the XD-Violence benchmark demonstrate that ourmethod outperforms state-of-the-art methods by a sizable margin.</description><author>Xiaogang Peng, Hao Wen, Yikai Luo, Xiao Zhou, Keyang Yu, Yigang Wang, Zizhao Wu</author><pubDate>Fri, 02 Jun 2023 05:11:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18797v2</guid></item><item><title>MetaMorphosis: Task-oriented Privacy Cognizant Feature Generation for Multi-task Learning</title><link>http://arxiv.org/abs/2305.07815v1</link><description>With the growth of computer vision applications, deep learning, and edgecomputing contribute to ensuring practical collaborative intelligence (CI) bydistributing the workload among edge devices and the cloud. However, runningseparate single-task models on edge devices is inefficient regarding therequired computational resource and time. In this context, multi-task learningallows leveraging a single deep learning model for performing multiple tasks,such as semantic segmentation and depth estimation on incoming video frames.This single processing pipeline generates common deep features that are sharedamong multi-task modules. However, in a collaborative intelligence scenario,generating common deep features has two major issues. First, the deep featuresmay inadvertently contain input information exposed to the downstream modules(violating input privacy). Second, the generated universal features expose apiece of collective information than what is intended for a certain task, inwhich features for one task can be utilized to perform another task (violatingtask privacy). This paper proposes a novel deep learning-basedprivacy-cognizant feature generation process called MetaMorphosis that limitsinference capability to specific tasks at hand. To achieve this, we propose achannel squeeze-excitation based feature metamorphosis module, Cross-SEC, toachieve distinct attention of all tasks and a de-correlation loss function withdifferential-privacy to train a deep learning model that produces distinctprivacy-aware features as an output for the respective tasks. With extensiveexperimentation on four datasets consisting of diverse images related to sceneunderstanding and facial attributes, we show that MetaMorphosis outperformsrecent adversarial learning and universal feature generation methods byguaranteeing privacy requirements in an efficient way for image and videoanalytics.</description><author>Md Adnan Arefeen, Zhouyu Li, Md Yusuf Sarwar Uddin, Anupam Das</author><pubDate>Sat, 13 May 2023 02:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07815v1</guid></item><item><title>Learning Weakly Supervised Audio-Visual Violence Detection in Hyperbolic Space</title><link>http://arxiv.org/abs/2305.18797v1</link><description>In recent years, the task of weakly supervised audio-visual violencedetection has gained considerable attention. The goal of this task is toidentify violent segments within multimodal data based on video-level labels.Despite advances in this field, traditional Euclidean neural networks, whichhave been used in prior research, encounter difficulties in capturing highlydiscriminative representations due to limitations of the feature space. Toovercome this, we propose HyperVD, a novel framework that learns snippetembeddings in hyperbolic space to improve model discrimination. Our frameworkcomprises a detour fusion module for multimodal fusion, effectively alleviatingmodality inconsistency between audio and visual signals. Additionally, wecontribute two branches of fully hyperbolic graph convolutional networks thatexcavate feature similarities and temporal relationships among snippets inhyperbolic space. By learning snippet representations in this space, theframework effectively learns semantic discrepancies between violent and normalevents. Extensive experiments on the XD-Violence benchmark demonstrate that ourmethod outperforms state-of-the-art methods by a sizable margin.</description><author>Xiaogang Peng, Hao Wen, Yikai Luo, Xiao Zhou, Keyang Yu, Yigang Wang, Zizhao Wu</author><pubDate>Tue, 30 May 2023 08:18:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18797v1</guid></item><item><title>TEyeD: Over 20 million real-world eye images with Pupil, Eyelid, and Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and Eye Movement Types</title><link>http://arxiv.org/abs/2102.02115v3</link><description>We present TEyeD, the world's largest unified public data set of eye imagestaken with head-mounted devices. TEyeD was acquired with seven differenthead-mounted eye trackers. Among them, two eye trackers were integrated intovirtual reality (VR) or augmented reality (AR) devices. The images in TEyeDwere obtained from various tasks, including car rides, simulator rides, outdoorsports activities, and daily indoor activities. The data set includes 2D and 3Dlandmarks, semantic segmentation, 3D eyeball annotation and the gaze vector andeye movement types for all images. Landmarks and semantic segmentation areprovided for the pupil, iris and eyelids. Video lengths vary from a few minutesto several hours. With more than 20 million carefully annotated images, TEyeDprovides a unique, coherent resource and a valuable foundation for advancingresearch in the field of computer vision, eye tracking and gaze estimation inmodern VR and AR applications. Download: Just connect via FTP as user TEyeDUser and without password tonephrit.cs.uni-tuebingen.de (ftp://nephrit.cs.uni-tuebingen.de).</description><author>Wolfgang Fuhl, Gjergji Kasneci, Enkelejda Kasneci</author><pubDate>Tue, 06 Jun 2023 09:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.02115v3</guid></item><item><title>Perception Test: A Diagnostic Benchmark for Multimodal Video Models</title><link>http://arxiv.org/abs/2305.13786v1</link><description>We propose a novel multimodal video benchmark - the Perception Test - toevaluate the perception and reasoning skills of pre-trained multimodal models(e.g. Flamingo, BEiT-3, or GPT-4). Compared to existing benchmarks that focuson computational tasks (e.g. classification, detection or tracking), thePerception Test focuses on skills (Memory, Abstraction, Physics, Semantics) andtypes of reasoning (descriptive, explanatory, predictive, counterfactual)across video, audio, and text modalities, to provide a comprehensive andefficient evaluation tool. The benchmark probes pre-trained models for theirtransfer capabilities, in a zero-shot / few-shot or limited finetuning regime.For these purposes, the Perception Test introduces 11.6k real-world videos, 23saverage length, designed to show perceptually interesting situations, filmed byaround 100 participants worldwide. The videos are densely annotated with sixtypes of labels (multiple-choice and grounded video question-answers, objectand point tracks, temporal action and sound segments), enabling both languageand non-language evaluations. The fine-tuning and validation splits of thebenchmark are publicly available (CC-BY license), in addition to a challengeserver with a held-out test split. Human baseline results compared tostate-of-the-art video QA models show a significant gap in performance (91.4%vs 43.6%), suggesting that there is significant room for improvement inmultimodal video understanding. Dataset, baselines code, and challenge server are available athttps://github.com/deepmind/perception_test</description><author>Viorica Pătrăucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, João Carreira</author><pubDate>Tue, 23 May 2023 08:54:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13786v1</guid></item><item><title>EasyPortrait -- Face Parsing and Portrait Segmentation Dataset</title><link>http://arxiv.org/abs/2304.13509v2</link><description>Recently, due to COVID-19 and the growing demand for remote work, videoconferencing apps have become especially widespread. The most valuable featuresof video chats are real-time background removal and face beautification. Whilesolving these tasks, computer vision researchers face the problem of havingrelevant data for the training stage. There is no large dataset withhigh-quality labeled and diverse images of people in front of a laptop orsmartphone camera to train a lightweight model without additional approaches.To boost the progress in this area, we provide a new image dataset,EasyPortrait, for portrait segmentation and face parsing tasks. It contains20,000 primarily indoor photos of 8,377 unique users, and fine-grainedsegmentation masks separated into 9 classes. Images are collected and labeledfrom crowdsourcing platforms. Unlike most face parsing datasets, inEasyPortrait, the beard is not considered part of the skin mask, and the insidearea of the mouth is separated from the teeth. These features allow usingEasyPortrait for skin enhancement and teeth whitening tasks. This paperdescribes the pipeline for creating a large-scale and clean image segmentationdataset using crowdsourcing platforms without additional synthetic data.Moreover, we trained several models on EasyPortrait and showed experimentalresults. Proposed dataset and trained models are publicly available.</description><author>Alexander Kapitanov, Karina Kvanchiani, Sofia Kirillova</author><pubDate>Tue, 02 May 2023 06:32:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13509v2</guid></item><item><title>Visual Causal Scene Refinement for Video Question Answering</title><link>http://arxiv.org/abs/2305.04224v1</link><description>Existing methods for video question answering (VideoQA) often suffer fromspurious correlations between different modalities, leading to a failure inidentifying the dominant visual evidence and the intended question. Moreover,these methods function as black boxes, making it difficult to interpret thevisual scene during the QA process. In this paper, to discover critical videosegments and frames that serve as the visual causal scene for generatingreliable answers, we present a causal analysis of VideoQA and propose aframework for cross-modal causal relational reasoning, named Visual CausalScene Refinement (VCSR). Particularly, a set of causal front-door interventionoperations is introduced to explicitly find the visual causal scenes at bothsegment and frame levels. Our VCSR involves two essential modules: i) theQuestion-Guided Refiner (QGR) module, which refines consecutive video framesguided by the question semantics to obtain more representative segment featuresfor causal front-door intervention; ii) the Causal Scene Separator (CSS)module, which discovers a collection of visual causal and non-causal scenesbased on the visual-linguistic causal relevance and estimates the causal effectof the scene-separating intervention in a contrastive learning manner.Extensive experiments on the NExT-QA, Causal-VidQA, and MSRVTT-QA datasetsdemonstrate the superiority of our VCSR in discovering visual causal scene andachieving robust video question answering.</description><author>Yushen Wei, Yang Liu, Hong Yan, Guanbin Li, Liang Lin</author><pubDate>Sun, 07 May 2023 10:05:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04224v1</guid></item><item><title>MarineVRS: Marine Video Retrieval System with Explainability via Semantic Understanding</title><link>http://arxiv.org/abs/2306.04593v1</link><description>Building a video retrieval system that is robust and reliable, especially forthe marine environment, is a challenging task due to several factors such asdealing with massive amounts of dense and repetitive data, occlusion,blurriness, low lighting conditions, and abstract queries. To address thesechallenges, we present MarineVRS, a novel and flexible video retrieval systemdesigned explicitly for the marine domain. MarineVRS integratesstate-of-the-art methods for visual and linguistic object representation toenable efficient and accurate search and analysis of vast volumes of underwatervideo data. In addition, unlike the conventional video retrieval system, whichonly permits users to index a collection of images or videos and search using afree-form natural language sentence, our retrieval system includes anadditional Explainability module that outputs the segmentation masks of theobjects that the input query referred to. This feature allows users to identifyand isolate specific objects in the video footage, leading to more detailedanalysis and understanding of their behavior and movements. Finally, with itsadaptability, explainability, accuracy, and scalability, MarineVRS is apowerful tool for marine researchers and scientists to efficiently andaccurately process vast amounts of data and gain deeper insights into thebehavior and movements of marine species.</description><author>Tan-Sang Ha, Hai Nguyen-Truong, Tuan-Anh Vu, Sai-Kit Yeung</author><pubDate>Wed, 07 Jun 2023 17:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04593v1</guid></item><item><title>Unifying (Machine) Vision via Counterfactual World Modeling</title><link>http://arxiv.org/abs/2306.01828v1</link><description>Leading approaches in machine vision employ different architectures fordifferent tasks, trained on costly task-specific labeled datasets. Thiscomplexity has held back progress in areas, such as robotics, where robusttask-general perception remains a bottleneck. In contrast, "foundation models"of natural language have shown how large pre-trained neural networks canprovide zero-shot solutions to a broad spectrum of apparently distinct tasks.Here we introduce Counterfactual World Modeling (CWM), a framework forconstructing a visual foundation model: a unified, unsupervised network thatcan be prompted to perform a wide variety of visual computations. CWM has twokey components, which resolve the core issues that have hindered application ofthe foundation model concept to vision. The first is structured masking, ageneralization of masked prediction methods that encourages a prediction modelto capture the low-dimensional structure in visual data. The model therebyfactors the key physical components of a scene and exposes an interface to themvia small sets of visual tokens. This in turn enables CWM's second main idea --counterfactual prompting -- the observation that many apparently distinctvisual representations can be computed, in a zero-shot manner, by comparing theprediction model's output on real inputs versus slightly modified("counterfactual") inputs. We show that CWM generates high-quality readouts onreal-world images and videos for a diversity of tasks, including estimation ofkeypoints, optical flow, occlusions, object segments, and relative depth. Takentogether, our results show that CWM is a promising path to unifying themanifold strands of machine vision in a conceptually simple foundation.</description><author>Daniel M. Bear, Kevin Feigelis, Honglin Chen, Wanhee Lee, Rahul Venkatesh, Klemen Kotar, Alex Durango, Daniel L. K. Yamins</author><pubDate>Fri, 02 Jun 2023 18:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.01828v1</guid></item><item><title>Audio-Visual Person-of-Interest DeepFake Detection</title><link>http://arxiv.org/abs/2204.03083v3</link><description>Face manipulation technology is advancing very rapidly, and new methods arebeing proposed day by day. The aim of this work is to propose a deepfakedetector that can cope with the wide variety of manipulation methods andscenarios encountered in the real world. Our key insight is that each personhas specific characteristics that a synthetic generator likely cannotreproduce. Accordingly, we extract audio-visual features which characterize theidentity of a person, and use them to create a person-of-interest (POI)deepfake detector. We leverage a contrastive learning paradigm to learn themoving-face and audio segment embeddings that are most discriminative for eachidentity. As a result, when the video and/or audio of a person is manipulated,its representation in the embedding space becomes inconsistent with the realidentity, allowing reliable detection. Training is carried out exclusively onreal talking-face video; thus, the detector does not depend on any specificmanipulation method and yields the highest generalization ability. In addition,our method can detect both single-modality (audio-only, video-only) andmulti-modality (audio-video) attacks, and is robust to low-quality or corruptedvideos. Experiments on a wide variety of datasets confirm that our methodensures a SOTA performance, especially on low quality videos. Code is publiclyavailable on-line at https://github.com/grip-unina/poi-forensics.</description><author>Davide Cozzolino, Alessandro Pianese, Matthias Nießner, Luisa Verdoliva</author><pubDate>Thu, 18 May 2023 07:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.03083v3</guid></item><item><title>Multi-Object Tracking and Segmentation with a Space-Time Memory Network</title><link>http://arxiv.org/abs/2110.11284v2</link><description>We propose a method for multi-object tracking and segmentation based on anovel memory-based mechanism to associate tracklets. The proposed tracker,MeNToS, addresses particularly the long-term data association problem, whenobjects are not observable for long time intervals. Indeed, the recentlyintroduced HOTA metric (High Order Tracking Accuracy), which has a betteralignment than the formerly established MOTA (Multiple Object TrackingAccuracy) with the human visual assessment of tracking, has shown thatimprovements are still needed for data association, despite the recentimprovement in object detection. In MeNToS, after creating tracklets usinginstance segmentation and optical flow, the proposed method relies on aspace-time memory network originally developed for one-shot video objectsegmentation to improve the association of sequence of detections (tracklets)with temporal gaps. We evaluate our tracker on KITTIMOTS and MOTSChallenge andwe show the benefit of our data association strategy with the HOTA metric.Additional ablation studies demonstrate that our approach using a space-timememory network gives better and more robust long-term association than thosebased on a re-identification network. Our project page is at\url{www.mehdimiah.com/mentos+}.</description><author>Mehdi Miah, Guillaume-Alexandre Bilodeau, Nicolas Saunier</author><pubDate>Tue, 16 May 2023 02:16:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.11284v2</guid></item><item><title>The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose</title><link>http://arxiv.org/abs/2007.00394v2</link><description>The availability of a large labeled dataset is a key requirement for applyingdeep learning methods to solve various computer vision tasks. In the context ofunderstanding human activities, existing public datasets, while large in size,are often limited to a single RGB camera and provide only per-frame or per-clipaction annotations. To enable richer analysis and understanding of humanactivities, we introduce IKEA ASM -- a three million frame, multi-view,furniture assembly video dataset that includes depth, atomic actions, objectsegmentation, and human pose. Additionally, we benchmark prominent methods forvideo action recognition, object segmentation and human pose estimation taskson this challenging dataset. The dataset enables the development of holisticmethods, which integrate multi-modal and multi-view data to better perform onthese tasks.</description><author>Yizhak Ben-Shabat, Xin Yu, Fatemeh Sadat Saleh, Dylan Campbell, Cristian Rodriguez-Opazo, Hongdong Li, Stephen Gould</author><pubDate>Wed, 17 May 2023 08:56:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.00394v2</guid></item><item><title>Self-supervised dense representation learning for live-cell microscopy with time arrow prediction</title><link>http://arxiv.org/abs/2305.05511v1</link><description>State-of-the-art object detection and segmentation methods for microscopyimages rely on supervised machine learning, which requires laborious manualannotation of training data. Here we present a self-supervised method based ontime arrow prediction pre-training that learns dense image representations fromraw, unlabeled live-cell microscopy videos. Our method builds upon the task ofpredicting the correct order of time-flipped image regions via a single-imagefeature extractor and a subsequent time arrow prediction head. We show that theresulting dense representations capture inherently time-asymmetric biologicalprocesses such as cell divisions on a pixel-level. We furthermore demonstratethe utility of these representations on several live-cell microscopy datasetsfor detection and segmentation of dividing cells, as well as for cell stateclassification. Our method outperforms supervised methods, particularly whenonly limited ground truth annotations are available as is commonly the case inpractice. We provide code at https://github.com/weigertlab/tarrow.</description><author>Benjamin Gallusser, Max Stieber, Martin Weigert</author><pubDate>Tue, 09 May 2023 15:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05511v1</guid></item><item><title>Unsupervised Object Learning via Common Fate</title><link>http://arxiv.org/abs/2110.06562v2</link><description>Learning generative object models from unlabelled videos is a long standingproblem and required for causal scene modeling. We decompose this problem intothree easier subtasks, and provide candidate solutions for each of them.Inspired by the Common Fate Principle of Gestalt Psychology, we first extract(noisy) masks of moving objects via unsupervised motion segmentation. Second,generative models are trained on the masks of the background and the movingobjects, respectively. Third, background and foreground models are combined ina conditional "dead leaves" scene model to sample novel scene configurationswhere occlusions and depth layering arise naturally. To evaluate the individualstages, we introduce the Fishbowl dataset positioned between complex real-worldscenes and common object-centric benchmarks of simplistic objects. We show thatour approach allows learning generative models that generalize beyond theocclusions present in the input videos, and represent scenes in a modularfashion that allows sampling plausible scenes outside the training distributionby permitting, for instance, object numbers or densities not observed in thetraining set.</description><author>Matthias Tangemann, Steffen Schneider, Julius von Kügelgen, Francesco Locatello, Peter Gehler, Thomas Brox, Matthias Kümmerer, Matthias Bethge, Bernhard Schölkopf</author><pubDate>Mon, 15 May 2023 13:22:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.06562v2</guid></item><item><title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer</title><link>http://arxiv.org/abs/2303.09681v3</link><description>Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available athttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be releasedupon paper acceptance.</description><author>Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng</author><pubDate>Thu, 11 May 2023 00:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09681v3</guid></item><item><title>Building Rearticulable Models for Arbitrary 3D Objects from 4D Point Clouds</title><link>http://arxiv.org/abs/2306.00979v1</link><description>We build rearticulable models for arbitrary everyday man-made objectscontaining an arbitrary number of parts that are connected together inarbitrary ways via 1 degree-of-freedom joints. Given point cloud videos of sucheveryday objects, our method identifies the distinct object parts, what partsare connected to what other parts, and the properties of the joints connectingeach part pair. We do this by jointly optimizing the part segmentation,transformation, and kinematics using a novel energy minimization framework. Ourinferred animatable models, enables retargeting to novel poses with sparsepoint correspondences guidance. We test our method on a new articulating robotdataset, and the Sapiens dataset with common daily objects, as well asreal-world scans. Experiments show that our method outperforms two leadingprior works on various metrics.</description><author>Shaowei Liu, Saurabh Gupta, Shenlong Wang</author><pubDate>Thu, 01 Jun 2023 18:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00979v1</guid></item><item><title>MeetingBank: A Benchmark Dataset for Meeting Summarization</title><link>http://arxiv.org/abs/2305.17529v1</link><description>As the number of recorded meetings increases, it becomes increasinglyimportant to utilize summarization technology to create useful summaries ofthese recordings. However, there is a crucial lack of annotated meeting corporafor developing this technology, as it can be hard to collect meetings,especially when the topics discussed are confidential. Furthermore, meetingsummaries written by experienced writers are scarce, making it hard forabstractive summarizers to produce sensible output without a reliablereference. This lack of annotated corpora has hindered the development ofmeeting summarization technology. In this paper, we present MeetingBank, a newbenchmark dataset of city council meetings over the past decade. MeetingBank isunique among other meeting corpora due to its divide-and-conquer approach,which involves dividing professionally written meeting minutes into shorterpassages and aligning them with specific segments of the meeting. This breaksdown the process of summarizing a lengthy meeting into smaller, more manageabletasks. The dataset provides a new testbed of various meeting summarizationsystems and also allows the public to gain insight into how council decisionsare made. We make the collection, including meeting video links, transcripts,reference summaries, agenda, and other metadata, publicly available tofacilitate the development of better meeting summarization techniques. Ourdataset can be accessed at: https://meetingbank.github.io</description><author>Yebowen Hu, Tim Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, Fei Liu</author><pubDate>Sat, 27 May 2023 18:09:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17529v1</guid></item><item><title>Phagocytosis Unveiled: A Scalable and Interpretable Deep learning Framework for Neurodegenerative Disease Analysis</title><link>http://arxiv.org/abs/2304.13764v1</link><description>Quantifying the phagocytosis of dynamic, unstained cells is essential forevaluating neurodegenerative diseases. However, measuring rapid cellinteractions and distinguishing cells from backgrounds make this taskchallenging when processing time-lapse phase-contrast video microscopy. In thisstudy, we introduce a fully automated, scalable, and versatile realtimeframework for quantifying and analyzing phagocytic activity. Our proposedpipeline can process large data-sets and includes a data quality verificationmodule to counteract potential perturbations such as microscope movements andframe blurring. We also propose an explainable cell segmentation module toimprove the interpretability of deep learning methods compared to black-boxalgorithms. This includes two interpretable deep learning capabilities: visualexplanation and model simplification. We demonstrate that interpretability indeep learning is not the opposite of high performance, but rather providesessential deep learning algorithm optimization insights and solutions.Incorporating interpretable modules results in an efficient architecture designand optimized execution time. We apply this pipeline to quantify and analyzemicroglial cell phagocytosis in frontotemporal dementia (FTD) and obtainstatistically reliable results showing that FTD mutant cells are larger andmore aggressive than control cells. To stimulate translational approaches andfuture research, we release an open-source pipeline and a unique microglialcells phagocytosis dataset for immune system characterization inneurodegenerative diseases research. This pipeline and dataset willconsistently crystallize future advances in this field, promoting thedevelopment of efficient and effective interpretable algorithms dedicated tothis critical domain. https://github.com/ounissimehdi/PhagoStat</description><author>Mehdi Ounissi, Morwena Latouche, Daniel Racoceanu</author><pubDate>Wed, 26 Apr 2023 19:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13764v1</guid></item><item><title>All Keypoints You Need: Detecting Arbitrary Keypoints on the Body of Triple, High, and Long Jump Athletes</title><link>http://arxiv.org/abs/2304.02939v2</link><description>Performance analyses based on videos are commonly used by coaches of athletesin various sports disciplines. In individual sports, these analyses mainlycomprise the body posture. This paper focuses on the disciplines of triple,high, and long jump, which require fine-grained locations of the athlete'sbody. Typical human pose estimation datasets provide only a very limited set ofkeypoints, which is not sufficient in this case. Therefore, we propose a methodto detect arbitrary keypoints on the whole body of the athlete by leveragingthe limited set of annotated keypoints and auto-generated segmentation masks ofbody parts. Evaluations show that our model is capable of detecting keypointson the head, torso, hands, feet, arms, and legs, including also bent elbows andknees. We analyze and compare different techniques to encode desired keypointsas the model's input and their embedding for the Transformer backbone.</description><author>Katja Ludwig, Julian Lorenz, Robin Schön, Rainer Lienhart</author><pubDate>Wed, 10 May 2023 09:15:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02939v2</guid></item><item><title>Can SAM Boost Video Super-Resolution?</title><link>http://arxiv.org/abs/2305.06524v2</link><description>The primary challenge in video super-resolution (VSR) is to handle largemotions in the input frames, which makes it difficult to accurately aggregateinformation from multiple frames. Existing works either adopt deformableconvolutions or estimate optical flow as a prior to establish correspondencesbetween frames for the effective alignment and fusion. However, they fail totake into account the valuable semantic information that can greatly enhanceit; and flow-based methods heavily rely on the accuracy of a flow estimatemodel, which may not provide precise flows given two low-resolution frames. In this paper, we investigate a more robust and semantic-aware prior forenhanced VSR by utilizing the Segment Anything Model (SAM), a powerfulfoundational model that is less susceptible to image degradation. To use theSAM-based prior, we propose a simple yet effective module -- SAM-guidEdrefinEment Module (SEEM), which can enhance both alignment and fusionprocedures by the utilization of semantic information. This light-weightplug-in module is specifically designed to not only leverage the attentionmechanism for the generation of semantic-aware feature but also be easily andseamlessly integrated into existing methods. Concretely, we apply our SEEM totwo representative methods, EDVR and BasicVSR, resulting in consistentlyimproved performance with minimal implementation effort, on three widely usedVSR datasets: Vimeo-90K, REDS and Vid4. More importantly, we found that theproposed SEEM can advance the existing methods in an efficient tuning manner,providing increased flexibility in adjusting the balance between performanceand the number of training parameters. Code will be open-source soon.</description><author>Zhihe Lu, Zeyu Xiao, Jiawang Bai, Zhiwei Xiong, Xinchao Wang</author><pubDate>Fri, 12 May 2023 02:43:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06524v2</guid></item><item><title>Materialistic: Selecting Similar Materials in Images</title><link>http://arxiv.org/abs/2305.13291v1</link><description>Separating an image into meaningful underlying components is a crucial firststep for both editing and understanding images. We present a method capable ofselecting the regions of a photograph exhibiting the same material as anartist-chosen area. Our proposed approach is robust to shading, specularhighlights, and cast shadows, enabling selection in real images. As we do notrely on semantic segmentation (different woods or metal should not be selectedtogether), we formulate the problem as a similarity-based grouping problembased on a user-provided image location. In particular, we propose to leveragethe unsupervised DINO features coupled with a proposed Cross-Similarity moduleand an MLP head to extract material similarities in an image. We train ourmodel on a new synthetic image dataset, that we release. We show that ourmethod generalizes well to real-world images. We carefully analyze our model'sbehavior on varying material properties and lighting. Additionally, we evaluateit against a hand-annotated benchmark of 50 real photographs. We furtherdemonstrate our model on a set of applications, including material editing,in-video selection, and retrieval of object photographs with similar materials.</description><author>Prafull Sharma, Julien Philip, Michaël Gharbi, William T. Freeman, Fredo Durand, Valentin Deschaintre</author><pubDate>Mon, 22 May 2023 18:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13291v1</guid></item><item><title>FDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models</title><link>http://arxiv.org/abs/2306.00783v1</link><description>The ability to create high-quality 3D faces from a single image has becomeincreasingly important with wide applications in video conferencing, AR/VR, andadvanced video editing in movie industries. In this paper, we propose FaceDiffusion NeRF (FDNeRF), a new generative method to reconstruct high-qualityFace NeRFs from single images, complete with semantic editing and relightingcapabilities. FDNeRF utilizes high-resolution 3D GAN inversion and expertlytrained 2D latent-diffusion model, allowing users to manipulate and constructFace NeRFs in zero-shot learning without the need for explicit 3D data. Withcarefully designed illumination and identity preserving loss, as well asmulti-modal pre-training, FD-NeRF offers users unparalleled control over theediting process enabling them to create and edit face NeRFs using justsingle-view images, text prompts, and explicit target lighting. The advancedfeatures of FDNeRF have been designed to produce more impressive results thanexisting 2D editing approaches that rely on 2D segmentation maps for editableattributes. Experiments show that our FDNeRF achieves exceptionally realisticresults and unprecedented flexibility in editing compared with state-of-the-art3D face reconstruction and editing methods. Our code will be available athttps://github.com/BillyXYB/FDNeRF.</description><author>Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing, Tai Chi-Keung Tang</author><pubDate>Thu, 01 Jun 2023 16:14:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00783v1</guid></item><item><title>SanskritShala: A Neural Sanskrit NLP Toolkit with Web-Based Interface for Pedagogical and Annotation Purposes</title><link>http://arxiv.org/abs/2302.09527v2</link><description>We present a neural Sanskrit Natural Language Processing (NLP) toolkit namedSanskritShala (a school of Sanskrit) to facilitate computational linguisticanalyses for several tasks such as word segmentation, morphological tagging,dependency parsing, and compound type identification. Our systems currentlyreport state-of-the-art performance on available benchmark datasets for alltasks. SanskritShala is deployed as a web-based application, which allows auser to get real-time analysis for the given input. It is built witheasy-to-use interactive data annotation features that allow annotators tocorrect the system predictions when it makes mistakes. We publicly release thesource codes of the 4 modules included in the toolkit, 7 word embedding modelsthat have been trained on publicly available Sanskrit corpora and multipleannotated datasets such as word similarity, relatedness, categorization,analogy prediction to assess intrinsic properties of word embeddings. So far aswe know, this is the first neural-based Sanskrit NLP toolkit that has aweb-based interface and a number of NLP modules. We are sure that the peoplewho are willing to work with Sanskrit will find it useful for pedagogical andannotative purposes. SanskritShala is available at:https://cnerg.iitkgp.ac.in/sanskritshala. The demo video of our platform can beaccessed at: https://youtu.be/x0X31Y9k0mw4.</description><author>Jivnesh Sandhan, Anshul Agarwal, Laxmidhar Behera, Tushar Sandhan, Pawan Goyal</author><pubDate>Mon, 29 May 2023 08:36:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.09527v2</guid></item><item><title>OmniDet: Surround View Cameras based Multi-task Visual Perception Network for Autonomous Driving</title><link>http://arxiv.org/abs/2102.07448v3</link><description>Surround View fisheye cameras are commonly deployed in automated driving for360\deg{} near-field sensing around the vehicle. This work presents amulti-task visual perception network on unrectified fisheye images to enablethe vehicle to sense its surrounding environment. It consists of six primarytasks necessary for an autonomous driving system: depth estimation, visualodometry, semantic segmentation, motion segmentation, object detection, andlens soiling detection. We demonstrate that the jointly trained model performsbetter than the respective single task versions. Our multi-task model has ashared encoder providing a significant computational advantage and hassynergized decoders where tasks support each other. We propose a novel camerageometry based adaptation mechanism to encode the fisheye distortion model bothat training and inference. This was crucial to enable training on the WoodScapedataset, comprised of data from different parts of the world collected by 12different cameras mounted on three different cars with different intrinsics andviewpoints. Given that bounding boxes is not a good representation fordistorted fisheye images, we also extend object detection to use a polygon withnon-uniformly sampled vertices. We additionally evaluate our model on standardautomotive datasets, namely KITTI and Cityscapes. We obtain thestate-of-the-art results on KITTI for depth estimation and pose estimationtasks and competitive performance on the other tasks. We perform extensiveablation studies on various architecture choices and task weightingmethodologies. A short video at https://youtu.be/xbSjZ5OfPes providesqualitative results.</description><author>Varun Ravi Kumar, Senthil Yogamani, Hazem Rashed, Ganesh Sistu, Christian Witt, Isabelle Leang, Stefan Milz, Patrick Mäder</author><pubDate>Tue, 06 Jun 2023 15:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.07448v3</guid></item><item><title>Streaming Object Detection on Fisheye Cameras for Automatic Parking</title><link>http://arxiv.org/abs/2305.14713v1</link><description>Fisheye cameras are widely employed in automatic parking, and the videostream object detection (VSOD) of the fisheye camera is a fundamentalperception function to ensure the safe operation of vehicles. In past researchwork, the difference between the output of the deep learning model and theactual situation at the current moment due to the existence of delay of theperception system is generally ignored. But the environment will inevitablychange within the delay time which may cause a potential safety hazard. In thispaper, we propose a real-time detection framework equipped with a dual-flowperception module (dynamic and static flows) that can predict the future andalleviate the time-lag problem. Meanwhile, we use a new scheme to evaluatelatency and accuracy. The standard bounding box is unsuitable for the object infisheye camera images due to the strong radial distortion of the fisheye cameraand the primary detection objects of parking perception are vehicles andpedestrians, so we adopt the rotate bounding box and propose a new periodicangle loss function to regress the angle of the box, which is the simple andaccurate representation method of objects. The instance segmentation groundtruth is used to supervise the training. Experiments demonstrate theeffectiveness of our approach. Code is released at:https://gitee.com/hiyanyx/fisheye-streaming-perception.</description><author>Yixiong Yan, Liangzhu Cheng, Yongxu Li, Xinjuan Tuo</author><pubDate>Wed, 24 May 2023 05:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14713v1</guid></item></channel></rss>