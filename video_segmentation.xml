<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo segmentation</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 08 May 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Track Anything: Segment Anything Meets Videos</title><link>http://arxiv.org/abs/2304.11968v2</link><description>Recently, the Segment Anything Model (SAM) gains lots of attention rapidlydue to its impressive segmentation performance on images. Regarding its strongability on image segmentation and high interactivity with different prompts, wefound that it performs poorly on consistent segmentation in videos. Therefore,in this report, we propose Track Anything Model (TAM), which achieveshigh-performance interactive tracking and segmentation in videos. To bedetailed, given a video sequence, only with very little human participation,i.e., several clicks, people can track anything they are interested in, and getsatisfactory results in one-pass inference. Without additional training, suchan interactive design performs impressively on video object tracking andsegmentation. All resources are available on{https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitaterelated research.</description><author>Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng</author><pubDate>Fri, 28 Apr 2023 04:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11968v2</guid></item><item><title>Boosting Weakly-Supervised Temporal Action Localization with Text Information</title><link>http://arxiv.org/abs/2305.00607v1</link><description>Due to the lack of temporal annotation, current Weakly-supervised TemporalAction Localization (WTAL) methods are generally stuck into over-complete orincomplete localization. In this paper, we aim to leverage the text informationto boost WTAL from two aspects, i.e., (a) the discriminative objective toenlarge the inter-class difference, thus reducing the over-complete; (b) thegenerative objective to enhance the intra-class integrity, thus finding morecomplete temporal boundaries. For the discriminative objective, we propose aText-Segment Mining (TSM) mechanism, which constructs a text description basedon the action class label, and regards the text as the query to mine allclass-related segments. Without the temporal annotation of actions, TSMcompares the text query with the entire videos across the dataset to mine thebest matching segments while ignoring irrelevant ones. Due to the sharedsub-actions in different categories of videos, merely applying TSM is toostrict to neglect the semantic-related segments, which results in incompletelocalization. We further introduce a generative objective named Video-textLanguage Completion (VLC), which focuses on all semantic-related segments fromvideos to complete the text sentence. We achieve the state-of-the-artperformance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find ourproposed method can be seamlessly applied to existing methods, and improvetheir performances with a clear margin. The code is available athttps://github.com/lgzlIlIlI/Boosting-WTAL.</description><author>Guozhang Li, De Cheng, Xinpeng Ding, Nannan Wang, Xiaoyu Wang, Xinbo Gao</author><pubDate>Mon, 01 May 2023 01:07:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00607v1</guid></item><item><title>Tracking through Containers and Occluders in the Wild</title><link>http://arxiv.org/abs/2305.03052v1</link><description>Tracking objects with persistence in cluttered and dynamic environmentsremains a difficult challenge for computer vision systems. In this paper, weintroduce $\textbf{TCOW}$, a new benchmark and model for visual trackingthrough heavy occlusion and containment. We set up a task where the goal is to,given a video sequence, segment both the projected extent of the target object,as well as the surrounding container or occluder whenever one exists. To studythis task, we create a mixture of synthetic and annotated real datasets tosupport both supervised learning and structured evaluation of model performanceunder various forms of task variation, such as moving or nested containment. Weevaluate two recent transformer-based video models and find that while they canbe surprisingly capable of tracking targets under certain settings of taskvariation, there remains a considerable performance gap before we can claim atracking model to have acquired a true notion of object permanence.</description><author>Basile Van Hoorick, Pavel Tokmakov, Simon Stent, Jie Li, Carl Vondrick</author><pubDate>Thu, 04 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03052v1</guid></item><item><title>PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?</title><link>http://arxiv.org/abs/2202.05821v3</link><description>This paper presents the design and results of the "PEg TRAnsfert Workflowrecognition" (PETRAW) challenge whose objective was to develop surgicalworkflow recognition methods based on one or several modalities, among video,kinematic, and segmentation data, in order to study their added value. ThePETRAW challenge provided a data set of 150 peg transfer sequences performed ona virtual simulator. This data set was composed of videos, kinematics, semanticsegmentation, and workflow annotations which described the sequences at threedifferent granularity levels: phase, step, and activity. Five tasks wereproposed to the participants: three of them were related to the recognition ofall granularities with one of the available modalities, while the othersaddressed the recognition with a combination of modalities. Averageapplication-dependent balanced accuracy (AD-Accuracy) was used as evaluationmetric to take unbalanced classes into account and because it is moreclinically relevant than a frame-by-frame score. Seven teams participated in atleast one task and four of them in all tasks. Best results are obtained withthe use of the video and the kinematics data with an AD-Accuracy between 93%and 90% for the four teams who participated in all tasks. The improvementbetween video/kinematic-based methods and the uni-modality ones was significantfor all of the teams. However, the difference in testing execution time betweenthe video/kinematic-based and the kinematic-based methods has to be taken intoconsideration. Is it relevant to spend 20 to 200 times more computing time forless than 3% of improvement? The PETRAW data set is publicly available atwww.synapse.org/PETRAW to encourage further research in surgical workflowrecognition.</description><author>Arnaud Huaulm√©, Kanako Harada, Quang-Minh Nguyen, Bogyu Park, Seungbum Hong, Min-Kook Choi, Michael Peven, Yunshuang Li, Yonghao Long, Qi Dou, Satyadwyoom Kumar, Seenivasan Lalithkumar, Ren Hongliang, Hiroki Matsuzaki, Yuto Ishikawa, Yuriko Harai, Satoshi Kondo, Mamoru Mitsuishi, Pierre Jannin</author><pubDate>Thu, 27 Apr 2023 14:27:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.05821v3</guid></item><item><title>Weakly-supervised Micro- and Macro-expression Spotting Based on Multi-level Consistency</title><link>http://arxiv.org/abs/2305.02734v1</link><description>Most micro- and macro-expression spotting methods in untrimmed videos sufferfrom the burden of video-wise collection and frame-wise annotation.Weakly-supervised expression spotting (WES) based on video-level labels canpotentially mitigate the complexity of frame-level annotation while achievingfine-grained frame-level spotting. However, we argue that existingweakly-supervised methods are based on multiple instance learning (MIL)involving inter-modality, inter-sample, and inter-task gaps. The inter-samplegap is primarily from the sample distribution and duration. Therefore, wepropose a novel and simple WES framework, MC-WES, using multi-consistencycollaborative mechanisms that include modal-level saliency, video-leveldistribution, label-level duration and segment-level feature consistencystrategies to implement fine frame-level spotting with only video-level labelsto alleviate the above gaps and merge prior knowledge. The modal-level saliencyconsistency strategy focuses on capturing key correlations between raw imagesand optical flow. The video-level distribution consistency strategy utilizesthe difference of sparsity in temporal distribution. The label-level durationconsistency strategy exploits the difference in the duration of facial muscles.The segment-level feature consistency strategy emphasizes that features underthe same labels maintain similarity. Experimental results on two challengingdatasets -- CAS(ME)$^2$ and SAMM-LV -- demonstrate that MC-WES is comparable tostate-of-the-art fully-supervised methods.</description><author>Wang-Wang Yu, Kai-Fu Yang, Hong-Mei Yan, Yong-Jie Li</author><pubDate>Thu, 04 May 2023 12:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02734v1</guid></item><item><title>Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos: A review of the state of the art</title><link>http://arxiv.org/abs/2304.13014v1</link><description>In the field of computer- and robot-assisted minimally invasive surgery,enormous progress has been made in recent years based on the recognition ofsurgical instruments in endoscopic images. Especially the determination of theposition and type of the instruments is of great interest here. Current workinvolves both spatial and temporal information with the idea, that theprediction of movement of surgical tools over time may improve the quality offinal segmentations. The provision of publicly available datasets has recentlyencouraged the development of new methods, mainly based on deep learning. Inthis review, we identify datasets used for method development and evaluation,as well as quantify their frequency of use in the literature. We furtherpresent an overview of the current state of research regarding the segmentationand tracking of minimally invasive surgical instruments in endoscopic images.The paper focuses on methods that work purely visually without attached markersof any kind on the instruments, taking into account both single-framesegmentation approaches as well as those involving temporal information. Adiscussion of the reviewed literature is provided, highlighting existingshortcomings and emphasizing available potential for future developments. Thepublications considered were identified through the platforms Google Scholar,Web of Science, and PubMed. The search terms used were "instrumentsegmentation", "instrument tracking", "surgical tool segmentation", and"surgical tool tracking" and result in 408 articles published between 2015 and2022 from which 109 were included using systematic selection criteria.</description><author>Tobias Rueckert, Daniel Rueckert, Christoph Palm</author><pubDate>Tue, 25 Apr 2023 18:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13014v1</guid></item><item><title>EasyPortrait -- Face Parsing and Portrait Segmentation Dataset</title><link>http://arxiv.org/abs/2304.13509v2</link><description>Recently, due to COVID-19 and the growing demand for remote work, videoconferencing apps have become especially widespread. The most valuable featuresof video chats are real-time background removal and face beautification. Whilesolving these tasks, computer vision researchers face the problem of havingrelevant data for the training stage. There is no large dataset withhigh-quality labeled and diverse images of people in front of a laptop orsmartphone camera to train a lightweight model without additional approaches.To boost the progress in this area, we provide a new image dataset,EasyPortrait, for portrait segmentation and face parsing tasks. It contains20,000 primarily indoor photos of 8,377 unique users, and fine-grainedsegmentation masks separated into 9 classes. Images are collected and labeledfrom crowdsourcing platforms. Unlike most face parsing datasets, inEasyPortrait, the beard is not considered part of the skin mask, and the insidearea of the mouth is separated from the teeth. These features allow usingEasyPortrait for skin enhancement and teeth whitening tasks. This paperdescribes the pipeline for creating a large-scale and clean image segmentationdataset using crowdsourcing platforms without additional synthetic data.Moreover, we trained several models on EasyPortrait and showed experimentalresults. Proposed dataset and trained models are publicly available.</description><author>Alexander Kapitanov, Karina Kvanchiani, Sofia Kirillova</author><pubDate>Tue, 02 May 2023 06:32:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13509v2</guid></item><item><title>Phagocytosis Unveiled: A Scalable and Interpretable Deep learning Framework for Neurodegenerative Disease Analysis</title><link>http://arxiv.org/abs/2304.13764v1</link><description>Quantifying the phagocytosis of dynamic, unstained cells is essential forevaluating neurodegenerative diseases. However, measuring rapid cellinteractions and distinguishing cells from backgrounds make this taskchallenging when processing time-lapse phase-contrast video microscopy. In thisstudy, we introduce a fully automated, scalable, and versatile realtimeframework for quantifying and analyzing phagocytic activity. Our proposedpipeline can process large data-sets and includes a data quality verificationmodule to counteract potential perturbations such as microscope movements andframe blurring. We also propose an explainable cell segmentation module toimprove the interpretability of deep learning methods compared to black-boxalgorithms. This includes two interpretable deep learning capabilities: visualexplanation and model simplification. We demonstrate that interpretability indeep learning is not the opposite of high performance, but rather providesessential deep learning algorithm optimization insights and solutions.Incorporating interpretable modules results in an efficient architecture designand optimized execution time. We apply this pipeline to quantify and analyzemicroglial cell phagocytosis in frontotemporal dementia (FTD) and obtainstatistically reliable results showing that FTD mutant cells are larger andmore aggressive than control cells. To stimulate translational approaches andfuture research, we release an open-source pipeline and a unique microglialcells phagocytosis dataset for immune system characterization inneurodegenerative diseases research. This pipeline and dataset willconsistently crystallize future advances in this field, promoting thedevelopment of efficient and effective interpretable algorithms dedicated tothis critical domain. https://github.com/ounissimehdi/PhagoStat</description><author>Mehdi Ounissi, Morwena Latouche, Daniel Racoceanu</author><pubDate>Wed, 26 Apr 2023 19:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13764v1</guid></item></channel></rss>