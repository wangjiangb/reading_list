<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo segmentation</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 12 May 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>TarViS: A Unified Approach for Target-based Video Segmentation</title><link>http://arxiv.org/abs/2301.02657v2</link><description>The general domain of video segmentation is currently fragmented intodifferent tasks spanning multiple benchmarks. Despite rapid progress in thestate-of-the-art, current methods are overwhelmingly task-specific and cannotconceptually generalize to other tasks. Inspired by recent approaches withmulti-task capability, we propose TarViS: a novel, unified network architecturethat can be applied to any task that requires segmenting a set of arbitrarilydefined 'targets' in video. Our approach is flexible with respect to how tasksdefine these targets, since it models the latter as abstract 'queries' whichare then used to predict pixel-precise target masks. A single TarViS model canbe trained jointly on a collection of datasets spanning different tasks, andcan hot-swap between tasks during inference without any task-specificretraining. To demonstrate its effectiveness, we apply TarViS to four differenttasks, namely Video Instance Segmentation (VIS), Video Panoptic Segmentation(VPS), Video Object Segmentation (VOS) and Point Exemplar-guided Tracking(PET). Our unified, jointly trained model achieves state-of-the-art performanceon 5/7 benchmarks spanning these four tasks, and competitive performance on theremaining two. Code and model weights are available at:https://github.com/Ali2500/TarViS</description><author>Ali Athar, Alexander Hermans, Jonathon Luiten, Deva Ramanan, Bastian Leibe</author><pubDate>Wed, 10 May 2023 17:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.02657v2</guid></item><item><title>Track Anything: Segment Anything Meets Videos</title><link>http://arxiv.org/abs/2304.11968v2</link><description>Recently, the Segment Anything Model (SAM) gains lots of attention rapidlydue to its impressive segmentation performance on images. Regarding its strongability on image segmentation and high interactivity with different prompts, wefound that it performs poorly on consistent segmentation in videos. Therefore,in this report, we propose Track Anything Model (TAM), which achieveshigh-performance interactive tracking and segmentation in videos. To bedetailed, given a video sequence, only with very little human participation,i.e., several clicks, people can track anything they are interested in, and getsatisfactory results in one-pass inference. Without additional training, suchan interactive design performs impressively on video object tracking andsegmentation. All resources are available on{https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitaterelated research.</description><author>Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng</author><pubDate>Fri, 28 Apr 2023 04:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11968v2</guid></item><item><title>Video Object Segmentation in Panoptic Wild Scenes</title><link>http://arxiv.org/abs/2305.04470v1</link><description>In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.</description><author>Yuanyou Xu, Zongxin Yang, Yi Yang</author><pubDate>Mon, 08 May 2023 06:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04470v1</guid></item><item><title>Transform-Equivariant Consistency Learning for Temporal Sentence Grounding</title><link>http://arxiv.org/abs/2305.04123v1</link><description>This paper addresses the temporal sentence grounding (TSG). Although existingmethods have made decent achievements in this task, they not only severely relyon abundant video-query paired data for training, but also easily fail into thedataset distribution bias. To alleviate these limitations, we introduce a novelEquivariant Consistency Regulation Learning (ECRL) framework to learn morediscriminative query-related frame-wise representations for each video, in aself-supervised manner. Our motivation comes from that the temporal boundary ofthe query-guided activity should be consistently predicted under variousvideo-level transformations. Concretely, we first design a series ofspatio-temporal augmentations on both foreground and background video segmentsto generate a set of synthetic video samples. In particular, we devise aself-refine module to enhance the completeness and smoothness of the augmentedvideo. Then, we present a novel self-supervised consistency loss (SSCL) appliedon the original and augmented videos to capture their invariant query-relatedsemantic by minimizing the KL-divergence between the sequence similarity of twovideos and a prior Gaussian distribution of timestamp distance. At last, ashared grounding head is introduced to predict the transform-equivariantquery-guided segment boundaries for both the original and augmented videos.Extensive experiments on three challenging datasets (ActivityNet, TACoS, andCharades-STA) demonstrate both effectiveness and efficiency of our proposedECRL framework.</description><author>Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Zichuan Xu, Haozhao Wang, Xing Di, Weining Lu, Yu Cheng</author><pubDate>Sat, 06 May 2023 20:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04123v1</guid></item><item><title>Multi-object Video Generation from Single Frame Layouts</title><link>http://arxiv.org/abs/2305.03983v1</link><description>In this paper, we study video synthesis with emphasis on simplifying thegeneration conditions. Most existing video synthesis models or datasets aredesigned to address complex motions of a single object, lacking the ability ofcomprehensively understanding the spatio-temporal relationships among multipleobjects. Besides, current methods are usually conditioned on intricateannotations (e.g. video segmentations) to generate new videos, beingfundamentally less practical. These motivate us to generate multi-object videosconditioning exclusively on object layouts from a single frame. To solve abovechallenges and inspired by recent research on image generation from layouts, wehave proposed a novel video generative framework capable of synthesizing globalscenes with local objects, via implicit neural representations and layoutmotion self-inference. Our framework is a non-trivial adaptation from imagegeneration methods, and is new to this field. In addition, our model has beenevaluated on two widely-used video recognition benchmarks, demonstratingeffectiveness compared to the baseline model.</description><author>Yang Wu, Zhibin Liu, Hefeng Wu, Liang Lin</author><pubDate>Sat, 06 May 2023 10:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03983v1</guid></item><item><title>Autonomous Stabilization of Retinal Videos for Streamlining Assessment of Spontaneous Venous Pulsations</title><link>http://arxiv.org/abs/2305.06043v1</link><description>Spontaneous retinal Venous Pulsations (SVP) are rhythmic changes in thecaliber of the central retinal vein and are observed in the optic disc region(ODR) of the retina. Its absence is a critical indicator of various ocular orneurological abnormalities. Recent advances in imaging technology have enabledthe development of portable smartphone-based devices for observing the retinaand assessment of SVPs. However, the quality of smartphone-based retinal videosis often poor due to noise and image jitting, which in return, can severelyobstruct the observation of SVPs. In this work, we developed a fully automatedretinal video stabilization method that enables the examination of SVPscaptured by various mobile devices. Specifically, we first propose an ODRSpatio-Temporal Localization (ODR-STL) module to localize visible ODR andremove noisy and jittering frames. Then, we introduce a Noise-Aware TemplateMatching (NATM) module to stabilize high-quality video segments at a fixedposition in the field of view. After the processing, the SVPs can be easilyobserved in the stabilized videos, significantly facilitating userobservations. Furthermore, our method is cost-effective and has been tested inboth subjective and objective evaluations. Both of the evaluations support itseffectiveness in facilitating the observation of SVPs. This can improve thetimely diagnosis and treatment of associated diseases, making it a valuabletool for eye health professionals.</description><author>Hongwei Sheng, Xin Yu, Feiyu Wang, MD Wahiduzzaman Khan, Hexuan Weng, Sahar Shariflou, S. Mojtaba Golzan</author><pubDate>Wed, 10 May 2023 11:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06043v1</guid></item><item><title>Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization</title><link>http://arxiv.org/abs/2204.06228v2</link><description>Due to its high societal impact, deepfake detection is getting activeattention in the computer vision community. Most deepfake detection methodsrely on identity, facial attributes, and adversarial perturbation-basedspatio-temporal modifications at the whole video or random locations whilekeeping the meaning of the content intact. However, a sophisticated deepfakemay contain only a small segment of video/audio manipulation, through which themeaning of the content can be, for example, completely inverted from asentiment perspective. We introduce a content-driven audio-visual deepfakedataset, termed Localized Audio Visual DeepFake (LAV-DF), explicitly designedfor the task of learning temporal forgery localization. Specifically, thecontent-driven audio-visual manipulations are performed strategically to changethe sentiment polarity of the whole video. Our baseline method for benchmarkingthe proposed dataset is a 3DCNN model, termed as Boundary Aware TemporalForgery Detection (BA-TFD), which is guided via contrastive, boundary matching,and frame classification loss functions. Our extensive quantitative andqualitative analysis demonstrates the proposed method's strong performance fortemporal forgery localization and deepfake detection tasks.</description><author>Zhixi Cai, Kalin Stefanov, Abhinav Dhall, Munawar Hayat</author><pubDate>Thu, 04 May 2023 01:41:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.06228v2</guid></item><item><title>Boosting Weakly-Supervised Temporal Action Localization with Text Information</title><link>http://arxiv.org/abs/2305.00607v1</link><description>Due to the lack of temporal annotation, current Weakly-supervised TemporalAction Localization (WTAL) methods are generally stuck into over-complete orincomplete localization. In this paper, we aim to leverage the text informationto boost WTAL from two aspects, i.e., (a) the discriminative objective toenlarge the inter-class difference, thus reducing the over-complete; (b) thegenerative objective to enhance the intra-class integrity, thus finding morecomplete temporal boundaries. For the discriminative objective, we propose aText-Segment Mining (TSM) mechanism, which constructs a text description basedon the action class label, and regards the text as the query to mine allclass-related segments. Without the temporal annotation of actions, TSMcompares the text query with the entire videos across the dataset to mine thebest matching segments while ignoring irrelevant ones. Due to the sharedsub-actions in different categories of videos, merely applying TSM is toostrict to neglect the semantic-related segments, which results in incompletelocalization. We further introduce a generative objective named Video-textLanguage Completion (VLC), which focuses on all semantic-related segments fromvideos to complete the text sentence. We achieve the state-of-the-artperformance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find ourproposed method can be seamlessly applied to existing methods, and improvetheir performances with a clear margin. The code is available athttps://github.com/lgzlIlIlI/Boosting-WTAL.</description><author>Guozhang Li, De Cheng, Xinpeng Ding, Nannan Wang, Xiaoyu Wang, Xinbo Gao</author><pubDate>Mon, 01 May 2023 01:07:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00607v1</guid></item><item><title>Tracking through Containers and Occluders in the Wild</title><link>http://arxiv.org/abs/2305.03052v1</link><description>Tracking objects with persistence in cluttered and dynamic environmentsremains a difficult challenge for computer vision systems. In this paper, weintroduce $\textbf{TCOW}$, a new benchmark and model for visual trackingthrough heavy occlusion and containment. We set up a task where the goal is to,given a video sequence, segment both the projected extent of the target object,as well as the surrounding container or occluder whenever one exists. To studythis task, we create a mixture of synthetic and annotated real datasets tosupport both supervised learning and structured evaluation of model performanceunder various forms of task variation, such as moving or nested containment. Weevaluate two recent transformer-based video models and find that while they canbe surprisingly capable of tracking targets under certain settings of taskvariation, there remains a considerable performance gap before we can claim atracking model to have acquired a true notion of object permanence.</description><author>Basile Van Hoorick, Pavel Tokmakov, Simon Stent, Jie Li, Carl Vondrick</author><pubDate>Thu, 04 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03052v1</guid></item><item><title>PEg TRAnsfer Workflow recognition challenge report: Does multi-modal data improve recognition?</title><link>http://arxiv.org/abs/2202.05821v3</link><description>This paper presents the design and results of the "PEg TRAnsfert Workflowrecognition" (PETRAW) challenge whose objective was to develop surgicalworkflow recognition methods based on one or several modalities, among video,kinematic, and segmentation data, in order to study their added value. ThePETRAW challenge provided a data set of 150 peg transfer sequences performed ona virtual simulator. This data set was composed of videos, kinematics, semanticsegmentation, and workflow annotations which described the sequences at threedifferent granularity levels: phase, step, and activity. Five tasks wereproposed to the participants: three of them were related to the recognition ofall granularities with one of the available modalities, while the othersaddressed the recognition with a combination of modalities. Averageapplication-dependent balanced accuracy (AD-Accuracy) was used as evaluationmetric to take unbalanced classes into account and because it is moreclinically relevant than a frame-by-frame score. Seven teams participated in atleast one task and four of them in all tasks. Best results are obtained withthe use of the video and the kinematics data with an AD-Accuracy between 93%and 90% for the four teams who participated in all tasks. The improvementbetween video/kinematic-based methods and the uni-modality ones was significantfor all of the teams. However, the difference in testing execution time betweenthe video/kinematic-based and the kinematic-based methods has to be taken intoconsideration. Is it relevant to spend 20 to 200 times more computing time forless than 3% of improvement? The PETRAW data set is publicly available atwww.synapse.org/PETRAW to encourage further research in surgical workflowrecognition.</description><author>Arnaud Huaulmé, Kanako Harada, Quang-Minh Nguyen, Bogyu Park, Seungbum Hong, Min-Kook Choi, Michael Peven, Yunshuang Li, Yonghao Long, Qi Dou, Satyadwyoom Kumar, Seenivasan Lalithkumar, Ren Hongliang, Hiroki Matsuzaki, Yuto Ishikawa, Yuriko Harai, Satoshi Kondo, Mamoru Mitsuishi, Pierre Jannin</author><pubDate>Thu, 27 Apr 2023 14:27:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.05821v3</guid></item><item><title>Weakly-supervised Micro- and Macro-expression Spotting Based on Multi-level Consistency</title><link>http://arxiv.org/abs/2305.02734v1</link><description>Most micro- and macro-expression spotting methods in untrimmed videos sufferfrom the burden of video-wise collection and frame-wise annotation.Weakly-supervised expression spotting (WES) based on video-level labels canpotentially mitigate the complexity of frame-level annotation while achievingfine-grained frame-level spotting. However, we argue that existingweakly-supervised methods are based on multiple instance learning (MIL)involving inter-modality, inter-sample, and inter-task gaps. The inter-samplegap is primarily from the sample distribution and duration. Therefore, wepropose a novel and simple WES framework, MC-WES, using multi-consistencycollaborative mechanisms that include modal-level saliency, video-leveldistribution, label-level duration and segment-level feature consistencystrategies to implement fine frame-level spotting with only video-level labelsto alleviate the above gaps and merge prior knowledge. The modal-level saliencyconsistency strategy focuses on capturing key correlations between raw imagesand optical flow. The video-level distribution consistency strategy utilizesthe difference of sparsity in temporal distribution. The label-level durationconsistency strategy exploits the difference in the duration of facial muscles.The segment-level feature consistency strategy emphasizes that features underthe same labels maintain similarity. Experimental results on two challengingdatasets -- CAS(ME)$^2$ and SAMM-LV -- demonstrate that MC-WES is comparable tostate-of-the-art fully-supervised methods.</description><author>Wang-Wang Yu, Kai-Fu Yang, Hong-Mei Yan, Yong-Jie Li</author><pubDate>Thu, 04 May 2023 12:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02734v1</guid></item><item><title>Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos: A review of the state of the art</title><link>http://arxiv.org/abs/2304.13014v1</link><description>In the field of computer- and robot-assisted minimally invasive surgery,enormous progress has been made in recent years based on the recognition ofsurgical instruments in endoscopic images. Especially the determination of theposition and type of the instruments is of great interest here. Current workinvolves both spatial and temporal information with the idea, that theprediction of movement of surgical tools over time may improve the quality offinal segmentations. The provision of publicly available datasets has recentlyencouraged the development of new methods, mainly based on deep learning. Inthis review, we identify datasets used for method development and evaluation,as well as quantify their frequency of use in the literature. We furtherpresent an overview of the current state of research regarding the segmentationand tracking of minimally invasive surgical instruments in endoscopic images.The paper focuses on methods that work purely visually without attached markersof any kind on the instruments, taking into account both single-framesegmentation approaches as well as those involving temporal information. Adiscussion of the reviewed literature is provided, highlighting existingshortcomings and emphasizing available potential for future developments. Thepublications considered were identified through the platforms Google Scholar,Web of Science, and PubMed. The search terms used were "instrumentsegmentation", "instrument tracking", "surgical tool segmentation", and"surgical tool tracking" and result in 408 articles published between 2015 and2022 from which 109 were included using systematic selection criteria.</description><author>Tobias Rueckert, Daniel Rueckert, Christoph Palm</author><pubDate>Tue, 25 Apr 2023 18:38:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13014v1</guid></item><item><title>EasyPortrait -- Face Parsing and Portrait Segmentation Dataset</title><link>http://arxiv.org/abs/2304.13509v2</link><description>Recently, due to COVID-19 and the growing demand for remote work, videoconferencing apps have become especially widespread. The most valuable featuresof video chats are real-time background removal and face beautification. Whilesolving these tasks, computer vision researchers face the problem of havingrelevant data for the training stage. There is no large dataset withhigh-quality labeled and diverse images of people in front of a laptop orsmartphone camera to train a lightweight model without additional approaches.To boost the progress in this area, we provide a new image dataset,EasyPortrait, for portrait segmentation and face parsing tasks. It contains20,000 primarily indoor photos of 8,377 unique users, and fine-grainedsegmentation masks separated into 9 classes. Images are collected and labeledfrom crowdsourcing platforms. Unlike most face parsing datasets, inEasyPortrait, the beard is not considered part of the skin mask, and the insidearea of the mouth is separated from the teeth. These features allow usingEasyPortrait for skin enhancement and teeth whitening tasks. This paperdescribes the pipeline for creating a large-scale and clean image segmentationdataset using crowdsourcing platforms without additional synthetic data.Moreover, we trained several models on EasyPortrait and showed experimentalresults. Proposed dataset and trained models are publicly available.</description><author>Alexander Kapitanov, Karina Kvanchiani, Sofia Kirillova</author><pubDate>Tue, 02 May 2023 06:32:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13509v2</guid></item><item><title>Visual Causal Scene Refinement for Video Question Answering</title><link>http://arxiv.org/abs/2305.04224v1</link><description>Existing methods for video question answering (VideoQA) often suffer fromspurious correlations between different modalities, leading to a failure inidentifying the dominant visual evidence and the intended question. Moreover,these methods function as black boxes, making it difficult to interpret thevisual scene during the QA process. In this paper, to discover critical videosegments and frames that serve as the visual causal scene for generatingreliable answers, we present a causal analysis of VideoQA and propose aframework for cross-modal causal relational reasoning, named Visual CausalScene Refinement (VCSR). Particularly, a set of causal front-door interventionoperations is introduced to explicitly find the visual causal scenes at bothsegment and frame levels. Our VCSR involves two essential modules: i) theQuestion-Guided Refiner (QGR) module, which refines consecutive video framesguided by the question semantics to obtain more representative segment featuresfor causal front-door intervention; ii) the Causal Scene Separator (CSS)module, which discovers a collection of visual causal and non-causal scenesbased on the visual-linguistic causal relevance and estimates the causal effectof the scene-separating intervention in a contrastive learning manner.Extensive experiments on the NExT-QA, Causal-VidQA, and MSRVTT-QA datasetsdemonstrate the superiority of our VCSR in discovering visual causal scene andachieving robust video question answering.</description><author>Yushen Wei, Yang Liu, Hong Yan, Guanbin Li, Liang Lin</author><pubDate>Sun, 07 May 2023 10:05:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04224v1</guid></item><item><title>Self-supervised dense representation learning for live-cell microscopy with time arrow prediction</title><link>http://arxiv.org/abs/2305.05511v1</link><description>State-of-the-art object detection and segmentation methods for microscopyimages rely on supervised machine learning, which requires laborious manualannotation of training data. Here we present a self-supervised method based ontime arrow prediction pre-training that learns dense image representations fromraw, unlabeled live-cell microscopy videos. Our method builds upon the task ofpredicting the correct order of time-flipped image regions via a single-imagefeature extractor and a subsequent time arrow prediction head. We show that theresulting dense representations capture inherently time-asymmetric biologicalprocesses such as cell divisions on a pixel-level. We furthermore demonstratethe utility of these representations on several live-cell microscopy datasetsfor detection and segmentation of dividing cells, as well as for cell stateclassification. Our method outperforms supervised methods, particularly whenonly limited ground truth annotations are available as is commonly the case inpractice. We provide code at https://github.com/weigertlab/tarrow.</description><author>Benjamin Gallusser, Max Stieber, Martin Weigert</author><pubDate>Tue, 09 May 2023 15:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05511v1</guid></item><item><title>Phagocytosis Unveiled: A Scalable and Interpretable Deep learning Framework for Neurodegenerative Disease Analysis</title><link>http://arxiv.org/abs/2304.13764v1</link><description>Quantifying the phagocytosis of dynamic, unstained cells is essential forevaluating neurodegenerative diseases. However, measuring rapid cellinteractions and distinguishing cells from backgrounds make this taskchallenging when processing time-lapse phase-contrast video microscopy. In thisstudy, we introduce a fully automated, scalable, and versatile realtimeframework for quantifying and analyzing phagocytic activity. Our proposedpipeline can process large data-sets and includes a data quality verificationmodule to counteract potential perturbations such as microscope movements andframe blurring. We also propose an explainable cell segmentation module toimprove the interpretability of deep learning methods compared to black-boxalgorithms. This includes two interpretable deep learning capabilities: visualexplanation and model simplification. We demonstrate that interpretability indeep learning is not the opposite of high performance, but rather providesessential deep learning algorithm optimization insights and solutions.Incorporating interpretable modules results in an efficient architecture designand optimized execution time. We apply this pipeline to quantify and analyzemicroglial cell phagocytosis in frontotemporal dementia (FTD) and obtainstatistically reliable results showing that FTD mutant cells are larger andmore aggressive than control cells. To stimulate translational approaches andfuture research, we release an open-source pipeline and a unique microglialcells phagocytosis dataset for immune system characterization inneurodegenerative diseases research. This pipeline and dataset willconsistently crystallize future advances in this field, promoting thedevelopment of efficient and effective interpretable algorithms dedicated tothis critical domain. https://github.com/ounissimehdi/PhagoStat</description><author>Mehdi Ounissi, Morwena Latouche, Daniel Racoceanu</author><pubDate>Wed, 26 Apr 2023 19:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13764v1</guid></item><item><title>All Keypoints You Need: Detecting Arbitrary Keypoints on the Body of Triple, High, and Long Jump Athletes</title><link>http://arxiv.org/abs/2304.02939v2</link><description>Performance analyses based on videos are commonly used by coaches of athletesin various sports disciplines. In individual sports, these analyses mainlycomprise the body posture. This paper focuses on the disciplines of triple,high, and long jump, which require fine-grained locations of the athlete'sbody. Typical human pose estimation datasets provide only a very limited set ofkeypoints, which is not sufficient in this case. Therefore, we propose a methodto detect arbitrary keypoints on the whole body of the athlete by leveragingthe limited set of annotated keypoints and auto-generated segmentation masks ofbody parts. Evaluations show that our model is capable of detecting keypointson the head, torso, hands, feet, arms, and legs, including also bent elbows andknees. We analyze and compare different techniques to encode desired keypointsas the model's input and their embedding for the Transformer backbone.</description><author>Katja Ludwig, Julian Lorenz, Robin Schön, Rainer Lienhart</author><pubDate>Wed, 10 May 2023 09:15:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.02939v2</guid></item></channel></rss>