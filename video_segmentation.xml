<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo segmentation</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 22 Aug 2023 14:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Tube-Link: A Flexible Cross Tube Framework for Universal Video Segmentation</title><link>http://arxiv.org/abs/2303.12782v3</link><description>Video segmentation aims to segment and track every pixel in diverse scenariosaccurately. In this paper, we present Tube-Link, a versatile framework thataddresses multiple core tasks of video segmentation with a unifiedarchitecture. Our framework is a near-online approach that takes a shortsubclip as input and outputs the corresponding spatial-temporal tube masks. Toenhance the modeling of cross-tube relationships, we propose an effective wayto perform tube-level linking via attention along the queries. In addition, weintroduce temporal contrastive learning to instance-wise discriminativefeatures for tube-level association. Our approach offers flexibility andefficiency for both short and long video inputs, as the length of each subclipcan be varied according to the needs of datasets or scenarios. Tube-Linkoutperforms existing specialized architectures by a significant margin on fivevideo segmentation datasets. Specifically, it achieves almost 13% relativeimprovements on VIPSeg and 4% improvements on KITTI-STEP over the strongbaseline Video K-Net. When using a ResNet50 backbone on Youtube-VIS-2019 and2021, Tube-Link boosts IDOL by 3% and 4%, respectively.</description><author>Xiangtai Li, Haobo Yuan, Wenwei Zhang, Guangliang Cheng, Jiangmiao Pang, Chen Change Loy</author><pubDate>Mon, 21 Aug 2023 13:46:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12782v3</guid></item><item><title>Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation</title><link>http://arxiv.org/abs/2307.07812v1</link><description>Few-shot video segmentation is the task of delineating a specific novel classin a query video using few labelled support images. Typical approaches comparesupport and query features while limiting comparisons to a single feature layerand thereby ignore potentially valuable information. We present a meta-learnedMultiscale Memory Comparator (MMC) for few-shot video segmentation thatcombines information across scales within a transformer decoder. Typicalmultiscale transformer decoders for segmentation tasks learn a compressedrepresentation, their queries, through information exchange across scales.Unlike previous work, we instead preserve the detailed feature maps duringacross scale information exchange via a multiscale memory transformer decodingto reduce confusion between the background and novel class. Integral to theapproach, we investigate multiple forms of information exchange across scalesin different tasks and provide insights with empirical evidence on which to usein each task. The overall comparisons among query and support features benefitfrom both rich semantics and precise localization. We demonstrate our approachprimarily on few-shot video object segmentation and an adapted version on thefully supervised counterpart. In all cases, our approach outperforms thebaseline and yields state-of-the-art performance. Our code is publiclyavailable at https://github.com/MSiam/MMC-MultiscaleMemory.</description><author>Mennatullah Siam, Rezaul Karim, He Zhao, Richard Wildes</author><pubDate>Sat, 15 Jul 2023 15:21:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07812v1</guid></item><item><title>MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions</title><link>http://arxiv.org/abs/2308.08544v1</link><description>This paper strives for motion expressions guided video segmentation, whichfocuses on segmenting objects in video content based on a sentence describingthe motion of the objects. Existing referring video object datasets typicallyfocus on salient objects and use language expressions that contain excessivestatic attributes that could potentially enable the target object to beidentified in a single frame. These datasets downplay the importance of motionin video content for language-guided video object segmentation. To investigatethe feasibility of using motion expressions to ground and segment objects invideos, we propose a large-scale dataset called MeViS, which contains numerousmotion expressions to indicate target objects in complex environments. Webenchmarked 5 existing referring video object segmentation (RVOS) methods andconducted a comprehensive comparison on the MeViS dataset. The results showthat current RVOS methods cannot effectively address motion expression-guidedvideo segmentation. We further analyze the challenges and propose a baselineapproach for the proposed MeViS dataset. The goal of our benchmark is toprovide a platform that enables the development of effective language-guidedvideo segmentation algorithms that leverage motion expressions as a primary cuefor object segmentation in complex video scenes. The proposed MeViS dataset hasbeen released at https://henghuiding.github.io/MeViS.</description><author>Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Chen Change Loy</author><pubDate>Wed, 16 Aug 2023 18:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08544v1</guid></item><item><title>3rd Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.06753v1</link><description>In order to deal with the task of video panoptic segmentation in the wild, wepropose a robust integrated video panoptic segmentation solution. In oursolution, we regard the video panoptic segmentation task as a segmentationtarget querying task, represent both semantic and instance targets as a set ofqueries, and then combine these queries with video features extracted by neuralnetworks to predict segmentation masks. In order to improve the learningaccuracy and convergence speed of the solution, we add additional tasks ofvideo semantic segmentation and video instance segmentation for joint training.In addition, we also add an additional image semantic segmentation model tofurther improve the performance of semantic classes. In addition, we also addsome additional operations to improve the robustness of the model. Extensiveexperiments on the VIPSeg dataset show that the proposed solution achievesstate-of-the-art performance with 50.04\% VPQ on the VIPSeg test set, which is3rd place on the video panoptic segmentation track of the PVUW Challenge 2023.</description><author>Jinming Su, Wangwang Yang, Junfeng Luo, Xiaolin Wei</author><pubDate>Sun, 11 Jun 2023 20:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06753v1</guid></item><item><title>Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal Feature Affinity Learning for Robust Video Segmentation</title><link>http://arxiv.org/abs/2307.05898v1</link><description>Noisy label problems are inevitably in existence within medical imagesegmentation causing severe performance degradation. Previous segmentationmethods for noisy label problems only utilize a single image while thepotential of leveraging the correlation between images has been overlooked.Especially for video segmentation, adjacent frames contain rich contextualinformation beneficial in cognizing noisy labels. Based on two insights, wepropose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework toresolve noisy-labeled medical video segmentation issues. First, we argue thesequential prior of videos is an effective reference, i.e., pixel-levelfeatures from adjacent frames are close in distance for the same class and farin distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) isdevised to indicate possible noisy labels by evaluating the affinity betweenpixels in two adjacent frames. We also notice that the noise distributionexhibits considerable variations across video, image, and pixel levels. In thisway, we introduce Multi-Scale Supervision (MSS) to supervise the network fromthree different perspectives by re-weighting and refining the samples. Thisdesign enables the network to concentrate on clean samples in a coarse-to-finemanner. Experiments with both synthetic and real-world label noise demonstratethat our method outperforms recent state-of-the-art robust segmentationapproaches. Code is available at https://github.com/BeileiCui/MS-TFAL.</description><author>Beilei Cui, Minqing Zhang, Mengya Xu, An Wang, Wu Yuan, Hongliang Ren</author><pubDate>Wed, 12 Jul 2023 05:10:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05898v1</guid></item><item><title>Towards Unbalanced Motion: Part-Decoupling Network for Video Portrait Segmentation</title><link>http://arxiv.org/abs/2307.16565v1</link><description>Video portrait segmentation (VPS), aiming at segmenting prominent foregroundportraits from video frames, has received much attention in recent years.However, simplicity of existing VPS datasets leads to a limitation on extensiveresearch of the task. In this work, we propose a new intricate large-scaleMulti-scene Video Portrait Segmentation dataset MVPS consisting of 101 videoclips in 7 scenario categories, in which 10,843 sampled frames are finelyannotated at pixel level. The dataset has diverse scenes and complicatedbackground environments, which is the most complex dataset in VPS to our bestknowledge. Through the observation of a large number of videos with portraitsduring dataset construction, we find that due to the joint structure of humanbody, motion of portraits is part-associated, which leads that different partsare relatively independent in motion. That is, motion of different parts of theportraits is unbalanced. Towards this unbalance, an intuitive and reasonableidea is that different motion states in portraits can be better exploited bydecoupling the portraits into parts. To achieve this, we propose aPart-Decoupling Network (PDNet) for video portrait segmentation. Specifically,an Inter-frame Part-Discriminated Attention (IPDA) module is proposed whichunsupervisely segments portrait into parts and utilizes different attentivenesson discriminative features specified to each different part. In this way,appropriate attention can be imposed to portrait parts with unbalanced motionto extract part-discriminated correlations, so that the portraits can besegmented more accurately. Experimental results demonstrate that our methodachieves leading performance with the comparison to state-of-the-art methods.</description><author>Tianshu Yu, Changqun Xia, Jia Li</author><pubDate>Mon, 31 Jul 2023 11:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16565v1</guid></item><item><title>Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation</title><link>http://arxiv.org/abs/2305.16318v1</link><description>Recently, video object segmentation (VOS) referred by multi-modal signals,e.g., language and audio, has evoked increasing attention in both industry andacademia. It is challenging for exploring the semantic alignment withinmodalities and the visual correspondence across frames. However, existingmethods adopt separate network architectures for different modalities, andneglect the inter-frame temporal interaction with references. In this paper, wepropose MUTR, a Multi-modal Unified Temporal transformer for Referring videoobject segmentation. With a unified framework for the first time, MUTR adopts aDETR-style transformer and is capable of segmenting video objects designated byeither text or audio reference. Specifically, we introduce two strategies tofully explore the temporal relations between videos and multi-modal signals.Firstly, for low-level temporal aggregation before the transformer, we enablethe multi-modal references to capture multi-scale visual cues from consecutivevideo frames. This effectively endows the text or audio signals with temporalknowledge and boosts the semantic alignment between modalities. Secondly, forhigh-level temporal interaction after the transformer, we conduct inter-framefeature communication for different object embeddings, contributing to betterobject-wise correspondence for tracking along the video. On Ref-YouTube-VOS andAVSBench datasets with respective text and audio references, MUTR achieves+4.2% and +4.2% J&amp;F improvements to state-of-the-art methods, demonstrating oursignificance for unified multi-modal VOS. Code is released athttps://github.com/OpenGVLab/MUTR.</description><author>Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Zhongjiang He, Peng Gao</author><pubDate>Thu, 25 May 2023 18:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16318v1</guid></item><item><title>XMem++: Production-level Video Segmentation From Few Annotated Frames</title><link>http://arxiv.org/abs/2307.15958v2</link><description>Despite advancements in user-guided video segmentation, extracting complexobjects consistently for highly complex scenes is still a labor-intensive task,especially for production. It is not uncommon that a majority of frames need tobe annotated. We introduce a novel semi-supervised video object segmentation(SSVOS) model, XMem++, that improves existing memory-based models, with apermanent memory module. Most existing methods focus on single frameannotations, while our approach can effectively handle multiple user-selectedframes with varying appearances of the same object or region. Our method canextract highly consistent results while keeping the required number of frameannotations low. We further introduce an iterative and attention-based framesuggestion mechanism, which computes the next best frame for annotation. Ourmethod is real-time and does not require retraining after each user input. Wealso introduce a new dataset, PUMaVOS, which covers new challenging use casesnot found in previous benchmarks. We demonstrate SOTA performance onchallenging (partial and multi-class) segmentation scenarios as well as longvideos, while ensuring significantly fewer frame annotations than any existingmethod. Project page: https://max810.github.io/xmem2-project-page/</description><author>Maksym Bekuzarov, Ariana Bermudez, Joon-Young Lee, Hao Li</author><pubDate>Tue, 15 Aug 2023 12:26:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15958v2</guid></item><item><title>Towards Open-Vocabulary Video Instance Segmentation</title><link>http://arxiv.org/abs/2304.01715v2</link><description>Video Instance Segmentation (VIS) aims at segmenting and categorizing objectsin videos from a closed set of training categories, lacking the generalizationability to handle novel categories in real-world videos. To address thislimitation, we make the following three contributions. First, we introduce thenovel task of Open-Vocabulary Video Instance Segmentation, which aims tosimultaneously segment, track, and classify objects in videos from open-setcategories, including novel categories unseen during training. Second, tobenchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video InstanceSegmentation dataset (LV-VIS), that contains well-annotated objects from 1,196diverse categories, significantly surpassing the category size of existingdatasets by more than one order of magnitude. Third, we propose an efficientMemory-Induced Transformer architecture, OV2Seg, to first achieveOpen-Vocabulary VIS in an end-to-end manner with near real-time inferencespeed. Extensive experiments on LV-VIS and four existing VIS datasetsdemonstrate the strong zero-shot generalization ability of OV2Seg on novelcategories. The dataset and code are released herehttps://github.com/haochenheheda/LVVIS.</description><author>Haochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang, XU Tang, Yao Hu, Weidi Xie, Efstratios Gavves</author><pubDate>Sun, 06 Aug 2023 21:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01715v2</guid></item><item><title>A spatio-temporal network for video semantic segmentation in surgical videos</title><link>http://arxiv.org/abs/2306.11052v1</link><description>Semantic segmentation in surgical videos has applications in intra-operativeguidance, post-operative analytics and surgical education. Segmentation modelsneed to provide accurate and consistent predictions since temporallyinconsistent identification of anatomical structures can impair usability andhinder patient safety. Video information can alleviate these challenges leadingto reliable models suitable for clinical use. We propose a novel architecturefor modelling temporal relationships in videos. The proposed model includes aspatio-temporal decoder to enable video semantic segmentation by improvingtemporal consistency across frames. The encoder processes individual frameswhilst the decoder processes a temporal batch of adjacent frames. The proposeddecoder can be used on top of any segmentation encoder to improve temporalconsistency. Model performance was evaluated on the CholecSeg8k dataset and aprivate dataset of robotic Partial Nephrectomy procedures. Segmentationperformance was improved when the temporal decoder was applied across bothdatasets. The proposed model also displayed improvements in temporalconsistency.</description><author>Maria Grammatikopoulou, Ricardo Sanchez-Matilla, Felix Bragman, David Owen, Lucy Culshaw, Karen Kerr, Danail Stoyanov, Imanol Luengo</author><pubDate>Mon, 19 Jun 2023 17:36:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11052v1</guid></item><item><title>1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.04091v1</link><description>Video panoptic segmentation is a challenging task that serves as thecornerstone of numerous downstream applications, including video editing andautonomous driving. We believe that the decoupling strategy proposed by DVISenables more effective utilization of temporal information for both "thing" and"stuff" objects. In this report, we successfully validated the effectiveness ofthe decoupling strategy in video panoptic segmentation. Finally, our methodachieved a VPQ score of 51.4 and 53.7 in the development and test phases,respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUWChallenge. The code is available at https://github.com/zhang-tao-whu/DVIS</description><author>Tao Zhang, Xingye Tian, Haoran Wei, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Wed, 07 Jun 2023 02:24:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04091v1</guid></item><item><title>1st Place Solution for PVUW Challenge 2023: Video Panoptic Segmentation</title><link>http://arxiv.org/abs/2306.04091v2</link><description>Video panoptic segmentation is a challenging task that serves as thecornerstone of numerous downstream applications, including video editing andautonomous driving. We believe that the decoupling strategy proposed by DVISenables more effective utilization of temporal information for both "thing" and"stuff" objects. In this report, we successfully validated the effectiveness ofthe decoupling strategy in video panoptic segmentation. Finally, our methodachieved a VPQ score of 51.4 and 53.7 in the development and test phases,respectively, and ultimately ranked 1st in the VPS track of the 2nd PVUWChallenge. The code is available at https://github.com/zhang-tao-whu/DVIS</description><author>Tao Zhang, Xingye Tian, Haoran Wei, Yu Wu, Shunping Ji, Xuebo Wang, Xin Tao, Yuan Zhang, Pengfei Wan</author><pubDate>Thu, 08 Jun 2023 09:19:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04091v2</guid></item><item><title>Tamed Warping Network for High-Resolution Semantic Video Segmentation</title><link>http://arxiv.org/abs/2005.01344v4</link><description>Recent approaches for fast semantic video segmentation have reducedredundancy by warping feature maps across adjacent frames, greatly speeding upthe inference phase. However, the accuracy drops seriously owing to the errorsincurred by warping. In this paper, we propose a novel framework and design asimple and effective correction stage after warping. Specifically, we build anon-key-frame CNN, fusing warped context features with current spatial details.Based on the feature fusion, our Context Feature Rectification~(CFR) modulelearns the model's difference from a per-frame model to correct the warpedfeatures. Furthermore, our Residual-Guided Attention~(RGA) module utilizes theresidual maps in the compressed domain to help CRF focus on error-proneregions. Results on Cityscapes show that the accuracy significantly increasesfrom $67.3\%$ to $71.6\%$, and the speed edges down from $65.5$ FPS to $61.8$FPS at a resolution of $1024\times 2048$. For non-rigid categories, e.g.,``human'' and ``object'', the improvements are even higher than 18 percentagepoints.</description><author>Songyuan Li, Junyi Feng, Xi Li</author><pubDate>Tue, 11 Jul 2023 09:54:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.01344v4</guid></item><item><title>UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model</title><link>http://arxiv.org/abs/2305.12659v1</link><description>Unsupervised video object segmentation has made significant progress inrecent years, but the manual annotation of video mask datasets is expensive andlimits the diversity of available datasets. The Segment Anything Model (SAM)has introduced a new prompt-driven paradigm for image segmentation, unlocking arange of previously unexplored capabilities. In this paper, we propose a novelparadigm called UVOSAM, which leverages SAM for unsupervised video objectsegmentation without requiring video mask labels. To address SAM's limitationsin instance discovery and identity association, we introduce a video salientobject tracking network that automatically generates trajectories for prominentforeground objects. These trajectories then serve as prompts for SAM to producevideo masks on a frame-by-frame basis. Our experimental results demonstratethat UVOSAM significantly outperforms current mask-supervised methods. Thesefindings suggest that UVOSAM has the potential to improve unsupervised videoobject segmentation and reduce the cost of manual annotation.</description><author>Zhenghao Zhang, Zhichao Wei, Shengfan Zhang, Zuozhuo Dai, Siyu Zhu</author><pubDate>Mon, 22 May 2023 04:03:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12659v1</guid></item><item><title>Temporal Transductive Inference for Few-Shot Video Object Segmentation</title><link>http://arxiv.org/abs/2203.14308v2</link><description>Few-shot video object segmentation (FS-VOS) aims at segmenting video framesusing a few labelled examples of classes not seen during initial training. Inthis paper, we present a simple but effective temporal transductive inference(TTI) approach that leverages temporal consistency in the unlabelled videoframes during few-shot inference. Key to our approach is the use of both globaland local temporal constraints. The objective of the global constraint is tolearn consistent linear classifiers for novel classes across the imagesequence, whereas the local constraint enforces the proportion offoreground/background regions in each frame to be coherent across a localtemporal window. These constraints act as spatiotemporal regularizers duringthe transductive inference to increase temporal coherence and reduceoverfitting on the few-shot support set. Empirically, our model outperformsstate-of-the-art meta-learning approaches in terms of mean intersection overunion on YouTube-VIS by 2.8%. In addition, we introduce improved benchmarksthat are exhaustively labelled (i.e. all object occurrences are labelled,unlike the currently available), and present a more realistic evaluationparadigm that targets data distribution shift between training and testingsets. Our empirical results and in-depth analysis confirm the added benefits ofthe proposed spatiotemporal regularizers to improve temporal coherence andovercome certain overfitting scenarios.</description><author>Mennatullah Siam, Konstantinos G. Derpanis, Richard P. Wildes</author><pubDate>Sun, 16 Jul 2023 14:31:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.14308v2</guid></item><item><title>Measuring Student Behavioral Engagement using Histogram of Actions</title><link>http://arxiv.org/abs/2307.09420v1</link><description>In this paper, we propose a novel technique for measuring behavioralengagement through students' actions recognition. The proposed approachrecognizes student actions then predicts the student behavioral engagementlevel. For student action recognition, we use human skeletons to model studentpostures and upper body movements. To learn the dynamics of student upper body,a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actionswithin every 2minute video segment then these actions are used to build ahistogram of actions which encodes the student actions and their frequencies.This histogram is utilized as an input to SVM classifier to classify whetherthe student is engaged or disengaged. To evaluate the proposed framework, webuild a dataset consisting of 1414 2-minute video segments annotated with 13actions and 112 video segments annotated with two engagement levels.Experimental results indicate that student actions can be recognized with top 1accuracy 83.63% and the proposed framework can capture the average engagementof the class.</description><author>Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</author><pubDate>Tue, 18 Jul 2023 17:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09420v1</guid></item><item><title>LVOS: A Benchmark for Long-term Video Object Segmentation</title><link>http://arxiv.org/abs/2211.10181v2</link><description>Existing video object segmentation (VOS) benchmarks focus on short-termvideos which just last about 3-5 seconds and where objects are visible most ofthe time. These videos are poorly representative of practical applications, andthe absence of long-term datasets restricts further investigation of VOS on theapplication in realistic scenarios. So, in this paper, we present a newbenchmark dataset named \textbf{LVOS}, which consists of 220 videos with atotal duration of 421 minutes. To the best of our knowledge, LVOS is the firstdensely annotated long-term VOS dataset. The videos in our LVOS last 1.59minutes on average, which is 20 times longer than videos in existing VOSdatasets. Each video includes various attributes, especially challengesderiving from the wild, such as long-term reappearing and cross-temporalsimilar objeccts.Based on LVOS, we assess existing video object segmentationalgorithms and propose a Diverse Dynamic Memory network (DDMemory) thatconsists of three complementary memory banks to exploit temporal informationadequately. The experimental results demonstrate the strength and weaknesses ofprior methods, pointing promising directions for further study. Data and codeare available at https://lingyihongfd.github.io/lvos.github.io/.</description><author>Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, Wenqiang Zhang</author><pubDate>Fri, 18 Aug 2023 13:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.10181v2</guid></item><item><title>EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2308.04162v1</link><description>Audio-guided Video Object Segmentation (A-VOS) and Referring Video ObjectSegmentation (R-VOS) are two highly-related tasks, which both aim to segmentspecific objects from video sequences according to user-provided expressionprompts. However, due to the challenges in modeling representations fordifferent modalities, contemporary methods struggle to strike a balance betweeninteraction flexibility and high-precision localization and segmentation. Inthis paper, we address this problem from two perspectives: the alignmentrepresentation of audio and text and the deep interaction among audio, text,and visual features. First, we propose a universal architecture, the ExpressionPrompt Collaboration Transformer, herein EPCFormer. Next, we propose anExpression Alignment (EA) mechanism for audio and text expressions. Byintroducing contrastive learning for audio and text expressions, the proposedEPCFormer realizes comprehension of the semantic equivalence between audio andtext expressions denoting the same objects. Then, to facilitate deepinteractions among audio, text, and video features, we introduce anExpression-Visual Attention (EVA) mechanism. The knowledge of video objectsegmentation in terms of the expression prompts can seamlessly transfer betweenthe two tasks by deeply exploring complementary cues between text and audio.Experiments on well-recognized benchmarks demonstrate that our universalEPCFormer attains state-of-the-art results on both tasks. The source code ofEPCFormer will be made publicly available athttps://github.com/lab206/EPCFormer.</description><author>Jiajun Chen, Jiacheng Lin, Zhiqiang Xiao, Haolong Fu, Ke Nai, Kailun Yang, Zhiyong Li</author><pubDate>Tue, 08 Aug 2023 10:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04162v1</guid></item><item><title>Scalable Video Object Segmentation with Identification Mechanism</title><link>http://arxiv.org/abs/2203.11442v6</link><description>This paper delves into the challenges of achieving scalable and effectivemulti-object modeling for semi-supervised Video Object Segmentation (VOS).Previous VOS methods decode features with a single positive object, limitingthe learning of multi-object representation as they must match and segment eachtarget separately under multi-object scenarios. Additionally, earliertechniques catered to specific application objectives and lacked theflexibility to fulfill different speed-accuracy requirements. To address theseproblems, we present two innovative approaches, Associating Objects withTransformers (AOT) and Associating Objects with Scalable Transformers (AOST).In pursuing effective multi-object modeling, AOT introduces the IDentification(ID) mechanism to allocate each object a unique identity. This approach enablesthe network to model the associations among all objects simultaneously, thusfacilitating the tracking and segmentation of objects in a single network pass.To address the challenge of inflexible deployment, AOST further integratesscalable long short-term transformers that incorporate layer-wise ID-basedattention and scalable supervision. This overcomes ID embeddings'representation limitations and enables online architecture scalability in VOSfor the first time. Given the absence of a benchmark for VOS involving denselymulti-object annotations, we propose a challenging Video Object Segmentation inthe Wild (VOSW) benchmark to validate our approaches. We evaluated various AOTand AOST variants using extensive experiments across VOSW and fivecommonly-used VOS benchmarks. Our approaches surpass the state-of-the-artcompetitors and display exceptional efficiency and scalability consistentlyacross all six benchmarks. Moreover, we notably achieved the 1st position inthe 3rd Large-scale Video Object Segmentation Challenge.</description><author>Zongxin Yang, Xiaohan Wang, Jiaxu Miao, Yunchao Wei, Wenguan Wang, Yi Yang</author><pubDate>Mon, 03 Jul 2023 05:58:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11442v6</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v2</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Thu, 08 Jun 2023 09:15:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v2</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v3</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Fri, 14 Jul 2023 09:46:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v3</guid></item><item><title>DVIS: Decoupled Video Instance Segmentation Framework</title><link>http://arxiv.org/abs/2306.03413v1</link><description>Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.</description><author>Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, Pengfei Wan</author><pubDate>Tue, 06 Jun 2023 06:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03413v1</guid></item><item><title>LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2306.08736v1</link><description>Referring video object segmentation (RVOS) aims to segment the targetinstance referred by a given text expression in a video clip. The textexpression normally contains sophisticated descriptions of the instance'sappearance, actions, and relations with others. It is therefore ratherdifficult for an RVOS model to capture all these attributes correspondingly inthe video; in fact, the model often favours more on the action- andrelation-related visual attribute of the instance. This can end up withincomplete or even incorrect mask prediction of the target instance. In thispaper, we tackle this problem by taking a subject-centric short text expressionfrom the original long text expression. The short one retains only theappearance-related information of the target instance so that we can use it tofocus the model's attention on the instance's appearance. We let the model makejoint predictions using both long and short text expressions and introduce along-short predictions intersection loss to align the joint predictions.Besides the improvement on the linguistic part, we also introduce aforward-backward visual consistency loss, which utilizes optical flows to warpvisual features between the annotated frames and their temporal neighbors forconsistency. We build our method on top of two state of the arttransformer-based pipelines for end-to-end training. Extensive experiments onA2D-Sentences and JHMDB-Sentences datasets show impressive improvements of ourmethod.</description><author>Linfeng Yuan, Miaojing Shi, Zijie Yue</author><pubDate>Wed, 14 Jun 2023 21:40:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08736v1</guid></item><item><title>Hierarchical Spatiotemporal Transformers for Video Object Segmentation</title><link>http://arxiv.org/abs/2307.08263v1</link><description>This paper presents a novel framework called HST for semi-supervised videoobject segmentation (VOS). HST extracts image and video features using thelatest Swin Transformer and Video Swin Transformer to inherit their inductivebias for the spatiotemporal locality, which is essential for temporallycoherent VOS. To take full advantage of the image and video features, HST castsimage and video features as a query and memory, respectively. By applyingefficient memory read operations at multiple scales, HST produces hierarchicalfeatures for the precise reconstruction of object masks. HST showseffectiveness and robustness in handling challenging scenarios with occludedand fast-moving objects under cluttered backgrounds. In particular, HST-Boutperforms the state-of-the-art competitors on multiple popular benchmarks,i.e., YouTube-VOS (85.0%), DAVIS 2017 (85.9%), and DAVIS 2016 (94.0%).</description><author>Jun-Sang Yoo, Hongjae Lee, Seung-Won Jung</author><pubDate>Mon, 17 Jul 2023 07:12:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.08263v1</guid></item><item><title>RefineVIS: Video Instance Segmentation with Temporal Attention Refinement</title><link>http://arxiv.org/abs/2306.04774v1</link><description>We introduce a novel framework called RefineVIS for Video InstanceSegmentation (VIS) that achieves good object association between frames andaccurate segmentation masks by iteratively refining the representations usingsequence context. RefineVIS learns two separate representations on top of anoff-the-shelf frame-level image instance segmentation model: an associationrepresentation responsible for associating objects across frames and asegmentation representation that produces accurate segmentation masks.Contrastive learning is utilized to learn temporally stable associationrepresentations. A Temporal Attention Refinement (TAR) module learnsdiscriminative segmentation representations by exploiting temporalrelationships and a novel temporal contrastive denoising technique. Our methodsupports both online and offline inference. It achieves state-of-the-art videoinstance segmentation accuracy on YouTube-VIS 2019 (64.4 AP), Youtube-VIS 2021(61.4 AP), and OVIS (46.1 AP) datasets. The visualization shows that the TARmodule can generate more accurate instance segmentation masks, particularly forchallenging cases such as highly occluded objects.</description><author>Andre Abrantes, Jiang Wang, Peng Chu, Quanzeng You, Zicheng Liu</author><pubDate>Wed, 07 Jun 2023 21:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04774v1</guid></item><item><title>BoxVIS: Video Instance Segmentation with Box Annotations</title><link>http://arxiv.org/abs/2303.14618v2</link><description>It is expensive and labour-extensive to label the pixel-wise object masks ina video. As a result, the amount of pixel-wise annotations in existing videoinstance segmentation (VIS) datasets is small, limiting the generalizationcapability of trained VIS models. An alternative but much cheaper solution isto use bounding boxes to label instances in videos. Inspired by the recentsuccess of box-supervised image instance segmentation, we adapt thestate-of-the-art pixel-supervised VIS models to a box-supervised VIS (BoxVIS)baseline, and observe slight performance degradation. We consequently proposeto improve the BoxVIS performance from two aspects. First, we propose abox-center guided spatial-temporal pairwise affinity (STPA) loss to predictinstance masks for better spatial and temporal consistency. Second, we collecta larger scale box-annotated VIS dataset (BVISD) by consolidating the videosfrom current VIS benchmarks and converting images from the COCO dataset toshort pseudo video clips. With the proposed BVISD and the STPA loss, ourtrained BoxVIS model achieves 43.2\% and 29.0\% mask AP on the YouTube-VIS 2021and OVIS valid sets, respectively. It exhibits comparable instance maskprediction performance and better generalization ability than state-of-the-artpixel-supervised VIS models by using only 16\% of their annotation time andcost. Codes and data can be found at \url{https://github.com/MinghanLi/BoxVIS}.</description><author>Minghan Li, Lei Zhang</author><pubDate>Wed, 12 Jul 2023 11:44:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14618v2</guid></item><item><title>OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2307.09356v1</link><description>Referring video object segmentation (RVOS) aims at segmenting an object in avideo following human instruction. Current state-of-the-art methods fall intoan offline pattern, in which each clip independently interacts with textembedding for cross-modal understanding. They usually present that the offlinepattern is necessary for RVOS, yet model limited temporal association withineach clip. In this work, we break up the previous offline belief and propose asimple yet effective online model using explicit query propagation, namedOnlineRefer. Specifically, our approach leverages target cues that gathersemantic information and position prior to improve the accuracy and ease ofreferring predictions for the current frame. Furthermore, we generalize ouronline model into a semi-online framework to be compatible with video-basedbackbones. To show the effectiveness of our method, we evaluate it on fourbenchmarks, \ie, Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, andJHMDB-Sentences. Without bells and whistles, our OnlineRefer with a Swin-Lbackbone achieves 63.5 J&amp;F and 64.8 J&amp;F on Refer-Youtube-VOS and Refer-DAVIS17,outperforming all other offline methods.</description><author>Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, Jianbing Shen</author><pubDate>Tue, 18 Jul 2023 16:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09356v1</guid></item><item><title>Spectrum-guided Multi-granularity Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2307.13537v1</link><description>Current referring video object segmentation (R-VOS) techniques extractconditional kernels from encoded (low-resolution) vision-language features tosegment the decoded high-resolution features. We discovered that this causessignificant feature drift, which the segmentation kernels struggle to perceiveduring the forward computation. This negatively affects the ability ofsegmentation kernels. To address the drift problem, we propose aSpectrum-guided Multi-granularity (SgMg) approach, which performs directsegmentation on the encoded features and employs visual details to furtheroptimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion(SCF) to perform intra-frame global interactions in the spectral domain foreffective multimodal representation. Finally, we extend SgMg to performmulti-object R-VOS, a new paradigm that enables simultaneous segmentation ofmultiple referred objects in a video. This not only makes R-VOS faster, butalso more practical. Extensive experiments show that SgMg achievesstate-of-the-art performance on four video benchmark datasets, outperformingthe nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMgenables multi-object R-VOS, runs about 3 times faster while maintainingsatisfactory performance. Code is available at https://github.com/bo-miao/SgMg.</description><author>Bo Miao, Mohammed Bennamoun, Yongsheng Gao, Ajmal Mian</author><pubDate>Tue, 25 Jul 2023 15:35:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13537v1</guid></item><item><title>Track Anything: Segment Anything Meets Videos</title><link>http://arxiv.org/abs/2304.11968v2</link><description>Recently, the Segment Anything Model (SAM) gains lots of attention rapidlydue to its impressive segmentation performance on images. Regarding its strongability on image segmentation and high interactivity with different prompts, wefound that it performs poorly on consistent segmentation in videos. Therefore,in this report, we propose Track Anything Model (TAM), which achieveshigh-performance interactive tracking and segmentation in videos. To bedetailed, given a video sequence, only with very little human participation,i.e., several clicks, people can track anything they are interested in, and getsatisfactory results in one-pass inference. Without additional training, suchan interactive design performs impressively on video object tracking andsegmentation. All resources are available on{https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitaterelated research.</description><author>Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng</author><pubDate>Fri, 28 Apr 2023 04:21:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11968v2</guid></item><item><title>Learning Referring Video Object Segmentation from Weak Annotation</title><link>http://arxiv.org/abs/2308.02162v1</link><description>Referring video object segmentation (RVOS) is a task that aims to segment thetarget object in all video frames based on a sentence describing the object.Previous RVOS methods have achieved significant performance withdensely-annotated datasets, whose construction is expensive and time-consuming.To relieve the burden of data annotation while maintaining sufficientsupervision for segmentation, we propose a new annotation scheme, in which welabel the frame where the object first appears with a mask and use boundingboxes for the subsequent frames. Based on this scheme, we propose a method tolearn from this weak annotation. Specifically, we design a cross framesegmentation method, which uses the language-guided dynamic filters tothoroughly leverage the valuable mask annotation and bounding boxes. We furtherdevelop a bi-level contrastive learning method to encourage the model to learndiscriminative representation at the pixel level. Extensive experiments andablative analyses show that our method is able to achieve competitiveperformance without the demand of dense mask annotation. The code will beavailable at https://github.com/wangbo-zhao/WRVOS/.</description><author>Wangbo Zhao, Kepan Nan, Songyang Zhang, Kai Chen, Dahua Lin, Yang You</author><pubDate>Fri, 04 Aug 2023 07:50:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02162v1</guid></item><item><title>Undercover Deepfakes: Detecting Fake Segments in Videos</title><link>http://arxiv.org/abs/2305.06564v2</link><description>The recent renaissance in generative models, driven primarily by the adventof diffusion models and iterative improvement in GAN methods, has enabled manycreative applications. However, each advancement is also accompanied by a risein the potential for misuse. In the arena of deepfake generation this is a keysocietal issue. In particular, the ability to modify segments of videos usingsuch generative techniques creates a new paradigm of deepfakes which are mostlyreal videos altered slightly to distort the truth. Current deepfake detectionmethods in the academic literature are not evaluated on this paradigm. In thispaper, we present a deepfake detection method able to address this issue byperforming both frame and video level deepfake prediction. To facilitatetesting our method we create a new benchmark dataset where videos have bothreal and fake frame sequences. Our method utilizes the Vision Transformer,Scaling and Shifting pretraining and Timeseries Transformer to temporallysegment videos to help facilitate the interpretation of possible deepfakes.Extensive experiments on a variety of deepfake generation methods showexcellent results on temporal segmentation and classical video levelpredictions as well. In particular, the paradigm we introduce will form apowerful tool for the moderation of deepfakes, where human oversight can bebetter targeted to the parts of videos suspected of being deepfakes. Allexperiments can be reproduced at:https://github.com/sanjaysaha1311/temporal-deepfake-segmentation.</description><author>Sanjay Saha, Rashindrie Perera, Sachith Seneviratne, Tamasha Malepathirana, Sanka Rasnayaka, Deshani Geethika, Terence Sim, Saman Halgamuge</author><pubDate>Tue, 16 May 2023 16:16:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06564v2</guid></item><item><title>Towards Robust Referring Video Object Segmentation with Cyclic Relational Consensus</title><link>http://arxiv.org/abs/2207.01203v2</link><description>Referring Video Object Segmentation (R-VOS) is a challenging task that aimsto segment an object in a video based on a linguistic expression. Most existingR-VOS methods have a critical assumption: the object referred to must appear inthe video. This assumption, which we refer to as semantic consensus, is oftenviolated in real-world scenarios, where the expression may be queried againstfalse videos. In this work, we highlight the need for a robust R-VOS model thatcan handle semantic mismatches. Accordingly, we propose an extended task calledRobust R-VOS, which accepts unpaired video-text inputs. We tackle this problemby jointly modeling the primary R-VOS problem and its dual (textreconstruction). A structural text-to-text cycle constraint is introduced todiscriminate semantic consensus between video-text pairs and impose it inpositive pairs, thereby achieving multi-modal alignment from both positive andnegative pairs. Our structural constraint effectively addresses the challengeposed by linguistic diversity, overcoming the limitations of previous methodsthat relied on the point-wise constraint. A new evaluation dataset,R\textsuperscript{2}-Youtube-VOSis constructed to measure the model robustness.Our model achieves state-of-the-art performance on R-VOS benchmarks,Ref-DAVIS17 and Ref-Youtube-VOS, and also ourR\textsuperscript{2}-Youtube-VOS~dataset.</description><author>Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Bhiksha Raj, Yan Lu</author><pubDate>Mon, 31 Jul 2023 19:30:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.01203v2</guid></item><item><title>GRAtt-VIS: Gated Residual Attention for Auto Rectifying Video Instance Segmentation</title><link>http://arxiv.org/abs/2305.17096v1</link><description>Recent trends in Video Instance Segmentation (VIS) have seen a growingreliance on online methods to model complex and lengthy video sequences.However, the degradation of representation and noise accumulation of the onlinemethods, especially during occlusion and abrupt changes, pose substantialchallenges. Transformer-based query propagation provides promising directionsat the cost of quadratic memory attention. However, they are susceptible to thedegradation of instance features due to the above-mentioned challenges andsuffer from cascading effects. The detection and rectification of such errorsremain largely underexplored. To this end, we introduce \textbf{GRAtt-VIS},\textbf{G}ated \textbf{R}esidual \textbf{Att}ention for \textbf{V}ideo\textbf{I}nstance \textbf{S}egmentation. Firstly, we leverage aGumbel-Softmax-based gate to detect possible errors in the current frame. Next,based on the gate activation, we rectify degraded features from its pastrepresentation. Such a residual configuration alleviates the need for dedicatedmemory and provides a continuous stream of relevant instance features.Secondly, we propose a novel inter-instance interaction using gate activationas a mask for self-attention. This masking strategy dynamically restricts theunrepresentative instance queries in the self-attention and preserves vitalinformation for long-term tracking. We refer to this novel combination of GatedResidual Connection and Masked Self-Attention as \textbf{GRAtt} block, whichcan easily be integrated into the existing propagation-based framework.Further, GRAtt blocks significantly reduce the attention overhead and simplifydynamic temporal modeling. GRAtt-VIS achieves state-of-the-art performance onYouTube-VIS and the highly challenging OVIS dataset, significantly improvingover previous methods. Code is available at\url{https://github.com/Tanveer81/GRAttVIS}.</description><author>Tanveer Hannan, Rajat Koner, Maximilian Bernhard, Suprosanna Shit, Bjoern Menze, Volker Tresp, Matthias Schubert, Thomas Seidl</author><pubDate>Fri, 26 May 2023 18:10:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17096v1</guid></item><item><title>FODVid: Flow-guided Object Discovery in Videos</title><link>http://arxiv.org/abs/2307.04392v1</link><description>Segmentation of objects in a video is challenging due to the nuances such asmotion blurring, parallax, occlusions, changes in illumination, etc. Instead ofaddressing these nuances separately, we focus on building a generalizablesolution that avoids overfitting to the individual intricacies. Such a solutionwould also help us save enormous resources involved in human annotation ofvideo corpora. To solve Video Object Segmentation (VOS) in an unsupervisedsetting, we propose a new pipeline (FODVid) based on the idea of guidingsegmentation outputs using flow-guided graph-cut and temporal consistency.Basically, we design a segmentation model incorporating intra-frame appearanceand flow similarities, and inter-frame temporal continuation of the objectsunder consideration. We perform an extensive experimental analysis of ourstraightforward methodology on the standard DAVIS16 video benchmark. Thoughsimple, our approach produces results comparable (within a range of ~2 mIoU) tothe existing top approaches in unsupervised VOS. The simplicity andeffectiveness of our technique opens up new avenues for research in the videodomain.</description><author>Silky Singh, Shripad Deshmukh, Mausoom Sarkar, Rishabh Jain, Mayur Hemani, Balaji Krishnamurthy</author><pubDate>Mon, 10 Jul 2023 08:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.04392v1</guid></item><item><title>SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation</title><link>http://arxiv.org/abs/2305.17011v1</link><description>This paper studies referring video object segmentation (RVOS) by boostingvideo-level visual-linguistic alignment. Recent approaches model the RVOS taskas a sequence prediction problem and perform multi-modal interaction as well assegmentation for each frame separately. However, the lack of a global view ofvideo content leads to difficulties in effectively utilizing inter-framerelationships and understanding textual descriptions of object temporalvariations. To address this issue, we propose Semantic-assisted Object Cluster(SOC), which aggregates video content and textual guidance for unified temporalmodeling and cross-modal alignment. By associating a group of frame-levelobject embeddings with language tokens, SOC facilitates joint space learningacross modalities and time steps. Moreover, we present multi-modal contrastivesupervision to help construct well-aligned joint space at the video level. Weconduct extensive experiments on popular RVOS benchmarks, and our methodoutperforms state-of-the-art competitors on all benchmarks by a remarkablemargin. Besides, the emphasis on temporal coherence enhances the segmentationstability and adaptability of our method in processing text expressions withtemporal variations. Code will be available.</description><author>Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang</author><pubDate>Fri, 26 May 2023 16:13:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17011v1</guid></item><item><title>Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation</title><link>http://arxiv.org/abs/2308.06693v1</link><description>Recent leading zero-shot video object segmentation (ZVOS) works devote tointegrating appearance and motion information by elaborately designing featurefusion modules and identically applying them in multiple feature stages. Ourpreliminary experiments show that with the strong long-range dependencymodeling capacity of Transformer, simply concatenating the two modalityfeatures and feeding them to vanilla Transformers for feature fusion candistinctly benefit the performance but at a cost of heavy computation. Throughfurther empirical analysis, we find that attention dependencies learned inTransformer in different stages exhibit completely different properties: globalquery-independent dependency in the low-level stages and semantic-specificdependency in the high-level stages. Motivated by the observations, we proposetwo Transformer variants: i) Context-Sharing Transformer (CST) that learns theglobal-shared contextual information within image frames with a lightweightcomputation. ii) Semantic Gathering-Scattering Transformer (SGST) that modelsthe semantic correlation separately for the foreground and background andreduces the computation cost with a soft token merging mechanism. We apply CSTand SGST for low-level and high-level feature fusions, respectively,formulating a level-isomerous Transformer framework for ZVOS task. Comparedwith the baseline that uses vanilla Transformers for multi-stage fusion, ourssignificantly increase the speed by 13 times and achieves new state-of-the-artZVOS performance. Code is available at https://github.com/DLUT-yyc/Isomer.</description><author>Yichen Yuan, Yifan Wang, Lijun Wang, Xiaoqi Zhao, Huchuan Lu, Yu Wang, Weibo Su, Lei Zhang</author><pubDate>Sun, 13 Aug 2023 07:12:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06693v1</guid></item><item><title>READMem: Robust Embedding Association for a Diverse Memory in Unconstrained Video Object Segmentation</title><link>http://arxiv.org/abs/2305.12823v1</link><description>We present READMem (Robust Embedding Association for a Diverse Memory), amodular framework for semi-automatic video object segmentation (sVOS) methodsdesigned to handle unconstrained videos. Contemporary sVOS works typicallyaggregate video frames in an ever-expanding memory, demanding high hardwareresources for long-term applications. To mitigate memory requirements andprevent near object duplicates (caused by information of adjacent frames),previous methods introduce a hyper-parameter that controls the frequency offrames eligible to be stored. This parameter has to be adjusted according toconcrete video properties (such as rapidity of appearance changes and videolength) and does not generalize well. Instead, we integrate the embedding of anew frame into the memory only if it increases the diversity of the memorycontent. Furthermore, we propose a robust association of the embeddings storedin the memory with query embeddings during the update process. Our approachavoids the accumulation of redundant data, allowing us in return, to restrictthe memory size and prevent extreme memory demands in long videos. We extendpopular sVOS baselines with READMem, which previously showed limitedperformance on long videos. Our approach achieves competitive results on theLong-time Video dataset (LV1) while not hindering performance on shortsequences. Our code is publicly available.</description><author>Stéphane Vujasinović, Sebastian Bullinger, Stefan Becker, Norbert Scherer-Negenborn, Michael Arens, Rainer Stiefelhagen</author><pubDate>Mon, 22 May 2023 09:31:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12823v1</guid></item><item><title>Online Unsupervised Video Object Segmentation via Contrastive Motion Clustering</title><link>http://arxiv.org/abs/2306.12048v1</link><description>Online unsupervised video object segmentation (UVOS) uses the previous framesas its input to automatically separate the primary object(s) from a streamingvideo without using any further manual annotation. A major challenge is thatthe model has no access to the future and must rely solely on the history,i.e., the segmentation mask is predicted from the current frame as soon as itis captured. In this work, a novel contrastive motion clustering algorithm withan optical flow as its input is proposed for the online UVOS by exploiting thecommon fate principle that visual elements tend to be perceived as a group ifthey possess the same motion pattern. We build a simple and effectiveauto-encoder to iteratively summarize non-learnable prototypical bases for themotion pattern, while the bases in turn help learn the representation of theembedding network. Further, a contrastive learning strategy based on a boundaryprior is developed to improve foreground and background feature discriminationin the representation learning stage. The proposed algorithm can be optimizedon arbitrarily-scale data i.e., frame, clip, dataset) and performed in anonline fashion. Experiments on $\textit{DAVIS}_{\textit{16}}$, $\textit{FBMS}$,and $\textit{SegTrackV2}$ datasets show that the accuracy of our methodsurpasses the previous state-of-the-art (SoTA) online UVOS method by a marginof 0.8%, 2.9%, and 1.1%, respectively. Furthermore, by using an online deepsubspace clustering to tackle the motion grouping, our method is able toachieve higher accuracy at $3\times$ faster inference time compared to SoTAonline UVOS method, and making a good trade-off between effectiveness andefficiency.</description><author>Lin Xi, Weihai Chen, Xingming Wu, Zhong Liu, Zhengguo Li</author><pubDate>Wed, 21 Jun 2023 07:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12048v1</guid></item><item><title>Segment Anything Meets Point Tracking</title><link>http://arxiv.org/abs/2307.01197v1</link><description>The Segment Anything Model (SAM) has established itself as a powerfulzero-shot image segmentation model, employing interactive prompts such aspoints to generate masks. This paper presents SAM-PT, a method extending SAM'scapability to tracking and segmenting anything in dynamic videos. SAM-PTleverages robust and sparse point selection and propagation techniques for maskgeneration, demonstrating that a SAM-based segmentation tracker can yieldstrong zero-shot performance across popular video object segmentationbenchmarks, including DAVIS, YouTube-VOS, and MOSE. Compared to traditionalobject-centric mask propagation strategies, we uniquely use point propagationto exploit local structure information that is agnostic to object semantics. Wehighlight the merits of point-based tracking through direct evaluation on thezero-shot open-world Unidentified Video Objects (UVO) benchmark. To furtherenhance our approach, we utilize K-Medoids clustering for point initializationand track both positive and negative points to clearly distinguish the targetobject. We also employ multiple mask decoding passes for mask refinement anddevise a point re-initialization strategy to improve tracking accuracy. Ourcode integrates different point trackers and video segmentation benchmarks andwill be released at https://github.com/SysCV/sam-pt.</description><author>Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu</author><pubDate>Mon, 03 Jul 2023 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01197v1</guid></item><item><title>A Similarity Alignment Model for Video Copy Segment Matching</title><link>http://arxiv.org/abs/2305.15679v1</link><description>With the development of multimedia technology, Video Copy Detection has beena crucial problem for social media platforms. Meta AI hold Video SimilarityChallenge on CVPR 2023 to push the technology forward. In this report, we shareour winner solutions on Matching Track. We propose a Similarity AlignmentModel(SAM) for video copy segment matching. Our SAM exhibits superiorperformance compared to other competitors, with a 0.108 / 0.144 absoluteimprovement over the second-place competitor in Phase 1 / Phase 2. Code isavailable athttps://github.com/FeipengMa6/VSC22-Submission/tree/main/VSC22-Matching-Track-1st.</description><author>Zhenhua Liu, Feipeng Ma, Tianyi Wang, Fengyun Rao</author><pubDate>Thu, 25 May 2023 04:08:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15679v1</guid></item><item><title>Video-Instrument Synergistic Network for Referring Video Instrument Segmentation in Robotic Surgery</title><link>http://arxiv.org/abs/2308.09475v1</link><description>Robot-assisted surgery has made significant progress, with instrumentsegmentation being a critical factor in surgical intervention quality. Itserves as the building block to facilitate surgical robot navigation andsurgical education for the next generation of operating intelligence. Althoughexisting methods have achieved accurate instrument segmentation results, theysimultaneously generate segmentation masks for all instruments, without thecapability to specify a target object and allow an interactive experience. Thiswork explores a new task of Referring Surgical Video Instrument Segmentation(RSVIS), which aims to automatically identify and segment the correspondingsurgical instruments based on the given language expression. To achieve this,we devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn bothvideo-level and instrument-level knowledge to boost performance, while previouswork only used video-level information. Meanwhile, we design a Graph-basedRelation-aware Module (GRM) to model the correlation between multi-modalinformation (i.e., textual description and video frame) to facilitate theextraction of instrument-level information. We are also the first to producetwo RSVIS datasets to promote related research. Our method is verified on thesedatasets, and experimental results exhibit that the VIS-Net can significantlyoutperform existing state-of-the-art referring segmentation methods. Our codeand our datasets will be released upon the publication of this work.</description><author>Hongqiu Wang, Lei Zhu, Guang Yang, Yike Guo, Shichen Zhang, Bo Xu, Yueming Jin</author><pubDate>Fri, 18 Aug 2023 12:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09475v1</guid></item><item><title>Video Object Segmentation in Panoptic Wild Scenes</title><link>http://arxiv.org/abs/2305.04470v2</link><description>In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.</description><author>Yuanyou Xu, Zongxin Yang, Yi Yang</author><pubDate>Sun, 25 Jun 2023 07:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04470v2</guid></item><item><title>Video Object Segmentation in Panoptic Wild Scenes</title><link>http://arxiv.org/abs/2305.04470v1</link><description>In this paper, we introduce semi-supervised video object segmentation (VOS)to panoptic wild scenes and present a large-scale benchmark as well as abaseline method for it. Previous benchmarks for VOS with sparse annotations arenot sufficient to train or evaluate a model that needs to process all possibleobjects in real-world scenarios. Our new benchmark (VIPOSeg) containsexhaustive object annotations and covers various real-world object categorieswhich are carefully divided into subsets of thing/stuff and seen/unseen classesfor comprehensive evaluation. Considering the challenges in panoptic VOS, wepropose a strong baseline method named panoptic object association withtransformers (PAOT), which uses panoptic identification to associate objectswith a pyramid architecture on multiple scales. Experimental results show thatVIPOSeg can not only boost the performance of VOS models by panoptic trainingbut also evaluate them comprehensively in panoptic scenes. Previous methods forclassic VOS still need to improve in performance and efficiency when dealingwith panoptic scenes, while our PAOT achieves SOTA performance with goodefficiency on VIPOSeg and previous VOS benchmarks. PAOT also ranks 1st in theVOT2022 challenge. Our dataset is available athttps://github.com/yoxu515/VIPOSeg-Benchmark.</description><author>Yuanyou Xu, Zongxin Yang, Yi Yang</author><pubDate>Mon, 08 May 2023 06:46:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04470v1</guid></item><item><title>CTVIS: Consistent Training for Online Video Instance Segmentation</title><link>http://arxiv.org/abs/2307.12616v1</link><description>The discrimination of instance embeddings plays a vital role in associatinginstances across time for online video instance segmentation (VIS). Instanceembedding learning is directly supervised by the contrastive loss computed uponthe contrastive items (CIs), which are sets of anchor/positive/negativeembeddings. Recent online VIS methods leverage CIs sourced from one referenceframe only, which we argue is insufficient for learning highly discriminativeembeddings. Intuitively, a possible strategy to enhance CIs is replicating theinference phase during training. To this end, we propose a simple yet effectivetraining strategy, called Consistent Training for Online VIS (CTVIS), whichdevotes to aligning the training and inference pipelines in terms of buildingCIs. Specifically, CTVIS constructs CIs by referring inference themomentum-averaged embedding and the memory bank storage mechanisms, and addingnoise to the relevant embeddings. Such an extension allows a reliablecomparison between embeddings of current instances and the stablerepresentations of historical instances, thereby conferring an advantage inmodeling VIS challenges such as occlusion, re-identification, and deformation.Empirically, CTVIS outstrips the SOTA VIS models by up to +5.0 points on threeVIS benchmarks, including YTVIS19 (55.1% AP), YTVIS21 (50.1% AP) and OVIS(35.5% AP). Furthermore, we find that pseudo-videos transformed from images cantrain robust models surpassing fully-supervised ones.</description><author>Kaining Ying, Qing Zhong, Weian Mao, Zhenhua Wang, Hao Chen, Lin Yuanbo Wu, Yifan Liu, Chengxiang Fan, Yunzhi Zhuge, Chunhua Shen</author><pubDate>Mon, 24 Jul 2023 09:44:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12616v1</guid></item><item><title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation</title><link>http://arxiv.org/abs/2307.02010v2</link><description>The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objectsegmentation. In this study, we introduce MSDeAOT, a variant of the AOT seriesthat incorporates transformers at multiple feature scales. Leveraging thehierarchical Gated Propagation Module (GPM), MSDeAOT efficiently propagatesobject masks from previous frames to the current frame using a feature scalewith a stride of 16. Additionally, we employ GPM in a more refined featurescale with a stride of 8, leading to improved accuracy in detecting andtracking small objects. Through the implementation of test-time augmentationsand model ensemble techniques, we achieve the top-ranking position in theEPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge.</description><author>Jiahao Li, Yuanyou Xu, Zongxin Yang, Yi Yang, Yueting Zhuang</author><pubDate>Mon, 10 Jul 2023 10:20:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02010v2</guid></item><item><title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation</title><link>http://arxiv.org/abs/2307.02010v1</link><description>The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objectsegmentation. In this study, we introduce MSDeAOT, a variant of the AOT seriesthat incorporates transformers at multiple feature scales. Leveraging thehierarchical Gated Propagation Module (GPM), MSDeAOT efficiently propagatesobject masks from previous frames to the current frame using a feature scalewith a stride of 16. Additionally, we employ GPM in a more refined featurescale with a stride of 8, leading to improved accuracy in detecting andtracking small objects. Through the implementation of test-time augmentationsand model ensemble techniques, we achieve the top-ranking position in theEPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge.</description><author>Jiahao Li, Yuanyou Xu, Zongxin Yang, Yi Yang, Yueting Zhuang</author><pubDate>Wed, 05 Jul 2023 04:43:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02010v1</guid></item><item><title>Self-Supervised Video Representation Learning via Latent Time Navigation</title><link>http://arxiv.org/abs/2305.06437v1</link><description>Self-supervised video representation learning aimed at maximizing similaritybetween different temporal segments of one video, in order to enforce featurepersistence over time. This leads to loss of pertinent information related totemporal relationships, rendering actions such as `enter' and `leave' to beindistinguishable. To mitigate this limitation, we propose Latent TimeNavigation (LTN), a time-parameterized contrastive learning strategy that isstreamlined to capture fine-grained motions. Specifically, we maximize therepresentation similarity between different video segments from one video,while maintaining their representations time-aware along a subspace of thelatent representation code including an orthogonal basis to represent temporalchanges. Our extensive experimental analysis suggests that learning videorepresentations by LTN consistently improves performance of actionclassification in fine-grained and human-oriented tasks (e.g., on ToyotaSmarthome dataset). In addition, we demonstrate that our proposed model, whenpre-trained on Kinetics-400, generalizes well onto the unseen real world videobenchmark datasets UCF101 and HMDB51, achieving state-of-the-art performance inaction recognition.</description><author>Di Yang, Yaohui Wang, Quan Kong, Antitza Dantcheva, Lorenzo Garattoni, Gianpiero Francesca, Francois Bremond</author><pubDate>Wed, 10 May 2023 21:06:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06437v1</guid></item><item><title>Enhancing Transformer Backbone for Egocentric Video Action Segmentation</title><link>http://arxiv.org/abs/2305.11365v1</link><description>Egocentric temporal action segmentation in videos is a crucial task incomputer vision with applications in various fields such as mixed reality,human behavior analysis, and robotics. Although recent research has utilizedadvanced visual-language frameworks, transformers remain the backbone of actionsegmentation models. Therefore, it is necessary to improve transformers toenhance the robustness of action segmentation models. In this work, we proposetwo novel ideas to enhance the state-of-the-art transformer for actionsegmentation. First, we introduce a dual dilated attention mechanism toadaptively capture hierarchical representations in both local-to-global andglobal-to-local contexts. Second, we incorporate cross-connections between theencoder and decoder blocks to prevent the loss of local context by the decoder.Additionally, we utilize state-of-the-art visual-language representationlearning techniques to extract richer and more compact features for ourtransformer. Our proposed approach outperforms other state-of-the-art methodson the Georgia Tech Egocentric Activities (GTEA) and HOI4D Office Toolsdatasets, and we validate our introduced components with ablation studies. Thesource code and supplementary materials are publicly available onhttps://www.sail-nu.com/dxformer.</description><author>Sakib Reza, Balaji Sundareshan, Mohsen Moghaddam, Octavia Camps</author><pubDate>Fri, 19 May 2023 02:00:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11365v1</guid></item><item><title>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</title><link>http://arxiv.org/abs/2306.08640v2</link><description>Recent research on Large Language Models (LLMs) has led to remarkableadvancements in general NLP AI assistants. Some studies have further exploredthe use of LLMs for planning and invoking models or APIs to address moregeneral multi-modal user queries. Despite this progress, complex visual-basedtasks still remain challenging due to the diverse nature of visual tasks. Thisdiversity is reflected in two aspects: 1) Reasoning paths. For many real-lifeapplications, it is hard to accurately decompose a query simply by examiningthe query itself. Planning based on the specific visual content and the resultsof each step is usually required. 2) Flexible inputs and intermediate results.Input forms could be flexible for in-the-wild cases, and involves not only asingle image or video but a mixture of videos and images, e.g., a user-viewimage with some reference videos. Besides, a complex reasoning process willalso generate diverse multimodal intermediate results, e.g., video narrations,segmented video clips, etc. To address such general cases, we propose amulti-modal AI assistant, AssistGPT, with an interleaved code and languagereasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrateLLMs with various tools. Specifically, the Planner is capable of using naturallanguage to plan which tool in Executor should do next based on the currentreasoning progress. Inspector is an efficient memory manager to assist thePlanner to feed proper visual information into a specific tool. Finally, sincethe entire reasoning process is complex and flexible, a Learner is designed toenable the model to autonomously explore and discover the optimal solution. Weconducted experiments on A-OKVQA and NExT-QA benchmarks, achievingstate-of-the-art results. Moreover, showcases demonstrate the ability of oursystem to handle questions far more complex than those found in the benchmarks.</description><author>Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, Mike Zheng Shou</author><pubDate>Wed, 28 Jun 2023 06:00:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08640v2</guid></item><item><title>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</title><link>http://arxiv.org/abs/2306.08640v1</link><description>Recent research on Large Language Models (LLMs) has led to remarkableadvancements in general NLP AI assistants. Some studies have further exploredthe use of LLMs for planning and invoking models or APIs to address moregeneral multi-modal user queries. Despite this progress, complex visual-basedtasks still remain challenging due to the diverse nature of visual tasks. Thisdiversity is reflected in two aspects: 1) Reasoning paths. For many real-lifeapplications, it is hard to accurately decompose a query simply by examiningthe query itself. Planning based on the specific visual content and the resultsof each step is usually required. 2) Flexible inputs and intermediate results.Input forms could be flexible for in-the-wild cases, and involves not only asingle image or video but a mixture of videos and images, e.g., a user-viewimage with some reference videos. Besides, a complex reasoning process willalso generate diverse multimodal intermediate results, e.g., video narrations,segmented video clips, etc. To address such general cases, we propose amulti-modal AI assistant, AssistGPT, with an interleaved code and languagereasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrateLLMs with various tools. Specifically, the Planner is capable of using naturallanguage to plan which tool in Executor should do next based on the currentreasoning progress. Inspector is an efficient memory manager to assist thePlanner to feed proper visual information into a specific tool. Finally, sincethe entire reasoning process is complex and flexible, a Learner is designed toenable the model to autonomously explore and discover the optimal solution. Weconducted experiments on A-OKVQA and NExT-QA benchmarks, achievingstate-of-the-art results. Moreover, showcases demonstrate the ability of oursystem to handle questions far more complex than those found in the benchmarks.</description><author>Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, Mike Zheng Shou</author><pubDate>Wed, 14 Jun 2023 18:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08640v1</guid></item><item><title>MultiSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos</title><link>http://arxiv.org/abs/2306.04216v1</link><description>Multimodal summarization with multimodal output (MSMO) has emerged as apromising research direction. Nonetheless, numerous limitations exist withinexisting public MSMO datasets, including insufficient upkeep, datainaccessibility, limited size, and the absence of proper categorization, whichpose significant challenges to effective research. To address these challengesand provide a comprehensive dataset for this new direction, we havemeticulously curated the MultiSum dataset. Our new dataset features (1)Human-validated summaries for both video and textual content, providingsuperior human instruction and labels for multimodal learning. (2)Comprehensively and meticulously arranged categorization, spanning 17 principalcategories and 170 subcategories to encapsulate a diverse array of real-worldscenarios. (3) Benchmark tests performed on the proposed dataset to assessvaried tasks and methods, including video temporal segmentation, videosummarization, text summarization, and multimodal summarization. To championaccessibility and collaboration, we release the MultiSum dataset and the datacollection tool as fully open-source resources, fostering transparency andaccelerating future developments. Our project website can be found athttps://multisum-dataset.github.io/.</description><author>Jielin Qiu, Jiacheng Zhu, William Han, Aditesh Kumar, Karthik Mittal, Claire Jin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Bo Li, Ding Zhao, Lijuan Wang</author><pubDate>Wed, 07 Jun 2023 08:43:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04216v1</guid></item><item><title>Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation</title><link>http://arxiv.org/abs/2302.11325v2</link><description>This paper presents a deep learning framework for medical video segmentation.Convolution neural network (CNN) and transformer-based methods have achievedgreat milestones in medical image segmentation tasks due to their incrediblesemantic feature encoding and global information comprehension abilities.However, most existing approaches ignore a salient aspect of medical video data- the temporal dimension. Our proposed framework explicitly extracts featuresfrom neighbouring frames across the temporal dimension and incorporates themwith a temporal feature blender, which then tokenises the high-levelspatio-temporal feature to form a strong global feature encoded via a SwinTransformer. The final segmentation results are produced via a UNet-likeencoder-decoder architecture. Our model outperforms other approaches by asignificant margin and improves the segmentation benchmarks on the VFSS2022dataset, achieving a dice coefficient of 0.8986 and 0.8186 for the two datasetstested. Our studies also show the efficacy of the temporal feature blendingscheme and cross-dataset transferability of learned capabilities. Code andmodels are fully available at https://github.com/SimonZeng7108/Video-SwinUNet.</description><author>Chengxi Zeng, Xinyu Yang, David Smithard, Majid Mirmehdi, Alberto M Gambaruto, Tilo Burghardt</author><pubDate>Tue, 04 Jul 2023 16:51:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11325v2</guid></item><item><title>Detect Any Shadow: Segment Anything for Video Shadow Detection</title><link>http://arxiv.org/abs/2305.16698v1</link><description>Segment anything model (SAM) has achieved great success in the field ofnatural image segmentation. Nevertheless, SAM tends to classify shadows asbackground, resulting in poor segmentation performance for shadow detectiontask. In this paper, we propose an simple but effective approach for finetuning SAM to detect shadows. Additionally, we also combine it with longshort-term attention mechanism to extend its capabilities to video shadowdetection. Specifically, we first fine tune SAM by utilizing shadow datacombined with sparse prompts and apply the fine-tuned model to detect aspecific frame (e.g., first frame) in the video with a little user assistance.Subsequently, using the detected frame as a reference, we employ a longshort-term network to learn spatial correlations between distant frames andtemporal consistency between contiguous frames, thereby achieving shadowinformation propagation across frames. Extensive experimental resultsdemonstrate that our method outperforms the state-of-the-art techniques, withimprovements of 17.2% and 3.3% in terms of MAE and IoU, respectively,validating the effectiveness of our method.</description><author>Yonghui Wang, Wengang Zhou, Yunyao Mao, Houqiang Li</author><pubDate>Fri, 26 May 2023 08:39:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16698v1</guid></item><item><title>Transform-Equivariant Consistency Learning for Temporal Sentence Grounding</title><link>http://arxiv.org/abs/2305.04123v1</link><description>This paper addresses the temporal sentence grounding (TSG). Although existingmethods have made decent achievements in this task, they not only severely relyon abundant video-query paired data for training, but also easily fail into thedataset distribution bias. To alleviate these limitations, we introduce a novelEquivariant Consistency Regulation Learning (ECRL) framework to learn morediscriminative query-related frame-wise representations for each video, in aself-supervised manner. Our motivation comes from that the temporal boundary ofthe query-guided activity should be consistently predicted under variousvideo-level transformations. Concretely, we first design a series ofspatio-temporal augmentations on both foreground and background video segmentsto generate a set of synthetic video samples. In particular, we devise aself-refine module to enhance the completeness and smoothness of the augmentedvideo. Then, we present a novel self-supervised consistency loss (SSCL) appliedon the original and augmented videos to capture their invariant query-relatedsemantic by minimizing the KL-divergence between the sequence similarity of twovideos and a prior Gaussian distribution of timestamp distance. At last, ashared grounding head is introduced to predict the transform-equivariantquery-guided segment boundaries for both the original and augmented videos.Extensive experiments on three challenging datasets (ActivityNet, TACoS, andCharades-STA) demonstrate both effectiveness and efficiency of our proposedECRL framework.</description><author>Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Zichuan Xu, Haozhao Wang, Xing Di, Weining Lu, Yu Cheng</author><pubDate>Sat, 06 May 2023 20:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04123v1</guid></item><item><title>TrickVOS: A Bag of Tricks for Video Object Segmentation</title><link>http://arxiv.org/abs/2306.15377v1</link><description>Space-time memory (STM) network methods have been dominant in semi-supervisedvideo object segmentation (SVOS) due to their remarkable performance. In thiswork, we identify three key aspects where we can improve such methods; i)supervisory signal, ii) pretraining and iii) spatial awareness. We then proposeTrickVOS; a generic, method-agnostic bag of tricks addressing each aspect withi) a structure-aware hybrid loss, ii) a simple decoder pretraining regime andiii) a cheap tracker that imposes spatial constraints in model predictions.Finally, we propose a lightweight network and show that when trained withTrickVOS, it achieves competitive results to state-of-the-art methods on DAVISand YouTube benchmarks, while being one of the first STM-based SVOS methodsthat can run in real-time on a mobile device.</description><author>Evangelos Skartados, Konstantinos Georgiadis, Mehmet Kerim Yucel, Koskinas Ioannis, Armando Domi, Anastasios Drosou, Bruno Manganelli, Albert Sa`a-Garriga</author><pubDate>Tue, 27 Jun 2023 11:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15377v1</guid></item><item><title>Multi-object Video Generation from Single Frame Layouts</title><link>http://arxiv.org/abs/2305.03983v1</link><description>In this paper, we study video synthesis with emphasis on simplifying thegeneration conditions. Most existing video synthesis models or datasets aredesigned to address complex motions of a single object, lacking the ability ofcomprehensively understanding the spatio-temporal relationships among multipleobjects. Besides, current methods are usually conditioned on intricateannotations (e.g. video segmentations) to generate new videos, beingfundamentally less practical. These motivate us to generate multi-object videosconditioning exclusively on object layouts from a single frame. To solve abovechallenges and inspired by recent research on image generation from layouts, wehave proposed a novel video generative framework capable of synthesizing globalscenes with local objects, via implicit neural representations and layoutmotion self-inference. Our framework is a non-trivial adaptation from imagegeneration methods, and is new to this field. In addition, our model has beenevaluated on two widely-used video recognition benchmarks, demonstratingeffectiveness compared to the baseline model.</description><author>Yang Wu, Zhibin Liu, Hefeng Wu, Liang Lin</author><pubDate>Sat, 06 May 2023 10:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03983v1</guid></item><item><title>TrickVOS: A Bag of Tricks for Video Object Segmentation</title><link>http://arxiv.org/abs/2306.15377v2</link><description>Space-time memory (STM) network methods have been dominant in semi-supervisedvideo object segmentation (SVOS) due to their remarkable performance. In thiswork, we identify three key aspects where we can improve such methods; i)supervisory signal, ii) pretraining and iii) spatial awareness. We then proposeTrickVOS; a generic, method-agnostic bag of tricks addressing each aspect withi) a structure-aware hybrid loss, ii) a simple decoder pretraining regime andiii) a cheap tracker that imposes spatial constraints in model predictions.Finally, we propose a lightweight network and show that when trained withTrickVOS, it achieves competitive results to state-of-the-art methods on DAVISand YouTube benchmarks, while being one of the first STM-based SVOS methodsthat can run in real-time on a mobile device.</description><author>Evangelos Skartados, Konstantinos Georgiadis, Mehmet Kerim Yucel, Koskinas Ioannis, Armando Domi, Anastasios Drosou, Bruno Manganelli, Albert Saa-Garriga</author><pubDate>Wed, 28 Jun 2023 13:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15377v2</guid></item><item><title>Multi-object Video Generation from Single Frame Layouts</title><link>http://arxiv.org/abs/2305.03983v2</link><description>In this paper, we study video synthesis with emphasis on simplifying thegeneration conditions. Most existing video synthesis models or datasets aredesigned to address complex motions of a single object, lacking the ability ofcomprehensively understanding the spatio-temporal relationships among multipleobjects. Besides, current methods are usually conditioned on intricateannotations (e.g. video segmentations) to generate new videos, beingfundamentally less practical. These motivate us to generate multi-object videosconditioning exclusively on object layouts from a single frame. To solve abovechallenges and inspired by recent research on image generation from layouts, wehave proposed a novel video generative framework capable of synthesizing globalscenes with local objects, via implicit neural representations and layoutmotion self-inference. Our framework is a non-trivial adaptation from imagegeneration methods, and is new to this field. In addition, our model has beenevaluated on two widely-used video recognition benchmarks, demonstratingeffectiveness compared to the baseline model.</description><author>Yang Wu, Zhibin Liu, Hefeng Wu, Liang Lin</author><pubDate>Tue, 23 May 2023 16:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03983v2</guid></item><item><title>GPS-GLASS: Learning Nighttime Semantic Segmentation Using Daytime Video and GPS data</title><link>http://arxiv.org/abs/2207.13297v5</link><description>Semantic segmentation for autonomous driving should be robust against variousin-the-wild environments. Nighttime semantic segmentation is especiallychallenging due to a lack of annotated nighttime images and a large domain gapfrom daytime images with sufficient annotation. In this paper, we propose anovel GPS-based training framework for nighttime semantic segmentation. GivenGPS-aligned pairs of daytime and nighttime images, we perform cross-domaincorrespondence matching to obtain pixel-level pseudo supervision. Moreover, weconduct flow estimation between daytime video frames and apply GPS-basedscaling to acquire another pixel-level pseudo supervision. Using these pseudosupervisions with a confidence map, we train a nighttime semantic segmentationnetwork without any annotation from nighttime images. Experimental resultsdemonstrate the effectiveness of the proposed method on several nighttimesemantic segmentation datasets. Our source code is available athttps://github.com/jimmy9704/GPS-GLASS.</description><author>Hongjae Lee, Changwoo Han, Jun-Sang Yoo, Seung-Won Jung</author><pubDate>Fri, 18 Aug 2023 07:38:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.13297v5</guid></item><item><title>Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment</title><link>http://arxiv.org/abs/2307.02682v1</link><description>Dense video captioning, a task of localizing meaningful moments andgenerating relevant captions for videos, often requires a large, expensivecorpus of annotated video segments paired with text. In an effort to minimizethe annotation cost, we propose ZeroTA, a novel method for dense videocaptioning in a zero-shot manner. Our method does not require any videos orannotations for training; instead, it localizes and describes events withineach input video at test time by optimizing solely on the input. This isaccomplished by introducing a soft moment mask that represents a temporalsegment in the video and jointly optimizing it with the prefix parameters of alanguage model. This joint optimization aligns a frozen language generationmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,CLIP) by maximizing the matching score between the generated text and a momentwithin the video. We also introduce a pairwise temporal IoU loss to let a setof soft moment masks capture multiple distinct events within the video. Ourmethod effectively discovers diverse significant events within the video, withthe resulting captions appropriately describing these events. The empiricalresults demonstrate that ZeroTA surpasses zero-shot baselines and evenoutperforms the state-of-the-art few-shot method on the widely-used benchmarkActivityNet Captions. Moreover, our method shows greater robustness compared tosupervised methods when evaluated in out-of-domain scenarios. This researchprovides insight into the potential of aligning widely-used models, such aslanguage generation models and vision-language models, to unlock a newcapability: understanding temporal aspects of videos.</description><author>Yongrae Jo, Seongyun Lee, Aiden SJ Lee, Hyunji Lee, Hanseok Oh, Minjoon Seo</author><pubDate>Thu, 06 Jul 2023 00:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02682v1</guid></item><item><title>Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment</title><link>http://arxiv.org/abs/2307.02682v2</link><description>Dense video captioning, a task of localizing meaningful moments andgenerating relevant captions for videos, often requires a large, expensivecorpus of annotated video segments paired with text. In an effort to minimizethe annotation cost, we propose ZeroTA, a novel method for dense videocaptioning in a zero-shot manner. Our method does not require any videos orannotations for training; instead, it localizes and describes events withineach input video at test time by optimizing solely on the input. This isaccomplished by introducing a soft moment mask that represents a temporalsegment in the video and jointly optimizing it with the prefix parameters of alanguage model. This joint optimization aligns a frozen language generationmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,CLIP) by maximizing the matching score between the generated text and a momentwithin the video. We also introduce a pairwise temporal IoU loss to let a setof soft moment masks capture multiple distinct events within the video. Ourmethod effectively discovers diverse significant events within the video, withthe resulting captions appropriately describing these events. The empiricalresults demonstrate that ZeroTA surpasses zero-shot baselines and evenoutperforms the state-of-the-art few-shot method on the widely-used benchmarkActivityNet Captions. Moreover, our method shows greater robustness compared tosupervised methods when evaluated in out-of-domain scenarios. This researchprovides insight into the potential of aligning widely-used models, such aslanguage generation models and vision-language models, to unlock a newcapability: understanding temporal aspects of videos.</description><author>Yongrae Jo, Seongyun Lee, Aiden SJ Lee, Hyunji Lee, Hanseok Oh, Minjoon Seo</author><pubDate>Tue, 11 Jul 2023 05:10:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02682v2</guid></item><item><title>Autonomous Stabilization of Retinal Videos for Streamlining Assessment of Spontaneous Venous Pulsations</title><link>http://arxiv.org/abs/2305.06043v1</link><description>Spontaneous retinal Venous Pulsations (SVP) are rhythmic changes in thecaliber of the central retinal vein and are observed in the optic disc region(ODR) of the retina. Its absence is a critical indicator of various ocular orneurological abnormalities. Recent advances in imaging technology have enabledthe development of portable smartphone-based devices for observing the retinaand assessment of SVPs. However, the quality of smartphone-based retinal videosis often poor due to noise and image jitting, which in return, can severelyobstruct the observation of SVPs. In this work, we developed a fully automatedretinal video stabilization method that enables the examination of SVPscaptured by various mobile devices. Specifically, we first propose an ODRSpatio-Temporal Localization (ODR-STL) module to localize visible ODR andremove noisy and jittering frames. Then, we introduce a Noise-Aware TemplateMatching (NATM) module to stabilize high-quality video segments at a fixedposition in the field of view. After the processing, the SVPs can be easilyobserved in the stabilized videos, significantly facilitating userobservations. Furthermore, our method is cost-effective and has been tested inboth subjective and objective evaluations. Both of the evaluations support itseffectiveness in facilitating the observation of SVPs. This can improve thetimely diagnosis and treatment of associated diseases, making it a valuabletool for eye health professionals.</description><author>Hongwei Sheng, Xin Yu, Feiyu Wang, MD Wahiduzzaman Khan, Hexuan Weng, Sahar Shariflou, S. Mojtaba Golzan</author><pubDate>Wed, 10 May 2023 11:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06043v1</guid></item><item><title>Gloss Attention for Gloss-free Sign Language Translation</title><link>http://arxiv.org/abs/2307.07361v1</link><description>Most sign language translation (SLT) methods to date require the use of glossannotations to provide additional supervision information, however, theacquisition of gloss is not easy. To solve this problem, we first perform ananalysis of existing models to confirm how gloss annotations make SLT easier.We find that it can provide two aspects of information for the model, 1) it canhelp the model implicitly learn the location of semantic boundaries incontinuous sign language videos, 2) it can help the model understand the signlanguage video globally. We then propose \emph{gloss attention}, which enablesthe model to keep its attention within video segments that have the samesemantics locally, just as gloss helps existing models do. Furthermore, wetransfer the knowledge of sentence-to-sentence similarity from the naturallanguage model to our gloss attention SLT network (GASLT) to help it understandsign language videos at the sentence level. Experimental results on multiplelarge-scale sign language datasets show that our proposed GASLT modelsignificantly outperforms existing methods. Our code is provided in\url{https://github.com/YinAoXiong/GASLT}.</description><author>Aoxiong Yin, Tianyun Zhong, Li Tang, Weike Jin, Tao Jin, Zhou Zhao</author><pubDate>Fri, 14 Jul 2023 15:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07361v1</guid></item><item><title>Progression-Guided Temporal Action Detection in Videos</title><link>http://arxiv.org/abs/2308.09268v1</link><description>We present a novel framework, Action Progression Network (APN), for temporalaction detection (TAD) in videos. The framework locates actions in videos bydetecting the action evolution process. To encode the action evolution, wequantify a complete action process into 101 ordered stages (0\%, 1\%, ...,100\%), referred to as action progressions. We then train a neural network torecognize the action progressions. The framework detects action boundaries bydetecting complete action processes in the videos, e.g., a video segment withdetected action progressions closely follow the sequence 0\%, 1\%, ..., 100\%.The framework offers three major advantages: (1) Our neural networks aretrained end-to-end, contrasting conventional methods that optimize modulesseparately; (2) The APN is trained using action frames exclusively, enablingmodels to be trained on action classification datasets and robust to videoswith temporal background styles differing from those in training; (3) Ourframework effectively avoids detecting incomplete actions and excels indetecting long-lasting actions due to the fine-grained and explicit encoding ofthe temporal structure of actions. Leveraging these advantages, the APNachieves competitive performance and significantly surpasses its counterpartsin detecting long-lasting actions. With an IoU threshold of 0.5, the APNachieves a mean Average Precision (mAP) of 58.3\% on the THUMOS14 dataset and98.9\% mAP on the DFMAD70 dataset.</description><author>Chongkai Lu, Man-Wai Mak, Ruimin Li, Zheru Chi, Hong Fu</author><pubDate>Fri, 18 Aug 2023 04:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09268v1</guid></item><item><title>Colonoscopy Coverage Revisited: Identifying Scanning Gaps in Real-Time</title><link>http://arxiv.org/abs/2305.10026v1</link><description>Colonoscopy is the most widely used medical technique for preventingColorectal Cancer, by detecting and removing polyps before they becomemalignant. Recent studies show that around one quarter of the existing polypsare routinely missed. While some of these do appear in the endoscopist's fieldof view, others are missed due to a partial coverage of the colon. The task ofdetecting and marking unseen regions of the colon has been addressed in recentwork, where the common approach is based on dense 3D reconstruction, whichproves to be challenging due to lack of 3D ground truth and periods with poorvisual content. In this paper we propose a novel and complementary method todetect deficient local coverage in real-time for video segments where areliable 3D reconstruction is impossible. Our method aims to identify skipsalong the colon caused by a drifted position of the endoscope during poorvisibility time intervals. The proposed solution consists of two phases. Duringthe first, time segments with good visibility of the colon and gaps betweenthem are identified. During the second phase, a trained model operates on eachgap, answering the question: Do you observe the same scene before and after thegap? If the answer is negative, the endoscopist is alerted and can be directedto the appropriate area in real-time. The second phase model is trained using acontrastive loss based on auto-generated examples. Our method evaluation on adataset of 250 procedures annotated by trained physicians provides sensitivityof 0.75 with specificity of 0.9.</description><author>G. Leifman, I. Kligvasser, R. Goldenberg, M. Elad, E. Rivlin</author><pubDate>Wed, 17 May 2023 09:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10026v1</guid></item><item><title>CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition</title><link>http://arxiv.org/abs/2305.16049v2</link><description>Audio-visual person recognition (AVPR) has received extensive attention.However, most datasets used for AVPR research so far are collected inconstrained environments, and thus cannot reflect the true performance of AVPRsystems in real-world scenarios. To meet the request for research on AVPR inunconstrained conditions, this paper presents a multi-genre AVPR datasetcollected `in the wild', named CN-Celeb-AV. This dataset contains more than419k video segments from 1,136 persons from public media. In particular, we putmore emphasis on two real-world complexities: (1) data in multiple genres; (2)segments with partial information. A comprehensive study was conducted tocompare CN-Celeb-AV with two popular public AVPR benchmark datasets, and theresults demonstrated that CN-Celeb-AV is more in line with real-world scenariosand can be regarded as a new benchmark dataset for AVPR research. The datasetalso involves a development set that can be used to boost the performance ofAVPR systems in real-life situations. The dataset is free for researchers andcan be downloaded from http://cnceleb.org/.</description><author>Lantian Li, Xiaolou Li, Haoyu Jiang, Chen Chen, Ruihai Hou, Dong Wang</author><pubDate>Fri, 28 Jul 2023 16:13:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16049v2</guid></item><item><title>Automatic Interaction and Activity Recognition from Videos of Human Manual Demonstrations with Application to Anomaly Detection</title><link>http://arxiv.org/abs/2304.09789v2</link><description>This paper presents a new method to describe spatio-temporal relationsbetween objects and hands, to recognize both interactions and activities withinvideo demonstrations of manual tasks. The approach exploits Scene Graphs toextract key interaction features from image sequences while simultaneouslyencoding motion patterns and context. Additionally, the method introducesevent-based automatic video segmentation and clustering, which allow for thegrouping of similar events and detect if a monitored activity is executedcorrectly. The effectiveness of the approach was demonstrated in twomulti-subject experiments, showing the ability to recognize and clusterhand-object and object-object interactions without prior knowledge of theactivity, as well as matching the same activity performed by differentsubjects.</description><author>Elena Merlo, Marta Lagomarsino, Edoardo Lamon, Arash Ajoudani</author><pubDate>Fri, 07 Jul 2023 09:31:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09789v2</guid></item><item><title>Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations</title><link>http://arxiv.org/abs/2307.01533v2</link><description>This paper aims to address the unsupervised video anomaly detection (VAD)problem, which involves classifying each frame in a video as normal orabnormal, without any access to labels. To accomplish this, the proposed methodemploys conditional diffusion models, where the input data is thespatiotemporal features extracted from a pre-trained network, and the conditionis the features extracted from compact motion representations that summarize agiven video segment in terms of its motion and appearance. Our method utilizesa data-driven threshold and considers a high reconstruction error as anindicator of anomalous events. This study is the first to utilize compactmotion representations for VAD and the experiments conducted on two large-scaleVAD benchmarks demonstrate that they supply relevant information to thediffusion model, and consequently improve VAD performances w.r.t the prior art.Importantly, our method exhibits better generalization performance acrossdifferent datasets, notably outperforming both the state-of-the-art andbaseline methods. The code of our method is available athttps://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion</description><author>Anil Osman Tur, Nicola Dall'Asen, Cigdem Beyan, Elisa Ricci</author><pubDate>Wed, 19 Jul 2023 07:39:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01533v2</guid></item><item><title>TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical Phase Recognition</title><link>http://arxiv.org/abs/2307.09997v1</link><description>To enable context-aware computer assistance in the operating room of thefuture, cognitive systems need to understand automatically which surgical phaseis being performed by the medical team. The primary source of information forsurgical phase recognition is typically video, which presents two challenges:extracting meaningful features from the video stream and effectively modelingtemporal information in the sequence of visual features. For temporal modeling,attention mechanisms have gained popularity due to their ability to capturelong-range dependencies. In this paper, we explore design choices for attentionin existing temporal models for surgical phase recognition and propose a novelapproach that does not resort to local attention or regularization of attentionweights: TUNeS is an efficient and simple temporal model that incorporatesself-attention at the coarsest stage of a U-Net-like structure. In addition, wepropose to train the feature extractor, a standard CNN, together with an LSTMon preferably long video segments, i.e., with long temporal context. In ourexperiments, all temporal models performed better on top of feature extractorsthat were trained with longer temporal context. On top of these contextualizedfeatures, TUNeS achieves state-of-the-art results on Cholec80.</description><author>Isabel Funke, Dominik Rivoir, Stefanie Krell, Stefanie Speidel</author><pubDate>Wed, 19 Jul 2023 15:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09997v1</guid></item><item><title>CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition</title><link>http://arxiv.org/abs/2305.16049v1</link><description>Audio-visual person recognition (AVPR) has received extensive attention.However, most datasets used for AVPR research so far are collected inconstrained environments, and thus cannot reflect the true performance of AVPRsystems in real-world scenarios. To meet the request for research on AVPR inunconstrained conditions, this paper presents a multi-genre AVPR datasetcollected `in the wild', named CN-Celeb-AV. This dataset contains more than420k video segments from 1,136 persons from public media. In particular, we putmore emphasis on two real-world complexities: (1) data in multiple genres; (2)segments with partial information. A comprehensive study was conducted tocompare CN-Celeb-AV with two popular public AVPR benchmark datasets, and theresults demonstrated that CN-Celeb-AV is more in line with real-world scenariosand can be regarded as a new benchmark dataset for AVPR research. The datasetalso involves a development set that can be used to boost the performance ofAVPR systems in real-life situations. The dataset is free for researchers andcan be downloaded from http://cnceleb.org/.</description><author>Lantian Li, Xiaolou Li, Haoyu Jiang, Chen Chen, Ruihai Hou, Dong Wang</author><pubDate>Thu, 25 May 2023 14:31:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16049v1</guid></item><item><title>Scene Separation &amp; Data Selection: Temporal Segmentation Algorithm for Real-Time Video Stream Analysis</title><link>http://arxiv.org/abs/2308.00210v1</link><description>We present 2SDS (Scene Separation and Data Selection algorithm), a temporalsegmentation algorithm used in real-time video stream interpretation. Itcomplements CNN-based models to make use of temporal information in videos.2SDS can detect the change between scenes in a video stream by com-paring theimage difference between two frames. It separates a video into segments(scenes), and by combining itself with a CNN model, 2SDS can select the optimalresult for each scene. In this paper, we will be discussing some basic methodsand concepts behind 2SDS, as well as presenting some preliminary experimentresults regarding 2SDS. During these experiments, 2SDS has achieved an overallaccuracy of over 90%.</description><author>Yuelin Xin, Zihan Zhou, Yuxuan Xia</author><pubDate>Tue, 01 Aug 2023 01:53:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00210v1</guid></item><item><title>DPMix: Mixture of Depth and Point Cloud Video Experts for 4D Action Segmentation</title><link>http://arxiv.org/abs/2307.16803v1</link><description>In this technical report, we present our findings from the research conductedon the Human-Object Interaction 4D (HOI4D) dataset for egocentric actionsegmentation task. As a relatively novel research area, point cloud videomethods might not be good at temporal modeling, especially for long point cloudvideos (\eg, 150 frames). In contrast, traditional video understanding methodshave been well developed. Their effectiveness on temporal modeling has beenwidely verified on many large scale video datasets. Therefore, we convert pointcloud videos into depth videos and employ traditional video modeling methods toimprove 4D action segmentation. By ensembling depth and point cloud videomethods, the accuracy is significantly improved. The proposed method, namedMixture of Depth and Point cloud video experts (DPMix), achieved the firstplace in the 4D Action Segmentation Track of the HOI4D Challenge 2023.</description><author>Yue Zhang, Hehe Fan, Yi Yang, Mohan Kankanhalli</author><pubDate>Mon, 31 Jul 2023 17:14:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16803v1</guid></item><item><title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking</title><link>http://arxiv.org/abs/2307.02508v1</link><description>The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objecttracking and segmentation. In this study, we convert the bounding boxes tomasks in reference frames with the help of the Segment Anything Model (SAM) andAlpha-Refine, and then propagate the masks to the current frame, transformingthe task from Video Object Tracking (VOT) to video object segmentation (VOS).Furthermore, we introduce MSDeAOT, a variant of the AOT series thatincorporates transformers at multiple feature scales. MSDeAOT efficientlypropagates object masks from previous frames to the current frame using twofeature scales of 16 and 8. As a testament to the effectiveness of our design,we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object TrackingChallenge.</description><author>Yuanyou Xu, Jiahao Li, Zongxin Yang, Yi Yang, Yueting Zhuang</author><pubDate>Wed, 05 Jul 2023 04:50:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02508v1</guid></item><item><title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking</title><link>http://arxiv.org/abs/2307.02508v2</link><description>The Associating Objects with Transformers (AOT) framework has exhibitedexceptional performance in a wide range of complex scenarios for video objecttracking and segmentation. In this study, we convert the bounding boxes tomasks in reference frames with the help of the Segment Anything Model (SAM) andAlpha-Refine, and then propagate the masks to the current frame, transformingthe task from Video Object Tracking (VOT) to video object segmentation (VOS).Furthermore, we introduce MSDeAOT, a variant of the AOT series thatincorporates transformers at multiple feature scales. MSDeAOT efficientlypropagates object masks from previous frames to the current frame using twofeature scales of 16 and 8. As a testament to the effectiveness of our design,we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object TrackingChallenge.</description><author>Yuanyou Xu, Jiahao Li, Zongxin Yang, Yi Yang, Yueting Zhuang</author><pubDate>Mon, 10 Jul 2023 10:17:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02508v2</guid></item><item><title>EPIC Fields: Marrying 3D Geometry and Video Understanding</title><link>http://arxiv.org/abs/2306.08731v1</link><description>Neural rendering is fuelling a unification of learning, 3D geometry and videounderstanding that has been waiting for more than two decades. Progress,however, is still hampered by a lack of suitable datasets and benchmarks. Toaddress this gap, we introduce EPIC Fields, an augmentation of EPIC-KITCHENSwith 3D camera information. Like other datasets for neural rendering, EPICFields removes the complex and expensive step of reconstructing cameras usingphotogrammetry, and allows researchers to focus on modelling problems. Weillustrate the challenge of photogrammetry in egocentric videos of dynamicactions and propose innovations to address them. Compared to other neuralrendering datasets, EPIC Fields is better tailored to video understandingbecause it is paired with labelled action segments and the recent VISOR segmentannotations. To further motivate the community, we also evaluate two benchmarktasks in neural rendering and segmenting dynamic objects, with strong baselinesthat showcase what is not possible today. We also highlight the advantage ofgeometry in semi-supervised video object segmentations on the VISORannotations. EPIC Fields reconstructs 96% of videos in EPICKITCHENS,registering 19M frames in 99 hours recorded in 45 kitchens.</description><author>Vadim Tschernezki, Ahmad Darkhalil, Zhifan Zhu, David Fouhey, Iro Laina, Diane Larlus, Dima Damen, Andrea Vedaldi</author><pubDate>Wed, 14 Jun 2023 21:33:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08731v1</guid></item><item><title>1st Place Solution for CVPR2023 BURST Long Tail and Open World Challenges</title><link>http://arxiv.org/abs/2308.04598v1</link><description>Currently, Video Instance Segmentation (VIS) aims at segmenting andcategorizing objects in videos from a closed set of training categories thatcontain only a few dozen of categories, lacking the ability to handle diverseobjects in real-world videos. As TAO and BURST datasets release, we have theopportunity to research VIS in long-tailed and open-world scenarios.Traditional VIS methods are evaluated on benchmarks limited to a small numberof common classes, But practical applications require trackers that go beyondthese common classes, detecting and tracking rare and even never-before-seenobjects. Inspired by the latest MOT paper for the long tail task (TrackingEvery Thing in the Wild, Siyuan Li et), for the BURST long tail challenge, wetrain our model on a combination of LVISv0.5 and the COCO dataset using repeatfactor sampling. First, train the detector with segmentation and CEM onLVISv0.5 + COCO dataset. And then, train the instance appearance similarityhead on the TAO dataset. at last, our method (LeTracker) gets 14.9 HOTAall inthe BURST test set, ranking 1st in the benchmark. for the open-worldchallenges, we only use 64 classes (Intersection classes of BURST Train subsetand COCO dataset, without LVIS dataset) annotations data training, and testingon BURST test set data and get 61.4 OWTAall, ranking 1st in the benchmark. Ourcode will be released to facilitate future research.</description><author>Kaer Huang</author><pubDate>Tue, 08 Aug 2023 22:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04598v1</guid></item><item><title>VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions</title><link>http://arxiv.org/abs/2305.18756v1</link><description>Video-grounded dialogue understanding is a challenging problem that requiresmachine to perceive, parse and reason over situated semantics extracted fromweakly aligned video and dialogues. Most existing benchmarks treat bothmodalities the same as a frame-independent visual understanding task, whileneglecting the intrinsic attributes in multimodal dialogues, such as scene andtopic transitions. In this paper, we present Video-grounded Scene&amp;Topic AwaRedialogue (VSTAR) dataset, a large scale video-grounded dialogue understandingdataset based on 395 TV series. Based on VSTAR, we propose two benchmarks forvideo-grounded dialogue understanding: scene segmentation and topicsegmentation, and one benchmark for video-grounded dialogue generation.Comprehensive experiments are performed on these benchmarks to demonstrate theimportance of multimodal information and segments in video-grounded dialogueunderstanding and generation.</description><author>Yuxuan Wang, Zilong Zheng, Xueliang Zhao, Jinpeng Li, Yueqian Wang, Dongyan Zhao</author><pubDate>Tue, 30 May 2023 06:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18756v1</guid></item><item><title>Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising</title><link>http://arxiv.org/abs/2305.18264v1</link><description>Leveraging large-scale image-text datasets and advancements in diffusionmodels, text-driven generative models have made remarkable strides in the fieldof image generation and editing. This study explores the potential of extendingthe text-driven ability to the generation and editing of multi-text conditionedlong videos. Current methodologies for video generation and editing, whileinnovative, are often confined to extremely short videos (typically less than24 frames) and are limited to a single text condition. These constraintssignificantly limit their applications given that real-world videos usuallyconsist of multiple segments, each bearing different semantic information. Toaddress this challenge, we introduce a novel paradigm dubbed as Gen-L-Video,capable of extending off-the-shelf short video diffusion models for generatingand editing videos comprising hundreds of frames with diverse semantic segmentswithout introducing additional training, all while preserving contentconsistency. We have implemented three mainstream text-driven video generationand editing methodologies and extended them to accommodate longer videos imbuedwith a variety of semantic segments with our proposed paradigm. Ourexperimental outcomes reveal that our approach significantly broadens thegenerative and editing capabilities of video diffusion models, offering newpossibilities for future research and applications. The code is available athttps://github.com/G-U-N/Gen-L-Video.</description><author>Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, Hongsheng Li</author><pubDate>Mon, 29 May 2023 18:38:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18264v1</guid></item><item><title>Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics</title><link>http://arxiv.org/abs/2307.16620v2</link><description>The audio-visual segmentation (AVS) task aims to segment sounding objectsfrom a given video. Existing works mainly focus on fusing audio and visualfeatures of a given video to achieve sounding object masks. However, weobserved that prior arts are prone to segment a certain salient object in avideo regardless of the audio information. This is because sounding objects areoften the most salient ones in the AVS dataset. Thus, current AVS methods mightfail to localize genuine sounding objects due to the dataset bias. In thiswork, we present an audio-visual instance-aware segmentation approach toovercome the dataset bias. In a nutshell, our method first localizes potentialsounding objects in a video by an object segmentation network, and thenassociates the sounding object candidates with the given audio. We notice thatan object could be a sounding object in one video but a silent one in anothervideo. This would bring ambiguity in training our object segmentation networkas only sounding objects have corresponding segmentation masks. We thus proposea silent object-aware segmentation objective to alleviate the ambiguity.Moreover, since the category information of audio is unknown, especially formultiple sounding sources, we propose to explore the audio-visual semanticcorrelation and then associate audio with potential objects. Specifically, weattend predicted audio category scores to potential instance masks and thesescores will highlight corresponding sounding instances while suppressinginaudible ones. When we enforce the attended instance masks to resemble theground-truth mask, we are able to establish audio-visual semantics correlation.Experimental results on the AVS benchmarks demonstrate that our method caneffectively segment sounding objects without being biased to salient objects.</description><author>Chen Liu, Peike Li, Xingqun Qi, Hu Zhang, Lincheng Li, Dadong Wang, Xin Yu</author><pubDate>Tue, 01 Aug 2023 02:40:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16620v2</guid></item><item><title>Audio-visual segmentation, sound localization, semantic-aware sounding objects localization</title><link>http://arxiv.org/abs/2307.16620v1</link><description>The audio-visual segmentation (AVS) task aims to segment sounding objectsfrom a given video. Existing works mainly focus on fusing audio and visualfeatures of a given video to achieve sounding object masks. However, weobserved that prior arts are prone to segment a certain salient object in avideo regardless of the audio information. This is because sounding objects areoften the most salient ones in the AVS dataset. Thus, current AVS methods mightfail to localize genuine sounding objects due to the dataset bias. In thiswork, we present an audio-visual instance-aware segmentation approach toovercome the dataset bias. In a nutshell, our method first localizes potentialsounding objects in a video by an object segmentation network, and thenassociates the sounding object candidates with the given audio. We notice thatan object could be a sounding object in one video but a silent one in anothervideo. This would bring ambiguity in training our object segmentation networkas only sounding objects have corresponding segmentation masks. We thus proposea silent object-aware segmentation objective to alleviate the ambiguity.Moreover, since the category information of audio is unknown, especially formultiple sounding sources, we propose to explore the audio-visual semanticcorrelation and then associate audio with potential objects. Specifically, weattend predicted audio category scores to potential instance masks and thesescores will highlight corresponding sounding instances while suppressinginaudible ones. When we enforce the attended instance masks to resemble theground-truth mask, we are able to establish audio-visual semantics correlation.Experimental results on the AVS benchmarks demonstrate that our method caneffectively segment sounding objects without being biased to salient objects.</description><author>Chen Liu, Peike Li, Xingqun Qi, Hu Zhang, Lincheng Li, Dadong Wang, Xin Yu</author><pubDate>Mon, 31 Jul 2023 13:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16620v1</guid></item><item><title>3rd Place Solution for PVUW2023 VSS Track: A Large Model for Semantic Segmentation on VSPW</title><link>http://arxiv.org/abs/2306.02291v2</link><description>In this paper, we introduce 3rd place solution for PVUW2023 VSS track.Semantic segmentation is a fundamental task in computer vision with numerousreal-world applications. We have explored various image-level visual backbonesand segmentation heads to tackle the problem of video semantic segmentation.Through our experimentation, we find that InternImage-H as the backbone andMask2former as the segmentation head achieves the best performance. Inaddition, we explore two post-precessing methods: CascadePSP and SegmentAnything Model (SAM). Ultimately, our approach obtains 62.60\% and 64.84\% mIoUon the VSPW test set1 and final test set, respectively, securing the thirdposition in the PVUW2023 VSS track.</description><author>Shijie Chang, Zeqi Hao, Ben Kang, Xiaoqi Zhao, Jiawen Zhu, Zhenyu Chen, Lihe Zhang, Lu Zhang, Huchuan Lu</author><pubDate>Tue, 06 Jun 2023 02:49:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02291v2</guid></item><item><title>Semantic Segmentation on VSPW Dataset through Contrastive Loss and Multi-dataset Training Approach</title><link>http://arxiv.org/abs/2306.03508v1</link><description>Video scene parsing incorporates temporal information, which can enhance theconsistency and accuracy of predictions compared to image scene parsing. Theadded temporal dimension enables a more comprehensive understanding of thescene, leading to more reliable results. This paper presents the winningsolution of the CVPR2023 workshop for video semantic segmentation, focusing onenhancing Spatial-Temporal correlations with contrastive loss. We also explorethe influence of multi-dataset training by utilizing a label-mapping technique.And the final result is aggregating the output of the above two models. Ourapproach achieves 65.95% mIoU performance on the VSPW dataset, ranked 1st placeon the VSPW challenge at CVPR 2023.</description><author>Min Yan, Qianxiong Ning, Qian Wang</author><pubDate>Tue, 06 Jun 2023 09:53:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03508v1</guid></item><item><title>Video-to-Music Recommendation using Temporal Alignment of Segments</title><link>http://arxiv.org/abs/2306.07187v1</link><description>We study cross-modal recommendation of music tracks to be used as soundtracksfor videos. This problem is known as the music supervision task. We build on aself-supervised system that learns a content association between music andvideo. In addition to the adequacy of content, adequacy of structure is crucialin music supervision to obtain relevant recommendations. We propose a novelapproach to significantly improve the system's performance usingstructure-aware recommendation. The core idea is to consider not only the fullaudio-video clips, but rather shorter segments for training and inference. Wefind that using semantic segments and ranking the tracks according to sequencealignment costs significantly improves the results. We investigate the impactof different ranking metrics and segmentation methods.</description><author>Laure Prétet, Gaël Richard, Clément Souchier, Geoffroy Peeters</author><pubDate>Mon, 12 Jun 2023 16:40:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07187v1</guid></item><item><title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</title><link>http://arxiv.org/abs/2306.00595v1</link><description>We focus on the weakly-supervised audio-visual video parsing task (AVVP),which aims to identify and locate all the events in audio/visual modalities.Previous works only concentrate on video-level overall label denoising acrossmodalities, but overlook the segment-level label noise, where adjacent videosegments (i.e., 1-second video clips) may contain different events. However,recognizing events in the segment is challenging because its label could be anycombination of events that occur in the video. To address this issue, weconsider tackling AVVP from the language perspective, since language couldfreely describe how various events appear in each segment beyond fixed labels.Specifically, we design language prompts to describe all cases of eventappearance for each video. Then, the similarity between language prompts andsegments is calculated, where the event of the most similar prompt is regardedas the segment-level label. In addition, to deal with the mislabeled segments,we propose to perform dynamic re-weighting on the unreliable segments to adjusttheir labels. Experiments show that our simple yet effective approachoutperforms state-of-the-art methods by a large margin.</description><author>Yingying Fan, Yu Wu, Yutian Lin, Bo Du</author><pubDate>Thu, 01 Jun 2023 13:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00595v1</guid></item><item><title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</title><link>http://arxiv.org/abs/2306.00595v4</link><description>We focus on the weakly-supervised audio-visual video parsing task (AVVP),which aims to identify and locate all the events in audio/visual modalities.Previous works only concentrate on video-level overall label denoising acrossmodalities, but overlook the segment-level label noise, where adjacent videosegments (i.e., 1-second video clips) may contain different events. However,recognizing events in the segment is challenging because its label could be anycombination of events that occur in the video. To address this issue, weconsider tackling AVVP from the language perspective, since language couldfreely describe how various events appear in each segment beyond fixed labels.Specifically, we design language prompts to describe all cases of eventappearance for each video. Then, the similarity between language prompts andsegments is calculated, where the event of the most similar prompt is regardedas the segment-level label. In addition, to deal with the mislabeled segments,we propose to perform dynamic re-weighting on the unreliable segments to adjusttheir labels. Experiments show that our simple yet effective approachoutperforms state-of-the-art methods by a large margin.</description><author>Yingying Fan, Yu Wu, Yutian Lin, Bo Du</author><pubDate>Wed, 21 Jun 2023 14:49:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00595v4</guid></item><item><title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</title><link>http://arxiv.org/abs/2306.00595v3</link><description>We focus on the weakly-supervised audio-visual video parsing task (AVVP),which aims to identify and locate all the events in audio/visual modalities.Previous works only concentrate on video-level overall label denoising acrossmodalities, but overlook the segment-level label noise, where adjacent videosegments (i.e., 1-second video clips) may contain different events. However,recognizing events in the segment is challenging because its label could be anycombination of events that occur in the video. To address this issue, weconsider tackling AVVP from the language perspective, since language couldfreely describe how various events appear in each segment beyond fixed labels.Specifically, we design language prompts to describe all cases of eventappearance for each video. Then, the similarity between language prompts andsegments is calculated, where the event of the most similar prompt is regardedas the segment-level label. In addition, to deal with the mislabeled segments,we propose to perform dynamic re-weighting on the unreliable segments to adjusttheir labels. Experiments show that our simple yet effective approachoutperforms state-of-the-art methods by a large margin.</description><author>Yingying Fan, Yu Wu, Yutian Lin, Bo Du</author><pubDate>Wed, 14 Jun 2023 15:12:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00595v3</guid></item><item><title>Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective</title><link>http://arxiv.org/abs/2306.00595v2</link><description>We focus on the weakly-supervised audio-visual video parsing task (AVVP),which aims to identify and locate all the events in audio/visual modalities.Previous works only concentrate on video-level overall label denoising acrossmodalities, but overlook the segment-level label noise, where adjacent videosegments (i.e., 1-second video clips) may contain different events. However,recognizing events in the segment is challenging because its label could be anycombination of events that occur in the video. To address this issue, weconsider tackling AVVP from the language perspective, since language couldfreely describe how various events appear in each segment beyond fixed labels.Specifically, we design language prompts to describe all cases of eventappearance for each video. Then, the similarity between language prompts andsegments is calculated, where the event of the most similar prompt is regardedas the segment-level label. In addition, to deal with the mislabeled segments,we propose to perform dynamic re-weighting on the unreliable segments to adjusttheir labels. Experiments show that our simple yet effective approachoutperforms state-of-the-art methods by a large margin.</description><author>Yingying Fan, Yu Wu, Yutian Lin, Bo Du</author><pubDate>Wed, 07 Jun 2023 07:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00595v2</guid></item><item><title>Holistic Prototype Attention Network for Few-Shot VOS</title><link>http://arxiv.org/abs/2307.07933v1</link><description>Few-shot video object segmentation (FSVOS) aims to segment dynamic objects ofunseen classes by resorting to a small set of support images that containpixel-level object annotations. Existing methods have demonstrated that thedomain agent-based attention mechanism is effective in FSVOS by learning thecorrelation between support images and query frames. However, the agent framecontains redundant pixel information and background noise, resulting ininferior segmentation performance. Moreover, existing methods tend to ignoreinter-frame correlations in query videos. To alleviate the above dilemma, wepropose a holistic prototype attention network (HPAN) for advancing FSVOS.Specifically, HPAN introduces a prototype graph attention module (PGAM) and abidirectional prototype attention module (BPAM), transferring informativeknowledge from seen to unseen classes. PGAM generates local prototypes from allforeground features and then utilizes their internal correlations to enhancethe representation of the holistic prototypes. BPAM exploits the holisticinformation from support images and video frames by fusing co-attention andself-attention to achieve support-query semantic consistency and inner-frametemporal consistency. Extensive experiments on YouTube-FSVOS have been providedto demonstrate the effectiveness and superiority of our proposed HPAN method.</description><author>Yin Tang, Tao Chen, Xiruo Jiang, Yazhou Yao, Guo-Sen Xie, Heng-Tao Shen</author><pubDate>Sun, 16 Jul 2023 04:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07933v1</guid></item><item><title>Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization</title><link>http://arxiv.org/abs/2204.06228v2</link><description>Due to its high societal impact, deepfake detection is getting activeattention in the computer vision community. Most deepfake detection methodsrely on identity, facial attributes, and adversarial perturbation-basedspatio-temporal modifications at the whole video or random locations whilekeeping the meaning of the content intact. However, a sophisticated deepfakemay contain only a small segment of video/audio manipulation, through which themeaning of the content can be, for example, completely inverted from asentiment perspective. We introduce a content-driven audio-visual deepfakedataset, termed Localized Audio Visual DeepFake (LAV-DF), explicitly designedfor the task of learning temporal forgery localization. Specifically, thecontent-driven audio-visual manipulations are performed strategically to changethe sentiment polarity of the whole video. Our baseline method for benchmarkingthe proposed dataset is a 3DCNN model, termed as Boundary Aware TemporalForgery Detection (BA-TFD), which is guided via contrastive, boundary matching,and frame classification loss functions. Our extensive quantitative andqualitative analysis demonstrates the proposed method's strong performance fortemporal forgery localization and deepfake detection tasks.</description><author>Zhixi Cai, Kalin Stefanov, Abhinav Dhall, Munawar Hayat</author><pubDate>Thu, 04 May 2023 01:41:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.06228v2</guid></item><item><title>RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation</title><link>http://arxiv.org/abs/2307.06577v1</link><description>Retinal vessel segmentation is generally grounded in image-based datasetscollected with bench-top devices. The static images naturally lose the dynamiccharacteristics of retina fluctuation, resulting in diminished datasetrichness, and the usage of bench-top devices further restricts datasetscalability due to its limited accessibility. Considering these limitations, weintroduce the first video-based retinal dataset by employing handheld devicesfor data acquisition. The dataset comprises 635 smartphone-based fundus videoscollected from four different clinics, involving 415 patients from 50 to 75years old. It delivers comprehensive and precise annotations of retinalstructures in both spatial and temporal dimensions, aiming to advance thelandscape of vasculature segmentation. Specifically, the dataset provides threelevels of spatial annotations: binary vessel masks for overall retinalstructure delineation, general vein-artery masks for distinguishing the veinand artery, and fine-grained vein-artery masks for further characterizing thegranularities of each artery and vein. In addition, the dataset offers temporalannotations that capture the vessel pulsation characteristics, assisting indetecting ocular diseases that require fine-grained recognition of hemodynamicfluctuation. In application, our dataset exhibits a significant domain shiftwith respect to data captured by bench-top devices, thus posing greatchallenges to existing methods. In the experiments, we provide evaluationmetrics and benchmark results on our dataset, reflecting both the potential andchallenges it offers for vessel segmentation tasks. We hope this challengingdataset would significantly contribute to the development of eye diseasediagnosis and early prevention.</description><author>MD Wahiduzzaman Khan, Hongwei Sheng, Hu Zhang, Heming Du, Sen Wang, Minas Theodore Coroneo, Farshid Hajati, Sahar Shariflou, Michael Kalloniatis, Jack Phu, Ashish Agar, Zi Huang, Mojtaba Golzan, Xin Yu</author><pubDate>Thu, 13 Jul 2023 07:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06577v1</guid></item><item><title>Boosting Weakly-Supervised Temporal Action Localization with Text Information</title><link>http://arxiv.org/abs/2305.00607v1</link><description>Due to the lack of temporal annotation, current Weakly-supervised TemporalAction Localization (WTAL) methods are generally stuck into over-complete orincomplete localization. In this paper, we aim to leverage the text informationto boost WTAL from two aspects, i.e., (a) the discriminative objective toenlarge the inter-class difference, thus reducing the over-complete; (b) thegenerative objective to enhance the intra-class integrity, thus finding morecomplete temporal boundaries. For the discriminative objective, we propose aText-Segment Mining (TSM) mechanism, which constructs a text description basedon the action class label, and regards the text as the query to mine allclass-related segments. Without the temporal annotation of actions, TSMcompares the text query with the entire videos across the dataset to mine thebest matching segments while ignoring irrelevant ones. Due to the sharedsub-actions in different categories of videos, merely applying TSM is toostrict to neglect the semantic-related segments, which results in incompletelocalization. We further introduce a generative objective named Video-textLanguage Completion (VLC), which focuses on all semantic-related segments fromvideos to complete the text sentence. We achieve the state-of-the-artperformance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find ourproposed method can be seamlessly applied to existing methods, and improvetheir performances with a clear margin. The code is available athttps://github.com/lgzlIlIlI/Boosting-WTAL.</description><author>Guozhang Li, De Cheng, Xinpeng Ding, Nannan Wang, Xiaoyu Wang, Xinbo Gao</author><pubDate>Mon, 01 May 2023 01:07:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00607v1</guid></item></channel></rss>