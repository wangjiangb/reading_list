<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivhot papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 09 Jun 2023 06:00:41 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Wavelet Coherence Of Total Solar Irradiance and Atlantic Climate</title><link>http://arxiv.org/abs/2305.02319v1</link><description>The oscillations of climatic parameters of North Atlantic Ocean playimportant role in various events in North America and Europe. Several climaticindices are associated with these oscillations. The long term Atlantictemperature anomalies are described by the Atlantic Multidecadal Oscillation(AMO). The Atlantic Multidecadal Oscillation also known as AtlanticMultidecadal Variability (AMV), is the variability of the sea surfacetemperature (SST) of the North Atlantic Ocean at the timescale of severaldecades. The AMO is correlated to air temperatures and rainfall over much ofthe Northern Hemisphere, in particular in the summer climate in North Americaand Europe. The long-term variations of surface temperature are driven mainlyby the cycles of solar activity, represented by the variations of the TotalSolar Irradiance (TSI). The frequency and amplitude dependences between the TSIand AMO are analyzed by wavelet coherence of millennial time series since 800AD till now. The results of wavelet coherence are compared with the detectedcommon solar and climate cycles in narrow frequency bands by the method ofPartial Fourier Approximation. The long-term coherence between TSI and AMO canhelp to understand better the recent climate change and can improve the longterm forecast.</description><author>Vasil Kolev, Yavor Chapanov</author><pubDate>Wed, 03 May 2023 18:59:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02319v1</guid></item><item><title>AG3D: Learning to Generate 3D Avatars from 2D Image Collections</title><link>http://arxiv.org/abs/2305.02312v1</link><description>While progress in 2D generative models of human appearance has been rapid,many applications require 3D avatars that can be animated and rendered.Unfortunately, most existing methods for learning generative models of 3Dhumans with diverse shape and appearance require 3D training data, which islimited and expensive to acquire. The key to progress is hence to learngenerative models of 3D avatars from abundant unstructured 2D imagecollections. However, learning realistic and complete 3D appearance andgeometry in this under-constrained setting remains challenging, especially inthe presence of loose clothing such as dresses. In this paper, we propose a newadversarial generative model of realistic 3D people from 2D images. Our methodcaptures shape and deformation of the body and loose clothing by adopting aholistic 3D generator and integrating an efficient and flexible articulationmodule. To improve realism, we train our model using multiple discriminatorswhile also integrating geometric cues in the form of predicted 2D normal maps.We experimentally find that our method outperforms previous 3D- andarticulation-aware methods in terms of geometry and appearance. We validate theeffectiveness of our model and the importance of each component via systematicablation studies.</description><author>Zijian Dong, Xu Chen, Jinlong Yang, Michael J. Black, Otmar Hilliges, Andreas Geiger</author><pubDate>Wed, 03 May 2023 18:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02312v1</guid></item><item><title>Real-Time Radiance Fields for Single-Image Portrait View Synthesis</title><link>http://arxiv.org/abs/2305.02310v1</link><description>We present a one-shot method to infer and render a photorealistic 3Drepresentation from a single unposed image (e.g., face portrait) in real-time.Given a single RGB input, our image encoder directly predicts a canonicaltriplane representation of a neural radiance field for 3D-aware novel viewsynthesis via volume rendering. Our method is fast (24 fps) on consumerhardware, and produces higher quality results than strong GAN-inversionbaselines that require test-time optimization. To train our triplane encoderpipeline, we use only synthetic data, showing how to distill the knowledge froma pretrained 3D GAN into a feedforward encoder. Technical contributions includea Vision Transformer-based triplane encoder, a camera data augmentationstrategy, and a well-designed loss function for synthetic data training. Webenchmark against the state-of-the-art methods, demonstrating significantimprovements in robustness and image quality in challenging real-worldsettings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ),but our algorithm can also be applied in the future to other categories with a3D-aware image generator.</description><author>Alex Trevithick, Matthew Chan, Michael Stengel, Eric R. Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano</author><pubDate>Wed, 03 May 2023 18:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02310v1</guid></item><item><title>CodeGen2: Lessons for Training LLMs on Programming and Natural Languages</title><link>http://arxiv.org/abs/2305.02309v1</link><description>Large language models (LLMs) have demonstrated remarkable abilities inrepresentation learning for program synthesis and understanding tasks. Thequality of the learned representations appears to be dictated by the neuralscaling laws as a function of the number of model parameters and observations,while imposing upper bounds on the model performance by the amount of availabledata and compute, which is costly. In this study, we attempt to render the training of LLMs for programsynthesis more efficient by unifying four key components: (1) modelarchitectures, (2) learning methods, (3) infill sampling, and, (4) datadistributions. Specifically, for the model architecture, we attempt to unifyencoder and decoder-based models into a single prefix-LM. For learning methods,(i) causal language modeling, (ii) span corruption, (iii) infilling are unifiedinto a simple learning algorithm. For infill sampling, we explore the claim ofa "free lunch" hypothesis. For data distributions, the effect of a mixturedistribution of programming and natural languages on model performance isexplored. We conduct a comprehensive series of empirical experiments on 1B LLMs, forwhich failures and successes of this exploration are distilled into fourlessons. We will provide a final recipe for training and release CodeGen2models in size 1B, 3.7B, 7B, and, 16B parameters, along with the trainingframework as open-source: https://github.com/salesforce/CodeGen2.</description><author>Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou</author><pubDate>Wed, 03 May 2023 18:55:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02309v1</guid></item><item><title>Fashionpedia-Taste: A Dataset towards Explaining Human Fashion Taste</title><link>http://arxiv.org/abs/2305.02307v1</link><description>Existing fashion datasets do not consider the multi-facts that cause aconsumer to like or dislike a fashion image. Even two consumers like a samefashion image, they could like this image for total different reasons. In thispaper, we study the reason why a consumer like a certain fashion image. Towardsthis goal, we introduce an interpretability dataset, Fashionpedia-taste,consist of rich annotation to explain why a subject like or dislike a fashionimage from the following 3 perspectives: 1) localized attributes; 2) humanattention; 3) caption. Furthermore, subjects are asked to provide theirpersonal attributes and preference on fashion, such as personality andpreferred fashion brands. Our dataset makes it possible for researchers tobuild computational models to fully understand and interpret human fashiontaste from different humanistic perspectives and modalities.</description><author>Mengyun Shi, Serge Belongie, Claire Cardie</author><pubDate>Wed, 03 May 2023 18:54:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02307v1</guid></item><item><title>A Kernel-Based View of Language Model Fine-Tuning</title><link>http://arxiv.org/abs/2210.05643v3</link><description>It has become standard to solve NLP tasks by fine-tuning pre-trained languagemodels (LMs), especially in low-data settings. There is minimal theoreticalunderstanding of empirical success, e.g., why fine-tuning a model with $10^8$or more parameters on a couple dozen training points does not result inoverfitting. We investigate whether the Neural Tangent Kernel (NTK) - whichoriginated as a model to study the gradient descent dynamics of infinitely widenetworks with suitable random initialization - describes fine-tuning ofpre-trained LMs. This study was inspired by the decent performance of NTK forcomputer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adamand use Tensor Programs (Yang, 2020) to characterize conditions under which theNTK lens may describe fine-tuning updates to pre-trained language models.Extensive experiments on 14 NLP tasks validate our theory and show thatformulating the downstream task as a masked word prediction problem throughprompting often induces kernel-based dynamics during fine-tuning. Finally, weuse this kernel view to propose an explanation for the success ofparameter-efficient subspace-based fine-tuning methods.</description><author>Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, Sanjeev Arora</author><pubDate>Wed, 03 May 2023 18:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.05643v3</guid></item><item><title>Calibrated Explanations: with Uncertainty Information and Counterfactuals</title><link>http://arxiv.org/abs/2305.02305v1</link><description>Artificial Intelligence (AI) has become an integral part of decision supportsystems (DSSs) in various domains, but the lack of transparency in thepredictive models used in AI-based DSSs can lead to misuse or disuse.Explainable Artificial Intelligence (XAI) aims to create AI systems that canexplain their rationale to human users. Local explanations in XAI can provideinformation about the causes of individual predictions in terms of featureimportance, but they suffer from drawbacks such as instability. To addressthese issues, we propose a new feature importance explanation method,Calibrated Explanations (CE), which is based on Venn-Abers and calibrates theunderlying model while generating feature importance explanations. CE providesfast, reliable, stable, and robust explanations, along with uncertaintyquantification of the probability estimates and feature importance weights.Furthermore, the method is model agnostic with easily understood conditionalrules and can also generate counterfactual explanations with uncertaintyquantification.</description><author>Helena Lofstrom, Tuwe Lofstrom, Ulf Johansson, Cecilia Sonstrod</author><pubDate>Wed, 03 May 2023 18:52:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02305v1</guid></item><item><title>New Equivalences Between Interpolation and SVMs: Kernels and Structured Features</title><link>http://arxiv.org/abs/2305.02304v1</link><description>The support vector machine (SVM) is a supervised learning algorithm thatfinds a maximum-margin linear classifier, often after mapping the data to ahigh-dimensional feature space via the kernel trick. Recent work hasdemonstrated that in certain sufficiently overparameterized settings, the SVMdecision function coincides exactly with the minimum-norm label interpolant.This phenomenon of support vector proliferation (SVP) is especially interestingbecause it allows us to understand SVM performance by leveraging recentanalyses of harmless interpolation in linear and kernel models. However,previous work on SVP has made restrictive assumptions on the data/featuredistribution and spectrum. In this paper, we present a new and flexibleanalysis framework for proving SVP in an arbitrary reproducing kernel Hilbertspace with a flexible class of generative models for the labels. We presentconditions for SVP for features in the families of general bounded orthonormalsystems (e.g. Fourier features) and independent sub-Gaussian features. In bothcases, we show that SVP occurs in many interesting settings not covered byprior work, and we leverage these results to prove novel generalization resultsfor kernel SVM classification.</description><author>Chiraag Kaushik, Andrew D. McRae, Mark A. Davenport, Vidya Muthukumar</author><pubDate>Wed, 03 May 2023 18:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02304v1</guid></item><item><title>Stream Efficient Learning</title><link>http://arxiv.org/abs/2305.02217v1</link><description>Data in many real-world applications are often accumulated over time, like astream. In contrast to conventional machine learning studies that focus onlearning from a given training data set, learning from data streams cannotignore the fact that the incoming data stream can be potentially endless withoverwhelming size and unknown changes, and it is impractical to assume to havesufficient computational/storage resource such that all received data can behandled in time. Thus, the generalization performance of learning from datastreams depends not only on how many data have been received, but also on howmany data can be well exploited timely, with resource and rapidity concerns, inaddition to the ability of learning algorithm and complexity of the problem.For this purpose, in this article we introduce the notion of machine learningthroughput, define Stream Efficient Learning and present a preliminarytheoretical framework.</description><author>Zhi-Hua Zhou</author><pubDate>Wed, 03 May 2023 16:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02217v1</guid></item><item><title>Exploiting Action Impact Regularity and Exogenous State Variables for Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2111.08066v5</link><description>Offline reinforcement learning -- learning a policy from a batch of data --is known to be hard for general MDPs. These results motivate the need to lookat specific classes of MDPs where offline reinforcement learning might befeasible. In this work, we explore a restricted class of MDPs to obtainguarantees for offline reinforcement learning. The key property, which we callAction Impact Regularity (AIR), is that actions primarily impact a part of thestate (an endogenous component) and have limited impact on the remaining partof the state (an exogenous component). AIR is a strong assumption, but itnonetheless holds in a number of real-world domains including financialmarkets. We discuss algorithms that exploit the AIR property, and provide atheoretical analysis for an algorithm based on Fitted-Q Iteration. Finally, wedemonstrate that the algorithm outperforms existing offline reinforcementlearning algorithms across different data collection policies in simulated andreal world environments where the regularity holds.</description><author>Vincent Liu, James R. Wright, Martha White</author><pubDate>Wed, 03 May 2023 18:51:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.08066v5</guid></item><item><title>Convergence for score-based generative modeling with polynomial complexity</title><link>http://arxiv.org/abs/2206.06227v2</link><description>Score-based generative modeling (SGM) is a highly successful approach forlearning a probability distribution from data and generating further samples.We prove the first polynomial convergence guarantees for the core mechanicbehind SGM: drawing samples from a probability density $p$ given a scoreestimate (an estimate of $\nabla \ln p$) that is accurate in $L^2(p)$. Comparedto previous works, we do not incur error that grows exponentially in time orthat suffers from a curse of dimensionality. Our guarantee works for any smoothdistribution and depends polynomially on its log-Sobolev constant. Using ourguarantee, we give a theoretical analysis of score-based generative modeling,which transforms white-noise input into samples from a learned datadistribution given score estimates at different noise scales. Our analysisgives theoretical grounding to the observation that an annealed procedure isrequired in practice to generate good samples, as our proof depends essentiallyon using annealing to obtain a warm start at each step. Moreover, we show thata predictor-corrector algorithm gives better convergence than using eitherportion alone.</description><author>Holden Lee, Jianfeng Lu, Yixin Tan</author><pubDate>Wed, 03 May 2023 18:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.06227v2</guid></item><item><title>Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes</title><link>http://arxiv.org/abs/2305.02301v1</link><description>Deploying large language models (LLMs) is challenging because they are memoryinefficient and compute-intensive for practical applications. In reaction,researchers train smaller task-specific models by either finetuning with humanlabels or distilling using LLM-generated labels. However, finetuning anddistillation require large amounts of training data to achieve comparableperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that(a) trains smaller models that outperform LLMs, and (b) achieves so byleveraging less training data needed by finetuning or distillation. Our methodextracts LLM rationales as additional supervision for small models within amulti-task training framework. We present three findings across 4 NLPbenchmarks: First, compared to both finetuning and distillation, our mechanismachieves better performance with much fewer labeled/unlabeled trainingexamples. Second, compared to LLMs, we achieve better performance usingsubstantially smaller model sizes. Third, we reduce both the model size and theamount of data required to outperform LLMs; our 770M T5 model outperforms the540B PaLM model using only 80% of available data on a benchmark task.</description><author>Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister</author><pubDate>Wed, 03 May 2023 18:50:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02301v1</guid></item><item><title>Evaluating the Efficacy of Length-Controllable Machine Translation</title><link>http://arxiv.org/abs/2305.02300v1</link><description>Length-controllable machine translation is a type of constrained translation.It aims to contain the original meaning as much as possible while controllingthe length of the translation. We can use automatic summarization or machinetranslation evaluation metrics for length-controllable machine translation, butthis is not necessarily suitable and accurate. This work is the first attemptto evaluate the automatic metrics for length-controllable machine translationtasks systematically. We conduct a rigorous human evaluation on two translationdirections and evaluate 18 summarization or translation evaluation metrics. Wefind that BLEURT and COMET have the highest correlation with human evaluationand are most suitable as evaluation metrics for length-controllable machinetranslation.</description><author>Hao Cheng, Meng Zhang, Weixuan Wang, Liangyou Li, Qun Liu, Zhihua Zhang</author><pubDate>Wed, 03 May 2023 18:50:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02300v1</guid></item><item><title>Dynamic Sparse Training with Structured Sparsity</title><link>http://arxiv.org/abs/2305.02299v1</link><description>DST methods achieve state-of-the-art results in sparse neural networktraining, matching the generalization of dense models while enabling sparsetraining and inference. Although the resulting models are highly sparse andtheoretically cheaper to train, achieving speedups with unstructured sparsityon real-world hardware is challenging. In this work we propose a DST method tolearn a variant of structured N:M sparsity, the acceleration of which ingeneral is commonly supported in commodity hardware. Furthermore, we motivatewith both a theoretical analysis and empirical results, the generalizationperformance of our specific N:M sparsity (constant fan-in), present a condensedrepresentation with a reduced parameter and memory footprint, and demonstratereduced inference time compared to dense models with a naive PyTorch CPUimplementation of the condensed representation Our source code is available athttps://github.com/calgaryml/condensed-sparsity</description><author>Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, Yani Ioannou</author><pubDate>Wed, 03 May 2023 18:48:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02299v1</guid></item><item><title>Making the Most of What You Have: Adapting Pre-trained Visual Language Models in the Low-data Regime</title><link>http://arxiv.org/abs/2305.02297v1</link><description>Large-scale visual language models are widely used as pre-trained models andthen adapted for various downstream tasks. While humans are known toefficiently learn new tasks from a few examples, deep learning models strugglewith adaptation from few examples. In this work, we look into task adaptationin the low-data regime, and provide a thorough study of the existing adaptationmethods for generative Visual Language Models. And we show important benefitsof self-labelling, i.e. using the model's own predictions to self-improve whenhaving access to a larger number of unlabelled images of the same distribution.Our study demonstrates significant gains using our proposed task adaptationpipeline across a wide range of visual language tasks such as visualclassification (ImageNet), visual captioning (COCO), detailed visual captioning(Localised Narratives) and visual question answering (VQAv2).</description><author>Chuhan Zhang, Antoine Miech, Jiajun Shen, Jean-Baptiste Alayrac, Pauline Luc</author><pubDate>Wed, 03 May 2023 18:42:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02297v1</guid></item><item><title>DynamicStereo: Consistent Dynamic Depth from Stereo Videos</title><link>http://arxiv.org/abs/2305.02296v1</link><description>We consider the problem of reconstructing a dynamic scene observed from astereo camera. Most existing methods for depth from stereo treat differentstereo frames independently, leading to temporally inconsistent depthpredictions. Temporal consistency is especially important for immersive AR orVR scenarios, where flickering greatly diminishes the user experience. Wepropose DynamicStereo, a novel transformer-based architecture to estimatedisparity for stereo videos. The network learns to pool information fromneighboring frames to improve the temporal consistency of its predictions. Ourarchitecture is designed to process stereo videos efficiently through dividedattention layers. We also introduce Dynamic Replica, a new benchmark datasetcontaining synthetic videos of people and animals in scanned environments,which provides complementary training and evaluation data for dynamic stereocloser to real applications than existing datasets. Training with this datasetfurther improves the quality of predictions of our proposed DynamicStereo aswell as prior methods. Finally, it acts as a benchmark for consistent stereomethods.</description><author>Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, Christian Rupprecht</author><pubDate>Wed, 03 May 2023 18:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02296v1</guid></item><item><title>Iranian License Plate Recognition Using a Reliable Deep Learning Approach</title><link>http://arxiv.org/abs/2305.02292v1</link><description>The issue of Automatic License Plate Recognition (ALPR) has been one of themost challenging issues in recent years. Weather conditions, camera angle ofview, lighting conditions, different characters written on license plates, andmany other factors are among the challenges for the issue of ALPR. Given theadvances that have been made in recent years in the field of deep neuralnetworks, some types of neural networks and models based on them can be used toperform the task of Iranian license plate recognition. In the proposed methodpresented in this paper, the license plate recognition is done in two steps.The first step is to detect the rectangles of the license plates from the inputimage. In the second step, these license plates are cropped from the image andtheir characters are recognized. For the first step, 3065 images includinglicense plates and for the second step, 3364 images including characters oflicense plates have been prepared and considered as the desired datasets. Inthe first step, license plates are detected using the YOLOv4-tiny model, whichis based on Convolutional Neural Network (CNN). In the next step, thecharacters of these license plates are recognized using Convolutional RecurrentNeural Network (CRNN), and Connectionist Temporal Classification (CTC). In thesecond step, there is no need to segment and label the characters separately,only one string of numbers and letters is enough for the labels.</description><author>Soheila Hatami, Majid Sadedel, Farideh Jamali</author><pubDate>Wed, 03 May 2023 18:34:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02292v1</guid></item><item><title>Evaluating BERT-based Scientific Relation Classifiers for Scholarly Knowledge Graph Construction on Digital Library Collections</title><link>http://arxiv.org/abs/2305.02291v1</link><description>The rapid growth of research publications has placed great demands on digitallibraries (DL) for advanced information management technologies. To cater tothese demands, techniques relying on knowledge-graph structures are beingadvocated. In such graph-based pipelines, inferring semantic relations betweenrelated scientific concepts is a crucial step. Recently, BERT-based pre-trainedmodels have been popularly explored for automatic relation classification.Despite significant progress, most of them were evaluated in differentscenarios, which limits their comparability. Furthermore, existing methods areprimarily evaluated on clean texts, which ignores the digitization context ofearly scholarly publications in terms of machine scanning and optical characterrecognition (OCR). In such cases, the texts may contain OCR noise, in turncreating uncertainty about existing classifiers' performances. To address theselimitations, we started by creating OCR-noisy texts based on three cleancorpora. Given these parallel corpora, we conducted a thorough empiricalevaluation of eight Bert-based classification models by focusing on threefactors: (1) Bert variants; (2) classification strategies; and, (3) OCR noiseimpacts. Experiments on clean data show that the domain-specific pre-trainedBert is the best variant to identify scientific relations. The strategy ofpredicting a single relation each time outperforms the one simultaneouslyidentifying multiple relations in general. The optimal classifier's performancecan decline by around 10% to 20% in F-score on the noisy corpora. Insightsdiscussed in this study can help DL stakeholders select techniques for buildingoptimal knowledge-graph-based systems.</description><author>Ming Jiang, Jennifer D'Souza, Sören Auer, J. Stephen Downie</author><pubDate>Wed, 03 May 2023 18:32:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02291v1</guid></item><item><title>Distributed Leader Follower Formation Control of Mobile Robots based on Bioinspired Neural Dynamics and Adaptive Sliding Innovation Filter</title><link>http://arxiv.org/abs/2305.02288v1</link><description>This paper investigated the distributed leader follower formation controlproblem for multiple differentially driven mobile robots. A distributedestimator is first introduced and it only requires the state information fromeach follower itself and its neighbors. Then, we propose a bioinspired neuraldynamic based backstepping and sliding mode control hybrid formation controlmethod with proof of its stability. The proposed control strategy resolves theimpractical speed jump issue that exists in the conventional backsteppingdesign. Additionally, considering the system and measurement noises, theproposed control strategy not only removes the chattering issue existing in theconventional sliding mode control but also provides smooth control input withextra robustness. After that, an adaptive sliding innovation filter isintegrated with the proposed control to provide accurate state estimates thatare robust to modeling uncertainties. Finally, we performed multiplesimulations to demonstrate the efficiency and effectiveness of the proposedformation control strategy.</description><author>Zhe Xu, Tao Yan, Simon X. Yang, S. Andrew Gadsden</author><pubDate>Wed, 03 May 2023 18:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02288v1</guid></item><item><title>Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models</title><link>http://arxiv.org/abs/2305.02279v1</link><description>During the continuous evolution of one organism's ancestry, its genesaccumulate extensive experiences and knowledge, enabling newborn descendants torapidly adapt to their specific environments. Motivated by this observation, wepropose a novel machine learning paradigm \textit{Learngene} to enable learningmodels to incorporate three key characteristics of genes. (i) Accumulating: theknowledge is accumulated during the continuous learning of an \textbf{ancestrymodel}. (ii) Condensing: the exhaustive accumulated knowledge is condensed intoa much more compact information piece, \ie \textbf{learngene}. (iii):Inheriting: the condensed \textbf{learngene} is inherited to make it easier for\textbf{descendant models} to adapt to new environments. Since accumulating hasbeen studied in some well-developed paradigms like large-scale pre-training andlifelong learning, we focus on condensing and inheriting, which induces threekey issues and we provide the preliminary solutions to these issues in thispaper: (i) \textit{Learngene} Form: the \textbf{learngene} is set to a fewintegral layers that can preserve the most commonality. (ii) \textit{Learngene}Condensing: we identify which layers among the ancestry model have the mostsimilarity as one pseudo descendant model. (iii) \textit{Learngene} Inheriting:to construct distinct descendant models for specific downstream tasks, we stacksome randomly initialized layers to the \textbf{learngene} layers. Extensiveexperiments of various settings, including using different networkarchitectures like Vision Transformer (ViT) and Convolutional Neural Networks(CNNs) on different datasets, are carried out to confirm five advantages andtwo characteristics of \textit{Learngene}.</description><author>Qiufeng Wang, Xu Yang, Shuxia Lin, Xin Geng</author><pubDate>Wed, 03 May 2023 18:15:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02279v1</guid></item><item><title>M2-CTTS: End-to-End Multi-scale Multi-modal Conversational Text-to-Speech Synthesis</title><link>http://arxiv.org/abs/2305.02269v1</link><description>Conversational text-to-speech (TTS) aims to synthesize speech with properprosody of reply based on the historical conversation. However, it is still achallenge to comprehensively model the conversation, and a majority ofconversational TTS systems only focus on extracting global information and omitlocal prosody features, which contain important fine-grained information likekeywords and emphasis. Moreover, it is insufficient to only consider thetextual features, and acoustic features also contain various prosodyinformation. Hence, we propose M2-CTTS, an end-to-end multi-scale multi-modalconversational text-to-speech system, aiming to comprehensively utilizehistorical conversation and enhance prosodic expression. More specifically, wedesign a textual context module and an acoustic context module with bothcoarse-grained and fine-grained modeling. Experimental results demonstrate thatour model mixed with fine-grained context information and additionallyconsidering acoustic features achieves better prosody performance andnaturalness in CMOS tests.</description><author>Jinlong Xue, Yayue Deng, Fengping Wang, Ya Li, Yingming Gao, Jianhua Tao, Jianqing Sun, Jiaen Liang</author><pubDate>Wed, 03 May 2023 17:59:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02269v1</guid></item><item><title>A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text</title><link>http://arxiv.org/abs/2305.02265v1</link><description>Pretrained Vision-Language Models (VLMs) have achieved remarkable performancein image retrieval from text. However, their performance drops drastically whenconfronted with linguistically complex texts that they struggle to comprehend.Inspired by the Divide-and-Conquer algorithm and dual-process theory, in thispaper, we regard linguistically complex texts as compound proposition textscomposed of multiple simple proposition sentences and propose an end-to-endNeural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains threemain components: 1)Divide: a proposition generator divides the compoundproposition text into simple proposition sentences and produces theircorresponding representations, 2)Conquer: a pretrained VLMs-basedvisual-linguistic interactor achieves the interaction between decomposedproposition sentences and images, 3)Combine: a neural-symbolic reasonercombines the above reasoning states to obtain the final solution via a neurallogic reasoning approach. According to the dual-process theory, thevisual-linguistic interactor and neural-symbolic reasoner could be regarded asanalogical reasoning System 1 and logical reasoning System 2. We conductextensive experiments on a challenging image retrieval from contextualdescriptions data set. Experimental results and analyses indicate NDCRsignificantly improves performance in the complex image-text reasoning problem.Code link: https://github.com/YunxinLi/NDCR.</description><author>Yunxin Li, Baotian Hu, Yunxin Ding, Lin Ma, Min Zhang</author><pubDate>Wed, 03 May 2023 17:55:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02265v1</guid></item><item><title>Multi-dimensional Signal Recovery using Low-rank Deconvolution</title><link>http://arxiv.org/abs/2305.02264v1</link><description>In this work we present Low-rank Deconvolution, a powerful framework forlow-level feature-map learning for efficient signal representation withapplication to signal recovery. Its formulation in multi-linear algebrainherits properties from convolutional sparse coding and low-rank approximationmethods as in this setting signals are decomposed in a set of filters convolvedwith a set of low-rank tensors. We show its advantages by learning compressedvideo representations and solving image in-painting problems.</description><author>David Reixach</author><pubDate>Wed, 03 May 2023 17:51:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02264v1</guid></item><item><title>End-to-end Training and Decoding for Pivot-based Cascaded Translation Model</title><link>http://arxiv.org/abs/2305.02261v1</link><description>Utilizing pivot language effectively can significantly improve low-resourcemachine translation. Usually, the two translation models, source-pivot andpivot-target, are trained individually and do not utilize the limited (source,target) parallel data. This work proposes an end-to-end training method for thecascaded translation model and configures an improved decoding algorithm. Theinput of the pivot-target model is modified to weighted pivot embedding basedon the probability distribution output by the source-pivot model. This allowsthe model to be trained end-to-end. In addition, we mitigate the inconsistencybetween tokens and probability distributions while using beam search in pivotdecoding. Experiments demonstrate that our method enhances the quality oftranslation.</description><author>Hao Cheng, Meng Zhang, Liangyou Li, Qun Liu, Zhihua Zhang</author><pubDate>Wed, 03 May 2023 17:48:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02261v1</guid></item><item><title>Standardized Benchmark Dataset for Localized Exposure to a Realistic Source at 10$-$90 GHz</title><link>http://arxiv.org/abs/2305.02260v1</link><description>The lack of freely available standardized datasets represents an aggravatingfactor during the development and testing the performance of novelcomputational techniques in exposure assessment and dosimetry research. Thishinders progress as researchers are required to generate numerical data (field,power and temperature distribution) anew using simulation software for eachexposure scenario. Other than being time consuming, this approach is highlysusceptible to errors that occur during the configuration of theelectromagnetic model. To address this issue, in this paper, the limitedavailable data on the incident power density and resultant maximum temperaturerise on the skin surface considering various steady-state exposure scenarios at10$-$90 GHz have been statistically modeled. The synthetic data have beensampled from the fitted statistical multivariate distribution with respect topredetermined dosimetric constraints. We thus present a comprehensive andopen-source dataset compiled of the high-fidelity numerical data consideringvarious exposures to a realistic source. Furthermore, different surrogatemodels for predicting maximum temperature rise on the skin surface were fittedbased on the synthetic dataset. All surrogate models were tested on theoriginally available data where satisfactory predictive performance has beendemonstrated. A simple technique of combining quadratic polynomial andtensor-product spline surrogates, each operating on its own cluster of data,has achieved the lowest mean absolute error of 0.058 {\deg}C. Therefore,overall experimental results indicate the validity of the proposed syntheticdataset.</description><author>Ante Kapetanovic, Dragan Poljak, Kun Li</author><pubDate>Wed, 03 May 2023 17:48:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02260v1</guid></item><item><title>Contextual Reasoning for Scene Generation (Technical Report)</title><link>http://arxiv.org/abs/2305.02255v1</link><description>We present a continuation to our previous work, in which we developed theMR-CKR framework to reason with knowledge overriding across contexts organizedin multi-relational hierarchies. Reasoning is realized via ASP with algebraicmeasures, allowing for flexible definitions of preferences. In this paper, weshow how to apply our theoretical work to real autonomous-vehicle scene data.Goal of this work is to apply MR-CKR to the problem of generating challengingscenes for autonomous vehicle learning. In practice, most of the scene data forAV learning models common situations, thus it might be difficult to capturecases where a particular situation occurs (e.g. partial occlusions of acrossing pedestrian). The MR-CKR model allows for data organization exploitingthe multi-dimensionality of such data (e.g., temporal and spatial). Reasoningover multiple contexts enables the verification and configuration of scenes,using the combination of different scene ontologies. We describe a frameworkfor semantically guided data generation, based on a combination of MR-CKR andAlgebraic Measures. The framework is implemented in a proof-of-conceptprototype exemplifying some cases of scene generation.</description><author>Loris Bozzato, Thomas Eiter, Rafael Kiesel, Daria Stepanova</author><pubDate>Wed, 03 May 2023 17:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02255v1</guid></item><item><title>An Adaptive Algorithm for Learning with Unknown Distribution Drift</title><link>http://arxiv.org/abs/2305.02252v1</link><description>We develop and analyze a general technique for learning with an unknowndistribution drift. Given a sequence of independent observations from the last$T$ steps of a drifting distribution, our algorithm agnostically learns afamily of functions with respect to the current distribution at time $T$.Unlike previous work, our technique does not require prior knowledge about themagnitude of the drift. Instead, the algorithm adapts to the sample data.Without explicitly estimating the drift, the algorithm learns a family offunctions with almost the same error as a learning algorithm that knows themagnitude of the drift in advance. Furthermore, since our algorithm adapts tothe data, it can guarantee a better learning error than an algorithm thatrelies on loose bounds on the drift.</description><author>Alessio Mazzetto, Eli Upfal</author><pubDate>Wed, 03 May 2023 17:37:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02252v1</guid></item><item><title>Character-Aware Models Improve Visual Text Rendering</title><link>http://arxiv.org/abs/2212.10562v2</link><description>Current image generation models struggle to reliably produce well-formedvisual text. In this paper, we investigate a key contributing factor: populartext-to-image models lack character-level input features, making it much harderto predict a word's visual makeup as a series of glyphs. To quantify thiseffect, we conduct a series of experiments comparing character-aware vs.character-blind text encoders. In the text-only domain, we find thatcharacter-aware models provide large gains on a novel spelling task(WikiSpell). Applying our learnings to the visual domain, we train a suite ofimage generation models, and show that character-aware variants outperformtheir character-blind counterparts across a range of novel text rendering tasks(our DrawText benchmark). Our models set a much higher state-of-the-art onvisual spelling, with 30+ point accuracy gains over competitors on rare words,despite training on far fewer examples.</description><author>Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi, Noah Constant</author><pubDate>Wed, 03 May 2023 17:36:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10562v2</guid></item><item><title>Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems</title><link>http://arxiv.org/abs/2305.02251v1</link><description>The paper surveys automated scientific discovery, from equation discovery andsymbolic regression to autonomous discovery systems and agents. It discussesthe individual approaches from a "big picture" perspective and in context, butalso discusses open issues and recent topics like the various roles of deepneural networks in this area, aiding in the discovery of human-interpretableknowledge. Further, we will present closed-loop scientific discovery systems,starting with the pioneering work on the Adam system up to current efforts infields from material science to astronomy. Finally, we will elaborate onautonomy from a machine learning perspective, but also in analogy to theautonomy levels in autonomous driving. The maximal level, level five, isdefined to require no human intervention at all in the production of scientificknowledge. Achieving this is one step towards solving the Nobel Turing GrandChallenge to develop AI Scientists: AI systems capable of making Nobel-qualityscientific discoveries highly autonomously at a level comparable, and possiblysuperior, to the best human scientists by 2050.</description><author>Stefan Kramer, Mattia Cerrato, Sašo Džeroski, Ross King</author><pubDate>Wed, 03 May 2023 17:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02251v1</guid></item><item><title>Select without Fear: Almost All Mini-Batch Schedules Generalize Optimally</title><link>http://arxiv.org/abs/2305.02247v1</link><description>We establish matching upper and lower generalization error bounds formini-batch Gradient Descent (GD) training with either deterministic orstochastic, data-independent, but otherwise arbitrary batch selection rules. Weconsider smooth Lipschitz-convex/nonconvex/strongly-convex loss functions, andshow that classical upper bounds for Stochastic GD (SGD) also hold verbatim forsuch arbitrary nonadaptive batch schedules, including all deterministic ones.Further, for convex and strongly-convex losses we prove matching lower boundsdirectly on the generalization error uniform over the aforementioned class ofbatch schedules, showing that all such batch schedules generalize optimally.Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch(deterministic) GD is essentially optimal, among all possible batch scheduleswithin the considered class, including all stochastic ones.</description><author>Konstantinos E. Nikolakakis, Amin Karbasi, Dionysis Kalogerias</author><pubDate>Wed, 03 May 2023 17:32:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02247v1</guid></item><item><title>DocILE Benchmark for Document Information Localization and Extraction</title><link>http://arxiv.org/abs/2302.05658v2</link><description>This paper introduces the DocILE benchmark with the largest dataset ofbusiness documents for the tasks of Key Information Localization and Extractionand Line Item Recognition. It contains 6.7k annotated business documents, 100ksynthetically generated documents, and nearly~1M unlabeled documents forunsupervised pre-training. The dataset has been built with knowledge of domain-and task-specific aspects, resulting in the following key features: (i)annotations in 55 classes, which surpasses the granularity of previouslypublished key information extraction datasets by a large margin; (ii) Line ItemRecognition represents a highly practical information extraction task, wherekey information has to be assigned to items in a table; (iii) documents comefrom numerous layouts and the test set includes zero- and few-shot cases aswell as layouts commonly seen in the training set. The benchmark comes withseveral baselines, including RoBERTa, LayoutLMv3 and DETR-based TableTransformer; applied to both tasks of the DocILE benchmark, with results sharedin this paper, offering a quick starting point for future work. The dataset,baselines and supplementary material are available athttps://github.com/rossumai/docile.</description><author>Štěpán Šimsa, Milan Šulc, Michal Uřičář, Yash Patel, Ahmed Hamdi, Matěj Kocián, Matyáš Skalický, Jiří Matas, Antoine Doucet, Mickaël Coustaty, Dimosthenis Karatzas</author><pubDate>Wed, 03 May 2023 17:24:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05658v2</guid></item><item><title>The Benefits of Label-Description Training for Zero-Shot Text Classification</title><link>http://arxiv.org/abs/2305.02239v1</link><description>Large language models have improved zero-shot text classification by allowingthe transfer of semantic knowledge from the training data in order to classifyamong specific label sets in downstream tasks. We propose a simple way tofurther improve zero-shot accuracies with minimal effort. We curate smallfinetuning datasets intended to describe the labels for a task. Unlike typicalfinetuning data, which has texts annotated with labels, our data simplydescribes the labels in language, e.g., using a few related terms,dictionary/encyclopedia entries, and short templates. Across a range of topicand sentiment datasets, our method is more accurate than zero-shot by 15-17%absolute. It is also more robust to choices required for zero-shotclassification, such as patterns for prompting the model to classify andmappings from labels to tokens in the model's vocabulary. Furthermore, sinceour data merely describes the labels but does not use input texts, finetuningon it yields a model that performs strongly on multiple text domains for agiven label set, even improving over few-shot out-of-domain classification inmultiple settings.</description><author>Lingyu Gao, Debanjan Ghosh, Kevin Gimpel</author><pubDate>Wed, 03 May 2023 17:19:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02239v1</guid></item><item><title>AttenWalker: Unsupervised Long-Document Question Answering via Attention-based Graph Walking</title><link>http://arxiv.org/abs/2305.02235v1</link><description>Annotating long-document question answering (long-document QA) pairs istime-consuming and expensive. To alleviate the problem, it might be possible togenerate long-document QA pairs via unsupervised question answering (UQA)methods. However, existing UQA tasks are based on short documents, and canhardly incorporate long-range information. To tackle the problem, we propose anew task, named unsupervised long-document question answering (ULQA), aiming togenerate high-quality long-document QA instances in an unsupervised manner.Besides, we propose AttenWalker, a novel unsupervised method to aggregate andgenerate answers with long-range dependency so as to construct long-document QApairs. Specifically, AttenWalker is composed of three modules, i.e., spancollector, span linker and answer aggregator. Firstly, the span collector takesadvantage of constituent parsing and reconstruction loss to select informativecandidate spans for constructing answers. Secondly, by going through theattention graph of a pre-trained long-document model, potentially interrelatedtext spans (that might be far apart) could be linked together via anattention-walking algorithm. Thirdly, in the answer aggregator, linked spansare aggregated into the final answer via the mask-filling ability of apre-trained model. Extensive experiments show that AttenWalker outperformsprevious methods on Qasper and NarrativeQA. In addition, AttenWalker also showsstrong performance in the few-shot learning setting.</description><author>Yuxiang Nie, Heyan Huang, Wei Wei, Xian-Ling Mao</author><pubDate>Wed, 03 May 2023 17:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02235v1</guid></item><item><title>Two Steps Forward and One Behind: Rethinking Time Series Forecasting with Deep Learning</title><link>http://arxiv.org/abs/2304.04553v2</link><description>The Transformer is a highly successful deep learning model that hasrevolutionised the world of artificial neural networks, first in naturallanguage processing and later in computer vision. This model is based on theattention mechanism and is able to capture complex semantic relationshipsbetween a variety of patterns present in the input data. Precisely because ofthese characteristics, the Transformer has recently been exploited for timeseries forecasting problems, assuming a natural adaptability to the domain ofcontinuous numerical series. Despite the acclaimed results in the literature,some works have raised doubts about the robustness and effectiveness of thisapproach. In this paper, we further investigate the effectiveness ofTransformer-based models applied to the domain of time series forecasting,demonstrate their limitations, and propose a set of alternative models that arebetter performing and significantly less complex. In particular, we empiricallyshow how simplifying Transformer-based forecasting models almost always leadsto an improvement, reaching state of the art performance. We also proposeshallow models without the attention mechanism, which compete with the overallstate of the art in long time series forecasting, and demonstrate their abilityto accurately predict time series over extremely long windows. From amethodological perspective, we show how it is always necessary to use a simplebaseline to verify the effectiveness of proposed models, and finally, weconclude the paper with a reflection on recent research paths and theopportunity to follow trends and hypes even where it may not be necessary.</description><author>Riccardo Ughi, Eugenio Lomurno, Matteo Matteucci</author><pubDate>Wed, 03 May 2023 17:12:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04553v2</guid></item><item><title>Data Privacy with Homomorphic Encryption in Neural Networks Training and Inference</title><link>http://arxiv.org/abs/2305.02225v1</link><description>The use of Neural Networks (NNs) for sensitive data processing is becomingincreasingly popular, raising concerns about data privacy and security.Homomorphic Encryption (HE) has the potential to be used as a solution topreserve data privacy in NN. This study provides a comprehensive analysis onthe use of HE for NN training and classification, focusing on the techniquesand strategies used to enhance data privacy and security. The currentstate-of-the-art in HE for NNs is analysed, and the challenges and limitationsthat need to be addressed to make it a reliable and efficient approach forprivacy preservation are identified. Also, the different categories of HEschemes and their suitability for NNs are discussed, as well as the techniquesused to optimize the accuracy and efficiency of encrypted models. The reviewreveals that HE has the potential to provide strong data privacy guarantees forNNs, but several challenges need to be addressed, such as limited support foradvanced NN operations, scalability issues, and performance trade-offs.</description><author>Ivone Amorim, Eva Maia, Pedro Barbosa, Isabel Praça</author><pubDate>Wed, 03 May 2023 17:05:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02225v1</guid></item><item><title>Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat</title><link>http://arxiv.org/abs/2305.02220v1</link><description>This paper describes our submission to the MEDIQA-Chat 2023 shared task forautomatic clinical note generation from doctor-patient conversations. We reportresults for two approaches: the first fine-tunes a pre-trained language model(PLM) on the shared task data, and the second uses few-shot in-context learning(ICL) with a large language model (LLM). Both achieve high performance asmeasured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second andfirst, respectively, of all submissions to the shared task. Expert humanscrutiny indicates that notes generated via the ICL-based approach with GPT-4are preferred about as often as human-written notes, making it a promising pathtoward automated note generation from doctor-patient conversations.</description><author>John Giorgi, Augustin Toma, Ronald Xie, Sondra Chen, Kevin R. An, Grace X. Zheng, Bo Wang</author><pubDate>Wed, 03 May 2023 16:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02220v1</guid></item><item><title>LESS-VFL: Communication-Efficient Feature Selection for Vertical Federated Learning</title><link>http://arxiv.org/abs/2305.02219v1</link><description>We propose LESS-VFL, a communication-efficient feature selection method fordistributed systems with vertically partitioned data. We consider a system of aserver and several parties with local datasets that share a sample ID space buthave different feature sets. The parties wish to collaboratively train a modelfor a prediction task. As part of the training, the parties wish to removeunimportant features in the system to improve generalization, efficiency, andexplainability. In LESS-VFL, after a short pre-training period, the serveroptimizes its part of the global model to determine the relevant outputs fromparty models. This information is shared with the parties to then allow localfeature selection without communication. We analytically prove that LESS-VFLremoves spurious features from model training. We provide extensive empiricalevidence that LESS-VFL can achieve high accuracy and remove spurious featuresat a fraction of the communication cost of other feature selection approaches.</description><author>Timothy Castiglia, Yi Zhou, Shiqiang Wang, Swanand Kadhe, Nathalie Baracaldo, Stacy Patterson</author><pubDate>Wed, 03 May 2023 16:56:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02219v1</guid></item><item><title>Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages</title><link>http://arxiv.org/abs/2305.02215v1</link><description>The overwhelming success of transformers is a real conundrum stimulating acompelling question: are these machines replicating some traditional linguisticmodels or discovering radically new theories? In this paper, we propose a novelstandpoint to investigate this important question. Using typologicalsimilarities among languages, we aim to layer-wise compare transformers fordifferent languages to observe whether these similarities emerge for particularlayers. For this investigation, we propose to use Centered kernel alignment tomeasure similarity among weight matrices. We discovered that syntactictypological similarity is consistent with the similarity among weights in themiddle layers. This finding confirms results obtained by syntactically probingBERT and, thus, gives an important confirmation that BERT is replicatingtraditional linguistic models.</description><author>Federico Ranaldi, Elena Sofia Ruzzetti, Felicia Logozzo, Michele Mastromattei, Leonardo Ranaldi, Fabio Massimo Zanzotto</author><pubDate>Wed, 03 May 2023 16:52:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02215v1</guid></item><item><title>DocLangID: Improving Few-Shot Training to Identify the Language of Historical Documents</title><link>http://arxiv.org/abs/2305.02208v1</link><description>Language identification describes the task of recognizing the language ofwritten text in documents. This information is crucial because it can be usedto support the analysis of a document's vocabulary and context. Supervisedlearning methods in recent years have advanced the task of languageidentification. However, these methods usually require large labeled datasets,which often need to be included for various domains of images, such asdocuments or scene images. In this work, we propose DocLangID, a transferlearning approach to identify the language of unlabeled historical documents.We achieve this by first leveraging labeled data from a different but relateddomain of historical documents. Secondly, we implement a distance-basedfew-shot learning approach to adapt a convolutional neural network to newlanguages of the unlabeled dataset. By introducing small amounts of manuallylabeled examples from the set of unlabeled images, our feature extractordevelops a better adaptability towards new and different data distributions ofhistorical documents. We show that such a model can be effectively fine-tunedfor the unlabeled set of images by only reusing the same few-shot examples. Weshowcase our work across 10 languages that mostly use the Latin script. Ourexperiments on historical documents demonstrate that our combined approachimproves the language identification performance, achieving 74% recognitionaccuracy on the four unseen languages of the unlabeled dataset.</description><author>Furkan Simsek, Brian Pfitzmann, Hendrik Raetz, Jona Otholt, Haojin Yang, Christoph Meinel</author><pubDate>Wed, 03 May 2023 16:45:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02208v1</guid></item><item><title>HEAT: A Highly Efficient and Affordable Training System for Collaborative Filtering Based Recommendation on CPUs</title><link>http://arxiv.org/abs/2304.07334v2</link><description>Collaborative filtering (CF) has been proven to be one of the most effectivetechniques for recommendation. Among all CF approaches, SimpleX is thestate-of-the-art method that adopts a novel loss function and a proper numberof negative samples. However, there is no work that optimizes SimpleX onmulti-core CPUs, leading to limited performance. To this end, we perform anin-depth profiling and analysis of existing SimpleX implementations andidentify their performance bottlenecks including (1) irregular memory accesses,(2) unnecessary memory copies, and (3) redundant computations. To address theseissues, we propose an efficient CF training system (called HEAT) that fullyenables the multi-level caching and multi-threading capabilities of modernCPUs. Specifically, the optimization of HEAT is threefold: (1) It tiles theembedding matrix to increase data locality and reduce cache misses (thusreduces read latency); (2) It optimizes stochastic gradient descent (SGD) withsampling by parallelizing vector products instead of matrix-matrixmultiplications, in particular the similarity computation therein, to avoidmemory copies for matrix data preparation; and (3) It aggressively reusesintermediate results from the forward phase in the backward phase to alleviateredundant computation. Evaluation on five widely used datasets with both x86-and ARM-architecture processors shows that HEAT achieves up to 45.2X speedupover existing CPU solution and 4.5X speedup and 7.9X cost reduction in Cloudover existing GPU solution with NVIDIA V100 GPU.</description><author>Chengming Zhang, Shaden Smith, Baixi Sun, Jiannan Tian, Jonathan Soifer, Xiaodong Yu, Shuaiwen Leon Song, Yuxiong He, Dingwen Tao</author><pubDate>Wed, 03 May 2023 16:42:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.07334v2</guid></item><item><title>OADAT: Experimental and Synthetic Clinical Optoacoustic Data for Standardized Image Processing</title><link>http://arxiv.org/abs/2206.08612v2</link><description>Optoacoustic (OA) imaging is based on excitation of biological tissues withnanosecond-duration laser pulses followed by subsequent detection of ultrasoundwaves generated via light-absorption-mediated thermoelastic expansion. OAimaging features a powerful combination between rich optical contrast and highresolution in deep tissues. This enabled the exploration of a number ofattractive new applications both in clinical and laboratory settings. However,no standardized datasets generated with different types of experimental set-upand associated processing methods are available to facilitate advances inbroader applications of OA in clinical settings. This complicates an objectivecomparison between new and established data processing methods, often leadingto qualitative results and arbitrary interpretations of the data. In thispaper, we provide both experimental and synthetic OA raw signals andreconstructed image domain datasets rendered with different experimentalparameters and tomographic acquisition geometries. We further provide trainedneural networks to tackle three important challenges related to OA imageprocessing, namely accurate reconstruction under limited view tomographicconditions, removal of spatial undersampling artifacts and anatomicalsegmentation for improved image reconstruction. Specifically, we define 44experiments corresponding to the aforementioned challenges as benchmarks to beused as a reference for the development of more advanced processing methods.</description><author>Firat Ozdemir, Berkan Lafci, Xosé Luís Deán-Ben, Daniel Razansky, Fernando Perez-Cruz</author><pubDate>Wed, 03 May 2023 16:40:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.08612v2</guid></item><item><title>Medical Image Deidentification, Cleaning and Compression Using Pylogik</title><link>http://arxiv.org/abs/2304.12322v2</link><description>Leveraging medical record information in the era of big data and machinelearning comes with the caveat that data must be cleaned and deidentified.Facilitating data sharing and harmonization for multi-center collaborations areparticularly difficult when protected health information (PHI) is contained orembedded in image meta-data. We propose a novel library in the Pythonframework, called PyLogik, to help alleviate this issue for ultrasound images,which are particularly challenging because of the frequent inclusion of PHIdirectly on the images. PyLogik processes the image volumes through a series oftext detection/extraction, filtering, thresholding, morphological and contourcomparisons. This methodology deidentifies the images, reduces file sizes, andprepares image volumes for applications in deep learning and data sharing. Toevaluate its effectiveness in the identification of regions of interest (ROI),a random sample of 50 cardiac ultrasounds (echocardiograms) were processedthrough PyLogik, and the outputs were compared with the manual segmentations byan expert user. The Dice coefficient of the two approaches achieved an averagevalue of 0.976. Next, an investigation was conducted to ascertain the degree ofinformation compression achieved using the algorithm. Resultant data was foundto be on average approximately 72% smaller after processing by PyLogik. Ourresults suggest that PyLogik is a viable methodology for ultrasound datacleaning and deidentification, determining ROI, and file compression which willfacilitate efficient storage, use, and dissemination of ultrasound data.</description><author>Adrienne Kline, Vinesh Appadurai, Yuan Luo, Sanjiv Shah</author><pubDate>Wed, 03 May 2023 16:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12322v2</guid></item><item><title>Inverse Global Illumination using a Neural Radiometric Prior</title><link>http://arxiv.org/abs/2305.02192v1</link><description>Inverse rendering methods that account for global illumination are becomingmore popular, but current methods require evaluating and automaticallydifferentiating millions of path integrals by tracing multiple light bounces,which remains expensive and prone to noise. Instead, this paper proposes aradiometric prior as a simple alternative to building complete path integralsin a traditional differentiable path tracer, while still correctly accountingfor global illumination. Inspired by the Neural Radiosity technique, we use aneural network as a radiance function, and we introduce a prior consisting ofthe norm of the residual of the rendering equation in the inverse renderingloss. We train our radiance network and optimize scene parameterssimultaneously using a loss consisting of both a photometric term betweenrenderings and the multi-view input images, and our radiometric prior (theresidual term). This residual term enforces a physical constraint on theoptimization that ensures that the radiance field accounts for globalillumination. We compare our method to a vanilla differentiable path tracer,and more advanced techniques such as Path Replay Backpropagation. Despite thesimplicity of our approach, we can recover scene parameters with comparable andin some cases better quality, at considerably lower computation times.</description><author>Saeed Hadadan, Geng Lin, Jan Novák, Fabrice Rousselle, Matthias Zwicker</author><pubDate>Wed, 03 May 2023 16:36:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02192v1</guid></item><item><title>Forecasting through deep learning and modal decomposition in two-phase concentric jets</title><link>http://arxiv.org/abs/2212.12731v2</link><description>This work aims to improve fuel chamber injectors' performance in turbofanengines, thus implying improved performance and reduction of pollutants. Thisrequires the development of models that allow real-time prediction andimprovement of the fuel/air mixture. However, the work carried out to dateinvolves using experimental data (complicated to measure) or the numericalresolution of the complete problem (computationally prohibitive). The latterinvolves the resolution of a system of partial differential equations (PDE).These problems make difficult to develop a real-time prediction tool.Therefore, in this work, we propose using machine learning in conjunction with(complementarily cheaper) single-phase flow numerical simulations in thepresence of tangential discontinuities to estimate the mixing process intwo-phase flows. In this meaning we study the application of two proposedneural network (NN) models as PDE surrogate models. Where the future dynamicsis predicted by the NN, given some preliminary information. We show the lowcomputational cost required by these models, both in their training andinference phases. We also show how NN training can be improved by reducing datacomplexity through a modal decomposition technique called higher order dynamicmode decomposition (HODMD), which identifies the main structures inside flowdynamics and reconstructs the original flow using only these main structures.This reconstruction has the same number of samples and spatial dimension as theoriginal flow, but with a less complex dynamics and preserving its mainfeatures. The core idea of this work is to test the limits of applicability ofdeep learning models to data forecasting in complex fluid dynamics problems.Generalization capabilities of the models are demonstrated by using the same NNarchitectures to forecast the future dynamics of four different two-phaseflows.</description><author>León Mata, Rodrigo Abadía-Heredia, Manuel Lopez-Martin, José M. Pérez, Soledad Le Clainche</author><pubDate>Wed, 03 May 2023 16:35:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12731v2</guid></item><item><title>Rethinking Graph Lottery Tickets: Graph Sparsity Matters</title><link>http://arxiv.org/abs/2305.02190v1</link><description>Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket(i.e., a properly pruned sub-network together with original weightinitialization) that can achieve competitive performance to the original densenetwork. A recent work, called UGS, extended LTH to prune graph neural networks(GNNs) for effectively accelerating GNN inference. UGS simultaneously prunesthe graph adjacency matrix and the model weights using the same maskingmechanism, but since the roles of the graph adjacency matrix and the weightmatrices are very different, we find that their sparsifications lead todifferent performance characteristics. Specifically, we find that theperformance of a sparsified GNN degrades significantly when the graph sparsitygoes beyond a certain extent. Therefore, we propose two techniques to improveGNN performance when the graph sparsity is high. First, UGS prunes theadjacency matrix using a loss formulation which, however, does not properlyinvolve all elements of the adjacency matrix; in contrast, we add a newauxiliary loss head to better guide the edge pruning by involving the entireadjacency matrix. Second, by regarding unfavorable graph sparsification asadversarial data perturbations, we formulate the pruning process as a min-maxoptimization problem to gain the robustness of lottery tickets when the graphsparsity is high. We further investigate the question: Can the "retrainable"winning ticket of a GNN be also effective for graph transferring learning? Wecall it the transferable graph lottery ticket (GLT) hypothesis. Extensiveexperiments were conducted which demonstrate the superiority of our proposedsparsification method over UGS, and which empirically verified our transferableGLT hypothesis.</description><author>Bo Hui, Da Yan, Xiaolong Ma, Wei-Shinn Ku</author><pubDate>Wed, 03 May 2023 16:35:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02190v1</guid></item><item><title>CLUSTSEG: Clustering for Universal Segmentation</title><link>http://arxiv.org/abs/2305.02187v1</link><description>We present CLUSTSEG, a general, transformer-based framework that tacklesdifferent image segmentation tasks (i.e., superpixel, semantic, instance, andpanoptic) through a unified neural clustering scheme. Regarding queries ascluster centers, CLUSTSEG is innovative in two aspects:1) cluster centers areinitialized in heterogeneous ways so as to pointedly address task-specificdemands (e.g., instance- or category-level distinctiveness), yet withoutmodifying the architecture; and 2) pixel-cluster assignment, formalized in across-attention fashion, is alternated with cluster center update, yet withoutlearning additional parameters. These innovations closely link CLUSTSEG to EMclustering and make it a transparent and powerful framework that yieldssuperior results across the above segmentation tasks.</description><author>James Liang, Tianfei Zhou, Dongfang Liu, Wenguan Wang</author><pubDate>Wed, 03 May 2023 16:31:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02187v1</guid></item><item><title>MIXER: Multiattribute, Multiway Fusion of Uncertain Pairwise Affinities</title><link>http://arxiv.org/abs/2210.08360v2</link><description>We present a multiway fusion algorithm capable of directly processinguncertain pairwise affinities. In contrast to existing works that requireinitial pairwise associations, our MIXER algorithm improves accuracy byleveraging the additional information provided by pairwise affinities. Our maincontribution is a multiway fusion formulation that is particularly suited toprocessing non-binary affinities and a novel continuous relaxation whosesolutions are guaranteed to be binary, thus avoiding the typical, butpotentially problematic, solution binarization steps that may causeinfeasibility. A crucial insight of our formulation is that it allows for threemodes of association, ranging from non-match, undecided, and match. Exploitingthis insight allows fusion to be delayed for some data pairs until moreinformation is available, which is an effective feature for fusion of data withmultiple attributes/information sources. We evaluate MIXER on typical syntheticdata and benchmark datasets and show increased accuracy against the state ofthe art in multiway matching, especially in noisy regimes with low observationredundancy. Additionally, we collect RGB data of cars in a parking lot todemonstrate MIXER's ability to fuse data having multiple attributes (color,visual appearance, and bounding box). On this challenging dataset, MIXERachieves 74% F1 accuracy and is 49x faster than the next best algorithm, whichhas 42% accuracy. Open source code is available athttps://github.com/mit-acl/mixer.</description><author>Parker C. Lusk, Kaveh Fathian, Jonathan P. How</author><pubDate>Wed, 03 May 2023 16:28:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.08360v2</guid></item><item><title>Few-Shot Learning for Biometric Verification</title><link>http://arxiv.org/abs/2211.06761v2</link><description>In machine learning applications, it is common practice to feed as muchinformation as possible. In most cases, the model can handle large data setsthat allow to predict more accurately. In the presence of data scarcity, aFew-Shot learning (FSL) approach aims to build more accurate algorithms withlimited training data. We propose a novel end-to-end lightweight architecturethat verifies biometric data by producing competitive results as compared tostate-of-the-art accuracies through Few-Shot learning methods. The dense layersadd to the complexity of state-of-the-art deep learning models which inhibitsthem to be used in low-power applications. In presented approach, a shallownetwork is coupled with a conventional machine learning technique that exploitshand-crafted features to verify biometric images from multi-modal sources suchas signatures, periocular region, iris, face, fingerprints etc. We introduce aself-estimated threshold that strictly monitors False Acceptance Rate (FAR)while generalizing its results hence eliminating user-defined thresholds fromROC curves that are likely to be biased on local data distribution. This hybridmodel benefits from few-shot learning to make up for scarcity of data inbiometric use-cases. We have conducted extensive experimentation with commonlyused biometric datasets. The obtained results provided an effective solutionfor biometric verification systems.</description><author>Saad Bin Ahmed, Umaid M. Zaffar, Marium Aslam, Muhammad Imran Malik</author><pubDate>Wed, 03 May 2023 16:24:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.06761v2</guid></item><item><title>Towards Greener and Attention-aware Solutions for Steering Angle Prediction</title><link>http://arxiv.org/abs/2211.11133v2</link><description>In this paper, we investigate the two most popular families of deep neuralarchitectures (i.e., ResNets and InceptionNets) for the autonomous driving taskof steering angle prediction. This work provides preliminary evidence thatInception architectures can perform as well or sometimes better than ResNetarchitectures with less complexity for the autonomous driving task. Our focusis on the compact end of the complexity spectrum. Compact neural networkarchitectures produce less carbon emissions and are thus more environmentallyfriendly. We look at various sizes of compact ResNet and InceptionNet models tocompare results. Our derived models can achieve state-of-the-art results interms of steering angle MSE. In addition, we also explore the attentionmechanism and investigate its influence on steering angle prediction.</description><author>Pramiti Barua, Jeremy C. Hagler, David J. Lamb, Qing Tian</author><pubDate>Wed, 03 May 2023 16:23:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.11133v2</guid></item><item><title>The Diminishing Returns of Masked Language Models to Science</title><link>http://arxiv.org/abs/2205.11342v2</link><description>Transformer-based masked language models such as BERT, trained on generalcorpora, have shown impressive performance on downstream tasks. It has alsobeen demonstrated that the downstream task performance of such models can beimproved by pretraining larger models for longer on more data. In this work, weempirically evaluate the extent to which these results extend to tasks inscience. We use 14 domain-specific transformer-based models (includingScholarBERT, a new 770M-parameter science-focused masked language modelpretrained on up to 225B tokens) to evaluate the impact of training data, modelsize, pretraining and finetuning time on 12 downstream scientific tasks.Interestingly, we find that increasing model sizes, training data, or computetime does not always lead to significant improvements (i.e., &gt;1% F1), if atall, in scientific information extraction tasks and offered possibleexplanations for the surprising performance differences.</description><author>Zhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon Duede, Kyle Chard, Ian Foster</author><pubDate>Wed, 03 May 2023 16:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.11342v2</guid></item><item><title>Transforming Visual Scene Graphs to Image Captions</title><link>http://arxiv.org/abs/2305.02177v1</link><description>We propose to Transform Scene Graphs (TSG) into more descriptive captions. InTSG, we apply multi-head attention (MHA) to design the Graph Neural Network(GNN) for embedding scene graphs. After embedding, different graph embeddingscontain diverse specific knowledge for generating the words with differentpart-of-speech, e.g., object/attribute embedding is good for generatingnouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-baseddecoder, where each expert is built on MHA, for discriminating the graphembeddings to generate different kinds of words. Since both the encoder anddecoder are built based on the MHA, as a result, we construct a homogeneousencoder-decoder unlike the previous heterogeneous ones which usually applyFully-Connected-based GNN and LSTM-based decoder. The homogeneous architectureenables us to unify the training configuration of the whole model instead ofspecifying different training strategies for diverse sub-networks as in theheterogeneous pipeline, which releases the training difficulty. Extensiveexperiments on the MS-COCO captioning benchmark validate the effectiveness ofour TSG. The code is in: https://anonymous.4open.science/r/ACL23_TSG.</description><author>Xu Yang, Jiawei Peng, Zihua Wang, Haiyang Xu, Qinghao Ye, Chenliang Li, Ming Yan, Fei Huang, Zhangzikang Li, Yu Zhang</author><pubDate>Wed, 03 May 2023 16:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02177v1</guid></item><item><title>Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity</title><link>http://arxiv.org/abs/2305.02176v1</link><description>Mixture-of-experts (MoE) models that employ sparse activation havedemonstrated effectiveness in significantly increasing the number of parameterswhile maintaining low computational requirements per token. However, recentstudies have established that MoE models are inherently parameter-inefficientas the improvement in performance diminishes with an increasing number ofexperts. We hypothesize this parameter inefficiency is a result of all expertshaving equal capacity, which may not adequately meet the varying complexityrequirements of different tokens or tasks, e.g., in a multilingual setting,languages based on their resource levels might require different capacities. Inlight of this, we propose Stratified Mixture of Experts(SMoE) models, whichfeature a stratified structure and can assign dynamic capacity to differenttokens. We demonstrate the effectiveness of SMoE on two multilingual machinetranslation benchmarks, where it outperforms multiple state-of-the-art MoEmodels. On a diverse 15-language dataset, SMoE improves the translation qualityover vanilla MoE by +0.93 BLEU points on average. Additionally, SMoE isparameter-efficient, matching vanilla MoE performance with around 50\% fewerparameters.</description><author>Haoran Xu, Maha Elbayad, Kenton Murray, Jean Maillard, Vedanuj Goswami</author><pubDate>Wed, 03 May 2023 16:18:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02176v1</guid></item><item><title>Distilling Knowledge for Short-to-Long Term Trajectory Prediction</title><link>http://arxiv.org/abs/2305.08553v1</link><description>Long-term trajectory forecasting is a challenging problem in the field ofcomputer vision and machine learning. In this paper, we propose a new methoddubbed Di-Long ("Distillation for Long-Term trajectory") for long-termtrajectory forecasting, which is based on knowledge distillation. Our approachinvolves training a student network to solve the long-term trajectoryforecasting problem, whereas the teacher network from which the knowledge isdistilled has a longer observation, and solves a short-term trajectoryprediction problem by regularizing the student's predictions. Specifically, weuse a teacher model to generate plausible trajectories for a shorter timehorizon, and then distill the knowledge from the teacher model to a studentmodel that solves the problem for a much higher time horizon. Our experimentsshow that the proposed Di-Long approach is beneficial for long-termforecasting, and our model achieves state-of-the-art performance on theIntersection Drone Dataset (inD) and the Stanford Drone Dataset (SDD).</description><author>Sourav Das, Guglielmo Camporese, Lamberto Ballan</author><pubDate>Mon, 15 May 2023 12:30:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08553v1</guid></item><item><title>Continual Reasoning: Non-Monotonic Reasoning in Neurosymbolic AI using Continual Learning</title><link>http://arxiv.org/abs/2305.02171v1</link><description>Despite the extensive investment and impressive recent progress at reasoningby similarity, deep learning continues to struggle with more complex forms ofreasoning such as non-monotonic and commonsense reasoning. Non-monotonicity isa property of non-classical reasoning typically seen in commonsense reasoning,whereby a reasoning system is allowed (differently from classical logic) tojump to conclusions which may be retracted later, when new information becomesavailable. Neural-symbolic systems such as Logic Tensor Networks (LTN) havebeen shown to be effective at enabling deep neural networks to achievereasoning capabilities. In this paper, we show that by combining aneural-symbolic system with methods from continual learning, LTN can obtain ahigher level of accuracy when addressing non-monotonic reasoning tasks.Continual learning is added to LTNs by adopting a curriculum of learning fromknowledge and data with recall. We call this process Continual Reasoning, a newmethodology for the application of neural-symbolic systems to reasoning tasks.Continual Reasoning is applied to a prototypical non-monotonic reasoningproblem as well as other reasoning examples. Experimentation is conducted tocompare and analyze the effects that different curriculum choices may have onoverall learning and reasoning results. Results indicate significantimprovement on the prototypical non-monotonic reasoning problem and a promisingoutlook for the proposed approach on statistical relational learning examples.</description><author>Sofoklis Kyriakopoulos, Artur S. d'Avila Garcez</author><pubDate>Wed, 03 May 2023 16:11:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02171v1</guid></item><item><title>A Data Mining Approach for Detecting Collusion in Unproctored Online Exams</title><link>http://arxiv.org/abs/2302.07014v2</link><description>Due to the precautionary measures during the COVID-19 pandemic manyuniversities offered unproctored take-home exams. We propose methods to detectpotential collusion between students and apply our approach on event log datafrom take-home exams during the pandemic. We find groups of students withsuspiciously similar exams. In addition, we compare our findings to a proctoredcontrol group. By this, we establish a rule of thumb for evaluating which casesare "outstandingly similar", i.e., suspicious cases.</description><author>Janine Langerbein, Till Massing, Jens Klenke, Natalie Reckmann, Michael Striewe, Michael Goedicke, Christoph Hanck</author><pubDate>Wed, 03 May 2023 16:10:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.07014v2</guid></item><item><title>A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus</title><link>http://arxiv.org/abs/2305.02170v1</link><description>We present a pipeline for a statistical textual exploration, offering astylometry-based explanation and statistical validation of a hypothesizedpartition of a text. Given a parameterization of the text, our pipeline: (1)detects literary features yielding the optimal overlap between the hypothesizedand unsupervised partitions, (2) performs a hypothesis-testing analysis toquantify the statistical significance of the optimal overlap, while conservingimplicit correlations between units of text that are more likely to be grouped,and (3) extracts and quantifies the importance of features most responsible forthe classification, estimates their statistical stability and cluster-wiseabundance. We apply our pipeline to the first two books in the Bible, where onestylistic component stands out in the eyes of biblical scholars, namely, thePriestly component. We identify and explore statistically significant stylisticdifferences between the Priestly and non-Priestly components.</description><author>Gideon Yoffe, Axel Bühler, Nachum Dershowitz, Israel Finkelstein, Eli Piasetzky, Thomas Römer, Barak Sober</author><pubDate>Wed, 03 May 2023 16:07:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02170v1</guid></item><item><title>A Closer Look at Self-Supervised Lightweight Vision Transformers</title><link>http://arxiv.org/abs/2205.14443v2</link><description>Self-supervised learning on large-scale Vision Transformers (ViTs) aspre-training methods has achieved promising downstream performance. Yet, howmuch these pre-training paradigms promote lightweight ViTs' performance isconsiderably less studied. In this work, we develop and benchmark severalself-supervised pre-training methods on image classification tasks and somedownstream dense prediction tasks. We surprisingly find that if properpre-training is adopted, even vanilla lightweight ViTs show comparableperformance to previous SOTA networks with delicate architecture design. Itbreaks the recently popular conception that vanilla ViTs are not suitable forvision tasks in lightweight regimes. We also point out some defects of suchpre-training, e.g., failing to benefit from large-scale pre-training data andshowing inferior performance on data-insufficient downstream tasks.Furthermore, we analyze and clearly show the effect of such pre-training byanalyzing the properties of the layer representation and attention maps forrelated models. Finally, based on the above analyses, a distillation strategyduring pre-training is developed, which leads to further downstream performanceimprovement for MAE-based pre-training. Code is available athttps://github.com/wangsr126/mae-lite.</description><author>Shaoru Wang, Jin Gao, Zeming Li, Xiaoqin Zhang, Weiming Hu</author><pubDate>Wed, 03 May 2023 16:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.14443v2</guid></item><item><title>Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows</title><link>http://arxiv.org/abs/2305.02164v1</link><description>Sliced-Wasserstein Flow (SWF) is a promising approach to nonparametricgenerative modeling but has not been widely adopted due to its suboptimalgenerative quality and lack of conditional modeling capabilities. In this work,we make two major contributions to bridging this gap. First, based on apleasant observation that (under certain conditions) the SWF of jointdistributions coincides with those of conditional distributions, we proposeConditional Sliced-Wasserstein Flow (CSWF), a simple yet effective extension ofSWF that enables nonparametric conditional modeling. Second, we introduceappropriate inductive biases of images into SWF with two techniques inspired bylocal connectivity and multiscale representation in vision research, whichgreatly improve the efficiency and quality of modeling images. With all theimprovements, we achieve generative performance comparable with many deepparametric generative models on both conditional and unconditional tasks in apurely nonparametric fashion, demonstrating its great potential.</description><author>Chao Du, Tianbo Li, Tianyu Pang, Shuicheng Yan, Min Lin</author><pubDate>Wed, 03 May 2023 15:55:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02164v1</guid></item><item><title>Flexible Differentiable Optimization via Model Transformations</title><link>http://arxiv.org/abs/2206.06135v2</link><description>We introduce DiffOpt.jl, a Julia library to differentiate through thesolution of optimization problems with respect to arbitrary parameters presentin the objective and/or constraints. The library builds upon MathOptInterface,thus leveraging the rich ecosystem of solvers and composing well with modelinglanguages like JuMP. DiffOpt offers both forward and reverse differentiationmodes, enabling multiple use cases from hyperparameter optimization tobackpropagation and sensitivity analysis, bridging constrained optimizationwith end-to-end differentiable programming. DiffOpt is built on two known rulesfor differentiating quadratic programming and conic programming standard forms.However, thanks ability to differentiate through model transformation, the useris not limited to these forms and can differentiate with respect to theparameters of any model that can be reformulated into these standard forms.This notably includes programs mixing affine conic constraints and convexquadratic constraints or objective function.</description><author>Akshay Sharma, Mathieu Besançon, Joaquim Dias Garcia, Benoît Legat</author><pubDate>Tue, 25 Apr 2023 19:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.06135v2</guid></item><item><title>A novel framework for medium-term wind power prediction based on temporal attention mechanisms</title><link>http://arxiv.org/abs/2302.01222v4</link><description>Wind energy is a widely distributed, recyclable and environmentally friendlyenergy source that plays an important role in mitigating global warming andenergy shortages. Wind energy's uncertainty and fluctuating nature makes gridintegration of large-scale wind energy systems challenging. Medium-term windpower forecasts can provide an essential basis for energy dispatch, so accuratewind power forecasts are essential. Much research has yielded excellent resultsin recent years. However, many of them require additional experimentation andanalysis when applied to other data. In this paper, we propose a novelshort-term forecasting framework by tree-structured parzen estimator (TPE) anddecomposition algorithms. This framework defines the TPE-VMD-TFT method for24-h and 48-h ahead wind power forecasting based on variational modedecomposition (VMD) and time fusion transformer (TFT). In the Engie winddataset from the electricity company in France, the results show that theproposed method significantly improves the prediction accuracy. In addition,the proposed framework can be used to other decomposition algorithms andrequire little manual work in model training.</description><author>Meiyu Jiang, Jun Shen, Xuetao Jiang, Qingguo Zhou, Rui Zhou</author><pubDate>Wed, 03 May 2023 15:55:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01222v4</guid></item><item><title>Prompt-based Zero-shot Relation Extraction with Semantic Knowledge Augmentation</title><link>http://arxiv.org/abs/2112.04539v2</link><description>In relation triplet extraction (RTE), recognizing unseen (new) relations forwhich there are no training instances is a challenging task. Efforts have beenmade to recognize unseen relations based on question-answering models orrelation descriptions. However, these approaches miss the semantic informationabout connections between seen and unseen relations. In this paper, We proposea prompt-based model with semantic knowledge augmentation (ZS-SKA) to recognizeunseen relations under the zero-shot setting. We present a new word-levelanalogy-based sentence translation rule and generate augmented instances withunseen relations from instances with seen relations using that new rule. Wedesign prompts with weighted virtual label construction based on an externalknowledge graph to integrate semantic knowledge information learned from seenrelations. Instead of using the actual label sets in the prompt template, weconstruct weighted virtual label words. We learn the representations of bothseen and unseen relations with augmented instances and prompts. We thencalculate the distance between the generated representations using prototypicalnetworks to predict unseen relations. Extensive experiments conducted on threepublic datasets FewRel, Wiki-ZSL, and NYT, show that ZS-SKA outperformsstate-of-the-art methods under the zero-shot scenarios. Our experimentalresults also demonstrate the effectiveness and robustness of ZS-SKA.</description><author>Jiaying Gong, Hoda Eldardiry</author><pubDate>Wed, 03 May 2023 15:49:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.04539v2</guid></item><item><title>Explaining Language Models' Predictions with High-Impact Concepts</title><link>http://arxiv.org/abs/2305.02160v1</link><description>The emergence of large-scale pretrained language models has posedunprecedented challenges in deriving explanations of why the model has madesome predictions. Stemmed from the compositional nature of languages, spuriouscorrelations have further undermined the trustworthiness of NLP systems,leading to unreliable model explanations that are merely correlated with theoutput predictions. To encourage fairness and transparency, there exists anurgent demand for reliable explanations that allow users to consistentlyunderstand the model's behavior. In this work, we propose a complete frameworkfor extending concept-based interpretability methods to NLP. Specifically, wepropose a post-hoc interpretability method for extracting predictive high-levelfeatures (concepts) from the pretrained model's hidden layer activations. Weoptimize for features whose existence causes the output predictions to changesubstantially, \ie generates a high impact. Moreover, we devise severalevaluation metrics that can be universally applied. Extensive experiments onreal and synthetic tasks demonstrate that our method achieves superior resultson {predictive impact}, usability, and faithfulness compared to the baselines.</description><author>Ruochen Zhao, Shafiq Joty, Yongjie Wang, Tan Wang</author><pubDate>Wed, 03 May 2023 15:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02160v1</guid></item><item><title>Shotgun crystal structure prediction using machine-learned formation energies</title><link>http://arxiv.org/abs/2305.02158v1</link><description>Stable or metastable crystal structures of assembled atoms can be predictedby finding the global or local minima of the energy surface with respect to theatomic configurations. Generally, this requires repeated first-principlesenergy calculations that are impractical for large systems, such as thosecontaining more than 30 atoms in the unit cell. Here, we have made significantprogress in solving the crystal structure prediction problem with a simple butpowerful machine-learning workflow; using a machine-learning surrogate forfirst-principles energy calculations, we performed non-iterative, single-shotscreening using a large library of virtually created crystal structures. Thepresent method relies on two key technical components: transfer learning, whichenables a highly accurate energy prediction of pre-relaxed crystalline statesgiven only a small set of training samples from first-principles calculations,and generative models to create promising and diverse crystal structures forscreening. Here, first-principles calculations were performed only to generatethe training samples, and for the optimization of a dozen or fewer finallynarrowed-down crystal structures. Our shotgun method was more than 5--10 timesless computationally demanding and achieved an outstanding prediction accuracythat was 2--6 times higher than that of the conventional methods that relyheavily on iterative first-principles calculations.</description><author>Chang Liu, Hiromasa Tamaki, Tomoyasu Yokoyama, Kensuke Wakasugi, Satoshi Yotsuhashi, Minoru Kusaba, Ryo Yoshida</author><pubDate>Wed, 03 May 2023 15:46:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02158v1</guid></item><item><title>Zero-Shot Listwise Document Reranking with a Large Language Model</title><link>http://arxiv.org/abs/2305.02156v1</link><description>Supervised ranking methods based on bi-encoder or cross-encoder architectureshave shown success in multi-stage text ranking tasks, but they require largeamounts of relevance judgments as training data. In this work, we proposeListwise Reranker with a Large Language Model (LRL), which achieves strongreranking effectiveness without using any task-specific training data.Different from the existing pointwise ranking methods, where documents arescored independently and ranked according to the scores, LRL directly generatesa reordered list of document identifiers given the candidate documents.Experiments on three TREC web search datasets demonstrate that LRL not onlyoutperforms zero-shot pointwise methods when reranking first-stage retrievalresults, but can also act as a final-stage reranker to improve the top-rankedresults of a pointwise method for improved efficiency. Additionally, we applyour approach to subsets of MIRACL, a recent multilingual retrieval dataset,with results showing its potential to generalize across different languages.</description><author>Xueguang Ma, Xinyu Zhang, Ronak Pradeep, Jimmy Lin</author><pubDate>Wed, 03 May 2023 15:45:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02156v1</guid></item><item><title>Improving Contrastive Learning of Sentence Embeddings from AI Feedback</title><link>http://arxiv.org/abs/2305.01918v1</link><description>Contrastive learning has become a popular approach in natural languageprocessing, particularly for the learning of sentence embeddings. However, thediscrete nature of natural language makes it difficult to ensure the quality ofpositive and negative sample pairs generated through data augmentation methods.Although supervised contrastive learning can produce more accurate sample pairswith human feedback labels, it still lacks fine-grained training signals. Inthis paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning ofsentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Ourmethod utilizes AI feedback from large pre-trained language models (LLMs) toconstruct sample pairs with fine-grained sample similarity scores to improvecontrastive learning. Besides, we combine human feedback and AI feedback toprovide better supervision signals for supervised contrastive learning ofsentence embeddings. Experimental results show that our method achievesstate-of-the-art performance on several semantic textual similarity (STS) andtransfer learning tasks compared to other unsupervised and supervisedcontrastive learning methods.</description><author>Qinyuan Cheng, Xiaogui Yang, Tianxiang Sun, Linyang Li, Xipeng Qiu</author><pubDate>Wed, 03 May 2023 07:26:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01918v1</guid></item><item><title>Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering</title><link>http://arxiv.org/abs/2212.10375v2</link><description>Despite the surprising few-shot performance of in-context learning (ICL), itis still a common practice to randomly sample examples to serve as context.This paper advocates a new principle for ICL: self-adaptive in-contextlearning. The self-adaption mechanism is introduced to help each sample find anin-context example permutation (i.e., selection and ordering) that can derivethe correct prediction, thus maximizing performance. To validate theeffectiveness of self-adaptive ICL, we propose a general select-then-rankframework and instantiate it with new selection and ranking algorithms. Uponextensive evaluation on eight different NLP datasets, our self-adaptive ICLmethod achieves a 40% relative improvement over the common practice setting.Further analysis reveals the enormous potential of self-adaptive ICL that itmight be able to close the gap between ICL and finetuning given more advancedalgorithms. Our code is released to facilitate future research in this area:https://github.com/Shark-NLP/self-adaptive-ICL</description><author>Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong</author><pubDate>Wed, 03 May 2023 15:43:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10375v2</guid></item><item><title>Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments</title><link>http://arxiv.org/abs/2212.09683v2</link><description>We present a human-in-the-loop evaluation framework for fact-checking novelmisinformation claims and identifying social media messages that support them.Our approach extracts check-worthy claims, which are aggregated and ranked forreview. Stance classifiers are then used to identify tweets supporting novelmisinformation claims, which are further reviewed to determine whether theyviolate relevant policies. To demonstrate the feasibility of our approach, wedevelop a baseline system based on modern NLP methods for human-in-the-loopfact-checking in the domain of COVID-19 treatments. Using our baseline system,we show that human fact-checkers can identify 124 tweets per hour that violateTwitter's policies on COVID-19 misinformation. We will make our code, data,baseline models, and detailed annotation guidelines available to support theevaluation of human-in-the-loop systems that identify novel misinformationdirectly from raw user-generated content.</description><author>Ethan Mendes, Yang Chen, Alan Ritter, Wei Xu</author><pubDate>Wed, 03 May 2023 15:37:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09683v2</guid></item><item><title>Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space</title><link>http://arxiv.org/abs/2305.02151v1</link><description>Prior research has investigated the impact of various linguistic features oncross-lingual transfer performance. In this study, we investigate the manner inwhich this effect can be mapped onto the representation space. While paststudies have focused on the impact on cross-lingual alignment in multilinguallanguage models during fine-tuning, this study examines the absolute evolutionof the respective language representation spaces produced by MLLMs. We place aspecific emphasis on the role of linguistic characteristics and investigatetheir inter-correlation with the impact on representation spaces andcross-lingual transfer performance. Additionally, this paper providespreliminary evidence of how these findings can be leveraged to enhance transferto linguistically distant languages.</description><author>Fred Philippy, Siwen Guo, Shohreh Haddadan</author><pubDate>Wed, 03 May 2023 15:33:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02151v1</guid></item><item><title>Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection</title><link>http://arxiv.org/abs/2302.03857v2</link><description>Adversarial contrastive learning (ACL) does not require expensive dataannotations but outputs a robust representation that withstands adversarialattacks and also generalizes to a wide range of downstream tasks. However, ACLneeds tremendous running time to generate the adversarial variants of alltraining data, which limits its scalability to large datasets. To speed up ACL,this paper proposes a robustness-aware coreset selection (RCS) method. RCS doesnot require label information and searches for an informative subset thatminimizes a representational divergence, which is the distance of therepresentation between natural data and their virtual adversarial variants. Thevanilla solution of RCS via traversing all possible subsets is computationallyprohibitive. Therefore, we theoretically transform RCS into a surrogate problemof submodular maximization, of which the greedy search is an efficient solutionwith an optimality guarantee for the original problem. Empirically, ourcomprehensive results corroborate that RCS can speed up ACL by a large marginwithout significantly hurting the robustness transferability. Notably, to thebest of our knowledge, we are the first to conduct ACL efficiently on thelarge-scale ImageNet-1K dataset to obtain an effective robust representationvia RCS.</description><author>Xilie Xu, Jingfeng Zhang, Feng Liu, Masashi Sugiyama, Mohan Kankanhalli</author><pubDate>Wed, 03 May 2023 15:32:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.03857v2</guid></item><item><title>Semi-Supervised Segmentation of Functional Tissue Units at the Cellular Level</title><link>http://arxiv.org/abs/2305.02148v1</link><description>We present a new method for functional tissue unit segmentation at thecellular level, which utilizes the latest deep learning semantic segmentationapproaches together with domain adaptation and semi-supervised learningtechniques. This approach allows for minimizing the domain gap, classimbalance, and captures settings influence between HPA and HubMAP datasets. Thepresented approach achieves comparable with state-of-the-art-result infunctional tissue unit segmentation at the cellular level. The source code isavailable at https://github.com/VSydorskyy/hubmap_2022_htt_solution</description><author>Volodymyr Sydorskyi, Igor Krashenyi, Denis Savka, Oleksandr Zarichkovyi</author><pubDate>Wed, 03 May 2023 15:29:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02148v1</guid></item><item><title>Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs</title><link>http://arxiv.org/abs/2302.02865v2</link><description>Contrastively trained encoders have recently been proven to invert thedata-generating process: they encode each input, e.g., an image, into the truelatent vector that generated the image (Zimmermann et al., 2021). However,real-world observations often have inherent ambiguities. For instance, imagesmay be blurred or only show a 2D view of a 3D object, so multiple latents couldhave generated them. This makes the true posterior for the latent vectorprobabilistic with heteroscedastic uncertainty. In this setup, we extend thecommon InfoNCE objective and encoders to predict latent distributions insteadof points. We prove that these distributions recover the correct posteriors ofthe data-generating process, including its level of aleatoric uncertainty, upto a rotation of the latent space. In addition to providing calibrateduncertainty estimates, these posteriors allow the computation of credibleintervals in image retrieval. They comprise images with the same latent as agiven query, subject to its uncertainty. Code is available athttps://github.com/mkirchhof/Probabilistic_Contrastive_Learning</description><author>Michael Kirchhof, Enkelejda Kasneci, Seong Joon Oh</author><pubDate>Wed, 03 May 2023 15:26:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.02865v2</guid></item><item><title>ProgDTD: Progressive Learned Image Compression with Double-Tail-Drop Training</title><link>http://arxiv.org/abs/2305.02145v1</link><description>Progressive compression allows images to start loading as low-resolutionversions, becoming clearer as more data is received. This increases userexperience when, for example, network connections are slow. Today, mostapproaches for image compression, both classical and learned ones, are designedto be non-progressive. This paper introduces ProgDTD, a training method thattransforms learned, non-progressive image compression approaches intoprogressive ones. The design of ProgDTD is based on the observation that theinformation stored within the bottleneck of a compression model commonly variesin importance. To create a progressive compression model, ProgDTD modifies thetraining steps to enforce the model to store the data in the bottleneck sortedby priority. We achieve progressive compression by transmitting the data inorder of its sorted index. ProgDTD is designed for CNN-based learned imagecompression models, does not need additional parameters, and has a customizablerange of progressiveness. For evaluation, we apply ProgDTDto the hyperpriormodel, one of the most common structures in learned image compression. Ourexperimental results show that ProgDTD performs comparably to itsnon-progressive counterparts and other state-of-the-art progressive models interms of MS-SSIM and accuracy.</description><author>Ali Hojjat, Janek Haberer, Olaf Landsiedel</author><pubDate>Wed, 03 May 2023 15:23:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02145v1</guid></item><item><title>Unsupervised out-of-distribution detection for safer robotically guided retinal microsurgery</title><link>http://arxiv.org/abs/2304.05040v2</link><description>Purpose: A fundamental problem in designing safe machine learning systems isidentifying when samples presented to a deployed model differ from thoseobserved at training time. Detecting so-called out-of-distribution (OoD)samples is crucial in safety-critical applications such as robotically guidedretinal microsurgery, where distances between the instrument and the retina arederived from sequences of 1D images that are acquired by aninstrument-integrated optical coherence tomography (iiOCT) probe. Methods: This work investigates the feasibility of using an OoD detector toidentify when images from the iiOCT probe are inappropriate for subsequentmachine learning-based distance estimation. We show how a simple OoD detectorbased on the Mahalanobis distance can successfully reject corrupted samplescoming from real-world ex vivo porcine eyes. Results: Our results demonstrate that the proposed approach can successfullydetect OoD samples and help maintain the performance of the downstream taskwithin reasonable levels. MahaAD outperformed a supervised approach trained onthe same kind of corruptions and achieved the best performance in detecting OoDcases from a collection of iiOCT samples with real-world corruptions. Conclusion: The results indicate that detecting corrupted iiOCT data throughOoD detection is feasible and does not need prior knowledge of possiblecorruptions. Consequently, MahaAD could aid in ensuring patient safety duringrobotically guided microsurgery by preventing deployed prediction models fromestimating distances that put the patient at risk.</description><author>Alain Jungo, Lars Doorenbos, Tommaso Da Col, Maarten Beelen, Martin Zinkernagel, Pablo Márquez-Neila, Raphael Sznitman</author><pubDate>Wed, 03 May 2023 15:23:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.05040v2</guid></item><item><title>GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions</title><link>http://arxiv.org/abs/2305.02143v1</link><description>In recent years, the increasing availability of personal data has raisedconcerns regarding privacy and security. One of the critical processes toaddress these concerns is data anonymization, which aims to protect individualprivacy and prevent the release of sensitive information. This research focuseson the importance of face anonymization. Therefore, we introduceGANonymization, a novel face anonymization framework with facialexpression-preserving abilities. Our approach is based on a high-levelrepresentation of a face which is synthesized into an anonymized version basedon a generative adversarial network (GAN). The effectiveness of the approachwas assessed by evaluating its performance in removing identifiable facialattributes to increase the anonymity of the given individual face.Additionally, the performance of preserving facial expressions was evaluated onseveral affect recognition datasets and outperformed the state-of-the-artmethod in most categories. Finally, our approach was analyzed for its abilityto remove various facial traits, such as jewelry, hair color, and multipleothers. Here, it demonstrated reliable performance in removing theseattributes. Our results suggest that GANonymization is a promising approach foranonymizing faces while preserving facial expressions.</description><author>Fabio Hellmann, Silvan Mertes, Mohamed Benouis, Alexander Hustinx, Tzung-Chien Hsieh, Cristina Conati, Peter Krawitz, Elisabeth André</author><pubDate>Wed, 03 May 2023 15:22:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02143v1</guid></item><item><title>A Curriculum View of Robust Loss Functions</title><link>http://arxiv.org/abs/2305.02139v1</link><description>Robust loss functions are designed to combat the adverse impacts of labelnoise, whose robustness is typically supported by theoretical bounds agnosticto the training dynamics. However, these bounds may fail to characterize theempirical performance as it remains unclear why robust loss functions canunderfit. We show that most loss functions can be rewritten into a form withthe same class-score margin and different sample-weighting functions. Theresulting curriculum view provides a straightforward analysis of the trainingdynamics, which helps attribute underfitting to diminished average sampleweights and noise robustness to larger weights for clean samples. We show thatsimple fixes to the curriculums can make underfitting robust loss functionscompetitive with the state-of-the-art, and training schedules can substantiallyaffect the noise robustness even with robust loss functions. Code is availableat \url{github}.</description><author>Zebin Ou, Yue Zhang</author><pubDate>Wed, 03 May 2023 15:13:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02139v1</guid></item><item><title>AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation</title><link>http://arxiv.org/abs/2305.01836v1</link><description>Segment Anything Model (SAM) has recently shown its powerful effectiveness invisual segmentation tasks. However, there is less exploration concerning howSAM works on audio-visual tasks, such as visual sound localization andsegmentation. In this work, we propose a simple yet effective audio-visuallocalization and segmentation framework based on the Segment Anything Model,namely AV-SAM, that can generate sounding object masks corresponding to theaudio. Specifically, our AV-SAM simply leverages pixel-wise audio-visual fusionacross audio features and visual features from the pre-trained image encoder inSAM to aggregate cross-modal representations. Then, the aggregated cross-modalfeatures are fed into the prompt encoder and mask decoder to generate the finalaudio-visual segmentation masks. We conduct extensive experiments onFlickr-SoundNet and AVSBench datasets. The results demonstrate that theproposed AV-SAM can achieve competitive performance on sounding objectlocalization and segmentation.</description><author>Shentong Mo, Yapeng Tian</author><pubDate>Wed, 03 May 2023 01:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01836v1</guid></item><item><title>Why Oatmeal is Cheap: Kolmogorov Complexity and Procedural Generation</title><link>http://arxiv.org/abs/2305.02131v1</link><description>Although procedural generation is popular among game developers, academicresearch on the topic has primarily focused on new applications, with someresearch into empirical analysis. In this paper we relate theoretical work ininformation theory to the generation of content for games. We prove that thereis a relationship between the Kolomogorov complexity of the most complexartifact a generator can produce, and the size of that generator's possibilityspace. In doing so, we identify the limiting relationship between the knowledgeencoded in a generator, the density of its output space, and the intricacy ofthe artifacts it produces. We relate our result to the experience of expertprocedural generator designers, and illustrate it with some examples.</description><author>Younès Rabii, Michael Cook</author><pubDate>Wed, 03 May 2023 14:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02131v1</guid></item><item><title>System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning</title><link>http://arxiv.org/abs/2305.02128v1</link><description>Evolutionary science provides evidence that diversity confers resilience.Yet, traditional multi-agent reinforcement learning techniques commonly enforcehomogeneity to increase training sample efficiency. When a system of learningagents is not constrained to homogeneous policies, individual agents maydevelop diverse behaviors, resulting in emergent complementarity that benefitsthe system. Despite this feat, there is a surprising lack of tools that measurebehavioral diversity in systems of learning agents. Such techniques would pavethe way towards understanding the impact of diversity in collective resilienceand performance. In this paper, we introduce System Neural Diversity (SND): ameasure of behavioral heterogeneity for multi-agent systems where agents havestochastic policies. %over a continuous state space. We discuss and prove itstheoretical properties, and compare it with alternate, state-of-the-artbehavioral diversity metrics used in cross-disciplinary domains. Throughsimulations of a variety of multi-agent tasks, we show how our metricconstitutes an important diagnostic tool to analyze latent properties ofbehavioral heterogeneity. By comparing SND with task reward in static tasks,where the problem does not change during training, we show that it is key tounderstanding the effectiveness of heterogeneous vs homogeneous agents. Indynamic tasks, where the problem is affected by repeated disturbances duringtraining, we show that heterogeneous agents are first able to learn specializedroles that allow them to cope with the disturbance, and then retain these roleswhen the disturbance is removed. SND allows a direct measurement of this latentresilience, while other proxies such as task performance (reward) fail to.</description><author>Matteo Bettini, Ajay Shankar, Amanda Prorok</author><pubDate>Wed, 03 May 2023 14:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02128v1</guid></item><item><title>Bicubic++: Slim, Slimmer, Slimmest -- Designing an Industry-Grade Super-Resolution Network</title><link>http://arxiv.org/abs/2305.02126v1</link><description>We propose a real-time and lightweight single-image super-resolution (SR)network named Bicubic++. Despite using spatial dimensions of the input imageacross the whole network, Bicubic++ first learns quick reversible downgradedand lower resolution features of the image in order to decrease the number ofcomputations. We also construct a training pipeline, where we apply anend-to-end global structured pruning of convolutional layers without usingmetrics like magnitude and gradient norms, and focus on optimizing the prunednetwork's PSNR on the validation set. Furthermore, we have experimentally shownthat the bias terms take considerable amount of the runtime while increasingPSNR marginally, hence we have also applied bias removal to the convolutionallayers. Our method adds ~1dB on Bicubic upscaling PSNR for all tested SRdatasets and runs with ~1.17ms on RTX3090 and ~2.9ms on RTX3070, for 720pinputs and 4K outputs, both in FP16 precision. Bicubic++ won NTIRE 2023 RTSRTrack 2 x3 SR competition and is the fastest among all competitive methods.Being almost as fast as the standard Bicubic upsampling method, we believe thatBicubic++ can set a new industry standard.</description><author>Bahri Batuhan Bilecen, Mustafa Ayazoglu</author><pubDate>Wed, 03 May 2023 14:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02126v1</guid></item><item><title>Automatically identifying ordinary differential equations from data</title><link>http://arxiv.org/abs/2304.11182v2</link><description>Discovering nonlinear differential equations that describe system dynamicsfrom empirical data is a fundamental challenge in contemporary science. Here,we propose a methodology to identify dynamical laws by integrating denoisingtechniques to smooth the signal, sparse regression to identify the relevantparameters, and bootstrap confidence intervals to quantify the uncertainty ofthe estimates. We evaluate our method on well-known ordinary differentialequations with an ensemble of random initial conditions, time series ofincreasing length, and varying signal-to-noise ratios. Our algorithmconsistently identifies three-dimensional systems, given moderately-sized timeseries and high levels of signal quality relative to background noise. Byaccurately discovering dynamical systems automatically, our methodology has thepotential to impact the understanding of complex systems, especially in fieldswhere data are abundant, but developing mathematical models demandsconsiderable effort.</description><author>Kevin Egan, Weizhen Li, Rui Carvalho</author><pubDate>Wed, 03 May 2023 14:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11182v2</guid></item><item><title>Pay More Attention to Relation Exploration for Knowledge Base Question Answering</title><link>http://arxiv.org/abs/2305.02118v1</link><description>Knowledge base question answering (KBQA) is a challenging task that aims toretrieve correct answers from large-scale knowledge bases. Existing attemptsprimarily focus on entity representation and final answer reasoning, whichresults in limited supervision for this task. Moreover, the relations, whichempirically determine the reasoning path selection, are not fully considered inrecent advancements. In this study, we propose a novel framework, RE-KBQA, thatutilizes relations in the knowledge base to enhance entity representation andintroduce additional supervision. We explore guidance from relations in threeaspects, including (1) distinguishing similar entities by employing avariational graph auto-encoder to learn relation importance; (2) exploringextra supervision by predicting relation distributions as soft labels with amulti-task scheme; (3) designing a relation-guided re-ranking algorithm forpost-processing. Experimental results on two benchmark datasets demonstrate theeffectiveness and superiority of our framework, improving the F1 score by 5.7%from 40.5 to 46.3 on CWQ and 5.8% from 62.8 to 68.5 on WebQSP, better or on parwith state-of-the-art methods.</description><author>Yong Cao, Xianzhi Li, Huiwen Liu, Wen Dai, Shuai Chen, Bin Wang, Min Chen, Daniel Hershcovich</author><pubDate>Wed, 03 May 2023 14:48:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02118v1</guid></item><item><title>The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share &amp; Requests</title><link>http://arxiv.org/abs/2304.14882v2</link><description>The ACM Multimedia 2023 Computational Paralinguistics Challenge addresses twodifferent problems for the first time in a research competition underwell-defined conditions: In the Emotion Share Sub-Challenge, a regression onspeech has to be made; and in the Requests Sub-Challenges, requests andcomplaints need to be detected. We describe the Sub-Challenges, baselinefeature extraction, and classifiers based on the usual ComPaRE features, theauDeep toolkit, and deep feature extraction from pre-trained CNNs using theDeepSpectRum toolkit; in addition, wav2vec2 models are used.</description><author>Björn W. Schuller, Anton Batliner, Shahin Amiriparian, Alexander Barnhill, Maurice Gerczuk, Andreas Triantafyllopoulos, Alice Baird, Panagiotis Tzirakis, Chris Gagne, Alan S. Cowen, Nikola Lackovic, Marie-José Caraty, Claude Montacié</author><pubDate>Mon, 01 May 2023 08:59:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14882v2</guid></item><item><title>Asymmetric quantum decision-making</title><link>http://arxiv.org/abs/2305.02117v1</link><description>Collective decision-making is crucial to information and communicationsystems. Decision conflicts among agents hinder the maximization of potentialutilities of the entire system. Quantum processes can realize conflict-freejoint decisions among two agents using the entanglement of photons or quantuminterference of orbital angular momentum (OAM). However, previous studies havealways presented symmetric resultant joint decisions. Although this propertyhelps maintain and preserve equality, it cannot resolve disparities. Globalchallenges, such as ethics and equity, are recognized in the field ofresponsible artificial intelligence as responsible research and innovationparadigm. Thus, decision-making systems must not only preserve existingequality but also tackle disparities. This study theoretically and numericallyinvestigates asymmetric collective decision-making using quantum interferenceof photons carrying OAM or entangled photons. Although asymmetry issuccessfully realized, a photon loss is inevitable in the proposed models. Theavailable range of asymmetry and method for obtaining the desired degree ofasymmetry are analytically formulated.</description><author>Honoka Shiratori, Hiroaki Shinkawa, André Röhm, Nicolas Chauvet, Etsuo Segawa, Jonathan Laurent, Guillaume Bachelier, Tomoki Yamagami, Ryoichi Horisaki, Makoto Naruse</author><pubDate>Wed, 03 May 2023 14:46:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02117v1</guid></item><item><title>Automatic Parameterization for Aerodynamic Shape Optimization via Deep Geometric Learning</title><link>http://arxiv.org/abs/2305.02116v1</link><description>We propose two deep learning models that fully automate shapeparameterization for aerodynamic shape optimization. Both models are optimizedto parameterize via deep geometric learning to embed human prior knowledge intolearned geometric patterns, eliminating the need for further handcrafting. TheLatent Space Model (LSM) learns a low-dimensional latent representation of anobject from a dataset of various geometries, while the Direct Mapping Model(DMM) builds parameterization on the fly using only one geometry of interest.We also devise a novel regularization loss that efficiently integratesvolumetric mesh deformation into the parameterization model. The modelsdirectly manipulate the high-dimensional mesh data by moving vertices. LSM andDMM are fully differentiable, enabling gradient-based, end-to-end pipelinedesign and plug-and-play deployment of surrogate models or adjoint solvers. Weperform shape optimization experiments on 2D airfoils and discuss theapplicable scenarios for the two models.</description><author>Zhen Wei, Pascal Fua, Michaël Bauerheim</author><pubDate>Wed, 03 May 2023 14:45:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02116v1</guid></item><item><title>Deep-Learning-based Vasculature Extraction for Single-Scan Optical Coherence Tomography Angiography</title><link>http://arxiv.org/abs/2304.08282v3</link><description>Optical coherence tomography angiography (OCTA) is a non-invasive imagingmodality that extends the functionality of OCT by extracting moving red bloodcell signals from surrounding static biological tissues. OCTA has emerged as avaluable tool for analyzing skin microvasculature, enabling more accuratediagnosis and treatment monitoring. Most existing OCTA extraction algorithms,such as speckle variance (SV)- and eigen-decomposition (ED)-OCTA, implement alarger number of repeated (NR) OCT scans at the same position to producehigh-quality angiography images. However, a higher NR requires a longer dataacquisition time, leading to more unpredictable motion artifacts. In thisstudy, we propose a vasculature extraction pipeline that uses only one-repeatedOCT scan to generate OCTA images. The pipeline is based on the proposedVasculature Extraction Transformer (VET), which leverages convolutionalprojection to better learn the spatial relationships between image patches. Incomparison to OCTA images obtained via the SV-OCTA (PSNR: 17.809) and ED-OCTA(PSNR: 18.049) using four-repeated OCT scans, OCTA images extracted by VETexhibit moderate quality (PSNR: 17.515) and higher image contrast whilereducing the required data acquisition time from ~8 s to ~2 s. Based on visualobservations, the proposed VET outperforms SV and ED algorithms when using neckand face OCTA data in areas that are challenging to scan. This study representsthat the VET has the capacity to extract vascularture images from a fastone-repeated OCT scan, facilitating accurate diagnosis for patients.</description><author>Jinpeng Liao, Tianyu Zhang, Yilong Zhang, Chunhui Li, Zhihong Huang</author><pubDate>Wed, 03 May 2023 14:37:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08282v3</guid></item><item><title>On the Convergence of SARSA with Linear Function Approximation</title><link>http://arxiv.org/abs/2202.06828v2</link><description>SARSA, a classical on-policy control algorithm for reinforcement learning, isknown to chatter when combined with linear function approximation: SARSA doesnot diverge but oscillates in a bounded region. However, little is known abouthow fast SARSA converges to that region and how large the region is. In thispaper, we make progress towards this open problem by showing the convergencerate of projected SARSA to a bounded region. Importantly, the region is muchsmaller than the region that we project into, provided that the magnitude ofthe reward is not too large. Existing works regarding the convergence of linearSARSA to a fixed point all require the Lipschitz constant of SARSA's policyimprovement operator to be sufficiently small; our analysis instead applies toarbitrary Lipschitz constants and thus characterizes the behavior of linearSARSA for a new regime.</description><author>Shangtong Zhang, Remi Tachet, Romain Laroche</author><pubDate>Wed, 03 May 2023 14:29:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.06828v2</guid></item><item><title>GPT-RE: In-context Learning for Relation Extraction using Large Language Models</title><link>http://arxiv.org/abs/2305.02105v1</link><description>In spite of the potential for ground-breaking achievements offered by largelanguage models (LLMs) (e.g., GPT-3), they still lag significantly behindfully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).This is due to the two major shortcomings of LLMs in RE: (1) low relevanceregarding entity and relation in retrieved demonstrations for in-contextlearning; and (2) the strong inclination to wrongly classify NULL examples intoother pre-defined labels. In this paper, we propose GPT-RE to bridge the gap between LLMs andfully-supervised baselines. GPT-RE successfully addresses the aforementionedissues by (1) incorporating task-specific entity representations indemonstration retrieval; and (2) enriching the demonstrations with goldlabel-induced reasoning logic. We evaluate GPT-RE on four widely-used REdatasets, and observe that GPT-RE achieves improvements over not only existingGPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-REachieves SOTA performances on the Semeval and SciERC datasets, and competitiveperformances on the TACRED and ACE05 datasets.</description><author>Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, Sadao Kurohashi</author><pubDate>Wed, 03 May 2023 14:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02105v1</guid></item><item><title>Background Knowledge Grounding for Readable, Relevant, and Factual Biomedical Lay Summaries</title><link>http://arxiv.org/abs/2305.02104v1</link><description>Communication of scientific findings to the public is important for keepingnon-experts informed of developments such as life-saving medical treatments.However, generating readable lay summaries from scientific documents ischallenging, and currently, these summaries suffer from critical factualerrors. One popular intervention for improving factuality is using additionalexternal knowledge to provide factual grounding. However, it is unclear howthese grounding sources should be retrieved, selected, or integrated, and howsupplementary grounding documents might affect the readability or relevance ofthe generated summaries. We develop a simple method for selecting groundingsources and integrating them with source documents. We then use the BioLaySumsummarization dataset to evaluate the effects of different grounding sources onsummary quality. We found that grounding source documents improves therelevance and readability of lay summaries but does not improve factuality oflay summaries. This continues to be true in zero-shot summarization settingswhere we hypothesized that grounding might be even more important for factuallay summaries.</description><author>Domenic Rosati</author><pubDate>Wed, 03 May 2023 14:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02104v1</guid></item><item><title>ScatterNeRF: Seeing Through Fog with Physically-Based Inverse Neural Rendering</title><link>http://arxiv.org/abs/2305.02103v1</link><description>Vision in adverse weather conditions, whether it be snow, rain, or fog ischallenging. In these scenarios, scattering and attenuation severly degradesimage quality. Handling such inclement weather conditions, however, isessential to operate autonomous vehicles, drones and robotic applications wherehuman performance is impeded the most. A large body of work explores removingweather-induced image degradations with dehazing methods. Most methods rely onsingle images as input and struggle to generalize from syntheticfully-supervised training approaches or to generate high fidelity results fromunpaired real-world datasets. With data as bottleneck and most of today'straining data relying on good weather conditions with inclement weather asoutlier, we rely on an inverse rendering approach to reconstruct the scenecontent. We introduce ScatterNeRF, a neural rendering method which adequatelyrenders foggy scenes and decomposes the fog-free background from theparticipating media-exploiting the multiple views from a short automotivesequence without the need for a large training data corpus. Instead, therendering approach is optimized on the multi-view scene itself, which can betypically captured by an autonomous vehicle, robot or drone during operation.Specifically, we propose a disentangled representation for the scatteringvolume and the scene objects, and learn the scene reconstruction withphysics-inspired losses. We validate our method by capturing multi-viewIn-the-Wild data and controlled captures in a large-scale fog chamber.</description><author>Andrea Ramazzina, Mario Bijelic, Stefanie Walz, Alessandro Sanvito, Dominik Scheuble, Felix Heide</author><pubDate>Wed, 03 May 2023 14:24:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02103v1</guid></item><item><title>What makes a good pause? Investigating the turn-holding effects of fillers</title><link>http://arxiv.org/abs/2305.02101v1</link><description>Filled pauses (or fillers), such as "uh" and "um", are frequent inspontaneous speech and can serve as a turn-holding cue for the listener,indicating that the current speaker is not done yet. In this paper, we use therecently proposed Voice Activity Projection (VAP) model, which is a deeplearning model trained to predict the dynamics of conversation, to analyse theeffects of filled pauses on the expected turn-hold probability. The resultsshow that, while filled pauses do indeed have a turn-holding effect, it isperhaps not as strong as could be expected, probably due to the redundancy ofother cues. We also find that the prosodic properties and position of thefiller has a significant effect on the turn-hold probability. However, contraryto what has been suggested in previous work, there is no difference between"uh" and "um" in this regard.</description><author>Bing'er Jiang, Erik Ekstedt, Gabriel Skantze</author><pubDate>Wed, 03 May 2023 14:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02101v1</guid></item><item><title>Single Image Deraining via Feature-based Deep Convolutional Neural Network</title><link>http://arxiv.org/abs/2305.02100v1</link><description>It is challenging to remove rain-steaks from a single rainy image because therain steaks are spatially varying in the rainy image. Although the CNN basedmethods have reported promising performance recently, there are still somedefects, such as data dependency and insufficient interpretation. A singleimage deraining algorithm based on the combination of data-driven andmodel-based approaches is proposed. Firstly, an improved weighted guided imagefilter (iWGIF) is used to extract high-frequency information and learn the rainsteaks to avoid interference from other information through the input image.Then, transfering the input image and rain steaks from the image domain to thefeature domain adaptively to learn useful features for high-quality imagederaining. Finally, networks with attention mechanisms is used to restorehigh-quality images from the latent features. Experiments show that theproposed algorithm significantly outperforms state-of-the-art methods in termsof both qualitative and quantitative measures.</description><author>Chaobing Zheng, Jun Jiang, Wenjian Ying, Shiqian Wu</author><pubDate>Wed, 03 May 2023 14:12:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02100v1</guid></item><item><title>Joint A-SNN: Joint Training of Artificial and Spiking Neural Networks via Self-Distillation and Weight Factorization</title><link>http://arxiv.org/abs/2305.02099v1</link><description>Emerged as a biology-inspired method, Spiking Neural Networks (SNNs) mimicthe spiking nature of brain neurons and have received lots of researchattention. SNNs deal with binary spikes as their activation and thereforederive extreme energy efficiency on hardware. However, it also leads to anintrinsic obstacle that training SNNs from scratch requires a re-definition ofthe firing function for computing gradient. Artificial Neural Networks (ANNs),however, are fully differentiable to be trained with gradient descent. In thispaper, we propose a joint training framework of ANN and SNN, in which the ANNcan guide the SNN's optimization. This joint framework contains two parts:First, the knowledge inside ANN is distilled to SNN by using multiple branchesfrom the networks. Second, we restrict the parameters of ANN and SNN, wherethey share partial parameters and learn different singular weights. Extensiveexperiments over several widely used network structures show that our methodconsistently outperforms many other state-of-the-art training methods. Forexample, on the CIFAR100 classification task, the spiking ResNet-18 modeltrained by our method can reach to 77.39% top-1 accuracy with only 4 timesteps.</description><author>Yufei Guo, Weihang Peng, Yuanpei Chen, Liwen Zhang, Xiaode Liu, Xuhui Huang, Zhe Ma</author><pubDate>Wed, 03 May 2023 14:12:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02099v1</guid></item><item><title>Removing Human Bottlenecks in Bird Classification Using Camera Trap Images and Deep Learning</title><link>http://arxiv.org/abs/2305.02097v1</link><description>Birds are important indicators for monitoring both biodiversity and habitathealth; they also play a crucial role in ecosystem management. Decline in birdpopulations can result in reduced eco-system services, including seeddispersal, pollination and pest control. Accurate and long-term monitoring ofbirds to identify species of concern while measuring the success ofconservation interventions is essential for ecologists. However, monitoring istime consuming, costly and often difficult to manage over long durations and atmeaningfully large spatial scales. Technology such as camera traps, acousticmonitors and drones provide methods for non-invasive monitoring. There are twomain problems with using camera traps for monitoring: a) cameras generate manyimages, making it difficult to process and analyse the data in a timely manner;and b) the high proportion of false positives hinders the processing andanalysis for reporting. In this paper, we outline an approach for overcomingthese issues by utilising deep learning for real-time classi-fication of birdspecies and automated removal of false positives in camera trap data. Imagesare classified in real-time using a Faster-RCNN architecture. Images aretransmitted over 3/4G cam-eras and processed using Graphical Processing Units(GPUs) to provide conservationists with key detection metrics thereforeremoving the requirement for manual observations. Our models achieved anaverage sensitivity of 88.79%, a specificity of 98.16% and accuracy of 96.71%.This demonstrates the effectiveness of using deep learning for automatic birdmonitoring.</description><author>Carl Chalmers, Paul Fergus, Serge Wich, Steven N Longmore, Naomi Davies Walsh, Philip Stephens, Chris Sutherland, Naomi Matthews, Jens Mudde, Amira Nuseibeh</author><pubDate>Wed, 03 May 2023 14:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02097v1</guid></item><item><title>Efficient Online Decision Tree Learning with Active Feature Acquisition</title><link>http://arxiv.org/abs/2305.02093v1</link><description>Constructing decision trees online is a classical machine learning problem.Existing works often assume that features are readily available for eachincoming data point. However, in many real world applications, both featurevalues and the labels are unknown a priori and can only be obtained at a cost.For example, in medical diagnosis, doctors have to choose which tests toperform (i.e., making costly feature queries) on a patient in order to make adiagnosis decision (i.e., predicting labels). We provide a fresh perspective totackle this practical challenge. Our framework consists of an active planningoracle embedded in an online learning scheme for which we investigate severalinformation acquisition functions. Specifically, we employ a surrogateinformation acquisition function based on adaptive submodularity to activelyquery feature values with a minimal cost, while using a posterior samplingscheme to maintain a low regret for online prediction. We demonstrate theefficiency and effectiveness of our framework via extensive experiments onvarious real-world datasets. Our framework also naturally adapts to thechallenging setting of online learning with concept drift and is shown to becompetitive with baseline models while being more flexible.</description><author>Arman Rahbar, Ziyu Ye, Yuxin Chen, Morteza Haghir Chehreghani</author><pubDate>Wed, 03 May 2023 13:56:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02093v1</guid></item><item><title>Efficient CNN-based Super Resolution Algorithms for mmWave Mobile Radar Imaging</title><link>http://arxiv.org/abs/2305.02092v1</link><description>In this paper, we introduce an innovative super resolution approach toemerging modes of near-field synthetic aperture radar (SAR) imaging. Recentresearch extends convolutional neural network (CNN) architectures from theoptical to the electromagnetic domain to achieve super resolution on imagesgenerated from radar signaling. Specifically, near-field synthetic apertureradar (SAR) imaging, a method for generating high-resolution images by scanninga radar across space to create a synthetic aperture, is of interest due to itshigh-fidelity spatial sensing capability, low cost devices, and largeapplication space. Since SAR imaging requires large aperture sizes to achievehigh resolution, super-resolution algorithms are valuable for manyapplications. Freehand smartphone SAR, an emerging sensing modality, requiresirregular SAR apertures in the near-field and computation on mobile devices.Achieving efficient high-resolution SAR images from irregularly sampled datacollected by freehand motion of a smartphone is a challenging task. In thispaper, we propose a novel CNN architecture to achieve SAR imagesuper-resolution for mobile applications by employing state-of-the-art SARprocessing and deep learning techniques. The proposed algorithm is verified viasimulation and an empirical study. Our algorithm demonstrates high-efficiencyand high-resolution radar imaging for near-field scenarios with irregularscanning geometries.</description><author>Christos Vasileiou, Josiah W. Smith, Shiva Thiagarajan, Matthew Nigh, Yiorgos Makris, Murat Torlak</author><pubDate>Wed, 03 May 2023 13:54:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02092v1</guid></item><item><title>Understanding cirrus clouds using explainable machine learning</title><link>http://arxiv.org/abs/2305.02090v1</link><description>Cirrus clouds are key modulators of Earth's climate. Their dependencies onmeteorological and aerosol conditions are among the largest uncertainties inglobal climate models. This work uses three years of satellite and reanalysisdata to study the link between cirrus drivers and cloud properties. We use agradient-boosted machine learning model and a Long Short-Term Memory (LSTM)network with an attention layer to predict the ice water content and icecrystal number concentration. The models show that meteorological and aerosolconditions can predict cirrus properties with $R^2 = 0.49$. Featureattributions are calculated with SHapley Additive exPlanations (SHAP) toquantify the link between meteorological and aerosol conditions and cirrusproperties. For instance, the minimum concentration of supermicron-sized dustparticles required to cause a decrease in ice crystal number concentrationpredictions is $2 \times 10^{-4}$ mg m\textsuperscript{-3}. The last 15 hoursbefore the observation predict all cirrus properties.</description><author>Kai Jeggle, David Neubauer, Gustau Camps-Valls, Ulrike Lohmann</author><pubDate>Wed, 03 May 2023 13:50:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02090v1</guid></item><item><title>A Categorical Framework of General Intelligence</title><link>http://arxiv.org/abs/2303.04571v2</link><description>Can machines think? Since Alan Turing asked this question in 1950, nobody isable to give a direct answer, due to the lack of solid mathematical foundationsfor general intelligence. In this paper, we introduce a categorical frameworktowards this goal, with two main results. First, we investigate objectrepresentation through presheaves, introducing the notion of self-stateawareness as a categorical analogue to self-consciousness, along withcorresponding algorithms for its enforcement and evaluation. Secondly, weextend object representation to scenario representation using diagrams andlimits, which then become building blocks for mathematical modeling,interpretability and AI safety. As an ancillary result, our frameworkintroduces various categorical invariance properties that can serve as thealignment signals for model training.</description><author>Yang Yuan</author><pubDate>Wed, 03 May 2023 13:50:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.04571v2</guid></item><item><title>Discrepancy-Guided Reconstruction Learning for Image Forgery Detection</title><link>http://arxiv.org/abs/2304.13349v2</link><description>In this paper, we propose a novel image forgery detection paradigm forboosting the model learning capacity on both forgery-sensitive and genuinecompact visual patterns. Compared to the existing methods that only focus onthe discrepant-specific patterns (\eg, noises, textures, and frequencies), ourmethod has a greater generalization. Specifically, we first propose aDiscrepancy-Guided Encoder (DisGE) to extract forgery-sensitive visualpatterns. DisGE consists of two branches, where the mainstream backbone branchis used to extract general semantic features, and the accessorial discrepantexternal attention branch is used to extract explicit forgery cues. Besides, aDouble-Head Reconstruction (DouHR) module is proposed to enhance genuinecompact visual patterns in different granular spaces. Under DouHR, we furtherintroduce a Discrepancy-Aggregation Detector (DisAD) to aggregate these genuinecompact visual patterns, such that the forgery detection capability on unknownpatterns can be improved. Extensive experimental results on four challengingdatasets validate the effectiveness of our proposed method againststate-of-the-art competitors.</description><author>Zenan Shi, Haipeng Chen, Long Chen, Dong Zhang</author><pubDate>Wed, 03 May 2023 13:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13349v2</guid></item><item><title>Attention Based Feature Fusion For Multi-Agent Collaborative Perception</title><link>http://arxiv.org/abs/2305.02061v1</link><description>In the domain of intelligent transportation systems (ITS), collaborativeperception has emerged as a promising approach to overcome the limitations ofindividual perception by enabling multiple agents to exchange information, thusenhancing their situational awareness. Collaborative perception overcomes thelimitations of individual sensors, allowing connected agents to perceiveenvironments beyond their line-of-sight and field of view. However, thereliability of collaborative perception heavily depends on the data aggregationstrategy and communication bandwidth, which must overcome the challenges posedby limited network resources. To improve the precision of object detection andalleviate limited network resources, we propose an intermediate collaborativeperception solution in the form of a graph attention network (GAT). Theproposed approach develops an attention-based aggregation strategy to fuseintermediate representations exchanged among multiple connected agents. Thisapproach adaptively highlights important regions in the intermediate featuremaps at both the channel and spatial levels, resulting in improved objectdetection precision. We propose a feature fusion scheme using attention-basedarchitectures and evaluate the results quantitatively in comparison to otherstate-of-the-art collaborative perception approaches. Our proposed approach isvalidated using the V2XSim dataset. The results of this work demonstrate theefficacy of the proposed approach for intermediate collaborative perception inimproving object detection average precision while reducing network resourceusage.</description><author>Ahmed N. Ahmed, Siegfried Mercelis, Ali Anwar</author><pubDate>Wed, 03 May 2023 13:06:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02061v1</guid></item><item><title>Streaming Algorithms for High-Dimensional Robust Statistics</title><link>http://arxiv.org/abs/2204.12399v2</link><description>We study high-dimensional robust statistics tasks in the streaming model. Arecent line of work obtained computationally efficient algorithms for a rangeof high-dimensional robust estimation tasks. Unfortunately, all previousalgorithms require storing the entire dataset, incurring memory at leastquadratic in the dimension. In this work, we develop the first efficientstreaming algorithms for high-dimensional robust statistics with near-optimalmemory requirements (up to logarithmic factors). Our main result is for thetask of high-dimensional robust mean estimation in (a strengthening of) Huber'scontamination model. We give an efficient single-pass streaming algorithm forthis task with near-optimal error guarantees and space complexity nearly-linearin the dimension. As a corollary, we obtain streaming algorithms withnear-optimal space complexity for several more complex tasks, including robustcovariance estimation, robust regression, and more generally robust stochasticoptimization.</description><author>Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia, Thanasis Pittas</author><pubDate>Wed, 03 May 2023 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.12399v2</guid></item></channel></rss>