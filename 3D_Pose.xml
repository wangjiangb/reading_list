<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 10 Mar 2024 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting</title><link>http://arxiv.org/abs/2402.18330v1</link><description>We present EgoTAP, a heatmap-to-3D pose lifting method for highly accuratestereo egocentric 3D pose estimation. Severe self-occlusion and out-of-viewlimbs in egocentric camera views make accurate pose estimation a challengingproblem. To address the challenge, prior methods employ jointheatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3Dpose conversion still remains an inaccurate process. We propose a novelheatmap-to-3D lifting method composed of the Grid ViT Encoder and thePropagation Network. The Grid ViT Encoder summarizes joint heatmaps intoeffective feature embedding using self-attention. Then, the Propagation Networkestimates the 3D pose by utilizing skeletal information to better estimate theposition of obscure joints. Our method significantly outperforms the previousstate-of-the-art qualitatively and quantitatively demonstrated by a 23.9\%reduction of error in an MPJPE metric. Our source code is available in GitHub.</description><author>Taeho Kang, Youngki Lee</author><pubDate>Wed, 28 Feb 2024 13:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18330v1</guid></item><item><title>3D Human Pose Estimation Based on 2D-3D Consistency with Synchronized Adversarial Training</title><link>http://arxiv.org/abs/2106.04274v4</link><description>3D human pose estimation from a single image is still a challenging problemdespite the large amount of work that has been performed in this field.Generally, most methods directly use neural networks and ignore certainconstraints (e.g., reprojection constraints, joint angle, and bone lengthconstraints). While a few methods consider these constraints but train thenetwork separately, they cannot effectively solve the depth ambiguity problem.In this paper, we propose a GAN-based model for 3D human pose estimation, inwhich a reprojection network is employed to learn the mapping of thedistribution from 3D poses to 2D poses, and a discriminator is employed for2D-3D consistency discrimination. We adopt a novel strategy to synchronouslytrain the generator, the reprojection network and the discriminator.Furthermore, inspired by the typical kinematic chain space (KCS) matrix, weintroduce a weighted KCS matrix and take it as one of the discriminator'sinputs to impose joint angle and bone length constraints. The experimentalresults on Human3.6M show that our method significantly outperformsstate-of-the-art methods in most cases.</description><author>Yicheng Deng, Cheng Sun, Yongqi Sun, Jiahui Zhu</author><pubDate>Tue, 05 Mar 2024 10:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.04274v4</guid></item><item><title>Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser</title><link>http://arxiv.org/abs/2403.04444v1</link><description>Recently, diffusion-based methods for monocular 3D human pose estimation haveachieved state-of-the-art (SOTA) performance by directly regressing the 3Djoint coordinates from the 2D pose sequence. Although some methods decomposethe task into bone length and bone direction prediction based on the humananatomical skeleton to explicitly incorporate more human body priorconstraints, the performance of these methods is significantly lower than thatof the SOTA diffusion-based methods. This can be attributed to the treestructure of the human skeleton. Direct application of the disentangled methodcould amplify the accumulation of hierarchical errors, propagating through eachhierarchy. Meanwhile, the hierarchical information has not been fully exploredby the previous methods. To address these problems, a DisentangledDiffusion-based 3D Human Pose Estimation method with Hierarchical Spatial andTemporal Denoiser is proposed, termed DDHPose. In our approach: (1) Wedisentangle the 3D pose and diffuse the bone length and bone direction duringthe forward process of the diffusion model to effectively model the human poseprior. A disentanglement loss is proposed to supervise diffusion modellearning. (2) For the reverse process, we propose Hierarchical Spatial andTemporal Denoiser (HSTDenoiser) to improve the hierarchical modeling of eachjoint. Our HSTDenoiser comprises two components: the Hierarchical-RelatedSpatial Transformer (HRST) and the Hierarchical-Related Temporal Transformer(HRTT). HRST exploits joint spatial information and the influence of the parentjoint on each joint for spatial modeling, while HRTT utilizes information fromboth the joint and its hierarchical adjacent joints to explore the hierarchicaltemporal correlations among joints.</description><author>Qingyuan Cai, Xuecai Hu, Saihui Hou, Li Yao, Yongzhen Huang</author><pubDate>Thu, 07 Mar 2024 12:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04444v1</guid></item><item><title>Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey</title><link>http://arxiv.org/abs/2402.18844v1</link><description>3D human pose estimation and mesh recovery have attracted widespread researchinterest in many areas, such as computer vision, autonomous driving, androbotics. Deep learning on 3D human pose estimation and mesh recovery hasrecently thrived, with numerous methods proposed to address different problemsin this area. In this paper, to stimulate future research, we present acomprehensive review of recent progress over the past five years in deeplearning methods for this area by delving into over 200 references. To the bestof our knowledge, this survey is arguably the first to comprehensively coverdeep learning methods for 3D human pose estimation, including bothsingle-person and multi-person approaches, as well as human mesh recovery,encompassing methods based on explicit models and implicit representations. Wealso present comparative results on several publicly available datasets,together with insightful observations and inspiring future research directions.A regularly updated project page can be found athttps://github.com/liuyangme/SOTA-3DHPE-HMR.</description><author>Yang Liu, Changzhen Qiu, Zhiyong Zhang</author><pubDate>Thu, 29 Feb 2024 04:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18844v1</guid></item><item><title>Occlusion Resilient 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2402.11036v1</link><description>Occlusions remain one of the key challenges in 3D body pose estimation fromsingle-camera video sequences. Temporal consistency has been extensively usedto mitigate their impact but the existing algorithms in the literature do notexplicitly model them. Here, we apply this by representing the deforming body as a spatio-temporalgraph. We then introduce a refinement network that performs graph convolutionsover this graph to output 3D poses. To ensure robustness to occlusions, wetrain this network with a set of binary masks that we use to disable some ofthe edges as in drop-out techniques. In effect, we simulate the fact that some joints can be hidden for periods oftime and train the network to be immune to that. We demonstrate theeffectiveness of this approach compared to state-of-the-art techniques thatinfer poses from single-camera sequences.</description><author>Soumava Kumar Roy, Ilia Badanin, Sina Honari, Pascal Fua</author><pubDate>Fri, 16 Feb 2024 19:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11036v1</guid></item><item><title>Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2403.04381v1</link><description>The pursuit of accurate 3D hand pose estimation stands as a keystone forunderstanding human activity in the realm of egocentric vision. The majority ofexisting estimation methods still rely on single-view images as input, leadingto potential limitations, e.g., limited field-of-view and ambiguity in depth.To address these problems, adding another camera to better capture the shape ofhands is a practical direction. However, existing multi-view hand poseestimation methods suffer from two main drawbacks: 1) Requiring multi-viewannotations for training, which are expensive. 2) During testing, the modelbecomes inapplicable if camera parameters/layout are not the same as those usedin training. In this paper, we propose a novel Single-to-Dual-view adaptation(S2DHand) solution that adapts a pre-trained single-view estimator to dualviews. Compared with existing multi-view training methods, 1) our adaptationprocess is unsupervised, eliminating the need for multi-view annotation. 2)Moreover, our method can handle arbitrary dual-view pairs with unknown cameraparameters, making the model applicable to diverse camera settings.Specifically, S2DHand is built on certain stereo constraints, includingpair-wise cross-view consensus and invariance of transformation between bothviews. These two stereo constraints are used in a complementary manner togenerate pseudo-labels, allowing reliable adaptation. Evaluation results revealthat S2DHand achieves significant improvements on arbitrary camera pairs underboth in-dataset and cross-dataset settings, and outperforms existing adaptationmethods with leading performance. Project page:https://github.com/MickeyLLG/S2DHand.</description><author>Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato</author><pubDate>Thu, 07 Mar 2024 10:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04381v1</guid></item><item><title>NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images</title><link>http://arxiv.org/abs/2402.18196v1</link><description>Human pose estimation (HPE) in the top-view using fisheye cameras presents apromising and innovative application domain. However, the availability ofdatasets capturing this viewpoint is extremely limited, especially those withhigh-quality 2D and 3D keypoint annotations. Addressing this gap, we leveragethe capabilities of Neural Radiance Fields (NeRF) technique to establish acomprehensive pipeline for generating human pose datasets from existing 2D and3D datasets, specifically tailored for the top-view fisheye perspective.Through this pipeline, we create a novel dataset NToP570K (NeRF-poweredTop-view human Pose dataset for fisheye cameras with over 570 thousand images),and conduct an extensive evaluation of its efficacy in enhancing neuralnetworks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-Bmodel achieves an improvement in AP of 33.3 % on our validation set for 2D HPEafter finetuning on our training set. A similarly finetuned HybrIK-Transformermodel gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.</description><author>Jingrui Yu, Dipankar Nandi, Roman Seidel, Gangolf Hirtz</author><pubDate>Wed, 28 Feb 2024 09:36:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18196v1</guid></item><item><title>LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network</title><link>http://arxiv.org/abs/2306.12525v2</link><description>Due to the difficulty of acquiring large-scale 3D human keypoint annotation,previous methods for 3D human pose estimation (HPE) have often relied on 2Dimage features and sequential 2D annotations. Furthermore, the training ofthese networks typically assumes the prediction of a human bounding box and theaccurate alignment of 3D point clouds with 2D images, making direct applicationin real-world scenarios challenging. In this paper, we present the 1stframework for end-to-end 3D human pose estimation, named LPFormer, which usesonly LiDAR as its input along with its corresponding 3D annotations. LPFormerconsists of two stages: firstly, it identifies the human bounding box andextracts multi-level feature representations, and secondly, it utilizes atransformer-based network to predict human keypoints based on these features.Our method demonstrates that 3D HPE can be seamlessly integrated into a strongLiDAR perception network and benefit from the features extracted by thenetwork. Experimental results on the Waymo Open Dataset demonstrate thestate-of-the-art performance, and improvements even compared to previousmulti-modal solutions.</description><author>Dongqiangzi Ye, Yufei Xie, Weijia Chen, Zixiang Zhou, Lingting Ge, Hassan Foroosh</author><pubDate>Sat, 02 Mar 2024 22:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12525v2</guid></item><item><title>That's My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation</title><link>http://arxiv.org/abs/2403.04755v1</link><description>This paper is about 3D pose estimation on LiDAR scans with extremely minimalstorage requirements to enable scalable mapping and localisation. We achievethis by clustering all points of segmented scans into semantic objects andrepresenting them only with their respective centroid and semantic class. Inthis way, each LiDAR scan is reduced to a compact collection of four-numbervectors. This abstracts away important structural information from the scenes,which is crucial for traditional registration approaches. To mitigate this, weintroduce an object-matching network based on self- and cross-correlation thatcaptures geometric and semantic relationships between entities. The respectivematches allow us to recover the relative transformation between scans throughweighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus(RANSAC). We demonstrate that such representation is sufficient for metriclocalisation by registering point clouds taken under different viewpoints onthe KITTI dataset, and at different periods of time localising between KITTIand KITTI-360. We achieve accurate metric estimates comparable withstate-of-the-art methods with almost half the representation size, specifically1.33 kB on average.</description><author>Georgi Pramatarov, Matthew Gadd, Paul Newman, Daniele De Martini</author><pubDate>Thu, 07 Mar 2024 18:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04755v1</guid></item><item><title>ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living</title><link>http://arxiv.org/abs/2402.17758v1</link><description>Hand-Object Interactions (HOIs) are conditioned on spatial and temporalcontexts like surrounding objects, pre- vious actions, and future intents (forexample, grasping and handover actions vary greatly based on objects proximityand trajectory obstruction). However, existing datasets for 4D HOI (3D HOI overtime) are limited to one subject inter- acting with one object only. Thisrestricts the generalization of learning-based HOI methods trained on thosedatasets. We introduce ADL4D, a dataset of up to two subjects inter- actingwith different sets of objects performing Activities of Daily Living (ADL) likebreakfast or lunch preparation ac- tivities. The transition between multipleobjects to complete a certain task over time introduces a unique contextlacking in existing datasets. Our dataset consists of 75 sequences with a totalof 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained actionannotations. We develop an automatic system for multi-view multi-hand 3D posean- notation capable of tracking hand poses over time. We inte- grate and testit against publicly available datasets. Finally, we evaluate our dataset on thetasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).</description><author>Marsil Zakour, Partha Pratim Nath, Ludwig Lohmer, Emre Faik Gökçe, Martin Piccolrovazzi, Constantin Patsch, Yuankai Wu, Rahul Chaudhari, Eckehard Steinbach</author><pubDate>Tue, 27 Feb 2024 18:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17758v1</guid></item><item><title>Dense Matchers for Dense Tracking</title><link>http://arxiv.org/abs/2402.11287v1</link><description>Optical flow is a useful input for various applications, including 3Dreconstruction, pose estimation, tracking, and structure-from-motion. Despiteits utility, the field of dense long-term tracking, especially over widebaselines, has not been extensively explored. This paper extends the concept ofcombining multiple optical flows over logarithmically spaced intervals asproposed by MFT. We demonstrate the compatibility of MFT with different opticalflow networks, yielding results that surpass their individual performance.Moreover, we present a simple yet effective combination of these networkswithin the MFT framework. This approach proves to be competitive with moresophisticated, non-causal methods in terms of position prediction accuracy,highlighting the potential of MFT in enhancing long-term tracking applications.</description><author>Tomáš Jelínek, Jonáš Šerých, Jiří Matas</author><pubDate>Sat, 17 Feb 2024 14:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11287v1</guid></item><item><title>CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization</title><link>http://arxiv.org/abs/2402.17214v2</link><description>In the field of digital content creation, generating high-quality 3Dcharacters from single images is challenging, especially given the complexitiesof various body poses and the issues of self-occlusion and pose ambiguity. Inthis paper, we present CharacterGen, a framework developed to efficientlygenerate 3D characters. CharacterGen introduces a streamlined generationpipeline along with an image-conditioned multi-view diffusion model. This modeleffectively calibrates input poses to a canonical form while retaining keyattributes of the input image, thereby addressing the challenges posed bydiverse poses. A transformer-based, generalizable sparse-view reconstructionmodel is the other core component of our approach, facilitating the creation ofdetailed 3D models from multi-view images. We also adopt atexture-back-projection strategy to produce high-quality texture maps.Additionally, we have curated a dataset of anime characters, rendered inmultiple poses and views, to train and evaluate our model. Our approach hasbeen thoroughly evaluated through quantitative and qualitative experiments,showing its proficiency in generating 3D characters with high-quality shapesand textures, ready for downstream applications such as rigging and animation.</description><author>Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu</author><pubDate>Wed, 28 Feb 2024 08:13:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17214v2</guid></item><item><title>Lester: rotoscope animation through video object segmentation and tracking</title><link>http://arxiv.org/abs/2402.09883v1</link><description>This article introduces Lester, a novel method to automatically synthetiseretro-style 2D animations from videos. The method approaches the challengemainly as an object segmentation and tracking problem. Video frames areprocessed with the Segment Anything Model (SAM) and the resulting masks aretracked through subsequent frames with DeAOT, a method of hierarchicalpropagation for semi-supervised video object segmentation. The geometry of themasks' contours is simplified with the Douglas-Peucker algorithm. Finally,facial traits, pixelation and a basic shadow effect can be optionally added.The results show that the method exhibits an excellent temporal consistency andcan correctly process videos with different poses and appearances, dynamicshots, partial shots and diverse backgrounds. The proposed method provides amore simple and deterministic approach than diffusion models basedvideo-to-video translation pipelines, which suffer from temporal consistencyproblems and do not cope well with pixelated and schematic outputs. The methodis also much most practical than techniques based on 3D human pose estimation,which require custom handcrafted 3D models and are very limited with respect tothe type of scenes they can process.</description><author>Ruben Tous</author><pubDate>Thu, 15 Feb 2024 11:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09883v1</guid></item><item><title>Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach</title><link>http://arxiv.org/abs/2402.19062v1</link><description>To facilitate diagnosis on cardiac ultrasound (US), clinical practice hasestablished several standard views of the heart, which serve as referencepoints for diagnostic measurements and define viewports from which images areacquired. Automatic view recognition involves grouping those images intoclasses of standard views. Although deep learning techniques have beensuccessful in achieving this, they still struggle with fully verifying thesuitability of an image for specific measurements due to factors like thecorrect location, pose, and potential occlusions of cardiac structures. Ourapproach goes beyond view classification and incorporates a 3D meshreconstruction of the heart that enables several more downstream tasks, likesegmentation and pose estimation. In this work, we explore learning 3D heartmeshes via graph convolutions, using similar techniques to learn 3D meshes innatural images, such as human pose estimation. As the availability of fullyannotated 3D images is limited, we generate synthetic US images from 3D meshesby training an adversarial denoising diffusion model. Experiments wereconducted on synthetic and clinical cases for view recognition and structuredetection. The approach yielded good performance on synthetic images and,despite being exclusively trained on synthetic data, it already showedpotential when applied to clinical images. With this proof-of-concept, we aimto demonstrate the benefits of graphs to improve cardiac view recognition thatcan ultimately lead to better efficiency in cardiac diagnosis.</description><author>Sarina Thomas, Cristiana Tiago, Børge Solli Andreassen, Svein-Arne Aase, Jurica Sprem, Erik Steen, Anne Solberg, Guy Ben-Yosef</author><pubDate>Thu, 29 Feb 2024 11:45:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19062v1</guid></item><item><title>Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach</title><link>http://arxiv.org/abs/2402.19062v2</link><description>To facilitate diagnosis on cardiac ultrasound (US), clinical practice hasestablished several standard views of the heart, which serve as referencepoints for diagnostic measurements and define viewports from which images areacquired. Automatic view recognition involves grouping those images intoclasses of standard views. Although deep learning techniques have beensuccessful in achieving this, they still struggle with fully verifying thesuitability of an image for specific measurements due to factors like thecorrect location, pose, and potential occlusions of cardiac structures. Ourapproach goes beyond view classification and incorporates a 3D meshreconstruction of the heart that enables several more downstream tasks, likesegmentation and pose estimation. In this work, we explore learning 3D heartmeshes via graph convolutions, using similar techniques to learn 3D meshes innatural images, such as human pose estimation. As the availability of fullyannotated 3D images is limited, we generate synthetic US images from 3D meshesby training an adversarial denoising diffusion model. Experiments wereconducted on synthetic and clinical cases for view recognition and structuredetection. The approach yielded good performance on synthetic images and,despite being exclusively trained on synthetic data, it already showedpotential when applied to clinical images. With this proof-of-concept, we aimto demonstrate the benefits of graphs to improve cardiac view recognition thatcan ultimately lead to better efficiency in cardiac diagnosis.</description><author>Sarina Thomas, Cristiana Tiago, Børge Solli Andreassen, Svein Arne Aase, Jurica Šprem, Erik Steen, Anne Solberg, Guy Ben-Yosef</author><pubDate>Fri, 01 Mar 2024 08:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19062v2</guid></item><item><title>MogaNet: Multi-order Gated Aggregation Network</title><link>http://arxiv.org/abs/2211.03295v3</link><description>By contextualizing the kernel as global as possible, Modern ConvNets haveshown great potential in computer vision tasks. However, recent progress on\textit{multi-order game-theoretic interaction} within deep neural networks(DNNs) reveals the representation bottleneck of modern ConvNets, where theexpressive interactions have not been effectively encoded with the increasedkernel size. To tackle this challenge, we propose a new family of modernConvNets, dubbed MogaNet, for discriminative visual representation learning inpure ConvNet-based models with favorable complexity-performance trade-offs.MogaNet encapsulates conceptually simple yet effective convolutions and gatedaggregation into a compact module, where discriminative features areefficiently gathered and contextualized adaptively. MogaNet exhibits greatscalability, impressive efficiency of parameters, and competitive performancecompared to state-of-the-art ViTs and ConvNets on ImageNet and variousdownstream vision benchmarks, including COCO object detection, ADE20K semanticsegmentation, 2D\&amp;3D human pose estimation, and video prediction. Notably,MogaNet hits 80.0\% and 87.8\% accuracy with 5.2M and 181M parameters onImageNet-1K, outperforming ParC-Net and ConvNeXt-L, while saving 59\% FLOPs and17M parameters, respectively. The source code is available at\url{https://github.com/Westlake-AI/MogaNet}.</description><author>Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Li</author><pubDate>Fri, 16 Feb 2024 14:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03295v3</guid></item><item><title>MagicDrive: Street View Generation with Diverse 3D Geometry Control</title><link>http://arxiv.org/abs/2310.02601v6</link><description>Recent advancements in diffusion models have significantly enhanced the datasynthesis with 2D control. Yet, precise 3D control in street view generation,crucial for 3D perception tasks, remains elusive. Specifically, utilizingBird's-Eye View (BEV) as the primary condition often leads to challenges ingeometry control (e.g., height), affecting the representation of object shapes,occlusion patterns, and road surface elevations, all of which are essential toperception data synthesis, especially for 3D object detection tasks. In thispaper, we introduce MagicDrive, a novel street view generation framework,offering diverse 3D geometry controls including camera poses, road maps, and 3Dbounding boxes, together with textual descriptions, achieved through tailoredencoding strategies. Besides, our design incorporates a cross-view attentionmodule, ensuring consistency across multiple camera views. With MagicDrive, weachieve high-fidelity street-view image &amp; video synthesis that captures nuanced3D geometry and various scene descriptions, enhancing tasks like BEVsegmentation and 3D object detection.</description><author>Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, Qiang Xu</author><pubDate>Fri, 01 Mar 2024 06:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02601v6</guid></item><item><title>MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction</title><link>http://arxiv.org/abs/2402.03762v3</link><description>Neural implicit representations have recently been demonstrated in manyfields including Simultaneous Localization And Mapping (SLAM). Current neuralSLAM can achieve ideal results in reconstructing bounded scenes, but thisrelies on the input of RGB-D images. Neural-based SLAM based only on RGB imagesis unable to reconstruct the scale of the scene accurately, and it also suffersfrom scale drift due to errors accumulated during tracking. To overcome theselimitations, we present MoD-SLAM, a monocular dense mapping method that allowsglobal pose optimization and 3D reconstruction in real-time in unboundedscenes. Optimizing scene reconstruction by monocular depth estimation and usingloop closure detection to update camera pose enable detailed and precisereconstruction on large scenes. Compared to previous work, our approach is morerobust, scalable and versatile. Our experiments demonstrate that MoD-SLAM hasmore excellent mapping performance than prior neural SLAM methods, especiallyin large borderless scenes.</description><author>Heng Zhou, Zhetao Guo, Shuhong Liu, Lechen Zhang, Qihao Wang, Yuxiang Ren, Mingrui Li</author><pubDate>Wed, 21 Feb 2024 17:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03762v3</guid></item><item><title>Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields</title><link>http://arxiv.org/abs/2402.13252v1</link><description>In this paper, we propose an algorithm that allows joint refinement of camerapose and scene geometry represented by decomposed low-rank tensor, using only2D images as supervision. First, we conduct a pilot study based on a 1D signaland relate our findings to 3D scenarios, where the naive joint poseoptimization on voxel-based NeRFs can easily lead to sub-optimal solutions.Moreover, based on the analysis of the frequency spectrum, we propose to applyconvolutional Gaussian filters on 2D and 3D radiance fields for acoarse-to-fine training schedule that enables joint camera pose optimization.Leveraging the decomposition property in decomposed low-rank tensor, our methodachieves an equivalent effect to brute-force 3D convolution with only incurringlittle computational overhead. To further improve the robustness and stabilityof joint optimization, we also propose techniques of smoothed 2D supervision,randomly scaled kernel parameters, and edge-guided loss mask. Extensivequantitative and qualitative evaluations demonstrate that our proposedframework achieves superior performance in novel view synthesis as well asrapid convergence for optimization.</description><author>Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu</author><pubDate>Tue, 20 Feb 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13252v1</guid></item><item><title>ROAM: Robust and Object-Aware Motion Generation Using Neural Pose Descriptors</title><link>http://arxiv.org/abs/2308.12969v2</link><description>Existing automatic approaches for 3D virtual character motion synthesissupporting scene interactions do not generalise well to new objects outsidetraining distributions, even when trained on extensive motion capture datasetswith diverse objects and annotated interactions. This paper addresses thislimitation and shows that robustness and generalisation to novel scene objectsin 3D object-aware character synthesis can be achieved by training a motionmodel with as few as one reference object. We leverage an implicit featurerepresentation trained on object-only datasets, which encodes anSE(3)-equivariant descriptor field around the object. Given an unseen objectand a reference pose-object pair, we optimise for the object-aware pose that isclosest in the feature space to the reference pose. Finally, we use l-NSM,i.e., our motion generation model that is trained to seamlessly transition fromlocomotion to object interaction with the proposed bidirectional pose blendingscheme. Through comprehensive numerical comparisons to state-of-the-art methodsand in a user study, we demonstrate substantial improvements in 3D virtualcharacter motion and interaction quality and robustness to scenarios withunseen objects. Our project page is available athttps://vcai.mpi-inf.mpg.de/projects/ROAM/.</description><author>Wanyue Zhang, Rishabh Dabral, Thomas Leimkühler, Vladislav Golyanik, Marc Habermann, Christian Theobalt</author><pubDate>Thu, 15 Feb 2024 08:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12969v2</guid></item><item><title>AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization</title><link>http://arxiv.org/abs/2308.10001v2</link><description>Neural Radiance Fields (NeRF) have shown promise in generating realisticnovel views from sparse scene images. However, existing NeRF approaches oftenencounter challenges due to the lack of explicit 3D supervision and imprecisecamera poses, resulting in suboptimal outcomes. To tackle these issues, wepropose AltNeRF -- a novel framework designed to create resilient NeRFrepresentations using self-supervised monocular depth estimation (SMDE) frommonocular videos, without relying on known camera poses. SMDE in AltNeRFmasterfully learns depth and pose priors to regulate NeRF training. The depthprior enriches NeRF's capacity for precise scene geometry depiction, while thepose prior provides a robust starting point for subsequent pose refinement.Moreover, we introduce an alternating algorithm that harmoniously melds NeRFoutputs into SMDE through a consistence-driven mechanism, thus enhancing theintegrity of depth priors. This alternation empowers AltNeRF to progressivelyrefine NeRF representations, yielding the synthesis of realistic novel views.Extensive experiments showcase the compelling capabilities of AltNeRF ingenerating high-fidelity and robust novel views that closely resemble reality.</description><author>Kun Wang, Zhiqiang Yan, Huang Tian, Zhenyu Zhang, Xiang Li, Jun Li, Jian Yang</author><pubDate>Fri, 23 Feb 2024 12:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10001v2</guid></item><item><title>SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation</title><link>http://arxiv.org/abs/2311.15707v2</link><description>Zero-shot 6D object pose estimation involves the detection of novel objectswith their 6D poses in cluttered scenes, presenting significant challenges formodel generalizability. Fortunately, the recent Segment Anything Model (SAM)has showcased remarkable zero-shot transfer performance, which provides apromising solution to tackle this task. Motivated by this, we introduce SAM-6D,a novel framework designed to realize the task through two steps, includinginstance segmentation and pose estimation. Given the target objects, SAM-6Demploys two dedicated sub-networks, namely Instance Segmentation Model (ISM)and Pose Estimation Model (PEM), to perform these steps on cluttered RGB-Dimages. ISM takes SAM as an advanced starting point to generate all possibleobject proposals and selectively preserves valid ones through meticulouslycrafted object matching scores in terms of semantics, appearance and geometry.By treating pose estimation as a partial-to-partial point matching problem, PEMperforms a two-stage point matching process featuring a novel design ofbackground tokens to construct dense 3D-3D correspondence, ultimately yieldingthe pose estimates. Without bells and whistles, SAM-6D outperforms the existingmethods on the seven core datasets of the BOP Benchmark for both instancesegmentation and pose estimation of novel objects.</description><author>Jiehong Lin, Lihua Liu, Dekun Lu, Kui Jia</author><pubDate>Wed, 06 Mar 2024 12:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15707v2</guid></item><item><title>Loopy-SLAM: Dense Neural SLAM with Loop Closures</title><link>http://arxiv.org/abs/2402.09944v1</link><description>Neural RGBD SLAM techniques have shown promise in dense SimultaneousLocalization And Mapping (SLAM), yet face challenges such as error accumulationduring camera tracking resulting in distorted maps. In response, we introduceLoopy-SLAM that globally optimizes poses and the dense 3D model. We useframe-to-model tracking using a data-driven point-based submap generationmethod and trigger loop closures online by performing global place recognition.Robust pose graph optimization is used to rigidly align the local submaps. Asour representation is point based, map corrections can be performed efficientlywithout the need to store the entire history of input frames used for mappingas typically required by methods employing a grid based mapping structure.Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNetdatasets demonstrate competitive or superior performance in tracking, mapping,and rendering accuracy when compared to existing dense neural RGBD SLAMmethods. Project page: notchla.github.io/Loopy-SLAM.</description><author>Lorenzo Liso, Erik Sandström, Vladimir Yugay, Luc Van Gool, Martin R. Oswald</author><pubDate>Wed, 14 Feb 2024 18:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09944v1</guid></item><item><title>MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction</title><link>http://arxiv.org/abs/2402.12712v1</link><description>This paper presents a neural architecture MVDiffusion++ for 3D objectreconstruction that synthesizes dense and high-resolution views of an objectgiven one or a few images without camera poses. MVDiffusion++ achieves superiorflexibility and scalability with two surprisingly simple ideas: 1) A``pose-free architecture'' where standard self-attention among 2D latentfeatures learns 3D consistency across an arbitrary number of conditional andgeneration views without explicitly using camera pose information; and 2) A``view dropout strategy'' that discards a substantial number of output viewsduring training, which reduces the training-time memory footprint and enablesdense and high-resolution view synthesis at test time. We use the Objaverse fortraining and the Google Scanned Objects for evaluation with standard novel viewsynthesis and 3D reconstruction metrics, where MVDiffusion++ significantlyoutperforms the current state of the arts. We also demonstrate a text-to-3Dapplication example by combining MVDiffusion++ with a text-to-image generativemodel.</description><author>Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, Rakesh Ranjan</author><pubDate>Tue, 20 Feb 2024 04:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12712v1</guid></item><item><title>Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving</title><link>http://arxiv.org/abs/2403.04112v1</link><description>This paper presents a novel multi-modal Multi-Object Tracking (MOT) algorithmfor self-driving cars that combines camera and LiDAR data. Camera frames areprocessed with a state-of-the-art 3D object detector, whereas classicalclustering techniques are used to process LiDAR observations. The proposed MOTalgorithm comprises a three-step association process, an Extended Kalman filterfor estimating the motion of each detected dynamic obstacle, and a trackmanagement phase. The EKF motion model requires the current measured relativeposition and orientation of the observed object and the longitudinal andangular velocities of the ego vehicle as inputs. Unlike most state-of-the-artmulti-modal MOT approaches, the proposed algorithm does not rely on maps orknowledge of the ego global pose. Moreover, it uses a 3D detector exclusivelyfor cameras and is agnostic to the type of LiDAR sensor used. The algorithm isvalidated both in simulation and with real-world data, with satisfactoryresults.</description><author>Riccardo Pieroni, Simone Specchia, Matteo Corno, Sergio Matteo Savaresi</author><pubDate>Wed, 06 Mar 2024 23:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04112v1</guid></item><item><title>Denoising Diffusion via Image-Based Rendering</title><link>http://arxiv.org/abs/2402.03445v2</link><description>Generating 3D scenes is a challenging open problem, which requiressynthesizing plausible content that is fully consistent in 3D space. Whilerecent methods such as neural radiance fields excel at view synthesis and 3Dreconstruction, they cannot synthesize plausible details in unobserved regionssince they lack a generative capability. Conversely, existing generativemethods are typically not capable of reconstructing detailed, large-scalescenes in the wild, as they use limited-capacity 3D scene representations,require aligned camera poses, or rely on additional regularizers. In this work,we introduce the first diffusion model able to perform fast, detailedreconstruction and generation of real-world 3D scenes. To achieve this, we makethree contributions. First, we introduce a new neural scene representation,IB-planes, that can efficiently and accurately represent large 3D scenes,dynamically allocating more capacity as needed to capture details visible ineach image. Second, we propose a denoising-diffusion framework to learn a priorover this novel 3D scene representation, using only 2D images without the needfor any additional supervision signal such as masks or depths. This supports 3Dreconstruction and generation in a unified architecture. Third, we develop aprincipled approach to avoid trivial 3D solutions when integrating theimage-based rendering with the diffusion model, by dropping out representationsof some images. We evaluate the model on several challenging datasets of realand synthetic images, and demonstrate superior results on generation, novelview synthesis and 3D reconstruction.</description><author>Titas Anciukevičius, Fabian Manhardt, Federico Tombari, Paul Henderson</author><pubDate>Tue, 20 Feb 2024 20:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03445v2</guid></item><item><title>Cameras as Rays: Pose Estimation via Ray Diffusion</title><link>http://arxiv.org/abs/2402.14817v1</link><description>Estimating camera poses is a fundamental task for 3D reconstruction andremains challenging given sparse views (&lt;10). In contrast to existingapproaches that pursue top-down prediction of global parametrizations of cameraextrinsics, we propose a distributed representation of camera pose that treatsa camera as a bundle of rays. This representation allows for a tight couplingwith spatial image features improving pose precision. We observe that thisrepresentation is naturally suited for set-level level transformers and developa regression-based approach that maps image patches to corresponding rays. Tocapture the inherent uncertainties in sparse-view pose inference, we adapt thisapproach to learn a denoising diffusion model which allows us to sampleplausible modes while improving performance. Our proposed methods, bothregression- and diffusion-based, demonstrate state-of-the-art performance oncamera pose estimation on CO3D while generalizing to unseen object categoriesand in-the-wild captures.</description><author>Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Thu, 22 Feb 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14817v1</guid></item><item><title>Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks</title><link>http://arxiv.org/abs/2402.14400v1</link><description>Reliable methods for the neurodevelopmental assessment of infants areessential for early detection of medical issues that may need promptinterventions. Spontaneous motor activity, or `kinetics', is shown to provide apowerful surrogate measure of upcoming neurodevelopment. However, itsassessment is by and large qualitative and subjective, focusing on visuallyidentified, age-specific gestures. Here, we follow an alternative approach,predicting infants' neurodevelopmental maturation based on data-drivenevaluation of individual motor patterns. We utilize 3D video recordings ofinfants processed with pose-estimation to extract spatio-temporal series ofanatomical landmarks, and apply adaptive graph convolutional networks topredict the actual age. We show that our data-driven approach achievesimprovement over traditional machine learning baselines based on manuallyengineered features.</description><author>Daniel Holmberg, Manu Airaksinen, Viviana Marchi, Andrea Guzzetta, Anna Kivi, Leena Haataja, Sampsa Vanhatalo, Teemu Roos</author><pubDate>Thu, 22 Feb 2024 09:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14400v1</guid></item><item><title>Towards Transferable Targeted 3D Adversarial Attack in the Physical World</title><link>http://arxiv.org/abs/2312.09558v2</link><description>Compared with transferable untargeted attacks, transferable targetedadversarial attacks could specify the misclassification categories ofadversarial samples, posing a greater threat to security-critical tasks. In themeanwhile, 3D adversarial samples, due to their potential of multi-viewrobustness, can more comprehensively identify weaknesses in existing deeplearning systems, possessing great application value. However, the field oftransferable targeted 3D adversarial attacks remains vacant. The goal of thiswork is to develop a more effective technique that could generate transferabletargeted 3D adversarial examples, filling the gap in this field. To achievethis goal, we design a novel framework named TT3D that could rapidlyreconstruct from few multi-view images into Transferable Targeted 3D texturedmeshes. While existing mesh-based texture optimization methods computegradients in the high-dimensional mesh space and easily fall into local optima,leading to unsatisfactory transferability and distinct distortions, TT3Dinnovatively performs dual optimization towards both feature grid andMulti-layer Perceptron (MLP) parameters in the grid-based NeRF space, whichsignificantly enhances black-box transferability while enjoying naturalness.Experimental results show that TT3D not only exhibits superior cross-modeltransferability but also maintains considerable adaptability across differentrenders and vision tasks. More importantly, we produce 3D adversarial exampleswith 3D printing techniques in the real world and verify their robustperformance under various scenarios.</description><author>Yao Huang, Yinpeng Dong, Shouwei Ruan, Xiao Yang, Hang Su, Xingxing Wei</author><pubDate>Wed, 28 Feb 2024 12:28:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09558v2</guid></item><item><title>Neural Refinement for Absolute Pose Regression with Feature Synthesis</title><link>http://arxiv.org/abs/2303.10087v2</link><description>Absolute Pose Regression (APR) methods use deep neural networks to directlyregress camera poses from RGB images. However, the predominant APRarchitectures only rely on 2D operations during inference, resulting in limitedaccuracy of pose estimation due to the lack of 3D geometry constraints orpriors. In this work, we propose a test-time refinement pipeline that leveragesimplicit geometric constraints using a robust feature field to enhance theability of APR methods to use 3D information during inference. We alsointroduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3Dgeometric features during training and directly renders dense novel viewfeatures at test time to refine APR methods. To enhance the robustness of ourmodel, we introduce a feature fusion module and a progressive trainingstrategy. Our proposed method achieves state-of-the-art single-image APRaccuracy on indoor and outdoor datasets.</description><author>Shuai Chen, Yash Bhalgat, Xinghui Li, Jiawang Bian, Kejie Li, Zirui Wang, Victor Adrian Prisacariu</author><pubDate>Fri, 01 Mar 2024 01:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10087v2</guid></item><item><title>Representing 3D sparse map points and lines for camera relocalization</title><link>http://arxiv.org/abs/2402.18011v1</link><description>Recent advancements in visual localization and mapping have demonstratedconsiderable success in integrating point and line features. However, expandingthe localization framework to include additional mapping components frequentlyresults in increased demand for memory and computational resources dedicated tomatching tasks. In this study, we show how a lightweight neural network canlearn to represent both 3D point and line features, and exhibit leading poseaccuracy by harnessing the power of multiple learned mappings. Specifically, weutilize a single transformer block to encode line features, effectivelytransforming them into distinctive point-like descriptors. Subsequently, wetreat these point and line descriptor sets as distinct yet interconnectedfeature sets. Through the integration of self- and cross-attention withinseveral graph layers, our method effectively refines each feature beforeregressing 3D maps using two simple MLPs. In comprehensive experiments, ourindoor localization findings surpass those of Hloc and Limap across bothpoint-based and line-assisted configurations. Moreover, in outdoor scenarios,our method secures a significant lead, marking the most considerableenhancement over state-of-the-art learning-based methodologies. The source codeand demo videos of this work are publicly available at:https://thpjp.github.io/pl2map/</description><author>Bach-Thuan Bui, Huy-Hoang Bui, Dinh-Tuan Tran, Joo-Ho Lee</author><pubDate>Wed, 28 Feb 2024 03:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18011v1</guid></item><item><title>CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge</title><link>http://arxiv.org/abs/2402.15726v1</link><description>Most of existing category-level object pose estimation methods devote tolearning the object category information from point cloud modality. However,the scale of 3D datasets is limited due to the high cost of 3D data collectionand annotation. Consequently, the category features extracted from theselimited point cloud samples may not be comprehensive. This motivates us toinvestigate whether we can draw on knowledge of other modalities to obtaincategory information. Inspired by this motivation, we propose CLIPose, a novel6D pose framework that employs the pre-trained vision-language model to developbetter learning of object category information, which can fully leverageabundant semantic knowledge in image and text modalities. To make the 3Dencoder learn category-specific features more efficiently, we alignrepresentations of three modalities in feature space via multi-modalcontrastive learning. In addition to exploiting the pre-trained knowledge ofthe CLIP's model, we also expect it to be more sensitive with pose parameters.Therefore, we introduce a prompt tuning approach to fine-tune image encoderwhile we incorporate rotations and translations information in the textdescriptions. CLIPose achieves state-of-the-art performance on two mainstreambenchmark datasets, REAL275 and CAMERA25, and runs in real-time duringinference (40FPS).</description><author>Xiao Lin, Minghao Zhu, Ronghao Dang, Guangliang Zhou, Shaolong Shu, Feng Lin, Chengju Liu, Qijun Chen</author><pubDate>Sat, 24 Feb 2024 05:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15726v1</guid></item><item><title>KVN: Keypoints Voting Network with Differentiable RANSAC for Stereo Pose Estimation</title><link>http://arxiv.org/abs/2307.11543v2</link><description>Object pose estimation is a fundamental computer vision task exploited inseveral robotics and augmented reality applications. Many establishedapproaches rely on predicting 2D-3D keypoint correspondences using RANSAC(Random sample consensus) and estimating the object pose using the PnP(Perspective-n-Point) algorithm. Being RANSAC non-differentiable,correspondences cannot be directly learned in an end-to-end fashion. In thispaper, we address the stereo image-based object pose estimation problem by i)introducing a differentiable RANSAC layer into a well-known monocular poseestimation network; ii) exploiting an uncertainty-driven multi-view PnP solverwhich can fuse information from multiple views. We evaluate our approach on achallenging public stereo object pose estimation dataset and a custom-builtdataset we call Transparent Tableware Dataset (TTD), yielding state-of-the-artresults against other recent approaches. Furthermore, in our ablation study, weshow that the differentiable RANSAC layer plays a significant role in theaccuracy of the proposed method. We release with this paper the code of ourmethod and the TTD dataset.</description><author>Ivano Donadi, Alberto Pretto</author><pubDate>Wed, 28 Feb 2024 15:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11543v2</guid></item><item><title>DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly</title><link>http://arxiv.org/abs/2402.19302v1</link><description>Reassembly tasks play a fundamental role in many fields and multipleapproaches exist to solve specific reassembly problems. In this context, weposit that a general unified model can effectively address them all,irrespective of the input data type (images, 3D, etc.). We introduceDiffAssemble, a Graph Neural Network (GNN)-based architecture that learns tosolve reassembly tasks using a diffusion model formulation. Our method treatsthe elements of a set, whether pieces of 2D patch or 3D object fragments, asnodes of a spatial graph. Training is performed by introducing noise into theposition and rotation of the elements and iteratively denoising them toreconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art(SOTA) results in most 2D and 3D reassembly tasks and is the firstlearning-based approach that solves 2D puzzles for both rotation andtranslation. Furthermore, we highlight its remarkable reduction in run-time,performing 11 times faster than the quickest optimization-based method forpuzzle solving. Code available at https://github.com/IIT-PAVIS/DiffAssemble</description><author>Gianluca Scarpellini, Stefano Fiorini, Francesco Giuliari, Pietro Morerio, Alessio Del Bue</author><pubDate>Thu, 29 Feb 2024 16:09:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19302v1</guid></item><item><title>OCT2Confocal: 3D CycleGAN based Translation of Retinal OCT Images to Confocal Microscopy</title><link>http://arxiv.org/abs/2311.10902v3</link><description>Optical coherence tomography (OCT) and confocal microscopy are pivotal inretinal imaging, each presenting unique benefits and limitations. In-vivo OCToffers rapid, non-invasive imaging but can be hampered by clarity issues andmotion artifacts. Ex-vivo confocal microscopy provides high-resolution,cellular detailed color images but is invasive and poses ethical concerns andpotential tissue damage. To bridge these modalities, we developed a 3D CycleGANframework for unsupervised translation of in-vivo OCT to ex-vivo confocalmicroscopy images. Applied to our OCT2Confocal dataset, this frameworkeffectively translates between 3D medical data domains, capturing vascular,textural, and cellular details with precision. This marks the first attempt toexploit the inherent 3D information of OCT and translate it into the rich,detailed color domain of confocal microscopy. Assessed through quantitative andqualitative evaluations, the 3D CycleGAN framework demonstrates commendableimage fidelity and quality, outperforming existing methods despite theconstraints of limited data. This non-invasive generation of retinal confocalimages has the potential to further enhance diagnostic and monitoringcapabilities in ophthalmology. Our source code and OCT2Confocal dataset areavailable at https://github.com/xintian-99/OCT2Confocal.</description><author>Xin Tian, Nantheera Anantrasirichai, Lindsay Nicholson, Alin Achim</author><pubDate>Sat, 17 Feb 2024 01:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10902v3</guid></item><item><title>SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation</title><link>http://arxiv.org/abs/2401.13560v3</link><description>The Transformer architecture has shown a remarkable ability in modelingglobal relationships. However, it poses a significant computational challengewhen processing high-dimensional medical images. This hinders its developmentand widespread adoption in this task. Mamba, as a State Space Model (SSM),recently emerged as a notable manner for long-range dependencies in sequentialmodeling, excelling in natural language processing filed with its remarkablememory efficiency and computational speed. Inspired by its success, weintroduce SegMamba, a novel 3D medical image \textbf{Seg}mentation\textbf{Mamba} model, designed to effectively capture long-range dependencieswithin whole volume features at every scale. Our SegMamba, in contrast toTransformer-based methods, excels in whole volume feature modeling from a statespace model standpoint, maintaining superior processing speed, even with volumefeatures at a resolution of {$64\times 64\times 64$}. Comprehensive experimentson the BraTS2023 dataset demonstrate the effectiveness and efficiency of ourSegMamba. The code for SegMamba is available at:https://github.com/ge-xing/SegMamba</description><author>Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, Lei Zhu</author><pubDate>Sun, 25 Feb 2024 14:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13560v3</guid></item><item><title>AccessLens: Auto-detecting Inaccessibility of Everyday Objects</title><link>http://arxiv.org/abs/2401.15996v2</link><description>In our increasingly diverse society, everyday physical interfaces oftenpresent barriers, impacting individuals across various contexts. Thisoversight, from small cabinet knobs to identical wall switches that can posedifferent contextual challenges, highlights an imperative need for solutions.Leveraging low-cost 3D-printed augmentations such as knob magnifiers andtactile labels seems promising, yet the process of discovering unrecognizedbarriers remains challenging because disability is context-dependent. Weintroduce AccessLens, an end-to-end system designed to identify inaccessibleinterfaces in daily objects, and recommend 3D-printable augmentations foraccessibility enhancement. Our approach involves training a detector using thenovel AccessDB dataset designed to automatically recognize 21 distinctInaccessibility Classes (e.g., bar-small and round-rotate) within 6 commonobject categories (e.g., handle and knob). AccessMeta serves as a robust way tobuild a comprehensive dictionary linking these accessibility classes toopen-source 3D augmentation designs. Experiments demonstrate our detector'sperformance in detecting inaccessible objects.</description><author>Nahyun Kwon, Qian Lu, Muhammad Hasham Qazi, Joanne Liu, Changhoon Oh, Shu Kong, Jeeeun Kim</author><pubDate>Fri, 23 Feb 2024 17:06:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15996v2</guid></item><item><title>EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection</title><link>http://arxiv.org/abs/2402.15272v1</link><description>In autonomous driving, cooperative perception makes use of multi-view camerasfrom both vehicles and infrastructure, providing a global vantage point withrich semantic context of road conditions beyond a single vehicle viewpoint.Currently, two major challenges persist in vehicle-infrastructure cooperative3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-viewimages, caused by time asynchrony across cameras; $2)$ information loss intransmission process resulted from limited communication bandwidth. To addressthese issues, we propose a novel camera-based 3D detection framework for VIC3Dtask, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploitholistic perspectives from both vehicles and infrastructure, we proposeMulti-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM)modules to enhance infrastructure and vehicle features at scale, spatial, andchannel levels to correct the pose error introduced by camera asynchrony. Wealso introduce a Feature Compression (FC) module with channel and spatialcompression blocks for transmission efficiency. Experiments show that EMIFFachieves SOTA on DAIR-V2X-C datasets, significantly outperforming previousearly-fusion and late-fusion methods with comparable transmission costs.</description><author>Zhe Wang, Siqi Fan, Xiaoliang Huo, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang</author><pubDate>Fri, 23 Feb 2024 11:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15272v1</guid></item><item><title>False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy</title><link>http://arxiv.org/abs/2403.02639v1</link><description>Recent studies have focused on enhancing the performance of 3D objectdetection models. Among various approaches, ground-truth sampling has beenproposed as an augmentation technique to address the challenges posed bylimited ground-truth data. However, an inherent issue with ground-truthsampling is its tendency to increase false positives. Therefore, this studyaims to overcome the limitations of ground-truth sampling and improve theperformance of 3D object detection models by developing a new augmentationtechnique called false-positive sampling. False-positive sampling involvesretraining the model using point clouds that are identified as false positivesin the model's predictions. We propose an algorithm that utilizes bothground-truth and false-positive sampling and an algorithm for building thefalse-positive sample database. Additionally, we analyze the principles behindthe performance enhancement due to false-positive sampling and propose atechnique that applies the concept of curriculum learning to the samplingstrategy that encompasses both false-positive and ground-truth samplingtechniques. Our experiments demonstrate that models utilizing false-positivesampling show a reduction in false positives and exhibit improved objectdetection performance. On the KITTI and Waymo Open datasets, models withfalse-positive sampling surpass the baseline models by a large margin.</description><author>Jiyong Oh, Junhaeng Lee, Woongchan Byun, Minsang Kong, Sang Hun Lee</author><pubDate>Tue, 05 Mar 2024 04:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02639v1</guid></item><item><title>False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy</title><link>http://arxiv.org/abs/2403.02639v2</link><description>Recent studies have focused on enhancing the performance of 3D objectdetection models. Among various approaches, ground-truth sampling has beenproposed as an augmentation technique to address the challenges posed bylimited ground-truth data. However, an inherent issue with ground-truthsampling is its tendency to increase false positives. Therefore, this studyaims to overcome the limitations of ground-truth sampling and improve theperformance of 3D object detection models by developing a new augmentationtechnique called false-positive sampling. False-positive sampling involvesretraining the model using point clouds that are identified as false positivesin the model's predictions. We propose an algorithm that utilizes bothground-truth and false-positive sampling and an algorithm for building thefalse-positive sample database. Additionally, we analyze the principles behindthe performance enhancement due to false-positive sampling and propose atechnique that applies the concept of curriculum learning to the samplingstrategy that encompasses both false-positive and ground-truth samplingtechniques. Our experiments demonstrate that models utilizing false-positivesampling show a reduction in false positives and exhibit improved objectdetection performance. On the KITTI and Waymo Open datasets, models withfalse-positive sampling surpass the baseline models by a large margin.</description><author>Jiyong Oh, Junhaeng Lee, Woongchan Byun, Minsang Kong, Sang Hun Lee</author><pubDate>Thu, 07 Mar 2024 12:24:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02639v2</guid></item><item><title>Multi-Model 3D Registration: Finding Multiple Moving Objects in Cluttered Point Clouds</title><link>http://arxiv.org/abs/2402.10865v1</link><description>We investigate a variation of the 3D registration problem, named multi-model3D registration. In the multi-model registration problem, we are given twopoint clouds picturing a set of objects at different poses (and possiblyincluding points belonging to the background) and we want to simultaneouslyreconstruct how all objects moved between the two point clouds. This setupgeneralizes standard 3D registration where one wants to reconstruct a singlepose, e.g., the motion of the sensor picturing a static scene. Moreover, itprovides a mathematically grounded formulation for relevant roboticsapplications, e.g., where a depth sensor onboard a robot perceives a dynamicscene and has the goal of estimating its own motion (from the static portion ofthe scene) while simultaneously recovering the motion of all dynamic objects.We assume a correspondence-based setup where we have putative matches betweenthe two point clouds and consider the practical case where thesecorrespondences are plagued with outliers. We then propose a simple approachbased on Expectation-Maximization (EM) and establish theoretical conditionsunder which the EM approach converges to the ground truth. We evaluate theapproach in simulated and real datasets ranging from table-top scenes toself-driving scenarios and demonstrate its effectiveness when combined withstate-of-the-art scene flow methods to establish dense correspondences.</description><author>David Jin, Sushrut Karmalkar, Harry Zhang, Luca Carlone</author><pubDate>Fri, 16 Feb 2024 18:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10865v1</guid></item><item><title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</title><link>http://arxiv.org/abs/2309.09737v5</link><description>Mobile autonomy relies on the precise perception of dynamic environments.Robustly tracking moving objects in 3D world thus plays a pivotal role forapplications like trajectory prediction, obstacle avoidance, and path planning.While most current methods utilize LiDARs or cameras for Multiple ObjectTracking (MOT), the capabilities of 4D imaging radars remain largelyunexplored. Recognizing the challenges posed by radar noise and point sparsityin 4D radar data, we introduce RaTrack, an innovative solution tailored forradar-based tracking. Bypassing the typical reliance on specific object typesand 3D bounding boxes, our method focuses on motion segmentation andclustering, enriched by a motion estimation module. Evaluated on theView-of-Delft dataset, RaTrack showcases superior tracking precision of movingobjects, largely surpassing the performance of the state of the art. We releaseour code and model at https://github.com/LJacksonPan/RaTrack.</description><author>Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu</author><pubDate>Sat, 24 Feb 2024 17:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09737v5</guid></item><item><title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</title><link>http://arxiv.org/abs/2309.09737v4</link><description>Mobile autonomy relies on the precise perception of dynamic environments.Robustly tracking moving objects in 3D world thus plays a pivotal role forapplications like trajectory prediction, obstacle avoidance, and path planning.While most current methods utilize LiDARs or cameras for Multiple ObjectTracking (MOT), the capabilities of 4D imaging radars remain largelyunexplored. Recognizing the challenges posed by radar noise and point sparsityin 4D radar data, we introduce RaTrack, an innovative solution tailored forradar-based tracking. Bypassing the typical reliance on specific object typesand 3D bounding boxes, our method focuses on motion segmentation andclustering, enriched by a motion estimation module. Evaluated on theView-of-Delft dataset, RaTrack showcases superior tracking precision of movingobjects, largely surpassing the performance of the state of the art. We releaseour code and model at https://github.com/LJacksonPan/RaTrack.</description><author>Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu</author><pubDate>Mon, 19 Feb 2024 17:53:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09737v4</guid></item><item><title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</title><link>http://arxiv.org/abs/2309.09737v6</link><description>Mobile autonomy relies on the precise perception of dynamic environments.Robustly tracking moving objects in 3D world thus plays a pivotal role forapplications like trajectory prediction, obstacle avoidance, and path planning.While most current methods utilize LiDARs or cameras for Multiple ObjectTracking (MOT), the capabilities of 4D imaging radars remain largelyunexplored. Recognizing the challenges posed by radar noise and point sparsityin 4D radar data, we introduce RaTrack, an innovative solution tailored forradar-based tracking. Bypassing the typical reliance on specific object typesand 3D bounding boxes, our method focuses on motion segmentation andclustering, enriched by a motion estimation module. Evaluated on theView-of-Delft dataset, RaTrack showcases superior tracking precision of movingobjects, largely surpassing the performance of the state of the art. We releaseour code and model at https://github.com/LJacksonPan/RaTrack.</description><author>Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu</author><pubDate>Wed, 06 Mar 2024 13:32:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09737v6</guid></item><item><title>Seeing the World through Your Eyes</title><link>http://arxiv.org/abs/2306.09348v2</link><description>The reflective nature of the human eye is an underappreciated source ofinformation about what the world around us looks like. By imaging the eyes of amoving person, we can collect multiple views of a scene outside the camera'sdirect line of sight through the reflections in the eyes. In this paper, wereconstruct a 3D scene beyond the camera's line of sight using portrait imagescontaining eye reflections. This task is challenging due to 1) the difficultyof accurately estimating eye poses and 2) the entangled appearance of the eyeiris and the scene reflections. Our method jointly refines the cornea poses,the radiance field depicting the scene, and the observer's eye iris texture. Wefurther propose a simple regularization prior on the iris texture pattern toimprove reconstruction quality. Through various experiments on synthetic andreal-world captures featuring people with varied eye colors, we demonstrate thefeasibility of our approach to recover 3D scenes using eye reflections.</description><author>Hadi Alzayer, Kevin Zhang, Brandon Feng, Christopher Metzler, Jia-Bin Huang</author><pubDate>Sat, 02 Mar 2024 16:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09348v2</guid></item><item><title>Extend Your Own Correspondences: Unsupervised Distant Point Cloud Registration by Progressive Distance Extension</title><link>http://arxiv.org/abs/2403.03532v1</link><description>Registration of point clouds collected from a pair of distant vehiclesprovides a comprehensive and accurate 3D view of the driving scenario, which isvital for driving safety related applications, yet existing literature suffersfrom the expensive pose label acquisition and the deficiency to generalize tonew data distributions. In this paper, we propose EYOC, an unsupervised distantpoint cloud registration method that adapts to new point cloud distributions onthe fly, requiring no global pose labels. The core idea of EYOC is to train afeature extractor in a progressive fashion, where in each round, the featureextractor, trained with near point cloud pairs, can label slightly fartherpoint cloud pairs, enabling self-supervision on such far point cloud pairs.This process continues until the derived extractor can be used to registerdistant point clouds. Particularly, to enable high-fidelity correspondencelabel generation, we devise an effective spatial filtering scheme to select themost representative correspondences to register a point cloud pair, and thenutilize the aligned point clouds to discover more correct correspondences.Experiments show that EYOC can achieve comparable performance withstate-of-the-art supervised methods at a lower training cost. Moreover, itoutwits supervised methods regarding generalization performance on new datadistributions.</description><author>Quan Liu, Hongzi Zhu, Zhenxi Wang, Yunsong Zhou, Shan Chang, Minyi Guo</author><pubDate>Wed, 06 Mar 2024 08:18:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03532v1</guid></item><item><title>Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis</title><link>http://arxiv.org/abs/2403.04116v1</link><description>X-ray is widely applied for transmission imaging due to its strongerpenetration than natural light. When rendering novel view X-ray projections,existing methods mainly based on NeRF suffer from long training time and slowinference speed. In this paper, we propose a 3D Gaussian splatting-basedframework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, weredesign a radiative Gaussian point cloud model inspired by the isotropicnature of X-ray imaging. Our model excludes the influence of view directionwhen learning to predict the radiation intensity of 3D points. Based on thismodel, we develop a Differentiable Radiative Rasterization (DRR) with CUDAimplementation. Secondly, we customize an Angle-pose Cuboid UniformInitialization (ACUI) strategy that directly uses the parameters of the X-rayscanner to compute the camera information and then uniformly samples pointpositions within a cuboid enclosing the scanned object. Experiments show thatour X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoyingless than 15% training time and over 73x inference speed. The application onsparse-view CT reconstruction also reveals the practical values of our method.Code and models will be publicly available athttps://github.com/caiyuanhao1998/X-Gaussian . A video demo of the trainingprocess visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .</description><author>Yuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, Alan Yuille</author><pubDate>Thu, 07 Mar 2024 00:12:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04116v1</guid></item><item><title>MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection</title><link>http://arxiv.org/abs/2402.11677v1</link><description>Multi-modal 3D object detection models for automated driving havedemonstrated exceptional performance on computer vision benchmarks likenuScenes. However, their reliance on densely sampled LiDAR point clouds andmeticulously calibrated sensor arrays poses challenges for real-worldapplications. Issues such as sensor misalignment, miscalibration, and disparatesampling frequencies lead to spatial and temporal misalignment in data fromLiDAR and cameras. Additionally, the integrity of LiDAR and camera data isoften compromised by adverse environmental conditions such as inclementweather, leading to occlusions and noise interference. To address thischallenge, we introduce MultiCorrupt, a comprehensive benchmark designed toevaluate the robustness of multi-modal 3D object detectors against ten distincttypes of corruptions. We evaluate five state-of-the-art multi-modal detectorson MultiCorrupt and analyze their performance in terms of their resistanceability. Our results show that existing methods exhibit varying degrees ofrobustness depending on the type of corruption and their fusion strategy. Weprovide insights into which multi-modal design choices make such models robustagainst certain perturbations. The dataset generation code and benchmark areopen-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt.</description><author>Till Beemelmanns, Quan Zhang, Lutz Eckstein</author><pubDate>Sun, 18 Feb 2024 18:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11677v1</guid></item><item><title>Dynamic 3D Point Cloud Sequences as 2D Videos</title><link>http://arxiv.org/abs/2403.01129v1</link><description>Dynamic 3D point cloud sequences serve as one of the most common andpractical representation modalities of dynamic real-world environments.However, their unstructured nature in both spatial and temporal domains posessignificant challenges to effective and efficient processing. Existing deeppoint cloud sequence modeling approaches imitate the mature 2D video learningmechanisms by developing complex spatio-temporal point neighbor grouping andfeature aggregation schemes, often resulting in methods lacking effectiveness,efficiency, and expressive power. In this paper, we propose a novel genericrepresentation called \textit{Structured Point Cloud Videos} (SPCVs).Intuitively, by leveraging the fact that 3D geometric shapes are essentially 2Dmanifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatialsmoothness and temporal consistency, where the pixel values correspond to the3D coordinates of points. The structured nature of our SPCV representationallows for the seamless adaptation of well-established 2D image/videotechniques, enabling efficient and effective processing and analysis of 3Dpoint cloud sequences. To achieve such re-organization, we design aself-supervised learning pipeline that is geometrically regularized and drivenby self-reconstructive and deformation field learning objectives. Additionally,we construct SPCV-based frameworks for both low-level and high-level 3D pointcloud sequence processing and analysis tasks, including action recognition,temporal interpolation, and compression. Extensive experiments demonstrate theversatility and superiority of the proposed SPCV, which has the potential tooffer new possibilities for deep learning on unstructured 3D point cloudsequences. Code will be released at https://github.com/ZENGYIMING-EAMON/SPCV.</description><author>Yiming Zeng, Junhui Hou, Qijian Zhang, Siyu Ren, Wenping Wang</author><pubDate>Sat, 02 Mar 2024 08:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01129v1</guid></item><item><title>UniMODE: Unified Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2402.18573v1</link><description>Realizing unified monocular 3D object detection, including both indoor andoutdoor scenes, holds great importance in applications like robot navigation.However, involving various scenarios of data to train models poses challengesdue to their significantly different characteristics, e.g., diverse geometryproperties and heterogeneous domain distributions. To address these challenges,we build a detector based on the bird's-eye-view (BEV) detection paradigm,where the explicit feature projection is beneficial to addressing the geometrylearning ambiguity when employing multiple scenarios of data to traindetectors. Then, we split the classical BEV detection architecture into twostages and propose an uneven BEV grid design to handle the convergenceinstability caused by the aforementioned challenges. Moreover, we develop asparse BEV feature projection strategy to reduce computational cost and aunified domain alignment method to handle heterogeneous domains. Combiningthese techniques, a unified detector UniMODE is derived, which surpasses theprevious state-of-the-art on the challenging Omni3D dataset (a large-scaledataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing thefirst successful generalization of a BEV detector to unified 3D objectdetection.</description><author>Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao</author><pubDate>Wed, 28 Feb 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18573v1</guid></item><item><title>An Endoscopic Chisel: Intraoperative Imaging Carves 3D Anatomical Models</title><link>http://arxiv.org/abs/2402.11840v1</link><description>Purpose: Preoperative imaging plays a pivotal role in sinus surgery where CTsoffer patient-specific insights of complex anatomy, enabling real-timeintraoperative navigation to complement endoscopy imaging. However, surgeryelicits anatomical changes not represented in the preoperative model,generating an inaccurate basis for navigation during surgery progression. Methods: We propose a first vision-based approach to update the preoperative3D anatomical model leveraging intraoperative endoscopic video for navigatedsinus surgery where relative camera poses are known. We rely on comparisons ofintraoperative monocular depth estimates and preoperative depth renders toidentify modified regions. The new depths are integrated in these regionsthrough volumetric fusion in a truncated signed distance functionrepresentation to generate an intraoperative 3D model that reflects tissuemanipulation. Results: We quantitatively evaluate our approach by sequentially updatingmodels for a five-step surgical progression in an ex vivo specimen. We computethe error between correspondences from the updated model and ground-truthintraoperative CT in the region of anatomical modification. The resultingmodels show a decrease in error during surgical progression as opposed toincreasing when no update is employed. Conclusion: Our findings suggest that preoperative 3D anatomical models canbe updated using intraoperative endoscopy video in navigated sinus surgery.Future work will investigate improvements to monocular depth estimation as wellas removing the need for external navigation systems. The resulting ability tocontinuously update the patient model may provide surgeons with a more preciseunderstanding of the current anatomical state and paves the way toward adigital twin paradigm for sinus surgery.</description><author>Jan Emily Mangulabnan, Roger D. Soberanis-Mukul, Timo Teufel, Manish Sahu, Jose L. Porras, S. Swaroop Vedula, Masaru Ishii, Gregory Hager, Russell H. Taylor, Mathias Unberath</author><pubDate>Mon, 19 Feb 2024 05:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11840v1</guid></item><item><title>EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization</title><link>http://arxiv.org/abs/2402.13537v1</link><description>Camera relocalization is pivotal in computer vision, with applications in AR,drones, robotics, and autonomous driving. It estimates 3D camera position andorientation (6-DoF) from images. Unlike traditional methods like SLAM, recentstrides use deep learning for direct end-to-end pose estimation. We proposeEffLoc, a novel efficient Vision Transformer for single-image camerarelocalization. EffLoc's hierarchical layout, memory-bound self-attention, andfeed-forward layers boost memory efficiency and inter-channel communication.Our introduced sequential group attention (SGA) module enhances computationalefficiency by diversifying input features, reducing redundancy, and expandingmodel capacity. EffLoc excels in efficiency and accuracy, outperforming priormethods, such as AtLoc and MapNet. It thrives on large-scale outdoorcar-driving scenario, ensuring simplicity, end-to-end trainability, andeliminating handcrafted loss functions.</description><author>Zhendong Xiao, Changhao Chen, Shan Yang, Wu Wei</author><pubDate>Wed, 21 Feb 2024 05:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13537v1</guid></item><item><title>OBMO: One Bounding Box Multiple Objects for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2212.10049v2</link><description>Compared to typical multi-sensor systems, monocular 3D object detection hasattracted much attention due to its simple configuration. However, there isstill a significant gap between LiDAR-based and monocular-based methods. Inthis paper, we find that the ill-posed nature of monocular imagery can lead todepth ambiguity. Specifically, objects with different depths can appear withthe same bounding boxes and similar visual features in the 2D image.Unfortunately, the network cannot accurately distinguish different depths fromsuch non-discriminative visual features, resulting in unstable depth training.To facilitate depth learning, we propose a simple yet effective plug-and-playmodule, \underline{O}ne \underline{B}ounding Box \underline{M}ultiple\underline{O}bjects (OBMO). Concretely, we add a set of suitable pseudo labelsby shifting the 3D bounding box along the viewing frustum. To constrain thepseudo-3D labels to be reasonable, we carefully design two label scoringstrategies to represent their quality. In contrast to the original hard depthlabels, such soft pseudo labels with quality scores allow the network to learna reasonable depth range, boosting training stability and thus improving finalperformance. Extensive experiments on KITTI and Waymo benchmarks show that ourmethod significantly improves state-of-the-art monocular 3D detectors by asignificant margin (The improvements under the moderate setting on KITTIvalidation set are $\mathbf{1.82\sim 10.91\%}$ \textbf{mAP in BEV} and$\mathbf{1.18\sim 9.36\%}$ \textbf{mAP in 3D}). Codes have been released at\url{https://github.com/mrsempress/OBMO}.</description><author>Chenxi Huang, Tong He, Haidong Ren, Wenxiao Wang, Binbin Lin, Deng Cai</author><pubDate>Tue, 20 Feb 2024 08:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10049v2</guid></item><item><title>RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes</title><link>http://arxiv.org/abs/2309.01513v2</link><description>Room geometry is important prior information for implementing realistic 3Daudio rendering. For this reason, various room geometry inference (RGI) methodshave been developed by utilizing the time of arrival (TOA) or time differenceof arrival (TDOA) information in room impulse responses. However, theconventional RGI technique poses several assumptions, such as convex roomshapes, the number of walls known in priori, and the visibility of first-orderreflections. In this work, we introduce the deep neural network (DNN), RGI-Net,which can estimate room geometries without the aforementioned assumptions.RGI-Net learns and exploits complex relationships between high-orderreflections in room impulse responses (RIRs) and, thus, can estimate roomshapes even when the shape is non-convex or first-order reflections are missingin the RIRs. The network takes RIRs measured from a compact audio deviceequipped with a circular microphone array and a single loudspeaker, whichgreatly improves its practical applicability. RGI-Net includes the evaluationnetwork that separately evaluates the presence probability of walls, so thegeometry inference is possible without prior knowledge of the number of walls.</description><author>Inmo Yeon, Jung-Woo Choi</author><pubDate>Wed, 21 Feb 2024 06:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01513v2</guid></item><item><title>VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks</title><link>http://arxiv.org/abs/2402.13609v2</link><description>In recent years, object-oriented simultaneous localization and mapping (SLAM)has attracted increasing attention due to its ability to provide high-levelsemantic information while maintaining computational efficiency. Someresearchers have attempted to enhance localization accuracy by integrating themodeled object residuals into bundle adjustment. However, few have demonstratedbetter results than feature-based visual SLAM systems, as the generic coarseobject models, such as cuboids or ellipsoids, are less accurate than featurepoints. In this paper, we propose a Visual Object Odometry and Mappingframework VOOM using high-level objects and low-level points as thehierarchical landmarks in a coarse-to-fine manner instead of directly usingobject residuals in bundle adjustment. Firstly, we introduce an improvedobservation model and a novel data association method for dual quadrics,employed to represent physical objects. It facilitates the creation of a 3D mapthat closely reflects reality. Next, we use object information to enhance thedata association of feature points and consequently update the map. In thevisual object odometry backend, the updated map is employed to further optimizethe camera pose and the objects. Meanwhile, local bundle adjustment isperformed utilizing the objects and points-based covisibility graphs in ourvisual object mapping process. Experiments show that VOOM outperforms bothobject-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in termsof localization. The implementation of our method is available athttps://github.com/yutongwangBIT/VOOM.git.</description><author>Yutong Wang, Chaoyang Jiang, Xieyuanli Chen</author><pubDate>Mon, 26 Feb 2024 10:02:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13609v2</guid></item><item><title>VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks</title><link>http://arxiv.org/abs/2402.13609v1</link><description>In recent years, object-oriented simultaneous localization and mapping (SLAM)has attracted increasing attention due to its ability to provide high-levelsemantic information while maintaining computational efficiency. Someresearchers have attempted to enhance localization accuracy by integrating themodeled object residuals into bundle adjustment. However, few have demonstratedbetter results than feature-based visual SLAM systems, as the generic coarseobject models, such as cuboids or ellipsoids, are less accurate than featurepoints. In this paper, we propose a Visual Object Odometry and Mappingframework VOOM using high-level objects and low-level points as thehierarchical landmarks in a coarse-to-fine manner instead of directly usingobject residuals in bundle adjustment. Firstly, we introduce an improvedobservation model and a novel data association method for dual quadrics,employed to represent physical objects. It facilitates the creation of a 3D mapthat closely reflects reality. Next, we use object information to enhance thedata association of feature points and consequently update the map. In thevisual object odometry backend, the updated map is employed to further optimizethe camera pose and the objects. Meanwhile, local bundle adjustment isperformed utilizing the objects and points-based covisibility graphs in ourvisual object mapping process. Experiments show that VOOM outperforms bothobject-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in termsof localization. The implementation of our method is available athttps://github.com/yutongwangBIT/VOOM.git.</description><author>Yutong Wang, Chaoyang Jiang, Xieyuanli Chen</author><pubDate>Wed, 21 Feb 2024 08:22:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13609v1</guid></item><item><title>GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects</title><link>http://arxiv.org/abs/2309.16264v3</link><description>Articulated objects like cabinets and doors are widespread in daily life.However, directly manipulating 3D articulated objects is challenging becausethey have diverse geometrical shapes, semantic categories, and kineticconstraints. Prior works mostly focused on recognizing and manipulatingarticulated objects with specific joint types. They can either estimate thejoint parameters or distinguish suitable grasp poses to facilitate trajectoryplanning. Although these approaches have succeeded in certain types ofarticulated objects, they lack generalizability to unseen objects, whichsignificantly impedes their application in broader scenarios. In this paper, wepropose a novel framework of Generalizable Articulation Modeling andManipulating for Articulated Objects (GAMMA), which learns both articulationmodeling and grasp pose affordance from diverse articulated objects withdifferent categories. In addition, GAMMA adopts adaptive manipulation toiteratively reduce the modeling errors and enhance manipulation performance. Wetrain GAMMA with the PartNet-Mobility dataset and evaluate with comprehensiveexperiments in SAPIEN simulation and real-world Franka robot. Results show thatGAMMA significantly outperforms SOTA articulation modeling and manipulationalgorithms in unseen and cross-category articulated objects. We willopen-source all codes and datasets in both simulation and real robots forreproduction in the final version. Images and videos are published on theproject website at: http://sites.google.com/view/gamma-articulation</description><author>Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang, Cewu Lu</author><pubDate>Fri, 01 Mar 2024 13:29:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16264v3</guid></item><item><title>S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR</title><link>http://arxiv.org/abs/2402.14461v1</link><description>Scene graph generation (SGG) of surgical procedures is crucial in enhancingholistically cognitive intelligence in the operating room (OR). However,previous works have primarily relied on the multi-stage learning that generatessemantic scene graphs dependent on intermediate processes with pose estimationand object detection, which may compromise model efficiency and efficacy, alsoimpose extra annotation burden. In this study, we introduce a novelsingle-stage bimodal transformer framework for SGG in the OR, termedS^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3Dpoint clouds for SGG in an end-to-end manner. Concretely, our model embraces aView-Sync Transfusion scheme to encourage multi-view visual informationinteraction. Concurrently, a Geometry-Visual Cohesion operation is designed tointegrate the synergic 2D semantic features into 3D point cloud features.Moreover, based on the augmented feature, we propose a novel relation-sensitivetransformer decoder that embeds dynamic entity-pair queries and relationaltrait priors, which enables the direct prediction of entity-pair relations forgraph generation without intermediate steps. Extensive experiments havevalidated the superior SGG performance and lower computational cost ofS^2Former-OR on 4D-OR benchmark, compared with current OR-SGG methods, e.g., 3%Precision increase and 24.2M reduction in model parameters. We further comparedour method with generic single-stage SGG methods with broader metrics for acomprehensive evaluation, with consistently better performance achieved. Thecode will be made available.</description><author>Jialun Pei, Diandian Guo, Jingyang Zhang, Manxi Lin, Yueming Jin, Pheng-Ann Heng</author><pubDate>Thu, 22 Feb 2024 11:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14461v1</guid></item><item><title>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</title><link>http://arxiv.org/abs/2402.03246v2</link><description>Semantic understanding plays a crucial role in Dense SimultaneousLocalization and Mapping (SLAM). Recent advancements that integrate GaussianSplatting into SLAM systems have demonstrated its effectiveness in generatinghigh-quality renderings. Building on this progress, we propose SGS-SLAM whichprovides precise 3D semantic segmentation alongside high-fidelityreconstructions. Specifically, we propose to employ multi-channel optimizationduring the mapping process, integrating appearance, geometric, and semanticconstraints with key-frame optimization to enhance reconstruction quality.Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-artperformance in camera pose estimation, map reconstruction, and semanticsegmentation. It outperforms existing methods by a large margin meanwhilepreserves real-time rendering ability.</description><author>Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang</author><pubDate>Sun, 25 Feb 2024 17:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03246v2</guid></item><item><title>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</title><link>http://arxiv.org/abs/2402.03246v3</link><description>Semantic understanding plays a crucial role in Dense SimultaneousLocalization and Mapping (SLAM). Recent advancements that integrate GaussianSplatting into SLAM systems have demonstrated its effectiveness in generatinghigh-quality renderings. Building on this progress, we propose SGS-SLAM whichprovides precise 3D semantic segmentation alongside high-fidelityreconstructions. Specifically, we propose to employ multi-channel optimizationduring the mapping process, integrating appearance, geometric, and semanticconstraints with key-frame optimization to enhance reconstruction quality.Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-artperformance in camera pose estimation, map reconstruction, and semanticsegmentation. It outperforms existing methods by a large margin meanwhilepreserving real-time rendering ability.</description><author>Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang</author><pubDate>Sat, 02 Mar 2024 13:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03246v3</guid></item><item><title>RMS: Redundancy-Minimizing Point Cloud Sampling for Real-Time Pose Estimation</title><link>http://arxiv.org/abs/2312.07337v2</link><description>The typical point cloud sampling methods used in state estimation for mobilerobots preserve a high level of point redundancy. This redundancy unnecessarilyslows down the estimation pipeline and may cause drift under real-timeconstraints. Such undue latency becomes a bottleneck for resource-constrainedrobots (especially UAVs), requiring minimal delay for agile and accurateoperation. We propose a novel, deterministic, uninformed, and single-parameterpoint cloud sampling method named RMS that minimizes redundancy within a 3Dpoint cloud. In contrast to the state of the art, RMS balances thetranslation-space observability by leveraging the fact that linear and planarsurfaces inherently exhibit high redundancy propagated into iterativeestimation pipelines. We define the concept of gradient flow, quantifying thelocal surface underlying a point. We also show that maximizing the entropy ofthe gradient flow minimizes point redundancy for robot ego-motion estimation.We integrate RMS into the point-based KISS-ICP and feature-based LOAM odometrypipelines and evaluate experimentally on KITTI, Hilti-Oxford, and customdatasets from multirotor UAVs. The experiments demonstrate that RMS outperformsstate-of-the-art methods in speed, compression, and accuracy inwell-conditioned as well as in geometrically-degenerated settings.</description><author>Pavel Petracek, Kostas Alexis, Martin Saska</author><pubDate>Thu, 29 Feb 2024 13:05:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07337v2</guid></item><item><title>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</title><link>http://arxiv.org/abs/2305.14836v2</link><description>We introduce a novel visual question answering (VQA) task in the context ofautonomous driving, aiming to answer natural language questions based onstreet-view clues. Compared to traditional VQA tasks, VQA in autonomous drivingscenario presents more challenges. Firstly, the raw visual data aremulti-modal, including images and point clouds captured by camera and LiDAR,respectively. Secondly, the data are multi-frame due to the continuous,real-time acquisition. Thirdly, the outdoor scenes exhibit both movingforeground and static background. Existing VQA benchmarks fail to adequatelyaddress these complexities. To bridge this gap, we propose NuScenes-QA, thefirst benchmark for VQA in the autonomous driving scenario, encompassing 34Kvisual scenes and 460K question-answer pairs. Specifically, we leverageexisting 3D detection annotations to generate scene graphs and design questiontemplates manually. Subsequently, the question-answer pairs are generatedprogrammatically based on these templates. Comprehensive statistics prove thatour NuScenes-QA is a balanced large-scale benchmark with diverse questionformats. Built upon it, we develop a series of baselines that employ advanced3D detection and VQA techniques. Our extensive experiments highlight thechallenges posed by this new task. Codes and dataset are available athttps://github.com/qiantianwen/NuScenes-QA.</description><author>Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang</author><pubDate>Tue, 20 Feb 2024 05:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14836v2</guid></item><item><title>Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation</title><link>http://arxiv.org/abs/2402.18920v1</link><description>Although 3D shape matching and interpolation are highly interrelated, theyare often studied separately and applied sequentially to relate different 3Dshapes, thus resulting in sub-optimal performance. In this work we present aunified framework to predict both point-wise correspondences and shapeinterpolation between 3D shapes. To this end, we combine the deep functionalmap framework with classical surface deformation models to map shapes in bothspectral and spatial domains. On the one hand, by incorporating spatial maps,our method obtains more accurate and smooth point-wise correspondences comparedto previous functional map methods for shape matching. On the other hand, byintroducing spectral maps, our method gets rid of commonly used butcomputationally expensive geodesic distance constraints that are only valid fornear-isometric shape deformations. Furthermore, we propose a novel test-timeadaptation scheme to capture both pose-dominant and shape-dominantdeformations. Using different challenging datasets, we demonstrate that ourmethod outperforms previous state-of-the-art methods for both shape matchingand interpolation, even compared to supervised approaches.</description><author>Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard</author><pubDate>Thu, 29 Feb 2024 07:26:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18920v1</guid></item><item><title>Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation</title><link>http://arxiv.org/abs/2402.18920v2</link><description>Although 3D shape matching and interpolation are highly interrelated, theyare often studied separately and applied sequentially to relate different 3Dshapes, thus resulting in sub-optimal performance. In this work we present aunified framework to predict both point-wise correspondences and shapeinterpolation between 3D shapes. To this end, we combine the deep functionalmap framework with classical surface deformation models to map shapes in bothspectral and spatial domains. On the one hand, by incorporating spatial maps,our method obtains more accurate and smooth point-wise correspondences comparedto previous functional map methods for shape matching. On the other hand, byintroducing spectral maps, our method gets rid of commonly used butcomputationally expensive geodesic distance constraints that are only valid fornear-isometric shape deformations. Furthermore, we propose a novel test-timeadaptation scheme to capture both pose-dominant and shape-dominantdeformations. Using different challenging datasets, we demonstrate that ourmethod outperforms previous state-of-the-art methods for both shape matchingand interpolation, even compared to supervised approaches.</description><author>Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard</author><pubDate>Mon, 04 Mar 2024 15:43:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18920v2</guid></item><item><title>SDGE: Stereo Guided Depth Estimation for 360° Camera Sets</title><link>http://arxiv.org/abs/2402.11791v1</link><description>Depth estimation is a critical technology in autonomous driving, andmulti-camera systems are often used to achieve a 360{\deg} perception. These360{\deg} camera sets often have limited or low-quality overlap regions, makingmulti-view stereo methods infeasible for the entire image. Alternatively,monocular methods may not produce consistent cross-view predictions. To addressthese issues, we propose the Stereo Guided Depth Estimation (SGDE) method,which enhances depth estimation of the full image by explicitly utilizingmulti-view stereo results on the overlap. We suggest building virtual pinholecameras to resolve the distortion problem of fisheye cameras and unify theprocessing for the two types of 360{\deg} cameras. For handling the varyingnoise on camera poses caused by unstable movement, the approach employs aself-calibration method to obtain highly accurate relative poses of theadjacent cameras with minor overlap. These enable the use of robust stereomethods to obtain high-quality depth prior in the overlap region. This priorserves not only as an additional input but also as pseudo-labels that enhancethe accuracy of depth estimation methods and improve cross-view predictionconsistency. The effectiveness of SGDE is evaluated on one fisheye cameradataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes.Our experiments demonstrate that SGDE is effective for both supervised andself-supervised depth estimation, and highlight the potential of our method foradvancing downstream autonomous driving technologies, such as 3D objectdetection and occupancy prediction.</description><author>Jialei Xu, Xianming Liu, Junjun Jiang, Xiangyang Ji</author><pubDate>Mon, 19 Feb 2024 02:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11791v1</guid></item><item><title>Finding Waldo: Towards Efficient Exploration of NeRF Scene Space</title><link>http://arxiv.org/abs/2403.04508v1</link><description>Neural Radiance Fields (NeRF) have quickly become the primary approach for 3Dreconstruction and novel view synthesis in recent years due to their remarkableperformance. Despite the huge interest in NeRF methods, a practical use case ofNeRFs has largely been ignored; the exploration of the scene space modelled bya NeRF. In this paper, for the first time in the literature, we propose andformally define the scene exploration framework as the efficient discovery ofNeRF model inputs (i.e. coordinates and viewing angles), using which one canrender novel views that adhere to user-selected criteria. To remedy the lack ofapproaches addressing scene exploration, we first propose two baseline methodscalled Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS).We then cast scene exploration as an optimization problem, and propose thecriteria-agnostic Evolution-Guided Pose Search (EGPS) for efficientexploration. We test all three approaches with various criteria (e.g. saliencymaximization, image quality maximization, photo-composition qualityimprovement) and show that our EGPS performs more favourably than otherbaselines. We finally highlight key points and limitations, and outlinedirections for future research in scene exploration.</description><author>Evangelos Skartados, Mehmet Kerim Yucel, Bruno Manganelli, Anastasios Drosou, Albert Saà-Garriga</author><pubDate>Thu, 07 Mar 2024 14:08:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04508v1</guid></item><item><title>SDGE: Stereo Guided Depth Estimation for 360$^\circ$ Camera Sets</title><link>http://arxiv.org/abs/2402.11791v3</link><description>Depth estimation is a critical technology in autonomous driving, andmulti-camera systems are often used to achieve a 360$^\circ$ perception. These360$^\circ$ camera sets often have limited or low-quality overlap regions,making multi-view stereo methods infeasible for the entire image.Alternatively, monocular methods may not produce consistent cross-viewpredictions. To address these issues, we propose the Stereo Guided DepthEstimation (SGDE) method, which enhances depth estimation of the full image byexplicitly utilizing multi-view stereo results on the overlap. We suggestbuilding virtual pinhole cameras to resolve the distortion problem of fisheyecameras and unify the processing for the two types of 360$^\circ$ cameras. Forhandling the varying noise on camera poses caused by unstable movement, theapproach employs a self-calibration method to obtain highly accurate relativeposes of the adjacent cameras with minor overlap. These enable the use ofrobust stereo methods to obtain high-quality depth prior in the overlap region.This prior serves not only as an additional input but also as pseudo-labelsthat enhance the accuracy of depth estimation methods and improve cross-viewprediction consistency. The effectiveness of SGDE is evaluated on one fisheyecamera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD andnuScenes. Our experiments demonstrate that SGDE is effective for bothsupervised and self-supervised depth estimation, and highlight the potential ofour method for advancing downstream autonomous driving technologies, such as 3Dobject detection and occupancy prediction.</description><author>Jialei Xu, Wei Yin, Dong Gong, Junjun Jiang, Xianming Liu</author><pubDate>Thu, 29 Feb 2024 06:33:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11791v3</guid></item><item><title>SDGE: Stereo Guided Depth Estimation for 360$^\circ$ Camera Sets</title><link>http://arxiv.org/abs/2402.11791v2</link><description>Depth estimation is a critical technology in autonomous driving, andmulti-camera systems are often used to achieve a 360$^\circ$ perception. These360$^\circ$ camera sets often have limited or low-quality overlap regions,making multi-view stereo methods infeasible for the entire image.Alternatively, monocular methods may not produce consistent cross-viewpredictions. To address these issues, we propose the Stereo Guided DepthEstimation (SGDE) method, which enhances depth estimation of the full image byexplicitly utilizing multi-view stereo results on the overlap. We suggestbuilding virtual pinhole cameras to resolve the distortion problem of fisheyecameras and unify the processing for the two types of 360$^\circ$ cameras. Forhandling the varying noise on camera poses caused by unstable movement, theapproach employs a self-calibration method to obtain highly accurate relativeposes of the adjacent cameras with minor overlap. These enable the use ofrobust stereo methods to obtain high-quality depth prior in the overlap region.This prior serves not only as an additional input but also as pseudo-labelsthat enhance the accuracy of depth estimation methods and improve cross-viewprediction consistency. The effectiveness of SGDE is evaluated on one fisheyecamera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD andnuScenes. Our experiments demonstrate that SGDE is effective for bothsupervised and self-supervised depth estimation, and highlight the potential ofour method for advancing downstream autonomous driving technologies, such as 3Dobject detection and occupancy prediction.</description><author>Jialei Xu, Wei Yin, Dong Gong, Xianming Liu, Junjun Jiang, Xiangyang Ji</author><pubDate>Mon, 26 Feb 2024 12:01:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11791v2</guid></item><item><title>Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot</title><link>http://arxiv.org/abs/2402.14654v1</link><description>We present Multi-HMR, a strong single-shot model for multi-person 3D humanmesh recovery from a single RGB image. Predictions encompass the whole body,i.e, including hands and facial expressions, using the SMPL-X parametric modeland spatial location in the camera coordinate system. Our model detects peopleby predicting coarse 2D heatmaps of person centers, using features produced bya standard Vision Transformer (ViT) backbone. It then predicts their whole-bodypose, shape and spatial location using a new cross-attention module called theHuman Prediction Head (HPH), with one query per detected center token,attending to the entire set of features. As direct prediction of SMPL-Xparameters yields suboptimal results, we introduce CUFFS; the Close-Up Framesof Full-Body Subjects dataset, containing humans close to the camera withdiverse hand poses. We show that incorporating this dataset into trainingfurther enhances predictions, particularly for hands, enabling us to achievestate-of-the-art performance. Multi-HMR also optionally accounts for cameraintrinsics, if available, by encoding camera ray directions for each imagetoken. This simple design achieves strong performance on whole-body andbody-only benchmarks simultaneously. We train models with various backbonesizes and input resolutions. In particular, using a ViT-S backbone and$448\times448$ input images already yields a fast and competitive model withrespect to state-of-the-art methods, while considering larger models and higherresolutions further improve performance.</description><author>Fabien Baradel, Matthieu Armando, Salma Galaaoui, Romain Brégier, Philippe Weinzaepfel, Grégory Rogez, Thomas Lucas</author><pubDate>Thu, 22 Feb 2024 16:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14654v1</guid></item><item><title>DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder</title><link>http://arxiv.org/abs/2303.17550v5</link><description>While recent research has made significant progress in speech-driven talkingface generation, the quality of the generated video still lags behind that ofreal recordings. One reason for this is the use of handcrafted intermediaterepresentations like facial landmarks and 3DMM coefficients, which are designedbased on human knowledge and are insufficient to precisely describe facialmovements. Additionally, these methods require an external pretrained model forextracting these representations, whose performance sets an upper bound ontalking face generation. To address these limitations, we propose a novelmethod called DAE-Talker that leverages data-driven latent representationsobtained from a diffusion autoencoder (DAE). DAE contains an image encoder thatencodes an image into a latent vector and a DDIM image decoder thatreconstructs the image from it. We train our DAE on talking face video framesand then extract their latent representations as the training target for aConformer-based speech2latent model. This allows DAE-Talker to synthesize fullvideo frames and produce natural head movements that align with the content ofspeech, rather than relying on a predetermined head pose from a template video.We also introduce pose modelling in speech2latent for pose controllability.Additionally, we propose a novel method for generating continuous video frameswith the DDIM image decoder trained on individual frames, eliminating the needfor modelling the joint distribution of consecutive frames directly. Ourexperiments show that DAE-Talker outperforms existing popular methods inlip-sync, video fidelity, and pose naturalness. We also conduct ablationstudies to analyze the effectiveness of the proposed techniques and demonstratethe pose controllability of DAE-Talker.</description><author>Chenpeng Du, Qi Chen, Xie Chen, Kai Yu</author><pubDate>Fri, 01 Mar 2024 11:43:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.17550v5</guid></item><item><title>CapHuman: Capture Your Moments in Parallel Universes</title><link>http://arxiv.org/abs/2402.00627v2</link><description>We concentrate on a novel human-centric image synthesis task, that is, givenonly one reference facial photograph, it is expected to generate specificindividual images with diverse head positions, poses, facial expressions, andilluminations in different contexts. To accomplish this goal, we argue that ourgenerative model should be capable of the following favorable characteristics:(1) a strong visual and semantic understanding of our world and human societyfor basic object and human image generation. (2) generalizable identitypreservation ability. (3) flexible and fine-grained head control. Recently,large pre-trained text-to-image diffusion models have shown remarkable results,serving as a powerful generative foundation. As a basis, we aim to unleash theabove two capabilities of the pre-trained model. In this work, we present a newframework named CapHuman. We embrace the "encode then learn to align" paradigm,which enables generalizable identity preservation for new individuals withoutcumbersome tuning at inference. CapHuman encodes identity features and thenlearns to align them into the latent space. Moreover, we introduce the 3Dfacial prior to equip our model with control over the human head in a flexibleand 3D-consistent manner. Extensive qualitative and quantitative analysesdemonstrate our CapHuman can produce well-identity-preserved, photo-realistic,and high-fidelity portraits with content-rich representations and various headrenditions, superior to established baselines. Code and checkpoint will bereleased at https://github.com/VamosC/CapHuman.</description><author>Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang</author><pubDate>Mon, 19 Feb 2024 11:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00627v2</guid></item><item><title>AdvGPS: Adversarial GPS for Multi-Agent Perception Attack</title><link>http://arxiv.org/abs/2401.17499v2</link><description>The multi-agent perception system collects visual data from sensors locatedon various agents and leverages their relative poses determined by GPS signalsto effectively fuse information, mitigating the limitations of single-agentsensing, such as occlusion. However, the precision of GPS signals can beinfluenced by a range of factors, including wireless transmission andobstructions like buildings. Given the pivotal role of GPS signals inperception fusion and the potential for various interference, it becomesimperative to investigate whether specific GPS signals can easily mislead themulti-agent perception system. To address this concern, we frame the task as anadversarial attack challenge and introduce \textsc{AdvGPS}, a method capable ofgenerating adversarial GPS signals which are also stealthy for individualagents within the system, significantly reducing object detection accuracy. Toenhance the success rates of these attacks in a black-box scenario, weintroduce three types of statistically sensitive natural discrepancies:appearance-based discrepancy, distribution-based discrepancy, and task-awarediscrepancy. Our extensive experiments on the OPV2V dataset demonstrate thatthese attacks substantially undermine the performance of state-of-the-artmethods, showcasing remarkable transferability across different point cloudbased 3D detection systems. This alarming revelation underscores the pressingneed to address security implications within multi-agent perception systems,thereby underscoring a critical area of research.</description><author>Jinlong Li, Baolu Li, Xinyu Liu, Jianwu Fang, Felix Juefei-Xu, Qing Guo, Hongkai Yu</author><pubDate>Tue, 20 Feb 2024 20:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17499v2</guid></item></channel></rss>