<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 03 Dec 2023 14:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling</title><link>http://arxiv.org/abs/2306.17201v1</link><description>Estimating 3D human poses only from a 2D human pose sequence is thoroughlyexplored in recent years. Yet, prior to this, no such work has attempted tounify 2D and 3D pose representations in the shared feature space. In thispaper, we propose MPM, a unified 2D-3D human pose representation framework viamasked pose modeling. We treat 2D and 3D poses as two different modalities likevision and language and build a single-stream transformer-based architecture.We apply three pretext tasks, which are masked 2D pose modeling, masked 3D posemodeling, and masked 2D pose lifting to pre-train our network and usefull-supervision to perform further fine-tuning. A high masking ratio of 72.5%in total with a spatio-temporal mask sampling strategy leading to betterrelation modeling both in spatial and temporal domains. MPM can handle multipletasks including 3D human pose estimation, 3D pose estimation from occluded 2Dpose, and 3D pose completion in a single framework. We conduct extensiveexperiments and ablation studies on several widely used human pose datasets andachieve state-of-the-art performance on Human3.6M and MPI-INF-3DHP. Codes andmodel checkpoints are available at https://github.com/vvirgooo2/MPM</description><author>Zhenyu Zhang, Wenhao Chai, Zhongyu Jiang, Tian Ye, Mingli Song, Jenq-Neng Hwang, Gaoang Wang</author><pubDate>Thu, 29 Jun 2023 11:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17201v1</guid></item><item><title>Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation</title><link>http://arxiv.org/abs/2303.11579v2</link><description>In this paper, a novel Diffusion-based 3D Pose estimation (D3DP) method withJoint-wise reProjection-based Multi-hypothesis Aggregation (JPMA) is proposedfor probabilistic 3D human pose estimation. On the one hand, D3DP generatesmultiple possible 3D pose hypotheses for a single 2D observation. It graduallydiffuses the ground truth 3D poses to a random distribution, and learns adenoiser conditioned on 2D keypoints to recover the uncontaminated 3D poses.The proposed D3DP is compatible with existing 3D pose estimators and supportsusers to balance efficiency and accuracy during inference through twocustomizable parameters. On the other hand, JPMA is proposed to assemblemultiple hypotheses generated by D3DP into a single 3D pose for practical use.It reprojects 3D pose hypotheses to the 2D camera plane, selects the besthypothesis joint-by-joint based on the reprojection errors, and combines theselected joints into the final pose. The proposed JPMA conducts aggregation atthe joint level and makes use of the 2D prior information, both of which havebeen overlooked by previous approaches. Extensive experiments on Human3.6M andMPI-INF-3DHP datasets show that our method outperforms the state-of-the-artdeterministic and probabilistic approaches by 1.5% and 8.9%, respectively. Codeis available at https://github.com/paTRICK-swk/D3DP.</description><author>Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang, Kai Han, Shanshe Wang, Siwei Ma, Wen Gao</author><pubDate>Wed, 23 Aug 2023 04:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11579v2</guid></item><item><title>PoseFix: Correcting 3D Human Poses with Natural Language</title><link>http://arxiv.org/abs/2309.08480v1</link><description>Automatically producing instructions to modify one's posture could open thedoor to endless applications, such as personalized coaching and in-homephysical therapy. Tackling the reverse problem (i.e., refining a 3D pose basedon some natural language feedback) could help for assisted 3D characteranimation or robot teaching, for instance. Although a few recent works explorethe connections between natural language and 3D human pose, none focus ondescribing 3D body pose differences. In this paper, we tackle the problem ofcorrecting 3D human poses with natural language. To this end, we introduce thePoseFix dataset, which consists of several thousand paired 3D poses and theircorresponding text feedback, that describe how the source pose needs to bemodified to obtain the target pose. We demonstrate the potential of thisdataset on two tasks: (1) text-based pose editing, that aims at generatingcorrected 3D body poses given a query pose and a text modifier; and (2)correctional text generation, where instructions are generated based on thedifferences between two body poses.</description><author>Ginger Delmas, Philippe Weinzaepfel, Francesc Moreno-Noguer, Grégory Rogez</author><pubDate>Fri, 15 Sep 2023 16:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08480v1</guid></item><item><title>Multi-person 3D pose estimation from unlabelled data</title><link>http://arxiv.org/abs/2212.08731v2</link><description>Its numerous applications make multi-human 3D pose estimation a remarkablyimpactful area of research. Nevertheless, assuming a multiple-view systemcomposed of several regular RGB cameras, 3D multi-pose estimation presentsseveral challenges. First of all, each person must be uniquely identified inthe different views to separate the 2D information provided by the cameras.Secondly, the 3D pose estimation process from the multi-view 2D information ofeach person must be robust against noise and potential occlusions in thescenario. In this work, we address these two challenges with the help of deeplearning. Specifically, we present a model based on Graph Neural Networkscapable of predicting the cross-view correspondence of the people in thescenario along with a Multilayer Perceptron that takes the 2D points to yieldthe 3D poses of each person. These two models are trained in a self-supervisedmanner, thus avoiding the need for large datasets with 3D annotations.</description><author>Daniel Rodriguez-Criado, Pilar Bachiller, George Vogiatzis, Luis J. Manso</author><pubDate>Thu, 19 Oct 2023 11:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08731v2</guid></item><item><title>3DHumanGAN: 3D-Aware Human Image Generation with 3D Pose Mapping</title><link>http://arxiv.org/abs/2212.07378v2</link><description>We present 3DHumanGAN, a 3D-aware generative adversarial network thatsynthesizes photorealistic images of full-body humans with consistentappearances under different view-angles and body-poses. To tackle therepresentational and computational challenges in synthesizing the articulatedstructure of human bodies, we propose a novel generator architecture in which a2D convolutional backbone is modulated by a 3D pose mapping network. The 3Dpose mapping network is formulated as a renderable implicit functionconditioned on a posed 3D human mesh. This design has several merits: i) itleverages the strength of 2D GANs to produce high-quality images; ii) itgenerates consistent images under varying view-angles and poses; iii) the modelcan incorporate the 3D human prior and enable pose conditioning. Project page:https://3dhumangan.github.io/.</description><author>Zhuoqian Yang, Shikai Li, Wayne Wu, Bo Dai</author><pubDate>Sun, 24 Sep 2023 23:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07378v2</guid></item><item><title>Two Views Are Better than One: Monocular 3D Pose Estimation with Multiview Consistency</title><link>http://arxiv.org/abs/2311.12421v1</link><description>Deducing a 3D human pose from a single 2D image or 2D keypoints is inherentlychallenging, given the fundamental ambiguity wherein multiple 3D poses cancorrespond to the same 2D representation. The acquisition of 3D data, whileinvaluable for resolving pose ambiguity, is expensive and requires an intricatesetup, often restricting its applicability to controlled lab environments. Weimprove performance of monocular human pose estimation models using multiviewdata for fine-tuning. We propose a novel loss function, multiview consistency,to enable adding additional training data with only 2D supervision. This lossenforces that the inferred 3D pose from one view aligns with the inferred 3Dpose from another view under similarity transformations. Our consistency losssubstantially improves performance for fine-tuning with no available 3D data.Our experiments demonstrate that two views offset by 90 degrees are enough toobtain good performance, with only marginal improvements by adding more views.Thus, we enable the acquisition of domain-specific data by capturing activitieswith off-the-shelf cameras, eliminating the need for elaborate calibrationprocedures. This research introduces new possibilities for domain adaptation in3D pose estimation, providing a practical and cost-effective solution tocustomize models for specific applications. The used dataset, featuringadditional views, will be made publicly available.</description><author>Christian Keilstrup Ingwersen, Anders Bjorholm Dahl, Janus Nørtoft Jensen, Morten Rieger Hannemose</author><pubDate>Tue, 21 Nov 2023 08:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12421v1</guid></item><item><title>3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and Point Cloud</title><link>http://arxiv.org/abs/2311.04699v1</link><description>Greenhouse production of fruits and vegetables in developed countries ischallenged by labor 12 scarcity and high labor costs. Robots offer a goodsolution for sustainable and cost-effective 13 production. Acquiring accuratespatial information about relevant plant parts is vital for 14 successful robotoperation. Robot perception in greenhouses is challenging due to variations in15 plant appearance, viewpoints, and illumination. This paper proposes akeypoint-detection-based 16 method using data from an RGB-D camera to estimatethe 3D pose of peduncle nodes, which 17 provides essential information toharvest the tomato bunches. 18 19 Specifically, this paper proposes a methodthat detects four anatomical landmarks in the color 20 image and thenintegrates 3D point-cloud information to determine the 3D pose. A 21comprehensive evaluation was conducted in a commercial greenhouse to gaininsight into the 22 performance of different parts of the method. The resultsshowed: (1) high accuracy in object 23 detection, achieving an AveragePrecision (AP) of AP@0.5=0.96; (2) an average Percentage of 24 Detected Joints(PDJ) of the keypoints of PhDJ@0.2=94.31%; and (3) 3D pose estimation 25accuracy with mean absolute errors (MAE) of 11.38o and 9.93o for the relativeupper and lower 26 angles between the peduncle and main stem, respectively.Furthermore, the capability to handle 27 variations in viewpoint wasinvestigated, demonstrating the method was robust to view changes. 28 However,canonical and higher views resulted in slightly higher performance compared toother 29 views. Although tomato was selected as a use case, the proposed methodis also applicable to 30 other greenhouse crops like pepper.</description><author>Jianchao Ci, Xin Wang, David Rapado-Rincón, Akshay K. Burusa, Gert Kootstra</author><pubDate>Wed, 08 Nov 2023 14:10:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04699v1</guid></item><item><title>Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.11667v1</link><description>As 3D human pose estimation can now be achieved with very high accuracy inthe supervised learning scenario, tackling the case where 3D pose annotationsare not available has received increasing attention. In particular, severalmethods have proposed to learn image representations in a self-supervisedfashion so as to disentangle the appearance information from the pose one. Themethods then only need a small amount of supervised data to train a poseregressor using the pose-related latent vector as input, as it should be freeof appearance information. In this paper, we carry out in-depth analysis tounderstand to what degree the state-of-the-art disentangled representationlearning methods truly separate the appearance information from the pose one.First, we study disentanglement from the perspective of the self-supervisednetwork, via diverse image synthesis experiments. Second, we investigatedisentanglement with respect to the 3D pose regressor following an adversarialattack perspective. Specifically, we design an adversarial strategy focusing ongenerating natural appearance changes of the subject, and against which wecould expect a disentangled network to be robust. Altogether, our analyses showthat disentanglement in the three state-of-the-art disentangled representationlearning frameworks if far from complete, and that their pose codes containsignificant appearance information. We believe that our approach provides avaluable testbed to evaluate the degree of disentanglement of pose fromappearance in self-supervised 3D human pose estimation.</description><author>Krishna Kanth Nakka, Mathieu Salzmann</author><pubDate>Wed, 20 Sep 2023 23:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11667v1</guid></item><item><title>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2308.09678v2</link><description>Existing 3D human pose estimators face challenges in adapting to new datasetsdue to the lack of 2D-3D pose pairs in training sets. To overcome this issue,we propose \textit{Multi-Hypothesis \textbf{P}ose \textbf{Syn}thesis\textbf{D}omain \textbf{A}daptation} (\textbf{PoSynDA}) framework to bridgethis data disparity gap in target domain. Typically, PoSynDA uses adiffusion-inspired structure to simulate 3D pose distribution in the targetdomain. By incorporating a multi-hypothesis network, PoSynDA generates diversepose hypotheses and aligns them with the target domain. To do this, it firstutilizes target-specific source augmentation to obtain the target domaindistribution data from the source domain by decoupling the scale and positionparameters. The process is then further refined through the teacher-studentparadigm and low-rank adaptation. With extensive comparison of benchmarks suchas Human3.6M and MPI-INF-3DHP, PoSynDA demonstrates competitive performance,even comparable to the target-trained MixSTE model\cite{zhang2022mixste}. Thiswork paves the way for the practical application of 3D human pose estimation inunseen domains. The code is available at https://github.com/hbing-l/PoSynDA.</description><author>Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 16 Oct 2023 18:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09678v2</guid></item><item><title>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</title><link>http://arxiv.org/abs/2308.11737v1</link><description>Accurately estimating the 3D pose and shape is an essential step towardsunderstanding animal behavior, and can potentially benefit many downstreamapplications, such as wildlife conservation. However, research in this area isheld back by the lack of a comprehensive and diverse dataset with high-quality3D pose and shape annotations. In this paper, we propose Animal3D, the firstcomprehensive dataset for mammal animal 3D pose and shape estimation. Animal3Dconsists of 3379 images collected from 40 mammal species, high-qualityannotations of 26 keypoints, and importantly the pose and shape parameters ofthe SMAL model. All annotations were labeled and checked manually in amulti-stage process to ensure highest quality results. Based on the Animal3Ddataset, we benchmark representative shape and pose estimation models at: (1)supervised learning from only the Animal3D data, (2) synthetic to real transferfrom synthetically generated images, and (3) fine-tuning human pose and shapeestimation models. Our experimental results demonstrate that predicting the 3Dshape and pose of animals across species remains a very challenging task,despite significant advances in human pose estimation. Our results furtherdemonstrate that synthetic pre-training is a viable strategy to boost the modelperformance. Overall, Animal3D opens new directions for facilitating futureresearch in animal 3D pose and shape estimation, and is publicly available.</description><author>Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang, Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen Cui, Alan Yuille, Adam Kortylewski</author><pubDate>Tue, 22 Aug 2023 19:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11737v1</guid></item><item><title>DiffuPose: Monocular 3D Human Pose Estimation via Denoising Diffusion Probabilistic Model</title><link>http://arxiv.org/abs/2212.02796v3</link><description>Thanks to the development of 2D keypoint detectors, monocular 3D human poseestimation (HPE) via 2D-to-3D uplifting approaches have achieved remarkableimprovements. Still, monocular 3D HPE is a challenging problem due to theinherent depth ambiguities and occlusions. To handle this problem, manyprevious works exploit temporal information to mitigate such difficulties.However, there are many real-world applications where frame sequences are notaccessible. This paper focuses on reconstructing a 3D pose from a single 2Dkeypoint detection. Rather than exploiting temporal information, we alleviatethe depth ambiguity by generating multiple 3D pose candidates which can bemapped to an identical 2D keypoint. We build a novel diffusion-based frameworkto effectively sample diverse 3D poses from an off-the-shelf 2D detector. Byconsidering the correlation between human joints by replacing the conventionaldenoising U-Net with graph convolutional network, our approach accomplishesfurther performance improvements. We evaluate our method on the widely adoptedHuman3.6M and HumanEva-I datasets. Comprehensive experiments are conducted toprove the efficacy of the proposed method, and they confirm that our modeloutperforms state-of-the-art multi-hypothesis 3D HPE methods.</description><author>Jeongjun Choi, Dongseok Shim, H. Jin Kim</author><pubDate>Thu, 03 Aug 2023 10:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02796v3</guid></item><item><title>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2308.09678v1</link><description>The current 3D human pose estimators face challenges in adapting to newdatasets due to the scarcity of 2D-3D pose pairs in target domain trainingsets. We present the \textit{Multi-Hypothesis \textbf{P}ose \textbf{Syn}thesis\textbf{D}omain \textbf{A}daptation} (\textbf{PoSynDA}) framework to overcomethis issue without extensive target domain annotation. Utilizing adiffusion-centric structure, PoSynDA simulates the 3D pose distribution in thetarget domain, filling the data diversity gap. By incorporating amulti-hypothesis network, it creates diverse pose hypotheses and aligns themwith the target domain. Target-specific source augmentation obtains the targetdomain distribution data from the source domain by decoupling the scale andposition parameters. The teacher-student paradigm and low-rank adaptationfurther refine the process. PoSynDA demonstrates competitive performance onbenchmarks, such as Human3.6M, MPI-INF-3DHP, and 3DPW, even comparable with thetarget-trained MixSTE model~\cite{zhang2022mixste}. This work paves the way forthe practical application of 3D human pose estimation. The code is available athttps://github.com/hbing-l/PoSynDA.</description><author>Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Fri, 18 Aug 2023 17:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09678v1</guid></item><item><title>PoseGPT: Chatting about 3D Human Pose</title><link>http://arxiv.org/abs/2311.18836v1</link><description>We introduce PoseGPT, a framework employing Large Language Models (LLMs) tounderstand and reason about 3D human poses from images or textual descriptions.Our work is motivated by the human ability to intuitively understand posturesfrom a single image or a brief description, a process that intertwines imageinterpretation, world knowledge, and an understanding of body language.Traditional human pose estimation methods, whether image-based or text-based,often lack holistic scene comprehension and nuanced reasoning, leading to adisconnect between visual data and its real-world implications. PoseGPTaddresses these limitations by embedding SMPL poses as a distinct signal tokenwithin a multi-modal LLM, enabling direct generation of 3D body poses from bothtextual and visual inputs. This approach not only simplifies pose predictionbut also empowers LLMs to apply their world knowledge in reasoning about humanposes, fostering two advanced tasks: speculative pose generation and reasoningabout pose estimation. These tasks involve reasoning about humans to generate3D poses from subtle text queries, possibly accompanied by images. We establishbenchmarks for these tasks, moving beyond traditional 3D pose generation andestimation methods. Our results show that PoseGPT outperforms existingmultimodal LLMs and task-sepcific methods on these newly proposed tasks.Furthermore, PoseGPT's ability to understand and generate 3D human poses basedon complex reasoning opens new directions in human pose analysis.</description><author>Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Michael J. Black</author><pubDate>Thu, 30 Nov 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18836v1</guid></item><item><title>POCO: 3D Pose and Shape Estimation with Confidence</title><link>http://arxiv.org/abs/2308.12965v1</link><description>The regression of 3D Human Pose and Shape (HPS) from an image is becomingincreasingly accurate. This makes the results useful for downstream tasks likehuman action recognition or 3D graphics. Yet, no regressor is perfect, andaccuracy can be affected by ambiguous image evidence or by poses and appearancethat are unseen during training. Most current HPS regressors, however, do notreport the confidence of their outputs, meaning that downstream tasks cannotdifferentiate accurate estimates from inaccurate ones. To address this, wedevelop POCO, a novel framework for training HPS regressors to estimate notonly a 3D human body, but also their confidence, in a single feed-forward pass.Specifically, POCO estimates both the 3D body pose and a per-sample variance.The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressinguncertainty that is highly correlated to pose reconstruction quality. The POCOframework can be applied to any HPS regressor and here we evaluate it bymodifying HMR, PARE, and CLIFF. In all cases, training the network to reasonabout uncertainty helps it learn to more accurately estimate 3D pose. Whilethis was not our goal, the improvement is modest but consistent. Our mainmotivation is to provide uncertainty estimates for downstream tasks; wedemonstrate this in two ways: (1) We use the confidence estimates to bootstrapHPS training. Given unlabelled image data, we take the confident estimates of aPOCO-trained regressor as pseudo ground truth. Retraining with thisautomatically-curated data improves accuracy. (2) We exploit uncertainty invideo pose estimation by automatically identifying uncertain frames (e.g. dueto occlusion) and inpainting these from confident frames. Code and models willbe available for research at https://poco.is.tue.mpg.de.</description><author>Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, Dimitrios Tzionas</author><pubDate>Thu, 24 Aug 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12965v1</guid></item><item><title>Improving 3D Pose Estimation for Sign Language</title><link>http://arxiv.org/abs/2308.09525v1</link><description>This work addresses 3D human pose reconstruction in single images. We presenta method that combines Forward Kinematics (FK) with neural networks to ensure afast and valid prediction of 3D pose. Pose is represented as a hierarchicaltree/graph with nodes corresponding to human joints that model their physicallimits. Given a 2D detection of keypoints in the image, we lift the skeleton to3D using neural networks to predict both the joint rotations and bone lengths.These predictions are then combined with skeletal constraints using an FK layerimplemented as a network layer in PyTorch. The result is a fast and accurateapproach to the estimation of 3D skeletal pose. Through quantitative andqualitative evaluation, we demonstrate the method is significantly moreaccurate than MediaPipe in terms of both per joint positional error and visualappearance. Furthermore, we demonstrate generalization over different datasets.The implementation in PyTorch runs at between 100-200 milliseconds per image(including CNN detection) using CPU only.</description><author>Maksym Ivashechkin, Oscar Mendez, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 14:05:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09525v1</guid></item><item><title>Unsupervised Reconstruction of 3D Human Pose Interactions From 2D Poses Alone</title><link>http://arxiv.org/abs/2309.14865v1</link><description>Current unsupervised 2D-3D human pose estimation (HPE) methods do not work inmulti-person scenarios due to perspective ambiguity in monocular images.Therefore, we present one of the first studies investigating the feasibility ofunsupervised multi-person 2D-3D HPE from just 2D poses alone, focusing onreconstructing human interactions. To address the issue of perspectiveambiguity, we expand upon prior work by predicting the cameras' elevation anglerelative to the subjects' pelvis. This allows us to rotate the predicted posesto be level with the ground plane, while obtaining an estimate for the verticaloffset in 3D between individuals. Our method involves independently liftingeach subject's 2D pose to 3D, before combining them in a shared 3D coordinatesystem. The poses are then rotated and offset by the predicted elevation anglebefore being scaled. This by itself enables us to retrieve an accurate 3Dreconstruction of their poses. We present our results on the CHI3D dataset,introducing its use for unsupervised 2D-3D pose estimation with three newquantitative metrics, and establishing a benchmark for future research.</description><author>Peter Hardy, Hansung Kim</author><pubDate>Tue, 26 Sep 2023 12:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14865v1</guid></item><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Neural Voting Field for Camera-Space 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2305.04328v1</link><description>We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.</description><author>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</author><pubDate>Sun, 07 May 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04328v1</guid></item><item><title>Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</title><link>http://arxiv.org/abs/2308.10305v1</link><description>Despite significant progress in single image-based 3D human mesh recovery,accurately and smoothly recovering 3D human motion from a video remainschallenging. Existing video-based methods generally recover human mesh byestimating the complex pose and shape parameters from coupled image features,whose high complexity and low representation ability often result ininconsistent pose motion and limited shape patterns. To alleviate this issue,we introduce 3D pose as the intermediary and propose a Pose and MeshCo-Evolution network (PMCE) that decouples this task into two parts: 1)video-based 3D human pose estimation and 2) mesh vertices regression from theestimated 3D pose and temporal image feature. Specifically, we propose atwo-stream encoder that estimates mid-frame 3D pose and extracts a temporalimage feature from the input image sequence. In addition, we design aco-evolution decoder that performs pose and mesh interactions with theimage-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit thehuman body shape. Extensive experiments demonstrate that the proposed PMCEoutperforms previous state-of-the-art methods in terms of both per-frameaccuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M,and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.</description><author>Yingxuan You, Hong Liu, Ti Wang, Wenhao Li, Runwei Ding, Xia Li</author><pubDate>Sun, 20 Aug 2023 17:03:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10305v1</guid></item><item><title>Dual-Side Feature Fusion 3D Pose Transfer</title><link>http://arxiv.org/abs/2305.14951v1</link><description>3D pose transfer solves the problem of additional input and correspondence oftraditional deformation transfer, only the source and target meshes need to beinput, and the pose of the source mesh can be transferred to the target mesh.Some lightweight methods proposed in recent years consume less memory but causespikes and distortions for some unseen poses, while others are costly intraining due to the inclusion of large matrix multiplication and adversarialnetworks. In addition, the meshes with different numbers of vertices alsoincrease the difficulty of pose transfer. In this work, we propose a Dual-SideFeature Fusion Pose Transfer Network to improve the pose transfer accuracy ofthe lightweight method. Our method takes the pose features as one of the sideinputs to the decoding network and fuses them into the target mesh layer bylayer at multiple scales. Our proposed Feature Fusion Adaptive InstanceNormalization has the characteristic of having two side input channels thatfuse pose features and identity features as denormalization parameters, thusenhancing the pose transfer capability of the network. Extensive experimentalresults show that our proposed method has stronger pose transfer capabilitythan state-of-the-art methods while maintaining a lightweight networkstructure, and can converge faster.</description><author>Jue Liu, Feipeng Da</author><pubDate>Wed, 24 May 2023 10:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14951v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video</title><link>http://arxiv.org/abs/2307.05853v2</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 14% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively). GitHub:https://github.com/bruceyo/GLA-GCN.</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Sat, 22 Jul 2023 02:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v2</guid></item><item><title>Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2303.16456v2</link><description>When applying a pre-trained 2D-to-3D human pose lifting model to a targetunseen dataset, large performance degradation is commonly encountered due todomain shift issues. We observe that the degradation is caused by two factors:1) the large distribution gap over global positions of poses between the sourceand target datasets due to variant camera parameters and settings, and 2) thedeficient diversity of local structures of poses in training. To this end, wecombine \textbf{global adaptation} and \textbf{local generalization} in\textit{PoseDA}, a simple yet effective framework of unsupervised domainadaptation for 3D human pose estimation. Specifically, global adaptation aimsto align global positions of poses from the source domain to the target domainwith a proposed global position alignment (GPA) module. And localgeneralization is designed to enhance the diversity of 2D-3D pose mapping witha local pose augmentation (LPA) module. These modules bring significantperformance improvement without introducing additional learnable parameters. Inaddition, we propose local pose augmentation (LPA) to enhance the diversity of3D poses following an adversarial training scheme consisting of 1) aaugmentation generator that generates the parameters of pre-defined posetransformations and 2) an anchor discriminator to ensure the reality andquality of the augmented data. Our approach can be applicable to almost all2D-3D lifting models. \textit{PoseDA} achieves 61.3 mm of MPJPE on MPI-INF-3DHPunder a cross-dataset evaluation setup, improving upon the previousstate-of-the-art method by 10.2\%.</description><author>Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang, Gaoang Wang</author><pubDate>Thu, 17 Aug 2023 07:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16456v2</guid></item><item><title>Scene-aware Egocentric 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2212.11684v3</link><description>Egocentric 3D human pose estimation with a single head-mounted fisheye camerahas recently attracted attention due to its numerous applications in virtualand augmented reality. Existing methods still struggle in challenging poseswhere the human body is highly occluded or is closely interacting with thescene. To address this issue, we propose a scene-aware egocentric poseestimation method that guides the prediction of the egocentric pose with sceneconstraints. To this end, we propose an egocentric depth estimation network topredict the scene depth map from a wide-view egocentric fisheye camera whilemitigating the occlusion of the human body with a depth-inpainting network.Next, we propose a scene-aware pose estimation network that projects the 2Dimage features and estimated depth map of the scene into a voxel space andregresses the 3D pose with a V2V network. The voxel-based featurerepresentation provides the direct geometric connection between 2D imagefeatures and scene geometry, and further facilitates the V2V network toconstrain the predicted pose based on the estimated scene geometry. To enablethe training of the aforementioned networks, we also generated a syntheticdataset, called EgoGTA, and an in-the-wild dataset based on EgoPW, calledEgoPW-Scene. The experimental results of our new evaluation sequences show thatthe predicted 3D egocentric poses are accurate and physically plausible interms of human-scene interaction, demonstrating that our method outperforms thestate-of-the-art methods both quantitatively and qualitatively.</description><author>Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, Diogo Luvizon, Christian Theobalt</author><pubDate>Mon, 25 Sep 2023 21:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.11684v3</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v1</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Tue, 01 Aug 2023 16:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v1</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v2</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenxuan Guo, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Sun, 06 Aug 2023 15:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v2</guid></item><item><title>Weakly-supervised 3D Pose Transfer with Keypoints</title><link>http://arxiv.org/abs/2307.13459v1</link><description>The main challenges of 3D pose transfer are: 1) Lack of paired training datawith different characters performing the same pose; 2) Disentangling pose andshape information from the target mesh; 3) Difficulty in applying to mesheswith different topologies. We thus propose a novel weakly-supervisedkeypoint-based framework to overcome these difficulties. Specifically, we use atopology-agnostic keypoint detector with inverse kinematics to computetransformations between the source and target meshes. Our method only requiressupervision on the keypoints, can be applied to meshes with differenttopologies and is shape-invariant for the target which allows extraction ofpose-only information from the target meshes without transferring shapeinformation. We further design a cycle reconstruction to performself-supervised pose transfer without the need for ground truth deformed meshwith the same pose and shape as the target and source, respectively. Weevaluate our approach on benchmark human and animal datasets, where we achievesuperior performance compared to the state-of-the-art unsupervised approachesand even comparable performance with the fully supervised approaches. We teston the more challenging Mixamo dataset to verify our approach's ability inhandling meshes with different topologies and complex clothes. Cross-datasetevaluation further shows the strong generalization ability of our approach.</description><author>Jinnan Chen, Chen Li, Gim Hee Lee</author><pubDate>Tue, 25 Jul 2023 13:40:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13459v1</guid></item><item><title>Weakly-supervised 3D Pose Transfer with Keypoints</title><link>http://arxiv.org/abs/2307.13459v2</link><description>The main challenges of 3D pose transfer are: 1) Lack of paired training datawith different characters performing the same pose; 2) Disentangling pose andshape information from the target mesh; 3) Difficulty in applying to mesheswith different topologies. We thus propose a novel weakly-supervisedkeypoint-based framework to overcome these difficulties. Specifically, we use atopology-agnostic keypoint detector with inverse kinematics to computetransformations between the source and target meshes. Our method only requiressupervision on the keypoints, can be applied to meshes with differenttopologies and is shape-invariant for the target which allows extraction ofpose-only information from the target meshes without transferring shapeinformation. We further design a cycle reconstruction to performself-supervised pose transfer without the need for ground truth deformed meshwith the same pose and shape as the target and source, respectively. Weevaluate our approach on benchmark human and animal datasets, where we achievesuperior performance compared to the state-of-the-art unsupervised approachesand even comparable performance with the fully supervised approaches. We teston the more challenging Mixamo dataset to verify our approach's ability inhandling meshes with different topologies and complex clothes. Cross-datasetevaluation further shows the strong generalization ability of our approach.</description><author>Jinnan Chen, Chen Li, Gim Hee Lee</author><pubDate>Thu, 17 Aug 2023 07:02:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13459v2</guid></item><item><title>3D Pose Nowcasting: Forecast the Future to Improve the Present</title><link>http://arxiv.org/abs/2308.12914v2</link><description>Technologies to enable safe and effective collaboration and coexistencebetween humans and robots have gained significant importance in the last fewyears. A critical component useful for realizing this collaborative paradigm isthe understanding of human and robot 3D poses using non-invasive systems.Therefore, in this paper, we propose a novel vision-based system leveragingdepth data to accurately establish the 3D locations of skeleton joints.Specifically, we introduce the concept of Pose Nowcasting, denoting thecapability of the proposed system to enhance its current pose estimationaccuracy by jointly learning to forecast future poses. The experimentalevaluation is conducted on two different datasets, providing accurate andreal-time performance and confirming the validity of the proposed method onboth the robotic and human scenarios.</description><author>Alessandro Simoni, Francesco Marchetti, Guido Borghi, Federico Becattini, Lorenzo Seidenari, Roberto Vezzani, Alberto Del Bimbo</author><pubDate>Sat, 18 Nov 2023 15:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12914v2</guid></item><item><title>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</title><link>http://arxiv.org/abs/2206.02257v3</link><description>In this survey, we present a systematic review of 3D hand pose estimationfrom the perspective of efficient annotation and learning. 3D hand poseestimation has been an important research area owing to its potential to enablevarious applications, such as video understanding, AR/VR, and robotics.However, the performance of models is tied to the quality and quantity ofannotated 3D hand poses. Under the status quo, acquiring such annotated 3D handposes is challenging, e.g., due to the difficulty of 3D annotation and thepresence of occlusion. To reveal this problem, we review the pros and cons ofexisting annotation methods classified as manual, synthetic-model-based,hand-sensor-based, and computational approaches. Additionally, we examinemethods for learning 3D hand poses when annotated data are scarce, includingself-supervised pretraining, semi-supervised learning, and domain adaptation.Based on the study of efficient annotation and learning, we further discusslimitations and possible future directions in this field.</description><author>Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato</author><pubDate>Wed, 26 Apr 2023 07:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02257v3</guid></item><item><title>Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields</title><link>http://arxiv.org/abs/2212.02493v3</link><description>Coordinate-based implicit neural networks, or neural fields, have emerged asuseful representations of shape and appearance in 3D computer vision. Despiteadvances, however, it remains challenging to build neural fields for categoriesof objects without datasets like ShapeNet that provide "canonicalized" objectinstances that are consistently aligned for their 3D position and orientation(pose). We present Canonical Field Network (CaFi-Net), a self-supervised methodto canonicalize the 3D pose of instances from an object category represented asneural fields, specifically neural radiance fields (NeRFs). CaFi-Net directlylearns from continuous and noisy radiance fields using a Siamese networkarchitecture that is designed to extract equivariant field features forcategory-level canonicalization. During inference, our method takes pre-trainedneural radiance fields of novel object instances at arbitrary 3D pose andestimates a canonical field with consistent 3D pose across the entire category.Extensive experiments on a new dataset of 1300 NeRF models across 13 objectcategories show that our method matches or exceeds the performance of 3D pointcloud-based methods.</description><author>Rohith Agaram, Shaurya Dewan, Rahul Sajnani, Adrien Poulenard, Madhava Krishna, Srinath Sridhar</author><pubDate>Wed, 17 May 2023 12:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02493v3</guid></item><item><title>Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet</title><link>http://arxiv.org/abs/2207.04320v3</link><description>Multi-person pose understanding from RGB videos involves three complex tasks:pose estimation, tracking and motion forecasting. Intuitively, accuratemulti-person pose estimation facilitates robust tracking, and robust trackingbuilds crucial history for correct motion forecasting. Most existing workseither focus on a single task or employ multi-stage approaches to solvingmultiple tasks separately, which tends to make sub-optimal decision at eachstage and also fail to exploit correlations among the three tasks. In thispaper, we propose Snipper, a unified framework to perform multi-person 3D poseestimation, tracking, and motion forecasting simultaneously in a single stage.We propose an efficient yet powerful deformable attention mechanism toaggregate spatiotemporal information from the video snippet. Building upon thisdeformable attention, a video transformer is learned to encode thespatiotemporal features from the multi-frame snippet and to decode informativepose features for multi-person pose queries. Finally, these pose queries areregressed to predict multi-person pose trajectories and future motions in asingle shot. In the experiments, we show the effectiveness of Snipper on threechallenging public datasets where our generic model rivals specializedstate-of-art baselines for pose estimation, tracking, and forecasting.</description><author>Shihao Zou, Yuanlu Xu, Chao Li, Lingni Ma, Li Cheng, Minh Vo</author><pubDate>Tue, 12 Sep 2023 22:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.04320v3</guid></item><item><title>Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving</title><link>http://arxiv.org/abs/2307.14889v1</link><description>Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomousvehicles (AVs) to make informed decisions and respond proactively in criticalroad scenarios. Promising results of 3D HPE have been gained in several domainssuch as human-computer interaction, robotics, sports and medical analytics,often based on data collected in well-controlled laboratory environments.Nevertheless, the transfer of 3D HPE methods to AVs has received limitedresearch attention, due to the challenges posed by obtaining accurate 3D poseannotations and the limited suitability of data from other domains. We present a simple yet efficient weakly supervised approach for 3D HPE inthe AV context by employing a high-level sensor fusion between camera and LiDARdata. The weakly supervised setting enables training on the target datasetswithout any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractorand pseudo labels generated from LiDAR to image projections. Our approachoutperforms state-of-the-art results by up to $\sim$ 13% on the Waymo OpenDataset in the weakly supervised setting and achieves state-of-the-art resultsin the supervised setting.</description><author>Peter Bauer, Arij Bouazizi, Ulrich Kressel, Fabian B. Flohr</author><pubDate>Thu, 27 Jul 2023 15:28:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14889v1</guid></item><item><title>Human Part-wise 3D Motion Context Learning for Sign Language Recognition</title><link>http://arxiv.org/abs/2308.09305v1</link><description>In this paper, we propose P3D, the human part-wise motion context learningframework for sign language recognition. Our main contributions lie in twodimensions: learning the part-wise motion context and employing the poseensemble to utilize 2D and 3D pose jointly. First, our empirical observationimplies that part-wise context encoding benefits the performance of signlanguage recognition. While previous methods of sign language recognitionlearned motion context from the sequence of the entire pose, we argue that suchmethods cannot exploit part-specific motion context. In order to utilizepart-wise motion context, we propose the alternating combination of a part-wiseencoding Transformer (PET) and a whole-body encoding Transformer (WET). PETencodes the motion contexts from a part sequence, while WET merges them into aunified context. By learning part-wise motion context, our P3D achievessuperior performance on WLASL compared to previous state-of-the-art methods.Second, our framework is the first to ensemble 2D and 3D poses for signlanguage recognition. Since the 3D pose holds rich motion context and depthinformation to distinguish the words, our P3D outperformed the previousstate-of-the-art methods employing a pose ensemble.</description><author>Taeryung Lee, Yeonguk Oh, Kyoung Mu Lee</author><pubDate>Fri, 18 Aug 2023 06:01:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09305v1</guid></item><item><title>PoseGraphNet++: Enriching 3D Human Pose with Orientation Estimation</title><link>http://arxiv.org/abs/2308.11440v1</link><description>Existing kinematic skeleton-based 3D human pose estimation methods onlypredict joint positions. Although this is sufficient to compute the yaw andpitch of the bone rotations, the roll around the axis of the bones remainsunresolved by these methods. In this paper, we propose a novel 2D-to-3D liftingGraph Convolution Network named PoseGraphNet++ to predict the complete humanpose including the joint positions and the bone orientations. We employ nodeand edge convolutions to utilize the joint and bone features. Our model isevaluated on multiple benchmark datasets, and its performance is either on parwith or better than the state-of-the-art in terms of both position and rotationmetrics. Through extensive ablation studies, we show that PoseGraphNet++benefits from exploiting the mutual relationship between the joints and thebones.</description><author>Soubarna Banik, Edvard Avagyan, Alejandro Mendoza Gracia, Alois Knoll</author><pubDate>Tue, 22 Aug 2023 14:42:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11440v1</guid></item><item><title>RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with Occlusion Handling</title><link>http://arxiv.org/abs/2311.14242v1</link><description>In the domain of 3D Human Pose Estimation, which finds widespread dailyapplications, the requirement for convenient acquisition equipment continues togrow. To satisfy this demand, we set our sights on a short-baseline binocularsetting that offers both portability and a geometric measurement property thatradically mitigates depth ambiguity. However, as the binocular baselineshortens, two serious challenges emerge: first, the robustness of 3Dreconstruction against 2D errors deteriorates; and second, occlusion reoccursdue to the limited visual differences between two views. To address the firstchallenge, we propose the Stereo Co-Keypoints Estimation module to improve theview consistency of 2D keypoints and enhance the 3D robustness. In this module,the disparity is utilized to represent the correspondence of binocular 2Dpoints and the Stereo Volume Feature is introduced to contain binocularfeatures across different disparities. Through the regression of SVF, two-view2D keypoints are simultaneously estimated in a collaborative way whichrestricts their view consistency. Furthermore, to deal with occlusions, aPre-trained Pose Transformer module is introduced. Through this module, 3Dposes are refined by perceiving pose coherence, a representation of jointcorrelations. This perception is injected by the Pose Transformer network andlearned through a pre-training task that recovers iterative masked joints.Comprehensive experiments carried out on H36M and MHAD datasets, complementedby visualizations, validate the effectiveness of our approach in theshort-baseline binocular 3D Human Pose Estimation and occlusion handling.</description><author>Xiaoyue Wan, Zhuo Chen, Yiming Bao, Xu Zhao</author><pubDate>Fri, 24 Nov 2023 01:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14242v1</guid></item><item><title>FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations</title><link>http://arxiv.org/abs/2211.14309v2</link><description>We present a generative approach to forecast long-term future human behaviorin 3D, requiring only weak supervision from readily available 2D human actiondata. This is a fundamental task enabling many downstream applications. Therequired ground-truth data is hard to capture in 3D (mocap suits, expensivesetups) but easy to acquire in 2D (simple RGB cameras). Thus, we design ourmethod to only require 2D RGB data while being able to generate 3D human motionsequences. We use a differentiable 2D projection scheme in an autoregressivemanner for weak supervision, and an adversarial loss for 3D regularization. Ourmethod predicts long and complex behavior sequences (e.g. cooking, assembly)consisting of multiple sub-actions. We tackle this in a semanticallyhierarchical manner, jointly predicting high-level coarse action labelstogether with their low-level fine-grained realizations as characteristic 3Dhuman poses. We observe that these two action representations are coupled innature, and joint prediction benefits both action and pose forecasting. Ourexperiments demonstrate the complementary nature of joint action and 3D poseprediction: our joint approach outperforms each task treated individually,enables robust longer-term sequence prediction, and outperforms alternativeapproaches to forecast actions and characteristic 3D poses.</description><author>Christian Diller, Thomas Funkhouser, Angela Dai</author><pubDate>Mon, 27 Nov 2023 18:48:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14309v2</guid></item><item><title>A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2311.03312v1</link><description>The dominant paradigm in 3D human pose estimation that lifts a 2D posesequence to 3D heavily relies on long-term temporal clues (i.e., using adaunting number of video frames) for improved accuracy, which incursperformance saturation, intractable computation and the non-causal problem.This can be attributed to their inherent inability to perceive spatial contextas plain 2D joint coordinates carry no visual cues. To address this issue, wepropose a straightforward yet powerful solution: leveraging the readilyavailable intermediate visual representations produced by off-the-shelf(pre-trained) 2D pose detectors -- no finetuning on the 3D task is even needed.The key observation is that, while the pose detector learns to localize 2Djoints, such representations (e.g., feature maps) implicitly encode thejoint-centric spatial context thanks to the regional operations in backbonenetworks. We design a simple baseline named Context-Aware PoseFormer toshowcase its effectiveness. Without access to any temporal information, theproposed method significantly outperforms its context-agnostic counterpart,PoseFormer, and other state-of-the-art methods using up to hundreds of videoframes regarding both speed and precision. Project page:https://qitaozhao.github.io/ContextAware-PoseFormer</description><author>Qitao Zhao, Ce Zheng, Mengyuan Liu, Chen Chen</author><pubDate>Mon, 06 Nov 2023 18:04:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03312v1</guid></item><item><title>A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2311.03312v2</link><description>The dominant paradigm in 3D human pose estimation that lifts a 2D posesequence to 3D heavily relies on long-term temporal clues (i.e., using adaunting number of video frames) for improved accuracy, which incursperformance saturation, intractable computation and the non-causal problem.This can be attributed to their inherent inability to perceive spatial contextas plain 2D joint coordinates carry no visual cues. To address this issue, wepropose a straightforward yet powerful solution: leveraging the readilyavailable intermediate visual representations produced by off-the-shelf(pre-trained) 2D pose detectors -- no finetuning on the 3D task is even needed.The key observation is that, while the pose detector learns to localize 2Djoints, such representations (e.g., feature maps) implicitly encode thejoint-centric spatial context thanks to the regional operations in backbonenetworks. We design a simple baseline named Context-Aware PoseFormer toshowcase its effectiveness. Without access to any temporal information, theproposed method significantly outperforms its context-agnostic counterpart,PoseFormer, and other state-of-the-art methods using up to hundreds of videoframes regarding both speed and precision. Project page:https://qitaozhao.github.io/ContextAware-PoseFormer</description><author>Qitao Zhao, Ce Zheng, Mengyuan Liu, Chen Chen</author><pubDate>Thu, 09 Nov 2023 04:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03312v2</guid></item><item><title>EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With Kinematic Structure Priors</title><link>http://arxiv.org/abs/2306.09615v1</link><description>Transformer is popular in recent 3D human pose estimation, which utilizeslong-term modeling to lift 2D keypoints into the 3D space. However, currenttransformer-based methods do not fully exploit the prior knowledge of the humanskeleton provided by the kinematic structure. In this paper, we propose a noveltransformer-based model EvoPose to introduce the human body prior knowledge for3D human pose estimation effectively. Specifically, a Structural PriorsRepresentation (SPR) module represents human priors as structural featurescarrying rich body patterns, e.g. joint relationships. The structural featuresare interacted with 2D pose sequences and help the model to achieve moreinformative spatiotemporal features. Moreover, a Recursive Refinement (RR)module is applied to refine the 3D pose outputs by utilizing estimated resultsand further injects human priors simultaneously. Extensive experimentsdemonstrate the effectiveness of EvoPose which achieves a new state of the arton two most popular benchmarks, Human3.6M and MPI-INF-3DHP.</description><author>Yaqi Zhang, Yan Lu, Bin Liu, Zhiwei Zhao, Qi Chu, Nenghai Yu</author><pubDate>Fri, 16 Jun 2023 05:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09615v1</guid></item><item><title>Iterative Graph Filtering Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16074v1</link><description>Graph convolutional networks (GCNs) have proven to be an effective approachfor 3D human pose estimation. By naturally modeling the skeleton structure ofthe human body as a graph, GCNs are able to capture the spatial relationshipsbetween joints and learn an efficient representation of the underlying pose.However, most GCN-based methods use a shared weight matrix, making itchallenging to accurately capture the different and complex relationshipsbetween joints. In this paper, we introduce an iterative graph filteringframework for 3D human pose estimation, which aims to predict the 3D jointpositions given a set of 2D joint locations in images. Our approach builds uponthe idea of iteratively solving graph filtering with Laplacian regularizationvia the Gauss-Seidel iterative method. Motivated by this iterative solution, wedesign a Gauss-Seidel network (GS-Net) architecture, which makes use of weightand adjacency modulation, skip connection, and a pure convolutional block withlayer normalization. Adjacency modulation facilitates the learning of edgesthat go beyond the inherent connections of body joints, resulting in anadjusted graph structure that reflects the human skeleton, while skipconnections help maintain crucial information from the input layer's initialfeatures as the network depth increases. We evaluate our proposed model on twostandard benchmark datasets, and compare it with a comprehensive set of strongbaseline methods for 3D human pose estimation. Our experimental resultsdemonstrate that our approach outperforms the baseline methods on bothdatasets, achieving state-of-the-art performance. Furthermore, we conductablation studies to analyze the contributions of different components of ourmodel architecture and show that the skip connection and adjacency modulationhelp improve the model performance.</description><author>Zaedul Islam, A. Ben Hamza</author><pubDate>Sat, 29 Jul 2023 21:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16074v1</guid></item><item><title>Iterative Graph Filtering Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16074v2</link><description>Graph convolutional networks (GCNs) have proven to be an effective approachfor 3D human pose estimation. By naturally modeling the skeleton structure ofthe human body as a graph, GCNs are able to capture the spatial relationshipsbetween joints and learn an efficient representation of the underlying pose.However, most GCN-based methods use a shared weight matrix, making itchallenging to accurately capture the different and complex relationshipsbetween joints. In this paper, we introduce an iterative graph filteringframework for 3D human pose estimation, which aims to predict the 3D jointpositions given a set of 2D joint locations in images. Our approach builds uponthe idea of iteratively solving graph filtering with Laplacian regularizationvia the Gauss-Seidel iterative method. Motivated by this iterative solution, wedesign a Gauss-Seidel network (GS-Net) architecture, which makes use of weightand adjacency modulation, skip connection, and a pure convolutional block withlayer normalization. Adjacency modulation facilitates the learning of edgesthat go beyond the inherent connections of body joints, resulting in anadjusted graph structure that reflects the human skeleton, while skipconnections help maintain crucial information from the input layer's initialfeatures as the network depth increases. We evaluate our proposed model on twostandard benchmark datasets, and compare it with a comprehensive set of strongbaseline methods for 3D human pose estimation. Our experimental resultsdemonstrate that our approach outperforms the baseline methods on bothdatasets, achieving state-of-the-art performance. Furthermore, we conductablation studies to analyze the contributions of different components of ourmodel architecture and show that the skip connection and adjacency modulationhelp improve the model performance.</description><author>Zaedul Islam, A. Ben Hamza</author><pubDate>Mon, 07 Aug 2023 23:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16074v2</guid></item><item><title>CLIP-Hand3D: Exploiting 3D Hand Pose Estimation via Context-Aware Prompting</title><link>http://arxiv.org/abs/2309.16140v1</link><description>Contrastive Language-Image Pre-training (CLIP) starts to emerge in manycomputer vision tasks and has achieved promising performance. However, itremains underexplored whether CLIP can be generalized to 3D hand poseestimation, as bridging text prompts with pose-aware features presentssignificant challenges due to the discrete nature of joint positions in 3Dspace. In this paper, we make one of the first attempts to propose a novel 3Dhand pose estimator from monocular images, dubbed as CLIP-Hand3D, whichsuccessfully bridges the gap between text prompts and irregular detailed posedistribution. In particular, the distribution order of hand joints in various3D space directions is derived from pose labels, forming corresponding textprompts that are subsequently encoded into text representations.Simultaneously, 21 hand joints in the 3D space are retrieved, and their spatialdistribution (in x, y, and z axes) is encoded to form pose-aware features.Subsequently, we maximize semantic consistency for a pair of pose-text featuresfollowing a CLIP-based contrastive learning paradigm. Furthermore, acoarse-to-fine mesh regressor is designed, which is capable of effectivelyquerying joint-aware cues from the feature pyramid. Extensive experiments onseveral public hand benchmarks show that the proposed model attains asignificantly faster inference speed while achieving state-of-the-artperformance compared to methods utilizing the similar scale backbone.</description><author>Shaoxiang Guo, Qing Cai, Lin Qi, Junyu Dong</author><pubDate>Thu, 28 Sep 2023 04:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16140v1</guid></item><item><title>FreeMan: Towards Benchmarking 3D Human Pose Estimation in the Wild</title><link>http://arxiv.org/abs/2309.05073v2</link><description>Estimating the 3D structure of the human body from natural scenes is afundamental aspect of visual perception. This task carries great importance forfields like AIGC and human-robot interaction. In practice, 3D human poseestimation in real-world settings is a critical initial step in solving thisproblem. However, the current datasets, often collected under controlledlaboratory conditions using complex motion capture equipment and unvaryingbackgrounds, are insufficient. The absence of real-world datasets is stallingthe progress of this crucial task. To facilitate the development of 3D poseestimation, we present FreeMan, the first large-scale, real-world multi-viewdataset. FreeMan was captured by synchronizing 8 smartphones across diversescenarios. It comprises 11M frames from 8000 sequences, viewed from differentperspectives. These sequences cover 40 subjects across 10 different scenarios,each with varying lighting conditions. We have also established an automated,precise labeling pipeline that allows for large-scale processing efficiently.We provide comprehensive evaluation baselines for a range of tasks, underliningthe significant challenges posed by FreeMan. Further evaluations of standardindoor/outdoor human sensing datasets reveal that FreeMan offers robustrepresentation transferability in real and complex scenes. FreeMan is nowpublicly available at https://wangjiongw.github.io/freeman.</description><author>Jiong Wang, Fengyu Yang, Wenbo Gou, Bingliang Li, Danqi Yan, Ailing Zeng, Yijun Gao, Junle Wang, Ruimao Zhang</author><pubDate>Tue, 12 Sep 2023 16:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05073v2</guid></item><item><title>3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking</title><link>http://arxiv.org/abs/2308.15316v2</link><description>Markerless methods for animal posture tracking have been developing recently,but frameworks and benchmarks for tracking large animal groups in 3D are stilllacking. To overcome this gap in the literature, we present 3D-MuPPET, aframework to estimate and track 3D poses of up to 10 pigeons at interactivespeed using multiple-views. We train a pose estimator to infer 2D keypoints andbounding boxes of multiple pigeons, then triangulate the keypoints to 3D. Forcorrespondence matching, we first dynamically match 2D detections to globalidentities in the first frame, then use a 2D tracker to maintaincorrespondences accross views in subsequent frames. We achieve comparableaccuracy to a state of the art 3D pose estimator for Root Mean Square Error(RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel usecase where our model trained with data of single pigeons provides comparableresults on data containing multiple pigeons. This can simplify the domain shiftto new species because annotating single animal data is less labour intensivethan multi-animal data. Additionally, we benchmark the inference speed of3D-MuPPET, with up to 10 fps in 2D and 1.5 fps in 3D, and perform quantitativetracking evaluation, which yields encouraging results. Finally, we show that3D-MuPPET also works in natural environments without model fine-tuning onadditional annotations. To the best of our knowledge we are the first topresent a framework for 2D/3D posture and trajectory tracking that works inboth indoor and outdoor environments.</description><author>Urs Waldmann, Alex Hoi Hang Chan, Hemal Naik, Máté Nagy, Iain D. Couzin, Oliver Deussen, Bastian Goldluecke, Fumihiro Kano</author><pubDate>Fri, 22 Sep 2023 10:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15316v2</guid></item><item><title>3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking</title><link>http://arxiv.org/abs/2308.15316v1</link><description>Markerless methods for animal posture tracking have been developing recently,but frameworks and benchmarks for tracking large animal groups in 3D are stilllacking. To overcome this gap in the literature, we present 3D-MuPPET, aframework to estimate and track 3D poses of up to 10 pigeons at interactivespeed using multiple-views. We train a pose estimator to infer 2D keypoints andbounding boxes of multiple pigeons, then triangulate the keypoints to 3D. Forcorrespondence matching, we first dynamically match 2D detections to globalidentities in the first frame, then use a 2D tracker to maintaincorrespondences accross views in subsequent frames. We achieve comparableaccuracy to a state of the art 3D pose estimator for Root Mean Square Error(RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel usecase where our model trained with data of single pigeons provides comparableresults on data containing multiple pigeons. This can simplify the domain shiftto new species because annotating single animal data is less labour intensivethan multi-animal data. Additionally, we benchmark the inference speed of3D-MuPPET, with up to 10 fps in 2D and 1.5 fps in 3D, and perform quantitativetracking evaluation, which yields encouraging results. Finally, we show that3D-MuPPET also works in natural environments without model fine-tuning onadditional annotations. To the best of our knowledge we are the first topresent a framework for 2D/3D posture and trajectory tracking that works inboth indoor and outdoor environments.</description><author>Urs Waldmann, Alex Hoi Hang Chan, Hemal Naik, Máté Nagy, Iain D. Couzin, Oliver Deussen, Bastian Goldluecke, Fumihiro Kano</author><pubDate>Tue, 29 Aug 2023 15:02:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15316v1</guid></item><item><title>Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2311.12028v1</link><description>Transformers have been successfully applied in the field of video-based 3Dhuman pose estimation. However, the high computational costs of these videopose transformers (VPTs) make them impractical on resource-constrained devices.In this paper, we present a plug-and-play pruning-and-recovering framework,called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human poseestimation from videos. Our HoT begins with pruning pose tokens of redundantframes and ends with recovering full-length tokens, resulting in a few posetokens in the intermediate transformer blocks and thus improving the modelefficiency. To effectively achieve this, we propose a token pruning cluster(TPC) that dynamically selects a few representative tokens with high semanticdiversity while eliminating the redundancy of video frames. In addition, wedevelop a token recovering attention (TRA) to restore the detailedspatio-temporal information based on the selected tokens, thereby expanding thenetwork output to the original full-length temporal resolution for fastinference. Extensive experiments on two benchmark datasets (i.e., Human3.6M andMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency andestimation accuracy compared to the original VPT models. For instance, applyingto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPswithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,respectively. Our source code will be open-sourced.</description><author>Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe</author><pubDate>Mon, 20 Nov 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12028v1</guid></item><item><title>HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.14581v2</link><description>2D-to-3D human pose lifting is fundamental for 3D human pose estimation(HPE). Graph Convolutional Network (GCN) has been proven inherently suitable tomodel the human skeletal topology. However, current GCN-based 3D HPE methodsupdate the node features by aggregating their neighbors' information withoutconsidering the interaction of joints in different motion patterns. Althoughsome studies import limb information to learn the movement patterns, the latentsynergies among joints, such as maintaining balance in the motion are seldominvestigated. We propose a hop-wise GraphFormer with intragroup jointrefinement (HopFIR) to tackle the 3D HPE problem. The HopFIR mainly consists ofa novel Hop-wise GraphFormer(HGF) module and an Intragroup JointRefinement(IJR) module which leverages the prior limb information forperipheral joints refinement. The HGF module groups the joints by $k$-hopneighbors and utilizes a hop-wise transformer-like attention mechanism amongthese groups to discover latent joint synergy. Extensive experimental resultsshow that HopFIR outperforms the SOTA methods with a large margin (on theHuman3.6M dataset, the mean per joint position error (MPJPE) is 32.67mm).Furthermore, it is also demonstrated that previous SOTA GCN-based methods canbenefit from the proposed hop-wise attention mechanism efficiently withsignificant performance promotion, such as SemGCN and MGCN are improved by 8.9%and 4.5%, respectively.</description><author>Kai Zhai, Qiang Nie, Bo Ouyang, Xiang Li, ShanLin Yang</author><pubDate>Tue, 18 Jul 2023 17:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14581v2</guid></item><item><title>Denoising Diffusion for 3D Hand Pose Estimation from Images</title><link>http://arxiv.org/abs/2308.09523v1</link><description>Hand pose estimation from a single image has many applications. However,approaches to full 3D body pose estimation are typically trained on day-to-dayactivities or actions. As such, detailed hand-to-hand interactions are poorlyrepresented, especially during motion. We see this in the failure cases oftechniques such as OpenPose or MediaPipe. However, accurate hand poseestimation is crucial for many applications where the global body motion isless important than accurate hand pose estimation. This paper addresses the problem of 3D hand pose estimation from monocularimages or sequences. We present a novel end-to-end framework for 3D handregression that employs diffusion models that have shown excellent ability tocapture the distribution of data for generative purposes. Moreover, we enforcekinematic constraints to ensure realistic poses are generated by incorporatingan explicit forward kinematic layer as part of the network. The proposed modelprovides state-of-the-art performance when lifting a 2D single-hand image to3D. However, when sequence data is available, we add a Transformer module overa temporal window of consecutive frames to refine the results, overcomingjittering and further increasing accuracy. The method is quantitatively and qualitatively evaluated showingstate-of-the-art robustness, generalization, and accuracy on several differentdatasets.</description><author>Maksym Ivashechkin, Oscar Mendez, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 13:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09523v1</guid></item><item><title>Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.01365v1</link><description>Accurately estimating the 3D pose of humans in video sequences requires bothaccuracy and a well-structured architecture. With the success of transformers,we introduce the Refined Temporal Pyramidal Compression-and-Amplification(RTPCA) transformer. Exploiting the temporal dimension, RTPCA extendsintra-block temporal modeling via its Temporal PyramidalCompression-and-Amplification (TPCA) structure and refines inter-block featureinteraction with a Cross-Layer Refinement (XLR) module. In particular, TPCAblock exploits a temporal pyramid paradigm, reinforcing key and valuerepresentation capabilities and seamlessly extracting spatial semantics frommotion sequences. We stitch these TPCA blocks with XLR that promotes richsemantic representation through continuous interaction of queries, keys, andvalues. This strategy embodies early-stage information with current flows,addressing typical deficits in detail and stability seen in othertransformer-based methods. We demonstrate the effectiveness of RTPCA byachieving state-of-the-art results on Human3.6M, HumanEva-I, and MPI-INF-3DHPbenchmarks with minimal computational overhead. The source code is available athttps://github.com/hbing-l/RTPCA.</description><author>Hanbing Li, Wangmeng Xiang, Jun-Yan He, Zhi-Qi Cheng, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 04 Sep 2023 06:25:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01365v1</guid></item><item><title>Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.01365v2</link><description>Accurately estimating the 3D pose of humans in video sequences requires bothaccuracy and a well-structured architecture. With the success of transformers,we introduce the Refined Temporal Pyramidal Compression-and-Amplification(RTPCA) transformer. Exploiting the temporal dimension, RTPCA extendsintra-block temporal modeling via its Temporal PyramidalCompression-and-Amplification (TPCA) structure and refines inter-block featureinteraction with a Cross-Layer Refinement (XLR) module. In particular, TPCAblock exploits a temporal pyramid paradigm, reinforcing key and valuerepresentation capabilities and seamlessly extracting spatial semantics frommotion sequences. We stitch these TPCA blocks with XLR that promotes richsemantic representation through continuous interaction of queries, keys, andvalues. This strategy embodies early-stage information with current flows,addressing typical deficits in detail and stability seen in othertransformer-based methods. We demonstrate the effectiveness of RTPCA byachieving state-of-the-art results on Human3.6M, HumanEva-I, and MPI-INF-3DHPbenchmarks with minimal computational overhead. The source code is available athttps://github.com/hbing-l/RTPCA.</description><author>Hanbing Liu, Wangmeng Xiang, Jun-Yan He, Zhi-Qi Cheng, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Wed, 06 Sep 2023 03:18:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01365v2</guid></item><item><title>Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.03833v2</link><description>Learning-based methods have dominated the 3D human pose estimation (HPE)tasks with significantly better performance in most benchmarks than traditionaloptimization-based methods. Nonetheless, 3D HPE in the wild is still thebiggest challenge of learning-based models, whether with 2D-3D lifting,image-to-3D, or diffusion-based methods, since the trained networks implicitlylearn camera intrinsic parameters and domain-based 3D human pose distributionsand estimate poses by statistical average. On the other hand, theoptimization-based methods estimate results case-by-case, which can predictmore diverse and sophisticated human poses in the wild. By combining theadvantages of optimization-based and learning-based methods, we propose theZero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve theproblem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDOachieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mmwithout training with any 2D-3D or image-3D pairs. Moreover, oursingle-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE$42.6$mm on cross-dataset evaluation, which even outperforms learning-basedmethods trained on 3DPW.</description><author>Zhongyu Jiang, Zhuoran Zhou, Lei Li, Wenhao Chai, Cheng-Yen Yang, Jenq-Neng Hwang</author><pubDate>Wed, 23 Aug 2023 18:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03833v2</guid></item><item><title>1st Place Solution of Egocentric 3D Hand Pose Estimation Challenge 2023 Technical Report:A Concise Pipeline for Egocentric Hand Pose Reconstruction</title><link>http://arxiv.org/abs/2310.04769v1</link><description>This report introduce our work on Egocentric 3D Hand Pose Estimationworkshop. Using AssemblyHands, this challenge focuses on egocentric 3D handpose estimation from a single-view image. In the competition, we adopt ViTbased backbones and a simple regressor for 3D keypoints prediction, whichprovides strong model baselines. We noticed that Hand-objects occlusions andself-occlusions lead to performance degradation, thus proposed a non-modelmethod to merge multi-view results in the post-process stage. Moreover, Weutilized test time augmentation and model ensemble to make further improvement.We also found that public dataset and rational preprocess are beneficial. Ourmethod achieved 12.21mm MPJPE on test dataset, achieve the first place inEgocentric 3D Hand Pose Estimation challenge.</description><author>Zhishan Zhou, Zhi Lv, Shihao Zhou, Minqiang Zou, Tong Wu, Mochen Yu, Yao Tang, Jiajun Liang</author><pubDate>Sat, 07 Oct 2023 11:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04769v1</guid></item><item><title>GloPro: Globally-Consistent Uncertainty-Aware 3D Human Pose Estimation &amp; Tracking in the Wild</title><link>http://arxiv.org/abs/2309.10369v2</link><description>An accurate and uncertainty-aware 3D human body pose estimation is key toenabling truly safe but efficient human-robot interactions. Currentuncertainty-aware methods in 3D human pose estimation are limited to predictingthe uncertainty of the body posture, while effectively neglecting the bodyshape and root pose. In this work, we present GloPro, which to the best of ourknowledge the first framework to predict an uncertainty distribution of a 3Dbody mesh including its shape, pose, and root pose, by efficiently fusingvisual clues with a learned motion model. We demonstrate that it vastlyoutperforms state-of-the-art methods in terms of human trajectory accuracy in aworld coordinate system (even in the presence of severe occlusions), yieldsconsistent uncertainty distributions, and can run in real-time.</description><author>Simon Schaefer, Dorian F. Henning, Stefan Leutenegger</author><pubDate>Wed, 20 Sep 2023 17:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10369v2</guid></item><item><title>GloPro: Globally-Consistent Uncertainty-Aware 3D Human Pose Estimation &amp; Tracking in the Wild</title><link>http://arxiv.org/abs/2309.10369v1</link><description>An accurate and uncertainty-aware 3D human body pose estimation is key toenabling truly safe but efficient human-robot interactions. Currentuncertainty-aware methods in 3D human pose estimation are limited to predictingthe uncertainty of the body posture, while effectively neglecting the bodyshape and root pose. In this work, we present GloPro, which to the best of ourknowledge the first framework to predict an uncertainty distribution of a 3Dbody mesh including its shape, pose, and root pose, by efficiently fusingvisual clues with a learned motion model. We demonstrate that it vastlyoutperforms state-of-the-art methods in terms of human trajectory accuracy in aworld coordinate system (even in the presence of severe occlusions), yieldsconsistent uncertainty distributions, and can run in real-time. Our code willbe released upon acceptance at https://github.com/smartroboticslab/GloPro.</description><author>Simon Schaefer, Dorian F. Henning, Stefan Leutenegger</author><pubDate>Tue, 19 Sep 2023 08:10:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10369v1</guid></item><item><title>Double-chain Constraints for 3D Human Pose Estimation in Images and Videos</title><link>http://arxiv.org/abs/2308.05298v1</link><description>Reconstructing 3D poses from 2D poses lacking depth information isparticularly challenging due to the complexity and diversity of human motion.The key is to effectively model the spatial constraints between joints toleverage their inherent dependencies. Thus, we propose a novel model, calledDouble-chain Graph Convolutional Transformer (DC-GCT), to constrain the posethrough a double-chain design consisting of local-to-global and global-to-localchains to obtain a complex representation more suitable for the current humanpose. Specifically, we combine the advantages of GCN and Transformer and designa Local Constraint Module (LCM) based on GCN and a Global Constraint Module(GCM) based on self-attention mechanism as well as a Feature Interaction Module(FIM). The proposed method fully captures the multi-level dependencies betweenhuman body joints to optimize the modeling capability of the model. Moreover,we propose a method to use temporal information into the single-frame model byguiding the video sequence embedding through the joint embedding of the targetframe, with negligible increase in computational cost. Experimental resultsdemonstrate that DC-GCT achieves state-of-the-art performance on twochallenging datasets (Human3.6M and MPI-INF-3DHP). Notably, our model achievesstate-of-the-art performance on all action categories in the Human3.6M datasetusing detected 2D poses from CPN, and our code is available at:https://github.com/KHB1698/DC-GCT.</description><author>Hongbo Kang, Yong Wang, Mengyuan Liu, Doudou Wu, Peng Liu, Wenming Yang</author><pubDate>Thu, 10 Aug 2023 03:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05298v1</guid></item><item><title>DSFFNet: Dual-Side Feature Fusion Network for 3D Pose Transfer</title><link>http://arxiv.org/abs/2305.14951v2</link><description>To solve the problem of pose distortion in the forward propagation of posefeatures in existing methods, this pa-per proposes a Dual-Side Feature FusionNetwork for pose transfer (DSFFNet). Firstly, a fixed-length pose code isextracted from the source mesh by a pose encoder and combined with the targetvertices to form a mixed feature; Then, a Feature Fusion Adaptive InstanceNormalization module (FFAdaIN) is designed, which can process both pose andidentity features simultaneously, so that the pose features can be compensatedin layer-by-layer for-ward propagation, thus solving the pose distortionproblem; Finally, using the mesh decoder composed of this module, the pose aregradually transferred to the target mesh. Experimental results on SMPL, SMAL,FAUST and MultiGarment datasets show that DSFFNet successfully solves the posedistortion problem while maintaining a smaller network structure with strongerpose transfer capability and faster convergence speed, and can adapt to mesheswith different numbers of vertices. Code is available athttps://github.com/YikiDragon/DSFFNet</description><author>Jue Liu</author><pubDate>Mon, 09 Oct 2023 09:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14951v2</guid></item><item><title>HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation</title><link>http://arxiv.org/abs/2305.06968v1</link><description>Monocular 3D human pose and shape estimation is an ill-posed problem sincemultiple 3D solutions can explain a 2D image of a subject. Recent approachespredict a probability distribution over plausible 3D pose and shape parametersconditioned on the image. We show that these approaches exhibit a trade-offbetween three key properties: (i) accuracy - the likelihood of the ground-truth3D solution under the predicted distribution, (ii) sample-input consistency -the extent to which 3D samples from the predicted distribution match thevisible 2D image evidence, and (iii) sample diversity - the range of plausible3D solutions modelled by the predicted distribution. Our method, HuManiFlow,predicts simultaneously accurate, consistent and diverse distributions. We usethe human kinematic tree to factorise full body pose into ancestor-conditionedper-body-part pose distributions in an autoregressive manner. Per-body-partdistributions are implemented using normalising flows that respect the manifoldstructure of SO(3), the Lie group of per-body-part poses. We show thatill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,and employ only probabilistic training losses. Code is available at:https://github.com/akashsengupta1997/HuManiFlow.</description><author>Akash Sengupta, Ignas Budvytis, Roberto Cipolla</author><pubDate>Thu, 11 May 2023 17:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06968v1</guid></item><item><title>NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation</title><link>http://arxiv.org/abs/2305.08590v1</link><description>With the progress of 3D human pose and shape estimation, state-of-the-artmethods can either be robust to occlusions or obtain pixel-aligned accuracy innon-occlusion cases. However, they cannot obtain robustness and mesh-imagealignment at the same time. In this work, we present NIKI (Neural InverseKinematics with Invertible Neural Network), which models bi-directional errorsto improve the robustness to occlusions and obtain pixel-aligned accuracy. NIKIcan learn from both the forward and inverse processes with invertible networks.In the inverse process, the model separates the error from the plausible 3Dpose manifold for a robust 3D human pose estimation. In the forward process, weenforce the zero-error boundary conditions to improve the sensitivity toreliable joint positions for better mesh-image alignment. Furthermore, NIKIemulates the analytical inverse kinematics algorithms with the twist-and-swingdecomposition for better interpretability. Experiments on standard andocclusion-specific benchmarks demonstrate the effectiveness of NIKI, where weexhibit robust and well-aligned results simultaneously. Code is available athttps://github.com/Jeff-sjtu/NIKI</description><author>Jiefeng Li, Siyuan Bian, Qi Liu, Jiasheng Tang, Fan Wang, Cewu Lu</author><pubDate>Mon, 15 May 2023 13:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08590v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human</title><link>http://arxiv.org/abs/2307.05853v1</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively).</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Wed, 12 Jul 2023 01:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v1</guid></item><item><title>Spatio-temporal MLP-graph network for 3D human pose estimation</title><link>http://arxiv.org/abs/2308.15313v1</link><description>Graph convolutional networks and their variants have shown significantpromise in 3D human pose estimation. Despite their success, most of thesemethods only consider spatial correlations between body joints and do not takeinto account temporal correlations, thereby limiting their ability to capturerelationships in the presence of occlusions and inherent ambiguity. To addressthis potential weakness, we propose a spatio-temporal network architecturecomposed of a joint-mixing multi-layer perceptron block that facilitatescommunication among different joints and a graph weighted Jacobi network blockthat enables communication among various feature channels. The major novelty ofour approach lies in a new weighted Jacobi feature propagation rule obtainedthrough graph filtering with implicit fairing. We leverage temporal informationfrom the 2D pose sequences, and integrate weight modulation into the model toenable untangling of the feature transformations of distinct nodes. We alsoemploy adjacency modulation with the aim of learning meaningful correlationsbeyond defined linkages between body joints by altering the graph topologythrough a learnable modulation matrix. Extensive experiments on two benchmarkdatasets demonstrate the effectiveness of our model, outperforming recentstate-of-the-art methods for 3D human pose estimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 29 Aug 2023 15:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15313v1</guid></item><item><title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer</title><link>http://arxiv.org/abs/2303.09681v4</link><description>Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available athttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be releasedupon paper acceptance.</description><author>Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng</author><pubDate>Wed, 06 Sep 2023 22:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09681v4</guid></item><item><title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer</title><link>http://arxiv.org/abs/2303.09681v3</link><description>Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available athttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be releasedupon paper acceptance.</description><author>Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng</author><pubDate>Thu, 11 May 2023 00:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09681v3</guid></item><item><title>H3WB: Human3.6M 3D WholeBody Dataset and Benchmark</title><link>http://arxiv.org/abs/2211.15692v2</link><description>We present a benchmark for 3D human whole-body pose estimation, whichinvolves identifying accurate 3D keypoints on the entire human body, includingface, hands, body, and feet. Currently, the lack of a fully annotated andaccurate 3D whole-body dataset results in deep networks being trainedseparately on specific body parts, which are combined during inference. Or theyrely on pseudo-groundtruth provided by parametric body models which are not asaccurate as detection based methods. To overcome these issues, we introduce theHuman3.6M 3D WholeBody (H3WB) dataset, which provides whole-body annotationsfor the Human3.6M dataset using the COCO Wholebody layout. H3WB comprises 133whole-body keypoint annotations on 100K images, made possible by our newmulti-view pipeline. We also propose three tasks: i) 3D whole-body pose liftingfrom 2D complete whole-body pose, ii) 3D whole-body pose lifting from 2Dincomplete whole-body pose, and iii) 3D whole-body pose estimation from asingle RGB image. Additionally, we report several baselines from popularmethods for these tasks. Furthermore, we also provide automated 3D whole-bodyannotations of TotalCapture and experimentally show that when used with H3WB ithelps to improve the performance. Code and dataset is available athttps://github.com/wholebody3d/wholebody3d</description><author>Yue Zhu, Nermin Samet, David Picard</author><pubDate>Wed, 06 Sep 2023 13:22:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15692v2</guid></item><item><title>LInKs "Lifting Independent Keypoints" -- Partial Pose Lifting for Occlusion Handling with Improved Accuracy in 2D-3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.07243v1</link><description>We present LInKs, a novel unsupervised learning method to recover 3D humanposes from 2D kinematic skeletons obtained from a single image, even whenocclusions are present. Our approach follows a unique two-step process, whichinvolves first lifting the occluded 2D pose to the 3D domain, followed byfilling in the occluded parts using the partially reconstructed 3D coordinates.This lift-then-fill approach leads to significantly more accurate resultscompared to models that complete the pose in 2D space alone. Additionally, weimprove the stability and likelihood estimation of normalising flows through acustom sampling function replacing PCA dimensionality reduction previously usedin prior work. Furthermore, we are the first to investigate if different partsof the 2D kinematic skeleton can be lifted independently which we find byitself reduces the error of current lifting approaches. We attribute this tothe reduction of long-range keypoint correlations. In our detailed evaluation,we quantify the error under various realistic occlusion scenarios, showcasingthe versatility and applicability of our model. Our results consistentlydemonstrate the superiority of handling all types of occlusions in 3D spacewhen compared to others that complete the pose in 2D space. Our approach alsoexhibits consistent accuracy in scenarios without occlusion, as evidenced by a7.9% reduction in reconstruction error compared to prior works on the Human3.6Mdataset. Furthermore, our method excels in accurately retrieving complete 3Dposes even in the presence of occlusions, making it highly applicable insituations where complete 2D pose information is unavailable.</description><author>Peter Hardy, Hansung Kim</author><pubDate>Wed, 13 Sep 2023 19:28:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07243v1</guid></item><item><title>Regular Splitting Graph Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2305.05785v1</link><description>In human pose estimation methods based on graph convolutional architectures,the human skeleton is usually modeled as an undirected graph whose nodes arebody joints and edges are connections between neighboring joints. However, mostof these methods tend to focus on learning relationships between body joints ofthe skeleton using first-order neighbors, ignoring higher-order neighbors andhence limiting their ability to exploit relationships between distant joints.In this paper, we introduce a higher-order regular splitting graph network(RS-Net) for 2D-to-3D human pose estimation using matrix splitting inconjunction with weight and adjacency modulation. The core idea is to capturelong-range dependencies between body joints using multi-hop neighborhoods andalso to learn different modulation vectors for different body joints as well asa modulation matrix added to the adjacency matrix associated to the skeleton.This learnable modulation matrix helps adjust the graph structure by addingextra graph edges in an effort to learn additional connections between bodyjoints. Instead of using a shared weight matrix for all neighboring bodyjoints, the proposed RS-Net model applies weight unsharing before aggregatingthe feature vectors associated to the joints in order to capture the differentrelations between them. Experiments and ablations studies performed on twobenchmark datasets demonstrate the effectiveness of our model, achievingsuperior performance over recent state-of-the-art methods for 3D human poseestimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 09 May 2023 23:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05785v1</guid></item><item><title>HDFormer: High-order Directed Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.01825v2</link><description>Human pose estimation is a challenging task due to its structured datasequence nature. Existing methods primarily focus on pair-wise interaction ofbody joints, which is insufficient for scenarios involving overlapping jointsand rapidly changing poses. To overcome these issues, we introduce a novelapproach, the High-order Directed Transformer (HDFormer), which leverageshigh-order bone and joint relationships for improved pose estimation.Specifically, HDFormer incorporates both self-attention and high-orderattention to formulate a multi-order attention module. This module facilitatesfirst-order "joint$\leftrightarrow$joint", second-order"bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint"interactions, effectively addressing issues in complex and occlusion-heavysituations. In addition, modern CNN techniques are integrated into thetransformer-based architecture, balancing the trade-off between performance andefficiency. HDFormer significantly outperforms state-of-the-art (SOTA) modelson Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parametersand significantly lower computational costs. Moreover, HDFormer demonstratesbroad real-world applicability, enabling real-time, accurate 3D poseestimation. The source code is in https://github.com/hyer/HDFormer</description><author>Hanyuan Chen, Jun-Yan He, Wangmeng Xiang, Zhi-Qi Cheng, Wei Liu, Hanbing Liu, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 22 May 2023 07:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01825v2</guid></item><item><title>Just Add $π$! Pose Induced Video Transformers for Understanding Activities of Daily Living</title><link>http://arxiv.org/abs/2311.18840v1</link><description>Video transformers have become the de facto standard for human actionrecognition, yet their exclusive reliance on the RGB modality still limitstheir adoption in certain domains. One such domain is Activities of DailyLiving (ADL), where RGB alone is not sufficient to distinguish between visuallysimilar actions, or actions observed from multiple viewpoints. To facilitatethe adoption of video transformers for ADL, we hypothesize that theaugmentation of RGB with human pose information, known for its sensitivity tofine-grained motion and multiple viewpoints, is essential. Consequently, weintroduce the first Pose Induced Video Transformer: PI-ViT (or $\pi$-ViT), anovel approach that augments the RGB representations learned by videotransformers with 2D and 3D pose information. The key elements of $\pi$-ViT aretwo plug-in modules, 2D Skeleton Induction Module and 3D Skeleton InductionModule, that are responsible for inducing 2D and 3D pose information into theRGB representations. These modules operate by performing pose-aware auxiliarytasks, a design choice that allows $\pi$-ViT to discard the modules duringinference. Notably, $\pi$-ViT achieves the state-of-the-art performance onthree prominent ADL datasets, encompassing both real-world and large-scaleRGB-D datasets, without requiring poses or additional computational overhead atinference.</description><author>Dominick Reilly, Srijan Das</author><pubDate>Thu, 30 Nov 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18840v1</guid></item><item><title>3DPortraitGAN: Learning One-Quarter Headshot 3D GANs from a Single-View Portrait Dataset with Diverse Body Poses</title><link>http://arxiv.org/abs/2307.14770v2</link><description>3D-aware face generators are typically trained on 2D real-life face imagedatasets that primarily consist of near-frontal face data, and as such, theyare unable to construct one-quarter headshot 3D portraits with complete head,neck, and shoulder geometry. Two reasons account for this issue: First,existing facial recognition methods struggle with extracting facial datacaptured from large camera angles or back views. Second, it is challenging tolearn a distribution of 3D portraits covering the one-quarter headshot regionfrom single-view data due to significant geometric deformation caused bydiverse body poses. To this end, we first create the dataset360{\deg}-Portrait-HQ (360{\deg}PHQ for short) which consists of high-qualitysingle-view real portraits annotated with a variety of camera parameters (theyaw angles span the entire 360{\deg} range) and body poses. We then propose3DPortraitGAN, the first 3D-aware one-quarter headshot portrait generator thatlearns a canonical 3D avatar distribution from the 360{\deg}PHQ dataset withbody pose self-learning. Our model can generate view-consistent portrait imagesfrom all camera angles with a canonical one-quarter headshot 3D representation.Our experiments show that the proposed framework can accurately predictportrait body poses and generate view-consistent, realistic portrait imageswith complete geometry from all camera angles.</description><author>Yiqian Wu, Hao Xu, Xiangjun Tang, Hongbo Fu, Xiaogang Jin</author><pubDate>Mon, 21 Aug 2023 07:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14770v2</guid></item><item><title>Robot at the Mirror: Learning to Imitate via Associating Self-supervised Models</title><link>http://arxiv.org/abs/2311.13226v1</link><description>We introduce an approach to building a custom model from ready-madeself-supervised models via their associating instead of training andfine-tuning. We demonstrate it with an example of a humanoid robot looking atthe mirror and learning to detect the 3D pose of its own body from the image itperceives. To build our model, we first obtain features from the visual inputand the postures of the robot's body via models prepared before the robot'soperation. Then, we map their corresponding latent spaces by a sample-efficientrobot's self-exploration at the mirror. In this way, the robot builds thesolicited 3D pose detector, which quality is immediately perfect on theacquired samples instead of obtaining the quality gradually. The mapping, whichemploys associating the pairs of feature vectors, is then implemented in thesame way as the key-value mechanism of the famous transformer models. Finally,deploying our model for imitation to a simulated robot allows us to study, tuneup, and systematically evaluate its hyperparameters without the involvement ofthe human counterpart, advancing our previous research.</description><author>Andrej Lúčny, Kristína Malinovská, Igor Farkaš</author><pubDate>Wed, 22 Nov 2023 08:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13226v1</guid></item><item><title>Interweaved Graph and Attention Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2304.14045v1</link><description>Despite substantial progress in 3D human pose estimation from a single-viewimage, prior works rarely explore global and local correlations, leading toinsufficient learning of human skeleton representations. To address this issue,we propose a novel Interweaved Graph and Attention Network (IGANet) that allowsbidirectional communications between graph convolutional networks (GCNs) andattentions. Specifically, we introduce an IGA module, where attentions areprovided with local information from GCNs and GCNs are injected with globalinformation from attentions. Additionally, we design a simple yet effectiveU-shaped multi-layer perceptron (uMLP), which can capture multi-granularityinformation for body joints. Extensive experiments on two popular benchmarkdatasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate ourproposed method.The results show that IGANet achieves state-of-the-artperformance on both datasets. Code is available athttps://github.com/xiu-cs/IGANet.</description><author>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li</author><pubDate>Thu, 27 Apr 2023 10:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14045v1</guid></item><item><title>Learning to Predict Scene-Level Implicit 3D from Posed RGBD Data</title><link>http://arxiv.org/abs/2306.08671v1</link><description>We introduce a method that can learn to predict scene-level implicitfunctions for 3D reconstruction from posed RGBD data. At test time, our systemmaps a previously unseen RGB image to a 3D reconstruction of a scene viaimplicit functions. While implicit functions for 3D reconstruction have oftenbeen tied to meshes, we show that we can train one using only a set of posedRGBD images. This setting may help 3D reconstruction unlock the sea ofaccelerometer+RGBD data that is coming with new phones. Our system, D2-DRDF,can match and sometimes outperform current methods that use mesh supervisionand shows better robustness to sparse data.</description><author>Nilesh Kulkarni, Linyi Jin, Justin Johnson, David F. Fouhey</author><pubDate>Wed, 14 Jun 2023 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08671v1</guid></item><item><title>EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild</title><link>http://arxiv.org/abs/2308.16894v1</link><description>We present EMDB, the Electromagnetic Database of Global 3D Human Pose andShape in the Wild. EMDB is a novel dataset that contains high-quality 3D SMPLpose and shape parameters with global body and camera trajectories forin-the-wild videos. We use body-worn, wireless electromagnetic (EM) sensors anda hand-held iPhone to record a total of 58 minutes of motion data, distributedover 81 indoor and outdoor sequences and 10 participants. Together withaccurate body poses and shapes, we also provide global camera poses and bodyroot trajectories. To construct EMDB, we propose a multi-stage optimizationprocedure, which first fits SMPL to the 6-DoF EM measurements and then refinesthe poses via image observations. To achieve high-quality results, we leveragea neural implicit avatar model to reconstruct detailed human surface geometryand appearance, which allows for improved alignment and smoothness via a densepixel-level objective. Our evaluations, conducted with a multi-view volumetriccapture system, indicate that EMDB has an expected accuracy of 2.3 cmpositional and 10.6 degrees angular error, surpassing the accuracy of previousin-the-wild datasets. We evaluate existing state-of-the-art monocular RGBmethods for camera-relative and global pose estimation on EMDB. EMDB ispublicly available under https://ait.ethz.ch/emdb</description><author>Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan Zarate, Otmar Hilliges</author><pubDate>Thu, 31 Aug 2023 18:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16894v1</guid></item><item><title>Occlusion Robust 3D Human Pose Estimation with StridedPoseGraphFormer and Data Augmentation</title><link>http://arxiv.org/abs/2304.12069v1</link><description>Occlusion is an omnipresent challenge in 3D human pose estimation (HPE). Inspite of the large amount of research dedicated to 3D HPE, only a limitednumber of studies address the problem of occlusion explicitly. To fill thisgap, we propose to combine exploitation of spatio-temporal features withsynthetic occlusion augmentation during training to deal with occlusion. Tothis end, we build a spatio-temporal 3D HPE model, StridedPoseGraphFormer basedon graph convolution and transformers, and train it using occlusionaugmentation. Unlike the existing occlusion-aware methods, that are only testedfor limited occlusion, we extensively evaluate our method for varying degreesof occlusion. We show that our proposed method compares favorably with thestate-of-the-art (SoA). Our experimental results also reveal that in theabsence of any occlusion handling mechanism, the performance of SoA 3D HPEmethods degrades significantly when they encounter occlusion.</description><author>Soubarna Banik, Patricia Gschoßmann, Alejandro Mendoza Garcia, Alois Knoll</author><pubDate>Mon, 24 Apr 2023 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12069v1</guid></item><item><title>EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision</title><link>http://arxiv.org/abs/2303.02862v2</link><description>Event camera shows great potential in 3D hand pose estimation, especiallyaddressing the challenges of fast motion and high dynamic range in a low-powerway. However, due to the asynchronous differential imaging mechanism, it ischallenging to design event representation to encode hand motion informationespecially when the hands are not moving (causing motion ambiguity), and it isinfeasible to fully annotate the temporally dense event stream. In this paper,we propose EvHandPose with novel hand flow representations in Event-to-Posemodule for accurate hand pose estimation and alleviating the motion ambiguityissue. To solve the problem under sparse annotation, we design contrastmaximization and hand-edge constraints in Pose-to-IWE (Image with WarpedEvents) module and formulate EvHandPose in a weakly-supervision framework. Wefurther build EvRealHands, the first large-scale real-world event-based handpose dataset on several challenging scenes to bridge the real-synthetic domaingap. Experiments on EvRealHands demonstrate that EvHandPose outperformsprevious event-based methods under all evaluation scenes, achieves accurate andstable hand pose estimation with high temporal resolution in fast motion andstrong light scenes compared with RGB-based methods, generalizes well tooutdoor scenes and another type of event camera, and shows the potential forthe hand gesture recognition task.</description><author>Jianping Jiang, Jiahe Li, Baowen Zhang, Xiaoming Deng, Boxin Shi</author><pubDate>Wed, 30 Aug 2023 04:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02862v2</guid></item><item><title>Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems</title><link>http://arxiv.org/abs/2308.08511v2</link><description>Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucialtechnologies in the field of medical imaging. Score-based models have proven tobe effective in addressing different inverse problems encountered in CT andMRI, such as sparse-view CT and fast MRI reconstruction. However, these modelsface challenges in achieving accurate three dimensional (3D) volumetricreconstruction. The existing score-based models primarily focus onreconstructing two dimensional (2D) data distribution, leading toinconsistencies between adjacent slices in the reconstructed 3D volumetricimages. To overcome this limitation, we propose a novel two-and-a-half orderscore-based model (TOSM). During the training phase, our TOSM learns datadistributions in 2D space, which reduces the complexity of training compared todirectly working on 3D volumes. However, in the reconstruction phase, the TOSMupdates the data distribution in 3D space, utilizing complementary scores alongthree directions (sagittal, coronal, and transaxial) to achieve a more precisereconstruction. The development of TOSM is built on robust theoreticalprinciples, ensuring its reliability and efficacy. Through extensiveexperimentation on large-scale sparse-view CT and fast MRI datasets, our methoddemonstrates remarkable advancements and attains state-of-the-art results insolving 3D ill-posed inverse problems. Notably, the proposed TOSM effectivelyaddresses the inter-slice inconsistency issue, resulting in high-quality 3Dvolumetric reconstruction.</description><author>Zirong Li, Yanyang Wang, Jianjia Zhang, Weiwen Wu, Hengyong Yu</author><pubDate>Thu, 17 Aug 2023 06:09:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08511v2</guid></item><item><title>Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems</title><link>http://arxiv.org/abs/2308.08511v1</link><description>Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucialtechnologies in the field of medical imaging. Score-based models have proven tobe effective in addressing different inverse problems encountered in CT andMRI, such as sparse-view CT and fast MRI reconstruction. However, these modelsface challenges in achieving accurate three dimensional (3D) volumetricreconstruction. The existing score-based models primarily focus onreconstructing two dimensional (2D) data distribution, leading toinconsistencies between adjacent slices in the reconstructed 3D volumetricimages. To overcome this limitation, we propose a novel two-and-a-half orderscore-based model (TOSM). During the training phase, our TOSM learns datadistributions in 2D space, which reduces the complexity of training compared todirectly working on 3D volumes. However, in the reconstruction phase, the TOSMupdates the data distribution in 3D space, utilizing complementary scores alongthree directions (sagittal, coronal, and transaxial) to achieve a more precisereconstruction. The development of TOSM is built on robust theoreticalprinciples, ensuring its reliability and efficacy. Through extensiveexperimentation on large-scale sparse-view CT and fast MRI datasets, our methoddemonstrates remarkable advancements and attains state-of-the-art results insolving 3D ill-posed inverse problems. Notably, the proposed TOSM effectivelyaddresses the inter-slice inconsistency issue, resulting in high-quality 3Dvolumetric reconstruction.</description><author>Zirong Li, Yanyang Wang, Jianjia Zhang, Weiwen Wu, Hengyong Yu</author><pubDate>Wed, 16 Aug 2023 18:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08511v1</guid></item><item><title>TAPE: Temporal Attention-based Probabilistic human pose and shape Estimation</title><link>http://arxiv.org/abs/2305.00181v1</link><description>Reconstructing 3D human pose and shape from monocular videos is awell-studied but challenging problem. Common challenges include occlusions, theinherent ambiguities in the 2D to 3D mapping and the computational complexityof video processing. Existing methods ignore the ambiguities of thereconstruction and provide a single deterministic estimate for the 3D pose. Inorder to address these issues, we present a Temporal Attention basedProbabilistic human pose and shape Estimation method (TAPE) that operates on anRGB video. More specifically, we propose to use a neural network to encodevideo frames to temporal features using an attention-based neural network.Given these features, we output a per-frame but temporally-informed probabilitydistribution for the human pose using Normalizing Flows. We show that TAPEoutperforms state-of-the-art methods in standard benchmarks and serves as aneffective video-based prior for optimization-based human pose and shapeestimation. Code is available at: https: //github.com/nikosvasilik/TAPE</description><author>Nikolaos Vasilikopoulos, Nikos Kolotouros, Aggeliki Tsoli, Antonis Argyros</author><pubDate>Sat, 29 Apr 2023 07:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00181v1</guid></item><item><title>Optimal and Robust Category-level Perception: Object Pose and Shape Estimation from 2D and 3D Semantic Keypoints</title><link>http://arxiv.org/abs/2206.12498v2</link><description>We consider a category-level perception problem, where one is given 2D or 3Dsensor data picturing an object of a given category (e.g., a car), and has toreconstruct the 3D pose and shape of the object despite intra-class variability(i.e., different car models have different shapes). We consider an active shapemodel, where -for an object category- we are given a library of potential CADmodels describing objects in that category, and we adopt a standard formulationwhere pose and shape are estimated from 2D or 3D keypoints via non-convexoptimization. Our first contribution is to develop PACE3D* and PACE2D*, thefirst certifiably optimal solvers for pose and shape estimation using 3D and 2Dkeypoints, respectively. Both solvers rely on the design of tight (i.e., exact)semidefinite relaxations. Our second contribution is to develop outlier-robustversions of both solvers, named PACE3D# and PACE2D#. Towards this goal, wepropose ROBIN, a general graph-theoretic framework to prune outliers, whichuses compatibility hypergraphs to model measurements' compatibility. We showthat in category-level perception problems these hypergraphs can be built fromthe winding orders of the keypoints (in 2D) or their convex hulls (in 3D), andmany outliers can be filtered out via maximum hyperclique computation. The lastcontribution is an extensive experimental evaluation. Besides providing anablation study on simulated datasets and on the PASCAL3D+ dataset, we combineour solver with a deep keypoint detector, and show that PACE3D# improves overthe state of the art in vehicle pose estimation in the ApolloScape datasets,and its runtime is compatible with practical applications. We release our codeat https://github.com/MIT-SPARK/PACE.</description><author>Jingnan Shi, Heng Yang, Luca Carlone</author><pubDate>Mon, 15 May 2023 04:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12498v2</guid></item><item><title>3D Human Pose Estimation via Intuitive Physics</title><link>http://arxiv.org/abs/2303.18246v3</link><description>Estimating 3D humans from images often produces implausible bodies that lean,float, or penetrate the floor. Such methods ignore the fact that bodies aretypically supported by the scene. A physics engine can be used to enforcephysical plausibility, but these are not differentiable, rely on unrealisticproxy bodies, and are difficult to integrate into existing optimization andlearning frameworks. In contrast, we exploit novel intuitive-physics (IP) termsthat can be inferred from a 3D SMPL body interacting with the scene. Inspiredby biomechanics, we infer the pressure heatmap on the body, the Center ofPressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). Withthese, we develop IPMAN, to estimate a 3D body from a color image in a "stable"configuration by encouraging plausible floor contact and overlapping CoP andCoM. Our IP terms are intuitive, easy to implement, fast to compute,differentiable, and can be integrated into existing optimization and regressionmethods. We evaluate IPMAN on standard datasets and MoYo, a new dataset withsynchronized multi-view images, ground-truth 3D bodies with complex poses,body-floor contact, CoM and pressure. IPMAN produces more plausible resultsthan the state of the art, improving accuracy for static poses, while nothurting dynamic ones. Code and data are available for research athttps://ipman.is.tue.mpg.de.</description><author>Shashank Tripathi, Lea Müller, Chun-Hao P. Huang, Omid Taheri, Michael J. Black, Dimitrios Tzionas</author><pubDate>Mon, 24 Jul 2023 06:35:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.18246v3</guid></item><item><title>Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification</title><link>http://arxiv.org/abs/2308.10658v3</link><description>Long-Term Person Re-Identification (LT-ReID) has become increasingly crucialin computer vision and biometrics. In this work, we aim to extend LT-ReIDbeyond pedestrian recognition to include a wider range of real-world humanactivities while still accounting for cloth-changing scenarios over large timegaps. This setting poses additional challenges due to the geometricmisalignment and appearance ambiguity caused by the diversity of human pose andclothing. To address these challenges, we propose a new approach 3DInvarReIDfor (i) disentangling identity from non-identity components (pose, clothingshape, and texture) of 3D clothed humans, and (ii) reconstructing accurate 3Dclothed body shapes and learning discriminative features of naked body shapesfor person ReID in a joint manner. To better evaluate our study of LT-ReID, wecollect a real-world dataset called CCDA, which contains a wide variety ofhuman activities and clothing changes. Experimentally, we show the superiorperformance of our approach for person ReID.</description><author>Feng Liu, Minchul Kim, ZiAng Gu, Anil Jain, Xiaoming Liu</author><pubDate>Thu, 21 Sep 2023 05:23:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10658v3</guid></item><item><title>Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification</title><link>http://arxiv.org/abs/2308.10658v2</link><description>Long-Term Person Re-Identification (LT-ReID) has become increasingly crucialin computer vision and biometrics. In this work, we aim to extend LT-ReIDbeyond pedestrian recognition to include a wider range of real-world humanactivities while still accounting for cloth-changing scenarios over large timegaps. This setting poses additional challenges due to the geometricmisalignment and appearance ambiguity caused by the diversity of human pose andclothing. To address these challenges, we propose a new approach 3DInvarReIDfor (i) disentangling identity from non-identity components (pose, clothingshape, and texture) of 3D clothed humans, and (ii) reconstructing accurate 3Dclothed body shapes and learning discriminative features of naked body shapesfor person ReID in a joint manner. To better evaluate our study of LT-ReID, wecollect a real-world dataset called CCDA, which contains a wide variety ofhuman activities and clothing changes. Experimentally, we show the superiorperformance of our approach for person ReID.</description><author>Feng Liu, Minchul Kim, ZiAng Gu, Anil Jain, Xiaoming Liu</author><pubDate>Tue, 29 Aug 2023 11:37:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10658v2</guid></item><item><title>Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification</title><link>http://arxiv.org/abs/2308.10658v1</link><description>Long-Term Person Re-Identification (LT-ReID) has become increasingly crucialin computer vision and biometrics. In this work, we aim to extend LT-ReIDbeyond pedestrian recognition to include a wider range of real-world humanactivities while still accounting for cloth-changing scenarios over large timegaps. This setting poses additional challenges due to the geometricmisalignment and appearance ambiguity caused by the diversity of human pose andclothing. To address these challenges, we propose a new approach 3DInvarReIDfor (i) disentangling identity from non-identity components (pose, clothingshape, and texture) of 3D clothed humans, and (ii) reconstructing accurate 3Dclothed body shapes and learning discriminative features of naked body shapesfor person ReID in a joint manner. To better evaluate our study of LT-ReID, wecollect a real-world dataset called CCDA, which contains a wide variety ofhuman activities and clothing changes. Experimentally, we show the superiorperformance of our approach for person ReID.</description><author>Feng Liu, Minchul Kim, ZiAng Gu, Anil Jian, Xiaoming Liu</author><pubDate>Mon, 21 Aug 2023 12:51:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10658v1</guid></item><item><title>Implicit 3D Human Mesh Recovery using Consistency with Pose and Shape from Unseen-view</title><link>http://arxiv.org/abs/2306.17651v1</link><description>From an image of a person, we can easily infer the natural 3D pose and shapeof the person even if ambiguity exists. This is because we have a mental modelthat allows us to imagine a person's appearance at different viewing directionsfrom a given image and utilize the consistency between them for inference.However, existing human mesh recovery methods only consider the direction inwhich the image was taken due to their structural limitations. Hence, wepropose "Implicit 3D Human Mesh Recovery (ImpHMR)" that can implicitly imaginea person in 3D space at the feature-level via Neural Feature Fields. In ImpHMR,feature fields are generated by CNN-based image encoder for a given image.Then, the 2D feature map is volume-rendered from the feature field for a givenviewing direction, and the pose and shape parameters are regressed from thefeature. To utilize consistency with pose and shape from unseen-view, if thereare 3D labels, the model predicts results including the silhouette from anarbitrary direction and makes it equal to the rotated ground-truth. In the caseof only 2D labels, we perform self-supervised learning through the constraintthat the pose and shape parameters inferred from different directions should bethe same. Extensive evaluations show the efficacy of the proposed method.</description><author>Hanbyel Cho, Yooshin Cho, Jaesung Ahn, Junmo Kim</author><pubDate>Fri, 30 Jun 2023 14:37:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17651v1</guid></item><item><title>Implicit 3D Human Mesh Recovery using Consistency with Pose and Shape from Unseen-view</title><link>http://arxiv.org/abs/2306.17651v2</link><description>From an image of a person, we can easily infer the natural 3D pose and shapeof the person even if ambiguity exists. This is because we have a mental modelthat allows us to imagine a person's appearance at different viewing directionsfrom a given image and utilize the consistency between them for inference.However, existing human mesh recovery methods only consider the direction inwhich the image was taken due to their structural limitations. Hence, wepropose "Implicit 3D Human Mesh Recovery (ImpHMR)" that can implicitly imaginea person in 3D space at the feature-level via Neural Feature Fields. In ImpHMR,feature fields are generated by CNN-based image encoder for a given image.Then, the 2D feature map is volume-rendered from the feature field for a givenviewing direction, and the pose and shape parameters are regressed from thefeature. To utilize consistency with pose and shape from unseen-view, if thereare 3D labels, the model predicts results including the silhouette from anarbitrary direction and makes it equal to the rotated ground-truth. In the caseof only 2D labels, we perform self-supervised learning through the constraintthat the pose and shape parameters inferred from different directions should bethe same. Extensive evaluations show the efficacy of the proposed method.</description><author>Hanbyel Cho, Yooshin Cho, Jaesung Ahn, Junmo Kim</author><pubDate>Mon, 03 Jul 2023 02:29:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17651v2</guid></item><item><title>Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception</title><link>http://arxiv.org/abs/2306.06362v2</link><description>We introduce the Aria Digital Twin (ADT) - an egocentric dataset capturedusing Aria glasses with extensive object, environment, and human level groundtruth. This ADT release contains 200 sequences of real-world activitiesconducted by Aria wearers in two real indoor scenes with 398 object instances(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of twomonochrome camera streams, one RGB camera stream, two IMU streams; b) completesensor calibration; c) ground truth data including continuous6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eyegaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)photo-realistic synthetic renderings. To the best of our knowledge, there is noexisting egocentric dataset with a level of accuracy, photo-realism andcomprehensiveness comparable to ADT. By contributing ADT to the researchcommunity, our mission is to set a new standard for evaluation in theegocentric machine perception domain, which includes very challenging researchproblems such as 3D object detection and tracking, scene reconstruction andunderstanding, sim-to-real learning, human pose prediction - while alsoinspiring new machine perception tasks for augmented reality (AR) applications.To kick start exploration of the ADT research use cases, we evaluated severalexisting state-of-the-art methods for object detection, segmentation and imagetranslation tasks that demonstrate the usefulness of ADT as a benchmarkingdataset.</description><author>Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, Carl Yuheng Ren</author><pubDate>Tue, 13 Jun 2023 07:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06362v2</guid></item><item><title>A Horse with no Labels: Self-Supervised Horse Pose Estimation from Unlabelled Images and Synthetic Prior</title><link>http://arxiv.org/abs/2308.03411v1</link><description>Obtaining labelled data to train deep learning methods for estimating animalpose is challenging. Recently, synthetic data has been widely used for poseestimation tasks, but most methods still rely on supervised learning paradigmsutilising synthetic images and labels. Can training be fully unsupervised? Is atiny synthetic dataset sufficient? What are the minimum assumptions that wecould make for estimating animal pose? Our proposal addresses these questionsthrough a simple yet effective self-supervised method that only assumes theavailability of unlabelled images and a small set of synthetic 2D poses. Wecompletely remove the need for any 3D or 2D pose annotations (or complex 3Danimal models), and surprisingly our approach can still learn accurate 3D and2D poses simultaneously. We train our method with unlabelled images of horsesmainly collected for YouTube videos and a prior consisting of 2D syntheticposes. The latter is three times smaller than the number of images needed fortraining. We test our method on a challenging set of horse images and evaluatethe predicted 3D and 2D poses. We demonstrate that it is possible to learnaccurate animal poses even with as few assumptions as unlabelled images and asmall set of 2D poses generated from synthetic data. Given the minimumrequirements and the abundance of unlabelled data, our method could be easilydeployed to different animals.</description><author>Jose Sosa, David Hogg</author><pubDate>Mon, 07 Aug 2023 10:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03411v1</guid></item><item><title>An Efficient Convex Hull-based Vehicle Pose Estimation Method for 3D LiDAR</title><link>http://arxiv.org/abs/2302.01034v2</link><description>Vehicle pose estimation with LiDAR is essential in the perception technologyof autonomous driving. However, due to incomplete observation measurements andsparsity of the LiDAR point cloud, it is challenging to achieve satisfactorypose extraction based on 3D LiDAR by using the existing pose estimationmethods. In addition, the requirement for real-time performance furtherincreases the difficulty of the pose estimation task. In this paper, weproposed a novel convex hull-based vehicle pose estimation method. Theextracted 3D cluster is reduced to the convex hull, reducing the computationburden and retaining contour information. Then a novel criterion based on theminimum occlusion area is developed for the search-based algorithm, which canachieve accurate pose estimation. This criterion also makes the proposedalgorithm especially suitable for obstacle avoidance. The proposed algorithm isvalidated on the KITTI dataset and a manually labeled dataset acquired at anindustrial park. The results show that our proposed method can achieve betteraccuracy than the state-of-the-art pose estimation method while maintainingreal-time speed.</description><author>Ningning Ding</author><pubDate>Sun, 02 Jul 2023 00:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01034v2</guid></item><item><title>KeyMatchNet: Zero-Shot Pose Estimation in 3D Point Clouds by Generalized Keypoint Matching</title><link>http://arxiv.org/abs/2303.16102v2</link><description>In this paper, we present KeyMatchNet, a novel network for zero-shot poseestimation in 3D point clouds. The network is trained to match object keypointswith scene-points, and these matches are then used for pose estimation. Themethod generalizes to new objects by using not only the scene point cloud asinput but also the object point cloud. This is in contrast with conventionalmethods where object features are stored in network weights. By having ageneralized network we avoid the need for training new models for novelobjects, thus significantly decreasing the computational requirements of themethod. However, as a result of the complexity, zero-shot pose estimation methodsgenerally have lower performance than networks trained for a single object. Toaddress this, we reduce the complexity of the task by including the scenarioinformation during training. This is generally not feasible as collecting realdata for new tasks increases the cost drastically. But, in the zero-shot poseestimation task, no retraining is needed for new objects. The expensive datacollection can thus be performed once, and the scenario information is retainedin the network weights. The network is trained on 1,500 objects and is tested on unseen objects. Wedemonstrate that the trained network can accurately estimate poses for novelobjects and demonstrate the ability of the network to perform outside of thetrained class. We believe that the presented method is valuable for manyreal-world scenarios. Code, trained network, and dataset will be made availableat publication.</description><author>Frederik Hagelskjær, Rasmus Laurvig Haugaard</author><pubDate>Tue, 26 Sep 2023 12:51:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16102v2</guid></item><item><title>PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds</title><link>http://arxiv.org/abs/2308.14492v1</link><description>Human pose and shape estimation (HPS) has attracted increasing attention inrecent years. While most existing studies focus on HPS from 2D images or videoswith inherent depth ambiguity, there are surging need to investigate HPS from3D point clouds as depth sensors have been frequently employed in commercialdevices. However, real-world sensory 3D points are usually noisy andincomplete, and also human bodies could have different poses of high diversity.To tackle these challenges, we propose a principled framework, PointHPS, foraccurate 3D HPS from point clouds captured in real-world settings, whichiteratively refines point features through a cascaded architecture.Specifically, each stage of PointHPS performs a series of downsampling andupsampling operations to extract and collate both local and global cues, whichare further enhanced by two novel modules: 1) Cross-stage Feature Fusion (CFF)for multi-scale feature propagation that allows information to flow effectivelythrough the stages, and 2) Intermediate Feature Enhancement (IFE) forbody-aware feature aggregation that improves feature quality after each stage.To facilitate a comprehensive study under various scenarios, we conduct ourexperiments on two large-scale benchmarks, comprising i) a dataset thatfeatures diverse subjects and actions captured by real commercial sensors in alaboratory environment, and ii) controlled synthetic data generated withrealistic considerations such as clothed humans in crowded outdoor scenes.Extensive experiments demonstrate that PointHPS, with its powerful pointfeature extraction and processing scheme, outperforms State-of-the-Art methodsby significant margins across the board. Homepage:https://caizhongang.github.io/projects/PointHPS/.</description><author>Zhongang Cai, Liang Pan, Chen Wei, Wanqi Yin, Fangzhou Hong, Mingyuan Zhang, Chen Change Loy, Lei Yang, Ziwei Liu</author><pubDate>Mon, 28 Aug 2023 12:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14492v1</guid></item><item><title>Learning from Abstract Images: on the Importance of Occlusion in a Minimalist Encoding of Human Poses</title><link>http://arxiv.org/abs/2307.09893v1</link><description>Existing 2D-to-3D pose lifting networks suffer from poor performance incross-dataset benchmarks. Although the use of 2D keypoints joined by"stick-figure" limbs has shown promise as an intermediate step, stick-figuresdo not account for occlusion information that is often inherent in an image. Inthis paper, we propose a novel representation using opaque 3D limbs thatpreserves occlusion information while implicitly encoding joint locations.Crucially, when training on data with accurate three-dimensional keypoints andwithout part-maps, this representation allows training on abstract syntheticimages, with occlusion, from as many synthetic viewpoints as desired. Theresult is a pose defined by limb angles rather than joint positions$\unicode{x2013}$ because poses are, in the real world, independent of cameras$\unicode{x2013}$ allowing us to predict poses that are completely independentof camera viewpoint. The result provides not only an improvement insame-dataset benchmarks, but a "quantum leap" in cross-dataset benchmarks.</description><author>Saad Manzur, Wayne Hayes</author><pubDate>Wed, 19 Jul 2023 11:45:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09893v1</guid></item><item><title>3D-Aware Object Localization using Gaussian Implicit Occupancy Function</title><link>http://arxiv.org/abs/2303.02058v2</link><description>To automatically localize a target object in an image is crucial for manycomputer vision applications. To represent the 2D object, ellipse labels haverecently been identified as a promising alternative to axis-aligned boundingboxes. This paper further considers 3D-aware ellipse labels, \textit{i.e.},ellipses which are projections of a 3D ellipsoidal approximation of the object,for 2D target localization. Indeed, projected ellipses carry more geometricinformation about the object geometry and pose (3D awareness) than traditional3D-agnostic bounding box labels. Moreover, such a generic 3D ellipsoidal modelallows for approximating known to coarsely known targets. We then propose tohave a new look at ellipse regression and replace the discontinuous geometricellipse parameters with the parameters of an implicit Gaussian distributionencoding object occupancy in the image. The models are trained to regress thevalues of this bivariate Gaussian distribution over the image pixels using astatistical loss function. We introduce a novel non-trainable differentiablelayer, E-DSNT, to extract the distribution parameters. Also, we describe how toreadily generate consistent 3D-aware Gaussian occupancy parameters using onlycoarse dimensions of the target and relative pose labels. We extend threeexisting spacecraft pose estimation datasets with 3D-aware Gaussian occupancylabels to validate our hypothesis. Labels and source code are publiclyaccessible here: https://cvi2.uni.lu/3d-aware-obj-loc/.</description><author>Vincent Gaudillière, Leo Pauly, Arunkumar Rathinam, Albert Garcia Sanchez, Mohamed Adel Musallam, Djamila Aouada</author><pubDate>Wed, 02 Aug 2023 15:21:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02058v2</guid></item><item><title>Cloth2Body: Generating 3D Human Body Mesh from 2D Clothing</title><link>http://arxiv.org/abs/2309.16189v1</link><description>In this paper, we define and study a new Cloth2Body problem which has a goalof generating 3D human body meshes from a 2D clothing image. Unlike theexisting human mesh recovery problem, Cloth2Body needs to address new andemerging challenges raised by the partial observation of the input and the highdiversity of the output. Indeed, there are three specific challenges. First,how to locate and pose human bodies into the clothes. Second, how toeffectively estimate body shapes out of various clothing types. Finally, how togenerate diverse and plausible results from a 2D clothing image. To this end,we propose an end-to-end framework that can accurately estimate 3D body meshparameterized by pose and shape from a 2D clothing image. Along this line, wefirst utilize Kinematics-aware Pose Estimation to estimate body poseparameters. 3D skeleton is employed as a proxy followed by an inversekinematics module to boost the estimation accuracy. We additionally design anadaptive depth trick to align the re-projected 3D mesh better with 2D clothingimage by disentangling the effects of object size and camera extrinsic. Next,we propose Physics-informed Shape Estimation to estimate body shape parameters.3D shape parameters are predicted based on partial body measurements estimatedfrom RGB image, which not only improves pixel-wise human-cloth alignment, butalso enables flexible user editing. Finally, we design Evolution-based posegeneration method, a skeleton transplanting method inspired by geneticalgorithms to generate diverse reasonable poses during inference. As shown byexperimental results on both synthetic and real-world data, the proposedframework achieves state-of-the-art performance and can effectively recovernatural and diverse 3D body meshes from 2D images that align well withclothing.</description><author>Lu Dai, Liqian Ma, Shenhan Qian, Hao Liu, Ziwei Liu, Hui Xiong</author><pubDate>Thu, 28 Sep 2023 07:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16189v1</guid></item></channel></rss>