<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 10 May 2023 06:00:27 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Neural Voting Field for Camera-Space 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2305.04328v1</link><description>We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.</description><author>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</author><pubDate>Sun, 07 May 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04328v1</guid></item><item><title>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</title><link>http://arxiv.org/abs/2206.02257v3</link><description>In this survey, we present a systematic review of 3D hand pose estimationfrom the perspective of efficient annotation and learning. 3D hand poseestimation has been an important research area owing to its potential to enablevarious applications, such as video understanding, AR/VR, and robotics.However, the performance of models is tied to the quality and quantity ofannotated 3D hand poses. Under the status quo, acquiring such annotated 3D handposes is challenging, e.g., due to the difficulty of 3D annotation and thepresence of occlusion. To reveal this problem, we review the pros and cons ofexisting annotation methods classified as manual, synthetic-model-based,hand-sensor-based, and computational approaches. Additionally, we examinemethods for learning 3D hand poses when annotated data are scarce, includingself-supervised pretraining, semi-supervised learning, and domain adaptation.Based on the study of efficient annotation and learning, we further discusslimitations and possible future directions in this field.</description><author>Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato</author><pubDate>Wed, 26 Apr 2023 07:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02257v3</guid></item><item><title>Interweaved Graph and Attention Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2304.14045v1</link><description>Despite substantial progress in 3D human pose estimation from a single-viewimage, prior works rarely explore global and local correlations, leading toinsufficient learning of human skeleton representations. To address this issue,we propose a novel Interweaved Graph and Attention Network (IGANet) that allowsbidirectional communications between graph convolutional networks (GCNs) andattentions. Specifically, we introduce an IGA module, where attentions areprovided with local information from GCNs and GCNs are injected with globalinformation from attentions. Additionally, we design a simple yet effectiveU-shaped multi-layer perceptron (uMLP), which can capture multi-granularityinformation for body joints. Extensive experiments on two popular benchmarkdatasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate ourproposed method.The results show that IGANet achieves state-of-the-artperformance on both datasets. Code is available athttps://github.com/xiu-cs/IGANet.</description><author>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li</author><pubDate>Thu, 27 Apr 2023 10:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14045v1</guid></item><item><title>Occlusion Robust 3D Human Pose Estimation with StridedPoseGraphFormer and Data Augmentation</title><link>http://arxiv.org/abs/2304.12069v1</link><description>Occlusion is an omnipresent challenge in 3D human pose estimation (HPE). Inspite of the large amount of research dedicated to 3D HPE, only a limitednumber of studies address the problem of occlusion explicitly. To fill thisgap, we propose to combine exploitation of spatio-temporal features withsynthetic occlusion augmentation during training to deal with occlusion. Tothis end, we build a spatio-temporal 3D HPE model, StridedPoseGraphFormer basedon graph convolution and transformers, and train it using occlusionaugmentation. Unlike the existing occlusion-aware methods, that are only testedfor limited occlusion, we extensively evaluate our method for varying degreesof occlusion. We show that our proposed method compares favorably with thestate-of-the-art (SoA). Our experimental results also reveal that in theabsence of any occlusion handling mechanism, the performance of SoA 3D HPEmethods degrades significantly when they encounter occlusion.</description><author>Soubarna Banik, Patricia Gscho√ümann, Alejandro Mendoza Garcia, Alois Knoll</author><pubDate>Mon, 24 Apr 2023 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12069v1</guid></item><item><title>TAPE: Temporal Attention-based Probabilistic human pose and shape Estimation</title><link>http://arxiv.org/abs/2305.00181v1</link><description>Reconstructing 3D human pose and shape from monocular videos is awell-studied but challenging problem. Common challenges include occlusions, theinherent ambiguities in the 2D to 3D mapping and the computational complexityof video processing. Existing methods ignore the ambiguities of thereconstruction and provide a single deterministic estimate for the 3D pose. Inorder to address these issues, we present a Temporal Attention basedProbabilistic human pose and shape Estimation method (TAPE) that operates on anRGB video. More specifically, we propose to use a neural network to encodevideo frames to temporal features using an attention-based neural network.Given these features, we output a per-frame but temporally-informed probabilitydistribution for the human pose using Normalizing Flows. We show that TAPEoutperforms state-of-the-art methods in standard benchmarks and serves as aneffective video-based prior for optimization-based human pose and shapeestimation. Code is available at: https: //github.com/nikosvasilik/TAPE</description><author>Nikolaos Vasilikopoulos, Nikos Kolotouros, Aggeliki Tsoli, Antonis Argyros</author><pubDate>Sat, 29 Apr 2023 07:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00181v1</guid></item><item><title>Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training</title><link>http://arxiv.org/abs/2206.11215v4</link><description>We consider a certifiable object pose estimation problem, where -- given apartial point cloud of an object -- the goal is to not only estimate the objectpose, but also to provide a certificate of correctness for the resultingestimate. Our first contribution is a general theory of certification forend-to-end perception models. In particular, we introduce the notion of$\zeta$-correctness, which bounds the distance between an estimate and theground truth. We show that $\zeta$-correctness can be assessed by implementingtwo certificates: (i) a certificate of observable correctness, that asserts ifthe model output is consistent with the input data and prior information, (ii)a certificate of non-degeneracy, that asserts whether the input data issufficient to compute a unique estimate. Our second contribution is to applythis theory and design a new learning-based certifiable pose estimator. Wepropose C-3PO, a semantic-keypoint-based pose estimation model, augmented withthe two certificates, to solve the certifiable pose estimation problem. C-3POalso includes a keypoint corrector, implemented as a differentiableoptimization layer, that can correct large detection errors (e.g. due to thesim-to-real gap). Our third contribution is a novel self-supervised trainingapproach that uses our certificate of observable correctness to provide thesupervisory signal to C-3PO during training. In it, the model trains only onthe observably correct input-output pairs, in each training iteration. Astraining progresses, we see that the observably correct input-output pairsgrow, eventually reaching near 100% in many cases. Our experiments show that(i) standard semantic-keypoint-based methods outperform more recentalternatives, (ii) C-3PO further improves performance and significantlyoutperforms all the baselines, and (iii) C-3PO's certificates are able todiscern correct pose estimates.</description><author>Rajat Talak, Lisa Peng, Luca Carlone</author><pubDate>Fri, 28 Apr 2023 20:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11215v4</guid></item><item><title>ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with Unsupervised Implicit Pose Embedding</title><link>http://arxiv.org/abs/2304.14005v1</link><description>Although 3D-aware GANs based on neural radiance fields have achievedcompetitive performance, their applicability is still limited to objects orscenes with the ground-truths or prediction models for clearly definedcanonical camera poses. To extend the scope of applicable datasets, we proposea novel 3D-aware GAN optimization technique through contrastive learning withimplicit pose embeddings. To this end, we first revise the discriminator designand remove dependency on ground-truth camera poses. Then, to capture complexand challenging 3D scene structures more effectively, we make the discriminatorestimate a high-dimensional implicit pose embedding from a given image andperform contrastive learning on the pose embedding. The proposed approach canbe employed for the dataset, where the canonical camera pose is ill-definedbecause it does not look up or estimate camera poses. Experimental results showthat our algorithm outperforms existing methods by large margins on thedatasets with multiple object categories and inconsistent canonical cameraposes.</description><author>Mijeoong Kim, Hyunjoon Lee, Bohyung Han</author><pubDate>Thu, 27 Apr 2023 08:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14005v1</guid></item><item><title>BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos</title><link>http://arxiv.org/abs/2212.07401v2</link><description>Quantifying motion in 3D is important for studying the behavior of humans andother animals, but manual pose annotations are expensive and time-consuming toobtain. Self-supervised keypoint discovery is a promising strategy forestimating 3D poses without annotations. However, current keypoint discoveryapproaches commonly process single 2D views and do not operate in the 3D space.We propose a new method to perform self-supervised keypoint discovery in 3Dfrom multi-view videos of behaving agents, without any keypoint or bounding boxsupervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoderarchitecture with a 3D volumetric heatmap, trained to reconstructspatiotemporal differences across multiple views, in addition to joint lengthconstraints on a learned 3D skeleton of the subject. In this way, we discoverkeypoints without requiring manual supervision in videos of humans and rats,demonstrating the potential of 3D keypoint discovery for studying behavior.</description><author>Jennifer J. Sun, Lili Karashchuk, Amil Dravid, Serim Ryou, Sonia Fereidooni, John Tuthill, Aggelos Katsaggelos, Bingni W. Brunton, Georgia Gkioxari, Ann Kennedy, Yisong Yue, Pietro Perona</author><pubDate>Sun, 07 May 2023 00:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07401v2</guid></item><item><title>Contact2Grasp: 3D Grasp Synthesis via Hand-Object Contact Constraint</title><link>http://arxiv.org/abs/2210.09245v3</link><description>3D grasp synthesis generates grasping poses given an input object. Existingworks tackle the problem by learning a direct mapping from objects to thedistributions of grasping poses. However, because the physical contact issensitive to small changes in pose, the high-nonlinear mapping between 3Dobject representation to valid poses is considerably non-smooth, leading topoor generation efficiency and restricted generality. To tackle the challenge,we introduce an intermediate variable for grasp contact areas to constrain thegrasp generation; in other words, we factorize the mapping into two sequentialstages by assuming that grasping poses are fully constrained given contactmaps: 1) we first learn contact map distributions to generate the potentialcontact maps for grasps; 2) then learn a mapping from the contact maps to thegrasping poses. Further, we propose a penetration-aware optimization with thegenerated contacts as a consistency constraint for grasp refinement. Extensivevalidations on two public datasets show that our method outperformsstate-of-the-art methods regarding grasp generation on various metrics.</description><author>Haoming Li, Xinzhuo Lin, Yang Zhou, Xiang Li, Yuchi Huo, Jiming Chen, Qi Ye</author><pubDate>Sat, 06 May 2023 08:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.09245v3</guid></item><item><title>OriCon3D: Effective 3D Object Detection using Orientation and Confidence</title><link>http://arxiv.org/abs/2304.14484v1</link><description>We introduce a technique for detecting 3D objects and estimating theirposition from a single image. Our method is built on top of a similarstate-of-the-art technique [1], but with improved accuracy. The approachfollowed in this research first estimates common 3D properties of an objectusing a Deep Convolutional Neural Network (DCNN), contrary to other frameworksthat only leverage centre-point predictions. We then combine these estimateswith geometric constraints provided by a 2D bounding box to produce a complete3D bounding box. The first output of our network estimates the 3D objectorientation using a discrete-continuous loss [1]. The second output predictsthe 3D object dimensions with minimal variance. Here we also present ourextensions by augmenting light-weight feature extractors and a customizedmultibin architecture. By combining these estimates with the geometricconstraints of the 2D bounding box, we can accurately (or comparatively)determine the 3D object pose better than our baseline [1] on the KITTI 3Ddetection benchmark [2].</description><author>Dhyey Manish Rajani, Rahul Kashyap Swayampakula, Surya Pratap Singh</author><pubDate>Thu, 27 Apr 2023 20:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14484v1</guid></item><item><title>End-to-end Weakly-supervised Single-stage Multiple 3D Hand Mesh Reconstruction from a Single RGB Image</title><link>http://arxiv.org/abs/2204.08154v3</link><description>In this paper, we consider the challenging task of simultaneously locatingand recovering multiple hands from a single 2D image. Previous studies eitherfocus on single hand reconstruction or solve this problem in a multi-stage way.Moreover, the conventional two-stage pipeline firstly detects hand areas, andthen estimates 3D hand pose from each cropped patch. To reduce thecomputational redundancy in preprocessing and feature extraction, for the firsttime, we propose a concise but efficient single-stage pipeline for multi-handreconstruction. Specifically, we design a multi-head auto-encoder structure,where each head network shares the same feature map and outputs the handcenter, pose and texture, respectively. Besides, we adopt a weakly-supervisedscheme to alleviate the burden of expensive 3D real-world data annotations. Tothis end, we propose a series of losses optimized by a stage-wise trainingscheme, where a multi-hand dataset with 2D annotations is generated based onthe publicly available single hand datasets. In order to further improve theaccuracy of the weakly supervised model, we adopt several feature consistencyconstraints in both single and multiple hand settings. Specifically, thekeypoints of each hand estimated from local features should be consistent withthe re-projected points predicted from global features. Extensive experimentson public benchmarks including FreiHAND, HO3D, InterHand2.6M and RHDdemonstrate that our method outperforms the state-of-the-art model-basedmethods in both weakly-supervised and fully-supervised manners. The code andmodels are available at {https://github.com/zijinxuxu/SMHR}.</description><author>Jinwei Ren, Jianke Zhu, Jialiang Zhang</author><pubDate>Sat, 06 May 2023 09:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.08154v3</guid></item><item><title>Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications</title><link>http://arxiv.org/abs/2302.05763v2</link><description>Human-robot interaction (HRI) research is progressively addressingmulti-party scenarios, where a robot interacts with more than one human user atthe same time. Conversely, research is still at an early stage for human-robotcollaboration. The use of machine learning techniques to handle such type ofcollaboration requires data that are less feasible to produce than in a typicalHRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRCapplications. Based upon these concepts, this study also proposes analternative way of gathering data regarding multi-user activity, by collectingdata related to single users and merging them in post-processing, to reduce theeffort involved in producing recordings of pair settings. To validate thisstatement, 3D skeleton poses of activity of single users were collected andmerged in pairs. After this, such datapoints were used to separately train along short-term memory (LSTM) network and a variational autoencoder (VAE)composed of spatio-temporal graph convolutional networks (STGCN) to recognisethe joint activities of the pairs of people. The results showed that it ispossible to make use of data collected in this way for pair HRC settings andget similar performances compared to using training data regarding groups ofusers recorded under the same settings, relieving from the technicaldifficulties involved in producing these data. The related code and collected data are publicly available.</description><author>Francesco Semeraro, Jon Carberry, Angelo Cangelosi</author><pubDate>Fri, 05 May 2023 16:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05763v2</guid></item><item><title>RelPose++: Recovering 6D Poses from Sparse-view Observations</title><link>http://arxiv.org/abs/2305.04926v1</link><description>We address the task of estimating 6D camera poses from sparse-view image sets(2-8 images). This task is a vital pre-processing stage for nearly allcontemporary (neural) reconstruction algorithms but remains challenging givensparse views, especially for objects with visual symmetries and texture-lesssurfaces. We build on the recent RelPose framework which learns a network thatinfers distributions over relative rotations over image pairs. We extend thisapproach in two key ways; first, we use attentional transformer layers toprocess multiple images jointly, since additional views of an object mayresolve ambiguous symmetries in any given image pair (such as the handle of amug that becomes visible in a third view). Second, we augment this network toalso report camera translations by defining an appropriate coordinate systemthat decouples the ambiguity in rotation estimation from translationprediction. Our final system results in large improvements in 6D poseprediction over prior art on both seen and unseen object categories and alsoenables pose estimation and 3D reconstruction for in-the-wild objects.</description><author>Amy Lin, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Mon, 08 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04926v1</guid></item><item><title>gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction</title><link>http://arxiv.org/abs/2304.11970v1</link><description>Signed distance functions (SDFs) is an attractive framework that has recentlyshown promising results for 3D shape reconstruction from images. SDFsseamlessly generalize to different shape resolutions and topologies but lackexplicit modelling of the underlying 3D geometry. In this work, we exploit thehand structure and use it as guidance for SDF-based shape reconstruction. Inparticular, we address reconstruction of hands and manipulated objects frommonocular RGB images. To this end, we estimate poses of hands and objects anduse them to guide 3D reconstruction. More specifically, we predict kinematicchains of pose transformations and align SDFs with highly-articulated handposes. We improve the visual features of 3D points with geometry alignment andfurther leverage temporal information to enhance the robustness to occlusionand motion blurs. We conduct extensive experiments on the challenging ObMan andDexYCB benchmarks and demonstrate significant improvements of the proposedmethod over the state of the art.</description><author>Zerui Chen, Shizhe Chen, Cordelia Schmid, Ivan Laptev</author><pubDate>Mon, 24 Apr 2023 11:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11970v1</guid></item><item><title>CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction</title><link>http://arxiv.org/abs/2304.14633v1</link><description>Recent advances in neural reconstruction using posed image sequences havemade remarkable progress. However, due to the lack of depth information,existing volumetric-based techniques simply duplicate 2D image features of theobject surface along the entire camera ray. We contend this duplicationintroduces noise in empty and occluded spaces, posing challenges for producinghigh-quality 3D geometry. Drawing inspiration from traditional multi-viewstereo methods, we propose an end-to-end 3D neural reconstruction frameworkCVRecon, designed to exploit the rich geometric embedding in the cost volumesto facilitate 3D geometric feature learning. Furthermore, we presentRay-contextual Compensated Cost Volume (RCCV), a novel 3D geometric featurerepresentation that encodes view-dependent information with improved integrityand robustness. Through comprehensive experiments, we demonstrate that ourapproach significantly improves the reconstruction quality in various metricsand recovers clear fine details of the 3D geometries. Our extensive ablationstudies provide insights into the development of effective 3D geometric featurelearning schemes. Project page: https://cvrecon.ziyue.cool/</description><author>Ziyue Feng, Leon Yang, Pengsheng Guo, Bing Li</author><pubDate>Fri, 28 Apr 2023 06:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14633v1</guid></item><item><title>ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation</title><link>http://arxiv.org/abs/2305.01618v1</link><description>We propose a new dataset and a novel approach to learning hand-objectinteraction priors for hand and articulated object pose estimation. We firstcollect a dataset using visual teleoperation, where the human operator candirectly play within a physical simulator to manipulate the articulatedobjects. We record the data and obtain free and accurate annotations on objectposes and contact information from the simulator. Our system only requires aniPhone to record human hand motion, which can be easily scaled up and largelylower the costs of data and annotation collection. With this data, we learn 3Dinteraction priors including a discriminator (in a GAN) capturing thedistribution of how object parts are arranged, and a diffusion model whichgenerates the contact regions on articulated objects, guiding the hand poseestimation. Such structural and contact priors can easily transfer toreal-world data with barely any domain gap. By using our data and learnedpriors, our method significantly improves the performance on joint hand andarticulated object poses estimation over the existing state-of-the-art methods.The project is available at https://zehaozhu.github.io/ContactArt/ .</description><author>Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang</author><pubDate>Tue, 02 May 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01618v1</guid></item><item><title>Depth-based 6DoF Object Pose Estimation using Swin Transformer</title><link>http://arxiv.org/abs/2303.02133v2</link><description>Accurately estimating the 6D pose of objects is crucial for manyapplications, such as robotic grasping, autonomous driving, and augmentedreality. However, this task becomes more challenging in poor lightingconditions or when dealing with textureless objects. To address this issue,depth images are becoming an increasingly popular choice due to theirinvariance to a scene's appearance and the implicit incorporation of essentialgeometric characteristics. However, fully leveraging depth information toimprove the performance of pose estimation remains a difficult andunder-investigated problem. To tackle this challenge, we propose a novelframework called SwinDePose, that uses only geometric information from depthimages to achieve accurate 6D pose estimation. SwinDePose first calculates theangles between each normal vector defined in a depth image and the threecoordinate axes in the camera coordinate system. The resulting angles are thenformed into an image, which is encoded using Swin Transformer. Additionally, weapply RandLA-Net to learn the representations from point clouds. The resultingimage and point clouds embeddings are concatenated and fed into a semanticsegmentation module and a 3D keypoints localization module. Finally, weestimate 6D poses using a least-square fitting approach based on the targetobject's predicted semantic mask and 3D keypoints. In experiments on theLineMod and Occlusion LineMod datasets, SwinDePose outperforms existingstate-of-the-art methods for 6D object pose estimation using depth images. Thisdemonstrates the effectiveness of our approach and highlights its potential forimproving performance in real-world scenarios. Our code is athttps://github.com/zhujunli1993/SwinDePose.</description><author>Zhujun Li, Ioannis Stamos</author><pubDate>Thu, 27 Apr 2023 19:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02133v2</guid></item><item><title>Towards Realistic Generative 3D Face Models</title><link>http://arxiv.org/abs/2304.12483v1</link><description>In recent years, there has been significant progress in 2D generative facemodels fueled by applications such as animation, synthetic data generation, anddigital avatars. However, due to the absence of 3D information, these 2D modelsoften struggle to accurately disentangle facial attributes like pose,expression, and illumination, limiting their editing capabilities. To addressthis limitation, this paper proposes a 3D controllable generative face model toproduce high-quality albedo and precise 3D shape leveraging existing 2Dgenerative models. By combining 2D face generative models with semantic facemanipulation, this method enables editing of detailed 3D rendered faces. Theproposed framework utilizes an alternating descent optimization approach overshape and albedo. Differentiable rendering is used to train high-quality shapesand albedo without 3D supervision. Moreover, this approach outperforms thestate-of-the-art (SOTA) methods in the well-known NoW benchmark for shapereconstruction. It also outperforms the SOTA reconstruction models inrecovering rendered faces' identities across novel poses by an average of 10%.Additionally, the paper demonstrates direct control of expressions in 3D facesby exploiting latent space leading to text-based editing of 3D faces.</description><author>Aashish Rai, Hiresh Gupta, Ayush Pandey, Francisco Vicente Carrasco, Shingo Jason Takagi, Amaury Aubel, Daeil Kim, Aayush Prakash, Fernando de la Torre</author><pubDate>Mon, 24 Apr 2023 23:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12483v1</guid></item><item><title>HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</title><link>http://arxiv.org/abs/2212.10428v4</link><description>Estimating the 6D pose of objects is a major 3D computer vision problem.Since the promising outcomes from instance-level approaches, research headsalso move towards category-level pose estimation for more practical applicationscenarios. However, unlike well-established instance-level pose datasets,available category-level datasets lack annotation quality and provided posequantity. We propose the new category-level 6D pose dataset HouseCat6Dfeaturing 1) Multi-modality of Polarimetric RGB and Depth (RGBD+P), 2) Highlydiverse 194 objects of 10 household object categories including 2photometrically challenging categories, 3) High-quality pose annotation with anerror range of only 1.35 mm to 1.74 mm, 4) 41 large-scale scenes with extensiveviewpoint coverage and occlusions, 5) Checkerboard-free environment throughoutthe entire scene, and 6) Additionally annotated dense 6D parallel-jaw grasps.Furthermore, we also provide benchmark results of state-of-the-artcategory-level pose estimation networks.</description><author>HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp, Guangyao Zhai, Hannah Schieber, Giulia Rizzoli, Pengyuan Wang, Hongcheng Zhao, Lorenzo Garattoni, Sven Meier, Daniel Roth, Nassir Navab, Benjamin Busam</author><pubDate>Wed, 26 Apr 2023 11:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10428v4</guid></item><item><title>Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN</title><link>http://arxiv.org/abs/2212.09098v4</link><description>Fine-grained semantic segmentation of a person's face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.</description><author>Lei Li, Tianfang Zhang, Zhongfeng Kang, Xikun Jiang</author><pubDate>Sat, 06 May 2023 19:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09098v4</guid></item><item><title>Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN</title><link>http://arxiv.org/abs/2212.09098v3</link><description>Fine-grained semantic segmentation of a person's face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.</description><author>Lei Li, Tianfang Zhang, Zhongfeng Kang, Xikun Jiang</author><pubDate>Sun, 30 Apr 2023 03:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09098v3</guid></item><item><title>Unsupervised Style-based Explicit 3D Face Reconstruction from Single Image</title><link>http://arxiv.org/abs/2304.12455v1</link><description>Inferring 3D object structures from a single image is an ill-posed task dueto depth ambiguity and occlusion. Typical resolutions in the literature includeleveraging 2D or 3D ground truth for supervised learning, as well as imposinghand-crafted symmetry priors or using an implicit representation to hallucinatenovel viewpoints for unsupervised methods. In this work, we propose a generaladversarial learning framework for solving Unsupervised 2D to Explicit 3D StyleTransfer (UE3DST). Specifically, we merge two architectures: the unsupervisedexplicit 3D reconstruction network of Wu et al.\ and the Generative AdversarialNetwork (GAN) named StarGAN-v2. We experiment across three facial datasets(Basel Face Model, 3DFAW and CelebA-HQ) and show that our solution is able tooutperform well established solutions such as DepthNet in 3D reconstruction andPix2NeRF in conditional style transfer, while we also justify the individualcontributions of our model components via ablation. In contrast to theaforementioned baselines, our scheme produces features for explicit 3Drendering, which can be manipulated and utilized in downstream tasks.</description><author>Heng Yu, Zoltan A. Milacski, Laszlo A. Jeni</author><pubDate>Mon, 24 Apr 2023 22:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12455v1</guid></item><item><title>Learning Hand-Held Object Reconstruction from In-The-Wild Videos</title><link>http://arxiv.org/abs/2305.03036v1</link><description>Prior works for reconstructing hand-held objects from a single image rely ondirect 3D shape supervision which is challenging to gather in real world atscale. Consequently, these approaches do not generalize well when presentedwith novel objects in in-the-wild settings. While 3D supervision is a majorbottleneck, there is an abundance of in-the-wild raw video data showinghand-object interactions. In this paper, we automatically extract 3Dsupervision (via multiview 2D supervision) from such raw video data to scale upthe learning of models for hand-held object reconstruction. This requirestackling two key challenges: unknown camera pose and occlusion. For the former,we use hand pose (predicted from existing techniques, e.g. FrankMocap) as aproxy for object pose. For the latter, we learn data-driven 3D shape priorsusing synthetic objects from the ObMan dataset. We use these indirect 3D cuesto train occupancy networks that predict the 3D shape of objects from a singleRGB image. Our experiments on the MOW and HO3D datasets show the effectivenessof these supervisory signals at predicting the 3D shape for real-worldhand-held objects without any direct real-world 3D supervision.</description><author>Aditya Prakash, Matthew Chang, Matthew Jin, Saurabh Gupta</author><pubDate>Thu, 04 May 2023 18:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03036v1</guid></item><item><title>Towards Realistic 3D Embedding via View Alignment</title><link>http://arxiv.org/abs/2007.07066v3</link><description>Recent advances in generative adversarial networks (GANs) have achieved greatsuccess in automated image composition that generates new images by embeddinginterested foreground objects into background images automatically. On theother hand, most existing works deal with foreground objects in two-dimensional(2D) images though foreground objects in three-dimensional (3D) models are moreflexible with 360-degree view freedom. This paper presents an innovative ViewAlignment GAN (VA-GAN) that composes new images by embedding 3D models into 2Dbackground images realistically and automatically. VA-GAN consists of a texturegenerator and a differential discriminator that are inter-connected andend-to-end trainable. The differential discriminator guides to learn geometrictransformation from background images so that the composed 3D models can bealigned with the background images with realistic poses and views. The texturegenerator adopts a novel view encoding mechanism for generating accurate objecttextures for the 3D models under the estimated views. Extensive experimentsover two synthesis tasks (car synthesis with KITTI and pedestrian synthesiswith Cityscapes) show that VA-GAN achieves high-fidelity compositionqualitatively and quantitatively as compared with state-of-the-art generationmethods.</description><author>Changgong Zhang, Fangneng Zhan, Shijian Lu, Feiying Ma, Xuansong Xie</author><pubDate>Mon, 24 Apr 2023 13:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.07066v3</guid></item><item><title>PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling</title><link>http://arxiv.org/abs/2304.13006v1</link><description>Creating pose-driven human avatars is about modeling the mapping from thelow-frequency driving pose to high-frequency dynamic human appearances, so aneffective pose encoding method that can encode high-fidelity human details isessential to human avatar modeling.To this end, we present PoseVocab, a novelpose encoding method that encourages the network to discover the optimal poseembeddings for learning the dynamic human appearance. Given multi-view RGBvideos of a character, PoseVocab constructs key poses and latent embeddingsbased on the training poses. To achieve pose generalization and temporalconsistency, we sample key rotations in $so(3)$ of each joint rather than theglobal pose vectors, and assign a pose embedding to each sampled key rotation.These joint-structured pose embeddings not only encode the dynamic appearancesunder different key poses, but also factorize the global pose embedding intojoint-structured ones to better learn the appearance variation related to themotion of each joint. To improve the representation ability of the poseembedding while maintaining memory efficiency, we introduce feature lines, acompact yet effective 3D representation, to model more fine-grained details ofhuman appearances. Furthermore, given a query pose and a spatial position, ahierarchical query strategy is introduced to interpolate pose embeddings andacquire the conditional pose feature for dynamic human synthesis. Overall,PoseVocab effectively encodes the dynamic details of human appearance andenables realistic and generalized animation under novel poses. Experiments showthat our method outperforms other state-of-the-art baselines both qualitativelyand quantitatively in terms of synthesis quality. Code is available athttps://github.com/lizhe00/PoseVocab.</description><author>Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, Yebin Liu</author><pubDate>Tue, 25 Apr 2023 18:25:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13006v1</guid></item><item><title>Animatable Implicit Neural Representations for Creating Realistic Avatars from Videos</title><link>http://arxiv.org/abs/2203.08133v4</link><description>This paper addresses the challenge of reconstructing an animatable humanmodel from a multi-view video. Some recent works have proposed to decompose anon-rigidly deforming scene into a canonical neural radiance field and a set ofdeformation fields that map observation-space points to the canonical space,thereby enabling them to learn the dynamic scene from images. However, theyrepresent the deformation field as translational vector field or SE(3) field,which makes the optimization highly under-constrained. Moreover, theserepresentations cannot be explicitly controlled by input motions. Instead, weintroduce a pose-driven deformation field based on the linear blend skinningalgorithm, which combines the blend weight field and the 3D human skeleton toproduce observation-to-canonical correspondences. Since 3D human skeletons aremore observable, they can regularize the learning of the deformation field.Moreover, the pose-driven deformation field can be controlled by input skeletalmotions to generate new deformation fields to animate the canonical humanmodel. Experiments show that our approach significantly outperforms recenthuman modeling methods. The code is available athttps://zju3dv.github.io/animatable_nerf/.</description><author>Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, Xiaowei Zhou</author><pubDate>Thu, 04 May 2023 08:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.08133v4</guid></item><item><title>AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions</title><link>http://arxiv.org/abs/2112.00246v6</link><description>Perceiving and interacting with 3D articulated objects, such as cabinets,doors, and faucets, pose particular challenges for future home-assistant robotsperforming daily tasks in human environments. Besides parsing the articulatedparts and joint parameters, researchers recently advocate learning manipulationaffordance over the input shape geometry which is more task-aware andgeometrically fine-grained. However, taking only passive observations asinputs, these methods ignore many hidden but important kinematic constraints(e.g., joint location and limits) and dynamic factors (e.g., joint friction andrestitution), therefore losing significant accuracy for test cases with suchuncertainties. In this paper, we propose a novel framework, named AdaAfford,that learns to perform very few test-time interactions for quickly adapting theaffordance priors to more accurate instance-specific posteriors. We conductlarge-scale experiments using the PartNet-Mobility dataset and prove that oursystem performs better than baselines.</description><author>Yian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas Guibas, Hao Dong</author><pubDate>Thu, 04 May 2023 15:47:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.00246v6</guid></item><item><title>Privacy-Preserving Representations are not Enough -- Recovering Scene Content from Camera Poses</title><link>http://arxiv.org/abs/2305.04603v1</link><description>Visual localization is the task of estimating the camera pose from which agiven image was taken and is central to several 3D computer visionapplications. With the rapid growth in the popularity of AR/VR/MR devices andcloud-based applications, privacy issues are becoming a very important aspectof the localization process. Existing work on privacy-preserving localizationaims to defend against an attacker who has access to a cloud-based service. Inthis paper, we show that an attacker can learn about details of a scene withoutany access by simply querying a localization service. The attack is based onthe observation that modern visual localization algorithms are robust tovariations in appearance and geometry. While this is in general a desiredproperty, it also leads to algorithms localizing objects that are similarenough to those present in a scene. An attacker can thus query a server with alarge enough set of images of objects, \eg, obtained from the Internet, andsome of them will be localized. The attacker can thus learn about objectplacements from the camera poses returned by the service (which is the minimalinformation returned by such a service). In this paper, we develop aproof-of-concept version of this attack and demonstrate its practicalfeasibility. The attack does not place any requirements on the localizationalgorithm used, and thus also applies to privacy-preserving representations.Current work on privacy-preserving representations alone is thus insufficient.</description><author>Kunal Chelani, Torsten Sattler, Fredrik Kahl, Zuzana Kukelova</author><pubDate>Mon, 08 May 2023 11:25:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04603v1</guid></item><item><title>StyleLipSync: Style-based Personalized Lip-sync Video Generation</title><link>http://arxiv.org/abs/2305.00521v1</link><description>In this paper, we present StyleLipSync, a style-based personalized lip-syncvideo generative model that can generate identity-agnostic lip-synchronizingvideo from arbitrary audio. To generate a video of arbitrary identities, weleverage expressive lip prior from the semantically rich latent space of apre-trained StyleGAN, where we can also design a video consistency with alinear transformation. In contrast to the previous lip-sync methods, weintroduce pose-aware masking that dynamically locates the mask to improve thenaturalness over frames by utilizing a 3D parametric mesh predictor frame byframe. Moreover, we propose a few-shot lip-sync adaptation method for anarbitrary person by introducing a sync regularizer that preserves lips-syncgeneralization while enhancing the person-specific visual information.Extensive experiments demonstrate that our model can generate accurate lip-syncvideos even with the zero-shot setting and enhance characteristics of an unseenface using a few seconds of target video through the proposed adaptationmethod. Please refer to our project page.</description><author>Taekyung Ki, Dongchan Min</author><pubDate>Sun, 30 Apr 2023 17:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00521v1</guid></item><item><title>Combining HoloLens with Instant-NeRFs: Advanced Real-Time 3D Mobile Mapping</title><link>http://arxiv.org/abs/2304.14301v2</link><description>This work represents a large step into modern ways of fast 3D reconstructionbased on RGB camera images. Utilizing a Microsoft HoloLens 2 as a multisensorplatform that includes an RGB camera and an inertial measurement unit forSLAM-based camera-pose determination, we train a Neural Radiance Field (NeRF)as a neural scene representation in real-time with the acquired data from theHoloLens. The HoloLens is connected via Wifi to a high-performance PC that isresponsible for the training and 3D reconstruction. After the data stream ends,the training is stopped and the 3D reconstruction is initiated, which extractsa point cloud of the scene. With our specialized inference algorithm, fivemillion scene points can be extracted within 1 second. In addition, the pointcloud also includes radiometry per point. Our method of 3D reconstructionoutperforms grid point sampling with NeRFs by multiple orders of magnitude andcan be regarded as a complete real-time 3D reconstruction method in a mobilemapping setup.</description><author>Dennis Haitz, Boris Jutzi, Markus Ulrich, Miriam Jaeger, Patrick Huebner</author><pubDate>Wed, 03 May 2023 12:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14301v2</guid></item><item><title>Towards Accurate Human Motion Prediction via Iterative Refinement</title><link>http://arxiv.org/abs/2305.04443v1</link><description>Human motion prediction aims to forecast an upcoming pose sequence given apast human motion trajectory. To address the problem, in this work we proposeFreqMRN, a human motion prediction framework that takes into account both thekinematic structure of the human body and the temporal smoothness nature ofmotion. Specifically, FreqMRN first generates a fixed-size motion historysummary using a motion attention module, which helps avoid inaccurate motionpredictions due to excessively long motion inputs. Then, supervised by theproposed spatial-temporal-aware, velocity-aware and global-smoothness-awarelosses, FreqMRN iteratively refines the predicted motion though the proposedmotion refinement module, which converts motion representations back and forthbetween pose space and frequency space. We evaluate FreqMRN on several standardbenchmark datasets, including Human3.6M, AMASS and 3DPW. Experimental resultsdemonstrate that FreqMRN outperforms previous methods by large margins for bothshort-term and long-term predictions, while demonstrating superior robustness.</description><author>Jiarui Sun, Girish Chowdhary</author><pubDate>Mon, 08 May 2023 04:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04443v1</guid></item><item><title>Object Semantics Give Us the Depth We Need: Multi-task Approach to Aerial Depth Completion</title><link>http://arxiv.org/abs/2304.12542v1</link><description>Depth completion and object detection are two crucial tasks often used foraerial 3D mapping, path planning, and collision avoidance of Uncrewed AerialVehicles (UAVs). Common solutions include using measurements from a LiDARsensor; however, the generated point cloud is often sparse and irregular andlimits the system's capabilities in 3D rendering and safety-criticaldecision-making. To mitigate this challenge, information from other sensors onthe UAV (viz., a camera used for object detection) is utilized to help thedepth completion process generate denser 3D models. Performing both aerialdepth completion and object detection tasks while fusing the data from the twosensors poses a challenge to resource efficiency. We address this challenge byproposing a novel approach to jointly execute the two tasks in a single pass.The proposed method is based on an encoder-focused multi-task learning modelthat exposes the two tasks to jointly learned features. We demonstrate howsemantic expectations of the objects in the scene learned by the objectdetection pathway can boost the performance of the depth completion pathwaywhile placing the missing depth values. Experimental results show that theproposed multi-task network outperforms its single-task counterpart,particularly when exposed to defective inputs.</description><author>Sara Hatami Gazani, Fardad Dadboud, Miodrag Bolic, Iraj Mantegh, Homayoun Najjaran</author><pubDate>Tue, 25 Apr 2023 04:21:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12542v1</guid></item><item><title>BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering</title><link>http://arxiv.org/abs/2112.05504v4</link><description>Neural radiance fields (NeRF) has achieved outstanding performance inmodeling 3D objects and controlled scenes, usually under a single scale. Inthis work, we focus on multi-scale cases where large changes in imagery areobserved at drastically different scales. This scenario vastly exists inreal-world 3D environments, such as city scenes, with views ranging fromsatellite level that captures the overview of a city, to ground level imageryshowing complex details of an architecture; and can also be commonly identifiedin landscape and delicate minecraft 3D models. The wide span of viewingpositions within these scenes yields multi-scale renderings with very differentlevels of detail, which poses great challenges to neural radiance field andbiases it towards compromised results. To address these issues, we introduceBungeeNeRF, a progressive neural radiance field that achieves level-of-detailrendering across drastically varied scales. Starting from fitting distant viewswith a shallow base block, as training progresses, new blocks are appended toaccommodate the emerging details in the increasingly closer views. The strategyprogressively activates high-frequency channels in NeRF's positional encodinginputs and successively unfolds more complex details as the training proceeds.We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scalescenes with drastically varying views on multiple data sources (city models,synthetic, and drone captured data) and its support for high-quality renderingin different levels of detail.</description><author>Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, Dahua Lin</author><pubDate>Tue, 09 May 2023 06:48:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.05504v4</guid></item><item><title>A Cooperative Perception System Robust to Localization Errors</title><link>http://arxiv.org/abs/2210.06289v2</link><description>Cooperative perception is challenging for safety-critical autonomous drivingapplications.The errors in the shared position and pose cause an inaccuraterelative transform estimation and disrupt the robust mapping of the Egovehicle. We propose a distributed object-level cooperative perception systemcalled OptiMatch, in which the detected 3D bounding boxes and local stateinformation are shared between the connected vehicles. To correct the noisyrelative transform, the local measurements of both connected vehicles (boundingboxes) are utilized, and an optimal transport theory-based algorithm isdeveloped to filter out those objects jointly detected by the vehicles alongwith their correspondence, constructing an associated co-visible set. Acorrection transform is estimated from the matched object pairs and furtherapplied to the noisy relative transform, followed by global fusion and dynamicmapping. Experiment results show that robust performance is achieved fordifferent levels of location and heading errors, and the proposed frameworkoutperforms the state-of-the-art benchmark fusion schemes, including early,late, and intermediate fusion, on average precision by a large margin whenlocation and/or heading errors occur.</description><author>Zhiying Song, Fuxi Wen, Hailiang Zhang, Jun Li</author><pubDate>Wed, 26 Apr 2023 01:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06289v2</guid></item><item><title>Robust Pose Transfer with Dynamic Details using Neural Video Rendering</title><link>http://arxiv.org/abs/2106.14132v3</link><description>Pose transfer of human videos aims to generate a high fidelity video of atarget person imitating actions of a source person. A few studies have madegreat progress either through image translation with deep latent features orneural rendering with explicit 3D features. However, both of them rely on largeamounts of training data to generate realistic results, and the performancedegrades on more accessible internet videos due to insufficient trainingframes. In this paper, we demonstrate that the dynamic details can be preservedeven trained from short monocular videos. Overall, we propose a neural videorendering framework coupled with an image-translation-based dynamic detailsgeneration network (D2G-Net), which fully utilizes both the stability ofexplicit 3D features and the capacity of learning components. To be specific, anovel texture representation is presented to encode both the static andpose-varying appearance characteristics, which is then mapped to the imagespace and rendered as a detail-rich frame in the neural rendering stage.Moreover, we introduce a concise temporal loss in the training stage tosuppress the detail flickering that is made more visible due to high-qualitydynamic details generated by our method. Through extensive comparisons, wedemonstrate that our neural human video renderer is capable of achieving bothclearer dynamic details and more robust performance even on accessible shortvideos with only 2k - 4k frames.</description><author>Yang-tian Sun, Hao-zhi Huang, Xuan Wang, Yu-kun Lai, Wei Liu, Lin Gao</author><pubDate>Mon, 08 May 2023 15:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.14132v3</guid></item><item><title>Domain independent post-processing with graph U-nets: Applications to Electrical Impedance Tomographic Imaging</title><link>http://arxiv.org/abs/2305.05020v1</link><description>Reconstruction of tomographic images from boundary measurements requiresflexibility with respect to target domains. For instance, when the systemequations are modeled by partial differential equations the reconstruction isusually done on finite element (FE) meshes, allowing for flexible geometries.Thus, any processing of the obtained reconstructions should be ideally done onthe FE mesh as well. For this purpose, we extend the hugely successful U-Netarchitecture that is limited to rectangular pixel or voxel domains to anequivalent that works flexibly on FE meshes. To achieve this, the FE mesh isconverted into a graph and we formulate a graph U-Net with a new clusterpooling and unpooling on the graph that mimics the classic neighborhood basedmax-pooling. We demonstrate effectiveness and flexibility of the graph U-Netfor improving reconstructions from electrical impedance tomographic (EIT)measurements, a nonlinear and highly ill-posed inverse problem. The performanceis evaluated for simulated data and from three measurement devices withdifferent measurement geometries and instrumentations. We successfully showthat such networks can be trained with a simple two-dimensional simulatedtraining set and generalize to very different domains, including measurementsfrom a three-dimensional device and subsequent 3D reconstructions.</description><author>William Herzberg, Andreas Hauptmann, Sarah J. Hamilton</author><pubDate>Mon, 08 May 2023 20:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05020v1</guid></item><item><title>Compositional 3D Human-Object Neural Animation</title><link>http://arxiv.org/abs/2304.14070v1</link><description>Human-object interactions (HOIs) are crucial for human-centric sceneunderstanding applications such as human-centric visual generation, AR/VR, androbotics. Since existing methods mainly explore capturing HOIs, rendering HOIremains less investigated. In this paper, we address this challenge in HOIanimation from a compositional perspective, i.e., animating novel HOIsincluding novel interaction, novel human and/or novel object driven by a novelpose sequence. Specifically, we adopt neural human-object deformation to modeland render HOI dynamics based on implicit neural representations. To enable theinteraction pose transferring among different persons and objects, we thendevise a new compositional conditional neural radiance field (or CC-NeRF),which decomposes the interdependence between human and object using latentcodes to enable compositionally animation control of novel HOIs. Experimentsshow that the proposed method can generalize well to various novel HOIanimation settings. Our project page is https://zhihou7.github.io/CHONA/</description><author>Zhi Hou, Baosheng Yu, Dacheng Tao</author><pubDate>Thu, 27 Apr 2023 11:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14070v1</guid></item><item><title>Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution</title><link>http://arxiv.org/abs/2305.03216v1</link><description>We present a neural network-based simulation super-resolution framework thatcan efficiently and realistically enhance a facial performance produced by alow-cost, realtime physics-based simulation to a level of detail that closelyapproximates that of a reference-quality off-line simulator with much higherresolution (26x element count in our examples) and accurate physical modeling.Our approach is rooted in our ability to construct - via simulation - atraining set of paired frames, from the low- and high-resolution simulatorsrespectively, that are in semantic correspondence with each other. We use faceanimation as an exemplar of such a simulation domain, where creating thissemantic congruence is achieved by simply dialing in the same muscle actuationcontrols and skeletal pose in the two simulators. Our proposed neural networksuper-resolution framework generalizes from this training set to unseenexpressions, compensates for modeling discrepancies between the two simulationsdue to limited resolution or cost-cutting approximations in the real-timevariant, and does not require any semantic descriptors or parameters to beprovided as input, other than the result of the real-time simulation. Weevaluate the efficacy of our pipeline on a variety of expressive performancesand provide comparisons and ablation experiments for plausible variations andalternatives to our proposed scheme.</description><author>Hyojoon Park, Sangeetha Grama Srinivasan, Matthew Cong, Doyub Kim, Byungsoo Kim, Jonathan Swartz, Ken Museth, Eftychios Sifakis</author><pubDate>Fri, 05 May 2023 01:09:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03216v1</guid></item><item><title>Multimodal-driven Talking Face Generation via a Unified Diffusion-based Generator</title><link>http://arxiv.org/abs/2305.02594v2</link><description>Multimodal-driven talking face generation refers to animating a portrait withthe given pose, expression, and gaze transferred from the driving image andvideo, or estimated from the text and audio. However, existing methods ignorethe potential of text modal, and their generators mainly follow thesource-oriented feature rearrange paradigm coupled with unstable GANframeworks. In this work, we first represent the emotion in the text prompt,which could inherit rich semantics from the CLIP, allowing flexible andgeneralized emotion control. We further reorganize these tasks as thetarget-oriented texture transfer and adopt the Diffusion Models. Morespecifically, given a textured face as the source and the rendered faceprojected from the desired 3DMM coefficients as the target, our proposedTexture-Geometry-aware Diffusion Model decomposes the complex transfer probleminto multi-conditional denoising process, where a Texture Attention-basedmodule accurately models the correspondences between appearance and geometrycues contained in source and target conditions, and incorporate extra implicitinformation for high-fidelity talking face generation. Additionally, TGDM canbe gracefully tailored for face swapping. We derive a novel paradigm free ofunstable seesaw-style optimization, resulting in simple, stable, and effectivetraining and inference schemes. Extensive experiments demonstrate thesuperiority of our method.</description><author>Chao Xu, Shaoting Zhu, Junwei Zhu, Tianxin Huang, Jiangning Zhang, Ying Tai, Yong Liu</author><pubDate>Tue, 09 May 2023 13:01:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02594v2</guid></item><item><title>Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model</title><link>http://arxiv.org/abs/2305.02594v1</link><description>Multimodal-driven talking face generation refers to animating a portrait withthe given pose, expression, and gaze transferred from the driving image andvideo, or estimated from the text and audio. However, existing methods ignorethe potential of text modal, and their generators mainly follow thesource-oriented feature rearrange paradigm coupled with unstable GANframeworks. In this work, we first represent the emotion in the text prompt,which could inherit rich semantics from the CLIP, allowing flexible andgeneralized emotion control. We further reorganize these tasks as thetarget-oriented texture transfer and adopt the Diffusion Models. Morespecifically, given a textured face as the source and the rendered faceprojected from the desired 3DMM coefficients as the target, our proposedTexture-Geometry-aware Diffusion Model decomposes the complex transfer probleminto multi-conditional denoising process, where a Texture Attention-basedmodule accurately models the correspondences between appearance and geometrycues contained in source and target conditions, and incorporate extra implicitinformation for high-fidelity talking face generation. Additionally, TGDM canbe gracefully tailored for face swapping. We derive a novel paradigm free ofunstable seesaw-style optimization, resulting in simple, stable, and effectivetraining and inference schemes. Extensive experiments demonstrate thesuperiority of our method.</description><author>Chao Xu, Shaoting Zhu, Junwei Zhu, Tianxin Huang, Jiangning Zhang, Ying Tai, Yong Liu</author><pubDate>Thu, 04 May 2023 08:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02594v1</guid></item></channel></rss>