<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 21 Jun 2023 06:00:48 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Neural Voting Field for Camera-Space 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2305.04328v1</link><description>We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.</description><author>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</author><pubDate>Sun, 07 May 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04328v1</guid></item><item><title>Dual-Side Feature Fusion 3D Pose Transfer</title><link>http://arxiv.org/abs/2305.14951v1</link><description>3D pose transfer solves the problem of additional input and correspondence oftraditional deformation transfer, only the source and target meshes need to beinput, and the pose of the source mesh can be transferred to the target mesh.Some lightweight methods proposed in recent years consume less memory but causespikes and distortions for some unseen poses, while others are costly intraining due to the inclusion of large matrix multiplication and adversarialnetworks. In addition, the meshes with different numbers of vertices alsoincrease the difficulty of pose transfer. In this work, we propose a Dual-SideFeature Fusion Pose Transfer Network to improve the pose transfer accuracy ofthe lightweight method. Our method takes the pose features as one of the sideinputs to the decoding network and fuses them into the target mesh layer bylayer at multiple scales. Our proposed Feature Fusion Adaptive InstanceNormalization has the characteristic of having two side input channels thatfuse pose features and identity features as denormalization parameters, thusenhancing the pose transfer capability of the network. Extensive experimentalresults show that our proposed method has stronger pose transfer capabilitythan state-of-the-art methods while maintaining a lightweight networkstructure, and can converge faster.</description><author>Jue Liu, Feipeng Da</author><pubDate>Wed, 24 May 2023 10:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14951v1</guid></item><item><title>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</title><link>http://arxiv.org/abs/2206.02257v3</link><description>In this survey, we present a systematic review of 3D hand pose estimationfrom the perspective of efficient annotation and learning. 3D hand poseestimation has been an important research area owing to its potential to enablevarious applications, such as video understanding, AR/VR, and robotics.However, the performance of models is tied to the quality and quantity ofannotated 3D hand poses. Under the status quo, acquiring such annotated 3D handposes is challenging, e.g., due to the difficulty of 3D annotation and thepresence of occlusion. To reveal this problem, we review the pros and cons ofexisting annotation methods classified as manual, synthetic-model-based,hand-sensor-based, and computational approaches. Additionally, we examinemethods for learning 3D hand poses when annotated data are scarce, includingself-supervised pretraining, semi-supervised learning, and domain adaptation.Based on the study of efficient annotation and learning, we further discusslimitations and possible future directions in this field.</description><author>Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato</author><pubDate>Wed, 26 Apr 2023 07:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02257v3</guid></item><item><title>Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields</title><link>http://arxiv.org/abs/2212.02493v3</link><description>Coordinate-based implicit neural networks, or neural fields, have emerged asuseful representations of shape and appearance in 3D computer vision. Despiteadvances, however, it remains challenging to build neural fields for categoriesof objects without datasets like ShapeNet that provide "canonicalized" objectinstances that are consistently aligned for their 3D position and orientation(pose). We present Canonical Field Network (CaFi-Net), a self-supervised methodto canonicalize the 3D pose of instances from an object category represented asneural fields, specifically neural radiance fields (NeRFs). CaFi-Net directlylearns from continuous and noisy radiance fields using a Siamese networkarchitecture that is designed to extract equivariant field features forcategory-level canonicalization. During inference, our method takes pre-trainedneural radiance fields of novel object instances at arbitrary 3D pose andestimates a canonical field with consistent 3D pose across the entire category.Extensive experiments on a new dataset of 1300 NeRF models across 13 objectcategories show that our method matches or exceeds the performance of 3D pointcloud-based methods.</description><author>Rohith Agaram, Shaurya Dewan, Rahul Sajnani, Adrien Poulenard, Madhava Krishna, Srinath Sridhar</author><pubDate>Wed, 17 May 2023 12:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02493v3</guid></item><item><title>EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With Kinematic Structure Priors</title><link>http://arxiv.org/abs/2306.09615v1</link><description>Transformer is popular in recent 3D human pose estimation, which utilizeslong-term modeling to lift 2D keypoints into the 3D space. However, currenttransformer-based methods do not fully exploit the prior knowledge of the humanskeleton provided by the kinematic structure. In this paper, we propose a noveltransformer-based model EvoPose to introduce the human body prior knowledge for3D human pose estimation effectively. Specifically, a Structural PriorsRepresentation (SPR) module represents human priors as structural featurescarrying rich body patterns, e.g. joint relationships. The structural featuresare interacted with 2D pose sequences and help the model to achieve moreinformative spatiotemporal features. Moreover, a Recursive Refinement (RR)module is applied to refine the 3D pose outputs by utilizing estimated resultsand further injects human priors simultaneously. Extensive experimentsdemonstrate the effectiveness of EvoPose which achieves a new state of the arton two most popular benchmarks, Human3.6M and MPI-INF-3DHP.</description><author>Yaqi Zhang, Yan Lu, Bin Liu, Zhiwei Zhao, Qi Chu, Nenghai Yu</author><pubDate>Fri, 16 Jun 2023 05:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09615v1</guid></item><item><title>HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation</title><link>http://arxiv.org/abs/2305.06968v1</link><description>Monocular 3D human pose and shape estimation is an ill-posed problem sincemultiple 3D solutions can explain a 2D image of a subject. Recent approachespredict a probability distribution over plausible 3D pose and shape parametersconditioned on the image. We show that these approaches exhibit a trade-offbetween three key properties: (i) accuracy - the likelihood of the ground-truth3D solution under the predicted distribution, (ii) sample-input consistency -the extent to which 3D samples from the predicted distribution match thevisible 2D image evidence, and (iii) sample diversity - the range of plausible3D solutions modelled by the predicted distribution. Our method, HuManiFlow,predicts simultaneously accurate, consistent and diverse distributions. We usethe human kinematic tree to factorise full body pose into ancestor-conditionedper-body-part pose distributions in an autoregressive manner. Per-body-partdistributions are implemented using normalising flows that respect the manifoldstructure of SO(3), the Lie group of per-body-part poses. We show thatill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,and employ only probabilistic training losses. Code is available at:https://github.com/akashsengupta1997/HuManiFlow.</description><author>Akash Sengupta, Ignas Budvytis, Roberto Cipolla</author><pubDate>Thu, 11 May 2023 17:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06968v1</guid></item><item><title>NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation</title><link>http://arxiv.org/abs/2305.08590v1</link><description>With the progress of 3D human pose and shape estimation, state-of-the-artmethods can either be robust to occlusions or obtain pixel-aligned accuracy innon-occlusion cases. However, they cannot obtain robustness and mesh-imagealignment at the same time. In this work, we present NIKI (Neural InverseKinematics with Invertible Neural Network), which models bi-directional errorsto improve the robustness to occlusions and obtain pixel-aligned accuracy. NIKIcan learn from both the forward and inverse processes with invertible networks.In the inverse process, the model separates the error from the plausible 3Dpose manifold for a robust 3D human pose estimation. In the forward process, weenforce the zero-error boundary conditions to improve the sensitivity toreliable joint positions for better mesh-image alignment. Furthermore, NIKIemulates the analytical inverse kinematics algorithms with the twist-and-swingdecomposition for better interpretability. Experiments on standard andocclusion-specific benchmarks demonstrate the effectiveness of NIKI, where weexhibit robust and well-aligned results simultaneously. Code is available athttps://github.com/Jeff-sjtu/NIKI</description><author>Jiefeng Li, Siyuan Bian, Qi Liu, Jiasheng Tang, Fan Wang, Cewu Lu</author><pubDate>Mon, 15 May 2023 13:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08590v1</guid></item><item><title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer</title><link>http://arxiv.org/abs/2303.09681v3</link><description>Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available athttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be releasedupon paper acceptance.</description><author>Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng</author><pubDate>Thu, 11 May 2023 00:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09681v3</guid></item><item><title>Regular Splitting Graph Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2305.05785v1</link><description>In human pose estimation methods based on graph convolutional architectures,the human skeleton is usually modeled as an undirected graph whose nodes arebody joints and edges are connections between neighboring joints. However, mostof these methods tend to focus on learning relationships between body joints ofthe skeleton using first-order neighbors, ignoring higher-order neighbors andhence limiting their ability to exploit relationships between distant joints.In this paper, we introduce a higher-order regular splitting graph network(RS-Net) for 2D-to-3D human pose estimation using matrix splitting inconjunction with weight and adjacency modulation. The core idea is to capturelong-range dependencies between body joints using multi-hop neighborhoods andalso to learn different modulation vectors for different body joints as well asa modulation matrix added to the adjacency matrix associated to the skeleton.This learnable modulation matrix helps adjust the graph structure by addingextra graph edges in an effort to learn additional connections between bodyjoints. Instead of using a shared weight matrix for all neighboring bodyjoints, the proposed RS-Net model applies weight unsharing before aggregatingthe feature vectors associated to the joints in order to capture the differentrelations between them. Experiments and ablations studies performed on twobenchmark datasets demonstrate the effectiveness of our model, achievingsuperior performance over recent state-of-the-art methods for 3D human poseestimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 09 May 2023 23:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05785v1</guid></item><item><title>HDFormer: High-order Directed Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.01825v2</link><description>Human pose estimation is a challenging task due to its structured datasequence nature. Existing methods primarily focus on pair-wise interaction ofbody joints, which is insufficient for scenarios involving overlapping jointsand rapidly changing poses. To overcome these issues, we introduce a novelapproach, the High-order Directed Transformer (HDFormer), which leverageshigh-order bone and joint relationships for improved pose estimation.Specifically, HDFormer incorporates both self-attention and high-orderattention to formulate a multi-order attention module. This module facilitatesfirst-order "joint$\leftrightarrow$joint", second-order"bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint"interactions, effectively addressing issues in complex and occlusion-heavysituations. In addition, modern CNN techniques are integrated into thetransformer-based architecture, balancing the trade-off between performance andefficiency. HDFormer significantly outperforms state-of-the-art (SOTA) modelson Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parametersand significantly lower computational costs. Moreover, HDFormer demonstratesbroad real-world applicability, enabling real-time, accurate 3D poseestimation. The source code is in https://github.com/hyer/HDFormer</description><author>Hanyuan Chen, Jun-Yan He, Wangmeng Xiang, Zhi-Qi Cheng, Wei Liu, Hanbing Liu, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 22 May 2023 07:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01825v2</guid></item><item><title>Interweaved Graph and Attention Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2304.14045v1</link><description>Despite substantial progress in 3D human pose estimation from a single-viewimage, prior works rarely explore global and local correlations, leading toinsufficient learning of human skeleton representations. To address this issue,we propose a novel Interweaved Graph and Attention Network (IGANet) that allowsbidirectional communications between graph convolutional networks (GCNs) andattentions. Specifically, we introduce an IGA module, where attentions areprovided with local information from GCNs and GCNs are injected with globalinformation from attentions. Additionally, we design a simple yet effectiveU-shaped multi-layer perceptron (uMLP), which can capture multi-granularityinformation for body joints. Extensive experiments on two popular benchmarkdatasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate ourproposed method.The results show that IGANet achieves state-of-the-artperformance on both datasets. Code is available athttps://github.com/xiu-cs/IGANet.</description><author>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li</author><pubDate>Thu, 27 Apr 2023 10:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14045v1</guid></item><item><title>Learning to Predict Scene-Level Implicit 3D from Posed RGBD Data</title><link>http://arxiv.org/abs/2306.08671v1</link><description>We introduce a method that can learn to predict scene-level implicitfunctions for 3D reconstruction from posed RGBD data. At test time, our systemmaps a previously unseen RGB image to a 3D reconstruction of a scene viaimplicit functions. While implicit functions for 3D reconstruction have oftenbeen tied to meshes, we show that we can train one using only a set of posedRGBD images. This setting may help 3D reconstruction unlock the sea ofaccelerometer+RGBD data that is coming with new phones. Our system, D2-DRDF,can match and sometimes outperform current methods that use mesh supervisionand shows better robustness to sparse data.</description><author>Nilesh Kulkarni, Linyi Jin, Justin Johnson, David F. Fouhey</author><pubDate>Wed, 14 Jun 2023 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08671v1</guid></item><item><title>Occlusion Robust 3D Human Pose Estimation with StridedPoseGraphFormer and Data Augmentation</title><link>http://arxiv.org/abs/2304.12069v1</link><description>Occlusion is an omnipresent challenge in 3D human pose estimation (HPE). Inspite of the large amount of research dedicated to 3D HPE, only a limitednumber of studies address the problem of occlusion explicitly. To fill thisgap, we propose to combine exploitation of spatio-temporal features withsynthetic occlusion augmentation during training to deal with occlusion. Tothis end, we build a spatio-temporal 3D HPE model, StridedPoseGraphFormer basedon graph convolution and transformers, and train it using occlusionaugmentation. Unlike the existing occlusion-aware methods, that are only testedfor limited occlusion, we extensively evaluate our method for varying degreesof occlusion. We show that our proposed method compares favorably with thestate-of-the-art (SoA). Our experimental results also reveal that in theabsence of any occlusion handling mechanism, the performance of SoA 3D HPEmethods degrades significantly when they encounter occlusion.</description><author>Soubarna Banik, Patricia Gschoßmann, Alejandro Mendoza Garcia, Alois Knoll</author><pubDate>Mon, 24 Apr 2023 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12069v1</guid></item><item><title>TAPE: Temporal Attention-based Probabilistic human pose and shape Estimation</title><link>http://arxiv.org/abs/2305.00181v1</link><description>Reconstructing 3D human pose and shape from monocular videos is awell-studied but challenging problem. Common challenges include occlusions, theinherent ambiguities in the 2D to 3D mapping and the computational complexityof video processing. Existing methods ignore the ambiguities of thereconstruction and provide a single deterministic estimate for the 3D pose. Inorder to address these issues, we present a Temporal Attention basedProbabilistic human pose and shape Estimation method (TAPE) that operates on anRGB video. More specifically, we propose to use a neural network to encodevideo frames to temporal features using an attention-based neural network.Given these features, we output a per-frame but temporally-informed probabilitydistribution for the human pose using Normalizing Flows. We show that TAPEoutperforms state-of-the-art methods in standard benchmarks and serves as aneffective video-based prior for optimization-based human pose and shapeestimation. Code is available at: https: //github.com/nikosvasilik/TAPE</description><author>Nikolaos Vasilikopoulos, Nikos Kolotouros, Aggeliki Tsoli, Antonis Argyros</author><pubDate>Sat, 29 Apr 2023 07:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00181v1</guid></item><item><title>Optimal and Robust Category-level Perception: Object Pose and Shape Estimation from 2D and 3D Semantic Keypoints</title><link>http://arxiv.org/abs/2206.12498v2</link><description>We consider a category-level perception problem, where one is given 2D or 3Dsensor data picturing an object of a given category (e.g., a car), and has toreconstruct the 3D pose and shape of the object despite intra-class variability(i.e., different car models have different shapes). We consider an active shapemodel, where -for an object category- we are given a library of potential CADmodels describing objects in that category, and we adopt a standard formulationwhere pose and shape are estimated from 2D or 3D keypoints via non-convexoptimization. Our first contribution is to develop PACE3D* and PACE2D*, thefirst certifiably optimal solvers for pose and shape estimation using 3D and 2Dkeypoints, respectively. Both solvers rely on the design of tight (i.e., exact)semidefinite relaxations. Our second contribution is to develop outlier-robustversions of both solvers, named PACE3D# and PACE2D#. Towards this goal, wepropose ROBIN, a general graph-theoretic framework to prune outliers, whichuses compatibility hypergraphs to model measurements' compatibility. We showthat in category-level perception problems these hypergraphs can be built fromthe winding orders of the keypoints (in 2D) or their convex hulls (in 3D), andmany outliers can be filtered out via maximum hyperclique computation. The lastcontribution is an extensive experimental evaluation. Besides providing anablation study on simulated datasets and on the PASCAL3D+ dataset, we combineour solver with a deep keypoint detector, and show that PACE3D# improves overthe state of the art in vehicle pose estimation in the ApolloScape datasets,and its runtime is compatible with practical applications. We release our codeat https://github.com/MIT-SPARK/PACE.</description><author>Jingnan Shi, Heng Yang, Luca Carlone</author><pubDate>Mon, 15 May 2023 04:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12498v2</guid></item><item><title>Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception</title><link>http://arxiv.org/abs/2306.06362v2</link><description>We introduce the Aria Digital Twin (ADT) - an egocentric dataset capturedusing Aria glasses with extensive object, environment, and human level groundtruth. This ADT release contains 200 sequences of real-world activitiesconducted by Aria wearers in two real indoor scenes with 398 object instances(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of twomonochrome camera streams, one RGB camera stream, two IMU streams; b) completesensor calibration; c) ground truth data including continuous6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eyegaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)photo-realistic synthetic renderings. To the best of our knowledge, there is noexisting egocentric dataset with a level of accuracy, photo-realism andcomprehensiveness comparable to ADT. By contributing ADT to the researchcommunity, our mission is to set a new standard for evaluation in theegocentric machine perception domain, which includes very challenging researchproblems such as 3D object detection and tracking, scene reconstruction andunderstanding, sim-to-real learning, human pose prediction - while alsoinspiring new machine perception tasks for augmented reality (AR) applications.To kick start exploration of the ADT research use cases, we evaluated severalexisting state-of-the-art methods for object detection, segmentation and imagetranslation tasks that demonstrate the usefulness of ADT as a benchmarkingdataset.</description><author>Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, Carl Yuheng Ren</author><pubDate>Tue, 13 Jun 2023 07:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06362v2</guid></item><item><title>MVP-Human Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames</title><link>http://arxiv.org/abs/2204.11184v2</link><description>In this paper, we consider a novel problem of reconstructing a 3D humanavatar from multiple unconstrained frames, independent of assumptions on cameracalibration, capture space, and constrained actions. The problem should beaddressed by a framework that takes multiple unconstrained images as inputs,and generates a shape-with-skinning avatar in the canonical space, finished inone feed-forward pass. To this end, we present 3D Avatar Reconstruction in thewild (ARwild), which first reconstructs the implicit skinning fields in amulti-level manner, by which the image features from multiple images arealigned and integrated to estimate a pixel-aligned implicit function thatrepresents the clothed shape. To enable the training and testing of the newframework, we contribute a large-scale dataset, MVP-Human (Multi-View andmulti-Pose 3D Human), which contains 400 subjects, each of which has 15 scansin different poses and 8-view images for each pose, providing 6,000 3D scansand 48,000 images in total. Overall, benefits from the specific networkarchitecture and the diverse data, the trained model enables 3D avatarreconstruction from unconstrained frames and achieves state-of-the-artperformance.</description><author>Xiangyu Zhu, Tingting Liao, Jiangjing Lyu, Xiang Yan, Yunfeng Wang, Kan Guo, Qiong Cao, Stan Z. Li, Zhen Lei</author><pubDate>Wed, 17 May 2023 11:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.11184v2</guid></item><item><title>Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training</title><link>http://arxiv.org/abs/2206.11215v4</link><description>We consider a certifiable object pose estimation problem, where -- given apartial point cloud of an object -- the goal is to not only estimate the objectpose, but also to provide a certificate of correctness for the resultingestimate. Our first contribution is a general theory of certification forend-to-end perception models. In particular, we introduce the notion of$\zeta$-correctness, which bounds the distance between an estimate and theground truth. We show that $\zeta$-correctness can be assessed by implementingtwo certificates: (i) a certificate of observable correctness, that asserts ifthe model output is consistent with the input data and prior information, (ii)a certificate of non-degeneracy, that asserts whether the input data issufficient to compute a unique estimate. Our second contribution is to applythis theory and design a new learning-based certifiable pose estimator. Wepropose C-3PO, a semantic-keypoint-based pose estimation model, augmented withthe two certificates, to solve the certifiable pose estimation problem. C-3POalso includes a keypoint corrector, implemented as a differentiableoptimization layer, that can correct large detection errors (e.g. due to thesim-to-real gap). Our third contribution is a novel self-supervised trainingapproach that uses our certificate of observable correctness to provide thesupervisory signal to C-3PO during training. In it, the model trains only onthe observably correct input-output pairs, in each training iteration. Astraining progresses, we see that the observably correct input-output pairsgrow, eventually reaching near 100% in many cases. Our experiments show that(i) standard semantic-keypoint-based methods outperform more recentalternatives, (ii) C-3PO further improves performance and significantlyoutperforms all the baselines, and (iii) C-3PO's certificates are able todiscern correct pose estimates.</description><author>Rajat Talak, Lisa Peng, Luca Carlone</author><pubDate>Fri, 28 Apr 2023 20:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11215v4</guid></item><item><title>ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with Unsupervised Implicit Pose Embedding</title><link>http://arxiv.org/abs/2304.14005v1</link><description>Although 3D-aware GANs based on neural radiance fields have achievedcompetitive performance, their applicability is still limited to objects orscenes with the ground-truths or prediction models for clearly definedcanonical camera poses. To extend the scope of applicable datasets, we proposea novel 3D-aware GAN optimization technique through contrastive learning withimplicit pose embeddings. To this end, we first revise the discriminator designand remove dependency on ground-truth camera poses. Then, to capture complexand challenging 3D scene structures more effectively, we make the discriminatorestimate a high-dimensional implicit pose embedding from a given image andperform contrastive learning on the pose embedding. The proposed approach canbe employed for the dataset, where the canonical camera pose is ill-definedbecause it does not look up or estimate camera poses. Experimental results showthat our algorithm outperforms existing methods by large margins on thedatasets with multiple object categories and inconsistent canonical cameraposes.</description><author>Mijeoong Kim, Hyunjoon Lee, Bohyung Han</author><pubDate>Thu, 27 Apr 2023 08:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14005v1</guid></item><item><title>BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos</title><link>http://arxiv.org/abs/2212.07401v3</link><description>Quantifying motion in 3D is important for studying the behavior of humans andother animals, but manual pose annotations are expensive and time-consuming toobtain. Self-supervised keypoint discovery is a promising strategy forestimating 3D poses without annotations. However, current keypoint discoveryapproaches commonly process single 2D views and do not operate in the 3D space.We propose a new method to perform self-supervised keypoint discovery in 3Dfrom multi-view videos of behaving agents, without any keypoint or bounding boxsupervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoderarchitecture with a 3D volumetric heatmap, trained to reconstructspatiotemporal differences across multiple views, in addition to joint lengthconstraints on a learned 3D skeleton of the subject. In this way, we discoverkeypoints without requiring manual supervision in videos of humans and rats,demonstrating the potential of 3D keypoint discovery for studying behavior.</description><author>Jennifer J. Sun, Lili Karashchuk, Amil Dravid, Serim Ryou, Sonia Fereidooni, John Tuthill, Aggelos Katsaggelos, Bingni W. Brunton, Georgia Gkioxari, Ann Kennedy, Yisong Yue, Pietro Perona</author><pubDate>Fri, 02 Jun 2023 06:03:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07401v3</guid></item><item><title>BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos</title><link>http://arxiv.org/abs/2212.07401v2</link><description>Quantifying motion in 3D is important for studying the behavior of humans andother animals, but manual pose annotations are expensive and time-consuming toobtain. Self-supervised keypoint discovery is a promising strategy forestimating 3D poses without annotations. However, current keypoint discoveryapproaches commonly process single 2D views and do not operate in the 3D space.We propose a new method to perform self-supervised keypoint discovery in 3Dfrom multi-view videos of behaving agents, without any keypoint or bounding boxsupervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoderarchitecture with a 3D volumetric heatmap, trained to reconstructspatiotemporal differences across multiple views, in addition to joint lengthconstraints on a learned 3D skeleton of the subject. In this way, we discoverkeypoints without requiring manual supervision in videos of humans and rats,demonstrating the potential of 3D keypoint discovery for studying behavior.</description><author>Jennifer J. Sun, Lili Karashchuk, Amil Dravid, Serim Ryou, Sonia Fereidooni, John Tuthill, Aggelos Katsaggelos, Bingni W. Brunton, Georgia Gkioxari, Ann Kennedy, Yisong Yue, Pietro Perona</author><pubDate>Sun, 07 May 2023 00:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07401v2</guid></item><item><title>Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction</title><link>http://arxiv.org/abs/2305.09510v1</link><description>Robotic manipulation systems operating in complex environments rely onperception systems that provide information about the geometry (pose and 3Dshape) of the objects in the scene along with other semantic information suchas object labels. This information is then used for choosing the feasiblegrasps on relevant objects. In this paper, we present a novel method to providethis geometric and semantic information of all objects in the scene as well asfeasible grasps on those objects simultaneously. The main advantage of ourmethod is its speed as it avoids sequential perception and grasp planningsteps. With detailed quantitative analysis, we show that our method deliverscompetitive performance compared to the state-of-the-art dedicated methods forobject shape, pose, and grasp predictions while providing fast inference at 30frames per second speed.</description><author>Shubham Agrawal, Nikhil Chavan-Dafle, Isaac Kasahara, Selim Engin, Jinwook Huh, Volkan Isler</author><pubDate>Tue, 16 May 2023 16:03:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09510v1</guid></item><item><title>EgoHumans: An Egocentric 3D Multi-Human Benchmark</title><link>http://arxiv.org/abs/2305.16487v1</link><description>We present EgoHumans, a new multi-view multi-human video benchmark to advancethe state-of-the-art of egocentric human 3D pose estimation and tracking.Existing egocentric benchmarks either capture single subject or indoor-onlyscenarios, which limit the generalization of computer vision algorithms forreal-world applications. We propose a novel 3D capture setup to construct acomprehensive egocentric multi-human benchmark in the wild with annotations tosupport diverse tasks such as human detection, tracking, 2D/3D pose estimation,and mesh recovery. We leverage consumer-grade wearable camera-equipped glassesfor the egocentric view, which enables us to capture dynamic activities likeplaying soccer, fencing, volleyball, etc. Furthermore, our multi-view setupgenerates accurate 3D ground truth even under severe or complete occlusion. Thedataset consists of more than 125k egocentric images, spanning diverse sceneswith a particular focus on challenging and unchoreographed multi-humanactivities and fast-moving egocentric views. We rigorously evaluate existingstate-of-the-art methods and highlight their limitations in the egocentricscenario, specifically on multi-human tracking. To address such limitations, wepropose EgoFormer, a novel approach with a multi-stream transformerarchitecture and explicit 3D spatial reasoning to estimate and track the humanpose. EgoFormer significantly outperforms prior art by 13.6% IDF1 and 9.3 HOTAon the EgoHumans dataset.</description><author>Rawal Khirodkar, Aayush Bansal, Lingni Ma, Richard Newcombe, Minh Vo, Kris Kitani</author><pubDate>Thu, 25 May 2023 22:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16487v1</guid></item><item><title>Object pop-up: Can we infer 3D objects and their poses from human interactions alone?</title><link>http://arxiv.org/abs/2306.00777v1</link><description>The intimate entanglement between objects affordances and human poses is oflarge interest, among others, for behavioural sciences, cognitive psychology,and Computer Vision communities. In recent years, the latter has developedseveral object-centric approaches: starting from items, learning pipelinessynthesizing human poses and dynamics in a realistic way, satisfying bothgeometrical and functional expectations. However, the inverse perspective issignificantly less explored: Can we infer 3D objects and their poses from humaninteractions alone? Our investigation follows this direction, showing that ageneric 3D human point cloud is enough to pop up an unobserved object, evenwhen the user is just imitating a functionality (e.g., looking through abinocular) without involving a tangible counterpart. We validate our methodqualitatively and quantitatively, with synthetic data and sequences acquiredfor the task, showing applicability for XR/VR. The code is available athttps://github.com/ptrvilya/object-popup.</description><author>Ilya A. Petrov, Riccardo Marin, Julian Chibane, Gerard Pons-Moll</author><pubDate>Thu, 01 Jun 2023 16:08:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00777v1</guid></item><item><title>Predictive Modeling of Equine Activity Budgets Using a 3D Skeleton Reconstructed from Surveillance Recordings</title><link>http://arxiv.org/abs/2306.05311v1</link><description>In this work, we present a pipeline to reconstruct the 3D pose of a horsefrom 4 simultaneous surveillance camera recordings. Our environment posesinteresting challenges to tackle, such as limited field view of the cameras anda relatively closed and small environment. The pipeline consists of training a2D markerless pose estimation model to work on every viewpoint, then applyingit to the videos and performing triangulation. We present numerical evaluationof the results (error analysis), as well as show the utility of the achievedposes in downstream tasks of selected behavioral predictions. Our analysis ofthe predictive model for equine behavior showed a bias towards pain-inducedhorses, which aligns with our understanding of how behavior varies acrosspainful and healthy subjects.</description><author>Ernest Pokropek, Sofia Broomé, Pia Haubro Andersen, Hedvig Kjellström</author><pubDate>Thu, 08 Jun 2023 17:00:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05311v1</guid></item><item><title>Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models</title><link>http://arxiv.org/abs/2211.09707v2</link><description>Diffusion models have experienced a surge of interest as highly expressiveyet efficiently trainable probabilistic models. We show that these models arean excellent fit for synthesising human motion that co-occurs with audio, e.g.,dancing and co-speech gesticulation, since motion is complex and highlyambiguous given audio, calling for a probabilistic description. Specifically,we adapt the DiffWave architecture to model 3D pose sequences, puttingConformers in place of dilated convolutions for improved modelling power. Wealso demonstrate control over motion style, using classifier-free guidance toadjust the strength of the stylistic expression. Experiments on gesture anddance generation confirm that the proposed method achieves top-of-the-linemotion quality, with distinctive styles whose expression can be made more orless pronounced. We also synthesise path-driven locomotion using the same modelarchitecture. Finally, we generalise the guidance procedure to obtainproduct-of-expert ensembles of diffusion models and demonstrate how these maybe used for, e.g., style interpolation, a contribution we believe is ofindependent interest. Seehttps://www.speech.kth.se/research/listen-denoise-action/ for video examples,data, and code.</description><author>Simon Alexanderson, Rajmund Nagy, Jonas Beskow, Gustav Eje Henter</author><pubDate>Tue, 16 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09707v2</guid></item><item><title>Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction</title><link>http://arxiv.org/abs/2209.05135v3</link><description>Learning fine-grained movements is a challenging topic in robotics,particularly in the context of robotic hands. One specific instance of thischallenge is the acquisition of fingerspelling sign language in robots. In thispaper, we propose an approach for learning dexterous motor imitation from videoexamples without additional information. To achieve this, we first build a URDFmodel of a robotic hand with a single actuator for each joint. We then leveragepre-trained deep vision models to extract the 3D pose of the hand from RGBvideos. Next, using state-of-the-art reinforcement learning algorithms formotion imitation (namely, proximal policy optimization and soft actor-critic),we train a policy to reproduce the movement extracted from the demonstrations.We identify the optimal set of hyperparameters for imitation based on areference motion. Finally, we demonstrate the generalizability of our approachby testing it on six different tasks, corresponding to fingerspelled letters.Our results show that our approach is able to successfully imitate thesefine-grained movements without additional information, highlighting itspotential for real-world applications in robotics.</description><author>Federico Tavella, Aphrodite Galata, Angelo Cangelosi</author><pubDate>Mon, 05 Jun 2023 13:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05135v3</guid></item><item><title>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</title><link>http://arxiv.org/abs/2306.09109v1</link><description>Recent advances in neural reconstruction enable high-quality 3D objectreconstruction from casually captured image collections. Current techniquesmostly analyze their progress on relatively simple image collections whereStructure-from-Motion (SfM) techniques can provide ground-truth (GT) cameraposes. We note that SfM techniques tend to fail on in-the-wild imagecollections such as image search results with varying backgrounds andilluminations. To enable systematic research progress on 3D reconstruction fromcasual image captures, we propose NAVI: a new dataset of category-agnosticimage collections of objects with high-quality 3D scans along with per-image2D-3D alignments providing near-perfect GT camera parameters. These 2D-3Dalignments allow us to extract accurate derivative annotations such as densepixel correspondences, depth and segmentation maps. We demonstrate the use ofNAVI image collections on different problem settings and show that NAVI enablesmore thorough evaluations that were not possible with existing datasets. Webelieve NAVI is beneficial for systematic research progress on 3Dreconstruction and correspondence estimation. Project page:https://navidataset.github.io</description><author>Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engelhardt, Arjun Karpur, Karen Truong, Kyle Sargent, Stefan Popov, André Araujo, Ricardo Martin-Brualla, Kaushal Patel, Daniel Vlasic, Vittorio Ferrari, Ameesh Makadia, Ce Liu, Yuanzhen Li, Howard Zhou</author><pubDate>Thu, 15 Jun 2023 14:11:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09109v1</guid></item><item><title>Towards 3D Face Reconstruction in Perspective Projection: Estimating 6DoF Face Pose from Monocular Image</title><link>http://arxiv.org/abs/2205.04126v2</link><description>In 3D face reconstruction, orthogonal projection has been widely employed tosubstitute perspective projection to simplify the fitting process. Thisapproximation performs well when the distance between camera and face is farenough. However, in some scenarios that the face is very close to camera ormoving along the camera axis, the methods suffer from the inaccuratereconstruction and unstable temporal fitting due to the distortion under theperspective projection. In this paper, we aim to address the problem ofsingle-image 3D face reconstruction under perspective projection. Specifically,a deep neural network, Perspective Network (PerspNet), is proposed tosimultaneously reconstruct 3D face shape in canonical space and learn thecorrespondence between 2D pixels and 3D points, by which the 6DoF (6 Degrees ofFreedom) face pose can be estimated to represent perspective projection.Besides, we contribute a large ARKitFace dataset to enable the training andevaluation of 3D face reconstruction solutions under the scenarios ofperspective projection, which has 902,724 2D facial images with ground-truth 3Dface mesh and annotated 6DoF pose parameters. Experimental results show thatour approach outperforms current state-of-the-art methods by a significantmargin. The code and data are available athttps://github.com/cbsropenproject/6dof_face.</description><author>Yueying Kao, Bowen Pan, Miao Xu, Jiangjing Lyu, Xiangyu Zhu, Yuanzhang Chang, Xiaobo Li, Zhen Lei</author><pubDate>Wed, 17 May 2023 12:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.04126v2</guid></item><item><title>Robust 3D-aware Object Classification via Discriminative Render-and-Compare</title><link>http://arxiv.org/abs/2305.14668v1</link><description>In real-world applications, it is essential to jointly estimate the 3D objectpose and class label of objects, i.e., to perform 3D-aware classification.Whilecurrent approaches for either image classification or pose estimation can beextended to 3D-aware classification, we observe that they are inherentlylimited: 1) Their performance is much lower compared to the respectivesingle-task models, and 2) they are not robust in out-of-distribution (OOD)scenarios. Our main contribution is a novel architecture for 3D-awareclassification, which builds upon a recent work and performs comparably tosingle-task models while being highly robust. In our method, an object categoryis represented as a 3D cuboid mesh composed of feature vectors at each meshvertex. Using differentiable rendering, we estimate the 3D object pose byminimizing the reconstruction error between the mesh and the featurerepresentation of the target image. Object classification is then performed bycomparing the reconstruction losses across object categories. Notably, theneural texture of the mesh is trained in a discriminative manner to enhance theclassification performance while also avoiding local optima in thereconstruction loss. Furthermore, we show how our method and feed-forwardneural networks can be combined to scale the render-and-compare approach tolarger numbers of categories. Our experiments on PASCAL3D+, occluded-PASCAL3D+,and OOD-CV show that our method outperforms all baselines at 3D-awareclassification by a wide margin in terms of performance and robustness.</description><author>Artur Jesslen, Guofeng Zhang, Angtian Wang, Alan Yuille, Adam Kortylewski</author><pubDate>Wed, 24 May 2023 04:20:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14668v1</guid></item><item><title>Robust 3D-aware Object Classification via Discriminative Render-and-Compare</title><link>http://arxiv.org/abs/2305.14668v2</link><description>In real-world applications, it is essential to jointly estimate the 3D objectpose and class label of objects, i.e., to perform 3D-aware classification.Whilecurrent approaches for either image classification or pose estimation can beextended to 3D-aware classification, we observe that they are inherentlylimited: 1) Their performance is much lower compared to the respectivesingle-task models, and 2) they are not robust in out-of-distribution (OOD)scenarios. Our main contribution is a novel architecture for 3D-awareclassification, which builds upon a recent work and performs comparably tosingle-task models while being highly robust. In our method, an object categoryis represented as a 3D cuboid mesh composed of feature vectors at each meshvertex. Using differentiable rendering, we estimate the 3D object pose byminimizing the reconstruction error between the mesh and the featurerepresentation of the target image. Object classification is then performed bycomparing the reconstruction losses across object categories. Notably, theneural texture of the mesh is trained in a discriminative manner to enhance theclassification performance while also avoiding local optima in thereconstruction loss. Furthermore, we show how our method and feed-forwardneural networks can be combined to scale the render-and-compare approach tolarger numbers of categories. Our experiments on PASCAL3D+, occluded-PASCAL3D+,and OOD-CV show that our method outperforms all baselines at 3D-awareclassification by a wide margin in terms of performance and robustness.</description><author>Artur Jesslen, Guofeng Zhang, Angtian Wang, Alan Yuille, Adam Kortylewski</author><pubDate>Mon, 05 Jun 2023 18:39:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14668v2</guid></item><item><title>VoxDet: Voxel Learning for Novel Instance Detection</title><link>http://arxiv.org/abs/2305.17220v3</link><description>Detecting unseen instances based on multi-view templates is a challengingproblem due to its open-world nature. Traditional methodologies, whichprimarily rely on 2D representations and matching techniques, are ofteninadequate in handling pose variations and occlusions. To solve this, weintroduce VoxDet, a pioneer 3D geometry-aware framework that fully utilizes thestrong 3D voxel representation and reliable voxel matching mechanism. VoxDetfirst ingeniously proposes template voxel aggregation (TVA) module, effectivelytransforming multi-view 2D images into 3D voxel features. By leveragingassociated camera poses, these features are aggregated into a compact 3Dtemplate voxel. In novel instance detection, this voxel representationdemonstrates heightened resilience to occlusion and pose variations. We alsodiscover that a 3D reconstruction objective helps to pre-train the 2D-3Dmapping in TVA. Second, to quickly align with the template voxel, VoxDetincorporates a Query Voxel Matching (QVM) module. The 2D queries are firstconverted into their voxel representation with the learned 2D-3D mapping. Wefind that since the 3D voxel representations encode the geometry, we can firstestimate the relative rotation and then compare the aligned voxels, leading toimproved accuracy and efficiency. Exhaustive experiments are conducted on thedemanding LineMod-Occlusion, YCB-video, and the newly built RoboToolsbenchmarks, where VoxDet outperforms various 2D baselines remarkably with 20%higher recall and faster speed. To the best of our knowledge, VoxDet is thefirst to incorporate implicit 3D knowledge for 2D detection tasks.</description><author>Bowen Li, Jiashun Wang, Yaoyu Hu, Chen Wang, Sebastian Scherer</author><pubDate>Sun, 04 Jun 2023 15:22:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17220v3</guid></item><item><title>VoxDet: Voxel Learning for Novel Instance Detection</title><link>http://arxiv.org/abs/2305.17220v2</link><description>Detecting unseen instances based on multi-view templates is a challengingproblem due to its open-world nature. Traditional methodologies, whichprimarily rely on 2D representations and matching techniques, are ofteninadequate in handling pose variations and occlusions. To solve this, weintroduce VoxDet, a pioneer 3D geometry-aware framework that fully utilizes thestrong 3D voxel representation and reliable voxel matching mechanism. VoxDetfirst ingeniously proposes template voxel aggregation (TVA) module, effectivelytransforming multi-view 2D images into 3D voxel features. By leveragingassociated camera poses, these features are aggregated into a compact 3Dtemplate voxel. In novel instance detection, this voxel representationdemonstrates heightened resilience to occlusion and pose variations. We alsodiscover that a 3D reconstruction objective helps to pre-train the 2D-3Dmapping in TVA. Second, to quickly align with the template voxel, VoxDetincorporates a Query Voxel Matching (QVM) module. The 2D queries are firstconverted into their voxel representation with the learned 2D-3D mapping. Wefind that since the 3D voxel representations encode the geometry, we can firstestimate the relative rotation and then compare the aligned voxels, leading toimproved accuracy and efficiency. Exhaustive experiments are conducted on thedemanding LineMod-Occlusion, YCB-video, and the newly built RoboToolsbenchmarks, where VoxDet outperforms various 2D baselines remarkably with 20%higher recall and faster speed. To the best of our knowledge, VoxDet is thefirst to incorporate implicit 3D knowledge for 2D tasks.</description><author>Bowen Li, Jiashun Wang, Yaoyu Hu, Chen Wang, Sebastian Scherer</author><pubDate>Tue, 30 May 2023 18:10:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17220v2</guid></item><item><title>Contact2Grasp: 3D Grasp Synthesis via Hand-Object Contact Constraint</title><link>http://arxiv.org/abs/2210.09245v3</link><description>3D grasp synthesis generates grasping poses given an input object. Existingworks tackle the problem by learning a direct mapping from objects to thedistributions of grasping poses. However, because the physical contact issensitive to small changes in pose, the high-nonlinear mapping between 3Dobject representation to valid poses is considerably non-smooth, leading topoor generation efficiency and restricted generality. To tackle the challenge,we introduce an intermediate variable for grasp contact areas to constrain thegrasp generation; in other words, we factorize the mapping into two sequentialstages by assuming that grasping poses are fully constrained given contactmaps: 1) we first learn contact map distributions to generate the potentialcontact maps for grasps; 2) then learn a mapping from the contact maps to thegrasping poses. Further, we propose a penetration-aware optimization with thegenerated contacts as a consistency constraint for grasp refinement. Extensivevalidations on two public datasets show that our method outperformsstate-of-the-art methods regarding grasp generation on various metrics.</description><author>Haoming Li, Xinzhuo Lin, Yang Zhou, Xiang Li, Yuchi Huo, Jiming Chen, Qi Ye</author><pubDate>Sat, 06 May 2023 08:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.09245v3</guid></item><item><title>OriCon3D: Effective 3D Object Detection using Orientation and Confidence</title><link>http://arxiv.org/abs/2304.14484v1</link><description>We introduce a technique for detecting 3D objects and estimating theirposition from a single image. Our method is built on top of a similarstate-of-the-art technique [1], but with improved accuracy. The approachfollowed in this research first estimates common 3D properties of an objectusing a Deep Convolutional Neural Network (DCNN), contrary to other frameworksthat only leverage centre-point predictions. We then combine these estimateswith geometric constraints provided by a 2D bounding box to produce a complete3D bounding box. The first output of our network estimates the 3D objectorientation using a discrete-continuous loss [1]. The second output predictsthe 3D object dimensions with minimal variance. Here we also present ourextensions by augmenting light-weight feature extractors and a customizedmultibin architecture. By combining these estimates with the geometricconstraints of the 2D bounding box, we can accurately (or comparatively)determine the 3D object pose better than our baseline [1] on the KITTI 3Ddetection benchmark [2].</description><author>Dhyey Manish Rajani, Rahul Kashyap Swayampakula, Surya Pratap Singh</author><pubDate>Thu, 27 Apr 2023 20:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14484v1</guid></item><item><title>End-to-end Weakly-supervised Single-stage Multiple 3D Hand Mesh Reconstruction from a Single RGB Image</title><link>http://arxiv.org/abs/2204.08154v3</link><description>In this paper, we consider the challenging task of simultaneously locatingand recovering multiple hands from a single 2D image. Previous studies eitherfocus on single hand reconstruction or solve this problem in a multi-stage way.Moreover, the conventional two-stage pipeline firstly detects hand areas, andthen estimates 3D hand pose from each cropped patch. To reduce thecomputational redundancy in preprocessing and feature extraction, for the firsttime, we propose a concise but efficient single-stage pipeline for multi-handreconstruction. Specifically, we design a multi-head auto-encoder structure,where each head network shares the same feature map and outputs the handcenter, pose and texture, respectively. Besides, we adopt a weakly-supervisedscheme to alleviate the burden of expensive 3D real-world data annotations. Tothis end, we propose a series of losses optimized by a stage-wise trainingscheme, where a multi-hand dataset with 2D annotations is generated based onthe publicly available single hand datasets. In order to further improve theaccuracy of the weakly supervised model, we adopt several feature consistencyconstraints in both single and multiple hand settings. Specifically, thekeypoints of each hand estimated from local features should be consistent withthe re-projected points predicted from global features. Extensive experimentson public benchmarks including FreiHAND, HO3D, InterHand2.6M and RHDdemonstrate that our method outperforms the state-of-the-art model-basedmethods in both weakly-supervised and fully-supervised manners. The code andmodels are available at {https://github.com/zijinxuxu/SMHR}.</description><author>Jinwei Ren, Jianke Zhu, Jialiang Zhang</author><pubDate>Sat, 06 May 2023 09:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.08154v3</guid></item><item><title>NeRF-GAN Distillation for Memory-Efficient 3D-Aware Generation with Convolutions</title><link>http://arxiv.org/abs/2303.12865v2</link><description>Pose-conditioned convolutional generative models struggle with high-quality3D-consistent image generation from single-view datasets, due to their lack ofsufficient 3D priors. Recently, the integration of Neural Radiance Fields(NeRFs) and generative models, such as Generative Adversarial Networks (GANs),has transformed 3D-aware generation from single-view images. NeRF-GANs exploitthe strong inductive bias of 3D neural representations and volumetric renderingat the cost of higher computational complexity. This study aims at revisitingpose-conditioned 2D GANs for memory-efficient 3D-aware generation at inferencetime by distilling 3D knowledge from pretrained NeRF-GANS. We propose a simpleand effective method, based on re-using the well-disentangled latent space of apre-trained NeRF-GAN in a pose-conditioned convolutional network to directlygenerate 3D-consistent images corresponding to the underlying 3Drepresentations. Experiments on several datasets demonstrate that the proposedmethod obtains results comparable with volumetric rendering in terms of qualityand 3D consistency while benefiting from the superior computational advantageof convolutional networks. The code will be available at:https://github.com/mshahbazi72/NeRF-GAN-Distillation</description><author>Mohamad Shahbazi, Evangelos Ntavelis, Alessio Tonioni, Edo Collins, Danda Pani Paudel, Martin Danelljan, Luc Van Gool</author><pubDate>Mon, 12 Jun 2023 17:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12865v2</guid></item><item><title>LPMM: Intuitive Pose Control for Neural Talking-Head Model via Landmark-Parameter Morphable Model</title><link>http://arxiv.org/abs/2305.10456v1</link><description>While current talking head models are capable of generating photorealistictalking head videos, they provide limited pose controllability. Most methodsrequire specific video sequences that should exactly contain the head posedesired, being far from user-friendly pose control. Three-dimensional morphablemodels (3DMM) offer semantic pose control, but they fail to capture certainexpressions. We present a novel method that utilizes parametric control of headorientation and facial expression over a pre-trained neural-talking head model.To enable this, we introduce a landmark-parameter morphable model (LPMM), whichoffers control over the facial landmark domain through a set of semanticparameters. Using LPMM, it is possible to adjust specific head pose factors,without distorting other facial attributes. The results show our approachprovides intuitive rig-like control over neural talking head models, allowingboth parameter and image-based inputs.</description><author>Kwangho Lee, Patrick Kwon, Myung Ki Lee, Namhyuk Ahn, Junsoo Lee</author><pubDate>Wed, 17 May 2023 07:11:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10456v1</guid></item><item><title>TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments</title><link>http://arxiv.org/abs/2306.02850v1</link><description>Although the estimation of 3D human pose and shape (HPS) is rapidlyprogressing, current methods still cannot reliably estimate moving humans inglobal coordinates, which is critical for many applications. This isparticularly challenging when the camera is also moving, entangling human andcamera motion. To address these issues, we adopt a novel 5D representation(space, time, and identity) that enables end-to-end reasoning about people inscenes. Our method, called TRACE, introduces several novel architecturalcomponents. Most importantly, it uses two new "maps" to reason about the 3Dtrajectory of people over time in camera, and world, coordinates. An additionalmemory unit enables persistent tracking of people even during long occlusions.TRACE is the first one-stage method to jointly recover and track 3D humans inglobal coordinates from dynamic cameras. By training it end-to-end, and usingfull image information, TRACE achieves state-of-the-art performance on trackingand HPS benchmarks. The code and dataset are released for research purposes.</description><author>Yu Sun, Qian Bao, Wu Liu, Tao Mei, Michael J. Black</author><pubDate>Mon, 05 Jun 2023 14:00:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02850v1</guid></item><item><title>Motion-DVAE: Unsupervised learning for fast human motion denoising</title><link>http://arxiv.org/abs/2306.05846v1</link><description>Pose and motion priors are crucial for recovering realistic and accuratehuman motion from noisy observations. Substantial progress has been made onpose and shape estimation from images, and recent works showed impressiveresults using priors to refine frame-wise predictions. However, a lot of motionpriors only model transitions between consecutive poses and are used intime-consuming optimization procedures, which is problematic for manyapplications requiring real-time motion capture. We introduce Motion-DVAE, amotion prior to capture the short-term dependencies of human motion. As part ofthe dynamical variational autoencoder (DVAE) models family, Motion-DVAEcombines the generative capability of VAE models and the temporal modeling ofrecurrent architectures. Together with Motion-DVAE, we introduce anunsupervised learned denoising method unifying regression- andoptimization-based approaches in a single framework for real-time 3D human poseestimation. Experiments show that the proposed approach reaches competitiveperformance with state-of-the-art methods while being much faster.</description><author>Guénolé Fiche, Simon Leglaive, Xavier Alameda-Pineda, Renaud Séguier</author><pubDate>Fri, 09 Jun 2023 13:18:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05846v1</guid></item><item><title>Learning Monocular Depth in Dynamic Environment via Context-aware Temporal Attention</title><link>http://arxiv.org/abs/2305.07397v1</link><description>The monocular depth estimation task has recently revealed encouragingprospects, especially for the autonomous driving task. To tackle the ill-posedproblem of 3D geometric reasoning from 2D monocular images, multi-framemonocular methods are developed to leverage the perspective correlationinformation from sequential temporal frames. However, moving objects such ascars and trains usually violate the static scene assumption, leading to featureinconsistency deviation and misaligned cost values, which would mislead theoptimization algorithm. In this work, we present CTA-Depth, a Context-awareTemporal Attention guided network for multi-frame monocular Depth estimation.Specifically, we first apply a multi-level attention enhancement module tointegrate multi-level image features to obtain an initial depth and poseestimation. Then the proposed CTA-Refiner is adopted to alternatively optimizethe depth and pose. During the refinement process, context-aware temporalattention (CTA) is developed to capture the global temporal-contextcorrelations to maintain the feature consistency and estimation integrity ofmoving objects. In particular, we propose a long-range geometry embedding (LGE)module to produce a long-range temporal geometry prior. Our approach achievessignificant improvements over state-of-the-art approaches on three benchmarkdatasets.</description><author>Zizhang Wu, Zhuozheng Li, Zhi-Gang Fan, Yunzhe Wu, Yuanzhu Gan, Jian Pu, Xianzhi Li</author><pubDate>Fri, 12 May 2023 12:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07397v1</guid></item><item><title>Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications</title><link>http://arxiv.org/abs/2302.05763v2</link><description>Human-robot interaction (HRI) research is progressively addressingmulti-party scenarios, where a robot interacts with more than one human user atthe same time. Conversely, research is still at an early stage for human-robotcollaboration. The use of machine learning techniques to handle such type ofcollaboration requires data that are less feasible to produce than in a typicalHRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRCapplications. Based upon these concepts, this study also proposes analternative way of gathering data regarding multi-user activity, by collectingdata related to single users and merging them in post-processing, to reduce theeffort involved in producing recordings of pair settings. To validate thisstatement, 3D skeleton poses of activity of single users were collected andmerged in pairs. After this, such datapoints were used to separately train along short-term memory (LSTM) network and a variational autoencoder (VAE)composed of spatio-temporal graph convolutional networks (STGCN) to recognisethe joint activities of the pairs of people. The results showed that it ispossible to make use of data collected in this way for pair HRC settings andget similar performances compared to using training data regarding groups ofusers recorded under the same settings, relieving from the technicaldifficulties involved in producing these data. The related code and collected data are publicly available.</description><author>Francesco Semeraro, Jon Carberry, Angelo Cangelosi</author><pubDate>Fri, 05 May 2023 16:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05763v2</guid></item><item><title>Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models</title><link>http://arxiv.org/abs/2305.11870v2</link><description>We propose a 3D generation pipeline that uses diffusion models to generaterealistic human digital avatars. Due to the wide variety of human identities,poses, and stochastic details, the generation of 3D human meshes has been achallenging problem. To address this, we decompose the problem into 2D normalmap generation and normal map-based 3D reconstruction. Specifically, we firstsimultaneously generate realistic normal maps for the front and backside of aclothed human, dubbed dual normal maps, using a pose-conditional diffusionmodel. For 3D reconstruction, we ``carve'' the prior SMPL-X mesh to a detailed3D mesh according to the normal maps through mesh optimization. To furtherenhance the high-frequency details, we present a diffusion resampling scheme onboth body and facial regions, thus encouraging the generation of realisticdigital avatars. We also seamlessly incorporate a recent text-to-imagediffusion model to support text-based human identity control. Our method,namely, Chupa, is capable of generating realistic 3D clothed humans with betterperceptual quality and identity variety.</description><author>Byungjun Kim, Patrick Kwon, Kwangho Lee, Myunggi Lee, Sookwan Han, Daesik Kim, Hanbyul Joo</author><pubDate>Mon, 29 May 2023 08:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11870v2</guid></item><item><title>BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives</title><link>http://arxiv.org/abs/2306.04166v2</link><description>Implicit neural representation has emerged as a powerful method forreconstructing 3D scenes from 2D images. Given a set of camera poses andassociated images, the models can be trained to synthesize novel, unseen views.In order to expand the use cases for implicit neural representations, we needto incorporate camera pose estimation capabilities as part of therepresentation learning, as this is necessary for reconstructing scenes fromreal-world video sequences where cameras are generally not being tracked.Existing approaches like COLMAP and, most recently, bundle-adjusting neuralradiance field methods often suffer from lengthy processing times. These delaysranging from hours to days, arise from laborious feature matching, hardwarelimitations, dense point sampling, and long training times required by amulti-layer perceptron structure with a large number of parameters. To addressthese challenges, we propose a framework called bundle-adjusting acceleratedneural graphics primitives (BAA-NGP). Our approach leverages acceleratedsampling and hash encoding to expedite both pose refinement/estimation and 3Dscene reconstruction. Experimental results demonstrate that our method achievesa more than 10 to 20 $\times$ speed improvement in novel view synthesiscompared to other bundle-adjusting neural radiance field methods withoutsacrificing the quality of pose estimation.</description><author>Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey Supikov, Michael Yip</author><pubDate>Fri, 09 Jun 2023 19:10:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04166v2</guid></item><item><title>BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives</title><link>http://arxiv.org/abs/2306.04166v1</link><description>Implicit neural representation has emerged as a powerful method forreconstructing 3D scenes from 2D images. Given a set of camera poses andassociated images, the models can be trained to synthesize novel, unseen views.In order to expand the use cases for implicit neural representations, we needto incorporate camera pose estimation capabilities as part of therepresentation learning, as this is necessary for reconstructing scenes fromreal-world video sequences where cameras are generally not being tracked.Existing approaches like COLMAP and, most recently, bundle-adjusting neuralradiance field methods often suffer from lengthy processing times. These delaysranging from hours to days, arise from laborious feature matching, hardwarelimitations, dense point sampling, and long training times required by amulti-layer perceptron structure with a large number of parameters. To addressthese challenges, we propose a framework called bundle-adjusting acceleratedneural graphics primitives (BAA-NGP). Our approach leverages acceleratedsampling and hash encoding to expedite both pose refinement/estimation and 3Dscene reconstruction. Experimental results demonstrate that our method achievesa more than 10 to 20 $\times$ speed improvement in novel view synthesiscompared to other bundle-adjusting neural radiance field methods withoutsacrificing the quality of pose estimation.</description><author>Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey Supikov, Michael Yip</author><pubDate>Wed, 07 Jun 2023 06:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04166v1</guid></item><item><title>End-to-end 2D-3D Registration between Image and LiDAR Point Cloud for Vehicle Localization</title><link>http://arxiv.org/abs/2306.11346v1</link><description>Robot localization using a previously built map is essential for a variety oftasks including highly accurate navigation and mobile manipulation. A popularapproach to robot localization is based on image-to-point cloud registration,which combines illumination-invariant LiDAR-based mapping with economicalimage-based localization. However, the recent works for image-to-point cloudregistration either divide the registration into separate modules or projectthe point cloud to the depth image to register the RGB and depth images. Inthis paper, we present I2PNet, a novel end-to-end 2D-3D registration network.I2PNet directly registers the raw 3D point cloud with the 2D RGB image usingdifferential modules with a unique target. The 2D-3D cost volume module fordifferential 2D-3D association is proposed to bridge feature extraction andpose regression. 2D-3D cost volume module implicitly constructs the softpoint-to-pixel correspondence on the intrinsic-independent normalized plane ofthe pinhole camera model. Moreover, we introduce an outlier mask predictionmodule to filter the outliers in the 2D-3D association before pose regression.Furthermore, we propose the coarse-to-fine 2D-3D registration architecture toincrease localization accuracy. We conduct extensive localization experimentson the KITTI Odometry and nuScenes datasets. The results demonstrate thatI2PNet outperforms the state-of-the-art by a large margin. In addition, I2PNethas a higher efficiency than the previous works and can perform thelocalization in real-time. Moreover, we extend the application of I2PNet to thecamera-LiDAR online calibration and demonstrate that I2PNet outperforms recentapproaches on the online calibration task.</description><author>Guangming Wang, Yu Zheng, Yanfeng Guo, Zhe Liu, Yixiang Zhu, Wolfram Burgard, Hesheng Wang</author><pubDate>Tue, 20 Jun 2023 08:28:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11346v1</guid></item><item><title>NeurOCS: Neural NOCS Supervision for Monocular 3D Object Localization</title><link>http://arxiv.org/abs/2305.17763v1</link><description>Monocular 3D object localization in driving scenes is a crucial task, butchallenging due to its ill-posed nature. Estimating 3D coordinates for eachpixel on the object surface holds great potential as it provides dense 2D-3Dgeometric constraints for the underlying PnP problem. However, high-qualityground truth supervision is not available in driving scenes due to sparsity andvarious artifacts of Lidar data, as well as the practical infeasibility ofcollecting per-instance CAD models. In this work, we present NeurOCS, aframework that uses instance masks and 3D boxes as input to learn 3D objectshapes by means of differentiable rendering, which further serves assupervision for learning dense object coordinates. Our approach rests oninsights in learning a category-level shape prior directly from real drivingscenes, while properly handling single-view ambiguities. Furthermore, we studyand make critical design choices to learn object coordinates more effectivelyfrom an object-centric view. Altogether, our framework leads to newstate-of-the-art in monocular 3D localization that ranks 1st on theKITTI-Object benchmark among published monocular methods.</description><author>Zhixiang Min, Bingbing Zhuang, Samuel Schulter, Buyu Liu, Enrique Dunn, Manmohan Chandraker</author><pubDate>Sun, 28 May 2023 17:18:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17763v1</guid></item><item><title>CAD-Estate: Large-scale CAD Model Annotation in RGB Videos</title><link>http://arxiv.org/abs/2306.09011v1</link><description>We propose a method for annotating videos of complex multi-object scenes witha globally-consistent 3D representation of the objects. We annotate each objectwith a CAD model from a database, and place it in the 3D coordinate frame ofthe scene with a 9-DoF pose transformation. Our method is semi-automatic andworks on commonly-available RGB videos, without requiring a depth sensor. Manysteps are performed automatically, and the tasks performed by humans aresimple, well-specified, and require only limited reasoning in 3D. This makesthem feasible for crowd-sourcing and has allowed us to construct a large-scaledataset by annotating real-estate videos from YouTube. Our dataset CAD-Estateoffers 108K instances of 12K unique CAD models placed in the 3D representationsof 21K videos. In comparison to Scan2CAD, the largest existing dataset with CADmodel annotations on real scenes, CAD-Estate has 8x more instances and 4x moreunique CAD models. We showcase the benefits of pre-training a Mask2CAD model onCAD-Estate for the task of automatic 3D object reconstruction and poseestimation, demonstrating that it leads to improvements on the popular Scan2CADbenchmark. We will release the data by mid July 2023.</description><author>Kevis-Kokitsi Maninis, Stefan Popov, Matthias Nießner, Vittorio Ferrari</author><pubDate>Thu, 15 Jun 2023 11:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09011v1</guid></item><item><title>StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation</title><link>http://arxiv.org/abs/2305.19012v1</link><description>The recent advancements in image-text diffusion models have stimulatedresearch interest in large-scale 3D generative models. Nevertheless, thelimited availability of diverse 3D resources presents significant challenges tolearning. In this paper, we present a novel method for generating high-quality,stylized 3D avatars that utilizes pre-trained image-text diffusion models fordata generation and a Generative Adversarial Network (GAN)-based 3D generationnetwork for training. Our method leverages the comprehensive priors ofappearance and geometry offered by image-text diffusion models to generatemulti-view images of avatars in various styles. During data generation, weemploy poses extracted from existing 3D models to guide the generation ofmulti-view images. To address the misalignment between poses and images indata, we investigate view-specific prompts and develop a coarse-to-finediscriminator for GAN training. We also delve into attribute-related prompts toincrease the diversity of the generated avatars. Additionally, we develop alatent diffusion model within the style space of StyleGAN to enable thegeneration of avatars based on image inputs. Our approach demonstrates superiorperformance over current state-of-the-art methods in terms of visual qualityand diversity of the produced avatars.</description><author>Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen</author><pubDate>Tue, 30 May 2023 14:09:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19012v1</guid></item><item><title>Tame a Wild Camera: In-the-Wild Monocular Camera Calibration</title><link>http://arxiv.org/abs/2306.10988v1</link><description>3D sensing for monocular in-the-wild images, e.g., depth estimation and 3Dobject detection, has become increasingly important. However, the unknownintrinsic parameter hinders their development and deployment. Previous methodsfor the monocular camera calibration rely on specific 3D objects or stronggeometry prior, such as using a checkerboard or imposing a Manhattan Worldassumption. This work solves the problem from the other perspective byexploiting the monocular 3D prior. Our method is assumption-free and calibratesthe complete $4$ Degree-of-Freedom (DoF) intrinsic parameters. First, wedemonstrate intrinsic is solved from two well-studied monocular priors, i.e.,monocular depthmap, and surface normal map. However, this solution imposes alow-bias and low-variance requirement for depth estimation. Alternatively, weintroduce a novel monocular 3D prior, the incidence field, defined as theincidence rays between points in 3D space and pixels in the 2D imaging plane.The incidence field is a pixel-wise parametrization of the intrinsic invariantto image cropping and resizing. With the estimated incidence field, a robustRANSAC algorithm recovers intrinsic. We demonstrate the effectiveness of ourmethod by showing superior performance on synthetic and zero-shot testingdatasets. Beyond calibration, we demonstrate downstream applications in imagemanipulation detection &amp; restoration, uncalibrated two-view pose estimation,and 3D sensing. Codes, models, and data will be held inhttps://github.com/ShngJZ/WildCamera.</description><author>Shengjie Zhu, Abhinav Kumar, Masa Hu, Xiaoming Liu</author><pubDate>Mon, 19 Jun 2023 15:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10988v1</guid></item><item><title>Learning Pose Image Manifolds Using Geometry-Preserving GANs and Elasticae</title><link>http://arxiv.org/abs/2305.10513v1</link><description>This paper investigates the challenge of learning image manifolds,specifically pose manifolds, of 3D objects using limited training data. Itproposes a DNN approach to manifold learning and for predicting images ofobjects for novel, continuous 3D rotations. The approach uses two distinctconcepts: (1) Geometric Style-GAN (Geom-SGAN), which maps images tolow-dimensional latent representations and maintains the (first-order) manifoldgeometry. That is, it seeks to preserve the pairwise distances between basepoints and their tangent spaces, and (2) uses Euler's elastica to smoothlyinterpolate between directed points (points + tangent directions) in thelow-dimensional latent space. When mapped back to the larger image space, theresulting interpolations resemble videos of rotating objects. Extensiveexperiments establish the superiority of this framework in learning paths onrotation manifolds, both visually and quantitatively, relative tostate-of-the-art GANs and VAEs.</description><author>Shenyuan Liang, Pavan Turaga, Anuj Srivastava</author><pubDate>Wed, 17 May 2023 19:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10513v1</guid></item><item><title>RelPose++: Recovering 6D Poses from Sparse-view Observations</title><link>http://arxiv.org/abs/2305.04926v1</link><description>We address the task of estimating 6D camera poses from sparse-view image sets(2-8 images). This task is a vital pre-processing stage for nearly allcontemporary (neural) reconstruction algorithms but remains challenging givensparse views, especially for objects with visual symmetries and texture-lesssurfaces. We build on the recent RelPose framework which learns a network thatinfers distributions over relative rotations over image pairs. We extend thisapproach in two key ways; first, we use attentional transformer layers toprocess multiple images jointly, since additional views of an object mayresolve ambiguous symmetries in any given image pair (such as the handle of amug that becomes visible in a third view). Second, we augment this network toalso report camera translations by defining an appropriate coordinate systemthat decouples the ambiguity in rotation estimation from translationprediction. Our final system results in large improvements in 6D poseprediction over prior art on both seen and unseen object categories and alsoenables pose estimation and 3D reconstruction for in-the-wild objects.</description><author>Amy Lin, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Mon, 08 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04926v1</guid></item><item><title>gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction</title><link>http://arxiv.org/abs/2304.11970v1</link><description>Signed distance functions (SDFs) is an attractive framework that has recentlyshown promising results for 3D shape reconstruction from images. SDFsseamlessly generalize to different shape resolutions and topologies but lackexplicit modelling of the underlying 3D geometry. In this work, we exploit thehand structure and use it as guidance for SDF-based shape reconstruction. Inparticular, we address reconstruction of hands and manipulated objects frommonocular RGB images. To this end, we estimate poses of hands and objects anduse them to guide 3D reconstruction. More specifically, we predict kinematicchains of pose transformations and align SDFs with highly-articulated handposes. We improve the visual features of 3D points with geometry alignment andfurther leverage temporal information to enhance the robustness to occlusionand motion blurs. We conduct extensive experiments on the challenging ObMan andDexYCB benchmarks and demonstrate significant improvements of the proposedmethod over the state of the art.</description><author>Zerui Chen, Shizhe Chen, Cordelia Schmid, Ivan Laptev</author><pubDate>Mon, 24 Apr 2023 11:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11970v1</guid></item><item><title>ObPose: Leveraging Pose for Object-Centric Scene Inference and Generation in 3D</title><link>http://arxiv.org/abs/2206.03591v3</link><description>We present ObPose, an unsupervised object-centric inference and generationmodel which learns 3D-structured latent representations from RGB-D scenes.Inspired by prior art in 2D representation learning, ObPose considers afactorised latent space, separately encoding object location (where) andappearance (what). ObPose further leverages an object's pose (i.e. location andorientation), defined via a minimum volume principle, as a novel inductive biasfor learning the where component. To achieve this, we propose an efficient,voxelised approximation approach to recover the object shape directly from aneural radiance field (NeRF). As a consequence, ObPose models each scene as acomposition of NeRFs, richly representing individual objects. To evaluate thequality of the learned representations, ObPose is evaluated quantitatively onthe YCB, MultiShapeNet, and CLEVR datatasets for unsupervised scenesegmentation, outperforming the current state-of-the-art in 3D scene inference(ObSuRF) by a significant margin. Generative results provide qualitativedemonstration that the same ObPose model can both generate novel scenes andflexibly edit the objects in them. These capacities again reflect the qualityof the learned latents and the benefits of disentangling the where and whatcomponents of a scene. Key design choices made in the ObPose encoder arevalidated with ablations.</description><author>Yizhe Wu, Oiwi Parker Jones, Ingmar Posner</author><pubDate>Fri, 09 Jun 2023 21:18:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.03591v3</guid></item><item><title>CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction</title><link>http://arxiv.org/abs/2304.14633v1</link><description>Recent advances in neural reconstruction using posed image sequences havemade remarkable progress. However, due to the lack of depth information,existing volumetric-based techniques simply duplicate 2D image features of theobject surface along the entire camera ray. We contend this duplicationintroduces noise in empty and occluded spaces, posing challenges for producinghigh-quality 3D geometry. Drawing inspiration from traditional multi-viewstereo methods, we propose an end-to-end 3D neural reconstruction frameworkCVRecon, designed to exploit the rich geometric embedding in the cost volumesto facilitate 3D geometric feature learning. Furthermore, we presentRay-contextual Compensated Cost Volume (RCCV), a novel 3D geometric featurerepresentation that encodes view-dependent information with improved integrityand robustness. Through comprehensive experiments, we demonstrate that ourapproach significantly improves the reconstruction quality in various metricsand recovers clear fine details of the 3D geometries. Our extensive ablationstudies provide insights into the development of effective 3D geometric featurelearning schemes. Project page: https://cvrecon.ziyue.cool/</description><author>Ziyue Feng, Leon Yang, Pengsheng Guo, Bing Li</author><pubDate>Fri, 28 Apr 2023 06:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14633v1</guid></item><item><title>ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation</title><link>http://arxiv.org/abs/2305.01618v1</link><description>We propose a new dataset and a novel approach to learning hand-objectinteraction priors for hand and articulated object pose estimation. We firstcollect a dataset using visual teleoperation, where the human operator candirectly play within a physical simulator to manipulate the articulatedobjects. We record the data and obtain free and accurate annotations on objectposes and contact information from the simulator. Our system only requires aniPhone to record human hand motion, which can be easily scaled up and largelylower the costs of data and annotation collection. With this data, we learn 3Dinteraction priors including a discriminator (in a GAN) capturing thedistribution of how object parts are arranged, and a diffusion model whichgenerates the contact regions on articulated objects, guiding the hand poseestimation. Such structural and contact priors can easily transfer toreal-world data with barely any domain gap. By using our data and learnedpriors, our method significantly improves the performance on joint hand andarticulated object poses estimation over the existing state-of-the-art methods.The project is available at https://zehaozhu.github.io/ContactArt/ .</description><author>Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang</author><pubDate>Tue, 02 May 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01618v1</guid></item><item><title>Domain-Adaptive Full-Face Gaze Estimation via Novel-View-Synthesis and Feature Disentanglement</title><link>http://arxiv.org/abs/2305.16140v1</link><description>Along with the recent development of deep neural networks, appearance-basedgaze estimation has succeeded considerably when training and testing within thesame domain. Compared to the within-domain task, the variance of differentdomains makes the cross-domain performance drop severely, preventing gazeestimation deployment in real-world applications. Among all the factors, rangesof head pose and gaze are believed to play a significant role in the finalperformance of gaze estimation, while collecting large ranges of data isexpensive. This work proposes an effective model training pipeline consistingof a training data synthesis and a gaze estimation model for unsuperviseddomain adaptation. The proposed data synthesis leverages the single-image 3Dreconstruction to expand the range of the head poses from the source domainwithout requiring a 3D facial shape dataset. To bridge the inevitable gapbetween synthetic and real images, we further propose an unsupervised domainadaptation method suitable for synthetic full-face data. We propose adisentangling autoencoder network to separate gaze-related features andintroduce background augmentation consistency loss to utilize thecharacteristics of the synthetic source domain. Through comprehensiveexperiments, we show that the model only using monocular-reconstructedsynthetic training data can perform comparably to real data with a large labelrange. Our proposed domain adaptation approach further improves the performanceon multiple target domains. The code and data will be available at\url{https://github.com/ut-vision/AdaptiveGaze}.</description><author>Jiawei Qin, Takuru Shimoyama, Xucong Zhang, Yusuke Sugano</author><pubDate>Thu, 25 May 2023 16:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16140v1</guid></item><item><title>S-Graphs+: Real-time Localization and Mapping leveraging Hierarchical Representations</title><link>http://arxiv.org/abs/2212.11770v3</link><description>In this paper, we present an evolved version of Situational Graphs, whichjointly models in a single optimizable factor graph (1) a pose graph, as a setof robot keyframes comprising associated measurements and robot poses, and (2)a 3D scene graph, as a high-level representation of the environment thatencodes its different geometric elements with semantic attributes and therelational information between them. Specifically, our S-Graphs+ is a novel four-layered factor graph thatincludes: (1) a keyframes layer with robot pose estimates, (2) a walls layerrepresenting wall surfaces, (3) a rooms layer encompassing sets of wall planes,and (4) a floors layer gathering the rooms within a given floor level. Theabove graph is optimized in real-time to obtain a robust and accurate estimateof the robots pose and its map, simultaneously constructing and leveraginghigh-level information of the environment. To extract this high-levelinformation, we present novel room and floor segmentation algorithms utilizingthe mapped wall planes and free-space clusters. We tested S-Graphs+ on multiple datasets, including simulated and real dataof indoor environments from varying construction sites, and on a real publicdataset of several indoor office areas. On average over our datasets, S-Graphs+outperforms the accuracy of the second-best method by a margin of 10.67%, whileextending the robot situational awareness by a richer scene model. Moreover, wemake the software available as a docker file.</description><author>Hriday Bavle, Jose Luis Sanchez-Lopez, Muhammad Shaheer, Javier Civera, Holger Voos</author><pubDate>Fri, 26 May 2023 10:50:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.11770v3</guid></item><item><title>Depth-based 6DoF Object Pose Estimation using Swin Transformer</title><link>http://arxiv.org/abs/2303.02133v2</link><description>Accurately estimating the 6D pose of objects is crucial for manyapplications, such as robotic grasping, autonomous driving, and augmentedreality. However, this task becomes more challenging in poor lightingconditions or when dealing with textureless objects. To address this issue,depth images are becoming an increasingly popular choice due to theirinvariance to a scene's appearance and the implicit incorporation of essentialgeometric characteristics. However, fully leveraging depth information toimprove the performance of pose estimation remains a difficult andunder-investigated problem. To tackle this challenge, we propose a novelframework called SwinDePose, that uses only geometric information from depthimages to achieve accurate 6D pose estimation. SwinDePose first calculates theangles between each normal vector defined in a depth image and the threecoordinate axes in the camera coordinate system. The resulting angles are thenformed into an image, which is encoded using Swin Transformer. Additionally, weapply RandLA-Net to learn the representations from point clouds. The resultingimage and point clouds embeddings are concatenated and fed into a semanticsegmentation module and a 3D keypoints localization module. Finally, weestimate 6D poses using a least-square fitting approach based on the targetobject's predicted semantic mask and 3D keypoints. In experiments on theLineMod and Occlusion LineMod datasets, SwinDePose outperforms existingstate-of-the-art methods for 6D object pose estimation using depth images. Thisdemonstrates the effectiveness of our approach and highlights its potential forimproving performance in real-world scenarios. Our code is athttps://github.com/zhujunli1993/SwinDePose.</description><author>Zhujun Li, Ioannis Stamos</author><pubDate>Thu, 27 Apr 2023 19:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02133v2</guid></item><item><title>D3L: Decomposition of 3D Rotation and Lift from 2D Joint to 3D for Human Mesh Recovery</title><link>http://arxiv.org/abs/2306.06406v1</link><description>Existing methods for 3D human mesh recovery always directly estimate SMPLparameters, which involve both joint rotations and shape parameters. However,these methods present rotation semantic ambiguity, rotation error accumulation,and shape estimation overfitting, which also leads to errors in the estimatedpose. Additionally, these methods have not efficiently leveraged theadvancements in another hot topic, human pose estimation. To address theseissues, we propose a novel approach, Decomposition of 3D Rotation and Lift from2D Joint to 3D mesh (D3L). We disentangle 3D joint rotation into bone directionand bone twist direction so that the human mesh recovery task is broken downinto estimation of pose, twist, and shape, which can be handled independently.Then we design a 2D-to-3D lifting network for estimating twist direction and 3Djoint position from 2D joint position sequences and introduce a nonlinearoptimization method for fitting shape parameters and bone directions. Ourapproach can leverage human pose estimation methods, and avoid pose errorsintroduced by shape estimation overfitting. We conduct experiments on theHuman3.6M dataset and demonstrate improved performance compared to existingmethods by a large margin.</description><author>Xiaoyang Hao, Han Li, Jun Cheng, Lei Wang</author><pubDate>Sat, 10 Jun 2023 11:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06406v1</guid></item><item><title>Digital Twin-Based 3D Map Management for Edge-Assisted Mobile Augmented Reality</title><link>http://arxiv.org/abs/2305.16571v1</link><description>In this paper, we design a 3D map management scheme for edge-assisted mobileaugmented reality (MAR) to support the pose estimation of individual MARdevice, which uploads camera frames to an edge server. Our objective is tominimize the pose estimation uncertainty of the MAR device by periodicallyselecting a proper set of camera frames for uploading to update the 3D map. Toaddress the challenges of the dynamic uplink data rate and the time-varyingpose of the MAR device, we propose a digital twin (DT)-based approach to 3D mapmanagement. First, a DT is created for the MAR device, which emulates 3D mapmanagement based on predicting subsequent camera frames. Second, a model-basedreinforcement learning (MBRL) algorithm is developed, utilizing the datacollected from both the actual and the emulated data to manage the 3D map. Withextensive emulated data provided by the DT, the MBRL algorithm can quicklyprovide an adaptive map management policy in a highly dynamic environment.Simulation results demonstrate that the proposed DT-based 3D map managementoutperforms benchmark schemes by achieving lower pose estimation uncertaintyand higher data efficiency in dynamic environments.</description><author>Conghao Zhou, Jie Gao, Mushu Li, Nan Cheng, Xuemin Shen, Weihua Zhuang</author><pubDate>Fri, 26 May 2023 02:38:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16571v1</guid></item><item><title>POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference</title><link>http://arxiv.org/abs/2305.15727v1</link><description>Despite the significant progress in six degrees-of-freedom (6DoF) object poseestimation, existing methods have limited applicability in real-world scenariosinvolving embodied agents and downstream 3D vision tasks. These limitationsmainly come from the necessity of 3D models, closed-category detection, and alarge number of densely annotated support views. To mitigate this issue, wepropose a general paradigm for object pose estimation, called Promptable ObjectPose Estimation (POPE). The proposed approach POPE enables zero-shot 6DoFobject pose estimation for any target object in any scene, while only a singlereference is adopted as the support view. To achieve this, POPE leverages thepower of the pre-trained large-scale 2D foundation model, employs a frameworkwith hierarchical feature representation and 3D geometry principles. Moreover,it estimates the relative camera pose between object prompts and the targetobject in new views, enabling both two-view and multi-view 6DoF pose estimationtasks. Comprehensive experimental results demonstrate that POPE exhibitsunrivaled robust performance in zero-shot settings, by achieving a significantreduction in the averaged Median Pose Error by 52.38% and 50.47% on the LINEMODand OnePose datasets, respectively. We also conduct more challenging testingsin causally captured images (see Figure 1), which further demonstrates therobustness of POPE. Project page can be found withhttps://paulpanwang.github.io/POPE/.</description><author>Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Dejia Xu, Hanwen Jiang, Zhangyang Wang</author><pubDate>Thu, 25 May 2023 06:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15727v1</guid></item><item><title>Towards Realistic Generative 3D Face Models</title><link>http://arxiv.org/abs/2304.12483v1</link><description>In recent years, there has been significant progress in 2D generative facemodels fueled by applications such as animation, synthetic data generation, anddigital avatars. However, due to the absence of 3D information, these 2D modelsoften struggle to accurately disentangle facial attributes like pose,expression, and illumination, limiting their editing capabilities. To addressthis limitation, this paper proposes a 3D controllable generative face model toproduce high-quality albedo and precise 3D shape leveraging existing 2Dgenerative models. By combining 2D face generative models with semantic facemanipulation, this method enables editing of detailed 3D rendered faces. Theproposed framework utilizes an alternating descent optimization approach overshape and albedo. Differentiable rendering is used to train high-quality shapesand albedo without 3D supervision. Moreover, this approach outperforms thestate-of-the-art (SOTA) methods in the well-known NoW benchmark for shapereconstruction. It also outperforms the SOTA reconstruction models inrecovering rendered faces' identities across novel poses by an average of 10%.Additionally, the paper demonstrates direct control of expressions in 3D facesby exploiting latent space leading to text-based editing of 3D faces.</description><author>Aashish Rai, Hiresh Gupta, Ayush Pandey, Francisco Vicente Carrasco, Shingo Jason Takagi, Amaury Aubel, Daeil Kim, Aayush Prakash, Fernando de la Torre</author><pubDate>Mon, 24 Apr 2023 23:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12483v1</guid></item><item><title>Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback</title><link>http://arxiv.org/abs/2305.15808v1</link><description>Generating and editing a 3D scene guided by natural language poses achallenge, primarily due to the complexity of specifying the positionalrelations and volumetric changes within the 3D space. Recent advancements inLarge Language Models (LLMs) have demonstrated impressive reasoning,conversational, and zero-shot generation abilities across various domains.Surprisingly, these models also show great potential in realizing andinterpreting the 3D space. In light of this, we propose a novel language-guidedinteractive 3D generation system, dubbed LI3D, that integrates LLMs as a 3Dlayout interpreter into the off-the-shelf layout-to-3D generative models,allowing users to flexibly and interactively generate visual content.Specifically, we design a versatile layout structure base on the bounding boxesand semantics to prompt the LLMs to model the spatial generation and reasoningfrom language. Our system also incorporates LLaVA, a large language and visionassistant, to provide generative feedback from the visual aspect for improvingthe visual quality of generated content. We validate the effectiveness of LI3D,primarily in 3D generation and editing through multi-round interactions, whichcan be flexibly extended to 2D generation and editing. Various experimentsdemonstrate the potential benefits of incorporating LLMs in generative AI forapplications, e.g., metaverse. Moreover, we benchmark the layout reasoningperformance of LLMs with neural visual artist tasks, revealing their emergentability in the spatial layout domain.</description><author>Yiqi Lin, Hao Wu, Ruichen Wang, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang</author><pubDate>Thu, 25 May 2023 08:43:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15808v1</guid></item><item><title>3D Registration with Maximal Cliques</title><link>http://arxiv.org/abs/2305.10854v1</link><description>As a fundamental problem in computer vision, 3D point cloud registration(PCR) aims to seek the optimal pose to align a point cloud pair. In this paper,we present a 3D registration method with maximal cliques (MAC). The key insightis to loosen the previous maximum clique constraint, and mine more localconsensus information in a graph for accurate pose hypotheses generation: 1) Acompatibility graph is constructed to render the affinity relationship betweeninitial correspondences. 2) We search for maximal cliques in the graph, each ofwhich represents a consensus set. We perform node-guided clique selection then,where each node corresponds to the maximal clique with the greatest graphweight. 3) Transformation hypotheses are computed for the selected cliques bythe SVD algorithm and the best hypothesis is used to perform registration.Extensive experiments on U3M, 3DMatch, 3DLoMatch and KITTI demonstrate that MACeffectively increases registration accuracy, outperforms variousstate-of-the-art methods and boosts the performance of deep-learned methods.MAC combined with deep-learned methods achieves state-of-the-art registrationrecall of 95.7% / 78.9% on 3DMatch / 3DLoMatch.</description><author>Xiyu Zhang, Jiaqi Yang, Shikun Zhang, Yanning Zhang</author><pubDate>Thu, 18 May 2023 11:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10854v1</guid></item><item><title>CloudWalker: Random walks for 3D point cloud shape analysis</title><link>http://arxiv.org/abs/2112.01050v4</link><description>Point clouds are gaining prominence as a method for representing 3D shapes,but their irregular structure poses a challenge for deep learning methods. Inthis paper we propose CloudWalker, a novel method for learning 3D shapes usingrandom walks. Previous works attempt to adapt Convolutional Neural Networks(CNNs) or impose a grid or mesh structure to 3D point clouds. This workpresents a different approach for representing and learning the shape from agiven point set. The key idea is to impose structure on the point set bymultiple random walks through the cloud for exploring different regions of the3D object. Then we learn a per-point and per-walk representation and aggregatemultiple walk predictions at inference. Our approach achieves state-of-the-artresults for two 3D shape analysis tasks: classification and retrieval.</description><author>Adi Mesika, Yizhak Ben-Shabat, Ayellet Tal</author><pubDate>Wed, 17 May 2023 08:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.01050v4</guid></item><item><title>HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</title><link>http://arxiv.org/abs/2212.10428v4</link><description>Estimating the 6D pose of objects is a major 3D computer vision problem.Since the promising outcomes from instance-level approaches, research headsalso move towards category-level pose estimation for more practical applicationscenarios. However, unlike well-established instance-level pose datasets,available category-level datasets lack annotation quality and provided posequantity. We propose the new category-level 6D pose dataset HouseCat6Dfeaturing 1) Multi-modality of Polarimetric RGB and Depth (RGBD+P), 2) Highlydiverse 194 objects of 10 household object categories including 2photometrically challenging categories, 3) High-quality pose annotation with anerror range of only 1.35 mm to 1.74 mm, 4) 41 large-scale scenes with extensiveviewpoint coverage and occlusions, 5) Checkerboard-free environment throughoutthe entire scene, and 6) Additionally annotated dense 6D parallel-jaw grasps.Furthermore, we also provide benchmark results of state-of-the-artcategory-level pose estimation networks.</description><author>HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp, Guangyao Zhai, Hannah Schieber, Giulia Rizzoli, Pengyuan Wang, Hongcheng Zhao, Lorenzo Garattoni, Sven Meier, Daniel Roth, Nassir Navab, Benjamin Busam</author><pubDate>Wed, 26 Apr 2023 11:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10428v4</guid></item><item><title>Perspective-1-Ellipsoid: Formulation, Analysis and Solutions of the Camera Pose Estimation Problem from One Ellipse-Ellipsoid Correspondence</title><link>http://arxiv.org/abs/2208.12513v3</link><description>In computer vision, camera pose estimation from correspondences between 3Dgeometric entities and their projections into the image has been a widelyinvestigated problem. Although most state-of-the-art methods exploit low-levelprimitives such as points or lines, the emergence of very effective CNN-basedobject detectors in the recent years has paved the way to the use ofhigher-level features carrying semantically meaningful information. Pioneeringworks in that direction have shown that modelling 3D objects by ellipsoids and2D detections by ellipses offers a convenient manner to link 2D and 3D data.However, the mathematical formalism most often used in the related litteraturedoes not enable to easily distinguish ellipsoids and ellipses from otherquadrics and conics, leading to a loss of specificity potentially detrimentalin some developments. Moreover, the linearization process of the projectionequation creates an over-representation of the camera parameters, also possiblycausing an efficiency loss. In this paper, we therefore introduce anellipsoid-specific theoretical framework and demonstrate its beneficialproperties in the context of pose estimation. More precisely, we first showthat the proposed formalism enables to reduce the pose estimation problem to aposition or orientation-only estimation problem in which the remaining unknownscan be derived in closed-form. Then, we demonstrate that it can be furtherreduced to a 1 Degree-of-Freedom (1DoF) problem and provide the analyticalderivations of the pose as a function of that unique scalar unknown. Weillustrate our theoretical considerations by visual examples and include adiscussion on the practical aspects. Finally, we release this paper along withthe corresponding source code in order to contribute towards more efficientresolutions of ellipsoid-related pose estimation problems.</description><author>Vincent Gaudillière, Gilles Simon, Marie-Odile Berger</author><pubDate>Wed, 14 Jun 2023 13:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12513v3</guid></item><item><title>SensorX2car: Sensors-to-car calibration for autonomous driving in road scenarios</title><link>http://arxiv.org/abs/2301.07279v2</link><description>Properly-calibrated sensors are the prerequisite for a dependable autonomousdriving system. However, most prior methods focus on extrinsic calibrationbetween sensors, and few focus on the misalignment between the sensors and thevehicle coordinate system. Existing targetless approaches rely on specificprior knowledge, such as driving routes and road features, to handle thismisalignment. This work removes these limitations and proposes more generalcalibration methods for four commonly used sensors: Camera, LiDAR, GNSS/INS,and millimeter-wave Radar. By utilizing sensor-specific patterns: imagefeature, 3D LiDAR points, GNSS/INS solved pose, and radar speed, we design fourcorresponding methods to mainly calibrate the rotation from sensor to carduring normal driving within minutes, composing a toolbox named SensorX2car.Real-world and simulated experiments demonstrate the practicality of ourproposed methods. Meanwhile, the related codes have been open-sourced tobenefit the community. To the best of our knowledge, SensorX2car is the firstopen-source sensor-to-car calibration toolbox. The code is available athttps://github.com/OpenCalib/SensorX2car.</description><author>Guohang Yan, Zhaotong Luo, Zhuochun Liu, Yikang Li</author><pubDate>Thu, 18 May 2023 09:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07279v2</guid></item><item><title>Recovering 3D Human Mesh from Monocular Images: A Survey</title><link>http://arxiv.org/abs/2203.01923v3</link><description>Estimating human pose and shape from monocular images is a long-standingproblem in computer vision. Since the release of statistical body models, 3Dhuman mesh recovery has been drawing broader attention. With the same goal ofobtaining well-aligned and physically plausible mesh results, two paradigmshave been developed to overcome challenges in the 2D-to-3D lifting process: i)an optimization-based paradigm, where different data terms and regularizationterms are exploited as optimization objectives; and ii) a regression-basedparadigm, where deep learning techniques are embraced to solve the problem inan end-to-end fashion. Meanwhile, continuous efforts are devoted to improvingthe quality of 3D mesh labels for a wide range of datasets. Though remarkableprogress has been achieved in the past decade, the task is still challengingdue to flexible body motions, diverse appearances, complex environments, andinsufficient in-the-wild annotations. To the best of our knowledge, this is thefirst survey that focuses on the task of monocular 3D human mesh recovery. Westart with the introduction of body models and then elaborate recoveryframeworks and training objectives by providing in-depth analyses of theirstrengths and weaknesses. We also summarize datasets, evaluation metrics, andbenchmark results. Open issues and future directions are discussed in the end,hoping to motivate researchers and facilitate their research in this area. Aregularly updated project page can be found athttps://github.com/tinatiansjz/hmr-survey.</description><author>Yating Tian, Hongwen Zhang, Yebin Liu, Limin Wang</author><pubDate>Sat, 10 Jun 2023 11:10:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.01923v3</guid></item><item><title>Generalizable One-shot Neural Head Avatar</title><link>http://arxiv.org/abs/2306.08768v1</link><description>We present a method that reconstructs and animates a 3D head avatar from asingle-view portrait image. Existing methods either involve time-consumingoptimization for a specific person with multiple images, or they struggle tosynthesize intricate appearance details beyond the facial region. To addressthese limitations, we propose a framework that not only generalizes to unseenidentities based on a single-view image without requiring person-specificoptimization, but also captures characteristic details within and beyond theface area (e.g. hairstyle, accessories, etc.). At the core of our method arethree branches that produce three tri-planes representing the coarse 3Dgeometry, detailed appearance of a source image, as well as the expression of atarget image. By applying volumetric rendering to the combination of the threetri-planes followed by a super-resolution module, our method yields a highfidelity image of the desired identity, expression and pose. Once trained, ourmodel enables efficient 3D head avatar reconstruction and animation via asingle forward pass through a network. Experiments show that the proposedapproach generalizes well to unseen validation datasets, surpassing SOTAbaseline methods by a large margin on head avatar reconstruction and animation.</description><author>Xueting Li, Shalini De Mello, Sifei Liu, Koki Nagano, Umar Iqbal, Jan Kautz</author><pubDate>Wed, 14 Jun 2023 23:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08768v1</guid></item><item><title>Decomposed Human Motion Prior for Video Pose Estimation via Adversarial Training</title><link>http://arxiv.org/abs/2305.18743v1</link><description>Estimating human pose from video is a task that receives considerableattention due to its applicability in numerous 3D fields. The complexity ofprior knowledge of human body movements poses a challenge to neural networkmodels in the task of regressing keypoints. In this paper, we address thisproblem by incorporating motion prior in an adversarial way. Different fromprevious methods, we propose to decompose holistic motion prior to joint motionprior, making it easier for neural networks to learn from prior knowledgethereby boosting the performance on the task. We also utilize a novelregularization loss to balance accuracy and smoothness introduced by motionprior. Our method achieves 9\% lower PA-MPJPE and 29\% lower acceleration errorthan previous methods tested on 3DPW. The estimator proves its robustness byachieving impressive performance on in-the-wild dataset.</description><author>Wenshuo Chen, Xiang Zhou, Zhengdi Yu, Zhaoyu Zheng, Weixi Gu, Kai Zhang</author><pubDate>Tue, 30 May 2023 05:53:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18743v1</guid></item><item><title>Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN</title><link>http://arxiv.org/abs/2212.09098v5</link><description>Fine-grained semantic segmentation of a person's face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.</description><author>Lei Li, Tianfang Zhang, Zhongfeng Kang, Xikun Jiang</author><pubDate>Tue, 30 May 2023 18:07:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09098v5</guid></item><item><title>BU-CVKit: Extendable Computer Vision Framework for Species Independent Tracking and Analysis</title><link>http://arxiv.org/abs/2306.04736v1</link><description>A major bottleneck of interdisciplinary computer vision (CV) research is thelack of a framework that eases the reuse and abstraction of state-of-the-art CVmodels by CV and non-CV researchers alike. We present here BU-CVKit, a computervision framework that allows the creation of research pipelines with chainableProcessors. The community can create plugins of their work for the framework,hence improving the re-usability, accessibility, and exposure of their workwith minimal overhead. Furthermore, we provide MuSeqPose Kit, a user interfacefor the pose estimation package of BU-CVKit, which automatically scans forinstalled plugins and programmatically generates an interface for them based onthe metadata provided by the user. It also provides software support forstandard pose estimation features such as annotations, 3D reconstruction,reprojection, and camera calibration. Finally, we show examples of behavioralneuroscience pipelines created through the sample plugins created for ourframework.</description><author>Mahir Patel, Lucas Carstensen, Yiwen Gu, Michael E. Hasselmo, Margrit Betke</author><pubDate>Wed, 07 Jun 2023 20:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04736v1</guid></item><item><title>Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN</title><link>http://arxiv.org/abs/2212.09098v3</link><description>Fine-grained semantic segmentation of a person's face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.</description><author>Lei Li, Tianfang Zhang, Zhongfeng Kang, Xikun Jiang</author><pubDate>Sun, 30 Apr 2023 03:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09098v3</guid></item><item><title>Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN</title><link>http://arxiv.org/abs/2212.09098v4</link><description>Fine-grained semantic segmentation of a person's face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.</description><author>Lei Li, Tianfang Zhang, Zhongfeng Kang, Xikun Jiang</author><pubDate>Sat, 06 May 2023 19:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09098v4</guid></item><item><title>Unsupervised Style-based Explicit 3D Face Reconstruction from Single Image</title><link>http://arxiv.org/abs/2304.12455v1</link><description>Inferring 3D object structures from a single image is an ill-posed task dueto depth ambiguity and occlusion. Typical resolutions in the literature includeleveraging 2D or 3D ground truth for supervised learning, as well as imposinghand-crafted symmetry priors or using an implicit representation to hallucinatenovel viewpoints for unsupervised methods. In this work, we propose a generaladversarial learning framework for solving Unsupervised 2D to Explicit 3D StyleTransfer (UE3DST). Specifically, we merge two architectures: the unsupervisedexplicit 3D reconstruction network of Wu et al.\ and the Generative AdversarialNetwork (GAN) named StarGAN-v2. We experiment across three facial datasets(Basel Face Model, 3DFAW and CelebA-HQ) and show that our solution is able tooutperform well established solutions such as DepthNet in 3D reconstruction andPix2NeRF in conditional style transfer, while we also justify the individualcontributions of our model components via ablation. In contrast to theaforementioned baselines, our scheme produces features for explicit 3Drendering, which can be manipulated and utilized in downstream tasks.</description><author>Heng Yu, Zoltan A. Milacski, Laszlo A. Jeni</author><pubDate>Mon, 24 Apr 2023 22:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12455v1</guid></item><item><title>Learning Hand-Held Object Reconstruction from In-The-Wild Videos</title><link>http://arxiv.org/abs/2305.03036v1</link><description>Prior works for reconstructing hand-held objects from a single image rely ondirect 3D shape supervision which is challenging to gather in real world atscale. Consequently, these approaches do not generalize well when presentedwith novel objects in in-the-wild settings. While 3D supervision is a majorbottleneck, there is an abundance of in-the-wild raw video data showinghand-object interactions. In this paper, we automatically extract 3Dsupervision (via multiview 2D supervision) from such raw video data to scale upthe learning of models for hand-held object reconstruction. This requirestackling two key challenges: unknown camera pose and occlusion. For the former,we use hand pose (predicted from existing techniques, e.g. FrankMocap) as aproxy for object pose. For the latter, we learn data-driven 3D shape priorsusing synthetic objects from the ObMan dataset. We use these indirect 3D cuesto train occupancy networks that predict the 3D shape of objects from a singleRGB image. Our experiments on the MOW and HO3D datasets show the effectivenessof these supervisory signals at predicting the 3D shape for real-worldhand-held objects without any direct real-world 3D supervision.</description><author>Aditya Prakash, Matthew Chang, Matthew Jin, Saurabh Gupta</author><pubDate>Thu, 04 May 2023 18:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03036v1</guid></item><item><title>PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction</title><link>http://arxiv.org/abs/2305.16914v1</link><description>Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D imagesand camera poses for Novel View Synthesis (NVS). Although NeRF can producephotorealistic results, it often suffers from overfitting to training views,leading to poor geometry reconstruction, especially in low-texture areas. Thislimitation restricts many important applications which require accurategeometry, such as extrapolated NVS, HD mapping and scene editing. To addressthis limitation, we propose a new method to improve NeRF's 3D structure usingonly RGB images and semantic maps. Our approach introduces a novel planeregularization based on Singular Value Decomposition (SVD), that does not relyon any geometric prior. In addition, we leverage the Structural SimilarityIndex Measure (SSIM) in our loss design to properly initialize the volumetricrepresentation of NeRF. Quantitative and qualitative results show that ourmethod outperforms popular regularization approaches in accurate geometryreconstruction for large-scale outdoor scenes and achieves SoTA renderingquality on the KITTI-360 NVS benchmark.</description><author>Fusang Wang, Arnaud Louys, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou</author><pubDate>Fri, 26 May 2023 14:26:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16914v1</guid></item><item><title>PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction</title><link>http://arxiv.org/abs/2305.16914v3</link><description>Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D imagesand camera poses for Novel View Synthesis (NVS). Although NeRF can producephotorealistic results, it often suffers from overfitting to training views,leading to poor geometry reconstruction, especially in low-texture areas. Thislimitation restricts many important applications which require accurategeometry, such as extrapolated NVS, HD mapping and scene editing. To addressthis limitation, we propose a new method to improve NeRF's 3D structure usingonly RGB images and semantic maps. Our approach introduces a novel planeregularization based on Singular Value Decomposition (SVD), that does not relyon any geometric prior. In addition, we leverage the Structural SimilarityIndex Measure (SSIM) in our loss design to properly initialize the volumetricrepresentation of NeRF. Quantitative and qualitative results show that ourmethod outperforms popular regularization approaches in accurate geometryreconstruction for large-scale outdoor scenes and achieves SoTA renderingquality on the KITTI-360 NVS benchmark.</description><author>Fusang Wang, Arnaud Louys, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou</author><pubDate>Tue, 06 Jun 2023 11:01:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16914v3</guid></item><item><title>PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction</title><link>http://arxiv.org/abs/2305.16914v2</link><description>Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D imagesand camera poses for Novel View Synthesis (NVS). Although NeRF can producephotorealistic results, it often suffers from overfitting to training views,leading to poor geometry reconstruction, especially in low-texture areas. Thislimitation restricts many important applications which require accurategeometry, such as extrapolated NVS, HD mapping and scene editing. To addressthis limitation, we propose a new method to improve NeRF's 3D structure usingonly RGB images and semantic maps. Our approach introduces a novel planeregularization based on Singular Value Decomposition (SVD), that does not relyon any geometric prior. In addition, we leverage the Structural SimilarityIndex Measure (SSIM) in our loss design to properly initialize the volumetricrepresentation of NeRF. Quantitative and qualitative results show that ourmethod outperforms popular regularization approaches in accurate geometryreconstruction for large-scale outdoor scenes and achieves SoTA renderingquality on the KITTI-360 NVS benchmark.</description><author>Fusang Wang, Arnaud Louys, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou</author><pubDate>Tue, 30 May 2023 15:21:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16914v2</guid></item><item><title>Learning Human Mesh Recovery in 3D Scenes</title><link>http://arxiv.org/abs/2306.03847v1</link><description>We present a novel method for recovering the absolute pose and shape of ahuman in a pre-scanned scene given a single image. Unlike previous methods thatperform sceneaware mesh optimization, we propose to first estimate absoluteposition and dense scene contacts with a sparse 3D CNN, and later enhance apretrained human mesh recovery network by cross-attention with the derived 3Dscene cues. Joint learning on images and scene geometry enables our method toreduce the ambiguity caused by depth and occlusion, resulting in morereasonable global postures and contacts. Encoding scene-aware cues in thenetwork also allows the proposed method to be optimization-free, and opens upthe opportunity for real-time applications. The experiments show that theproposed network is capable of recovering accurate and physically-plausiblemeshes by a single forward pass and outperforms state-of-the-art methods interms of both accuracy and speed.</description><author>Zehong Shen, Zhi Cen, Sida Peng, Qing Shuai, Hujun Bao, Xiaowei Zhou</author><pubDate>Tue, 06 Jun 2023 17:35:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.03847v1</guid></item><item><title>Towards Realistic 3D Embedding via View Alignment</title><link>http://arxiv.org/abs/2007.07066v3</link><description>Recent advances in generative adversarial networks (GANs) have achieved greatsuccess in automated image composition that generates new images by embeddinginterested foreground objects into background images automatically. On theother hand, most existing works deal with foreground objects in two-dimensional(2D) images though foreground objects in three-dimensional (3D) models are moreflexible with 360-degree view freedom. This paper presents an innovative ViewAlignment GAN (VA-GAN) that composes new images by embedding 3D models into 2Dbackground images realistically and automatically. VA-GAN consists of a texturegenerator and a differential discriminator that are inter-connected andend-to-end trainable. The differential discriminator guides to learn geometrictransformation from background images so that the composed 3D models can bealigned with the background images with realistic poses and views. The texturegenerator adopts a novel view encoding mechanism for generating accurate objecttextures for the 3D models under the estimated views. Extensive experimentsover two synthesis tasks (car synthesis with KITTI and pedestrian synthesiswith Cityscapes) show that VA-GAN achieves high-fidelity compositionqualitatively and quantitatively as compared with state-of-the-art generationmethods.</description><author>Changgong Zhang, Fangneng Zhan, Shijian Lu, Feiying Ma, Xuansong Xie</author><pubDate>Mon, 24 Apr 2023 13:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.07066v3</guid></item><item><title>Multi-State RNA Design with Geometric Multi-Graph Neural Networks</title><link>http://arxiv.org/abs/2305.14749v3</link><description>Computational RNA design has broad applications across synthetic biology andtherapeutic development. Fundamental to the diverse biological functions of RNAis its conformational flexibility, enabling single sequences to adopt a varietyof distinct 3D states. Currently, computational biomolecule design tasks areoften posed as inverse problems, where sequences are designed based on adoptinga single desired structural conformation. In this work, we propose gRNAde, ageometric RNA design pipeline that operates on sets of 3D RNA backbonestructures to explicitly account for and reflect RNA conformational diversityin its designs. We demonstrate the utility of gRNAde for improving nativesequence recovery over single-state approaches on a new large-scale 3D RNAdesign dataset, especially for multi-state and structurally diverse RNAs. Ourcode is available at https://github.com/chaitjo/geometric-rna-design</description><author>Chaitanya K. Joshi, Arian R. Jamasb, Ramon Viñas, Charles Harris, Simon Mathis, Pietro Liò</author><pubDate>Sun, 28 May 2023 23:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14749v3</guid></item><item><title>Multi-State RNA Design with Geometric Multi-Graph Neural Networks</title><link>http://arxiv.org/abs/2305.14749v1</link><description>Computational RNA design has broad applications across synthetic biology andtherapeutic development. Fundamental to the diverse biological functions of RNAis its conformational flexibility, enabling single sequences to adopt a varietyof distinct 3D states. Currently, computational biomolecule design tasks areoften posed as inverse problems, where sequences are designed based on adoptinga single desired structural conformation. In this work, we propose gRNAde, ageometric RNA design pipeline that operates on sets of 3D RNA backbonestructures to explicitly account for and reflect RNA conformational diversityin its designs. We demonstrate the utility of gRNAde for improving nativesequence recovery over single-state approaches on a new large-scale 3D RNAdesign dataset, especially for multi-state and structurally diverse RNAs. Ourcode is available at https://github.com/chaitjo/geometric-rna-design</description><author>Chaitanya K. Joshi, Arian R. Jamasb, Ramon Viñas, Charles Harris, Simon Mathis, Pietro Liò</author><pubDate>Wed, 24 May 2023 06:46:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14749v1</guid></item><item><title>Multi-State RNA Design with Geometric Multi-Graph Neural Networks</title><link>http://arxiv.org/abs/2305.14749v2</link><description>Computational RNA design has broad applications across synthetic biology andtherapeutic development. Fundamental to the diverse biological functions of RNAis its conformational flexibility, enabling single sequences to adopt a varietyof distinct 3D states. Currently, computational biomolecule design tasks areoften posed as inverse problems, where sequences are designed based on adoptinga single desired structural conformation. In this work, we propose gRNAde, ageometric RNA design pipeline that operates on sets of 3D RNA backbonestructures to explicitly account for and reflect RNA conformational diversityin its designs. We demonstrate the utility of gRNAde for improving nativesequence recovery over single-state approaches on a new large-scale 3D RNAdesign dataset, especially for multi-state and structurally diverse RNAs. Ourcode is available at https://github.com/chaitjo/geometric-rna-design</description><author>Chaitanya K. Joshi, Arian R. Jamasb, Ramon Viñas, Charles Harris, Simon Mathis, Pietro Liò</author><pubDate>Thu, 25 May 2023 15:53:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14749v2</guid></item><item><title>PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling</title><link>http://arxiv.org/abs/2304.13006v2</link><description>Creating pose-driven human avatars is about modeling the mapping from thelow-frequency driving pose to high-frequency dynamic human appearances, so aneffective pose encoding method that can encode high-fidelity human details isessential to human avatar modeling. To this end, we present PoseVocab, a novelpose encoding method that encourages the network to discover the optimal poseembeddings for learning the dynamic human appearance. Given multi-view RGBvideos of a character, PoseVocab constructs key poses and latent embeddingsbased on the training poses. To achieve pose generalization and temporalconsistency, we sample key rotations in $so(3)$ of each joint rather than theglobal pose vectors, and assign a pose embedding to each sampled key rotation.These joint-structured pose embeddings not only encode the dynamic appearancesunder different key poses, but also factorize the global pose embedding intojoint-structured ones to better learn the appearance variation related to themotion of each joint. To improve the representation ability of the poseembedding while maintaining memory efficiency, we introduce feature lines, acompact yet effective 3D representation, to model more fine-grained details ofhuman appearances. Furthermore, given a query pose and a spatial position, ahierarchical query strategy is introduced to interpolate pose embeddings andacquire the conditional pose feature for dynamic human synthesis. Overall,PoseVocab effectively encodes the dynamic details of human appearance andenables realistic and generalized animation under novel poses. Experiments showthat our method outperforms other state-of-the-art baselines both qualitativelyand quantitatively in terms of synthesis quality. Code is available athttps://github.com/lizhe00/PoseVocab.</description><author>Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, Yebin Liu</author><pubDate>Sun, 14 May 2023 14:15:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13006v2</guid></item><item><title>PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling</title><link>http://arxiv.org/abs/2304.13006v1</link><description>Creating pose-driven human avatars is about modeling the mapping from thelow-frequency driving pose to high-frequency dynamic human appearances, so aneffective pose encoding method that can encode high-fidelity human details isessential to human avatar modeling.To this end, we present PoseVocab, a novelpose encoding method that encourages the network to discover the optimal poseembeddings for learning the dynamic human appearance. Given multi-view RGBvideos of a character, PoseVocab constructs key poses and latent embeddingsbased on the training poses. To achieve pose generalization and temporalconsistency, we sample key rotations in $so(3)$ of each joint rather than theglobal pose vectors, and assign a pose embedding to each sampled key rotation.These joint-structured pose embeddings not only encode the dynamic appearancesunder different key poses, but also factorize the global pose embedding intojoint-structured ones to better learn the appearance variation related to themotion of each joint. To improve the representation ability of the poseembedding while maintaining memory efficiency, we introduce feature lines, acompact yet effective 3D representation, to model more fine-grained details ofhuman appearances. Furthermore, given a query pose and a spatial position, ahierarchical query strategy is introduced to interpolate pose embeddings andacquire the conditional pose feature for dynamic human synthesis. Overall,PoseVocab effectively encodes the dynamic details of human appearance andenables realistic and generalized animation under novel poses. Experiments showthat our method outperforms other state-of-the-art baselines both qualitativelyand quantitatively in terms of synthesis quality. Code is available athttps://github.com/lizhe00/PoseVocab.</description><author>Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, Yebin Liu</author><pubDate>Tue, 25 Apr 2023 18:25:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13006v1</guid></item><item><title>Equivariant Light Field Convolution and Transformer</title><link>http://arxiv.org/abs/2212.14871v2</link><description>3D reconstruction and novel view rendering can greatly benefit from geometricpriors when the input views are not sufficient in terms of coverage andinter-view baselines. Deep learning of geometric priors from 2D images oftenrequires each image to be represented in a $2D$ canonical frame and the priorto be learned in a given or learned $3D$ canonical frame. In this paper, givenonly the relative poses of the cameras, we show how to learn priors frommultiple views equivariant to coordinate frame transformations by proposing an$SE(3)$-equivariant convolution and transformer in the space of rays in 3D.This enables the creation of a light field that remains equivariant to thechoice of coordinate frame. The light field as defined in our work, refers bothto the radiance field and the feature field defined on the ray space. We modelthe ray space, the domain of the light field, as a homogeneous space of $SE(3)$and introduce the $SE(3)$-equivariant convolution in ray space. Depending onthe output domain of the convolution, we present convolution-based$SE(3)$-equivariant maps from ray space to ray space and to $\mathbb{R}^3$. Ourmathematical framework allows us to go beyond convolution to$SE(3)$-equivariant attention in the ray space. We demonstrate how to tailorand adapt the equivariant convolution and transformer in the tasks ofequivariant neural rendering and $3D$ reconstruction from multiple views. Wedemonstrate $SE(3)$-equivariance by obtaining robust results in roto-translateddatasets without performing transformation augmentation.</description><author>Yinshuang Xu, Jiahui Lei, Kostas Daniilidis</author><pubDate>Wed, 07 Jun 2023 19:00:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.14871v2</guid></item></channel></rss>