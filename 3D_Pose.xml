<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 29 Jan 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>D3PRefiner: A Diffusion-based Denoise Method for 3D Human Pose Refinement</title><link>http://arxiv.org/abs/2401.03914v1</link><description>Three-dimensional (3D) human pose estimation using a monocular camera hasgained increasing attention due to its ease of implementation and the abundanceof data available from daily life. However, owing to the inherent depthambiguity in images, the accuracy of existing monocular camera-based 3D poseestimation methods remains unsatisfactory, and the estimated 3D poses usuallyinclude much noise. By observing the histogram of this noise, we find eachdimension of the noise follows a certain distribution, which indicates thepossibility for a neural network to learn the mapping between noisy poses andground truth poses. In this work, in order to obtain more accurate 3D poses, aDiffusion-based 3D Pose Refiner (D3PRefiner) is proposed to refine the outputof any existing 3D pose estimator. We first introduce a conditionalmultivariate Gaussian distribution to model the distribution of noisy 3D poses,using paired 2D poses and noisy 3D poses as conditions to achieve greateraccuracy. Additionally, we leverage the architecture of current diffusionmodels to convert the distribution of noisy 3D poses into ground truth 3Dposes. To evaluate the effectiveness of the proposed method, twostate-of-the-art sequence-to-sequence 3D pose estimators are used as basic 3Dpose estimation models, and the proposed method is evaluated on different typesof 2D poses and different lengths of the input sequence. Experimental resultsdemonstrate the proposed architecture can significantly improve the performanceof current sequence-to-sequence 3D pose estimators, with a reduction of atleast 10.3% in the mean per joint position error (MPJPE) and at least 11.0% inthe Procrustes MPJPE (P-MPJPE).</description><author>Danqi Yan, Qing Gao, Yuepeng Qian, Xinxing Chen, Chenglong Fu, Yuquan Leng</author><pubDate>Mon, 08 Jan 2024 14:21:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.03914v1</guid></item><item><title>MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling</title><link>http://arxiv.org/abs/2306.17201v1</link><description>Estimating 3D human poses only from a 2D human pose sequence is thoroughlyexplored in recent years. Yet, prior to this, no such work has attempted tounify 2D and 3D pose representations in the shared feature space. In thispaper, we propose MPM, a unified 2D-3D human pose representation framework viamasked pose modeling. We treat 2D and 3D poses as two different modalities likevision and language and build a single-stream transformer-based architecture.We apply three pretext tasks, which are masked 2D pose modeling, masked 3D posemodeling, and masked 2D pose lifting to pre-train our network and usefull-supervision to perform further fine-tuning. A high masking ratio of 72.5%in total with a spatio-temporal mask sampling strategy leading to betterrelation modeling both in spatial and temporal domains. MPM can handle multipletasks including 3D human pose estimation, 3D pose estimation from occluded 2Dpose, and 3D pose completion in a single framework. We conduct extensiveexperiments and ablation studies on several widely used human pose datasets andachieve state-of-the-art performance on Human3.6M and MPI-INF-3DHP. Codes andmodel checkpoints are available at https://github.com/vvirgooo2/MPM</description><author>Zhenyu Zhang, Wenhao Chai, Zhongyu Jiang, Tian Ye, Mingli Song, Jenq-Neng Hwang, Gaoang Wang</author><pubDate>Thu, 29 Jun 2023 11:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17201v1</guid></item><item><title>Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation</title><link>http://arxiv.org/abs/2303.11579v2</link><description>In this paper, a novel Diffusion-based 3D Pose estimation (D3DP) method withJoint-wise reProjection-based Multi-hypothesis Aggregation (JPMA) is proposedfor probabilistic 3D human pose estimation. On the one hand, D3DP generatesmultiple possible 3D pose hypotheses for a single 2D observation. It graduallydiffuses the ground truth 3D poses to a random distribution, and learns adenoiser conditioned on 2D keypoints to recover the uncontaminated 3D poses.The proposed D3DP is compatible with existing 3D pose estimators and supportsusers to balance efficiency and accuracy during inference through twocustomizable parameters. On the other hand, JPMA is proposed to assemblemultiple hypotheses generated by D3DP into a single 3D pose for practical use.It reprojects 3D pose hypotheses to the 2D camera plane, selects the besthypothesis joint-by-joint based on the reprojection errors, and combines theselected joints into the final pose. The proposed JPMA conducts aggregation atthe joint level and makes use of the 2D prior information, both of which havebeen overlooked by previous approaches. Extensive experiments on Human3.6M andMPI-INF-3DHP datasets show that our method outperforms the state-of-the-artdeterministic and probabilistic approaches by 1.5% and 8.9%, respectively. Codeis available at https://github.com/paTRICK-swk/D3DP.</description><author>Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang, Kai Han, Shanshe Wang, Siwei Ma, Wen Gao</author><pubDate>Wed, 23 Aug 2023 04:07:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11579v2</guid></item><item><title>PoseFix: Correcting 3D Human Poses with Natural Language</title><link>http://arxiv.org/abs/2309.08480v1</link><description>Automatically producing instructions to modify one's posture could open thedoor to endless applications, such as personalized coaching and in-homephysical therapy. Tackling the reverse problem (i.e., refining a 3D pose basedon some natural language feedback) could help for assisted 3D characteranimation or robot teaching, for instance. Although a few recent works explorethe connections between natural language and 3D human pose, none focus ondescribing 3D body pose differences. In this paper, we tackle the problem ofcorrecting 3D human poses with natural language. To this end, we introduce thePoseFix dataset, which consists of several thousand paired 3D poses and theircorresponding text feedback, that describe how the source pose needs to bemodified to obtain the target pose. We demonstrate the potential of thisdataset on two tasks: (1) text-based pose editing, that aims at generatingcorrected 3D body poses given a query pose and a text modifier; and (2)correctional text generation, where instructions are generated based on thedifferences between two body poses.</description><author>Ginger Delmas, Philippe Weinzaepfel, Francesc Moreno-Noguer, Grégory Rogez</author><pubDate>Fri, 15 Sep 2023 16:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08480v1</guid></item><item><title>PoseFix: Correcting 3D Human Poses with Natural Language</title><link>http://arxiv.org/abs/2309.08480v2</link><description>Automatically producing instructions to modify one's posture could open thedoor to endless applications, such as personalized coaching and in-homephysical therapy. Tackling the reverse problem (i.e., refining a 3D pose basedon some natural language feedback) could help for assisted 3D characteranimation or robot teaching, for instance. Although a few recent works explorethe connections between natural language and 3D human pose, none focus ondescribing 3D body pose differences. In this paper, we tackle the problem ofcorrecting 3D human poses with natural language. To this end, we introduce thePoseFix dataset, which consists of several thousand paired 3D poses and theircorresponding text feedback, that describe how the source pose needs to bemodified to obtain the target pose. We demonstrate the potential of thisdataset on two tasks: (1) text-based pose editing, that aims at generatingcorrected 3D body poses given a query pose and a text modifier; and (2)correctional text generation, where instructions are generated based on thedifferences between two body poses.</description><author>Ginger Delmas, Philippe Weinzaepfel, Francesc Moreno-Noguer, Grégory Rogez</author><pubDate>Wed, 17 Jan 2024 10:09:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08480v2</guid></item><item><title>Multi-person 3D pose estimation from unlabelled data</title><link>http://arxiv.org/abs/2212.08731v2</link><description>Its numerous applications make multi-human 3D pose estimation a remarkablyimpactful area of research. Nevertheless, assuming a multiple-view systemcomposed of several regular RGB cameras, 3D multi-pose estimation presentsseveral challenges. First of all, each person must be uniquely identified inthe different views to separate the 2D information provided by the cameras.Secondly, the 3D pose estimation process from the multi-view 2D information ofeach person must be robust against noise and potential occlusions in thescenario. In this work, we address these two challenges with the help of deeplearning. Specifically, we present a model based on Graph Neural Networkscapable of predicting the cross-view correspondence of the people in thescenario along with a Multilayer Perceptron that takes the 2D points to yieldthe 3D poses of each person. These two models are trained in a self-supervisedmanner, thus avoiding the need for large datasets with 3D annotations.</description><author>Daniel Rodriguez-Criado, Pilar Bachiller, George Vogiatzis, Luis J. Manso</author><pubDate>Thu, 19 Oct 2023 11:41:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08731v2</guid></item><item><title>Improving the Robustness of 3D Human Pose Estimation: A Benchmark and Learning from Noisy Input</title><link>http://arxiv.org/abs/2312.06797v1</link><description>Despite the promising performance of current 3D human pose estimationtechniques, understanding and enhancing their generalization on challengingin-the-wild videos remain an open problem. In this work, we focus on therobustness of 2D-to-3D pose lifters. To this end, we develop two benchmarkdatasets, namely Human3.6M-C and HumanEva-I-C, to examine the robustness ofvideo-based 3D pose lifters to a wide range of common video corruptionsincluding temporary occlusion, motion blur, and pixel-level noise. We observethe poor generalization of state-of-the-art 3D pose lifters in the presence ofcorruption and establish two techniques to tackle this issue. First, weintroduce Temporal Additive Gaussian Noise (TAGN) as a simple yet effective 2Dinput pose data augmentation. Additionally, to incorporate the confidencescores output by the 2D pose detectors, we design a confidence-awareconvolution (CA-Conv) block. Extensively tested on corrupted videos, theproposed strategies consistently boost the robustness of 3D pose lifters andserve as new baselines for future research.</description><author>Trung-Hieu Hoang, Mona Zehni, Huy Phan, Minh N. Do</author><pubDate>Mon, 11 Dec 2023 19:13:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06797v1</guid></item><item><title>3DHumanGAN: 3D-Aware Human Image Generation with 3D Pose Mapping</title><link>http://arxiv.org/abs/2212.07378v2</link><description>We present 3DHumanGAN, a 3D-aware generative adversarial network thatsynthesizes photorealistic images of full-body humans with consistentappearances under different view-angles and body-poses. To tackle therepresentational and computational challenges in synthesizing the articulatedstructure of human bodies, we propose a novel generator architecture in which a2D convolutional backbone is modulated by a 3D pose mapping network. The 3Dpose mapping network is formulated as a renderable implicit functionconditioned on a posed 3D human mesh. This design has several merits: i) itleverages the strength of 2D GANs to produce high-quality images; ii) itgenerates consistent images under varying view-angles and poses; iii) the modelcan incorporate the 3D human prior and enable pose conditioning. Project page:https://3dhumangan.github.io/.</description><author>Zhuoqian Yang, Shikai Li, Wayne Wu, Bo Dai</author><pubDate>Sun, 24 Sep 2023 23:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07378v2</guid></item><item><title>Two Views Are Better than One: Monocular 3D Pose Estimation with Multiview Consistency</title><link>http://arxiv.org/abs/2311.12421v1</link><description>Deducing a 3D human pose from a single 2D image or 2D keypoints is inherentlychallenging, given the fundamental ambiguity wherein multiple 3D poses cancorrespond to the same 2D representation. The acquisition of 3D data, whileinvaluable for resolving pose ambiguity, is expensive and requires an intricatesetup, often restricting its applicability to controlled lab environments. Weimprove performance of monocular human pose estimation models using multiviewdata for fine-tuning. We propose a novel loss function, multiview consistency,to enable adding additional training data with only 2D supervision. This lossenforces that the inferred 3D pose from one view aligns with the inferred 3Dpose from another view under similarity transformations. Our consistency losssubstantially improves performance for fine-tuning with no available 3D data.Our experiments demonstrate that two views offset by 90 degrees are enough toobtain good performance, with only marginal improvements by adding more views.Thus, we enable the acquisition of domain-specific data by capturing activitieswith off-the-shelf cameras, eliminating the need for elaborate calibrationprocedures. This research introduces new possibilities for domain adaptation in3D pose estimation, providing a practical and cost-effective solution tocustomize models for specific applications. The used dataset, featuringadditional views, will be made publicly available.</description><author>Christian Keilstrup Ingwersen, Anders Bjorholm Dahl, Janus Nørtoft Jensen, Morten Rieger Hannemose</author><pubDate>Tue, 21 Nov 2023 08:21:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12421v1</guid></item><item><title>3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and Point Cloud</title><link>http://arxiv.org/abs/2311.04699v1</link><description>Greenhouse production of fruits and vegetables in developed countries ischallenged by labor 12 scarcity and high labor costs. Robots offer a goodsolution for sustainable and cost-effective 13 production. Acquiring accuratespatial information about relevant plant parts is vital for 14 successful robotoperation. Robot perception in greenhouses is challenging due to variations in15 plant appearance, viewpoints, and illumination. This paper proposes akeypoint-detection-based 16 method using data from an RGB-D camera to estimatethe 3D pose of peduncle nodes, which 17 provides essential information toharvest the tomato bunches. 18 19 Specifically, this paper proposes a methodthat detects four anatomical landmarks in the color 20 image and thenintegrates 3D point-cloud information to determine the 3D pose. A 21comprehensive evaluation was conducted in a commercial greenhouse to gaininsight into the 22 performance of different parts of the method. The resultsshowed: (1) high accuracy in object 23 detection, achieving an AveragePrecision (AP) of AP@0.5=0.96; (2) an average Percentage of 24 Detected Joints(PDJ) of the keypoints of PhDJ@0.2=94.31%; and (3) 3D pose estimation 25accuracy with mean absolute errors (MAE) of 11.38o and 9.93o for the relativeupper and lower 26 angles between the peduncle and main stem, respectively.Furthermore, the capability to handle 27 variations in viewpoint wasinvestigated, demonstrating the method was robust to view changes. 28 However,canonical and higher views resulted in slightly higher performance compared toother 29 views. Although tomato was selected as a use case, the proposed methodis also applicable to 30 other greenhouse crops like pepper.</description><author>Jianchao Ci, Xin Wang, David Rapado-Rincón, Akshay K. Burusa, Gert Kootstra</author><pubDate>Wed, 08 Nov 2023 14:10:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.04699v1</guid></item><item><title>Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.11667v1</link><description>As 3D human pose estimation can now be achieved with very high accuracy inthe supervised learning scenario, tackling the case where 3D pose annotationsare not available has received increasing attention. In particular, severalmethods have proposed to learn image representations in a self-supervisedfashion so as to disentangle the appearance information from the pose one. Themethods then only need a small amount of supervised data to train a poseregressor using the pose-related latent vector as input, as it should be freeof appearance information. In this paper, we carry out in-depth analysis tounderstand to what degree the state-of-the-art disentangled representationlearning methods truly separate the appearance information from the pose one.First, we study disentanglement from the perspective of the self-supervisednetwork, via diverse image synthesis experiments. Second, we investigatedisentanglement with respect to the 3D pose regressor following an adversarialattack perspective. Specifically, we design an adversarial strategy focusing ongenerating natural appearance changes of the subject, and against which wecould expect a disentangled network to be robust. Altogether, our analyses showthat disentanglement in the three state-of-the-art disentangled representationlearning frameworks if far from complete, and that their pose codes containsignificant appearance information. We believe that our approach provides avaluable testbed to evaluate the degree of disentanglement of pose fromappearance in self-supervised 3D human pose estimation.</description><author>Krishna Kanth Nakka, Mathieu Salzmann</author><pubDate>Wed, 20 Sep 2023 23:22:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.11667v1</guid></item><item><title>Mask as Supervision: Leveraging Unified Mask Information for Unsupervised 3D Pose Estimation</title><link>http://arxiv.org/abs/2312.07051v1</link><description>Automatic estimation of 3D human pose from monocular RGB images is achallenging and unsolved problem in computer vision. In a supervised manner,approaches heavily rely on laborious annotations and present hamperedgeneralization ability due to the limited diversity of 3D pose datasets. Toaddress these challenges, we propose a unified framework that leverages mask assupervision for unsupervised 3D pose estimation. With general unsupervisedsegmentation algorithms, the proposed model employs skeleton and physiquerepresentations that exploit accurate pose information from coarse to fine.Compared with previous unsupervised approaches, we organize the human skeletonin a fully unsupervised way which enables the processing of annotation-freedata and provides ready-to-use estimation results. Comprehensive experimentsdemonstrate our state-of-the-art pose estimation performance on Human3.6M andMPI-INF-3DHP datasets. Further experiments on in-the-wild datasets alsoillustrate the capability to access more data to boost our model. Code will beavailable at https://github.com/Charrrrrlie/Mask-as-Supervision.</description><author>Yuchen Yang, Yu Qiao, Xiao Sun</author><pubDate>Tue, 12 Dec 2023 08:08:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07051v1</guid></item><item><title>PoseScript: Linking 3D Human Poses and Natural Language</title><link>http://arxiv.org/abs/2210.11795v2</link><description>Natural language plays a critical role in many computer vision applications,such as image captioning, visual question answering, and cross-modal retrieval,to provide fine-grained semantic information. Unfortunately, while human poseis key to human understanding, current 3D human pose datasets lack detailedlanguage descriptions. To address this issue, we have introduced the PoseScriptdataset. This dataset pairs more than six thousand 3D human poses from AMASSwith rich human-annotated descriptions of the body parts and their spatialrelationships. Additionally, to increase the size of the dataset to a scalethat is compatible with data-hungry learning algorithms, we have proposed anelaborate captioning process that generates automatic synthetic descriptions innatural language from given 3D keypoints. This process extracts low-level poseinformation, known as "posecodes", using a set of simple but generic rules onthe 3D keypoints. These posecodes are then combined into higher level textualdescriptions using syntactic rules. With automatic annotations, the amount ofavailable data significantly scales up (100k), making it possible toeffectively pretrain deep models for finetuning on human captions. To showcasethe potential of annotated poses, we present three multi-modal learning tasksthat utilize the PoseScript dataset. Firstly, we develop a pipeline that maps3D poses and textual descriptions into a joint embedding space, allowing forcross-modal retrieval of relevant poses from large-scale datasets. Secondly, weestablish a baseline for a text-conditioned model generating 3D poses. Thirdly,we present a learned process for generating pose descriptions. Theseapplications demonstrate the versatility and usefulness of annotated poses invarious tasks and pave the way for future research in the field.</description><author>Ginger Delmas, Philippe Weinzaepfel, Thomas Lucas, Francesc Moreno-Noguer, Grégory Rogez</author><pubDate>Fri, 19 Jan 2024 14:08:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.11795v2</guid></item><item><title>ManiPose: Manifold-Constrained Multi-Hypothesis 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2312.06386v1</link><description>Monocular 3D human pose estimation (3D-HPE) is an inherently ambiguous task,as a 2D pose in an image might originate from different possible 3D poses. Yet,most 3D-HPE methods rely on regression models, which assume a one-to-onemapping between inputs and outputs. In this work, we provide theoretical andempirical evidence that, because of this ambiguity, common regression modelsare bound to predict topologically inconsistent poses, and that traditionalevaluation metrics, such as the MPJPE, P-MPJPE and PCK, are insufficient toassess this aspect. As a solution, we propose ManiPose, a novelmanifold-constrained multi-hypothesis model capable of proposing multiplecandidate 3D poses for each 2D input, together with their correspondingplausibility. Unlike previous multi-hypothesis approaches, our solution iscompletely supervised and does not rely on complex generative models, thusgreatly facilitating its training and usage. Furthermore, by constraining ourmodel to lie within the human pose manifold, we can guarantee the consistencyof all hypothetical poses predicted with our approach, which was not possiblein previous works. We illustrate the usefulness of ManiPose in a synthetic1D-to-2D lifting setting and demonstrate on real-world datasets that itoutperforms state-of-the-art models in pose consistency by a large margin,while still reaching competitive MPJPE performance.</description><author>Cédric Rommel, Victor Letzelter, Nermin Samet, Renaud Marlet, Matthieu Cord, Patrick Pérez, Eduardo Valle</author><pubDate>Mon, 11 Dec 2023 13:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06386v1</guid></item><item><title>PoseGen: Learning to Generate 3D Human Pose Dataset with NeRF</title><link>http://arxiv.org/abs/2312.14915v1</link><description>This paper proposes an end-to-end framework for generating 3D human posedatasets using Neural Radiance Fields (NeRF). Public datasets generally havelimited diversity in terms of human poses and camera viewpoints, largely due tothe resource-intensive nature of collecting 3D human pose data. As a result,pose estimators trained on public datasets significantly underperform whenapplied to unseen out-of-distribution samples. Previous works proposedaugmenting public datasets by generating 2D-3D pose pairs or rendering a largeamount of random data. Such approaches either overlook image rendering orresult in suboptimal datasets for pre-trained models. Here we propose PoseGen,which learns to generate a dataset (human 3D poses and images) with a feedbackloss from a given pre-trained pose estimator. In contrast to prior art, ourgenerated data is optimized to improve the robustness of the pre-trained model.The objective of PoseGen is to learn a distribution of data that maximizes theprediction error of a given pre-trained model. As the learned data distributioncontains OOD samples of the pre-trained model, sampling data from such adistribution for further fine-tuning a pre-trained model improves thegeneralizability of the model. This is the first work that proposes NeRFs for3D human data generation. NeRFs are data-driven and do not require 3D scans ofhumans. Therefore, using NeRF for data generation is a new direction forconvenient user-specific data generation. Our extensive experiments show thatthe proposed PoseGen improves two baseline models (SPIN and HybrIK) on fourdatasets with an average 6% relative improvement.</description><author>Mohsen Gholami, Rabab Ward, Z. Jane Wang</author><pubDate>Fri, 22 Dec 2023 18:50:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14915v1</guid></item><item><title>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2308.09678v2</link><description>Existing 3D human pose estimators face challenges in adapting to new datasetsdue to the lack of 2D-3D pose pairs in training sets. To overcome this issue,we propose \textit{Multi-Hypothesis \textbf{P}ose \textbf{Syn}thesis\textbf{D}omain \textbf{A}daptation} (\textbf{PoSynDA}) framework to bridgethis data disparity gap in target domain. Typically, PoSynDA uses adiffusion-inspired structure to simulate 3D pose distribution in the targetdomain. By incorporating a multi-hypothesis network, PoSynDA generates diversepose hypotheses and aligns them with the target domain. To do this, it firstutilizes target-specific source augmentation to obtain the target domaindistribution data from the source domain by decoupling the scale and positionparameters. The process is then further refined through the teacher-studentparadigm and low-rank adaptation. With extensive comparison of benchmarks suchas Human3.6M and MPI-INF-3DHP, PoSynDA demonstrates competitive performance,even comparable to the target-trained MixSTE model\cite{zhang2022mixste}. Thiswork paves the way for the practical application of 3D human pose estimation inunseen domains. The code is available at https://github.com/hbing-l/PoSynDA.</description><author>Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 16 Oct 2023 18:07:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09678v2</guid></item><item><title>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</title><link>http://arxiv.org/abs/2308.11737v1</link><description>Accurately estimating the 3D pose and shape is an essential step towardsunderstanding animal behavior, and can potentially benefit many downstreamapplications, such as wildlife conservation. However, research in this area isheld back by the lack of a comprehensive and diverse dataset with high-quality3D pose and shape annotations. In this paper, we propose Animal3D, the firstcomprehensive dataset for mammal animal 3D pose and shape estimation. Animal3Dconsists of 3379 images collected from 40 mammal species, high-qualityannotations of 26 keypoints, and importantly the pose and shape parameters ofthe SMAL model. All annotations were labeled and checked manually in amulti-stage process to ensure highest quality results. Based on the Animal3Ddataset, we benchmark representative shape and pose estimation models at: (1)supervised learning from only the Animal3D data, (2) synthetic to real transferfrom synthetically generated images, and (3) fine-tuning human pose and shapeestimation models. Our experimental results demonstrate that predicting the 3Dshape and pose of animals across species remains a very challenging task,despite significant advances in human pose estimation. Our results furtherdemonstrate that synthetic pre-training is a viable strategy to boost the modelperformance. Overall, Animal3D opens new directions for facilitating futureresearch in animal 3D pose and shape estimation, and is publicly available.</description><author>Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang, Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen Cui, Alan Yuille, Adam Kortylewski</author><pubDate>Tue, 22 Aug 2023 19:57:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11737v1</guid></item><item><title>DiffuPose: Monocular 3D Human Pose Estimation via Denoising Diffusion Probabilistic Model</title><link>http://arxiv.org/abs/2212.02796v3</link><description>Thanks to the development of 2D keypoint detectors, monocular 3D human poseestimation (HPE) via 2D-to-3D uplifting approaches have achieved remarkableimprovements. Still, monocular 3D HPE is a challenging problem due to theinherent depth ambiguities and occlusions. To handle this problem, manyprevious works exploit temporal information to mitigate such difficulties.However, there are many real-world applications where frame sequences are notaccessible. This paper focuses on reconstructing a 3D pose from a single 2Dkeypoint detection. Rather than exploiting temporal information, we alleviatethe depth ambiguity by generating multiple 3D pose candidates which can bemapped to an identical 2D keypoint. We build a novel diffusion-based frameworkto effectively sample diverse 3D poses from an off-the-shelf 2D detector. Byconsidering the correlation between human joints by replacing the conventionaldenoising U-Net with graph convolutional network, our approach accomplishesfurther performance improvements. We evaluate our method on the widely adoptedHuman3.6M and HumanEva-I datasets. Comprehensive experiments are conducted toprove the efficacy of the proposed method, and they confirm that our modeloutperforms state-of-the-art multi-hypothesis 3D HPE methods.</description><author>Jeongjun Choi, Dongseok Shim, H. Jin Kim</author><pubDate>Thu, 03 Aug 2023 10:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02796v3</guid></item><item><title>PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2308.09678v1</link><description>The current 3D human pose estimators face challenges in adapting to newdatasets due to the scarcity of 2D-3D pose pairs in target domain trainingsets. We present the \textit{Multi-Hypothesis \textbf{P}ose \textbf{Syn}thesis\textbf{D}omain \textbf{A}daptation} (\textbf{PoSynDA}) framework to overcomethis issue without extensive target domain annotation. Utilizing adiffusion-centric structure, PoSynDA simulates the 3D pose distribution in thetarget domain, filling the data diversity gap. By incorporating amulti-hypothesis network, it creates diverse pose hypotheses and aligns themwith the target domain. Target-specific source augmentation obtains the targetdomain distribution data from the source domain by decoupling the scale andposition parameters. The teacher-student paradigm and low-rank adaptationfurther refine the process. PoSynDA demonstrates competitive performance onbenchmarks, such as Human3.6M, MPI-INF-3DHP, and 3DPW, even comparable with thetarget-trained MixSTE model~\cite{zhang2022mixste}. This work paves the way forthe practical application of 3D human pose estimation. The code is available athttps://github.com/hbing-l/PoSynDA.</description><author>Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Fri, 18 Aug 2023 17:57:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09678v1</guid></item><item><title>PointVoxel: A Simple and Effective Pipeline for Multi-View Multi-Modal 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2312.06409v2</link><description>Recently, several methods have been proposed to estimate 3D human pose frommulti-view images and achieved impressive performance on public datasetscollected in relatively easy scenarios. However, there are limited approachesfor extracting 3D human skeletons from multimodal inputs (e.g., RGB andpointcloud) that can enhance the accuracy of predicting 3D poses in challengingsituations. We fill this gap by introducing a pipeline called PointVoxel thatfuses multi-view RGB and pointcloud inputs to obtain 3D human poses. Wedemonstrate that volumetric representation is an effective architecture forintegrating these different modalities. Moreover, in order to overcome thechallenges of annotating 3D human pose labels in difficult scenarios, wedevelop a synthetic dataset generator for pretraining and design anunsupervised domain adaptation strategy so that we can obtain a well-trained 3Dhuman pose estimator without using any manual annotations. We evaluate ourapproach on four datasets (two public datasets, one synthetic dataset, and onechallenging dataset named BasketBall collected by ourselves), showing promisingresults. The code and dataset will be released soon.</description><author>Zhiyu Pan, Zhicheng Zhong, Wenxuan Guo, Yifan Chen, Jianjiang Feng, Jie Zhou</author><pubDate>Tue, 12 Dec 2023 04:37:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06409v2</guid></item><item><title>PointVoxel: A Simple and Effective Pipeline for Multi-View Multi-Modal 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2312.06409v1</link><description>Recently, several methods have been proposed to estimate 3D human pose frommulti-view images and achieved impressive performance on public datasetscollected in relatively easy scenarios. However, there are limited approachesfor extracting 3D human skeletons from multimodal inputs (e.g., RGB andpointcloud) that can enhance the accuracy of predicting 3D poses in challengingsituations. We fill this gap by introducing a pipeline called PointVoxel thatfuses multi-view RGB and pointcloud inputs to obtain 3D human poses. Wedemonstrate that volumetric representation is an effective architecture forintegrating these different modalities. Moreover, in order to overcome thechallenges of annotating 3D human pose labels in difficult scenarios, wedevelop a synthetic dataset generator for pretraining and design anunsupervised domain adaptation strategy so that we can obtain a well-trained 3Dhuman pose estimator without using any manual annotations. We evaluate ourapproach on four datasets (two public datasets, one synthetic dataset, and onechallenging dataset named BasketBall collected by ourselves), showing promisingresults. The code and dataset will be released soon.</description><author>Zhiyu Pan, Zhicheng Zhong, Wenxuan Guo, Yifan Chen, Jianjiang Feng, Jie Zhou</author><pubDate>Mon, 11 Dec 2023 14:30:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06409v1</guid></item><item><title>PoseGPT: Chatting about 3D Human Pose</title><link>http://arxiv.org/abs/2311.18836v1</link><description>We introduce PoseGPT, a framework employing Large Language Models (LLMs) tounderstand and reason about 3D human poses from images or textual descriptions.Our work is motivated by the human ability to intuitively understand posturesfrom a single image or a brief description, a process that intertwines imageinterpretation, world knowledge, and an understanding of body language.Traditional human pose estimation methods, whether image-based or text-based,often lack holistic scene comprehension and nuanced reasoning, leading to adisconnect between visual data and its real-world implications. PoseGPTaddresses these limitations by embedding SMPL poses as a distinct signal tokenwithin a multi-modal LLM, enabling direct generation of 3D body poses from bothtextual and visual inputs. This approach not only simplifies pose predictionbut also empowers LLMs to apply their world knowledge in reasoning about humanposes, fostering two advanced tasks: speculative pose generation and reasoningabout pose estimation. These tasks involve reasoning about humans to generate3D poses from subtle text queries, possibly accompanied by images. We establishbenchmarks for these tasks, moving beyond traditional 3D pose generation andestimation methods. Our results show that PoseGPT outperforms existingmultimodal LLMs and task-sepcific methods on these newly proposed tasks.Furthermore, PoseGPT's ability to understand and generate 3D human poses basedon complex reasoning opens new directions in human pose analysis.</description><author>Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Michael J. Black</author><pubDate>Thu, 30 Nov 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18836v1</guid></item><item><title>POCO: 3D Pose and Shape Estimation with Confidence</title><link>http://arxiv.org/abs/2308.12965v1</link><description>The regression of 3D Human Pose and Shape (HPS) from an image is becomingincreasingly accurate. This makes the results useful for downstream tasks likehuman action recognition or 3D graphics. Yet, no regressor is perfect, andaccuracy can be affected by ambiguous image evidence or by poses and appearancethat are unseen during training. Most current HPS regressors, however, do notreport the confidence of their outputs, meaning that downstream tasks cannotdifferentiate accurate estimates from inaccurate ones. To address this, wedevelop POCO, a novel framework for training HPS regressors to estimate notonly a 3D human body, but also their confidence, in a single feed-forward pass.Specifically, POCO estimates both the 3D body pose and a per-sample variance.The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressinguncertainty that is highly correlated to pose reconstruction quality. The POCOframework can be applied to any HPS regressor and here we evaluate it bymodifying HMR, PARE, and CLIFF. In all cases, training the network to reasonabout uncertainty helps it learn to more accurately estimate 3D pose. Whilethis was not our goal, the improvement is modest but consistent. Our mainmotivation is to provide uncertainty estimates for downstream tasks; wedemonstrate this in two ways: (1) We use the confidence estimates to bootstrapHPS training. Given unlabelled image data, we take the confident estimates of aPOCO-trained regressor as pseudo ground truth. Retraining with thisautomatically-curated data improves accuracy. (2) We exploit uncertainty invideo pose estimation by automatically identifying uncertain frames (e.g. dueto occlusion) and inpainting these from confident frames. Code and models willbe available for research at https://poco.is.tue.mpg.de.</description><author>Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, Dimitrios Tzionas</author><pubDate>Thu, 24 Aug 2023 18:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12965v1</guid></item><item><title>Improving 3D Pose Estimation for Sign Language</title><link>http://arxiv.org/abs/2308.09525v1</link><description>This work addresses 3D human pose reconstruction in single images. We presenta method that combines Forward Kinematics (FK) with neural networks to ensure afast and valid prediction of 3D pose. Pose is represented as a hierarchicaltree/graph with nodes corresponding to human joints that model their physicallimits. Given a 2D detection of keypoints in the image, we lift the skeleton to3D using neural networks to predict both the joint rotations and bone lengths.These predictions are then combined with skeletal constraints using an FK layerimplemented as a network layer in PyTorch. The result is a fast and accurateapproach to the estimation of 3D skeletal pose. Through quantitative andqualitative evaluation, we demonstrate the method is significantly moreaccurate than MediaPipe in terms of both per joint positional error and visualappearance. Furthermore, we demonstrate generalization over different datasets.The implementation in PyTorch runs at between 100-200 milliseconds per image(including CNN detection) using CPU only.</description><author>Maksym Ivashechkin, Oscar Mendez, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 14:05:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09525v1</guid></item><item><title>Unsupervised Reconstruction of 3D Human Pose Interactions From 2D Poses Alone</title><link>http://arxiv.org/abs/2309.14865v1</link><description>Current unsupervised 2D-3D human pose estimation (HPE) methods do not work inmulti-person scenarios due to perspective ambiguity in monocular images.Therefore, we present one of the first studies investigating the feasibility ofunsupervised multi-person 2D-3D HPE from just 2D poses alone, focusing onreconstructing human interactions. To address the issue of perspectiveambiguity, we expand upon prior work by predicting the cameras' elevation anglerelative to the subjects' pelvis. This allows us to rotate the predicted posesto be level with the ground plane, while obtaining an estimate for the verticaloffset in 3D between individuals. Our method involves independently liftingeach subject's 2D pose to 3D, before combining them in a shared 3D coordinatesystem. The poses are then rotated and offset by the predicted elevation anglebefore being scaled. This by itself enables us to retrieve an accurate 3Dreconstruction of their poses. We present our results on the CHI3D dataset,introducing its use for unsupervised 2D-3D pose estimation with three newquantitative metrics, and establishing a benchmark for future research.</description><author>Peter Hardy, Hansung Kim</author><pubDate>Tue, 26 Sep 2023 12:42:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14865v1</guid></item><item><title>Diff-OP3D: Bridging 2D Diffusion for Open Pose 3D Zero-Shot Classification</title><link>http://arxiv.org/abs/2312.07039v1</link><description>With the explosive 3D data growth, the urgency of utilizing zero-shotlearning to facilitate data labeling becomes evident. Recently, the methods viatransferring Contrastive Language-Image Pre-training (CLIP) to 3D vision havemade great progress in the 3D zero-shot classification task. However, thesemethods primarily focus on aligned pose 3D objects (ap-3os), overlooking therecognition of 3D objects with open poses (op-3os) typically encountered inreal-world scenarios, such as an overturned chair or a lying teddy bear. Tothis end, we propose a more challenging benchmark for 3D open-pose zero-shotclassification. Echoing our benchmark, we design a concise angle-refinementmechanism that automatically optimizes one ideal pose as well as classifiesthese op-3os. Furthermore, we make a first attempt to bridge 2D pre-traineddiffusion model as a classifer to 3D zero-shot classification without anyadditional training. Such 2D diffusion to 3D objects proves vital in improvingzero-shot classification for both ap-3os and op-3os. Our model notably improvesby 3.5% and 15.8% on ModelNet10$^{\ddag}$ and McGill$^{\ddag}$ open posebenchmarks, respectively, and surpasses the current state-of-the-art by 6.8% onthe aligned pose ModelNet10, affirming diffusion's efficacy in 3D zero-shottasks.</description><author>Weiguang Zhao, Guanyu Yang, Chaolong Yang, Chenru Jiang, Yuyao Yan, Rui Zhang, Kaizhu Huang</author><pubDate>Tue, 12 Dec 2023 07:52:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07039v1</guid></item><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Neural Voting Field for Camera-Space 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2305.04328v1</link><description>We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.</description><author>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</author><pubDate>Sun, 07 May 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04328v1</guid></item><item><title>Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</title><link>http://arxiv.org/abs/2308.10305v1</link><description>Despite significant progress in single image-based 3D human mesh recovery,accurately and smoothly recovering 3D human motion from a video remainschallenging. Existing video-based methods generally recover human mesh byestimating the complex pose and shape parameters from coupled image features,whose high complexity and low representation ability often result ininconsistent pose motion and limited shape patterns. To alleviate this issue,we introduce 3D pose as the intermediary and propose a Pose and MeshCo-Evolution network (PMCE) that decouples this task into two parts: 1)video-based 3D human pose estimation and 2) mesh vertices regression from theestimated 3D pose and temporal image feature. Specifically, we propose atwo-stream encoder that estimates mid-frame 3D pose and extracts a temporalimage feature from the input image sequence. In addition, we design aco-evolution decoder that performs pose and mesh interactions with theimage-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit thehuman body shape. Extensive experiments demonstrate that the proposed PMCEoutperforms previous state-of-the-art methods in terms of both per-frameaccuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M,and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.</description><author>Yingxuan You, Hong Liu, Ti Wang, Wenhao Li, Runwei Ding, Xia Li</author><pubDate>Sun, 20 Aug 2023 17:03:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10305v1</guid></item><item><title>Dual-Side Feature Fusion 3D Pose Transfer</title><link>http://arxiv.org/abs/2305.14951v1</link><description>3D pose transfer solves the problem of additional input and correspondence oftraditional deformation transfer, only the source and target meshes need to beinput, and the pose of the source mesh can be transferred to the target mesh.Some lightweight methods proposed in recent years consume less memory but causespikes and distortions for some unseen poses, while others are costly intraining due to the inclusion of large matrix multiplication and adversarialnetworks. In addition, the meshes with different numbers of vertices alsoincrease the difficulty of pose transfer. In this work, we propose a Dual-SideFeature Fusion Pose Transfer Network to improve the pose transfer accuracy ofthe lightweight method. Our method takes the pose features as one of the sideinputs to the decoding network and fuses them into the target mesh layer bylayer at multiple scales. Our proposed Feature Fusion Adaptive InstanceNormalization has the characteristic of having two side input channels thatfuse pose features and identity features as denormalization parameters, thusenhancing the pose transfer capability of the network. Extensive experimentalresults show that our proposed method has stronger pose transfer capabilitythan state-of-the-art methods while maintaining a lightweight networkstructure, and can converge faster.</description><author>Jue Liu, Feipeng Da</author><pubDate>Wed, 24 May 2023 10:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14951v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video</title><link>http://arxiv.org/abs/2307.05853v2</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 14% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively). GitHub:https://github.com/bruceyo/GLA-GCN.</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Sat, 22 Jul 2023 02:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v2</guid></item><item><title>Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2303.16456v2</link><description>When applying a pre-trained 2D-to-3D human pose lifting model to a targetunseen dataset, large performance degradation is commonly encountered due todomain shift issues. We observe that the degradation is caused by two factors:1) the large distribution gap over global positions of poses between the sourceand target datasets due to variant camera parameters and settings, and 2) thedeficient diversity of local structures of poses in training. To this end, wecombine \textbf{global adaptation} and \textbf{local generalization} in\textit{PoseDA}, a simple yet effective framework of unsupervised domainadaptation for 3D human pose estimation. Specifically, global adaptation aimsto align global positions of poses from the source domain to the target domainwith a proposed global position alignment (GPA) module. And localgeneralization is designed to enhance the diversity of 2D-3D pose mapping witha local pose augmentation (LPA) module. These modules bring significantperformance improvement without introducing additional learnable parameters. Inaddition, we propose local pose augmentation (LPA) to enhance the diversity of3D poses following an adversarial training scheme consisting of 1) aaugmentation generator that generates the parameters of pre-defined posetransformations and 2) an anchor discriminator to ensure the reality andquality of the augmented data. Our approach can be applicable to almost all2D-3D lifting models. \textit{PoseDA} achieves 61.3 mm of MPJPE on MPI-INF-3DHPunder a cross-dataset evaluation setup, improving upon the previousstate-of-the-art method by 10.2\%.</description><author>Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang, Gaoang Wang</author><pubDate>Thu, 17 Aug 2023 07:55:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16456v2</guid></item><item><title>Scene-aware Egocentric 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2212.11684v3</link><description>Egocentric 3D human pose estimation with a single head-mounted fisheye camerahas recently attracted attention due to its numerous applications in virtualand augmented reality. Existing methods still struggle in challenging poseswhere the human body is highly occluded or is closely interacting with thescene. To address this issue, we propose a scene-aware egocentric poseestimation method that guides the prediction of the egocentric pose with sceneconstraints. To this end, we propose an egocentric depth estimation network topredict the scene depth map from a wide-view egocentric fisheye camera whilemitigating the occlusion of the human body with a depth-inpainting network.Next, we propose a scene-aware pose estimation network that projects the 2Dimage features and estimated depth map of the scene into a voxel space andregresses the 3D pose with a V2V network. The voxel-based featurerepresentation provides the direct geometric connection between 2D imagefeatures and scene geometry, and further facilitates the V2V network toconstrain the predicted pose based on the estimated scene geometry. To enablethe training of the aforementioned networks, we also generated a syntheticdataset, called EgoGTA, and an in-the-wild dataset based on EgoPW, calledEgoPW-Scene. The experimental results of our new evaluation sequences show thatthe predicted 3D egocentric poses are accurate and physically plausible interms of human-scene interaction, demonstrating that our method outperforms thestate-of-the-art methods both quantitatively and qualitatively.</description><author>Jian Wang, Lingjie Liu, Weipeng Xu, Kripasindhu Sarkar, Diogo Luvizon, Christian Theobalt</author><pubDate>Mon, 25 Sep 2023 21:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.11684v3</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v2</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenxuan Guo, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Sun, 06 Aug 2023 15:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v2</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v1</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Tue, 01 Aug 2023 16:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v1</guid></item><item><title>Weakly-supervised 3D Pose Transfer with Keypoints</title><link>http://arxiv.org/abs/2307.13459v2</link><description>The main challenges of 3D pose transfer are: 1) Lack of paired training datawith different characters performing the same pose; 2) Disentangling pose andshape information from the target mesh; 3) Difficulty in applying to mesheswith different topologies. We thus propose a novel weakly-supervisedkeypoint-based framework to overcome these difficulties. Specifically, we use atopology-agnostic keypoint detector with inverse kinematics to computetransformations between the source and target meshes. Our method only requiressupervision on the keypoints, can be applied to meshes with differenttopologies and is shape-invariant for the target which allows extraction ofpose-only information from the target meshes without transferring shapeinformation. We further design a cycle reconstruction to performself-supervised pose transfer without the need for ground truth deformed meshwith the same pose and shape as the target and source, respectively. Weevaluate our approach on benchmark human and animal datasets, where we achievesuperior performance compared to the state-of-the-art unsupervised approachesand even comparable performance with the fully supervised approaches. We teston the more challenging Mixamo dataset to verify our approach's ability inhandling meshes with different topologies and complex clothes. Cross-datasetevaluation further shows the strong generalization ability of our approach.</description><author>Jinnan Chen, Chen Li, Gim Hee Lee</author><pubDate>Thu, 17 Aug 2023 07:02:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13459v2</guid></item><item><title>Weakly-supervised 3D Pose Transfer with Keypoints</title><link>http://arxiv.org/abs/2307.13459v1</link><description>The main challenges of 3D pose transfer are: 1) Lack of paired training datawith different characters performing the same pose; 2) Disentangling pose andshape information from the target mesh; 3) Difficulty in applying to mesheswith different topologies. We thus propose a novel weakly-supervisedkeypoint-based framework to overcome these difficulties. Specifically, we use atopology-agnostic keypoint detector with inverse kinematics to computetransformations between the source and target meshes. Our method only requiressupervision on the keypoints, can be applied to meshes with differenttopologies and is shape-invariant for the target which allows extraction ofpose-only information from the target meshes without transferring shapeinformation. We further design a cycle reconstruction to performself-supervised pose transfer without the need for ground truth deformed meshwith the same pose and shape as the target and source, respectively. Weevaluate our approach on benchmark human and animal datasets, where we achievesuperior performance compared to the state-of-the-art unsupervised approachesand even comparable performance with the fully supervised approaches. We teston the more challenging Mixamo dataset to verify our approach's ability inhandling meshes with different topologies and complex clothes. Cross-datasetevaluation further shows the strong generalization ability of our approach.</description><author>Jinnan Chen, Chen Li, Gim Hee Lee</author><pubDate>Tue, 25 Jul 2023 13:40:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13459v1</guid></item><item><title>3D Pose Nowcasting: Forecast the Future to Improve the Present</title><link>http://arxiv.org/abs/2308.12914v2</link><description>Technologies to enable safe and effective collaboration and coexistencebetween humans and robots have gained significant importance in the last fewyears. A critical component useful for realizing this collaborative paradigm isthe understanding of human and robot 3D poses using non-invasive systems.Therefore, in this paper, we propose a novel vision-based system leveragingdepth data to accurately establish the 3D locations of skeleton joints.Specifically, we introduce the concept of Pose Nowcasting, denoting thecapability of the proposed system to enhance its current pose estimationaccuracy by jointly learning to forecast future poses. The experimentalevaluation is conducted on two different datasets, providing accurate andreal-time performance and confirming the validity of the proposed method onboth the robotic and human scenarios.</description><author>Alessandro Simoni, Francesco Marchetti, Guido Borghi, Federico Becattini, Lorenzo Seidenari, Roberto Vezzani, Alberto Del Bimbo</author><pubDate>Sat, 18 Nov 2023 15:22:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12914v2</guid></item><item><title>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</title><link>http://arxiv.org/abs/2206.02257v3</link><description>In this survey, we present a systematic review of 3D hand pose estimationfrom the perspective of efficient annotation and learning. 3D hand poseestimation has been an important research area owing to its potential to enablevarious applications, such as video understanding, AR/VR, and robotics.However, the performance of models is tied to the quality and quantity ofannotated 3D hand poses. Under the status quo, acquiring such annotated 3D handposes is challenging, e.g., due to the difficulty of 3D annotation and thepresence of occlusion. To reveal this problem, we review the pros and cons ofexisting annotation methods classified as manual, synthetic-model-based,hand-sensor-based, and computational approaches. Additionally, we examinemethods for learning 3D hand poses when annotated data are scarce, includingself-supervised pretraining, semi-supervised learning, and domain adaptation.Based on the study of efficient annotation and learning, we further discusslimitations and possible future directions in this field.</description><author>Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato</author><pubDate>Wed, 26 Apr 2023 07:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02257v3</guid></item><item><title>Geometry-Biased Transformer for Robust Multi-View 3D Human Pose Reconstruction</title><link>http://arxiv.org/abs/2312.17106v1</link><description>We address the challenges in estimating 3D human poses from multiple viewsunder occlusion and with limited overlapping views. We approach multi-view,single-person 3D human pose reconstruction as a regression problem and proposea novel encoder-decoder Transformer architecture to estimate 3D poses frommulti-view 2D pose sequences. The encoder refines 2D skeleton joints detectedacross different views and times, fusing multi-view and temporal informationthrough global self-attention. We enhance the encoder by incorporating ageometry-biased attention mechanism, effectively leveraging geometricrelationships between views. Additionally, we use detection scores provided bythe 2D pose detector to further guide the encoder's attention based on thereliability of the 2D detections. The decoder subsequently regresses the 3Dpose sequence from these refined tokens, using pre-defined queries for eachjoint. To enhance the generalization of our method to unseen scenes and improveresilience to missing joints, we implement strategies including scenecentering, synthetic views, and token dropout. We conduct extensive experimentson three benchmark public datasets, Human3.6M, CMU Panoptic andOcclusion-Persons. Our results demonstrate the efficacy of our approach,particularly in occluded scenes and when few views are available, which aretraditionally challenging scenarios for triangulation-based methods.</description><author>Olivier Moliner, Sangxia Huang, Kalle Åström</author><pubDate>Thu, 28 Dec 2023 16:30:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17106v1</guid></item><item><title>Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields</title><link>http://arxiv.org/abs/2212.02493v3</link><description>Coordinate-based implicit neural networks, or neural fields, have emerged asuseful representations of shape and appearance in 3D computer vision. Despiteadvances, however, it remains challenging to build neural fields for categoriesof objects without datasets like ShapeNet that provide "canonicalized" objectinstances that are consistently aligned for their 3D position and orientation(pose). We present Canonical Field Network (CaFi-Net), a self-supervised methodto canonicalize the 3D pose of instances from an object category represented asneural fields, specifically neural radiance fields (NeRFs). CaFi-Net directlylearns from continuous and noisy radiance fields using a Siamese networkarchitecture that is designed to extract equivariant field features forcategory-level canonicalization. During inference, our method takes pre-trainedneural radiance fields of novel object instances at arbitrary 3D pose andestimates a canonical field with consistent 3D pose across the entire category.Extensive experiments on a new dataset of 1300 NeRF models across 13 objectcategories show that our method matches or exceeds the performance of 3D pointcloud-based methods.</description><author>Rohith Agaram, Shaurya Dewan, Rahul Sajnani, Adrien Poulenard, Madhava Krishna, Srinath Sridhar</author><pubDate>Wed, 17 May 2023 12:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02493v3</guid></item><item><title>Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet</title><link>http://arxiv.org/abs/2207.04320v3</link><description>Multi-person pose understanding from RGB videos involves three complex tasks:pose estimation, tracking and motion forecasting. Intuitively, accuratemulti-person pose estimation facilitates robust tracking, and robust trackingbuilds crucial history for correct motion forecasting. Most existing workseither focus on a single task or employ multi-stage approaches to solvingmultiple tasks separately, which tends to make sub-optimal decision at eachstage and also fail to exploit correlations among the three tasks. In thispaper, we propose Snipper, a unified framework to perform multi-person 3D poseestimation, tracking, and motion forecasting simultaneously in a single stage.We propose an efficient yet powerful deformable attention mechanism toaggregate spatiotemporal information from the video snippet. Building upon thisdeformable attention, a video transformer is learned to encode thespatiotemporal features from the multi-frame snippet and to decode informativepose features for multi-person pose queries. Finally, these pose queries areregressed to predict multi-person pose trajectories and future motions in asingle shot. In the experiments, we show the effectiveness of Snipper on threechallenging public datasets where our generic model rivals specializedstate-of-art baselines for pose estimation, tracking, and forecasting.</description><author>Shihao Zou, Yuanlu Xu, Chao Li, Lingni Ma, Li Cheng, Minh Vo</author><pubDate>Tue, 12 Sep 2023 22:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.04320v3</guid></item><item><title>Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving</title><link>http://arxiv.org/abs/2307.14889v1</link><description>Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomousvehicles (AVs) to make informed decisions and respond proactively in criticalroad scenarios. Promising results of 3D HPE have been gained in several domainssuch as human-computer interaction, robotics, sports and medical analytics,often based on data collected in well-controlled laboratory environments.Nevertheless, the transfer of 3D HPE methods to AVs has received limitedresearch attention, due to the challenges posed by obtaining accurate 3D poseannotations and the limited suitability of data from other domains. We present a simple yet efficient weakly supervised approach for 3D HPE inthe AV context by employing a high-level sensor fusion between camera and LiDARdata. The weakly supervised setting enables training on the target datasetswithout any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractorand pseudo labels generated from LiDAR to image projections. Our approachoutperforms state-of-the-art results by up to $\sim$ 13% on the Waymo OpenDataset in the weakly supervised setting and achieves state-of-the-art resultsin the supervised setting.</description><author>Peter Bauer, Arij Bouazizi, Ulrich Kressel, Fabian B. Flohr</author><pubDate>Thu, 27 Jul 2023 15:28:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14889v1</guid></item><item><title>Human Part-wise 3D Motion Context Learning for Sign Language Recognition</title><link>http://arxiv.org/abs/2308.09305v1</link><description>In this paper, we propose P3D, the human part-wise motion context learningframework for sign language recognition. Our main contributions lie in twodimensions: learning the part-wise motion context and employing the poseensemble to utilize 2D and 3D pose jointly. First, our empirical observationimplies that part-wise context encoding benefits the performance of signlanguage recognition. While previous methods of sign language recognitionlearned motion context from the sequence of the entire pose, we argue that suchmethods cannot exploit part-specific motion context. In order to utilizepart-wise motion context, we propose the alternating combination of a part-wiseencoding Transformer (PET) and a whole-body encoding Transformer (WET). PETencodes the motion contexts from a part sequence, while WET merges them into aunified context. By learning part-wise motion context, our P3D achievessuperior performance on WLASL compared to previous state-of-the-art methods.Second, our framework is the first to ensemble 2D and 3D poses for signlanguage recognition. Since the 3D pose holds rich motion context and depthinformation to distinguish the words, our P3D outperformed the previousstate-of-the-art methods employing a pose ensemble.</description><author>Taeryung Lee, Yeonguk Oh, Kyoung Mu Lee</author><pubDate>Fri, 18 Aug 2023 06:01:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09305v1</guid></item><item><title>PoseGraphNet++: Enriching 3D Human Pose with Orientation Estimation</title><link>http://arxiv.org/abs/2308.11440v1</link><description>Existing kinematic skeleton-based 3D human pose estimation methods onlypredict joint positions. Although this is sufficient to compute the yaw andpitch of the bone rotations, the roll around the axis of the bones remainsunresolved by these methods. In this paper, we propose a novel 2D-to-3D liftingGraph Convolution Network named PoseGraphNet++ to predict the complete humanpose including the joint positions and the bone orientations. We employ nodeand edge convolutions to utilize the joint and bone features. Our model isevaluated on multiple benchmark datasets, and its performance is either on parwith or better than the state-of-the-art in terms of both position and rotationmetrics. Through extensive ablation studies, we show that PoseGraphNet++benefits from exploiting the mutual relationship between the joints and thebones.</description><author>Soubarna Banik, Edvard Avagyan, Alejandro Mendoza Gracia, Alois Knoll</author><pubDate>Tue, 22 Aug 2023 14:42:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11440v1</guid></item><item><title>RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with Occlusion Handling</title><link>http://arxiv.org/abs/2311.14242v1</link><description>In the domain of 3D Human Pose Estimation, which finds widespread dailyapplications, the requirement for convenient acquisition equipment continues togrow. To satisfy this demand, we set our sights on a short-baseline binocularsetting that offers both portability and a geometric measurement property thatradically mitigates depth ambiguity. However, as the binocular baselineshortens, two serious challenges emerge: first, the robustness of 3Dreconstruction against 2D errors deteriorates; and second, occlusion reoccursdue to the limited visual differences between two views. To address the firstchallenge, we propose the Stereo Co-Keypoints Estimation module to improve theview consistency of 2D keypoints and enhance the 3D robustness. In this module,the disparity is utilized to represent the correspondence of binocular 2Dpoints and the Stereo Volume Feature is introduced to contain binocularfeatures across different disparities. Through the regression of SVF, two-view2D keypoints are simultaneously estimated in a collaborative way whichrestricts their view consistency. Furthermore, to deal with occlusions, aPre-trained Pose Transformer module is introduced. Through this module, 3Dposes are refined by perceiving pose coherence, a representation of jointcorrelations. This perception is injected by the Pose Transformer network andlearned through a pre-training task that recovers iterative masked joints.Comprehensive experiments carried out on H36M and MHAD datasets, complementedby visualizations, validate the effectiveness of our approach in theshort-baseline binocular 3D Human Pose Estimation and occlusion handling.</description><author>Xiaoyue Wan, Zhuo Chen, Yiming Bao, Xu Zhao</author><pubDate>Fri, 24 Nov 2023 01:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14242v1</guid></item><item><title>Diffusion-based Pose Refinement and Muti-hypothesis Generation for 3D Human Pose Estimaiton</title><link>http://arxiv.org/abs/2401.04921v1</link><description>Previous probabilistic models for 3D Human Pose Estimation (3DHPE) aimed toenhance pose accuracy by generating multiple hypotheses. However, most of thehypotheses generated deviate substantially from the true pose. Compared todeterministic models, the excessive uncertainty in probabilistic models leadsto weaker performance in single-hypothesis prediction. To address these twochallenges, we propose a diffusion-based refinement framework called DRPose,which refines the output of deterministic models by reverse diffusion andachieves more suitable multi-hypothesis prediction for the current posebenchmark by multi-step refinement with multiple noises. To this end, wepropose a Scalable Graph Convolution Transformer (SGCT) and a Pose RefinementModule (PRM) for denoising and refining. Extensive experiments on Human3.6M andMPI-INF-3DHP datasets demonstrate that our method achieves state-of-the-artperformance on both single and multi-hypothesis 3DHPE. Code is available athttps://github.com/KHB1698/DRPose.</description><author>Hongbo Kang, Yong Wang, Mengyuan Liu, Doudou Wu, Peng Liu, Xinlin Yuan, Wenming Yang</author><pubDate>Wed, 10 Jan 2024 04:07:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.04921v1</guid></item><item><title>FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations</title><link>http://arxiv.org/abs/2211.14309v2</link><description>We present a generative approach to forecast long-term future human behaviorin 3D, requiring only weak supervision from readily available 2D human actiondata. This is a fundamental task enabling many downstream applications. Therequired ground-truth data is hard to capture in 3D (mocap suits, expensivesetups) but easy to acquire in 2D (simple RGB cameras). Thus, we design ourmethod to only require 2D RGB data while being able to generate 3D human motionsequences. We use a differentiable 2D projection scheme in an autoregressivemanner for weak supervision, and an adversarial loss for 3D regularization. Ourmethod predicts long and complex behavior sequences (e.g. cooking, assembly)consisting of multiple sub-actions. We tackle this in a semanticallyhierarchical manner, jointly predicting high-level coarse action labelstogether with their low-level fine-grained realizations as characteristic 3Dhuman poses. We observe that these two action representations are coupled innature, and joint prediction benefits both action and pose forecasting. Ourexperiments demonstrate the complementary nature of joint action and 3D poseprediction: our joint approach outperforms each task treated individually,enables robust longer-term sequence prediction, and outperforms alternativeapproaches to forecast actions and characteristic 3D poses.</description><author>Christian Diller, Thomas Funkhouser, Angela Dai</author><pubDate>Mon, 27 Nov 2023 18:48:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.14309v2</guid></item><item><title>A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2311.03312v2</link><description>The dominant paradigm in 3D human pose estimation that lifts a 2D posesequence to 3D heavily relies on long-term temporal clues (i.e., using adaunting number of video frames) for improved accuracy, which incursperformance saturation, intractable computation and the non-causal problem.This can be attributed to their inherent inability to perceive spatial contextas plain 2D joint coordinates carry no visual cues. To address this issue, wepropose a straightforward yet powerful solution: leveraging the readilyavailable intermediate visual representations produced by off-the-shelf(pre-trained) 2D pose detectors -- no finetuning on the 3D task is even needed.The key observation is that, while the pose detector learns to localize 2Djoints, such representations (e.g., feature maps) implicitly encode thejoint-centric spatial context thanks to the regional operations in backbonenetworks. We design a simple baseline named Context-Aware PoseFormer toshowcase its effectiveness. Without access to any temporal information, theproposed method significantly outperforms its context-agnostic counterpart,PoseFormer, and other state-of-the-art methods using up to hundreds of videoframes regarding both speed and precision. Project page:https://qitaozhao.github.io/ContextAware-PoseFormer</description><author>Qitao Zhao, Ce Zheng, Mengyuan Liu, Chen Chen</author><pubDate>Thu, 09 Nov 2023 04:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03312v2</guid></item><item><title>A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2311.03312v1</link><description>The dominant paradigm in 3D human pose estimation that lifts a 2D posesequence to 3D heavily relies on long-term temporal clues (i.e., using adaunting number of video frames) for improved accuracy, which incursperformance saturation, intractable computation and the non-causal problem.This can be attributed to their inherent inability to perceive spatial contextas plain 2D joint coordinates carry no visual cues. To address this issue, wepropose a straightforward yet powerful solution: leveraging the readilyavailable intermediate visual representations produced by off-the-shelf(pre-trained) 2D pose detectors -- no finetuning on the 3D task is even needed.The key observation is that, while the pose detector learns to localize 2Djoints, such representations (e.g., feature maps) implicitly encode thejoint-centric spatial context thanks to the regional operations in backbonenetworks. We design a simple baseline named Context-Aware PoseFormer toshowcase its effectiveness. Without access to any temporal information, theproposed method significantly outperforms its context-agnostic counterpart,PoseFormer, and other state-of-the-art methods using up to hundreds of videoframes regarding both speed and precision. Project page:https://qitaozhao.github.io/ContextAware-PoseFormer</description><author>Qitao Zhao, Ce Zheng, Mengyuan Liu, Chen Chen</author><pubDate>Mon, 06 Nov 2023 18:04:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.03312v1</guid></item><item><title>EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With Kinematic Structure Priors</title><link>http://arxiv.org/abs/2306.09615v1</link><description>Transformer is popular in recent 3D human pose estimation, which utilizeslong-term modeling to lift 2D keypoints into the 3D space. However, currenttransformer-based methods do not fully exploit the prior knowledge of the humanskeleton provided by the kinematic structure. In this paper, we propose a noveltransformer-based model EvoPose to introduce the human body prior knowledge for3D human pose estimation effectively. Specifically, a Structural PriorsRepresentation (SPR) module represents human priors as structural featurescarrying rich body patterns, e.g. joint relationships. The structural featuresare interacted with 2D pose sequences and help the model to achieve moreinformative spatiotemporal features. Moreover, a Recursive Refinement (RR)module is applied to refine the 3D pose outputs by utilizing estimated resultsand further injects human priors simultaneously. Extensive experimentsdemonstrate the effectiveness of EvoPose which achieves a new state of the arton two most popular benchmarks, Human3.6M and MPI-INF-3DHP.</description><author>Yaqi Zhang, Yan Lu, Bin Liu, Zhiwei Zhao, Qi Chu, Nenghai Yu</author><pubDate>Fri, 16 Jun 2023 05:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09615v1</guid></item><item><title>Iterative Graph Filtering Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16074v1</link><description>Graph convolutional networks (GCNs) have proven to be an effective approachfor 3D human pose estimation. By naturally modeling the skeleton structure ofthe human body as a graph, GCNs are able to capture the spatial relationshipsbetween joints and learn an efficient representation of the underlying pose.However, most GCN-based methods use a shared weight matrix, making itchallenging to accurately capture the different and complex relationshipsbetween joints. In this paper, we introduce an iterative graph filteringframework for 3D human pose estimation, which aims to predict the 3D jointpositions given a set of 2D joint locations in images. Our approach builds uponthe idea of iteratively solving graph filtering with Laplacian regularizationvia the Gauss-Seidel iterative method. Motivated by this iterative solution, wedesign a Gauss-Seidel network (GS-Net) architecture, which makes use of weightand adjacency modulation, skip connection, and a pure convolutional block withlayer normalization. Adjacency modulation facilitates the learning of edgesthat go beyond the inherent connections of body joints, resulting in anadjusted graph structure that reflects the human skeleton, while skipconnections help maintain crucial information from the input layer's initialfeatures as the network depth increases. We evaluate our proposed model on twostandard benchmark datasets, and compare it with a comprehensive set of strongbaseline methods for 3D human pose estimation. Our experimental resultsdemonstrate that our approach outperforms the baseline methods on bothdatasets, achieving state-of-the-art performance. Furthermore, we conductablation studies to analyze the contributions of different components of ourmodel architecture and show that the skip connection and adjacency modulationhelp improve the model performance.</description><author>Zaedul Islam, A. Ben Hamza</author><pubDate>Sat, 29 Jul 2023 21:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16074v1</guid></item><item><title>Iterative Graph Filtering Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16074v2</link><description>Graph convolutional networks (GCNs) have proven to be an effective approachfor 3D human pose estimation. By naturally modeling the skeleton structure ofthe human body as a graph, GCNs are able to capture the spatial relationshipsbetween joints and learn an efficient representation of the underlying pose.However, most GCN-based methods use a shared weight matrix, making itchallenging to accurately capture the different and complex relationshipsbetween joints. In this paper, we introduce an iterative graph filteringframework for 3D human pose estimation, which aims to predict the 3D jointpositions given a set of 2D joint locations in images. Our approach builds uponthe idea of iteratively solving graph filtering with Laplacian regularizationvia the Gauss-Seidel iterative method. Motivated by this iterative solution, wedesign a Gauss-Seidel network (GS-Net) architecture, which makes use of weightand adjacency modulation, skip connection, and a pure convolutional block withlayer normalization. Adjacency modulation facilitates the learning of edgesthat go beyond the inherent connections of body joints, resulting in anadjusted graph structure that reflects the human skeleton, while skipconnections help maintain crucial information from the input layer's initialfeatures as the network depth increases. We evaluate our proposed model on twostandard benchmark datasets, and compare it with a comprehensive set of strongbaseline methods for 3D human pose estimation. Our experimental resultsdemonstrate that our approach outperforms the baseline methods on bothdatasets, achieving state-of-the-art performance. Furthermore, we conductablation studies to analyze the contributions of different components of ourmodel architecture and show that the skip connection and adjacency modulationhelp improve the model performance.</description><author>Zaedul Islam, A. Ben Hamza</author><pubDate>Mon, 07 Aug 2023 23:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16074v2</guid></item><item><title>3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking</title><link>http://arxiv.org/abs/2308.15316v3</link><description>Markerless methods for animal posture tracking have been rapidly developingrecently, but frameworks and benchmarks for tracking large animal groups in 3Dare still lacking. To overcome this gap in the literature, we present3D-MuPPET, a framework to estimate and track 3D poses of up to 10 pigeons atinteractive speed using multiple camera views. We train a pose estimator toinfer 2D keypoints and bounding boxes of multiple pigeons, then triangulate thekeypoints to 3D. For identity matching of individuals in all views, we firstdynamically match 2D detections to global identities in the first frame, thenuse a 2D tracker to maintain IDs across views in subsequent frames. We achievecomparable accuracy to a state of the art 3D pose estimator in terms of medianerror and Percentage of Correct Keypoints. Additionally, we benchmark theinference speed of 3D-MuPPET, with up to 9.45 fps in 2D and 1.89 fps in 3D, andperform quantitative tracking evaluation, which yields encouraging results.Finally, we showcase two novel applications for 3D-MuPPET. First, we train amodel with data of single pigeons and achieve comparable results in 2D and 3Dposture estimation for up to 5 pigeons. Second, we show that 3D-MuPPET alsoworks in outdoors without additional annotations from natural environments.Both use cases simplify the domain shift to new species and environments,largely reducing annotation effort needed for 3D posture tracking. To the bestof our knowledge we are the first to present a framework for 2D/3D animalposture and trajectory tracking that works in both indoor and outdoorenvironments for up to 10 individuals. We hope that the framework can open upnew opportunities in studying animal collective behaviour and encouragesfurther developments in 3D multi-animal posture tracking.</description><author>Urs Waldmann, Alex Hoi Hang Chan, Hemal Naik, Máté Nagy, Iain D. Couzin, Oliver Deussen, Bastian Goldluecke, Fumihiro Kano</author><pubDate>Fri, 15 Dec 2023 14:40:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15316v3</guid></item><item><title>CLIP-Hand3D: Exploiting 3D Hand Pose Estimation via Context-Aware Prompting</title><link>http://arxiv.org/abs/2309.16140v1</link><description>Contrastive Language-Image Pre-training (CLIP) starts to emerge in manycomputer vision tasks and has achieved promising performance. However, itremains underexplored whether CLIP can be generalized to 3D hand poseestimation, as bridging text prompts with pose-aware features presentssignificant challenges due to the discrete nature of joint positions in 3Dspace. In this paper, we make one of the first attempts to propose a novel 3Dhand pose estimator from monocular images, dubbed as CLIP-Hand3D, whichsuccessfully bridges the gap between text prompts and irregular detailed posedistribution. In particular, the distribution order of hand joints in various3D space directions is derived from pose labels, forming corresponding textprompts that are subsequently encoded into text representations.Simultaneously, 21 hand joints in the 3D space are retrieved, and their spatialdistribution (in x, y, and z axes) is encoded to form pose-aware features.Subsequently, we maximize semantic consistency for a pair of pose-text featuresfollowing a CLIP-based contrastive learning paradigm. Furthermore, acoarse-to-fine mesh regressor is designed, which is capable of effectivelyquerying joint-aware cues from the feature pyramid. Extensive experiments onseveral public hand benchmarks show that the proposed model attains asignificantly faster inference speed while achieving state-of-the-artperformance compared to methods utilizing the similar scale backbone.</description><author>Shaoxiang Guo, Qing Cai, Lin Qi, Junyu Dong</author><pubDate>Thu, 28 Sep 2023 04:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16140v1</guid></item><item><title>FreeMan: Towards Benchmarking 3D Human Pose Estimation in the Wild</title><link>http://arxiv.org/abs/2309.05073v2</link><description>Estimating the 3D structure of the human body from natural scenes is afundamental aspect of visual perception. This task carries great importance forfields like AIGC and human-robot interaction. In practice, 3D human poseestimation in real-world settings is a critical initial step in solving thisproblem. However, the current datasets, often collected under controlledlaboratory conditions using complex motion capture equipment and unvaryingbackgrounds, are insufficient. The absence of real-world datasets is stallingthe progress of this crucial task. To facilitate the development of 3D poseestimation, we present FreeMan, the first large-scale, real-world multi-viewdataset. FreeMan was captured by synchronizing 8 smartphones across diversescenarios. It comprises 11M frames from 8000 sequences, viewed from differentperspectives. These sequences cover 40 subjects across 10 different scenarios,each with varying lighting conditions. We have also established an automated,precise labeling pipeline that allows for large-scale processing efficiently.We provide comprehensive evaluation baselines for a range of tasks, underliningthe significant challenges posed by FreeMan. Further evaluations of standardindoor/outdoor human sensing datasets reveal that FreeMan offers robustrepresentation transferability in real and complex scenes. FreeMan is nowpublicly available at https://wangjiongw.github.io/freeman.</description><author>Jiong Wang, Fengyu Yang, Wenbo Gou, Bingliang Li, Danqi Yan, Ailing Zeng, Yijun Gao, Junle Wang, Ruimao Zhang</author><pubDate>Tue, 12 Sep 2023 16:39:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.05073v2</guid></item><item><title>3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking</title><link>http://arxiv.org/abs/2308.15316v1</link><description>Markerless methods for animal posture tracking have been developing recently,but frameworks and benchmarks for tracking large animal groups in 3D are stilllacking. To overcome this gap in the literature, we present 3D-MuPPET, aframework to estimate and track 3D poses of up to 10 pigeons at interactivespeed using multiple-views. We train a pose estimator to infer 2D keypoints andbounding boxes of multiple pigeons, then triangulate the keypoints to 3D. Forcorrespondence matching, we first dynamically match 2D detections to globalidentities in the first frame, then use a 2D tracker to maintaincorrespondences accross views in subsequent frames. We achieve comparableaccuracy to a state of the art 3D pose estimator for Root Mean Square Error(RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel usecase where our model trained with data of single pigeons provides comparableresults on data containing multiple pigeons. This can simplify the domain shiftto new species because annotating single animal data is less labour intensivethan multi-animal data. Additionally, we benchmark the inference speed of3D-MuPPET, with up to 10 fps in 2D and 1.5 fps in 3D, and perform quantitativetracking evaluation, which yields encouraging results. Finally, we show that3D-MuPPET also works in natural environments without model fine-tuning onadditional annotations. To the best of our knowledge we are the first topresent a framework for 2D/3D posture and trajectory tracking that works inboth indoor and outdoor environments.</description><author>Urs Waldmann, Alex Hoi Hang Chan, Hemal Naik, Máté Nagy, Iain D. Couzin, Oliver Deussen, Bastian Goldluecke, Fumihiro Kano</author><pubDate>Tue, 29 Aug 2023 15:02:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15316v1</guid></item><item><title>3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking</title><link>http://arxiv.org/abs/2308.15316v2</link><description>Markerless methods for animal posture tracking have been developing recently,but frameworks and benchmarks for tracking large animal groups in 3D are stilllacking. To overcome this gap in the literature, we present 3D-MuPPET, aframework to estimate and track 3D poses of up to 10 pigeons at interactivespeed using multiple-views. We train a pose estimator to infer 2D keypoints andbounding boxes of multiple pigeons, then triangulate the keypoints to 3D. Forcorrespondence matching, we first dynamically match 2D detections to globalidentities in the first frame, then use a 2D tracker to maintaincorrespondences accross views in subsequent frames. We achieve comparableaccuracy to a state of the art 3D pose estimator for Root Mean Square Error(RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel usecase where our model trained with data of single pigeons provides comparableresults on data containing multiple pigeons. This can simplify the domain shiftto new species because annotating single animal data is less labour intensivethan multi-animal data. Additionally, we benchmark the inference speed of3D-MuPPET, with up to 10 fps in 2D and 1.5 fps in 3D, and perform quantitativetracking evaluation, which yields encouraging results. Finally, we show that3D-MuPPET also works in natural environments without model fine-tuning onadditional annotations. To the best of our knowledge we are the first topresent a framework for 2D/3D posture and trajectory tracking that works inboth indoor and outdoor environments.</description><author>Urs Waldmann, Alex Hoi Hang Chan, Hemal Naik, Máté Nagy, Iain D. Couzin, Oliver Deussen, Bastian Goldluecke, Fumihiro Kano</author><pubDate>Fri, 22 Sep 2023 10:00:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15316v2</guid></item><item><title>3D Hand Pose Estimation in Egocentric Images in the Wild</title><link>http://arxiv.org/abs/2312.06583v1</link><description>We present WildHands, a method for 3D hand pose estimation in egocentricimages in the wild. This is challenging due to (a) lack of 3D hand poseannotations for images in the wild, and (b) a form of perspectivedistortion-induced shape ambiguity that arises in the analysis of crops aroundhands. For the former, we use auxiliary supervision on in-the-wild data in theform of segmentation masks &amp; grasp labels in addition to 3D supervisionavailable in lab datasets. For the latter, we provide spatial cues about thelocation of the hand crop in the camera's field of view. Our approach achievesthe best 3D hand pose on the ARCTIC leaderboard and outperforms FrankMocap, apopular and robust approach for estimating hand pose in the wild, by 45.3% whenevaluated on 2D hand pose on our EPIC-HandKps dataset.</description><author>Aditya Prakash, Ruisen Tu, Matthew Chang, Saurabh Gupta</author><pubDate>Mon, 11 Dec 2023 18:15:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06583v1</guid></item><item><title>3D Human Pose Perception from Egocentric Stereo Videos</title><link>http://arxiv.org/abs/2401.00889v1</link><description>While head-mounted devices are becoming more compact, they provide egocentricviews with significant self-occlusions of the device user. Hence, existingmethods often fail to accurately estimate complex 3D poses from egocentricviews. In this work, we propose a new transformer-based framework to improveegocentric stereo 3D human pose estimation, which leverages the sceneinformation and temporal context of egocentric stereo videos. Specifically, weutilize 1) depth features from our 3D scene reconstruction module withuniformly sampled windows of egocentric stereo frames, and 2) human jointqueries enhanced by temporal features of the video inputs. Our method is ableto accurately estimate human poses even in challenging scenarios, such ascrouching and sitting. Furthermore, we introduce two new benchmark datasets,i.e., UnrealEgo2 and UnrealEgo-RW (RealWorld). The proposed datasets offer amuch larger number of egocentric stereo views with a wider variety of humanmotions than the existing datasets, allowing comprehensive evaluation ofexisting and upcoming methods. Our extensive experiments show that the proposedapproach significantly outperforms previous methods. We will releaseUnrealEgo2, UnrealEgo-RW, and trained models on our project page.</description><author>Hiroyasu Akada, Jian Wang, Vladislav Golyanik, Christian Theobalt</author><pubDate>Sat, 30 Dec 2023 21:21:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00889v1</guid></item><item><title>Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2311.12028v1</link><description>Transformers have been successfully applied in the field of video-based 3Dhuman pose estimation. However, the high computational costs of these videopose transformers (VPTs) make them impractical on resource-constrained devices.In this paper, we present a plug-and-play pruning-and-recovering framework,called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human poseestimation from videos. Our HoT begins with pruning pose tokens of redundantframes and ends with recovering full-length tokens, resulting in a few posetokens in the intermediate transformer blocks and thus improving the modelefficiency. To effectively achieve this, we propose a token pruning cluster(TPC) that dynamically selects a few representative tokens with high semanticdiversity while eliminating the redundancy of video frames. In addition, wedevelop a token recovering attention (TRA) to restore the detailedspatio-temporal information based on the selected tokens, thereby expanding thenetwork output to the original full-length temporal resolution for fastinference. Extensive experiments on two benchmark datasets (i.e., Human3.6M andMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency andestimation accuracy compared to the original VPT models. For instance, applyingto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPswithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,respectively. Our source code will be open-sourced.</description><author>Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe</author><pubDate>Mon, 20 Nov 2023 18:59:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12028v1</guid></item><item><title>HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.14581v2</link><description>2D-to-3D human pose lifting is fundamental for 3D human pose estimation(HPE). Graph Convolutional Network (GCN) has been proven inherently suitable tomodel the human skeletal topology. However, current GCN-based 3D HPE methodsupdate the node features by aggregating their neighbors' information withoutconsidering the interaction of joints in different motion patterns. Althoughsome studies import limb information to learn the movement patterns, the latentsynergies among joints, such as maintaining balance in the motion are seldominvestigated. We propose a hop-wise GraphFormer with intragroup jointrefinement (HopFIR) to tackle the 3D HPE problem. The HopFIR mainly consists ofa novel Hop-wise GraphFormer(HGF) module and an Intragroup JointRefinement(IJR) module which leverages the prior limb information forperipheral joints refinement. The HGF module groups the joints by $k$-hopneighbors and utilizes a hop-wise transformer-like attention mechanism amongthese groups to discover latent joint synergy. Extensive experimental resultsshow that HopFIR outperforms the SOTA methods with a large margin (on theHuman3.6M dataset, the mean per joint position error (MPJPE) is 32.67mm).Furthermore, it is also demonstrated that previous SOTA GCN-based methods canbenefit from the proposed hop-wise attention mechanism efficiently withsignificant performance promotion, such as SemGCN and MGCN are improved by 8.9%and 4.5%, respectively.</description><author>Kai Zhai, Qiang Nie, Bo Ouyang, Xiang Li, ShanLin Yang</author><pubDate>Tue, 18 Jul 2023 17:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14581v2</guid></item><item><title>3D Pose Estimation of Two Interacting Hands from a Monocular Event Camera</title><link>http://arxiv.org/abs/2312.14157v1</link><description>3D hand tracking from a monocular video is a very challenging problem due tohand interactions, occlusions, left-right hand ambiguity, and fast motion. Mostexisting methods rely on RGB inputs, which have severe limitations underlow-light conditions and suffer from motion blur. In contrast, event camerascapture local brightness changes instead of full image frames and do not sufferfrom the described effects. Unfortunately, existing image-based techniquescannot be directly applied to events due to significant differences in the datamodalities. In response to these challenges, this paper introduces the firstframework for 3D tracking of two fast-moving and interacting hands from asingle monocular event camera. Our approach tackles the left-right handambiguity with a novel semi-supervised feature-wise attention mechanism andintegrates an intersection loss to fix hand collisions. To facilitate advancesin this research domain, we release a new synthetic large-scale dataset of twointeracting hands, Ev2Hands-S, and a new real benchmark with real event streamsand ground-truth 3D annotations, Ev2Hands-R. Our approach outperforms existingmethods in terms of the 3D reconstruction accuracy and generalises to real dataunder severe light conditions.</description><author>Christen Millerdurai, Diogo Luvizon, Viktor Rudnev, André Jonas, Jiayi Wang, Christian Theobalt, Vladislav Golyanik</author><pubDate>Thu, 21 Dec 2023 18:59:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14157v1</guid></item><item><title>Denoising Diffusion for 3D Hand Pose Estimation from Images</title><link>http://arxiv.org/abs/2308.09523v1</link><description>Hand pose estimation from a single image has many applications. However,approaches to full 3D body pose estimation are typically trained on day-to-dayactivities or actions. As such, detailed hand-to-hand interactions are poorlyrepresented, especially during motion. We see this in the failure cases oftechniques such as OpenPose or MediaPipe. However, accurate hand poseestimation is crucial for many applications where the global body motion isless important than accurate hand pose estimation. This paper addresses the problem of 3D hand pose estimation from monocularimages or sequences. We present a novel end-to-end framework for 3D handregression that employs diffusion models that have shown excellent ability tocapture the distribution of data for generative purposes. Moreover, we enforcekinematic constraints to ensure realistic poses are generated by incorporatingan explicit forward kinematic layer as part of the network. The proposed modelprovides state-of-the-art performance when lifting a 2D single-hand image to3D. However, when sequence data is available, we add a Transformer module overa temporal window of consecutive frames to refine the results, overcomingjittering and further increasing accuracy. The method is quantitatively and qualitatively evaluated showingstate-of-the-art robustness, generalization, and accuracy on several differentdatasets.</description><author>Maksym Ivashechkin, Oscar Mendez, Richard Bowden</author><pubDate>Fri, 18 Aug 2023 13:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09523v1</guid></item><item><title>Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.01365v1</link><description>Accurately estimating the 3D pose of humans in video sequences requires bothaccuracy and a well-structured architecture. With the success of transformers,we introduce the Refined Temporal Pyramidal Compression-and-Amplification(RTPCA) transformer. Exploiting the temporal dimension, RTPCA extendsintra-block temporal modeling via its Temporal PyramidalCompression-and-Amplification (TPCA) structure and refines inter-block featureinteraction with a Cross-Layer Refinement (XLR) module. In particular, TPCAblock exploits a temporal pyramid paradigm, reinforcing key and valuerepresentation capabilities and seamlessly extracting spatial semantics frommotion sequences. We stitch these TPCA blocks with XLR that promotes richsemantic representation through continuous interaction of queries, keys, andvalues. This strategy embodies early-stage information with current flows,addressing typical deficits in detail and stability seen in othertransformer-based methods. We demonstrate the effectiveness of RTPCA byachieving state-of-the-art results on Human3.6M, HumanEva-I, and MPI-INF-3DHPbenchmarks with minimal computational overhead. The source code is available athttps://github.com/hbing-l/RTPCA.</description><author>Hanbing Li, Wangmeng Xiang, Jun-Yan He, Zhi-Qi Cheng, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 04 Sep 2023 06:25:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01365v1</guid></item><item><title>Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.01365v2</link><description>Accurately estimating the 3D pose of humans in video sequences requires bothaccuracy and a well-structured architecture. With the success of transformers,we introduce the Refined Temporal Pyramidal Compression-and-Amplification(RTPCA) transformer. Exploiting the temporal dimension, RTPCA extendsintra-block temporal modeling via its Temporal PyramidalCompression-and-Amplification (TPCA) structure and refines inter-block featureinteraction with a Cross-Layer Refinement (XLR) module. In particular, TPCAblock exploits a temporal pyramid paradigm, reinforcing key and valuerepresentation capabilities and seamlessly extracting spatial semantics frommotion sequences. We stitch these TPCA blocks with XLR that promotes richsemantic representation through continuous interaction of queries, keys, andvalues. This strategy embodies early-stage information with current flows,addressing typical deficits in detail and stability seen in othertransformer-based methods. We demonstrate the effectiveness of RTPCA byachieving state-of-the-art results on Human3.6M, HumanEva-I, and MPI-INF-3DHPbenchmarks with minimal computational overhead. The source code is available athttps://github.com/hbing-l/RTPCA.</description><author>Hanbing Liu, Wangmeng Xiang, Jun-Yan He, Zhi-Qi Cheng, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Wed, 06 Sep 2023 03:18:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01365v2</guid></item><item><title>Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.03833v2</link><description>Learning-based methods have dominated the 3D human pose estimation (HPE)tasks with significantly better performance in most benchmarks than traditionaloptimization-based methods. Nonetheless, 3D HPE in the wild is still thebiggest challenge of learning-based models, whether with 2D-3D lifting,image-to-3D, or diffusion-based methods, since the trained networks implicitlylearn camera intrinsic parameters and domain-based 3D human pose distributionsand estimate poses by statistical average. On the other hand, theoptimization-based methods estimate results case-by-case, which can predictmore diverse and sophisticated human poses in the wild. By combining theadvantages of optimization-based and learning-based methods, we propose theZero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve theproblem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDOachieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mmwithout training with any 2D-3D or image-3D pairs. Moreover, oursingle-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE$42.6$mm on cross-dataset evaluation, which even outperforms learning-basedmethods trained on 3DPW.</description><author>Zhongyu Jiang, Zhuoran Zhou, Lei Li, Wenhao Chai, Cheng-Yen Yang, Jenq-Neng Hwang</author><pubDate>Wed, 23 Aug 2023 18:40:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03833v2</guid></item><item><title>1st Place Solution of Egocentric 3D Hand Pose Estimation Challenge 2023 Technical Report:A Concise Pipeline for Egocentric Hand Pose Reconstruction</title><link>http://arxiv.org/abs/2310.04769v1</link><description>This report introduce our work on Egocentric 3D Hand Pose Estimationworkshop. Using AssemblyHands, this challenge focuses on egocentric 3D handpose estimation from a single-view image. In the competition, we adopt ViTbased backbones and a simple regressor for 3D keypoints prediction, whichprovides strong model baselines. We noticed that Hand-objects occlusions andself-occlusions lead to performance degradation, thus proposed a non-modelmethod to merge multi-view results in the post-process stage. Moreover, Weutilized test time augmentation and model ensemble to make further improvement.We also found that public dataset and rational preprocess are beneficial. Ourmethod achieved 12.21mm MPJPE on test dataset, achieve the first place inEgocentric 3D Hand Pose Estimation challenge.</description><author>Zhishan Zhou, Zhi Lv, Shihao Zhou, Minqiang Zou, Tong Wu, Mochen Yu, Yao Tang, Jiajun Liang</author><pubDate>Sat, 07 Oct 2023 11:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04769v1</guid></item><item><title>GloPro: Globally-Consistent Uncertainty-Aware 3D Human Pose Estimation &amp; Tracking in the Wild</title><link>http://arxiv.org/abs/2309.10369v2</link><description>An accurate and uncertainty-aware 3D human body pose estimation is key toenabling truly safe but efficient human-robot interactions. Currentuncertainty-aware methods in 3D human pose estimation are limited to predictingthe uncertainty of the body posture, while effectively neglecting the bodyshape and root pose. In this work, we present GloPro, which to the best of ourknowledge the first framework to predict an uncertainty distribution of a 3Dbody mesh including its shape, pose, and root pose, by efficiently fusingvisual clues with a learned motion model. We demonstrate that it vastlyoutperforms state-of-the-art methods in terms of human trajectory accuracy in aworld coordinate system (even in the presence of severe occlusions), yieldsconsistent uncertainty distributions, and can run in real-time.</description><author>Simon Schaefer, Dorian F. Henning, Stefan Leutenegger</author><pubDate>Wed, 20 Sep 2023 17:22:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10369v2</guid></item><item><title>GloPro: Globally-Consistent Uncertainty-Aware 3D Human Pose Estimation &amp; Tracking in the Wild</title><link>http://arxiv.org/abs/2309.10369v1</link><description>An accurate and uncertainty-aware 3D human body pose estimation is key toenabling truly safe but efficient human-robot interactions. Currentuncertainty-aware methods in 3D human pose estimation are limited to predictingthe uncertainty of the body posture, while effectively neglecting the bodyshape and root pose. In this work, we present GloPro, which to the best of ourknowledge the first framework to predict an uncertainty distribution of a 3Dbody mesh including its shape, pose, and root pose, by efficiently fusingvisual clues with a learned motion model. We demonstrate that it vastlyoutperforms state-of-the-art methods in terms of human trajectory accuracy in aworld coordinate system (even in the presence of severe occlusions), yieldsconsistent uncertainty distributions, and can run in real-time. Our code willbe released upon acceptance at https://github.com/smartroboticslab/GloPro.</description><author>Simon Schaefer, Dorian F. Henning, Stefan Leutenegger</author><pubDate>Tue, 19 Sep 2023 08:10:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10369v1</guid></item><item><title>Double-chain Constraints for 3D Human Pose Estimation in Images and Videos</title><link>http://arxiv.org/abs/2308.05298v1</link><description>Reconstructing 3D poses from 2D poses lacking depth information isparticularly challenging due to the complexity and diversity of human motion.The key is to effectively model the spatial constraints between joints toleverage their inherent dependencies. Thus, we propose a novel model, calledDouble-chain Graph Convolutional Transformer (DC-GCT), to constrain the posethrough a double-chain design consisting of local-to-global and global-to-localchains to obtain a complex representation more suitable for the current humanpose. Specifically, we combine the advantages of GCN and Transformer and designa Local Constraint Module (LCM) based on GCN and a Global Constraint Module(GCM) based on self-attention mechanism as well as a Feature Interaction Module(FIM). The proposed method fully captures the multi-level dependencies betweenhuman body joints to optimize the modeling capability of the model. Moreover,we propose a method to use temporal information into the single-frame model byguiding the video sequence embedding through the joint embedding of the targetframe, with negligible increase in computational cost. Experimental resultsdemonstrate that DC-GCT achieves state-of-the-art performance on twochallenging datasets (Human3.6M and MPI-INF-3DHP). Notably, our model achievesstate-of-the-art performance on all action categories in the Human3.6M datasetusing detected 2D poses from CPN, and our code is available at:https://github.com/KHB1698/DC-GCT.</description><author>Hongbo Kang, Yong Wang, Mengyuan Liu, Doudou Wu, Peng Liu, Wenming Yang</author><pubDate>Thu, 10 Aug 2023 03:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05298v1</guid></item><item><title>DSFFNet: Dual-Side Feature Fusion Network for 3D Pose Transfer</title><link>http://arxiv.org/abs/2305.14951v2</link><description>To solve the problem of pose distortion in the forward propagation of posefeatures in existing methods, this pa-per proposes a Dual-Side Feature FusionNetwork for pose transfer (DSFFNet). Firstly, a fixed-length pose code isextracted from the source mesh by a pose encoder and combined with the targetvertices to form a mixed feature; Then, a Feature Fusion Adaptive InstanceNormalization module (FFAdaIN) is designed, which can process both pose andidentity features simultaneously, so that the pose features can be compensatedin layer-by-layer for-ward propagation, thus solving the pose distortionproblem; Finally, using the mesh decoder composed of this module, the pose aregradually transferred to the target mesh. Experimental results on SMPL, SMAL,FAUST and MultiGarment datasets show that DSFFNet successfully solves the posedistortion problem while maintaining a smaller network structure with strongerpose transfer capability and faster convergence speed, and can adapt to mesheswith different numbers of vertices. Code is available athttps://github.com/YikiDragon/DSFFNet</description><author>Jue Liu</author><pubDate>Mon, 09 Oct 2023 09:57:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14951v2</guid></item><item><title>HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation</title><link>http://arxiv.org/abs/2305.06968v1</link><description>Monocular 3D human pose and shape estimation is an ill-posed problem sincemultiple 3D solutions can explain a 2D image of a subject. Recent approachespredict a probability distribution over plausible 3D pose and shape parametersconditioned on the image. We show that these approaches exhibit a trade-offbetween three key properties: (i) accuracy - the likelihood of the ground-truth3D solution under the predicted distribution, (ii) sample-input consistency -the extent to which 3D samples from the predicted distribution match thevisible 2D image evidence, and (iii) sample diversity - the range of plausible3D solutions modelled by the predicted distribution. Our method, HuManiFlow,predicts simultaneously accurate, consistent and diverse distributions. We usethe human kinematic tree to factorise full body pose into ancestor-conditionedper-body-part pose distributions in an autoregressive manner. Per-body-partdistributions are implemented using normalising flows that respect the manifoldstructure of SO(3), the Lie group of per-body-part poses. We show thatill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,and employ only probabilistic training losses. Code is available at:https://github.com/akashsengupta1997/HuManiFlow.</description><author>Akash Sengupta, Ignas Budvytis, Roberto Cipolla</author><pubDate>Thu, 11 May 2023 17:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06968v1</guid></item><item><title>NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation</title><link>http://arxiv.org/abs/2305.08590v1</link><description>With the progress of 3D human pose and shape estimation, state-of-the-artmethods can either be robust to occlusions or obtain pixel-aligned accuracy innon-occlusion cases. However, they cannot obtain robustness and mesh-imagealignment at the same time. In this work, we present NIKI (Neural InverseKinematics with Invertible Neural Network), which models bi-directional errorsto improve the robustness to occlusions and obtain pixel-aligned accuracy. NIKIcan learn from both the forward and inverse processes with invertible networks.In the inverse process, the model separates the error from the plausible 3Dpose manifold for a robust 3D human pose estimation. In the forward process, weenforce the zero-error boundary conditions to improve the sensitivity toreliable joint positions for better mesh-image alignment. Furthermore, NIKIemulates the analytical inverse kinematics algorithms with the twist-and-swingdecomposition for better interpretability. Experiments on standard andocclusion-specific benchmarks demonstrate the effectiveness of NIKI, where weexhibit robust and well-aligned results simultaneously. Code is available athttps://github.com/Jeff-sjtu/NIKI</description><author>Jiefeng Li, Siyuan Bian, Qi Liu, Jiasheng Tang, Fan Wang, Cewu Lu</author><pubDate>Mon, 15 May 2023 13:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08590v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human</title><link>http://arxiv.org/abs/2307.05853v1</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively).</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Wed, 12 Jul 2023 01:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v1</guid></item><item><title>Spatio-temporal MLP-graph network for 3D human pose estimation</title><link>http://arxiv.org/abs/2308.15313v1</link><description>Graph convolutional networks and their variants have shown significantpromise in 3D human pose estimation. Despite their success, most of thesemethods only consider spatial correlations between body joints and do not takeinto account temporal correlations, thereby limiting their ability to capturerelationships in the presence of occlusions and inherent ambiguity. To addressthis potential weakness, we propose a spatio-temporal network architecturecomposed of a joint-mixing multi-layer perceptron block that facilitatescommunication among different joints and a graph weighted Jacobi network blockthat enables communication among various feature channels. The major novelty ofour approach lies in a new weighted Jacobi feature propagation rule obtainedthrough graph filtering with implicit fairing. We leverage temporal informationfrom the 2D pose sequences, and integrate weight modulation into the model toenable untangling of the feature transformations of distinct nodes. We alsoemploy adjacency modulation with the aim of learning meaningful correlationsbeyond defined linkages between body joints by altering the graph topologythrough a learnable modulation matrix. Extensive experiments on two benchmarkdatasets demonstrate the effectiveness of our model, outperforming recentstate-of-the-art methods for 3D human pose estimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 29 Aug 2023 15:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.15313v1</guid></item><item><title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer</title><link>http://arxiv.org/abs/2303.09681v4</link><description>Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available athttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be releasedupon paper acceptance.</description><author>Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng</author><pubDate>Wed, 06 Sep 2023 22:34:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09681v4</guid></item><item><title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer</title><link>http://arxiv.org/abs/2303.09681v3</link><description>Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available athttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be releasedupon paper acceptance.</description><author>Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng</author><pubDate>Thu, 11 May 2023 00:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09681v3</guid></item><item><title>H3WB: Human3.6M 3D WholeBody Dataset and Benchmark</title><link>http://arxiv.org/abs/2211.15692v2</link><description>We present a benchmark for 3D human whole-body pose estimation, whichinvolves identifying accurate 3D keypoints on the entire human body, includingface, hands, body, and feet. Currently, the lack of a fully annotated andaccurate 3D whole-body dataset results in deep networks being trainedseparately on specific body parts, which are combined during inference. Or theyrely on pseudo-groundtruth provided by parametric body models which are not asaccurate as detection based methods. To overcome these issues, we introduce theHuman3.6M 3D WholeBody (H3WB) dataset, which provides whole-body annotationsfor the Human3.6M dataset using the COCO Wholebody layout. H3WB comprises 133whole-body keypoint annotations on 100K images, made possible by our newmulti-view pipeline. We also propose three tasks: i) 3D whole-body pose liftingfrom 2D complete whole-body pose, ii) 3D whole-body pose lifting from 2Dincomplete whole-body pose, and iii) 3D whole-body pose estimation from asingle RGB image. Additionally, we report several baselines from popularmethods for these tasks. Furthermore, we also provide automated 3D whole-bodyannotations of TotalCapture and experimentally show that when used with H3WB ithelps to improve the performance. Code and dataset is available athttps://github.com/wholebody3d/wholebody3d</description><author>Yue Zhu, Nermin Samet, David Picard</author><pubDate>Wed, 06 Sep 2023 13:22:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.15692v2</guid></item><item><title>LInKs "Lifting Independent Keypoints" -- Partial Pose Lifting for Occlusion Handling with Improved Accuracy in 2D-3D Human Pose Estimation</title><link>http://arxiv.org/abs/2309.07243v1</link><description>We present LInKs, a novel unsupervised learning method to recover 3D humanposes from 2D kinematic skeletons obtained from a single image, even whenocclusions are present. Our approach follows a unique two-step process, whichinvolves first lifting the occluded 2D pose to the 3D domain, followed byfilling in the occluded parts using the partially reconstructed 3D coordinates.This lift-then-fill approach leads to significantly more accurate resultscompared to models that complete the pose in 2D space alone. Additionally, weimprove the stability and likelihood estimation of normalising flows through acustom sampling function replacing PCA dimensionality reduction previously usedin prior work. Furthermore, we are the first to investigate if different partsof the 2D kinematic skeleton can be lifted independently which we find byitself reduces the error of current lifting approaches. We attribute this tothe reduction of long-range keypoint correlations. In our detailed evaluation,we quantify the error under various realistic occlusion scenarios, showcasingthe versatility and applicability of our model. Our results consistentlydemonstrate the superiority of handling all types of occlusions in 3D spacewhen compared to others that complete the pose in 2D space. Our approach alsoexhibits consistent accuracy in scenarios without occlusion, as evidenced by a7.9% reduction in reconstruction error compared to prior works on the Human3.6Mdataset. Furthermore, our method excels in accurately retrieving complete 3Dposes even in the presence of occlusions, making it highly applicable insituations where complete 2D pose information is unavailable.</description><author>Peter Hardy, Hansung Kim</author><pubDate>Wed, 13 Sep 2023 19:28:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07243v1</guid></item><item><title>Regular Splitting Graph Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2305.05785v1</link><description>In human pose estimation methods based on graph convolutional architectures,the human skeleton is usually modeled as an undirected graph whose nodes arebody joints and edges are connections between neighboring joints. However, mostof these methods tend to focus on learning relationships between body joints ofthe skeleton using first-order neighbors, ignoring higher-order neighbors andhence limiting their ability to exploit relationships between distant joints.In this paper, we introduce a higher-order regular splitting graph network(RS-Net) for 2D-to-3D human pose estimation using matrix splitting inconjunction with weight and adjacency modulation. The core idea is to capturelong-range dependencies between body joints using multi-hop neighborhoods andalso to learn different modulation vectors for different body joints as well asa modulation matrix added to the adjacency matrix associated to the skeleton.This learnable modulation matrix helps adjust the graph structure by addingextra graph edges in an effort to learn additional connections between bodyjoints. Instead of using a shared weight matrix for all neighboring bodyjoints, the proposed RS-Net model applies weight unsharing before aggregatingthe feature vectors associated to the joints in order to capture the differentrelations between them. Experiments and ablations studies performed on twobenchmark datasets demonstrate the effectiveness of our model, achievingsuperior performance over recent state-of-the-art methods for 3D human poseestimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 09 May 2023 23:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05785v1</guid></item><item><title>HDFormer: High-order Directed Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.01825v2</link><description>Human pose estimation is a challenging task due to its structured datasequence nature. Existing methods primarily focus on pair-wise interaction ofbody joints, which is insufficient for scenarios involving overlapping jointsand rapidly changing poses. To overcome these issues, we introduce a novelapproach, the High-order Directed Transformer (HDFormer), which leverageshigh-order bone and joint relationships for improved pose estimation.Specifically, HDFormer incorporates both self-attention and high-orderattention to formulate a multi-order attention module. This module facilitatesfirst-order "joint$\leftrightarrow$joint", second-order"bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint"interactions, effectively addressing issues in complex and occlusion-heavysituations. In addition, modern CNN techniques are integrated into thetransformer-based architecture, balancing the trade-off between performance andefficiency. HDFormer significantly outperforms state-of-the-art (SOTA) modelson Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parametersand significantly lower computational costs. Moreover, HDFormer demonstratesbroad real-world applicability, enabling real-time, accurate 3D poseestimation. The source code is in https://github.com/hyer/HDFormer</description><author>Hanyuan Chen, Jun-Yan He, Wangmeng Xiang, Zhi-Qi Cheng, Wei Liu, Hanbing Liu, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 22 May 2023 07:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01825v2</guid></item><item><title>Just Add $π$! Pose Induced Video Transformers for Understanding Activities of Daily Living</title><link>http://arxiv.org/abs/2311.18840v1</link><description>Video transformers have become the de facto standard for human actionrecognition, yet their exclusive reliance on the RGB modality still limitstheir adoption in certain domains. One such domain is Activities of DailyLiving (ADL), where RGB alone is not sufficient to distinguish between visuallysimilar actions, or actions observed from multiple viewpoints. To facilitatethe adoption of video transformers for ADL, we hypothesize that theaugmentation of RGB with human pose information, known for its sensitivity tofine-grained motion and multiple viewpoints, is essential. Consequently, weintroduce the first Pose Induced Video Transformer: PI-ViT (or $\pi$-ViT), anovel approach that augments the RGB representations learned by videotransformers with 2D and 3D pose information. The key elements of $\pi$-ViT aretwo plug-in modules, 2D Skeleton Induction Module and 3D Skeleton InductionModule, that are responsible for inducing 2D and 3D pose information into theRGB representations. These modules operate by performing pose-aware auxiliarytasks, a design choice that allows $\pi$-ViT to discard the modules duringinference. Notably, $\pi$-ViT achieves the state-of-the-art performance onthree prominent ADL datasets, encompassing both real-world and large-scaleRGB-D datasets, without requiring poses or additional computational overhead atinference.</description><author>Dominick Reilly, Srijan Das</author><pubDate>Thu, 30 Nov 2023 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18840v1</guid></item><item><title>3DPortraitGAN: Learning One-Quarter Headshot 3D GANs from a Single-View Portrait Dataset with Diverse Body Poses</title><link>http://arxiv.org/abs/2307.14770v2</link><description>3D-aware face generators are typically trained on 2D real-life face imagedatasets that primarily consist of near-frontal face data, and as such, theyare unable to construct one-quarter headshot 3D portraits with complete head,neck, and shoulder geometry. Two reasons account for this issue: First,existing facial recognition methods struggle with extracting facial datacaptured from large camera angles or back views. Second, it is challenging tolearn a distribution of 3D portraits covering the one-quarter headshot regionfrom single-view data due to significant geometric deformation caused bydiverse body poses. To this end, we first create the dataset360{\deg}-Portrait-HQ (360{\deg}PHQ for short) which consists of high-qualitysingle-view real portraits annotated with a variety of camera parameters (theyaw angles span the entire 360{\deg} range) and body poses. We then propose3DPortraitGAN, the first 3D-aware one-quarter headshot portrait generator thatlearns a canonical 3D avatar distribution from the 360{\deg}PHQ dataset withbody pose self-learning. Our model can generate view-consistent portrait imagesfrom all camera angles with a canonical one-quarter headshot 3D representation.Our experiments show that the proposed framework can accurately predictportrait body poses and generate view-consistent, realistic portrait imageswith complete geometry from all camera angles.</description><author>Yiqian Wu, Hao Xu, Xiangjun Tang, Hongbo Fu, Xiaogang Jin</author><pubDate>Mon, 21 Aug 2023 07:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14770v2</guid></item><item><title>Robot at the Mirror: Learning to Imitate via Associating Self-supervised Models</title><link>http://arxiv.org/abs/2311.13226v1</link><description>We introduce an approach to building a custom model from ready-madeself-supervised models via their associating instead of training andfine-tuning. We demonstrate it with an example of a humanoid robot looking atthe mirror and learning to detect the 3D pose of its own body from the image itperceives. To build our model, we first obtain features from the visual inputand the postures of the robot's body via models prepared before the robot'soperation. Then, we map their corresponding latent spaces by a sample-efficientrobot's self-exploration at the mirror. In this way, the robot builds thesolicited 3D pose detector, which quality is immediately perfect on theacquired samples instead of obtaining the quality gradually. The mapping, whichemploys associating the pairs of feature vectors, is then implemented in thesame way as the key-value mechanism of the famous transformer models. Finally,deploying our model for imitation to a simulated robot allows us to study, tuneup, and systematically evaluate its hyperparameters without the involvement ofthe human counterpart, advancing our previous research.</description><author>Andrej Lúčny, Kristína Malinovská, Igor Farkaš</author><pubDate>Wed, 22 Nov 2023 08:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13226v1</guid></item><item><title>Exploring Latent Cross-Channel Embedding for Accurate 3D Human Pose Reconstruction in a Diffusion Framework</title><link>http://arxiv.org/abs/2401.09836v1</link><description>Monocular 3D human pose estimation poses significant challenges due to theinherent depth ambiguities that arise during the reprojection process from 2Dto 3D. Conventional approaches that rely on estimating an over-fit projectionmatrix struggle to effectively address these challenges and often result innoisy outputs. Recent advancements in diffusion models have shown promise inincorporating structural priors to address reprojection ambiguities. However,there is still ample room for improvement as these methods often overlook theexploration of correlation between the 2D and 3D joint-level features. In thisstudy, we propose a novel cross-channel embedding framework that aims to fullyexplore the correlation between joint-level features of 3D coordinates andtheir 2D projections. In addition, we introduce a context guidance mechanism tofacilitate the propagation of joint graph attention across latent channelsduring the iterative diffusion process. To evaluate the effectiveness of ourproposed method, we conduct experiments on two benchmark datasets, namelyHuman3.6M and MPI-INF-3DHP. Our results demonstrate a significant improvementin terms of reconstruction accuracy compared to state-of-the-art methods. Thecode for our method will be made available online for further reference.</description><author>Junkun Jiang, Jie Chen</author><pubDate>Thu, 18 Jan 2024 09:53:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09836v1</guid></item><item><title>Interweaved Graph and Attention Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2304.14045v1</link><description>Despite substantial progress in 3D human pose estimation from a single-viewimage, prior works rarely explore global and local correlations, leading toinsufficient learning of human skeleton representations. To address this issue,we propose a novel Interweaved Graph and Attention Network (IGANet) that allowsbidirectional communications between graph convolutional networks (GCNs) andattentions. Specifically, we introduce an IGA module, where attentions areprovided with local information from GCNs and GCNs are injected with globalinformation from attentions. Additionally, we design a simple yet effectiveU-shaped multi-layer perceptron (uMLP), which can capture multi-granularityinformation for body joints. Extensive experiments on two popular benchmarkdatasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate ourproposed method.The results show that IGANet achieves state-of-the-artperformance on both datasets. Code is available athttps://github.com/xiu-cs/IGANet.</description><author>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li</author><pubDate>Thu, 27 Apr 2023 10:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14045v1</guid></item><item><title>Learning to Predict Scene-Level Implicit 3D from Posed RGBD Data</title><link>http://arxiv.org/abs/2306.08671v1</link><description>We introduce a method that can learn to predict scene-level implicitfunctions for 3D reconstruction from posed RGBD data. At test time, our systemmaps a previously unseen RGB image to a 3D reconstruction of a scene viaimplicit functions. While implicit functions for 3D reconstruction have oftenbeen tied to meshes, we show that we can train one using only a set of posedRGBD images. This setting may help 3D reconstruction unlock the sea ofaccelerometer+RGBD data that is coming with new phones. Our system, D2-DRDF,can match and sometimes outperform current methods that use mesh supervisionand shows better robustness to sparse data.</description><author>Nilesh Kulkarni, Linyi Jin, Justin Johnson, David F. Fouhey</author><pubDate>Wed, 14 Jun 2023 18:59:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08671v1</guid></item><item><title>EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild</title><link>http://arxiv.org/abs/2308.16894v1</link><description>We present EMDB, the Electromagnetic Database of Global 3D Human Pose andShape in the Wild. EMDB is a novel dataset that contains high-quality 3D SMPLpose and shape parameters with global body and camera trajectories forin-the-wild videos. We use body-worn, wireless electromagnetic (EM) sensors anda hand-held iPhone to record a total of 58 minutes of motion data, distributedover 81 indoor and outdoor sequences and 10 participants. Together withaccurate body poses and shapes, we also provide global camera poses and bodyroot trajectories. To construct EMDB, we propose a multi-stage optimizationprocedure, which first fits SMPL to the 6-DoF EM measurements and then refinesthe poses via image observations. To achieve high-quality results, we leveragea neural implicit avatar model to reconstruct detailed human surface geometryand appearance, which allows for improved alignment and smoothness via a densepixel-level objective. Our evaluations, conducted with a multi-view volumetriccapture system, indicate that EMDB has an expected accuracy of 2.3 cmpositional and 10.6 degrees angular error, surpassing the accuracy of previousin-the-wild datasets. We evaluate existing state-of-the-art monocular RGBmethods for camera-relative and global pose estimation on EMDB. EMDB ispublicly available under https://ait.ethz.ch/emdb</description><author>Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan Zarate, Otmar Hilliges</author><pubDate>Thu, 31 Aug 2023 18:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16894v1</guid></item><item><title>Occlusion Robust 3D Human Pose Estimation with StridedPoseGraphFormer and Data Augmentation</title><link>http://arxiv.org/abs/2304.12069v1</link><description>Occlusion is an omnipresent challenge in 3D human pose estimation (HPE). Inspite of the large amount of research dedicated to 3D HPE, only a limitednumber of studies address the problem of occlusion explicitly. To fill thisgap, we propose to combine exploitation of spatio-temporal features withsynthetic occlusion augmentation during training to deal with occlusion. Tothis end, we build a spatio-temporal 3D HPE model, StridedPoseGraphFormer basedon graph convolution and transformers, and train it using occlusionaugmentation. Unlike the existing occlusion-aware methods, that are only testedfor limited occlusion, we extensively evaluate our method for varying degreesof occlusion. We show that our proposed method compares favorably with thestate-of-the-art (SoA). Our experimental results also reveal that in theabsence of any occlusion handling mechanism, the performance of SoA 3D HPEmethods degrades significantly when they encounter occlusion.</description><author>Soubarna Banik, Patricia Gschoßmann, Alejandro Mendoza Garcia, Alois Knoll</author><pubDate>Mon, 24 Apr 2023 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12069v1</guid></item><item><title>EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision</title><link>http://arxiv.org/abs/2303.02862v3</link><description>Event camera shows great potential in 3D hand pose estimation, especiallyaddressing the challenges of fast motion and high dynamic range in a low-powerway. However, due to the asynchronous differential imaging mechanism, it ischallenging to design event representation to encode hand motion informationespecially when the hands are not moving (causing motion ambiguity), and it isinfeasible to fully annotate the temporally dense event stream. In this paper,we propose EvHandPose with novel hand flow representations in Event-to-Posemodule for accurate hand pose estimation and alleviating the motion ambiguityissue. To solve the problem under sparse annotation, we design contrastmaximization and hand-edge constraints in Pose-to-IWE (Image with WarpedEvents) module and formulate EvHandPose in a weakly-supervision framework. Wefurther build EvRealHands, the first large-scale real-world event-based handpose dataset on several challenging scenes to bridge the real-synthetic domaingap. Experiments on EvRealHands demonstrate that EvHandPose outperformsprevious event-based methods under all evaluation scenes, achieves accurate andstable hand pose estimation with high temporal resolution in fast motion andstrong light scenes compared with RGB-based methods, generalizes well tooutdoor scenes and another type of event camera, and shows the potential forthe hand gesture recognition task.</description><author>Jianping Jiang, Jiahe Li, Baowen Zhang, Xiaoming Deng, Boxin Shi</author><pubDate>Thu, 28 Dec 2023 08:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02862v3</guid></item><item><title>EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision</title><link>http://arxiv.org/abs/2303.02862v2</link><description>Event camera shows great potential in 3D hand pose estimation, especiallyaddressing the challenges of fast motion and high dynamic range in a low-powerway. However, due to the asynchronous differential imaging mechanism, it ischallenging to design event representation to encode hand motion informationespecially when the hands are not moving (causing motion ambiguity), and it isinfeasible to fully annotate the temporally dense event stream. In this paper,we propose EvHandPose with novel hand flow representations in Event-to-Posemodule for accurate hand pose estimation and alleviating the motion ambiguityissue. To solve the problem under sparse annotation, we design contrastmaximization and hand-edge constraints in Pose-to-IWE (Image with WarpedEvents) module and formulate EvHandPose in a weakly-supervision framework. Wefurther build EvRealHands, the first large-scale real-world event-based handpose dataset on several challenging scenes to bridge the real-synthetic domaingap. Experiments on EvRealHands demonstrate that EvHandPose outperformsprevious event-based methods under all evaluation scenes, achieves accurate andstable hand pose estimation with high temporal resolution in fast motion andstrong light scenes compared with RGB-based methods, generalizes well tooutdoor scenes and another type of event camera, and shows the potential forthe hand gesture recognition task.</description><author>Jianping Jiang, Jiahe Li, Baowen Zhang, Xiaoming Deng, Boxin Shi</author><pubDate>Wed, 30 Aug 2023 04:21:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02862v2</guid></item></channel></rss>