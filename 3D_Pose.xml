<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 27 May 2023 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Neural Voting Field for Camera-Space 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2305.04328v1</link><description>We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.</description><author>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</author><pubDate>Sun, 07 May 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04328v1</guid></item><item><title>Dual-Side Feature Fusion 3D Pose Transfer</title><link>http://arxiv.org/abs/2305.14951v1</link><description>3D pose transfer solves the problem of additional input and correspondence oftraditional deformation transfer, only the source and target meshes need to beinput, and the pose of the source mesh can be transferred to the target mesh.Some lightweight methods proposed in recent years consume less memory but causespikes and distortions for some unseen poses, while others are costly intraining due to the inclusion of large matrix multiplication and adversarialnetworks. In addition, the meshes with different numbers of vertices alsoincrease the difficulty of pose transfer. In this work, we propose a Dual-SideFeature Fusion Pose Transfer Network to improve the pose transfer accuracy ofthe lightweight method. Our method takes the pose features as one of the sideinputs to the decoding network and fuses them into the target mesh layer bylayer at multiple scales. Our proposed Feature Fusion Adaptive InstanceNormalization has the characteristic of having two side input channels thatfuse pose features and identity features as denormalization parameters, thusenhancing the pose transfer capability of the network. Extensive experimentalresults show that our proposed method has stronger pose transfer capabilitythan state-of-the-art methods while maintaining a lightweight networkstructure, and can converge faster.</description><author>Jue Liu, Feipeng Da</author><pubDate>Wed, 24 May 2023 10:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14951v1</guid></item><item><title>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</title><link>http://arxiv.org/abs/2206.02257v3</link><description>In this survey, we present a systematic review of 3D hand pose estimationfrom the perspective of efficient annotation and learning. 3D hand poseestimation has been an important research area owing to its potential to enablevarious applications, such as video understanding, AR/VR, and robotics.However, the performance of models is tied to the quality and quantity ofannotated 3D hand poses. Under the status quo, acquiring such annotated 3D handposes is challenging, e.g., due to the difficulty of 3D annotation and thepresence of occlusion. To reveal this problem, we review the pros and cons ofexisting annotation methods classified as manual, synthetic-model-based,hand-sensor-based, and computational approaches. Additionally, we examinemethods for learning 3D hand poses when annotated data are scarce, includingself-supervised pretraining, semi-supervised learning, and domain adaptation.Based on the study of efficient annotation and learning, we further discusslimitations and possible future directions in this field.</description><author>Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato</author><pubDate>Wed, 26 Apr 2023 07:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02257v3</guid></item><item><title>Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields</title><link>http://arxiv.org/abs/2212.02493v3</link><description>Coordinate-based implicit neural networks, or neural fields, have emerged asuseful representations of shape and appearance in 3D computer vision. Despiteadvances, however, it remains challenging to build neural fields for categoriesof objects without datasets like ShapeNet that provide "canonicalized" objectinstances that are consistently aligned for their 3D position and orientation(pose). We present Canonical Field Network (CaFi-Net), a self-supervised methodto canonicalize the 3D pose of instances from an object category represented asneural fields, specifically neural radiance fields (NeRFs). CaFi-Net directlylearns from continuous and noisy radiance fields using a Siamese networkarchitecture that is designed to extract equivariant field features forcategory-level canonicalization. During inference, our method takes pre-trainedneural radiance fields of novel object instances at arbitrary 3D pose andestimates a canonical field with consistent 3D pose across the entire category.Extensive experiments on a new dataset of 1300 NeRF models across 13 objectcategories show that our method matches or exceeds the performance of 3D pointcloud-based methods.</description><author>Rohith Agaram, Shaurya Dewan, Rahul Sajnani, Adrien Poulenard, Madhava Krishna, Srinath Sridhar</author><pubDate>Wed, 17 May 2023 12:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02493v3</guid></item><item><title>Robust Category-Level 3D Pose Estimation from Synthetic Data</title><link>http://arxiv.org/abs/2305.16124v1</link><description>Obtaining accurate 3D object poses is vital for numerous computer visionapplications, such as 3D reconstruction and scene understanding. However,annotating real-world objects is time-consuming and challenging. Whilesynthetically generated training data is a viable alternative, the domain shiftbetween real and synthetic data is a significant challenge. In this work, weaim to narrow the performance gap between models trained on synthetic data andfew real images and fully supervised models trained on large-scale data. Weachieve this by approaching the problem from two perspectives: 1) We introduceSyntheticP3D, a new synthetic dataset for object pose estimation generated fromCAD models and enhanced with a novel algorithm. 2) We propose a novel approach(CC3D) for training neural mesh models that perform pose estimation via inverserendering. In particular, we exploit the spatial relationships between featureson the mesh surface and a contrastive learning scheme to guide the domainadaptation process. Combined, these two approaches enable our models to performcompetitively with state-of-the-art models using only 10% of the respectivereal training images, while outperforming the SOTA model by 10.4% with athreshold of pi/18 using only 50% of the real training data. Our trained modelfurther demonstrates robust generalization to out-of-distribution scenariosdespite being trained with minimal real data.</description><author>Jiahao Yang, Wufei Ma, Angtian Wang, Xiaoding Yuan, Alan Yuille, Adam Kortylewski</author><pubDate>Thu, 25 May 2023 15:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16124v1</guid></item><item><title>HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation</title><link>http://arxiv.org/abs/2305.06968v1</link><description>Monocular 3D human pose and shape estimation is an ill-posed problem sincemultiple 3D solutions can explain a 2D image of a subject. Recent approachespredict a probability distribution over plausible 3D pose and shape parametersconditioned on the image. We show that these approaches exhibit a trade-offbetween three key properties: (i) accuracy - the likelihood of the ground-truth3D solution under the predicted distribution, (ii) sample-input consistency -the extent to which 3D samples from the predicted distribution match thevisible 2D image evidence, and (iii) sample diversity - the range of plausible3D solutions modelled by the predicted distribution. Our method, HuManiFlow,predicts simultaneously accurate, consistent and diverse distributions. We usethe human kinematic tree to factorise full body pose into ancestor-conditionedper-body-part pose distributions in an autoregressive manner. Per-body-partdistributions are implemented using normalising flows that respect the manifoldstructure of SO(3), the Lie group of per-body-part poses. We show thatill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,and employ only probabilistic training losses. Code is available at:https://github.com/akashsengupta1997/HuManiFlow.</description><author>Akash Sengupta, Ignas Budvytis, Roberto Cipolla</author><pubDate>Thu, 11 May 2023 17:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06968v1</guid></item><item><title>NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation</title><link>http://arxiv.org/abs/2305.08590v1</link><description>With the progress of 3D human pose and shape estimation, state-of-the-artmethods can either be robust to occlusions or obtain pixel-aligned accuracy innon-occlusion cases. However, they cannot obtain robustness and mesh-imagealignment at the same time. In this work, we present NIKI (Neural InverseKinematics with Invertible Neural Network), which models bi-directional errorsto improve the robustness to occlusions and obtain pixel-aligned accuracy. NIKIcan learn from both the forward and inverse processes with invertible networks.In the inverse process, the model separates the error from the plausible 3Dpose manifold for a robust 3D human pose estimation. In the forward process, weenforce the zero-error boundary conditions to improve the sensitivity toreliable joint positions for better mesh-image alignment. Furthermore, NIKIemulates the analytical inverse kinematics algorithms with the twist-and-swingdecomposition for better interpretability. Experiments on standard andocclusion-specific benchmarks demonstrate the effectiveness of NIKI, where weexhibit robust and well-aligned results simultaneously. Code is available athttps://github.com/Jeff-sjtu/NIKI</description><author>Jiefeng Li, Siyuan Bian, Qi Liu, Jiasheng Tang, Fan Wang, Cewu Lu</author><pubDate>Mon, 15 May 2023 13:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08590v1</guid></item><item><title>Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer</title><link>http://arxiv.org/abs/2303.09681v3</link><description>Event camera, as an emerging biologically-inspired vision sensor forcapturing motion dynamics, presents new potential for 3D human pose tracking,or video-based 3D human pose estimation. However, existing works in posetracking either require the presence of additional gray-scale images toestablish a solid starting pose, or ignore the temporal dependencies alltogether by collapsing segments of event streams to form static event frames.Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs,a.k.a. dense deep learning) has been showcased in many event-based tasks, theuse of ANNs tends to neglect the fact that compared to the dense frame-basedimage sequences, the occurrence of events from an event camera isspatiotemporally much sparser. Motivated by the above mentioned issues, wepresent in this paper a dedicated end-to-end sparse deep learning approach forevent-based pose tracking: 1) to our knowledge this is the first time that 3Dhuman pose tracking is obtained from events only, thus eliminating the need ofaccessing to any frame-based images as part of input; 2) our approach is basedentirely upon the framework of Spiking Neural Networks (SNNs), which consistsof Spike-Element-Wise (SEW) ResNet and a novel Spiking SpatiotemporalTransformer; 3) a large-scale synthetic dataset is constructed that features abroad and diverse set of annotated 3D human motions, as well as longer hours ofevent stream data, named SynEventHPD. Empirical experiments demonstrate that,with superior performance over the state-of-the-art (SOTA) ANNs counterparts,our approach also achieves a significant computation reduction of 80% in FLOPS.Furthermore, our proposed method also outperforms SOTA SNNs in the regressiontask of human pose tracking. Our implementation is available athttps://github.com/JimmyZou/HumanPoseTracking_SNN and dataset will be releasedupon paper acceptance.</description><author>Shihao Zou, Yuxuan Mu, Xinxin Zuo, Sen Wang, Li Cheng</author><pubDate>Thu, 11 May 2023 00:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.09681v3</guid></item><item><title>Regular Splitting Graph Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2305.05785v1</link><description>In human pose estimation methods based on graph convolutional architectures,the human skeleton is usually modeled as an undirected graph whose nodes arebody joints and edges are connections between neighboring joints. However, mostof these methods tend to focus on learning relationships between body joints ofthe skeleton using first-order neighbors, ignoring higher-order neighbors andhence limiting their ability to exploit relationships between distant joints.In this paper, we introduce a higher-order regular splitting graph network(RS-Net) for 2D-to-3D human pose estimation using matrix splitting inconjunction with weight and adjacency modulation. The core idea is to capturelong-range dependencies between body joints using multi-hop neighborhoods andalso to learn different modulation vectors for different body joints as well asa modulation matrix added to the adjacency matrix associated to the skeleton.This learnable modulation matrix helps adjust the graph structure by addingextra graph edges in an effort to learn additional connections between bodyjoints. Instead of using a shared weight matrix for all neighboring bodyjoints, the proposed RS-Net model applies weight unsharing before aggregatingthe feature vectors associated to the joints in order to capture the differentrelations between them. Experiments and ablations studies performed on twobenchmark datasets demonstrate the effectiveness of our model, achievingsuperior performance over recent state-of-the-art methods for 3D human poseestimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 09 May 2023 23:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05785v1</guid></item><item><title>HDFormer: High-order Directed Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.01825v2</link><description>Human pose estimation is a challenging task due to its structured datasequence nature. Existing methods primarily focus on pair-wise interaction ofbody joints, which is insufficient for scenarios involving overlapping jointsand rapidly changing poses. To overcome these issues, we introduce a novelapproach, the High-order Directed Transformer (HDFormer), which leverageshigh-order bone and joint relationships for improved pose estimation.Specifically, HDFormer incorporates both self-attention and high-orderattention to formulate a multi-order attention module. This module facilitatesfirst-order "joint$\leftrightarrow$joint", second-order"bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint"interactions, effectively addressing issues in complex and occlusion-heavysituations. In addition, modern CNN techniques are integrated into thetransformer-based architecture, balancing the trade-off between performance andefficiency. HDFormer significantly outperforms state-of-the-art (SOTA) modelson Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parametersand significantly lower computational costs. Moreover, HDFormer demonstratesbroad real-world applicability, enabling real-time, accurate 3D poseestimation. The source code is in https://github.com/hyer/HDFormer</description><author>Hanyuan Chen, Jun-Yan He, Wangmeng Xiang, Zhi-Qi Cheng, Wei Liu, Hanbing Liu, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 22 May 2023 07:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01825v2</guid></item><item><title>Interweaved Graph and Attention Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2304.14045v1</link><description>Despite substantial progress in 3D human pose estimation from a single-viewimage, prior works rarely explore global and local correlations, leading toinsufficient learning of human skeleton representations. To address this issue,we propose a novel Interweaved Graph and Attention Network (IGANet) that allowsbidirectional communications between graph convolutional networks (GCNs) andattentions. Specifically, we introduce an IGA module, where attentions areprovided with local information from GCNs and GCNs are injected with globalinformation from attentions. Additionally, we design a simple yet effectiveU-shaped multi-layer perceptron (uMLP), which can capture multi-granularityinformation for body joints. Extensive experiments on two popular benchmarkdatasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate ourproposed method.The results show that IGANet achieves state-of-the-artperformance on both datasets. Code is available athttps://github.com/xiu-cs/IGANet.</description><author>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li</author><pubDate>Thu, 27 Apr 2023 10:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14045v1</guid></item><item><title>Occlusion Robust 3D Human Pose Estimation with StridedPoseGraphFormer and Data Augmentation</title><link>http://arxiv.org/abs/2304.12069v1</link><description>Occlusion is an omnipresent challenge in 3D human pose estimation (HPE). Inspite of the large amount of research dedicated to 3D HPE, only a limitednumber of studies address the problem of occlusion explicitly. To fill thisgap, we propose to combine exploitation of spatio-temporal features withsynthetic occlusion augmentation during training to deal with occlusion. Tothis end, we build a spatio-temporal 3D HPE model, StridedPoseGraphFormer basedon graph convolution and transformers, and train it using occlusionaugmentation. Unlike the existing occlusion-aware methods, that are only testedfor limited occlusion, we extensively evaluate our method for varying degreesof occlusion. We show that our proposed method compares favorably with thestate-of-the-art (SoA). Our experimental results also reveal that in theabsence of any occlusion handling mechanism, the performance of SoA 3D HPEmethods degrades significantly when they encounter occlusion.</description><author>Soubarna Banik, Patricia Gschoßmann, Alejandro Mendoza Garcia, Alois Knoll</author><pubDate>Mon, 24 Apr 2023 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12069v1</guid></item><item><title>TAPE: Temporal Attention-based Probabilistic human pose and shape Estimation</title><link>http://arxiv.org/abs/2305.00181v1</link><description>Reconstructing 3D human pose and shape from monocular videos is awell-studied but challenging problem. Common challenges include occlusions, theinherent ambiguities in the 2D to 3D mapping and the computational complexityof video processing. Existing methods ignore the ambiguities of thereconstruction and provide a single deterministic estimate for the 3D pose. Inorder to address these issues, we present a Temporal Attention basedProbabilistic human pose and shape Estimation method (TAPE) that operates on anRGB video. More specifically, we propose to use a neural network to encodevideo frames to temporal features using an attention-based neural network.Given these features, we output a per-frame but temporally-informed probabilitydistribution for the human pose using Normalizing Flows. We show that TAPEoutperforms state-of-the-art methods in standard benchmarks and serves as aneffective video-based prior for optimization-based human pose and shapeestimation. Code is available at: https: //github.com/nikosvasilik/TAPE</description><author>Nikolaos Vasilikopoulos, Nikos Kolotouros, Aggeliki Tsoli, Antonis Argyros</author><pubDate>Sat, 29 Apr 2023 07:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00181v1</guid></item><item><title>Optimal and Robust Category-level Perception: Object Pose and Shape Estimation from 2D and 3D Semantic Keypoints</title><link>http://arxiv.org/abs/2206.12498v2</link><description>We consider a category-level perception problem, where one is given 2D or 3Dsensor data picturing an object of a given category (e.g., a car), and has toreconstruct the 3D pose and shape of the object despite intra-class variability(i.e., different car models have different shapes). We consider an active shapemodel, where -for an object category- we are given a library of potential CADmodels describing objects in that category, and we adopt a standard formulationwhere pose and shape are estimated from 2D or 3D keypoints via non-convexoptimization. Our first contribution is to develop PACE3D* and PACE2D*, thefirst certifiably optimal solvers for pose and shape estimation using 3D and 2Dkeypoints, respectively. Both solvers rely on the design of tight (i.e., exact)semidefinite relaxations. Our second contribution is to develop outlier-robustversions of both solvers, named PACE3D# and PACE2D#. Towards this goal, wepropose ROBIN, a general graph-theoretic framework to prune outliers, whichuses compatibility hypergraphs to model measurements' compatibility. We showthat in category-level perception problems these hypergraphs can be built fromthe winding orders of the keypoints (in 2D) or their convex hulls (in 3D), andmany outliers can be filtered out via maximum hyperclique computation. The lastcontribution is an extensive experimental evaluation. Besides providing anablation study on simulated datasets and on the PASCAL3D+ dataset, we combineour solver with a deep keypoint detector, and show that PACE3D# improves overthe state of the art in vehicle pose estimation in the ApolloScape datasets,and its runtime is compatible with practical applications. We release our codeat https://github.com/MIT-SPARK/PACE.</description><author>Jingnan Shi, Heng Yang, Luca Carlone</author><pubDate>Mon, 15 May 2023 04:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12498v2</guid></item><item><title>MVP-Human Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames</title><link>http://arxiv.org/abs/2204.11184v2</link><description>In this paper, we consider a novel problem of reconstructing a 3D humanavatar from multiple unconstrained frames, independent of assumptions on cameracalibration, capture space, and constrained actions. The problem should beaddressed by a framework that takes multiple unconstrained images as inputs,and generates a shape-with-skinning avatar in the canonical space, finished inone feed-forward pass. To this end, we present 3D Avatar Reconstruction in thewild (ARwild), which first reconstructs the implicit skinning fields in amulti-level manner, by which the image features from multiple images arealigned and integrated to estimate a pixel-aligned implicit function thatrepresents the clothed shape. To enable the training and testing of the newframework, we contribute a large-scale dataset, MVP-Human (Multi-View andmulti-Pose 3D Human), which contains 400 subjects, each of which has 15 scansin different poses and 8-view images for each pose, providing 6,000 3D scansand 48,000 images in total. Overall, benefits from the specific networkarchitecture and the diverse data, the trained model enables 3D avatarreconstruction from unconstrained frames and achieves state-of-the-artperformance.</description><author>Xiangyu Zhu, Tingting Liao, Jiangjing Lyu, Xiang Yan, Yunfeng Wang, Kan Guo, Qiong Cao, Stan Z. Li, Zhen Lei</author><pubDate>Wed, 17 May 2023 11:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.11184v2</guid></item><item><title>Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training</title><link>http://arxiv.org/abs/2206.11215v4</link><description>We consider a certifiable object pose estimation problem, where -- given apartial point cloud of an object -- the goal is to not only estimate the objectpose, but also to provide a certificate of correctness for the resultingestimate. Our first contribution is a general theory of certification forend-to-end perception models. In particular, we introduce the notion of$\zeta$-correctness, which bounds the distance between an estimate and theground truth. We show that $\zeta$-correctness can be assessed by implementingtwo certificates: (i) a certificate of observable correctness, that asserts ifthe model output is consistent with the input data and prior information, (ii)a certificate of non-degeneracy, that asserts whether the input data issufficient to compute a unique estimate. Our second contribution is to applythis theory and design a new learning-based certifiable pose estimator. Wepropose C-3PO, a semantic-keypoint-based pose estimation model, augmented withthe two certificates, to solve the certifiable pose estimation problem. C-3POalso includes a keypoint corrector, implemented as a differentiableoptimization layer, that can correct large detection errors (e.g. due to thesim-to-real gap). Our third contribution is a novel self-supervised trainingapproach that uses our certificate of observable correctness to provide thesupervisory signal to C-3PO during training. In it, the model trains only onthe observably correct input-output pairs, in each training iteration. Astraining progresses, we see that the observably correct input-output pairsgrow, eventually reaching near 100% in many cases. Our experiments show that(i) standard semantic-keypoint-based methods outperform more recentalternatives, (ii) C-3PO further improves performance and significantlyoutperforms all the baselines, and (iii) C-3PO's certificates are able todiscern correct pose estimates.</description><author>Rajat Talak, Lisa Peng, Luca Carlone</author><pubDate>Fri, 28 Apr 2023 20:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11215v4</guid></item><item><title>ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with Unsupervised Implicit Pose Embedding</title><link>http://arxiv.org/abs/2304.14005v1</link><description>Although 3D-aware GANs based on neural radiance fields have achievedcompetitive performance, their applicability is still limited to objects orscenes with the ground-truths or prediction models for clearly definedcanonical camera poses. To extend the scope of applicable datasets, we proposea novel 3D-aware GAN optimization technique through contrastive learning withimplicit pose embeddings. To this end, we first revise the discriminator designand remove dependency on ground-truth camera poses. Then, to capture complexand challenging 3D scene structures more effectively, we make the discriminatorestimate a high-dimensional implicit pose embedding from a given image andperform contrastive learning on the pose embedding. The proposed approach canbe employed for the dataset, where the canonical camera pose is ill-definedbecause it does not look up or estimate camera poses. Experimental results showthat our algorithm outperforms existing methods by large margins on thedatasets with multiple object categories and inconsistent canonical cameraposes.</description><author>Mijeoong Kim, Hyunjoon Lee, Bohyung Han</author><pubDate>Thu, 27 Apr 2023 08:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14005v1</guid></item><item><title>BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos</title><link>http://arxiv.org/abs/2212.07401v2</link><description>Quantifying motion in 3D is important for studying the behavior of humans andother animals, but manual pose annotations are expensive and time-consuming toobtain. Self-supervised keypoint discovery is a promising strategy forestimating 3D poses without annotations. However, current keypoint discoveryapproaches commonly process single 2D views and do not operate in the 3D space.We propose a new method to perform self-supervised keypoint discovery in 3Dfrom multi-view videos of behaving agents, without any keypoint or bounding boxsupervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoderarchitecture with a 3D volumetric heatmap, trained to reconstructspatiotemporal differences across multiple views, in addition to joint lengthconstraints on a learned 3D skeleton of the subject. In this way, we discoverkeypoints without requiring manual supervision in videos of humans and rats,demonstrating the potential of 3D keypoint discovery for studying behavior.</description><author>Jennifer J. Sun, Lili Karashchuk, Amil Dravid, Serim Ryou, Sonia Fereidooni, John Tuthill, Aggelos Katsaggelos, Bingni W. Brunton, Georgia Gkioxari, Ann Kennedy, Yisong Yue, Pietro Perona</author><pubDate>Sun, 07 May 2023 00:11:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.07401v2</guid></item><item><title>Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction</title><link>http://arxiv.org/abs/2305.09510v1</link><description>Robotic manipulation systems operating in complex environments rely onperception systems that provide information about the geometry (pose and 3Dshape) of the objects in the scene along with other semantic information suchas object labels. This information is then used for choosing the feasiblegrasps on relevant objects. In this paper, we present a novel method to providethis geometric and semantic information of all objects in the scene as well asfeasible grasps on those objects simultaneously. The main advantage of ourmethod is its speed as it avoids sequential perception and grasp planningsteps. With detailed quantitative analysis, we show that our method deliverscompetitive performance compared to the state-of-the-art dedicated methods forobject shape, pose, and grasp predictions while providing fast inference at 30frames per second speed.</description><author>Shubham Agrawal, Nikhil Chavan-Dafle, Isaac Kasahara, Selim Engin, Jinwook Huh, Volkan Isler</author><pubDate>Tue, 16 May 2023 16:03:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09510v1</guid></item><item><title>Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models</title><link>http://arxiv.org/abs/2211.09707v2</link><description>Diffusion models have experienced a surge of interest as highly expressiveyet efficiently trainable probabilistic models. We show that these models arean excellent fit for synthesising human motion that co-occurs with audio, e.g.,dancing and co-speech gesticulation, since motion is complex and highlyambiguous given audio, calling for a probabilistic description. Specifically,we adapt the DiffWave architecture to model 3D pose sequences, puttingConformers in place of dilated convolutions for improved modelling power. Wealso demonstrate control over motion style, using classifier-free guidance toadjust the strength of the stylistic expression. Experiments on gesture anddance generation confirm that the proposed method achieves top-of-the-linemotion quality, with distinctive styles whose expression can be made more orless pronounced. We also synthesise path-driven locomotion using the same modelarchitecture. Finally, we generalise the guidance procedure to obtainproduct-of-expert ensembles of diffusion models and demonstrate how these maybe used for, e.g., style interpolation, a contribution we believe is ofindependent interest. Seehttps://www.speech.kth.se/research/listen-denoise-action/ for video examples,data, and code.</description><author>Simon Alexanderson, Rajmund Nagy, Jonas Beskow, Gustav Eje Henter</author><pubDate>Tue, 16 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.09707v2</guid></item><item><title>Towards 3D Face Reconstruction in Perspective Projection: Estimating 6DoF Face Pose from Monocular Image</title><link>http://arxiv.org/abs/2205.04126v2</link><description>In 3D face reconstruction, orthogonal projection has been widely employed tosubstitute perspective projection to simplify the fitting process. Thisapproximation performs well when the distance between camera and face is farenough. However, in some scenarios that the face is very close to camera ormoving along the camera axis, the methods suffer from the inaccuratereconstruction and unstable temporal fitting due to the distortion under theperspective projection. In this paper, we aim to address the problem ofsingle-image 3D face reconstruction under perspective projection. Specifically,a deep neural network, Perspective Network (PerspNet), is proposed tosimultaneously reconstruct 3D face shape in canonical space and learn thecorrespondence between 2D pixels and 3D points, by which the 6DoF (6 Degrees ofFreedom) face pose can be estimated to represent perspective projection.Besides, we contribute a large ARKitFace dataset to enable the training andevaluation of 3D face reconstruction solutions under the scenarios ofperspective projection, which has 902,724 2D facial images with ground-truth 3Dface mesh and annotated 6DoF pose parameters. Experimental results show thatour approach outperforms current state-of-the-art methods by a significantmargin. The code and data are available athttps://github.com/cbsropenproject/6dof_face.</description><author>Yueying Kao, Bowen Pan, Miao Xu, Jiangjing Lyu, Xiangyu Zhu, Yuanzhang Chang, Xiaobo Li, Zhen Lei</author><pubDate>Wed, 17 May 2023 12:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.04126v2</guid></item><item><title>Contact2Grasp: 3D Grasp Synthesis via Hand-Object Contact Constraint</title><link>http://arxiv.org/abs/2210.09245v3</link><description>3D grasp synthesis generates grasping poses given an input object. Existingworks tackle the problem by learning a direct mapping from objects to thedistributions of grasping poses. However, because the physical contact issensitive to small changes in pose, the high-nonlinear mapping between 3Dobject representation to valid poses is considerably non-smooth, leading topoor generation efficiency and restricted generality. To tackle the challenge,we introduce an intermediate variable for grasp contact areas to constrain thegrasp generation; in other words, we factorize the mapping into two sequentialstages by assuming that grasping poses are fully constrained given contactmaps: 1) we first learn contact map distributions to generate the potentialcontact maps for grasps; 2) then learn a mapping from the contact maps to thegrasping poses. Further, we propose a penetration-aware optimization with thegenerated contacts as a consistency constraint for grasp refinement. Extensivevalidations on two public datasets show that our method outperformsstate-of-the-art methods regarding grasp generation on various metrics.</description><author>Haoming Li, Xinzhuo Lin, Yang Zhou, Xiang Li, Yuchi Huo, Jiming Chen, Qi Ye</author><pubDate>Sat, 06 May 2023 08:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.09245v3</guid></item><item><title>OriCon3D: Effective 3D Object Detection using Orientation and Confidence</title><link>http://arxiv.org/abs/2304.14484v1</link><description>We introduce a technique for detecting 3D objects and estimating theirposition from a single image. Our method is built on top of a similarstate-of-the-art technique [1], but with improved accuracy. The approachfollowed in this research first estimates common 3D properties of an objectusing a Deep Convolutional Neural Network (DCNN), contrary to other frameworksthat only leverage centre-point predictions. We then combine these estimateswith geometric constraints provided by a 2D bounding box to produce a complete3D bounding box. The first output of our network estimates the 3D objectorientation using a discrete-continuous loss [1]. The second output predictsthe 3D object dimensions with minimal variance. Here we also present ourextensions by augmenting light-weight feature extractors and a customizedmultibin architecture. By combining these estimates with the geometricconstraints of the 2D bounding box, we can accurately (or comparatively)determine the 3D object pose better than our baseline [1] on the KITTI 3Ddetection benchmark [2].</description><author>Dhyey Manish Rajani, Rahul Kashyap Swayampakula, Surya Pratap Singh</author><pubDate>Thu, 27 Apr 2023 20:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14484v1</guid></item><item><title>End-to-end Weakly-supervised Single-stage Multiple 3D Hand Mesh Reconstruction from a Single RGB Image</title><link>http://arxiv.org/abs/2204.08154v3</link><description>In this paper, we consider the challenging task of simultaneously locatingand recovering multiple hands from a single 2D image. Previous studies eitherfocus on single hand reconstruction or solve this problem in a multi-stage way.Moreover, the conventional two-stage pipeline firstly detects hand areas, andthen estimates 3D hand pose from each cropped patch. To reduce thecomputational redundancy in preprocessing and feature extraction, for the firsttime, we propose a concise but efficient single-stage pipeline for multi-handreconstruction. Specifically, we design a multi-head auto-encoder structure,where each head network shares the same feature map and outputs the handcenter, pose and texture, respectively. Besides, we adopt a weakly-supervisedscheme to alleviate the burden of expensive 3D real-world data annotations. Tothis end, we propose a series of losses optimized by a stage-wise trainingscheme, where a multi-hand dataset with 2D annotations is generated based onthe publicly available single hand datasets. In order to further improve theaccuracy of the weakly supervised model, we adopt several feature consistencyconstraints in both single and multiple hand settings. Specifically, thekeypoints of each hand estimated from local features should be consistent withthe re-projected points predicted from global features. Extensive experimentson public benchmarks including FreiHAND, HO3D, InterHand2.6M and RHDdemonstrate that our method outperforms the state-of-the-art model-basedmethods in both weakly-supervised and fully-supervised manners. The code andmodels are available at {https://github.com/zijinxuxu/SMHR}.</description><author>Jinwei Ren, Jianke Zhu, Jialiang Zhang</author><pubDate>Sat, 06 May 2023 09:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.08154v3</guid></item><item><title>LPMM: Intuitive Pose Control for Neural Talking-Head Model via Landmark-Parameter Morphable Model</title><link>http://arxiv.org/abs/2305.10456v1</link><description>While current talking head models are capable of generating photorealistictalking head videos, they provide limited pose controllability. Most methodsrequire specific video sequences that should exactly contain the head posedesired, being far from user-friendly pose control. Three-dimensional morphablemodels (3DMM) offer semantic pose control, but they fail to capture certainexpressions. We present a novel method that utilizes parametric control of headorientation and facial expression over a pre-trained neural-talking head model.To enable this, we introduce a landmark-parameter morphable model (LPMM), whichoffers control over the facial landmark domain through a set of semanticparameters. Using LPMM, it is possible to adjust specific head pose factors,without distorting other facial attributes. The results show our approachprovides intuitive rig-like control over neural talking head models, allowingboth parameter and image-based inputs.</description><author>Kwangho Lee, Patrick Kwon, Myung Ki Lee, Namhyuk Ahn, Junsoo Lee</author><pubDate>Wed, 17 May 2023 07:11:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10456v1</guid></item><item><title>Learning Monocular Depth in Dynamic Environment via Context-aware Temporal Attention</title><link>http://arxiv.org/abs/2305.07397v1</link><description>The monocular depth estimation task has recently revealed encouragingprospects, especially for the autonomous driving task. To tackle the ill-posedproblem of 3D geometric reasoning from 2D monocular images, multi-framemonocular methods are developed to leverage the perspective correlationinformation from sequential temporal frames. However, moving objects such ascars and trains usually violate the static scene assumption, leading to featureinconsistency deviation and misaligned cost values, which would mislead theoptimization algorithm. In this work, we present CTA-Depth, a Context-awareTemporal Attention guided network for multi-frame monocular Depth estimation.Specifically, we first apply a multi-level attention enhancement module tointegrate multi-level image features to obtain an initial depth and poseestimation. Then the proposed CTA-Refiner is adopted to alternatively optimizethe depth and pose. During the refinement process, context-aware temporalattention (CTA) is developed to capture the global temporal-contextcorrelations to maintain the feature consistency and estimation integrity ofmoving objects. In particular, we propose a long-range geometry embedding (LGE)module to produce a long-range temporal geometry prior. Our approach achievessignificant improvements over state-of-the-art approaches on three benchmarkdatasets.</description><author>Zizhang Wu, Zhuozheng Li, Zhi-Gang Fan, Yunzhe Wu, Yuanzhu Gan, Jian Pu, Xianzhi Li</author><pubDate>Fri, 12 May 2023 12:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07397v1</guid></item><item><title>Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications</title><link>http://arxiv.org/abs/2302.05763v2</link><description>Human-robot interaction (HRI) research is progressively addressingmulti-party scenarios, where a robot interacts with more than one human user atthe same time. Conversely, research is still at an early stage for human-robotcollaboration. The use of machine learning techniques to handle such type ofcollaboration requires data that are less feasible to produce than in a typicalHRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRCapplications. Based upon these concepts, this study also proposes analternative way of gathering data regarding multi-user activity, by collectingdata related to single users and merging them in post-processing, to reduce theeffort involved in producing recordings of pair settings. To validate thisstatement, 3D skeleton poses of activity of single users were collected andmerged in pairs. After this, such datapoints were used to separately train along short-term memory (LSTM) network and a variational autoencoder (VAE)composed of spatio-temporal graph convolutional networks (STGCN) to recognisethe joint activities of the pairs of people. The results showed that it ispossible to make use of data collected in this way for pair HRC settings andget similar performances compared to using training data regarding groups ofusers recorded under the same settings, relieving from the technicaldifficulties involved in producing these data. The related code and collected data are publicly available.</description><author>Francesco Semeraro, Jon Carberry, Angelo Cangelosi</author><pubDate>Fri, 05 May 2023 16:15:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.05763v2</guid></item><item><title>RelPose++: Recovering 6D Poses from Sparse-view Observations</title><link>http://arxiv.org/abs/2305.04926v1</link><description>We address the task of estimating 6D camera poses from sparse-view image sets(2-8 images). This task is a vital pre-processing stage for nearly allcontemporary (neural) reconstruction algorithms but remains challenging givensparse views, especially for objects with visual symmetries and texture-lesssurfaces. We build on the recent RelPose framework which learns a network thatinfers distributions over relative rotations over image pairs. We extend thisapproach in two key ways; first, we use attentional transformer layers toprocess multiple images jointly, since additional views of an object mayresolve ambiguous symmetries in any given image pair (such as the handle of amug that becomes visible in a third view). Second, we augment this network toalso report camera translations by defining an appropriate coordinate systemthat decouples the ambiguity in rotation estimation from translationprediction. Our final system results in large improvements in 6D poseprediction over prior art on both seen and unseen object categories and alsoenables pose estimation and 3D reconstruction for in-the-wild objects.</description><author>Amy Lin, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Mon, 08 May 2023 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04926v1</guid></item><item><title>Learning Pose Image Manifolds Using Geometry-Preserving GANs and Elasticae</title><link>http://arxiv.org/abs/2305.10513v1</link><description>This paper investigates the challenge of learning image manifolds,specifically pose manifolds, of 3D objects using limited training data. Itproposes a DNN approach to manifold learning and for predicting images ofobjects for novel, continuous 3D rotations. The approach uses two distinctconcepts: (1) Geometric Style-GAN (Geom-SGAN), which maps images tolow-dimensional latent representations and maintains the (first-order) manifoldgeometry. That is, it seeks to preserve the pairwise distances between basepoints and their tangent spaces, and (2) uses Euler's elastica to smoothlyinterpolate between directed points (points + tangent directions) in thelow-dimensional latent space. When mapped back to the larger image space, theresulting interpolations resemble videos of rotating objects. Extensiveexperiments establish the superiority of this framework in learning paths onrotation manifolds, both visually and quantitatively, relative tostate-of-the-art GANs and VAEs.</description><author>Shenyuan Liang, Pavan Turaga, Anuj Srivastava</author><pubDate>Wed, 17 May 2023 19:45:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10513v1</guid></item><item><title>gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction</title><link>http://arxiv.org/abs/2304.11970v1</link><description>Signed distance functions (SDFs) is an attractive framework that has recentlyshown promising results for 3D shape reconstruction from images. SDFsseamlessly generalize to different shape resolutions and topologies but lackexplicit modelling of the underlying 3D geometry. In this work, we exploit thehand structure and use it as guidance for SDF-based shape reconstruction. Inparticular, we address reconstruction of hands and manipulated objects frommonocular RGB images. To this end, we estimate poses of hands and objects anduse them to guide 3D reconstruction. More specifically, we predict kinematicchains of pose transformations and align SDFs with highly-articulated handposes. We improve the visual features of 3D points with geometry alignment andfurther leverage temporal information to enhance the robustness to occlusionand motion blurs. We conduct extensive experiments on the challenging ObMan andDexYCB benchmarks and demonstrate significant improvements of the proposedmethod over the state of the art.</description><author>Zerui Chen, Shizhe Chen, Cordelia Schmid, Ivan Laptev</author><pubDate>Mon, 24 Apr 2023 11:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11970v1</guid></item><item><title>CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction</title><link>http://arxiv.org/abs/2304.14633v1</link><description>Recent advances in neural reconstruction using posed image sequences havemade remarkable progress. However, due to the lack of depth information,existing volumetric-based techniques simply duplicate 2D image features of theobject surface along the entire camera ray. We contend this duplicationintroduces noise in empty and occluded spaces, posing challenges for producinghigh-quality 3D geometry. Drawing inspiration from traditional multi-viewstereo methods, we propose an end-to-end 3D neural reconstruction frameworkCVRecon, designed to exploit the rich geometric embedding in the cost volumesto facilitate 3D geometric feature learning. Furthermore, we presentRay-contextual Compensated Cost Volume (RCCV), a novel 3D geometric featurerepresentation that encodes view-dependent information with improved integrityand robustness. Through comprehensive experiments, we demonstrate that ourapproach significantly improves the reconstruction quality in various metricsand recovers clear fine details of the 3D geometries. Our extensive ablationstudies provide insights into the development of effective 3D geometric featurelearning schemes. Project page: https://cvrecon.ziyue.cool/</description><author>Ziyue Feng, Leon Yang, Pengsheng Guo, Bing Li</author><pubDate>Fri, 28 Apr 2023 06:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14633v1</guid></item><item><title>ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation</title><link>http://arxiv.org/abs/2305.01618v1</link><description>We propose a new dataset and a novel approach to learning hand-objectinteraction priors for hand and articulated object pose estimation. We firstcollect a dataset using visual teleoperation, where the human operator candirectly play within a physical simulator to manipulate the articulatedobjects. We record the data and obtain free and accurate annotations on objectposes and contact information from the simulator. Our system only requires aniPhone to record human hand motion, which can be easily scaled up and largelylower the costs of data and annotation collection. With this data, we learn 3Dinteraction priors including a discriminator (in a GAN) capturing thedistribution of how object parts are arranged, and a diffusion model whichgenerates the contact regions on articulated objects, guiding the hand poseestimation. Such structural and contact priors can easily transfer toreal-world data with barely any domain gap. By using our data and learnedpriors, our method significantly improves the performance on joint hand andarticulated object poses estimation over the existing state-of-the-art methods.The project is available at https://zehaozhu.github.io/ContactArt/ .</description><author>Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang</author><pubDate>Tue, 02 May 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01618v1</guid></item><item><title>Domain-Adaptive Full-Face Gaze Estimation via Novel-View-Synthesis and Feature Disentanglement</title><link>http://arxiv.org/abs/2305.16140v1</link><description>Along with the recent development of deep neural networks, appearance-basedgaze estimation has succeeded considerably when training and testing within thesame domain. Compared to the within-domain task, the variance of differentdomains makes the cross-domain performance drop severely, preventing gazeestimation deployment in real-world applications. Among all the factors, rangesof head pose and gaze are believed to play a significant role in the finalperformance of gaze estimation, while collecting large ranges of data isexpensive. This work proposes an effective model training pipeline consistingof a training data synthesis and a gaze estimation model for unsuperviseddomain adaptation. The proposed data synthesis leverages the single-image 3Dreconstruction to expand the range of the head poses from the source domainwithout requiring a 3D facial shape dataset. To bridge the inevitable gapbetween synthetic and real images, we further propose an unsupervised domainadaptation method suitable for synthetic full-face data. We propose adisentangling autoencoder network to separate gaze-related features andintroduce background augmentation consistency loss to utilize thecharacteristics of the synthetic source domain. Through comprehensiveexperiments, we show that the model only using monocular-reconstructedsynthetic training data can perform comparably to real data with a large labelrange. Our proposed domain adaptation approach further improves the performanceon multiple target domains. The code and data will be available at\url{https://github.com/ut-vision/AdaptiveGaze}.</description><author>Jiawei Qin, Takuru Shimoyama, Xucong Zhang, Yusuke Sugano</author><pubDate>Thu, 25 May 2023 16:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16140v1</guid></item><item><title>Depth-based 6DoF Object Pose Estimation using Swin Transformer</title><link>http://arxiv.org/abs/2303.02133v2</link><description>Accurately estimating the 6D pose of objects is crucial for manyapplications, such as robotic grasping, autonomous driving, and augmentedreality. However, this task becomes more challenging in poor lightingconditions or when dealing with textureless objects. To address this issue,depth images are becoming an increasingly popular choice due to theirinvariance to a scene's appearance and the implicit incorporation of essentialgeometric characteristics. However, fully leveraging depth information toimprove the performance of pose estimation remains a difficult andunder-investigated problem. To tackle this challenge, we propose a novelframework called SwinDePose, that uses only geometric information from depthimages to achieve accurate 6D pose estimation. SwinDePose first calculates theangles between each normal vector defined in a depth image and the threecoordinate axes in the camera coordinate system. The resulting angles are thenformed into an image, which is encoded using Swin Transformer. Additionally, weapply RandLA-Net to learn the representations from point clouds. The resultingimage and point clouds embeddings are concatenated and fed into a semanticsegmentation module and a 3D keypoints localization module. Finally, weestimate 6D poses using a least-square fitting approach based on the targetobject's predicted semantic mask and 3D keypoints. In experiments on theLineMod and Occlusion LineMod datasets, SwinDePose outperforms existingstate-of-the-art methods for 6D object pose estimation using depth images. Thisdemonstrates the effectiveness of our approach and highlights its potential forimproving performance in real-world scenarios. Our code is athttps://github.com/zhujunli1993/SwinDePose.</description><author>Zhujun Li, Ioannis Stamos</author><pubDate>Thu, 27 Apr 2023 19:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02133v2</guid></item><item><title>POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference</title><link>http://arxiv.org/abs/2305.15727v1</link><description>Despite the significant progress in six degrees-of-freedom (6DoF) object poseestimation, existing methods have limited applicability in real-world scenariosinvolving embodied agents and downstream 3D vision tasks. These limitationsmainly come from the necessity of 3D models, closed-category detection, and alarge number of densely annotated support views. To mitigate this issue, wepropose a general paradigm for object pose estimation, called Promptable ObjectPose Estimation (POPE). The proposed approach POPE enables zero-shot 6DoFobject pose estimation for any target object in any scene, while only a singlereference is adopted as the support view. To achieve this, POPE leverages thepower of the pre-trained large-scale 2D foundation model, employs a frameworkwith hierarchical feature representation and 3D geometry principles. Moreover,it estimates the relative camera pose between object prompts and the targetobject in new views, enabling both two-view and multi-view 6DoF pose estimationtasks. Comprehensive experimental results demonstrate that POPE exhibitsunrivaled robust performance in zero-shot settings, by achieving a significantreduction in the averaged Median Pose Error by 52.38% and 50.47% on the LINEMODand OnePose datasets, respectively. We also conduct more challenging testingsin causally captured images (see Figure 1), which further demonstrates therobustness of POPE. Project page can be found withhttps://paulpanwang.github.io/POPE/.</description><author>Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Dejia Xu, Hanwen Jiang, Zhangyang Wang</author><pubDate>Thu, 25 May 2023 06:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15727v1</guid></item><item><title>Towards Realistic Generative 3D Face Models</title><link>http://arxiv.org/abs/2304.12483v1</link><description>In recent years, there has been significant progress in 2D generative facemodels fueled by applications such as animation, synthetic data generation, anddigital avatars. However, due to the absence of 3D information, these 2D modelsoften struggle to accurately disentangle facial attributes like pose,expression, and illumination, limiting their editing capabilities. To addressthis limitation, this paper proposes a 3D controllable generative face model toproduce high-quality albedo and precise 3D shape leveraging existing 2Dgenerative models. By combining 2D face generative models with semantic facemanipulation, this method enables editing of detailed 3D rendered faces. Theproposed framework utilizes an alternating descent optimization approach overshape and albedo. Differentiable rendering is used to train high-quality shapesand albedo without 3D supervision. Moreover, this approach outperforms thestate-of-the-art (SOTA) methods in the well-known NoW benchmark for shapereconstruction. It also outperforms the SOTA reconstruction models inrecovering rendered faces' identities across novel poses by an average of 10%.Additionally, the paper demonstrates direct control of expressions in 3D facesby exploiting latent space leading to text-based editing of 3D faces.</description><author>Aashish Rai, Hiresh Gupta, Ayush Pandey, Francisco Vicente Carrasco, Shingo Jason Takagi, Amaury Aubel, Daeil Kim, Aayush Prakash, Fernando de la Torre</author><pubDate>Mon, 24 Apr 2023 23:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12483v1</guid></item><item><title>Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback</title><link>http://arxiv.org/abs/2305.15808v1</link><description>Generating and editing a 3D scene guided by natural language poses achallenge, primarily due to the complexity of specifying the positionalrelations and volumetric changes within the 3D space. Recent advancements inLarge Language Models (LLMs) have demonstrated impressive reasoning,conversational, and zero-shot generation abilities across various domains.Surprisingly, these models also show great potential in realizing andinterpreting the 3D space. In light of this, we propose a novel language-guidedinteractive 3D generation system, dubbed LI3D, that integrates LLMs as a 3Dlayout interpreter into the off-the-shelf layout-to-3D generative models,allowing users to flexibly and interactively generate visual content.Specifically, we design a versatile layout structure base on the bounding boxesand semantics to prompt the LLMs to model the spatial generation and reasoningfrom language. Our system also incorporates LLaVA, a large language and visionassistant, to provide generative feedback from the visual aspect for improvingthe visual quality of generated content. We validate the effectiveness of LI3D,primarily in 3D generation and editing through multi-round interactions, whichcan be flexibly extended to 2D generation and editing. Various experimentsdemonstrate the potential benefits of incorporating LLMs in generative AI forapplications, e.g., metaverse. Moreover, we benchmark the layout reasoningperformance of LLMs with neural visual artist tasks, revealing their emergentability in the spatial layout domain.</description><author>Yiqi Lin, Hao Wu, Ruichen Wang, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang</author><pubDate>Thu, 25 May 2023 08:43:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15808v1</guid></item><item><title>3D Registration with Maximal Cliques</title><link>http://arxiv.org/abs/2305.10854v1</link><description>As a fundamental problem in computer vision, 3D point cloud registration(PCR) aims to seek the optimal pose to align a point cloud pair. In this paper,we present a 3D registration method with maximal cliques (MAC). The key insightis to loosen the previous maximum clique constraint, and mine more localconsensus information in a graph for accurate pose hypotheses generation: 1) Acompatibility graph is constructed to render the affinity relationship betweeninitial correspondences. 2) We search for maximal cliques in the graph, each ofwhich represents a consensus set. We perform node-guided clique selection then,where each node corresponds to the maximal clique with the greatest graphweight. 3) Transformation hypotheses are computed for the selected cliques bythe SVD algorithm and the best hypothesis is used to perform registration.Extensive experiments on U3M, 3DMatch, 3DLoMatch and KITTI demonstrate that MACeffectively increases registration accuracy, outperforms variousstate-of-the-art methods and boosts the performance of deep-learned methods.MAC combined with deep-learned methods achieves state-of-the-art registrationrecall of 95.7% / 78.9% on 3DMatch / 3DLoMatch.</description><author>Xiyu Zhang, Jiaqi Yang, Shikun Zhang, Yanning Zhang</author><pubDate>Thu, 18 May 2023 11:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10854v1</guid></item><item><title>CloudWalker: Random walks for 3D point cloud shape analysis</title><link>http://arxiv.org/abs/2112.01050v4</link><description>Point clouds are gaining prominence as a method for representing 3D shapes,but their irregular structure poses a challenge for deep learning methods. Inthis paper we propose CloudWalker, a novel method for learning 3D shapes usingrandom walks. Previous works attempt to adapt Convolutional Neural Networks(CNNs) or impose a grid or mesh structure to 3D point clouds. This workpresents a different approach for representing and learning the shape from agiven point set. The key idea is to impose structure on the point set bymultiple random walks through the cloud for exploring different regions of the3D object. Then we learn a per-point and per-walk representation and aggregatemultiple walk predictions at inference. Our approach achieves state-of-the-artresults for two 3D shape analysis tasks: classification and retrieval.</description><author>Adi Mesika, Yizhak Ben-Shabat, Ayellet Tal</author><pubDate>Wed, 17 May 2023 08:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.01050v4</guid></item><item><title>HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</title><link>http://arxiv.org/abs/2212.10428v4</link><description>Estimating the 6D pose of objects is a major 3D computer vision problem.Since the promising outcomes from instance-level approaches, research headsalso move towards category-level pose estimation for more practical applicationscenarios. However, unlike well-established instance-level pose datasets,available category-level datasets lack annotation quality and provided posequantity. We propose the new category-level 6D pose dataset HouseCat6Dfeaturing 1) Multi-modality of Polarimetric RGB and Depth (RGBD+P), 2) Highlydiverse 194 objects of 10 household object categories including 2photometrically challenging categories, 3) High-quality pose annotation with anerror range of only 1.35 mm to 1.74 mm, 4) 41 large-scale scenes with extensiveviewpoint coverage and occlusions, 5) Checkerboard-free environment throughoutthe entire scene, and 6) Additionally annotated dense 6D parallel-jaw grasps.Furthermore, we also provide benchmark results of state-of-the-artcategory-level pose estimation networks.</description><author>HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp, Guangyao Zhai, Hannah Schieber, Giulia Rizzoli, Pengyuan Wang, Hongcheng Zhao, Lorenzo Garattoni, Sven Meier, Daniel Roth, Nassir Navab, Benjamin Busam</author><pubDate>Wed, 26 Apr 2023 11:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10428v4</guid></item><item><title>SensorX2car: Sensors-to-car calibration for autonomous driving in road scenarios</title><link>http://arxiv.org/abs/2301.07279v2</link><description>Properly-calibrated sensors are the prerequisite for a dependable autonomousdriving system. However, most prior methods focus on extrinsic calibrationbetween sensors, and few focus on the misalignment between the sensors and thevehicle coordinate system. Existing targetless approaches rely on specificprior knowledge, such as driving routes and road features, to handle thismisalignment. This work removes these limitations and proposes more generalcalibration methods for four commonly used sensors: Camera, LiDAR, GNSS/INS,and millimeter-wave Radar. By utilizing sensor-specific patterns: imagefeature, 3D LiDAR points, GNSS/INS solved pose, and radar speed, we design fourcorresponding methods to mainly calibrate the rotation from sensor to carduring normal driving within minutes, composing a toolbox named SensorX2car.Real-world and simulated experiments demonstrate the practicality of ourproposed methods. Meanwhile, the related codes have been open-sourced tobenefit the community. To the best of our knowledge, SensorX2car is the firstopen-source sensor-to-car calibration toolbox. The code is available athttps://github.com/OpenCalib/SensorX2car.</description><author>Guohang Yan, Zhaotong Luo, Zhuochun Liu, Yikang Li</author><pubDate>Thu, 18 May 2023 09:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.07279v2</guid></item><item><title>Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN</title><link>http://arxiv.org/abs/2212.09098v3</link><description>Fine-grained semantic segmentation of a person's face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.</description><author>Lei Li, Tianfang Zhang, Zhongfeng Kang, Xikun Jiang</author><pubDate>Sun, 30 Apr 2023 03:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09098v3</guid></item><item><title>Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN</title><link>http://arxiv.org/abs/2212.09098v4</link><description>Fine-grained semantic segmentation of a person's face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.</description><author>Lei Li, Tianfang Zhang, Zhongfeng Kang, Xikun Jiang</author><pubDate>Sat, 06 May 2023 19:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09098v4</guid></item><item><title>Unsupervised Style-based Explicit 3D Face Reconstruction from Single Image</title><link>http://arxiv.org/abs/2304.12455v1</link><description>Inferring 3D object structures from a single image is an ill-posed task dueto depth ambiguity and occlusion. Typical resolutions in the literature includeleveraging 2D or 3D ground truth for supervised learning, as well as imposinghand-crafted symmetry priors or using an implicit representation to hallucinatenovel viewpoints for unsupervised methods. In this work, we propose a generaladversarial learning framework for solving Unsupervised 2D to Explicit 3D StyleTransfer (UE3DST). Specifically, we merge two architectures: the unsupervisedexplicit 3D reconstruction network of Wu et al.\ and the Generative AdversarialNetwork (GAN) named StarGAN-v2. We experiment across three facial datasets(Basel Face Model, 3DFAW and CelebA-HQ) and show that our solution is able tooutperform well established solutions such as DepthNet in 3D reconstruction andPix2NeRF in conditional style transfer, while we also justify the individualcontributions of our model components via ablation. In contrast to theaforementioned baselines, our scheme produces features for explicit 3Drendering, which can be manipulated and utilized in downstream tasks.</description><author>Heng Yu, Zoltan A. Milacski, Laszlo A. Jeni</author><pubDate>Mon, 24 Apr 2023 22:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12455v1</guid></item><item><title>Learning Hand-Held Object Reconstruction from In-The-Wild Videos</title><link>http://arxiv.org/abs/2305.03036v1</link><description>Prior works for reconstructing hand-held objects from a single image rely ondirect 3D shape supervision which is challenging to gather in real world atscale. Consequently, these approaches do not generalize well when presentedwith novel objects in in-the-wild settings. While 3D supervision is a majorbottleneck, there is an abundance of in-the-wild raw video data showinghand-object interactions. In this paper, we automatically extract 3Dsupervision (via multiview 2D supervision) from such raw video data to scale upthe learning of models for hand-held object reconstruction. This requirestackling two key challenges: unknown camera pose and occlusion. For the former,we use hand pose (predicted from existing techniques, e.g. FrankMocap) as aproxy for object pose. For the latter, we learn data-driven 3D shape priorsusing synthetic objects from the ObMan dataset. We use these indirect 3D cuesto train occupancy networks that predict the 3D shape of objects from a singleRGB image. Our experiments on the MOW and HO3D datasets show the effectivenessof these supervisory signals at predicting the 3D shape for real-worldhand-held objects without any direct real-world 3D supervision.</description><author>Aditya Prakash, Matthew Chang, Matthew Jin, Saurabh Gupta</author><pubDate>Thu, 04 May 2023 18:56:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03036v1</guid></item><item><title>Towards Realistic 3D Embedding via View Alignment</title><link>http://arxiv.org/abs/2007.07066v3</link><description>Recent advances in generative adversarial networks (GANs) have achieved greatsuccess in automated image composition that generates new images by embeddinginterested foreground objects into background images automatically. On theother hand, most existing works deal with foreground objects in two-dimensional(2D) images though foreground objects in three-dimensional (3D) models are moreflexible with 360-degree view freedom. This paper presents an innovative ViewAlignment GAN (VA-GAN) that composes new images by embedding 3D models into 2Dbackground images realistically and automatically. VA-GAN consists of a texturegenerator and a differential discriminator that are inter-connected andend-to-end trainable. The differential discriminator guides to learn geometrictransformation from background images so that the composed 3D models can bealigned with the background images with realistic poses and views. The texturegenerator adopts a novel view encoding mechanism for generating accurate objecttextures for the 3D models under the estimated views. Extensive experimentsover two synthesis tasks (car synthesis with KITTI and pedestrian synthesiswith Cityscapes) show that VA-GAN achieves high-fidelity compositionqualitatively and quantitatively as compared with state-of-the-art generationmethods.</description><author>Changgong Zhang, Fangneng Zhan, Shijian Lu, Feiying Ma, Xuansong Xie</author><pubDate>Mon, 24 Apr 2023 13:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.07066v3</guid></item><item><title>Multi-State RNA Design with Geometric Multi-Graph Neural Networks</title><link>http://arxiv.org/abs/2305.14749v1</link><description>Computational RNA design has broad applications across synthetic biology andtherapeutic development. Fundamental to the diverse biological functions of RNAis its conformational flexibility, enabling single sequences to adopt a varietyof distinct 3D states. Currently, computational biomolecule design tasks areoften posed as inverse problems, where sequences are designed based on adoptinga single desired structural conformation. In this work, we propose gRNAde, ageometric RNA design pipeline that operates on sets of 3D RNA backbonestructures to explicitly account for and reflect RNA conformational diversityin its designs. We demonstrate the utility of gRNAde for improving nativesequence recovery over single-state approaches on a new large-scale 3D RNAdesign dataset, especially for multi-state and structurally diverse RNAs. Ourcode is available at https://github.com/chaitjo/geometric-rna-design</description><author>Chaitanya K. Joshi, Arian R. Jamasb, Ramon Viñas, Charles Harris, Simon Mathis, Pietro Liò</author><pubDate>Wed, 24 May 2023 06:46:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14749v1</guid></item><item><title>Multi-State RNA Design with Geometric Multi-Graph Neural Networks</title><link>http://arxiv.org/abs/2305.14749v2</link><description>Computational RNA design has broad applications across synthetic biology andtherapeutic development. Fundamental to the diverse biological functions of RNAis its conformational flexibility, enabling single sequences to adopt a varietyof distinct 3D states. Currently, computational biomolecule design tasks areoften posed as inverse problems, where sequences are designed based on adoptinga single desired structural conformation. In this work, we propose gRNAde, ageometric RNA design pipeline that operates on sets of 3D RNA backbonestructures to explicitly account for and reflect RNA conformational diversityin its designs. We demonstrate the utility of gRNAde for improving nativesequence recovery over single-state approaches on a new large-scale 3D RNAdesign dataset, especially for multi-state and structurally diverse RNAs. Ourcode is available at https://github.com/chaitjo/geometric-rna-design</description><author>Chaitanya K. Joshi, Arian R. Jamasb, Ramon Viñas, Charles Harris, Simon Mathis, Pietro Liò</author><pubDate>Thu, 25 May 2023 15:53:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14749v2</guid></item><item><title>PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling</title><link>http://arxiv.org/abs/2304.13006v1</link><description>Creating pose-driven human avatars is about modeling the mapping from thelow-frequency driving pose to high-frequency dynamic human appearances, so aneffective pose encoding method that can encode high-fidelity human details isessential to human avatar modeling.To this end, we present PoseVocab, a novelpose encoding method that encourages the network to discover the optimal poseembeddings for learning the dynamic human appearance. Given multi-view RGBvideos of a character, PoseVocab constructs key poses and latent embeddingsbased on the training poses. To achieve pose generalization and temporalconsistency, we sample key rotations in $so(3)$ of each joint rather than theglobal pose vectors, and assign a pose embedding to each sampled key rotation.These joint-structured pose embeddings not only encode the dynamic appearancesunder different key poses, but also factorize the global pose embedding intojoint-structured ones to better learn the appearance variation related to themotion of each joint. To improve the representation ability of the poseembedding while maintaining memory efficiency, we introduce feature lines, acompact yet effective 3D representation, to model more fine-grained details ofhuman appearances. Furthermore, given a query pose and a spatial position, ahierarchical query strategy is introduced to interpolate pose embeddings andacquire the conditional pose feature for dynamic human synthesis. Overall,PoseVocab effectively encodes the dynamic details of human appearance andenables realistic and generalized animation under novel poses. Experiments showthat our method outperforms other state-of-the-art baselines both qualitativelyand quantitatively in terms of synthesis quality. Code is available athttps://github.com/lizhe00/PoseVocab.</description><author>Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, Yebin Liu</author><pubDate>Tue, 25 Apr 2023 18:25:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13006v1</guid></item><item><title>PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling</title><link>http://arxiv.org/abs/2304.13006v2</link><description>Creating pose-driven human avatars is about modeling the mapping from thelow-frequency driving pose to high-frequency dynamic human appearances, so aneffective pose encoding method that can encode high-fidelity human details isessential to human avatar modeling. To this end, we present PoseVocab, a novelpose encoding method that encourages the network to discover the optimal poseembeddings for learning the dynamic human appearance. Given multi-view RGBvideos of a character, PoseVocab constructs key poses and latent embeddingsbased on the training poses. To achieve pose generalization and temporalconsistency, we sample key rotations in $so(3)$ of each joint rather than theglobal pose vectors, and assign a pose embedding to each sampled key rotation.These joint-structured pose embeddings not only encode the dynamic appearancesunder different key poses, but also factorize the global pose embedding intojoint-structured ones to better learn the appearance variation related to themotion of each joint. To improve the representation ability of the poseembedding while maintaining memory efficiency, we introduce feature lines, acompact yet effective 3D representation, to model more fine-grained details ofhuman appearances. Furthermore, given a query pose and a spatial position, ahierarchical query strategy is introduced to interpolate pose embeddings andacquire the conditional pose feature for dynamic human synthesis. Overall,PoseVocab effectively encodes the dynamic details of human appearance andenables realistic and generalized animation under novel poses. Experiments showthat our method outperforms other state-of-the-art baselines both qualitativelyand quantitatively in terms of synthesis quality. Code is available athttps://github.com/lizhe00/PoseVocab.</description><author>Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, Yebin Liu</author><pubDate>Sun, 14 May 2023 14:15:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13006v2</guid></item><item><title>HypLiLoc: Towards Effective LiDAR Pose Regression with Hyperbolic Fusion</title><link>http://arxiv.org/abs/2304.00932v2</link><description>LiDAR relocalization plays a crucial role in many fields, including robotics,autonomous driving, and computer vision. LiDAR-based retrieval from a databasetypically incurs high computation storage costs and can lead to globallyinaccurate pose estimations if the database is too sparse. On the other hand,pose regression methods take images or point clouds as inputs and directlyregress global poses in an end-to-end manner. They do not perform databasematching and are more computationally efficient than retrieval techniques. Wepropose HypLiLoc, a new model for LiDAR pose regression. We use two branchedbackbones to extract 3D features and 2D projection features, respectively. Weconsider multi-modal feature fusion in both Euclidean and hyperbolic spaces toobtain more effective feature representations. Experimental results indicatethat HypLiLoc achieves state-of-the-art performance in both outdoor and indoordatasets. We also conduct extensive ablation studies on the framework design,which demonstrate the effectiveness of multi-modal feature extraction andmulti-space embedding. Our code is released at:https://github.com/sijieaaa/HypLiLoc</description><author>Sijie Wang, Qiyu Kang, Rui She, Wei Wang, Kai Zhao, Yang Song, Wee Peng Tay</author><pubDate>Thu, 25 May 2023 13:10:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.00932v2</guid></item><item><title>Animatable Implicit Neural Representations for Creating Realistic Avatars from Videos</title><link>http://arxiv.org/abs/2203.08133v4</link><description>This paper addresses the challenge of reconstructing an animatable humanmodel from a multi-view video. Some recent works have proposed to decompose anon-rigidly deforming scene into a canonical neural radiance field and a set ofdeformation fields that map observation-space points to the canonical space,thereby enabling them to learn the dynamic scene from images. However, theyrepresent the deformation field as translational vector field or SE(3) field,which makes the optimization highly under-constrained. Moreover, theserepresentations cannot be explicitly controlled by input motions. Instead, weintroduce a pose-driven deformation field based on the linear blend skinningalgorithm, which combines the blend weight field and the 3D human skeleton toproduce observation-to-canonical correspondences. Since 3D human skeletons aremore observable, they can regularize the learning of the deformation field.Moreover, the pose-driven deformation field can be controlled by input skeletalmotions to generate new deformation fields to animate the canonical humanmodel. Experiments show that our approach significantly outperforms recenthuman modeling methods. The code is available athttps://zju3dv.github.io/animatable_nerf/.</description><author>Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, Xiaowei Zhou</author><pubDate>Thu, 04 May 2023 08:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.08133v4</guid></item><item><title>You Only Look at One: Category-Level Object Representations for Pose Estimation From a Single Example</title><link>http://arxiv.org/abs/2305.12626v1</link><description>In order to meaningfully interact with the world, robot manipulators must beable to interpret objects they encounter. A critical aspect of thisinterpretation is pose estimation: inferring quantities that describe theposition and orientation of an object in 3D space. Most existing approaches topose estimation make limiting assumptions, often working only for specific,known object instances, or at best generalising to an object category usinglarge pose-labelled datasets. In this work, we present a method for achievingcategory-level pose estimation by inspection of just a single object from adesired category. We show that we can subsequently perform accurate poseestimation for unseen objects from an inspected category, and considerablyoutperform prior work by exploiting multi-view correspondences. We demonstratethat our method runs in real-time, enabling a robot manipulator equipped withan RGBD sensor to perform online 6D pose estimation for novel objects. Finally,we showcase our method in a continual learning setting, with a robot able todetermine whether objects belong to known categories, and if not, use activeperception to produce a one-shot category representation for subsequent poseestimation.</description><author>Walter Goodwin, Ioannis Havoutis, Ingmar Posner</author><pubDate>Mon, 22 May 2023 02:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12626v1</guid></item><item><title>AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions</title><link>http://arxiv.org/abs/2112.00246v6</link><description>Perceiving and interacting with 3D articulated objects, such as cabinets,doors, and faucets, pose particular challenges for future home-assistant robotsperforming daily tasks in human environments. Besides parsing the articulatedparts and joint parameters, researchers recently advocate learning manipulationaffordance over the input shape geometry which is more task-aware andgeometrically fine-grained. However, taking only passive observations asinputs, these methods ignore many hidden but important kinematic constraints(e.g., joint location and limits) and dynamic factors (e.g., joint friction andrestitution), therefore losing significant accuracy for test cases with suchuncertainties. In this paper, we propose a novel framework, named AdaAfford,that learns to perform very few test-time interactions for quickly adapting theaffordance priors to more accurate instance-specific posteriors. We conductlarge-scale experiments using the PartNet-Mobility dataset and prove that oursystem performs better than baselines.</description><author>Yian Wang, Ruihai Wu, Kaichun Mo, Jiaqi Ke, Qingnan Fan, Leonidas Guibas, Hao Dong</author><pubDate>Thu, 04 May 2023 15:47:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.00246v6</guid></item><item><title>Privacy-Preserving Representations are not Enough -- Recovering Scene Content from Camera Poses</title><link>http://arxiv.org/abs/2305.04603v1</link><description>Visual localization is the task of estimating the camera pose from which agiven image was taken and is central to several 3D computer visionapplications. With the rapid growth in the popularity of AR/VR/MR devices andcloud-based applications, privacy issues are becoming a very important aspectof the localization process. Existing work on privacy-preserving localizationaims to defend against an attacker who has access to a cloud-based service. Inthis paper, we show that an attacker can learn about details of a scene withoutany access by simply querying a localization service. The attack is based onthe observation that modern visual localization algorithms are robust tovariations in appearance and geometry. While this is in general a desiredproperty, it also leads to algorithms localizing objects that are similarenough to those present in a scene. An attacker can thus query a server with alarge enough set of images of objects, \eg, obtained from the Internet, andsome of them will be localized. The attacker can thus learn about objectplacements from the camera poses returned by the service (which is the minimalinformation returned by such a service). In this paper, we develop aproof-of-concept version of this attack and demonstrate its practicalfeasibility. The attack does not place any requirements on the localizationalgorithm used, and thus also applies to privacy-preserving representations.Current work on privacy-preserving representations alone is thus insufficient.</description><author>Kunal Chelani, Torsten Sattler, Fredrik Kahl, Zuzana Kukelova</author><pubDate>Mon, 08 May 2023 11:25:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04603v1</guid></item><item><title>Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2305.15801v1</link><description>A successful tactic that is followed by the scientific community foradvancing AI is to treat games as problems, which has been proven to lead tovarious breakthroughs. We adapt this strategy in order to study Rocket League,a widely popular but rather under-explored 3D multiplayer video game with adistinct physics engine and complex dynamics that pose a significant challengein developing efficient and high-performance game-playing agents. In thispaper, we present Lucy-SKG, a Reinforcement Learning-based model that learnedhow to play Rocket League in a sample-efficient manner, outperforming by anotable margin the two highest-ranking bots in this game, namely Necto (2022bot champion) and its successor Nexto, thus becoming a state-of-the-art agent.Our contributions include: a) the development of a reward analysis andvisualization library, b) novel parameterizable reward shape functions thatcapture the utility of complex reward types via our proposed Kinesthetic RewardCombination (KRC) technique, and c) design of auxiliary neural architecturesfor training on reward prediction and state representation tasks in anon-policy fashion for enhanced efficiency in learning speed and performance. Byperforming thorough ablation studies for each component of Lucy-SKG, we showedtheir independent effectiveness in overall performance. In doing so, wedemonstrate the prospects and challenges of using sample-efficientReinforcement Learning techniques for controlling complex dynamical systemsunder competitive team-based multiplayer conditions.</description><author>Vasileios Moschopoulos, Pantelis Kyriakidis, Aristotelis Lazaridis, Ioannis Vlahavas</author><pubDate>Thu, 25 May 2023 08:33:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15801v1</guid></item><item><title>Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion Data and Natural Language</title><link>http://arxiv.org/abs/2305.15842v1</link><description>Due to recent advances in pose-estimation methods, human motion can beextracted from a common video in the form of 3D skeleton sequences. Despitewonderful application opportunities, effective and efficient content-basedaccess to large volumes of such spatio-temporal skeleton data still remains achallenging problem. In this paper, we propose a novel content-basedtext-to-motion retrieval task, which aims at retrieving relevant motions basedon a specified natural-language textual description. To define baselines forthis uncharted task, we employ the BERT and CLIP language representations toencode the text modality and successful spatio-temporal models to encode themotion modality. We additionally introduce our transformer-based approach,called Motion Transformer (MoT), which employs divided space-time attention toeffectively aggregate the different skeleton joints in space and time. Inspiredby the recent progress in text-to-image/video matching, we experiment with twowidely-adopted metric-learning loss functions. Finally, we set up a commonevaluation protocol by defining qualitative metrics for assessing the qualityof the retrieved motions, targeting the two recently-introduced KITMotion-Language and HumanML3D datasets. The code for reproducing our results isavailable at https://github.com/mesnico/text-to-motion-retrieval.</description><author>Nicola Messina, Jan Sedmidubsky, Fabrizio Falchi, Tomáš Rebok</author><pubDate>Thu, 25 May 2023 09:32:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15842v1</guid></item><item><title>StyleLipSync: Style-based Personalized Lip-sync Video Generation</title><link>http://arxiv.org/abs/2305.00521v1</link><description>In this paper, we present StyleLipSync, a style-based personalized lip-syncvideo generative model that can generate identity-agnostic lip-synchronizingvideo from arbitrary audio. To generate a video of arbitrary identities, weleverage expressive lip prior from the semantically rich latent space of apre-trained StyleGAN, where we can also design a video consistency with alinear transformation. In contrast to the previous lip-sync methods, weintroduce pose-aware masking that dynamically locates the mask to improve thenaturalness over frames by utilizing a 3D parametric mesh predictor frame byframe. Moreover, we propose a few-shot lip-sync adaptation method for anarbitrary person by introducing a sync regularizer that preserves lips-syncgeneralization while enhancing the person-specific visual information.Extensive experiments demonstrate that our model can generate accurate lip-syncvideos even with the zero-shot setting and enhance characteristics of an unseenface using a few seconds of target video through the proposed adaptationmethod. Please refer to our project page.</description><author>Taekyung Ki, Dongchan Min</author><pubDate>Sun, 30 Apr 2023 17:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00521v1</guid></item><item><title>Combining HoloLens with Instant-NeRFs: Advanced Real-Time 3D Mobile Mapping</title><link>http://arxiv.org/abs/2304.14301v2</link><description>This work represents a large step into modern ways of fast 3D reconstructionbased on RGB camera images. Utilizing a Microsoft HoloLens 2 as a multisensorplatform that includes an RGB camera and an inertial measurement unit forSLAM-based camera-pose determination, we train a Neural Radiance Field (NeRF)as a neural scene representation in real-time with the acquired data from theHoloLens. The HoloLens is connected via Wifi to a high-performance PC that isresponsible for the training and 3D reconstruction. After the data stream ends,the training is stopped and the 3D reconstruction is initiated, which extractsa point cloud of the scene. With our specialized inference algorithm, fivemillion scene points can be extracted within 1 second. In addition, the pointcloud also includes radiometry per point. Our method of 3D reconstructionoutperforms grid point sampling with NeRFs by multiple orders of magnitude andcan be regarded as a complete real-time 3D reconstruction method in a mobilemapping setup.</description><author>Dennis Haitz, Boris Jutzi, Markus Ulrich, Miriam Jaeger, Patrick Huebner</author><pubDate>Wed, 03 May 2023 12:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14301v2</guid></item><item><title>Towards Accurate Human Motion Prediction via Iterative Refinement</title><link>http://arxiv.org/abs/2305.04443v1</link><description>Human motion prediction aims to forecast an upcoming pose sequence given apast human motion trajectory. To address the problem, in this work we proposeFreqMRN, a human motion prediction framework that takes into account both thekinematic structure of the human body and the temporal smoothness nature ofmotion. Specifically, FreqMRN first generates a fixed-size motion historysummary using a motion attention module, which helps avoid inaccurate motionpredictions due to excessively long motion inputs. Then, supervised by theproposed spatial-temporal-aware, velocity-aware and global-smoothness-awarelosses, FreqMRN iteratively refines the predicted motion though the proposedmotion refinement module, which converts motion representations back and forthbetween pose space and frequency space. We evaluate FreqMRN on several standardbenchmark datasets, including Human3.6M, AMASS and 3DPW. Experimental resultsdemonstrate that FreqMRN outperforms previous methods by large margins for bothshort-term and long-term predictions, while demonstrating superior robustness.</description><author>Jiarui Sun, Girish Chowdhary</author><pubDate>Mon, 08 May 2023 04:43:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04443v1</guid></item><item><title>Object Semantics Give Us the Depth We Need: Multi-task Approach to Aerial Depth Completion</title><link>http://arxiv.org/abs/2304.12542v1</link><description>Depth completion and object detection are two crucial tasks often used foraerial 3D mapping, path planning, and collision avoidance of Uncrewed AerialVehicles (UAVs). Common solutions include using measurements from a LiDARsensor; however, the generated point cloud is often sparse and irregular andlimits the system's capabilities in 3D rendering and safety-criticaldecision-making. To mitigate this challenge, information from other sensors onthe UAV (viz., a camera used for object detection) is utilized to help thedepth completion process generate denser 3D models. Performing both aerialdepth completion and object detection tasks while fusing the data from the twosensors poses a challenge to resource efficiency. We address this challenge byproposing a novel approach to jointly execute the two tasks in a single pass.The proposed method is based on an encoder-focused multi-task learning modelthat exposes the two tasks to jointly learned features. We demonstrate howsemantic expectations of the objects in the scene learned by the objectdetection pathway can boost the performance of the depth completion pathwaywhile placing the missing depth values. Experimental results show that theproposed multi-task network outperforms its single-task counterpart,particularly when exposed to defective inputs.</description><author>Sara Hatami Gazani, Fardad Dadboud, Miodrag Bolic, Iraj Mantegh, Homayoun Najjaran</author><pubDate>Tue, 25 Apr 2023 04:21:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12542v1</guid></item><item><title>HoloDiffusion: Training a 3D Diffusion Model using 2D Images</title><link>http://arxiv.org/abs/2303.16509v2</link><description>Diffusion models have emerged as the best approach for generative modeling of2D images. Part of their success is due to the possibility of training them onmillions if not billions of images with a stable learning objective. However,extending these models to 3D remains difficult for two reasons. First, findinga large quantity of 3D training data is much more complex than for 2D images.Second, while it is conceptually trivial to extend the models to operate on 3Drather than 2D grids, the associated cubic growth in memory and computecomplexity makes this infeasible. We address the first challenge by introducinga new diffusion setup that can be trained, end-to-end, with only posed 2Dimages for supervision; and the second challenge by proposing an imageformation model that decouples model memory from spatial memory. We evaluateour method on real-world data, using the CO3D dataset which has not been usedto train 3D generative models before. We show that our diffusion models arescalable, train robustly, and are competitive in terms of sample quality andfidelity to existing approaches for 3D generative modeling.</description><author>Animesh Karnewar, Andrea Vedaldi, David Novotny, Niloy Mitra</author><pubDate>Sun, 21 May 2023 23:38:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16509v2</guid></item><item><title>Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</title><link>http://arxiv.org/abs/2305.16321v1</link><description>Decomposing an object's appearance into representations of its materials andthe surrounding illumination is difficult, even when the object's 3D shape isknown beforehand. This problem is ill-conditioned because diffuse materialsseverely blur incoming light, and is ill-posed because diffuse materials underhigh-frequency lighting can be indistinguishable from shiny materials underlow-frequency lighting. We show that it is possible to recover precisematerials and illumination -- even from diffuse objects -- by exploitingunintended shadows, like the ones cast onto an object by the photographer whomoves around it. These shadows are a nuisance in most previous inverserendering pipelines, but here we exploit them as signals that improveconditioning and help resolve material-lighting ambiguities. We present amethod based on differentiable Monte Carlo ray tracing that uses images of anobject to jointly recover its spatially-varying materials, the surroundingillumination environment, and the shapes of the unseen light occluders whoinadvertently cast shadows upon it.</description><author>Dor Verbin, Ben Mildenhall, Peter Hedman, Jonathan T. Barron, Todd Zickler, Pratul P. Srinivasan</author><pubDate>Thu, 25 May 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16321v1</guid></item><item><title>Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</title><link>http://arxiv.org/abs/2305.10973v1</link><description>Synthesizing visual content that meets users' needs often requires flexibleand precise controllability of the pose, shape, expression, and layout of thegenerated objects. Existing approaches gain controllability of generativeadversarial networks (GANs) via manually annotated training data or a prior 3Dmodel, which often lack flexibility, precision, and generality. In this work,we study a powerful yet much less explored way of controlling GANs, that is, to"drag" any points of the image to precisely reach target points in auser-interactive manner, as shown in Fig.1. To achieve this, we proposeDragGAN, which consists of two main components: 1) a feature-based motionsupervision that drives the handle point to move towards the target position,and 2) a new point tracking approach that leverages the discriminativegenerator features to keep localizing the position of the handle points.Through DragGAN, anyone can deform an image with precise control over wherepixels go, thus manipulating the pose, shape, expression, and layout of diversecategories such as animals, cars, humans, landscapes, etc. As thesemanipulations are performed on the learned generative image manifold of a GAN,they tend to produce realistic outputs even for challenging scenarios such ashallucinating occluded content and deforming shapes that consistently followthe object's rigidity. Both qualitative and quantitative comparisonsdemonstrate the advantage of DragGAN over prior approaches in the tasks ofimage manipulation and point tracking. We also showcase the manipulation ofreal images through GAN inversion.</description><author>Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, Christian Theobalt</author><pubDate>Thu, 18 May 2023 14:41:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10973v1</guid></item><item><title>Accelerated Coordinate Encoding: Learning to Relocalize in Minutes using RGB and Poses</title><link>http://arxiv.org/abs/2305.14059v1</link><description>Learning-based visual relocalizers exhibit leading pose accuracy, but requirehours or days of training. Since training needs to happen on each new sceneagain, long training times make learning-based relocalization impractical formost applications, despite its promise of high accuracy. In this paper we showhow such a system can actually achieve the same accuracy in less than 5minutes. We start from the obvious: a relocalization network can be split in ascene-agnostic feature backbone, and a scene-specific prediction head. Lessobvious: using an MLP prediction head allows us to optimize across thousands ofview points simultaneously in each single training iteration. This leads tostable and extremely fast convergence. Furthermore, we substitute effective butslow end-to-end training using a robust pose solver with a curriculum over areprojection loss. Our approach does not require privileged knowledge, such adepth maps or a 3D model, for speedy training. Overall, our approach is up to300x faster in mapping than state-of-the-art scene coordinate regression, whilekeeping accuracy on par.</description><author>Eric Brachmann, Tommaso Cavallari, Victor Adrian Prisacariu</author><pubDate>Tue, 23 May 2023 14:38:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14059v1</guid></item><item><title>BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering</title><link>http://arxiv.org/abs/2112.05504v4</link><description>Neural radiance fields (NeRF) has achieved outstanding performance inmodeling 3D objects and controlled scenes, usually under a single scale. Inthis work, we focus on multi-scale cases where large changes in imagery areobserved at drastically different scales. This scenario vastly exists inreal-world 3D environments, such as city scenes, with views ranging fromsatellite level that captures the overview of a city, to ground level imageryshowing complex details of an architecture; and can also be commonly identifiedin landscape and delicate minecraft 3D models. The wide span of viewingpositions within these scenes yields multi-scale renderings with very differentlevels of detail, which poses great challenges to neural radiance field andbiases it towards compromised results. To address these issues, we introduceBungeeNeRF, a progressive neural radiance field that achieves level-of-detailrendering across drastically varied scales. Starting from fitting distant viewswith a shallow base block, as training progresses, new blocks are appended toaccommodate the emerging details in the increasingly closer views. The strategyprogressively activates high-frequency channels in NeRF's positional encodinginputs and successively unfolds more complex details as the training proceeds.We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scalescenes with drastically varying views on multiple data sources (city models,synthetic, and drone captured data) and its support for high-quality renderingin different levels of detail.</description><author>Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, Dahua Lin</author><pubDate>Tue, 09 May 2023 06:48:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.05504v4</guid></item><item><title>Full-Body Articulated Human-Object Interaction</title><link>http://arxiv.org/abs/2212.10621v2</link><description>Fine-grained capturing of 3D HOI boosts human activity understanding andfacilitates downstream visual tasks, including action recognition, holisticscene reconstruction, and human motion synthesis. Despite its significance,existing works mostly assume that humans interact with rigid objects using onlya few body parts, limiting their scope. In this paper, we address thechallenging problem of f-AHOI, wherein the whole human bodies interact witharticulated objects, whose parts are connected by movable joints. We presentCHAIRS, a large-scale motion-captured f-AHOI dataset, consisting of 16.2 hoursof versatile interactions between 46 participants and 81 articulated and rigidsittable objects. CHAIRS provides 3D meshes of both humans and articulatedobjects during the entire interactive process, as well as realistic andphysically plausible full-body interactions. We show the value of CHAIRS withobject pose estimation. By learning the geometrical relationships in HOI, wedevise the very first model that leverage human pose estimation to tackle theestimation of articulated object poses and shapes during whole-bodyinteractions. Given an image and an estimated human pose, our model firstreconstructs the pose and shape of the object, then optimizes thereconstruction according to a learned interaction prior. Under both evaluationsettings (e.g., with or without the knowledge of objects'geometries/structures), our model significantly outperforms baselines. We hopeCHAIRS will promote the community towards finer-grained interactionunderstanding. We will make the data/code publicly available.</description><author>Nan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Zhiyuan zhang, Yixin Chen, He Wang, Yixin Zhu, Siyuan Huang</author><pubDate>Tue, 16 May 2023 20:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10621v2</guid></item><item><title>A Cooperative Perception System Robust to Localization Errors</title><link>http://arxiv.org/abs/2210.06289v2</link><description>Cooperative perception is challenging for safety-critical autonomous drivingapplications.The errors in the shared position and pose cause an inaccuraterelative transform estimation and disrupt the robust mapping of the Egovehicle. We propose a distributed object-level cooperative perception systemcalled OptiMatch, in which the detected 3D bounding boxes and local stateinformation are shared between the connected vehicles. To correct the noisyrelative transform, the local measurements of both connected vehicles (boundingboxes) are utilized, and an optimal transport theory-based algorithm isdeveloped to filter out those objects jointly detected by the vehicles alongwith their correspondence, constructing an associated co-visible set. Acorrection transform is estimated from the matched object pairs and furtherapplied to the noisy relative transform, followed by global fusion and dynamicmapping. Experiment results show that robust performance is achieved fordifferent levels of location and heading errors, and the proposed frameworkoutperforms the state-of-the-art benchmark fusion schemes, including early,late, and intermediate fusion, on average precision by a large margin whenlocation and/or heading errors occur.</description><author>Zhiying Song, Fuxi Wen, Hailiang Zhang, Jun Li</author><pubDate>Wed, 26 Apr 2023 01:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06289v2</guid></item><item><title>Neural Novel Actor: Learning a Generalized Animatable Neural Representation for Human Actors</title><link>http://arxiv.org/abs/2208.11905v2</link><description>We propose a new method for learning a generalized animatable neural humanrepresentation from a sparse set of multi-view imagery of multiple persons. Thelearned representation can be used to synthesize novel view images of anarbitrary person from a sparse set of cameras, and further animate them withthe user's pose control. While existing methods can either generalize to newpersons or synthesize animations with user control, none of them can achieveboth at the same time. We attribute this accomplishment to the employment of a3D proxy for a shared multi-person human model, and further the warping of thespaces of different poses to a shared canonical pose space, in which we learn aneural field and predict the person- and pose-dependent deformations, as wellas appearance with the features extracted from input images. To cope with thecomplexity of the large variations in body shapes, poses, and clothingdeformations, we design our neural human model with disentangled geometry andappearance. Furthermore, we utilize the image features both at the spatialpoint and on the surface points of the 3D proxy for predicting person- andpose-dependent properties. Experiments show that our method significantlyoutperforms the state-of-the-arts on both tasks. The video and code areavailable at https://talegqz.github.io/neural_novel_actor.</description><author>Yiming Wang, Qingzhe Gao, Libin Liu, Lingjie Liu, Christian Theobalt, Baoquan Chen</author><pubDate>Tue, 23 May 2023 07:56:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.11905v2</guid></item><item><title>Robust Pose Transfer with Dynamic Details using Neural Video Rendering</title><link>http://arxiv.org/abs/2106.14132v3</link><description>Pose transfer of human videos aims to generate a high fidelity video of atarget person imitating actions of a source person. A few studies have madegreat progress either through image translation with deep latent features orneural rendering with explicit 3D features. However, both of them rely on largeamounts of training data to generate realistic results, and the performancedegrades on more accessible internet videos due to insufficient trainingframes. In this paper, we demonstrate that the dynamic details can be preservedeven trained from short monocular videos. Overall, we propose a neural videorendering framework coupled with an image-translation-based dynamic detailsgeneration network (D2G-Net), which fully utilizes both the stability ofexplicit 3D features and the capacity of learning components. To be specific, anovel texture representation is presented to encode both the static andpose-varying appearance characteristics, which is then mapped to the imagespace and rendered as a detail-rich frame in the neural rendering stage.Moreover, we introduce a concise temporal loss in the training stage tosuppress the detail flickering that is made more visible due to high-qualitydynamic details generated by our method. Through extensive comparisons, wedemonstrate that our neural human video renderer is capable of achieving bothclearer dynamic details and more robust performance even on accessible shortvideos with only 2k - 4k frames.</description><author>Yang-tian Sun, Hao-zhi Huang, Xuan Wang, Yu-kun Lai, Wei Liu, Lin Gao</author><pubDate>Mon, 08 May 2023 15:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.14132v3</guid></item><item><title>MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with Bird's Eye View based Appearance and Motion Features</title><link>http://arxiv.org/abs/2305.07336v1</link><description>Identifying moving objects is an essential capability for autonomous systems,as it provides critical information for pose estimation, navigation, collisionavoidance and static map construction. In this paper, we present MotionBEV, afast and accurate framework for LiDAR moving object segmentation, whichsegments moving objects with appearance and motion features in bird's eye view(BEV) domain. Our approach converts 3D LiDAR scans into 2D polar BEVrepresentation to achieve real-time performance. Specifically, we learnappearance features with a simplified PointNet, and compute motion featuresthrough the height differences of consecutive frames of point clouds projectedonto vertical columns in the polar BEV coordinate system. We employ adual-branch network bridged by the Appearance-Motion Co-attention Module (AMCM)to adaptively fuse the spatio-temporal information from appearance and motionfeatures. Our approach achieves state-of-the-art performance on theSemanticKITTI-MOS benchmark, with an average inference time of 23ms on an RTX3090 GPU. Furthermore, to demonstrate the practical effectiveness of ourmethod, we provide a LiDAR-MOS dataset recorded by a solid-state LiDAR, whichfeatures non-repetitive scanning patterns and small field of view.</description><author>Bo Zhou, Jiapeng Xie, Yan Pan, Jiajie Wu, Chuanzhao Lu</author><pubDate>Fri, 12 May 2023 10:28:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07336v1</guid></item><item><title>Synthesizing Diverse Human Motions in 3D Indoor Scenes</title><link>http://arxiv.org/abs/2305.12411v1</link><description>We present a novel method for populating 3D indoor scenes with virtual humansthat can navigate the environment and interact with objects in a realisticmanner. Existing approaches rely on high-quality training sequences thatcapture a diverse range of human motions in 3D scenes. However, such motiondata is costly, difficult to obtain and can never cover the full range ofplausible human-scene interactions in complex indoor environments. To addressthese challenges, we propose a reinforcement learning-based approach to learnpolicy networks that predict latent variables of a powerful generative motionmodel that is trained on a large-scale motion capture dataset (AMASS). Fornavigating in a 3D environment, we propose a scene-aware policy training schemewith a novel collision avoidance reward function. Combined with the powerfulgenerative motion model, we can synthesize highly diverse human motionsnavigating 3D indoor scenes, meanwhile effectively avoiding obstacles. Fordetailed human-object interactions, we carefully curate interaction-awarereward functions by leveraging a marker-based body representation and thesigned distance field (SDF) representation of the 3D scene. With a number ofimportant training design schemes, our method can synthesize realistic anddiverse human-object interactions (e.g.,~sitting on a chair and then gettingup) even for out-of-distribution test scenarios with different object shapes,orientations, starting body positions, and poses. Experimental resultsdemonstrate that our approach outperforms state-of-the-art human-sceneinteraction synthesis frameworks in terms of both motion naturalness anddiversity. Video results are available on the project page:\href{https://zkf1997.github.io/DIMOS}{https://zkf1997.github.io/DIMOS}.</description><author>Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, Siyu Tang</author><pubDate>Sun, 21 May 2023 10:22:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12411v1</guid></item><item><title>Domain independent post-processing with graph U-nets: Applications to Electrical Impedance Tomographic Imaging</title><link>http://arxiv.org/abs/2305.05020v1</link><description>Reconstruction of tomographic images from boundary measurements requiresflexibility with respect to target domains. For instance, when the systemequations are modeled by partial differential equations the reconstruction isusually done on finite element (FE) meshes, allowing for flexible geometries.Thus, any processing of the obtained reconstructions should be ideally done onthe FE mesh as well. For this purpose, we extend the hugely successful U-Netarchitecture that is limited to rectangular pixel or voxel domains to anequivalent that works flexibly on FE meshes. To achieve this, the FE mesh isconverted into a graph and we formulate a graph U-Net with a new clusterpooling and unpooling on the graph that mimics the classic neighborhood basedmax-pooling. We demonstrate effectiveness and flexibility of the graph U-Netfor improving reconstructions from electrical impedance tomographic (EIT)measurements, a nonlinear and highly ill-posed inverse problem. The performanceis evaluated for simulated data and from three measurement devices withdifferent measurement geometries and instrumentations. We successfully showthat such networks can be trained with a simple two-dimensional simulatedtraining set and generalize to very different domains, including measurementsfrom a three-dimensional device and subsequent 3D reconstructions.</description><author>William Herzberg, Andreas Hauptmann, Sarah J. Hamilton</author><pubDate>Mon, 08 May 2023 20:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05020v1</guid></item><item><title>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</title><link>http://arxiv.org/abs/2305.14836v1</link><description>We introduce a novel visual question answering (VQA) task in the context ofautonomous driving, aiming to answer natural language questions based onstreet-view clues. Compared to traditional VQA tasks, VQA in autonomous drivingscenario presents more challenges. Firstly, the raw visual data aremulti-modal, including images and point clouds captured by camera and LiDAR,respectively. Secondly, the data are multi-frame due to the continuous,real-time acquisition. Thirdly, the outdoor scenes exhibit both movingforeground and static background. Existing VQA benchmarks fail to adequatelyaddress these complexities. To bridge this gap, we propose NuScenes-QA, thefirst benchmark for VQA in the autonomous driving scenario, encompassing 34Kvisual scenes and 460K question-answer pairs. Specifically, we leverageexisting 3D detection annotations to generate scene graphs and design questiontemplates manually. Subsequently, the question-answer pairs are generatedprogrammatically based on these templates. Comprehensive statistics prove thatour NuScenes-QA is a balanced large-scale benchmark with diverse questionformats. Built upon it, we develop a series of baselines that employ advanced3D detection and VQA techniques. Our extensive experiments highlight thechallenges posed by this new task. Codes and dataset are available athttps://github.com/qiantianwen/NuScenes-QA.</description><author>Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang</author><pubDate>Wed, 24 May 2023 08:40:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14836v1</guid></item><item><title>Compositional 3D Human-Object Neural Animation</title><link>http://arxiv.org/abs/2304.14070v1</link><description>Human-object interactions (HOIs) are crucial for human-centric sceneunderstanding applications such as human-centric visual generation, AR/VR, androbotics. Since existing methods mainly explore capturing HOIs, rendering HOIremains less investigated. In this paper, we address this challenge in HOIanimation from a compositional perspective, i.e., animating novel HOIsincluding novel interaction, novel human and/or novel object driven by a novelpose sequence. Specifically, we adopt neural human-object deformation to modeland render HOI dynamics based on implicit neural representations. To enable theinteraction pose transferring among different persons and objects, we thendevise a new compositional conditional neural radiance field (or CC-NeRF),which decomposes the interdependence between human and object using latentcodes to enable compositionally animation control of novel HOIs. Experimentsshow that the proposed method can generalize well to various novel HOIanimation settings. Our project page is https://zhihou7.github.io/CHONA/</description><author>Zhi Hou, Baosheng Yu, Dacheng Tao</author><pubDate>Thu, 27 Apr 2023 11:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14070v1</guid></item><item><title>NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects</title><link>http://arxiv.org/abs/2305.14345v1</link><description>Deep generative models have been recently extended to synthesizing 3D digitalhumans. However, previous approaches treat clothed humans as a single chunk ofgeometry without considering the compositionality of clothing and accessories.As a result, individual items cannot be naturally composed into novelidentities, leading to limited expressiveness and controllability of generative3D avatars. While several methods attempt to address this by leveragingsynthetic data, the interaction between humans and objects is not authentic dueto the domain gap, and manual asset creation is difficult to scale for a widevariety of objects. In this work, we present a novel framework for learning acompositional generative model of humans and objects (backpacks, coats,scarves, and more) from real-world 3D scans. Our compositional model isinteraction-aware, meaning the spatial relationship between humans and objects,and the mutual shape change by physical contact is fully incorporated. The keychallenge is that, since humans and objects are in contact, their 3D scans aremerged into a single piece. To decompose them without manual annotations, wepropose to leverage two sets of 3D scans of a single person with and withoutobjects. Our approach learns to decompose objects and naturally compose themback into a generative human model in an unsupervised manner. Despite oursimple setup requiring only the capture of a single subject with objects, ourexperiments demonstrate the strong generalization of our model by enabling thenatural composition of objects to diverse identities in various poses and thecomposition of multiple objects, which is unseen in training data.</description><author>Taeksoo Kim, Shunsuke Saito, Hanbyul Joo</author><pubDate>Tue, 23 May 2023 18:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14345v1</guid></item><item><title>A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training</title><link>http://arxiv.org/abs/2302.06019v2</link><description>Real-world robotics applications demand object pose estimation methods thatwork reliably across a variety of scenarios. Modern learning-based approachesrequire large labeled datasets and tend to perform poorly outside the trainingdomain. Our first contribution is to develop a robust corrector module thatcorrects pose estimates using depth information, thus enabling existing methodsto better generalize to new test domains; the corrector operates on semantickeypoints (but is also applicable to other pose estimators) and is fullydifferentiable. Our second contribution is an ensemble self-training approachthat simultaneously trains multiple pose estimators in a self-supervisedmanner. Our ensemble self-training architecture uses the robust corrector torefine the output of each pose estimator; then, it evaluates the quality of theoutputs using observable correctness certificates; finally, it uses theobservably correct outputs for further training, without requiring externalsupervision. As an additional contribution, we propose small improvements to aregression-based keypoint detection architecture, to enhance its robustness tooutliers; these improvements include a robust pooling scheme and a robustcentroid computation. Experiments on the YCBV and TLESS datasets show theproposed ensemble self-training outperforms fully supervised baselines whilenot requiring 3D annotations on real data.</description><author>Jingnan Shi, Rajat Talak, Dominic Maggio, Luca Carlone</author><pubDate>Thu, 11 May 2023 19:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06019v2</guid></item><item><title>Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution</title><link>http://arxiv.org/abs/2305.03216v1</link><description>We present a neural network-based simulation super-resolution framework thatcan efficiently and realistically enhance a facial performance produced by alow-cost, realtime physics-based simulation to a level of detail that closelyapproximates that of a reference-quality off-line simulator with much higherresolution (26x element count in our examples) and accurate physical modeling.Our approach is rooted in our ability to construct - via simulation - atraining set of paired frames, from the low- and high-resolution simulatorsrespectively, that are in semantic correspondence with each other. We use faceanimation as an exemplar of such a simulation domain, where creating thissemantic congruence is achieved by simply dialing in the same muscle actuationcontrols and skeletal pose in the two simulators. Our proposed neural networksuper-resolution framework generalizes from this training set to unseenexpressions, compensates for modeling discrepancies between the two simulationsdue to limited resolution or cost-cutting approximations in the real-timevariant, and does not require any semantic descriptors or parameters to beprovided as input, other than the result of the real-time simulation. Weevaluate the efficacy of our pipeline on a variety of expressive performancesand provide comparisons and ablation experiments for plausible variations andalternatives to our proposed scheme.</description><author>Hyojoon Park, Sangeetha Grama Srinivasan, Matthew Cong, Doyub Kim, Byungsoo Kim, Jonathan Swartz, Ken Museth, Eftychios Sifakis</author><pubDate>Fri, 05 May 2023 01:09:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03216v1</guid></item><item><title>VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation</title><link>http://arxiv.org/abs/2305.10874v1</link><description>We present VideoFactory, an innovative framework for generating high-qualityopen-domain videos. VideoFactory excels in producing high-definition(1376x768), widescreen (16:9) videos without watermarks, creating an engaginguser experience. Generating videos guided by text instructions posessignificant challenges, such as modeling the complex relationship between spaceand time, and the lack of large-scale text-video paired data. Previousapproaches extend pretrained text-to-image generation models by adding temporal1D convolution/attention modules for video generation. However, theseapproaches overlook the importance of jointly modeling space and time,inevitably leading to temporal distortions and misalignment between texts andvideos. In this paper, we propose a novel approach that strengthens theinteraction between spatial and temporal perceptions. In particular, we utilizea swapped cross-attention mechanism in 3D windows that alternates the "query"role between spatial and temporal blocks, enabling mutual reinforcement foreach other. To fully unlock model capabilities for high-quality videogeneration, we curate a large-scale video dataset called HD-VG-130M. Thisdataset comprises 130 million text-video pairs from the open-domain, ensuringhigh-definition, widescreen and watermark-free characters. Objective metricsand user studies demonstrate the superiority of our approach in terms ofper-frame quality, temporal correlation, and text-video alignment, with clearmargins.</description><author>Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, Jiaying Liu</author><pubDate>Thu, 18 May 2023 12:06:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10874v1</guid></item><item><title>A Multi-modal Garden Dataset and Hybrid 3D Dense Reconstruction Framework Based on Panoramic Stereo Images for a Trimming Robot</title><link>http://arxiv.org/abs/2305.06278v1</link><description>Recovering an outdoor environment's surface mesh is vital for an agriculturalrobot during task planning and remote visualization. Our proposed solution isbased on a newly-designed panoramic stereo camera along with a hybrid novelsoftware framework that consists of three fusion modules. The panoramic stereocamera with a pentagon shape consists of 5 stereo vision camera pairs to streamsynchronized panoramic stereo images for the following three fusion modules. Inthe disparity fusion module, rectified stereo images produce the initialdisparity maps using multiple stereo vision algorithms. Then, these initialdisparity maps, along with the intensity images, are input into a disparityfusion network to produce refined disparity maps. Next, the refined disparitymaps are converted into full-view point clouds or single-view point clouds forthe pose fusion module. The pose fusion module adopts a two-stageglobal-coarse-to-local-fine strategy. In the first stage, each pair offull-view point clouds is registered by a global point cloud matching algorithmto estimate the transformation for a global pose graph's edge, whicheffectively implements loop closure. In the second stage, a local point cloudmatching algorithm is used to match single-view point clouds in differentnodes. Next, we locally refine the poses of all corresponding edges in theglobal pose graph using three proposed rules, thus constructing a refined posegraph. The refined pose graph is optimized to produce a global pose trajectoryfor volumetric fusion. In the volumetric fusion module, the global poses of allthe nodes are used to integrate the single-view point clouds into the volume toproduce the mesh of the whole garden. The proposed framework and its threefusion modules are tested on a real outdoor garden dataset to show thesuperiority of the performance.</description><author>Can Pu, Chuanyu Yang, Jinnian Pu, Radim Tylecek, Robert B. Fisher</author><pubDate>Wed, 10 May 2023 17:15:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06278v1</guid></item><item><title>Multimodal-driven Talking Face Generation via a Unified Diffusion-based Generator</title><link>http://arxiv.org/abs/2305.02594v2</link><description>Multimodal-driven talking face generation refers to animating a portrait withthe given pose, expression, and gaze transferred from the driving image andvideo, or estimated from the text and audio. However, existing methods ignorethe potential of text modal, and their generators mainly follow thesource-oriented feature rearrange paradigm coupled with unstable GANframeworks. In this work, we first represent the emotion in the text prompt,which could inherit rich semantics from the CLIP, allowing flexible andgeneralized emotion control. We further reorganize these tasks as thetarget-oriented texture transfer and adopt the Diffusion Models. Morespecifically, given a textured face as the source and the rendered faceprojected from the desired 3DMM coefficients as the target, our proposedTexture-Geometry-aware Diffusion Model decomposes the complex transfer probleminto multi-conditional denoising process, where a Texture Attention-basedmodule accurately models the correspondences between appearance and geometrycues contained in source and target conditions, and incorporate extra implicitinformation for high-fidelity talking face generation. Additionally, TGDM canbe gracefully tailored for face swapping. We derive a novel paradigm free ofunstable seesaw-style optimization, resulting in simple, stable, and effectivetraining and inference schemes. Extensive experiments demonstrate thesuperiority of our method.</description><author>Chao Xu, Shaoting Zhu, Junwei Zhu, Tianxin Huang, Jiangning Zhang, Ying Tai, Yong Liu</author><pubDate>Tue, 09 May 2023 13:01:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02594v2</guid></item><item><title>Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model</title><link>http://arxiv.org/abs/2305.02594v1</link><description>Multimodal-driven talking face generation refers to animating a portrait withthe given pose, expression, and gaze transferred from the driving image andvideo, or estimated from the text and audio. However, existing methods ignorethe potential of text modal, and their generators mainly follow thesource-oriented feature rearrange paradigm coupled with unstable GANframeworks. In this work, we first represent the emotion in the text prompt,which could inherit rich semantics from the CLIP, allowing flexible andgeneralized emotion control. We further reorganize these tasks as thetarget-oriented texture transfer and adopt the Diffusion Models. Morespecifically, given a textured face as the source and the rendered faceprojected from the desired 3DMM coefficients as the target, our proposedTexture-Geometry-aware Diffusion Model decomposes the complex transfer probleminto multi-conditional denoising process, where a Texture Attention-basedmodule accurately models the correspondences between appearance and geometrycues contained in source and target conditions, and incorporate extra implicitinformation for high-fidelity talking face generation. Additionally, TGDM canbe gracefully tailored for face swapping. We derive a novel paradigm free ofunstable seesaw-style optimization, resulting in simple, stable, and effectivetraining and inference schemes. Extensive experiments demonstrate thesuperiority of our method.</description><author>Chao Xu, Shaoting Zhu, Junwei Zhu, Tianxin Huang, Jiangning Zhang, Ying Tai, Yong Liu</author><pubDate>Thu, 04 May 2023 08:01:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02594v1</guid></item><item><title>Efficient and Deterministic Search Strategy Based on Residual Projections for Point Cloud Registration</title><link>http://arxiv.org/abs/2305.11716v1</link><description>Estimating the rigid transformation between two LiDAR scans through putative3D correspondences is a typical point cloud registration paradigm. Current 3Dfeature matching approaches commonly lead to numerous outlier correspondences,making outlier-robust registration techniques indispensable. Many recentstudies have adopted the branch and bound (BnB) optimization framework to solvethe correspondence-based point cloud registration problem globally anddeterministically. Nonetheless, BnB-based methods are time-consuming to searchthe entire 6-dimensional parameter space, since their computational complexityis exponential to the dimension of the solution domain. In order to enhancealgorithm efficiency, existing works attempt to decouple the 6 degrees offreedom (DOF) original problem into two 3-DOF sub-problems, thereby reducingthe dimension of the parameter space. In contrast, our proposed approachintroduces a novel pose decoupling strategy based on residual projections,effectively decomposing the raw problem into three 2-DOF rotation searchsub-problems. Subsequently, we employ a novel BnB-based search method to solvethese sub-problems, achieving efficient and deterministic registration.Furthermore, our method can be adapted to address the challenging problem ofsimultaneous pose and correspondence registration (SPCR). Through extensiveexperiments conducted on synthetic and real-world datasets, we demonstrate thatour proposed method outperforms state-of-the-art methods in terms ofefficiency, while simultaneously ensuring robustness.</description><author>Xinyi Li, Yinlong Liu, Hu Cao, Xueli Liu, Feihu Zhang, Alois Knoll</author><pubDate>Fri, 19 May 2023 15:52:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11716v1</guid></item><item><title>Navya3DSeg -- Navya 3D Semantic Segmentation Dataset &amp; split generation for autonomous vehicles</title><link>http://arxiv.org/abs/2302.08292v2</link><description>Autonomous driving (AD) perception today relies heavily on deep learningbased architectures requiring large scale annotated datasets with theirassociated costs for curation and annotation. The 3D semantic data are usefulfor core perception tasks such as obstacle detection and ego-vehiclelocalization. We propose a new dataset, Navya 3D Segmentation (Navya3DSeg),with a diverse label space corresponding to a large scale production gradeoperational domain, including rural, urban, industrial sites and universitiesfrom 13 countries. It contains 23 labeled sequences and 25 supplementarysequences without labels, designed to explore self-supervised andsemi-supervised semantic segmentation benchmarks on point clouds. We alsopropose a novel method for sequential dataset split generation based oniterative multi-label stratification, and demonstrated to achieve a +1.2% mIoUimprovement over the original split proposed by SemanticKITTI dataset. Acomplete benchmark for semantic segmentation task was performed, with state ofthe art methods. Finally, we demonstrate an Active Learning (AL) based datasetdistillation framework. We introduce a novel heuristic-free sampling methodcalled ego-pose distance based sampling in the context of AL. A detailedpresentation on the dataset is available herehttps://www.youtube.com/watch?v=5m6ALIs-s20.</description><author>Alexandre Almin, Léo Lemarié, Anh Duong, B Ravi Kiran</author><pubDate>Mon, 22 May 2023 15:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08292v2</guid></item></channel></rss>