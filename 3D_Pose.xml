<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 19 Mar 2024 06:00:02 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>STRIDE: Single-video based Temporally Continuous Occlusion Robust 3D Pose Estimation</title><link>http://arxiv.org/abs/2312.16221v2</link><description>The capability to accurately estimate 3D human poses is crucial for diversefields such as action recognition, gait recognition, and virtual/augmentedreality. However, a persistent and significant challenge within this field isthe accurate prediction of human poses under conditions of severe occlusion.Traditional image-based estimators struggle with heavy occlusions due to a lackof temporal context, resulting in inconsistent predictions. While video-basedmodels benefit from processing temporal data, they encounter limitations whenfaced with prolonged occlusions that extend over multiple frames. Thischallenge arises because these models struggle to generalize beyond theirtraining datasets, and the variety of occlusions is hard to capture in thetraining data. Addressing these challenges, we propose STRIDE (Single-videobased TempoRally contInuous occlusion Robust 3D Pose Estimation), a novelTest-Time Training (TTT) approach to fit a human motion prior for each video.This approach specifically handles occlusions that were not encountered duringthe model's training. By employing STRIDE, we can refine a sequence of noisyinitial pose estimates into accurate, temporally coherent poses during testtime, effectively overcoming the limitations of prior methods. Our frameworkdemonstrates flexibility by being model-agnostic, allowing us to use anyoff-the-shelf 3D pose estimation method for improving robustness and temporalconsistency. We validate STRIDE's efficacy through comprehensive experiments onchallenging datasets like Occluded Human3.6M, Human3.6M, and OCMotion, where itnot only outperforms existing single-image and video-based pose estimationmodels but also showcases superior handling of substantial occlusions,achieving fast, robust, accurate, and temporally consistent 3D pose estimates.</description><author>Rohit Lal, Saketh Bachu, Yash Garg, Arindam Dutta, Calvin-Khang Ta, Dripta S. Raychaudhuri, Hannah Dela Cruz, M. Salman Asif, Amit K. Roy-Chowdhury</author><pubDate>Thu, 14 Mar 2024 04:36:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16221v2</guid></item><item><title>Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting</title><link>http://arxiv.org/abs/2402.18330v1</link><description>We present EgoTAP, a heatmap-to-3D pose lifting method for highly accuratestereo egocentric 3D pose estimation. Severe self-occlusion and out-of-viewlimbs in egocentric camera views make accurate pose estimation a challengingproblem. To address the challenge, prior methods employ jointheatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3Dpose conversion still remains an inaccurate process. We propose a novelheatmap-to-3D lifting method composed of the Grid ViT Encoder and thePropagation Network. The Grid ViT Encoder summarizes joint heatmaps intoeffective feature embedding using self-attention. Then, the Propagation Networkestimates the 3D pose by utilizing skeletal information to better estimate theposition of obscure joints. Our method significantly outperforms the previousstate-of-the-art qualitatively and quantitatively demonstrated by a 23.9\%reduction of error in an MPJPE metric. Our source code is available in GitHub.</description><author>Taeho Kang, Youngki Lee</author><pubDate>Wed, 28 Feb 2024 13:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18330v1</guid></item><item><title>Unsupervised Multi-Person 3D Human Pose Estimation From 2D Poses Alone</title><link>http://arxiv.org/abs/2309.14865v3</link><description>Current unsupervised 2D-3D human pose estimation (HPE) methods do not work inmulti-person scenarios due to perspective ambiguity in monocular images.Therefore, we present one of the first studies investigating the feasibility ofunsupervised multi-person 2D-3D HPE from just 2D poses alone, focusing onreconstructing human interactions. To address the issue of perspectiveambiguity, we expand upon prior work by predicting the cameras' elevation anglerelative to the subjects' pelvis. This allows us to rotate the predicted posesto be level with the ground plane, while obtaining an estimate for the verticaloffset in 3D between individuals. Our method involves independently liftingeach subject's 2D pose to 3D, before combining them in a shared 3D coordinatesystem. The poses are then rotated and offset by the predicted elevation anglebefore being scaled. This by itself enables us to retrieve an accurate 3Dreconstruction of their poses. We present our results on the CHI3D dataset,introducing its use for unsupervised 2D-3D pose estimation with three newquantitative metrics, and establishing a benchmark for future research.</description><author>Peter Hardy, Hansung Kim</author><pubDate>Tue, 12 Mar 2024 18:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14865v3</guid></item><item><title>3D Human Pose Estimation Based on 2D-3D Consistency with Synchronized Adversarial Training</title><link>http://arxiv.org/abs/2106.04274v4</link><description>3D human pose estimation from a single image is still a challenging problemdespite the large amount of work that has been performed in this field.Generally, most methods directly use neural networks and ignore certainconstraints (e.g., reprojection constraints, joint angle, and bone lengthconstraints). While a few methods consider these constraints but train thenetwork separately, they cannot effectively solve the depth ambiguity problem.In this paper, we propose a GAN-based model for 3D human pose estimation, inwhich a reprojection network is employed to learn the mapping of thedistribution from 3D poses to 2D poses, and a discriminator is employed for2D-3D consistency discrimination. We adopt a novel strategy to synchronouslytrain the generator, the reprojection network and the discriminator.Furthermore, inspired by the typical kinematic chain space (KCS) matrix, weintroduce a weighted KCS matrix and take it as one of the discriminator'sinputs to impose joint angle and bone length constraints. The experimentalresults on Human3.6M show that our method significantly outperformsstate-of-the-art methods in most cases.</description><author>Yicheng Deng, Cheng Sun, Yongqi Sun, Jiahui Zhu</author><pubDate>Tue, 05 Mar 2024 10:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.04274v4</guid></item><item><title>Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser</title><link>http://arxiv.org/abs/2403.04444v1</link><description>Recently, diffusion-based methods for monocular 3D human pose estimation haveachieved state-of-the-art (SOTA) performance by directly regressing the 3Djoint coordinates from the 2D pose sequence. Although some methods decomposethe task into bone length and bone direction prediction based on the humananatomical skeleton to explicitly incorporate more human body priorconstraints, the performance of these methods is significantly lower than thatof the SOTA diffusion-based methods. This can be attributed to the treestructure of the human skeleton. Direct application of the disentangled methodcould amplify the accumulation of hierarchical errors, propagating through eachhierarchy. Meanwhile, the hierarchical information has not been fully exploredby the previous methods. To address these problems, a DisentangledDiffusion-based 3D Human Pose Estimation method with Hierarchical Spatial andTemporal Denoiser is proposed, termed DDHPose. In our approach: (1) Wedisentangle the 3D pose and diffuse the bone length and bone direction duringthe forward process of the diffusion model to effectively model the human poseprior. A disentanglement loss is proposed to supervise diffusion modellearning. (2) For the reverse process, we propose Hierarchical Spatial andTemporal Denoiser (HSTDenoiser) to improve the hierarchical modeling of eachjoint. Our HSTDenoiser comprises two components: the Hierarchical-RelatedSpatial Transformer (HRST) and the Hierarchical-Related Temporal Transformer(HRTT). HRST exploits joint spatial information and the influence of the parentjoint on each joint for spatial modeling, while HRTT utilizes information fromboth the joint and its hierarchical adjacent joints to explore the hierarchicaltemporal correlations among joints.</description><author>Qingyuan Cai, Xuecai Hu, Saihui Hou, Li Yao, Yongzhen Huang</author><pubDate>Thu, 07 Mar 2024 12:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04444v1</guid></item><item><title>ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image</title><link>http://arxiv.org/abs/2403.09871v1</link><description>In this work, we present ThermoHands, a new benchmark for thermal image-basedegocentric 3D hand pose estimation, aimed at overcoming challenges like varyinglighting and obstructions (e.g., handwear). The benchmark includes a diversedataset from 28 subjects performing hand-object and hand-virtual interactions,accurately annotated with 3D hand poses through an automated process. Weintroduce a bespoken baseline method, TheFormer, utilizing dual transformermodules for effective egocentric 3D hand pose estimation in thermal imagery.Our experimental results highlight TheFormer's leading performance and affirmthermal imaging's effectiveness in enabling robust 3D hand pose estimation inadverse conditions.</description><author>Fangqiang Ding, Yunzhou Zhu, Xiangyu Wen, Chris Xiaoxuan Lu</author><pubDate>Thu, 14 Mar 2024 22:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09871v1</guid></item><item><title>Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey</title><link>http://arxiv.org/abs/2402.18844v1</link><description>3D human pose estimation and mesh recovery have attracted widespread researchinterest in many areas, such as computer vision, autonomous driving, androbotics. Deep learning on 3D human pose estimation and mesh recovery hasrecently thrived, with numerous methods proposed to address different problemsin this area. In this paper, to stimulate future research, we present acomprehensive review of recent progress over the past five years in deeplearning methods for this area by delving into over 200 references. To the bestof our knowledge, this survey is arguably the first to comprehensively coverdeep learning methods for 3D human pose estimation, including bothsingle-person and multi-person approaches, as well as human mesh recovery,encompassing methods based on explicit models and implicit representations. Wealso present comparative results on several publicly available datasets,together with insightful observations and inspiring future research directions.A regularly updated project page can be found athttps://github.com/liuyangme/SOTA-3DHPE-HMR.</description><author>Yang Liu, Changzhen Qiu, Zhiyong Zhang</author><pubDate>Thu, 29 Feb 2024 04:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18844v1</guid></item><item><title>Occlusion Resilient 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2402.11036v1</link><description>Occlusions remain one of the key challenges in 3D body pose estimation fromsingle-camera video sequences. Temporal consistency has been extensively usedto mitigate their impact but the existing algorithms in the literature do notexplicitly model them. Here, we apply this by representing the deforming body as a spatio-temporalgraph. We then introduce a refinement network that performs graph convolutionsover this graph to output 3D poses. To ensure robustness to occlusions, wetrain this network with a set of binary masks that we use to disable some ofthe edges as in drop-out techniques. In effect, we simulate the fact that some joints can be hidden for periods oftime and train the network to be immune to that. We demonstrate theeffectiveness of this approach compared to state-of-the-art techniques thatinfer poses from single-camera sequences.</description><author>Soumava Kumar Roy, Ilia Badanin, Sina Honari, Pascal Fua</author><pubDate>Fri, 16 Feb 2024 19:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11036v1</guid></item><item><title>Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting</title><link>http://arxiv.org/abs/2403.09437v1</link><description>Current human pose estimation systems focus on retrieving an accurate 3Dglobal estimate of a single person. Therefore, this paper presents one of thefirst 3D multi-person human pose estimation systems that is able to work inreal-time and is also able to handle basic forms of occlusion. First, we adjustan off-the-shelf 2D detector and an unsupervised 2D-3D lifting model for usewith a 360$^\circ$ panoramic camera and mmWave radar sensors. We then introduceseveral contributions, including camera and radar calibrations, and theimproved matching of people within the image and radar space. The systemaddresses both the depth and scale ambiguity problems by employing alightweight 2D-3D pose lifting algorithm that is able to work in real-timewhile exhibiting accurate performance in both indoor and outdoor environmentswhich offers both an affordable and scalable solution. Notably, our system'stime complexity remains nearly constant irrespective of the number of detectedindividuals, achieving a frame rate of approximately 7-8 fps on a laptop with acommercial-grade GPU.</description><author>Pawel Knap, Peter Hardy, Alberto Tamajo, Hwasup Lim, Hansung Kim</author><pubDate>Thu, 14 Mar 2024 15:30:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09437v1</guid></item><item><title>On the Utility of 3D Hand Poses for Action Recognition</title><link>http://arxiv.org/abs/2403.09805v1</link><description>3D hand poses are an under-explored modality for action recognition. Posesare compact yet informative and can greatly benefit applications with limitedcompute budgets. However, poses alone offer an incomplete understanding ofactions, as they cannot fully capture objects and environments with whichhumans interact. To efficiently model hand-object interactions, we proposeHandFormer, a novel multimodal transformer. HandFormer combines 3D hand posesat a high temporal resolution for fine-grained motion modeling with sparselysampled RGB frames for encoding scene semantics. Observing the uniquecharacteristics of hand poses, we temporally factorize hand modeling andrepresent each joint by its short-term trajectories. This factorized poserepresentation combined with sparse RGB samples is remarkably efficient andachieves high accuracy. Unimodal HandFormer with only hand poses outperformsexisting skeleton-based methods at 5x fewer FLOPs. With RGB, we achieve newstate-of-the-art performance on Assembly101 and H2O with significantimprovements in egocentric action recognition.</description><author>Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</author><pubDate>Thu, 14 Mar 2024 19:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09805v1</guid></item><item><title>Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2403.04381v1</link><description>The pursuit of accurate 3D hand pose estimation stands as a keystone forunderstanding human activity in the realm of egocentric vision. The majority ofexisting estimation methods still rely on single-view images as input, leadingto potential limitations, e.g., limited field-of-view and ambiguity in depth.To address these problems, adding another camera to better capture the shape ofhands is a practical direction. However, existing multi-view hand poseestimation methods suffer from two main drawbacks: 1) Requiring multi-viewannotations for training, which are expensive. 2) During testing, the modelbecomes inapplicable if camera parameters/layout are not the same as those usedin training. In this paper, we propose a novel Single-to-Dual-view adaptation(S2DHand) solution that adapts a pre-trained single-view estimator to dualviews. Compared with existing multi-view training methods, 1) our adaptationprocess is unsupervised, eliminating the need for multi-view annotation. 2)Moreover, our method can handle arbitrary dual-view pairs with unknown cameraparameters, making the model applicable to diverse camera settings.Specifically, S2DHand is built on certain stereo constraints, includingpair-wise cross-view consensus and invariance of transformation between bothviews. These two stereo constraints are used in a complementary manner togenerate pseudo-labels, allowing reliable adaptation. Evaluation results revealthat S2DHand achieves significant improvements on arbitrary camera pairs underboth in-dataset and cross-dataset settings, and outperforms existing adaptationmethods with leading performance. Project page:https://github.com/MickeyLLG/S2DHand.</description><author>Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato</author><pubDate>Thu, 07 Mar 2024 10:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04381v1</guid></item><item><title>NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images</title><link>http://arxiv.org/abs/2402.18196v1</link><description>Human pose estimation (HPE) in the top-view using fisheye cameras presents apromising and innovative application domain. However, the availability ofdatasets capturing this viewpoint is extremely limited, especially those withhigh-quality 2D and 3D keypoint annotations. Addressing this gap, we leveragethe capabilities of Neural Radiance Fields (NeRF) technique to establish acomprehensive pipeline for generating human pose datasets from existing 2D and3D datasets, specifically tailored for the top-view fisheye perspective.Through this pipeline, we create a novel dataset NToP570K (NeRF-poweredTop-view human Pose dataset for fisheye cameras with over 570 thousand images),and conduct an extensive evaluation of its efficacy in enhancing neuralnetworks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-Bmodel achieves an improvement in AP of 33.3 % on our validation set for 2D HPEafter finetuning on our training set. A similarly finetuned HybrIK-Transformermodel gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.</description><author>Jingrui Yu, Dipankar Nandi, Roman Seidel, Gangolf Hirtz</author><pubDate>Wed, 28 Feb 2024 09:36:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18196v1</guid></item><item><title>GGRt: Towards Generalizable 3D Gaussians without Pose Priors in Real-Time</title><link>http://arxiv.org/abs/2403.10147v1</link><description>This paper presents GGRt, a novel approach to generalizable novel viewsynthesis that alleviates the need for real camera poses, complexity inprocessing high-resolution images, and lengthy optimization processes, thusfacilitating stronger applicability of 3D Gaussian Splatting (3D-GS) inreal-world scenarios. Specifically, we design a novel joint learning frameworkthat consists of an Iterative Pose Optimization Network (IPO-Net) and aGeneralizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism,the proposed framework can inherently estimate robust relative pose informationfrom the image observations and thus primarily alleviate the requirement ofreal camera poses. Moreover, we implement a deferred back-propagation mechanismthat enables high-resolution training and inference, overcoming the resolutionconstraints of previous methods. To enhance the speed and efficiency, wefurther introduce a progressive Gaussian cache module that dynamically adjustsduring training and inference. As the first pose-free generalizable 3D-GSframework, GGRt achieves inference at $\ge$ 5 FPS and real-time rendering at$\ge$ 100 FPS. Through extensive experimentation, we demonstrate that ourmethod outperforms existing NeRF-based pose-free techniques in terms ofinference speed and effectiveness. It can also approach the real pose-based3D-GS methods. Our contributions provide a significant leap forward for theintegration of computer vision and computer graphics into practicalapplications, offering state-of-the-art results on LLFF, KITTI, and Waymo Opendatasets and enabling real-time rendering for immersive experiences.</description><author>Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han</author><pubDate>Fri, 15 Mar 2024 10:47:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10147v1</guid></item><item><title>Generating Images with 3D Annotations Using Diffusion Models</title><link>http://arxiv.org/abs/2306.08103v3</link><description>Diffusion models have emerged as a powerful generative method, capable ofproducing stunning photo-realistic images from natural language descriptions.However, these models lack explicit control over the 3D structure in thegenerated images. Consequently, this hinders our ability to obtain detailed 3Dannotations for the generated images or to craft instances with specific posesand distances. In this paper, we propose 3D Diffusion Style Transfer (3D-DST),which incorporates 3D geometry control into diffusion models. Our methodexploits ControlNet, which extends diffusion models by using visual prompts inaddition to text prompts. We generate images of the 3D objects taken from 3Dshape repositories~(e.g., ShapeNet and Objaverse), render them from a varietyof poses and viewing directions, compute the edge maps of the rendered images,and use these edge maps as visual prompts to generate realistic images. Withexplicit 3D geometry control, we can easily change the 3D structures of theobjects in the generated images and obtain ground-truth 3D annotationsautomatically. This allows us to improve a wide range of vision tasks, e.g.,classification and 3D pose estimation, in both in-distribution (ID) andout-of-distribution (OOD) settings. We demonstrate the effectiveness of ourmethod through extensive experiments on ImageNet-100/200, ImageNet-R,PASCAL3D+, ObjectNet3D, and OOD-CV. The results show that our methodsignificantly outperforms existing methods, e.g., 3.8 percentage points onImageNet-100 using DeiT-B.</description><author>Wufei Ma, Qihao Liu, Jiahao Wang, Angtian Wang, Xiaoding Yuan, Yi Zhang, Zihao Xiao, Guofeng Zhang, Beijia Lu, Ruxiao Duan, Yongrui Qi, Adam Kortylewski, Yaoyao Liu, Alan Yuille</author><pubDate>Fri, 15 Mar 2024 02:16:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08103v3</guid></item><item><title>LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network</title><link>http://arxiv.org/abs/2306.12525v2</link><description>Due to the difficulty of acquiring large-scale 3D human keypoint annotation,previous methods for 3D human pose estimation (HPE) have often relied on 2Dimage features and sequential 2D annotations. Furthermore, the training ofthese networks typically assumes the prediction of a human bounding box and theaccurate alignment of 3D point clouds with 2D images, making direct applicationin real-world scenarios challenging. In this paper, we present the 1stframework for end-to-end 3D human pose estimation, named LPFormer, which usesonly LiDAR as its input along with its corresponding 3D annotations. LPFormerconsists of two stages: firstly, it identifies the human bounding box andextracts multi-level feature representations, and secondly, it utilizes atransformer-based network to predict human keypoints based on these features.Our method demonstrates that 3D HPE can be seamlessly integrated into a strongLiDAR perception network and benefit from the features extracted by thenetwork. Experimental results on the Waymo Open Dataset demonstrate thestate-of-the-art performance, and improvements even compared to previousmulti-modal solutions.</description><author>Dongqiangzi Ye, Yufei Xie, Weijia Chen, Zixiang Zhou, Lingting Ge, Hassan Foroosh</author><pubDate>Sat, 02 Mar 2024 22:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12525v2</guid></item><item><title>That's My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation</title><link>http://arxiv.org/abs/2403.04755v1</link><description>This paper is about 3D pose estimation on LiDAR scans with extremely minimalstorage requirements to enable scalable mapping and localisation. We achievethis by clustering all points of segmented scans into semantic objects andrepresenting them only with their respective centroid and semantic class. Inthis way, each LiDAR scan is reduced to a compact collection of four-numbervectors. This abstracts away important structural information from the scenes,which is crucial for traditional registration approaches. To mitigate this, weintroduce an object-matching network based on self- and cross-correlation thatcaptures geometric and semantic relationships between entities. The respectivematches allow us to recover the relative transformation between scans throughweighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus(RANSAC). We demonstrate that such representation is sufficient for metriclocalisation by registering point clouds taken under different viewpoints onthe KITTI dataset, and at different periods of time localising between KITTIand KITTI-360. We achieve accurate metric estimates comparable withstate-of-the-art methods with almost half the representation size, specifically1.33 kB on average.</description><author>Georgi Pramatarov, Matthew Gadd, Paul Newman, Daniele De Martini</author><pubDate>Thu, 07 Mar 2024 18:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04755v1</guid></item><item><title>ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living</title><link>http://arxiv.org/abs/2402.17758v1</link><description>Hand-Object Interactions (HOIs) are conditioned on spatial and temporalcontexts like surrounding objects, pre- vious actions, and future intents (forexample, grasping and handover actions vary greatly based on objects proximityand trajectory obstruction). However, existing datasets for 4D HOI (3D HOI overtime) are limited to one subject inter- acting with one object only. Thisrestricts the generalization of learning-based HOI methods trained on thosedatasets. We introduce ADL4D, a dataset of up to two subjects inter- actingwith different sets of objects performing Activities of Daily Living (ADL) likebreakfast or lunch preparation ac- tivities. The transition between multipleobjects to complete a certain task over time introduces a unique contextlacking in existing datasets. Our dataset consists of 75 sequences with a totalof 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained actionannotations. We develop an automatic system for multi-view multi-hand 3D posean- notation capable of tracking hand poses over time. We inte- grate and testit against publicly available datasets. Finally, we evaluate our dataset on thetasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).</description><author>Marsil Zakour, Partha Pratim Nath, Ludwig Lohmer, Emre Faik Gökçe, Martin Piccolrovazzi, Constantin Patsch, Yuankai Wu, Rahul Chaudhari, Eckehard Steinbach</author><pubDate>Tue, 27 Feb 2024 18:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17758v1</guid></item><item><title>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</title><link>http://arxiv.org/abs/2403.08764v1</link><description>We propose VLOGGER, a method for audio-driven human video generation from asingle input image of a person, which builds on the success of recentgenerative diffusion models. Our method consists of 1) a stochastichuman-to-3d-motion diffusion model, and 2) a novel diffusion-based architecturethat augments text-to-image models with both spatial and temporal controls.This supports the generation of high quality video of variable length, easilycontrollable through high-level representations of human faces and bodies. Incontrast to previous work, our method does not require training for eachperson, does not rely on face detection and cropping, generates the completeimage (not just the face or the lips), and considers a broad spectrum ofscenarios (e.g. visible torso or diverse subject identities) that are criticalto correctly synthesize humans who communicate. We also curate MENTOR, a newand diverse dataset with 3d pose and expression annotations, one order ofmagnitude larger than previous ones (800,000 identities) and with dynamicgestures, on which we train and ablate our main technical contributions. VLOGGER outperforms state-of-the-art methods in three public benchmarks,considering image quality, identity preservation and temporal consistency whilealso generating upper-body gestures. We analyze the performance of VLOGGER withrespect to multiple diversity metrics, showing that our architectural choicesand the use of MENTOR benefit training a fair and unbiased model at scale.Finally we show applications in video editing and personalization.</description><author>Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, Cristian Sminchisescu</author><pubDate>Wed, 13 Mar 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08764v1</guid></item><item><title>Dense Matchers for Dense Tracking</title><link>http://arxiv.org/abs/2402.11287v1</link><description>Optical flow is a useful input for various applications, including 3Dreconstruction, pose estimation, tracking, and structure-from-motion. Despiteits utility, the field of dense long-term tracking, especially over widebaselines, has not been extensively explored. This paper extends the concept ofcombining multiple optical flows over logarithmically spaced intervals asproposed by MFT. We demonstrate the compatibility of MFT with different opticalflow networks, yielding results that surpass their individual performance.Moreover, we present a simple yet effective combination of these networkswithin the MFT framework. This approach proves to be competitive with moresophisticated, non-causal methods in terms of position prediction accuracy,highlighting the potential of MFT in enhancing long-term tracking applications.</description><author>Tomáš Jelínek, Jonáš Šerých, Jiří Matas</author><pubDate>Sat, 17 Feb 2024 14:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11287v1</guid></item><item><title>CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization</title><link>http://arxiv.org/abs/2402.17214v2</link><description>In the field of digital content creation, generating high-quality 3Dcharacters from single images is challenging, especially given the complexitiesof various body poses and the issues of self-occlusion and pose ambiguity. Inthis paper, we present CharacterGen, a framework developed to efficientlygenerate 3D characters. CharacterGen introduces a streamlined generationpipeline along with an image-conditioned multi-view diffusion model. This modeleffectively calibrates input poses to a canonical form while retaining keyattributes of the input image, thereby addressing the challenges posed bydiverse poses. A transformer-based, generalizable sparse-view reconstructionmodel is the other core component of our approach, facilitating the creation ofdetailed 3D models from multi-view images. We also adopt atexture-back-projection strategy to produce high-quality texture maps.Additionally, we have curated a dataset of anime characters, rendered inmultiple poses and views, to train and evaluate our model. Our approach hasbeen thoroughly evaluated through quantitative and qualitative experiments,showing its proficiency in generating 3D characters with high-quality shapesand textures, ready for downstream applications such as rigging and animation.</description><author>Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu</author><pubDate>Wed, 28 Feb 2024 08:13:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17214v2</guid></item><item><title>MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation</title><link>http://arxiv.org/abs/2403.08019v2</link><description>We propose a single-shot approach to determining 6-DoF pose of an object withavailable 3D computer-aided design (CAD) model from a single RGB image. Ourmethod, dubbed MRC-Net, comprises two stages. The first performs poseclassification and renders the 3D object in the classified pose. The secondstage performs regression to predict fine-grained residual pose within class.Connecting the two stages is a novel multi-scale residual correlation (MRC)layer that captures high-and-low level correspondences between the input imageand rendering from first stage. MRC-Net employs a Siamese network with sharedweights between both stages to learn embeddings for input and rendered images.To mitigate ambiguity when predicting discrete pose class labels on symmetricobjects, we use soft probabilistic labels to define pose class in the firststage. We demonstrate state-of-the-art accuracy, outperforming all competingRGB-based methods on four challenging BOP benchmark datasets: T-LESS, LM-O,YCB-V, and ITODD. Our method is non-iterative and requires no complexpost-processing.</description><author>Yuelong Li, Yafei Mao, Raja Bala, Sunil Hadap</author><pubDate>Fri, 15 Mar 2024 18:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08019v2</guid></item><item><title>Lester: rotoscope animation through video object segmentation and tracking</title><link>http://arxiv.org/abs/2402.09883v1</link><description>This article introduces Lester, a novel method to automatically synthetiseretro-style 2D animations from videos. The method approaches the challengemainly as an object segmentation and tracking problem. Video frames areprocessed with the Segment Anything Model (SAM) and the resulting masks aretracked through subsequent frames with DeAOT, a method of hierarchicalpropagation for semi-supervised video object segmentation. The geometry of themasks' contours is simplified with the Douglas-Peucker algorithm. Finally,facial traits, pixelation and a basic shadow effect can be optionally added.The results show that the method exhibits an excellent temporal consistency andcan correctly process videos with different poses and appearances, dynamicshots, partial shots and diverse backgrounds. The proposed method provides amore simple and deterministic approach than diffusion models basedvideo-to-video translation pipelines, which suffer from temporal consistencyproblems and do not cope well with pixelated and schematic outputs. The methodis also much most practical than techniques based on 3D human pose estimation,which require custom handcrafted 3D models and are very limited with respect tothe type of scenes they can process.</description><author>Ruben Tous</author><pubDate>Thu, 15 Feb 2024 11:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09883v1</guid></item><item><title>3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface</title><link>http://arxiv.org/abs/2403.08768v1</link><description>This paper introduces 3DFIRES, a novel system for scene-level 3Dreconstruction from posed images. Designed to work with as few as one view,3DFIRES reconstructs the complete geometry of unseen scenes, including hiddensurfaces. With multiple view inputs, our method produces full reconstructionwithin all camera frustums. A key feature of our approach is the fusion ofmulti-view information at the feature level, enabling the production ofcoherent and comprehensive 3D reconstruction. We train our system onnon-watertight scans from large-scale real scene dataset. We show it matchesthe efficacy of single-view reconstruction methods with only one input andsurpasses existing techniques in both quantitative and qualitative measures forsparse-view 3D reconstruction.</description><author>Linyi Jin, Nilesh Kulkarni, David Fouhey</author><pubDate>Wed, 13 Mar 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08768v1</guid></item><item><title>Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach</title><link>http://arxiv.org/abs/2402.19062v1</link><description>To facilitate diagnosis on cardiac ultrasound (US), clinical practice hasestablished several standard views of the heart, which serve as referencepoints for diagnostic measurements and define viewports from which images areacquired. Automatic view recognition involves grouping those images intoclasses of standard views. Although deep learning techniques have beensuccessful in achieving this, they still struggle with fully verifying thesuitability of an image for specific measurements due to factors like thecorrect location, pose, and potential occlusions of cardiac structures. Ourapproach goes beyond view classification and incorporates a 3D meshreconstruction of the heart that enables several more downstream tasks, likesegmentation and pose estimation. In this work, we explore learning 3D heartmeshes via graph convolutions, using similar techniques to learn 3D meshes innatural images, such as human pose estimation. As the availability of fullyannotated 3D images is limited, we generate synthetic US images from 3D meshesby training an adversarial denoising diffusion model. Experiments wereconducted on synthetic and clinical cases for view recognition and structuredetection. The approach yielded good performance on synthetic images and,despite being exclusively trained on synthetic data, it already showedpotential when applied to clinical images. With this proof-of-concept, we aimto demonstrate the benefits of graphs to improve cardiac view recognition thatcan ultimately lead to better efficiency in cardiac diagnosis.</description><author>Sarina Thomas, Cristiana Tiago, Børge Solli Andreassen, Svein-Arne Aase, Jurica Sprem, Erik Steen, Anne Solberg, Guy Ben-Yosef</author><pubDate>Thu, 29 Feb 2024 11:45:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19062v1</guid></item><item><title>Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach</title><link>http://arxiv.org/abs/2402.19062v2</link><description>To facilitate diagnosis on cardiac ultrasound (US), clinical practice hasestablished several standard views of the heart, which serve as referencepoints for diagnostic measurements and define viewports from which images areacquired. Automatic view recognition involves grouping those images intoclasses of standard views. Although deep learning techniques have beensuccessful in achieving this, they still struggle with fully verifying thesuitability of an image for specific measurements due to factors like thecorrect location, pose, and potential occlusions of cardiac structures. Ourapproach goes beyond view classification and incorporates a 3D meshreconstruction of the heart that enables several more downstream tasks, likesegmentation and pose estimation. In this work, we explore learning 3D heartmeshes via graph convolutions, using similar techniques to learn 3D meshes innatural images, such as human pose estimation. As the availability of fullyannotated 3D images is limited, we generate synthetic US images from 3D meshesby training an adversarial denoising diffusion model. Experiments wereconducted on synthetic and clinical cases for view recognition and structuredetection. The approach yielded good performance on synthetic images and,despite being exclusively trained on synthetic data, it already showedpotential when applied to clinical images. With this proof-of-concept, we aimto demonstrate the benefits of graphs to improve cardiac view recognition thatcan ultimately lead to better efficiency in cardiac diagnosis.</description><author>Sarina Thomas, Cristiana Tiago, Børge Solli Andreassen, Svein Arne Aase, Jurica Šprem, Erik Steen, Anne Solberg, Guy Ben-Yosef</author><pubDate>Fri, 01 Mar 2024 08:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19062v2</guid></item><item><title>CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model</title><link>http://arxiv.org/abs/2403.05034v1</link><description>Feed-forward 3D generative models like the Large Reconstruction Model (LRM)have demonstrated exceptional generation speed. However, the transformer-basedmethods do not leverage the geometric priors of the triplane component in theirarchitecture, often leading to sub-optimal quality given the limited size of 3Ddata and slow training. In this work, we present the ConvolutionalReconstruction Model (CRM), a high-fidelity feed-forward single image-to-3Dgenerative model. Recognizing the limitations posed by sparse 3D data, wehighlight the necessity of integrating geometric priors into network design.CRM builds on the key observation that the visualization of triplane exhibitsspatial correspondence of six orthographic images. First, it generates sixorthographic view images from a single input image, then feeds these imagesinto a convolutional U-Net, leveraging its strong pixel-level alignmentcapabilities and significant bandwidth to create a high-resolution triplane.CRM further employs Flexicubes as geometric representation, facilitating directend-to-end optimization on textured meshes. Overall, our model delivers ahigh-fidelity textured mesh from an image in just 10 seconds, without anytest-time optimization.</description><author>Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, Jun Zhu</author><pubDate>Fri, 08 Mar 2024 04:25:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05034v1</guid></item><item><title>RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training</title><link>http://arxiv.org/abs/2403.09948v1</link><description>The integration of artificial intelligence (AI) with radiology has marked atransformative era in medical diagnostics. Vision foundation models have beenadopted to enhance radiologic imaging analysis. However, the distinctcomplexities of radiological imaging, including the interpretation of 2D and 3Dradiological data, pose unique challenges that existing models, trained ongeneral non-medical images, fail to address adequately. To bridge this gap andcapitalize on the diagnostic precision required in medical imaging, weintroduce RadCLIP: a pioneering cross-modal foundational model that harnessesContrastive Language-Image Pre-training (CLIP) to refine radiologic imageanalysis. RadCLIP incorporates a novel 3D slice pooling mechanism tailored forvolumetric image analysis and is trained using a comprehensive and diversedataset of radiologic image-text pairs. Our evaluations demonstrate thatRadCLIP effectively aligns radiological images with their corresponding textualannotations, and in the meantime, offers a robust vision backbone forradiologic imagery with significant promise.</description><author>Zhixiu Lu, Hailong Li, Lili He</author><pubDate>Fri, 15 Mar 2024 02:18:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09948v1</guid></item><item><title>MogaNet: Multi-order Gated Aggregation Network</title><link>http://arxiv.org/abs/2211.03295v3</link><description>By contextualizing the kernel as global as possible, Modern ConvNets haveshown great potential in computer vision tasks. However, recent progress on\textit{multi-order game-theoretic interaction} within deep neural networks(DNNs) reveals the representation bottleneck of modern ConvNets, where theexpressive interactions have not been effectively encoded with the increasedkernel size. To tackle this challenge, we propose a new family of modernConvNets, dubbed MogaNet, for discriminative visual representation learning inpure ConvNet-based models with favorable complexity-performance trade-offs.MogaNet encapsulates conceptually simple yet effective convolutions and gatedaggregation into a compact module, where discriminative features areefficiently gathered and contextualized adaptively. MogaNet exhibits greatscalability, impressive efficiency of parameters, and competitive performancecompared to state-of-the-art ViTs and ConvNets on ImageNet and variousdownstream vision benchmarks, including COCO object detection, ADE20K semanticsegmentation, 2D\&amp;3D human pose estimation, and video prediction. Notably,MogaNet hits 80.0\% and 87.8\% accuracy with 5.2M and 181M parameters onImageNet-1K, outperforming ParC-Net and ConvNeXt-L, while saving 59\% FLOPs and17M parameters, respectively. The source code is available at\url{https://github.com/Westlake-AI/MogaNet}.</description><author>Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Li</author><pubDate>Fri, 16 Feb 2024 14:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03295v3</guid></item><item><title>Score-Guided Diffusion for 3D Human Recovery</title><link>http://arxiv.org/abs/2403.09623v1</link><description>We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach forsolving inverse problems for 3D human pose and shape reconstruction. Theseinverse problems involve fitting a human body model to image observations,traditionally solved through optimization techniques. ScoreHMR mimics modelfitting approaches, but alignment with the image observation is achievedthrough score guidance in the latent space of a diffusion model. The diffusionmodel is trained to capture the conditional distribution of the human modelparameters given an input image. By guiding its denoising process with atask-specific score, ScoreHMR effectively solves inverse problems for variousapplications without the need for retraining the task-agnostic diffusion model.We evaluate our approach on three settings/applications. These are: (i)single-frame model fitting; (ii) reconstruction from multiple uncalibratedviews; (iii) reconstructing humans in video sequences. ScoreHMR consistentlyoutperforms all optimization baselines on popular benchmarks across allsettings. We make our code and models available at thehttps://statho.github.io/ScoreHMR.</description><author>Anastasis Stathopoulos, Ligong Han, Dimitris Metaxas</author><pubDate>Thu, 14 Mar 2024 18:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09623v1</guid></item><item><title>MagicDrive: Street View Generation with Diverse 3D Geometry Control</title><link>http://arxiv.org/abs/2310.02601v6</link><description>Recent advancements in diffusion models have significantly enhanced the datasynthesis with 2D control. Yet, precise 3D control in street view generation,crucial for 3D perception tasks, remains elusive. Specifically, utilizingBird's-Eye View (BEV) as the primary condition often leads to challenges ingeometry control (e.g., height), affecting the representation of object shapes,occlusion patterns, and road surface elevations, all of which are essential toperception data synthesis, especially for 3D object detection tasks. In thispaper, we introduce MagicDrive, a novel street view generation framework,offering diverse 3D geometry controls including camera poses, road maps, and 3Dbounding boxes, together with textual descriptions, achieved through tailoredencoding strategies. Besides, our design incorporates a cross-view attentionmodule, ensuring consistency across multiple camera views. With MagicDrive, weachieve high-fidelity street-view image &amp; video synthesis that captures nuanced3D geometry and various scene descriptions, enhancing tasks like BEVsegmentation and 3D object detection.</description><author>Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, Qiang Xu</author><pubDate>Fri, 01 Mar 2024 06:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02601v6</guid></item><item><title>DD-VNB: A Depth-based Dual-Loop Framework for Real-time Visually Navigated Bronchoscopy</title><link>http://arxiv.org/abs/2403.01683v2</link><description>Real-time 6 DOF localization of bronchoscopes is crucial for enhancingintervention quality. However, current vision-based technologies struggle tobalance between generalization to unseen data and computational speed. In thisstudy, we propose a Depth-based Dual-Loop framework for real-time VisuallyNavigated Bronchoscopy (DD-VNB) that can generalize across patient caseswithout the need of re-training. The DD-VNB framework integrates two keymodules: depth estimation and dual-loop localization. To address the domain gapamong patients, we propose a knowledge-embedded depth estimation network thatmaps endoscope frames to depth, ensuring generalization by eliminatingpatient-specific textures. The network embeds view synthesis knowledge into acycle adversarial architecture for scale-constrained monocular depthestimation. For real-time performance, our localization module embeds a fastego-motion estimation network into the loop of depth registration. Theego-motion inference network estimates the pose change of the bronchoscope inhigh frequency while depth registration against the pre-operative 3D modelprovides absolute pose periodically. Specifically, the relative pose changesare fed into the registration process as the initial guess to boost itsaccuracy and speed. Experiments on phantom and in-vivo data from patientsdemonstrate the effectiveness of our framework: 1) monocular depth estimationoutperforms SOTA, 2) localization achieves an accuracy of Absolute TrackingError (ATE) of 4.7 $\pm$ 3.17 mm in phantom and 6.49 $\pm$ 3.88 mm in patientdata, 3) with a frame-rate approaching video capture speed, 4) without thenecessity of case-wise network retraining. The framework's superior speed andaccuracy demonstrate its promising clinical potential for real-timebronchoscopic navigation.</description><author>Qingyao Tian, Huai Liao, Xinyan Huang, Jian Chen, Zihui Zhang, Bingyu Yang, Sebastien Ourselin, Hongbin Liu</author><pubDate>Fri, 15 Mar 2024 08:25:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01683v2</guid></item><item><title>MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction</title><link>http://arxiv.org/abs/2402.03762v3</link><description>Neural implicit representations have recently been demonstrated in manyfields including Simultaneous Localization And Mapping (SLAM). Current neuralSLAM can achieve ideal results in reconstructing bounded scenes, but thisrelies on the input of RGB-D images. Neural-based SLAM based only on RGB imagesis unable to reconstruct the scale of the scene accurately, and it also suffersfrom scale drift due to errors accumulated during tracking. To overcome theselimitations, we present MoD-SLAM, a monocular dense mapping method that allowsglobal pose optimization and 3D reconstruction in real-time in unboundedscenes. Optimizing scene reconstruction by monocular depth estimation and usingloop closure detection to update camera pose enable detailed and precisereconstruction on large scenes. Compared to previous work, our approach is morerobust, scalable and versatile. Our experiments demonstrate that MoD-SLAM hasmore excellent mapping performance than prior neural SLAM methods, especiallyin large borderless scenes.</description><author>Heng Zhou, Zhetao Guo, Shuhong Liu, Lechen Zhang, Qihao Wang, Yuxiang Ren, Mingrui Li</author><pubDate>Wed, 21 Feb 2024 17:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03762v3</guid></item><item><title>Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields</title><link>http://arxiv.org/abs/2402.13252v1</link><description>In this paper, we propose an algorithm that allows joint refinement of camerapose and scene geometry represented by decomposed low-rank tensor, using only2D images as supervision. First, we conduct a pilot study based on a 1D signaland relate our findings to 3D scenarios, where the naive joint poseoptimization on voxel-based NeRFs can easily lead to sub-optimal solutions.Moreover, based on the analysis of the frequency spectrum, we propose to applyconvolutional Gaussian filters on 2D and 3D radiance fields for acoarse-to-fine training schedule that enables joint camera pose optimization.Leveraging the decomposition property in decomposed low-rank tensor, our methodachieves an equivalent effect to brute-force 3D convolution with only incurringlittle computational overhead. To further improve the robustness and stabilityof joint optimization, we also propose techniques of smoothed 2D supervision,randomly scaled kernel parameters, and edge-guided loss mask. Extensivequantitative and qualitative evaluations demonstrate that our proposedframework achieves superior performance in novel view synthesis as well asrapid convergence for optimization.</description><author>Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu</author><pubDate>Tue, 20 Feb 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13252v1</guid></item><item><title>SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human Generation</title><link>http://arxiv.org/abs/2403.10166v1</link><description>With the development of neural radiance fields and generative models,numerous methods have been proposed for learning 3D human generation from 2Dimages. These methods allow control over the pose of the generated 3D human andenable rendering from different viewpoints. However, none of these methodsexplore semantic disentanglement in human image synthesis, i.e., they can notdisentangle the generation of different semantic parts, such as the body, tops,and bottoms. Furthermore, existing methods are limited to synthesize images at$512^2$ resolution due to the high computational cost of neural radiancefields. To address these limitations, we introduce SemanticHuman-HD, the firstmethod to achieve semantic disentangled human image synthesis. Notably,SemanticHuman-HD is also the first method to achieve 3D-aware image synthesisat $1024^2$ resolution, benefiting from our proposed 3D-aware super-resolutionmodule. By leveraging the depth maps and semantic masks as guidance for the3D-aware super-resolution, we significantly reduce the number of samplingpoints during volume rendering, thereby reducing the computational cost. Ourcomparative experiments demonstrate the superiority of our method. Theeffectiveness of each proposed component is also verified through ablationstudies. Moreover, our method opens up exciting possibilities for variousapplications, including 3D garment generation, semantic-aware image synthesis,controllable image synthesis, and out-of-domain image synthesis.</description><author>Peng Zheng, Tao Liu, Zili Yi, Rui Ma</author><pubDate>Fri, 15 Mar 2024 11:18:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10166v1</guid></item><item><title>CLOAF: CoLlisiOn-Aware Human Flow</title><link>http://arxiv.org/abs/2403.09050v1</link><description>Even the best current algorithms for estimating body 3D shape and pose yieldresults that include body self-intersections. In this paper, we present CLOAF,which exploits the diffeomorphic nature of Ordinary Differential Equations toeliminate such self-intersections while still imposing body shape constraints.We show that, unlike earlier approaches to addressing this issue, ourscompletely eliminates the self-intersections without compromising the accuracyof the reconstructions. Being differentiable, CLOAF can be used to fine-tunepose and shape estimation baselines to improve their overall performance andeliminate self-intersections in their predictions. Furthermore, we demonstratehow our CLOAF strategy can be applied to practically any motion field inducedby the user. CLOAF also makes it possible to edit motion to interact with theenvironment without worrying about potential collision or loss of body-shapeprior.</description><author>Andrey Davydov, Martin Engilberge, Mathieu Salzmann, Pascal Fua</author><pubDate>Thu, 14 Mar 2024 03:38:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09050v1</guid></item><item><title>ROAM: Robust and Object-Aware Motion Generation Using Neural Pose Descriptors</title><link>http://arxiv.org/abs/2308.12969v2</link><description>Existing automatic approaches for 3D virtual character motion synthesissupporting scene interactions do not generalise well to new objects outsidetraining distributions, even when trained on extensive motion capture datasetswith diverse objects and annotated interactions. This paper addresses thislimitation and shows that robustness and generalisation to novel scene objectsin 3D object-aware character synthesis can be achieved by training a motionmodel with as few as one reference object. We leverage an implicit featurerepresentation trained on object-only datasets, which encodes anSE(3)-equivariant descriptor field around the object. Given an unseen objectand a reference pose-object pair, we optimise for the object-aware pose that isclosest in the feature space to the reference pose. Finally, we use l-NSM,i.e., our motion generation model that is trained to seamlessly transition fromlocomotion to object interaction with the proposed bidirectional pose blendingscheme. Through comprehensive numerical comparisons to state-of-the-art methodsand in a user study, we demonstrate substantial improvements in 3D virtualcharacter motion and interaction quality and robustness to scenarios withunseen objects. Our project page is available athttps://vcai.mpi-inf.mpg.de/projects/ROAM/.</description><author>Wanyue Zhang, Rishabh Dabral, Thomas Leimkühler, Vladislav Golyanik, Marc Habermann, Christian Theobalt</author><pubDate>Thu, 15 Feb 2024 08:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12969v2</guid></item><item><title>AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization</title><link>http://arxiv.org/abs/2308.10001v2</link><description>Neural Radiance Fields (NeRF) have shown promise in generating realisticnovel views from sparse scene images. However, existing NeRF approaches oftenencounter challenges due to the lack of explicit 3D supervision and imprecisecamera poses, resulting in suboptimal outcomes. To tackle these issues, wepropose AltNeRF -- a novel framework designed to create resilient NeRFrepresentations using self-supervised monocular depth estimation (SMDE) frommonocular videos, without relying on known camera poses. SMDE in AltNeRFmasterfully learns depth and pose priors to regulate NeRF training. The depthprior enriches NeRF's capacity for precise scene geometry depiction, while thepose prior provides a robust starting point for subsequent pose refinement.Moreover, we introduce an alternating algorithm that harmoniously melds NeRFoutputs into SMDE through a consistence-driven mechanism, thus enhancing theintegrity of depth priors. This alternation empowers AltNeRF to progressivelyrefine NeRF representations, yielding the synthesis of realistic novel views.Extensive experiments showcase the compelling capabilities of AltNeRF ingenerating high-fidelity and robust novel views that closely resemble reality.</description><author>Kun Wang, Zhiqiang Yan, Huang Tian, Zhenyu Zhang, Xiang Li, Jun Li, Jian Yang</author><pubDate>Fri, 23 Feb 2024 12:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10001v2</guid></item><item><title>SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation</title><link>http://arxiv.org/abs/2311.15707v2</link><description>Zero-shot 6D object pose estimation involves the detection of novel objectswith their 6D poses in cluttered scenes, presenting significant challenges formodel generalizability. Fortunately, the recent Segment Anything Model (SAM)has showcased remarkable zero-shot transfer performance, which provides apromising solution to tackle this task. Motivated by this, we introduce SAM-6D,a novel framework designed to realize the task through two steps, includinginstance segmentation and pose estimation. Given the target objects, SAM-6Demploys two dedicated sub-networks, namely Instance Segmentation Model (ISM)and Pose Estimation Model (PEM), to perform these steps on cluttered RGB-Dimages. ISM takes SAM as an advanced starting point to generate all possibleobject proposals and selectively preserves valid ones through meticulouslycrafted object matching scores in terms of semantics, appearance and geometry.By treating pose estimation as a partial-to-partial point matching problem, PEMperforms a two-stage point matching process featuring a novel design ofbackground tokens to construct dense 3D-3D correspondence, ultimately yieldingthe pose estimates. Without bells and whistles, SAM-6D outperforms the existingmethods on the seven core datasets of the BOP Benchmark for both instancesegmentation and pose estimation of novel objects.</description><author>Jiehong Lin, Lihua Liu, Dekun Lu, Kui Jia</author><pubDate>Wed, 06 Mar 2024 12:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15707v2</guid></item><item><title>Loopy-SLAM: Dense Neural SLAM with Loop Closures</title><link>http://arxiv.org/abs/2402.09944v1</link><description>Neural RGBD SLAM techniques have shown promise in dense SimultaneousLocalization And Mapping (SLAM), yet face challenges such as error accumulationduring camera tracking resulting in distorted maps. In response, we introduceLoopy-SLAM that globally optimizes poses and the dense 3D model. We useframe-to-model tracking using a data-driven point-based submap generationmethod and trigger loop closures online by performing global place recognition.Robust pose graph optimization is used to rigidly align the local submaps. Asour representation is point based, map corrections can be performed efficientlywithout the need to store the entire history of input frames used for mappingas typically required by methods employing a grid based mapping structure.Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNetdatasets demonstrate competitive or superior performance in tracking, mapping,and rendering accuracy when compared to existing dense neural RGBD SLAMmethods. Project page: notchla.github.io/Loopy-SLAM.</description><author>Lorenzo Liso, Erik Sandström, Vladimir Yugay, Luc Van Gool, Martin R. Oswald</author><pubDate>Wed, 14 Feb 2024 18:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09944v1</guid></item><item><title>Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints Scene Coordinate Regression</title><link>http://arxiv.org/abs/2403.10297v1</link><description>Classical structural-based visual localization methods offer high accuracybut face trade-offs in terms of storage, speed, and privacy. A recentinnovation, keypoint scene coordinate regression (KSCR) named D2S addressesthese issues by leveraging graph attention networks to enhance keypointrelationships and predict their 3D coordinates using a simple multilayerperceptron (MLP). Camera pose is then determined via PnP+RANSAC, usingestablished 2D-3D correspondences. While KSCR achieves competitive results,rivaling state-of-the-art image-retrieval methods like HLoc across multiplebenchmarks, its performance is hindered when data samples are limited due tothe deep learning model's reliance on extensive data. This paper proposes asolution to this challenge by introducing a pipeline for keypoint descriptorsynthesis using Neural Radiance Field (NeRF). By generating novel poses andfeeding them into a trained NeRF model to create new views, our approachenhances the KSCR's generalization capabilities in data-scarce environments.The proposed system could significantly improve localization accuracy by up to50\% and cost only a fraction of time for data synthesis. Furthermore, itsmodular design allows for the integration of multiple NeRFs, offering aversatile and efficient solution for visual localization. The implementation ispublicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.</description><author>Huy-Hoang Bui, Bach-Thuan Bui, Dinh-Tuan Tran, Joo-Ho Lee</author><pubDate>Fri, 15 Mar 2024 14:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10297v1</guid></item><item><title>MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction</title><link>http://arxiv.org/abs/2402.12712v1</link><description>This paper presents a neural architecture MVDiffusion++ for 3D objectreconstruction that synthesizes dense and high-resolution views of an objectgiven one or a few images without camera poses. MVDiffusion++ achieves superiorflexibility and scalability with two surprisingly simple ideas: 1) A``pose-free architecture'' where standard self-attention among 2D latentfeatures learns 3D consistency across an arbitrary number of conditional andgeneration views without explicitly using camera pose information; and 2) A``view dropout strategy'' that discards a substantial number of output viewsduring training, which reduces the training-time memory footprint and enablesdense and high-resolution view synthesis at test time. We use the Objaverse fortraining and the Google Scanned Objects for evaluation with standard novel viewsynthesis and 3D reconstruction metrics, where MVDiffusion++ significantlyoutperforms the current state of the arts. We also demonstrate a text-to-3Dapplication example by combining MVDiffusion++ with a text-to-image generativemodel.</description><author>Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, Rakesh Ranjan</author><pubDate>Tue, 20 Feb 2024 04:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12712v1</guid></item><item><title>SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM</title><link>http://arxiv.org/abs/2403.07494v1</link><description>We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3DGaussian representation, that enables accurate 3D semantic mapping, robustcamera tracking, and high-quality rendering in real-time. In this system, weincorporate semantic feature embedding into 3D Gaussian representation, whicheffectively encodes semantic information within the spatial layout of theenvironment for precise semantic scene representation. Furthermore, we proposefeature-level loss for updating 3D Gaussian representation, enablinghigher-level guidance for 3D Gaussian optimization. In addition, to reducecumulative drift and improve reconstruction accuracy, we introducesemantic-informed bundle adjustment leveraging semantic associations for jointoptimization of 3D Gaussian representation and camera poses, leading to morerobust tracking and consistent mapping. Our SemGauss-SLAM method demonstratessuperior performance over existing dense semantic SLAM methods in terms ofmapping and tracking accuracy on Replica and ScanNet datasets, while alsoshowing excellent capabilities in novel-view semantic synthesis and 3D semanticmapping.</description><author>Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, Hesheng Wang</author><pubDate>Tue, 12 Mar 2024 11:33:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07494v1</guid></item><item><title>Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling</title><link>http://arxiv.org/abs/2311.16096v2</link><description>Modeling animatable human avatars from RGB videos is a long-standing andchallenging problem. Recent works usually adopt MLP-based neural radiancefields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs toregress pose-dependent garment details. To this end, we introduce AnimatableGaussians, a new avatar representation that leverages powerful 2D CNNs and 3DGaussian splatting to create high-fidelity avatars. To associate 3D Gaussianswith the animatable avatar, we learn a parametric template from the inputvideos, and then parameterize the template on two front \&amp; back canonicalGaussian maps where each pixel represents a 3D Gaussian. The learned templateis adaptive to the wearing garments for modeling looser clothes like dresses.Such template-guided 2D parameterization enables us to employ a powerfulStyleGAN-based CNN to learn the pose-dependent Gaussian maps for modelingdetailed dynamic appearances. Furthermore, we introduce a pose projectionstrategy for better generalization given novel poses. Overall, our method cancreate lifelike avatars with dynamic, realistic and generalized appearances.Experiments show that our method outperforms other state-of-the-art approaches.Code: https://github.com/lizhe00/AnimatableGaussians</description><author>Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu</author><pubDate>Fri, 15 Mar 2024 09:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16096v2</guid></item><item><title>Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving</title><link>http://arxiv.org/abs/2403.04112v1</link><description>This paper presents a novel multi-modal Multi-Object Tracking (MOT) algorithmfor self-driving cars that combines camera and LiDAR data. Camera frames areprocessed with a state-of-the-art 3D object detector, whereas classicalclustering techniques are used to process LiDAR observations. The proposed MOTalgorithm comprises a three-step association process, an Extended Kalman filterfor estimating the motion of each detected dynamic obstacle, and a trackmanagement phase. The EKF motion model requires the current measured relativeposition and orientation of the observed object and the longitudinal andangular velocities of the ego vehicle as inputs. Unlike most state-of-the-artmulti-modal MOT approaches, the proposed algorithm does not rely on maps orknowledge of the ego global pose. Moreover, it uses a 3D detector exclusivelyfor cameras and is agnostic to the type of LiDAR sensor used. The algorithm isvalidated both in simulation and with real-world data, with satisfactoryresults.</description><author>Riccardo Pieroni, Simone Specchia, Matteo Corno, Sergio Matteo Savaresi</author><pubDate>Wed, 06 Mar 2024 23:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04112v1</guid></item><item><title>Denoising Diffusion via Image-Based Rendering</title><link>http://arxiv.org/abs/2402.03445v2</link><description>Generating 3D scenes is a challenging open problem, which requiressynthesizing plausible content that is fully consistent in 3D space. Whilerecent methods such as neural radiance fields excel at view synthesis and 3Dreconstruction, they cannot synthesize plausible details in unobserved regionssince they lack a generative capability. Conversely, existing generativemethods are typically not capable of reconstructing detailed, large-scalescenes in the wild, as they use limited-capacity 3D scene representations,require aligned camera poses, or rely on additional regularizers. In this work,we introduce the first diffusion model able to perform fast, detailedreconstruction and generation of real-world 3D scenes. To achieve this, we makethree contributions. First, we introduce a new neural scene representation,IB-planes, that can efficiently and accurately represent large 3D scenes,dynamically allocating more capacity as needed to capture details visible ineach image. Second, we propose a denoising-diffusion framework to learn a priorover this novel 3D scene representation, using only 2D images without the needfor any additional supervision signal such as masks or depths. This supports 3Dreconstruction and generation in a unified architecture. Third, we develop aprincipled approach to avoid trivial 3D solutions when integrating theimage-based rendering with the diffusion model, by dropping out representationsof some images. We evaluate the model on several challenging datasets of realand synthetic images, and demonstrate superior results on generation, novelview synthesis and 3D reconstruction.</description><author>Titas Anciukevičius, Fabian Manhardt, Federico Tombari, Paul Henderson</author><pubDate>Tue, 20 Feb 2024 20:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03445v2</guid></item><item><title>Cameras as Rays: Pose Estimation via Ray Diffusion</title><link>http://arxiv.org/abs/2402.14817v1</link><description>Estimating camera poses is a fundamental task for 3D reconstruction andremains challenging given sparse views (&lt;10). In contrast to existingapproaches that pursue top-down prediction of global parametrizations of cameraextrinsics, we propose a distributed representation of camera pose that treatsa camera as a bundle of rays. This representation allows for a tight couplingwith spatial image features improving pose precision. We observe that thisrepresentation is naturally suited for set-level level transformers and developa regression-based approach that maps image patches to corresponding rays. Tocapture the inherent uncertainties in sparse-view pose inference, we adapt thisapproach to learn a denoising diffusion model which allows us to sampleplausible modes while improving performance. Our proposed methods, bothregression- and diffusion-based, demonstrate state-of-the-art performance oncamera pose estimation on CO3D while generalizing to unseen object categoriesand in-the-wild captures.</description><author>Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Thu, 22 Feb 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14817v1</guid></item><item><title>DrFER: Learning Disentangled Representations for 3D Facial Expression Recognition</title><link>http://arxiv.org/abs/2403.08318v1</link><description>Facial Expression Recognition (FER) has consistently been a focal point inthe field of facial analysis. In the context of existing methodologies for 3DFER or 2D+3D FER, the extraction of expression features often gets entangledwith identity information, compromising the distinctiveness of these features.To tackle this challenge, we introduce the innovative DrFER method, whichbrings the concept of disentangled representation learning to the field of 3DFER. DrFER employs a dual-branch framework to effectively disentangleexpression information from identity information. Diverging from priordisentanglement endeavors in the 3D facial domain, we have carefullyreconfigured both the loss functions and network structure to make the overallframework adaptable to point cloud data. This adaptation enhances thecapability of the framework in recognizing facial expressions, even in casesinvolving varying head poses. Extensive evaluations conducted on the BU-3DFEand Bosphorus datasets substantiate that DrFER surpasses the performance ofother 3D FER methods.</description><author>Hebeizi Li, Hongyu Yang, Di Huang</author><pubDate>Wed, 13 Mar 2024 09:00:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08318v1</guid></item><item><title>RadarDistill: Boosting Radar-based Object Detection Performance via Knowledge Distillation from LiDAR Features</title><link>http://arxiv.org/abs/2403.05061v1</link><description>The inherent noisy and sparse characteristics of radar data pose challengesin finding effective representations for 3D object detection. In this paper, wepropose RadarDistill, a novel knowledge distillation (KD) method, which canimprove the representation of radar data by leveraging LiDAR data. RadarDistillsuccessfully transfers desirable characteristics of LiDAR features into radarfeatures using three key components: Cross-Modality Alignment (CMA),Activation-based Feature Distillation (AFD), and Proposal-based FeatureDistillation (PFD). CMA enhances the density of radar features through multiplelayers of dilation operations, effectively addressing the challenges ofinefficient knowledge transfer from LiDAR to radar. AFD is designed to transferknowledge from significant areas of the LiDAR features, specifically thoseregions where activation intensity exceeds a predetermined threshold. PFDguides the radar network to mimic LiDAR network features in the objectproposals for accurately detected results while moderating features formisdetected proposals like false positives. Our comparative analyses conductedon the nuScenes datasets demonstrate that RadarDistill achievesstate-of-the-art (SOTA) performance for radar-only object detection task,recording 20.5% in mAP and 43.7% in NDS. Also, RadarDistill significantlyimproves the performance of the camera-radar fusion model.</description><author>Geonho Bang, Kwangjin Choi, Jisong Kim, Dongsuk Kum, Jun Won Choi</author><pubDate>Fri, 08 Mar 2024 05:15:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05061v1</guid></item><item><title>Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks</title><link>http://arxiv.org/abs/2402.14400v1</link><description>Reliable methods for the neurodevelopmental assessment of infants areessential for early detection of medical issues that may need promptinterventions. Spontaneous motor activity, or `kinetics', is shown to provide apowerful surrogate measure of upcoming neurodevelopment. However, itsassessment is by and large qualitative and subjective, focusing on visuallyidentified, age-specific gestures. Here, we follow an alternative approach,predicting infants' neurodevelopmental maturation based on data-drivenevaluation of individual motor patterns. We utilize 3D video recordings ofinfants processed with pose-estimation to extract spatio-temporal series ofanatomical landmarks, and apply adaptive graph convolutional networks topredict the actual age. We show that our data-driven approach achievesimprovement over traditional machine learning baselines based on manuallyengineered features.</description><author>Daniel Holmberg, Manu Airaksinen, Viviana Marchi, Andrea Guzzetta, Anna Kivi, Leena Haataja, Sampsa Vanhatalo, Teemu Roos</author><pubDate>Thu, 22 Feb 2024 09:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14400v1</guid></item><item><title>Towards Transferable Targeted 3D Adversarial Attack in the Physical World</title><link>http://arxiv.org/abs/2312.09558v2</link><description>Compared with transferable untargeted attacks, transferable targetedadversarial attacks could specify the misclassification categories ofadversarial samples, posing a greater threat to security-critical tasks. In themeanwhile, 3D adversarial samples, due to their potential of multi-viewrobustness, can more comprehensively identify weaknesses in existing deeplearning systems, possessing great application value. However, the field oftransferable targeted 3D adversarial attacks remains vacant. The goal of thiswork is to develop a more effective technique that could generate transferabletargeted 3D adversarial examples, filling the gap in this field. To achievethis goal, we design a novel framework named TT3D that could rapidlyreconstruct from few multi-view images into Transferable Targeted 3D texturedmeshes. While existing mesh-based texture optimization methods computegradients in the high-dimensional mesh space and easily fall into local optima,leading to unsatisfactory transferability and distinct distortions, TT3Dinnovatively performs dual optimization towards both feature grid andMulti-layer Perceptron (MLP) parameters in the grid-based NeRF space, whichsignificantly enhances black-box transferability while enjoying naturalness.Experimental results show that TT3D not only exhibits superior cross-modeltransferability but also maintains considerable adaptability across differentrenders and vision tasks. More importantly, we produce 3D adversarial exampleswith 3D printing techniques in the real world and verify their robustperformance under various scenarios.</description><author>Yao Huang, Yinpeng Dong, Shouwei Ruan, Xiao Yang, Hang Su, Xingxing Wei</author><pubDate>Wed, 28 Feb 2024 12:28:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09558v2</guid></item><item><title>Neural Refinement for Absolute Pose Regression with Feature Synthesis</title><link>http://arxiv.org/abs/2303.10087v2</link><description>Absolute Pose Regression (APR) methods use deep neural networks to directlyregress camera poses from RGB images. However, the predominant APRarchitectures only rely on 2D operations during inference, resulting in limitedaccuracy of pose estimation due to the lack of 3D geometry constraints orpriors. In this work, we propose a test-time refinement pipeline that leveragesimplicit geometric constraints using a robust feature field to enhance theability of APR methods to use 3D information during inference. We alsointroduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3Dgeometric features during training and directly renders dense novel viewfeatures at test time to refine APR methods. To enhance the robustness of ourmodel, we introduce a feature fusion module and a progressive trainingstrategy. Our proposed method achieves state-of-the-art single-image APRaccuracy on indoor and outdoor datasets.</description><author>Shuai Chen, Yash Bhalgat, Xinghui Li, Jiawang Bian, Kejie Li, Zirui Wang, Victor Adrian Prisacariu</author><pubDate>Fri, 01 Mar 2024 01:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10087v2</guid></item><item><title>Representing 3D sparse map points and lines for camera relocalization</title><link>http://arxiv.org/abs/2402.18011v1</link><description>Recent advancements in visual localization and mapping have demonstratedconsiderable success in integrating point and line features. However, expandingthe localization framework to include additional mapping components frequentlyresults in increased demand for memory and computational resources dedicated tomatching tasks. In this study, we show how a lightweight neural network canlearn to represent both 3D point and line features, and exhibit leading poseaccuracy by harnessing the power of multiple learned mappings. Specifically, weutilize a single transformer block to encode line features, effectivelytransforming them into distinctive point-like descriptors. Subsequently, wetreat these point and line descriptor sets as distinct yet interconnectedfeature sets. Through the integration of self- and cross-attention withinseveral graph layers, our method effectively refines each feature beforeregressing 3D maps using two simple MLPs. In comprehensive experiments, ourindoor localization findings surpass those of Hloc and Limap across bothpoint-based and line-assisted configurations. Moreover, in outdoor scenarios,our method secures a significant lead, marking the most considerableenhancement over state-of-the-art learning-based methodologies. The source codeand demo videos of this work are publicly available at:https://thpjp.github.io/pl2map/</description><author>Bach-Thuan Bui, Huy-Hoang Bui, Dinh-Tuan Tran, Joo-Ho Lee</author><pubDate>Wed, 28 Feb 2024 03:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18011v1</guid></item><item><title>CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge</title><link>http://arxiv.org/abs/2402.15726v1</link><description>Most of existing category-level object pose estimation methods devote tolearning the object category information from point cloud modality. However,the scale of 3D datasets is limited due to the high cost of 3D data collectionand annotation. Consequently, the category features extracted from theselimited point cloud samples may not be comprehensive. This motivates us toinvestigate whether we can draw on knowledge of other modalities to obtaincategory information. Inspired by this motivation, we propose CLIPose, a novel6D pose framework that employs the pre-trained vision-language model to developbetter learning of object category information, which can fully leverageabundant semantic knowledge in image and text modalities. To make the 3Dencoder learn category-specific features more efficiently, we alignrepresentations of three modalities in feature space via multi-modalcontrastive learning. In addition to exploiting the pre-trained knowledge ofthe CLIP's model, we also expect it to be more sensitive with pose parameters.Therefore, we introduce a prompt tuning approach to fine-tune image encoderwhile we incorporate rotations and translations information in the textdescriptions. CLIPose achieves state-of-the-art performance on two mainstreambenchmark datasets, REAL275 and CAMERA25, and runs in real-time duringinference (40FPS).</description><author>Xiao Lin, Minghao Zhu, Ronghao Dang, Guangliang Zhou, Shaolong Shu, Feng Lin, Chengju Liu, Qijun Chen</author><pubDate>Sat, 24 Feb 2024 05:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15726v1</guid></item><item><title>A PnP Algorithm for Two-Dimensional Pose Estimation</title><link>http://arxiv.org/abs/2312.08488v3</link><description>We propose a PnP algorithm for a camera constrained to two-dimensional motion(applicable, for instance, to many wheeled robotics platforms). Leveraging thisassumption allows accuracy and performance improvements over 3D PnP algorithmsdue to the reduction in search space dimensionality. It also reduces theincidence of ambiguous pose estimates (as, in most cases, the spurioussolutions fall outside the plane of movement). Our algorithm finds anapproximate solution by solving a polynomial system and refines its predictioniteratively to minimize the reprojection error. The algorithm comparesfavorably to existing 3D PnP algorithms in terms of accuracy, performance, androbustness to noise.</description><author>Joshua Wang</author><pubDate>Fri, 08 Mar 2024 05:19:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08488v3</guid></item><item><title>KVN: Keypoints Voting Network with Differentiable RANSAC for Stereo Pose Estimation</title><link>http://arxiv.org/abs/2307.11543v2</link><description>Object pose estimation is a fundamental computer vision task exploited inseveral robotics and augmented reality applications. Many establishedapproaches rely on predicting 2D-3D keypoint correspondences using RANSAC(Random sample consensus) and estimating the object pose using the PnP(Perspective-n-Point) algorithm. Being RANSAC non-differentiable,correspondences cannot be directly learned in an end-to-end fashion. In thispaper, we address the stereo image-based object pose estimation problem by i)introducing a differentiable RANSAC layer into a well-known monocular poseestimation network; ii) exploiting an uncertainty-driven multi-view PnP solverwhich can fuse information from multiple views. We evaluate our approach on achallenging public stereo object pose estimation dataset and a custom-builtdataset we call Transparent Tableware Dataset (TTD), yielding state-of-the-artresults against other recent approaches. Furthermore, in our ablation study, weshow that the differentiable RANSAC layer plays a significant role in theaccuracy of the proposed method. We release with this paper the code of ourmethod and the TTD dataset.</description><author>Ivano Donadi, Alberto Pretto</author><pubDate>Wed, 28 Feb 2024 15:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11543v2</guid></item><item><title>DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly</title><link>http://arxiv.org/abs/2402.19302v1</link><description>Reassembly tasks play a fundamental role in many fields and multipleapproaches exist to solve specific reassembly problems. In this context, weposit that a general unified model can effectively address them all,irrespective of the input data type (images, 3D, etc.). We introduceDiffAssemble, a Graph Neural Network (GNN)-based architecture that learns tosolve reassembly tasks using a diffusion model formulation. Our method treatsthe elements of a set, whether pieces of 2D patch or 3D object fragments, asnodes of a spatial graph. Training is performed by introducing noise into theposition and rotation of the elements and iteratively denoising them toreconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art(SOTA) results in most 2D and 3D reassembly tasks and is the firstlearning-based approach that solves 2D puzzles for both rotation andtranslation. Furthermore, we highlight its remarkable reduction in run-time,performing 11 times faster than the quickest optimization-based method forpuzzle solving. Code available at https://github.com/IIT-PAVIS/DiffAssemble</description><author>Gianluca Scarpellini, Stefano Fiorini, Francesco Giuliari, Pietro Morerio, Alessio Del Bue</author><pubDate>Thu, 29 Feb 2024 16:09:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19302v1</guid></item><item><title>OCT2Confocal: 3D CycleGAN based Translation of Retinal OCT Images to Confocal Microscopy</title><link>http://arxiv.org/abs/2311.10902v3</link><description>Optical coherence tomography (OCT) and confocal microscopy are pivotal inretinal imaging, each presenting unique benefits and limitations. In-vivo OCToffers rapid, non-invasive imaging but can be hampered by clarity issues andmotion artifacts. Ex-vivo confocal microscopy provides high-resolution,cellular detailed color images but is invasive and poses ethical concerns andpotential tissue damage. To bridge these modalities, we developed a 3D CycleGANframework for unsupervised translation of in-vivo OCT to ex-vivo confocalmicroscopy images. Applied to our OCT2Confocal dataset, this frameworkeffectively translates between 3D medical data domains, capturing vascular,textural, and cellular details with precision. This marks the first attempt toexploit the inherent 3D information of OCT and translate it into the rich,detailed color domain of confocal microscopy. Assessed through quantitative andqualitative evaluations, the 3D CycleGAN framework demonstrates commendableimage fidelity and quality, outperforming existing methods despite theconstraints of limited data. This non-invasive generation of retinal confocalimages has the potential to further enhance diagnostic and monitoringcapabilities in ophthalmology. Our source code and OCT2Confocal dataset areavailable at https://github.com/xintian-99/OCT2Confocal.</description><author>Xin Tian, Nantheera Anantrasirichai, Lindsay Nicholson, Alin Achim</author><pubDate>Sat, 17 Feb 2024 01:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10902v3</guid></item><item><title>SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation</title><link>http://arxiv.org/abs/2401.13560v3</link><description>The Transformer architecture has shown a remarkable ability in modelingglobal relationships. However, it poses a significant computational challengewhen processing high-dimensional medical images. This hinders its developmentand widespread adoption in this task. Mamba, as a State Space Model (SSM),recently emerged as a notable manner for long-range dependencies in sequentialmodeling, excelling in natural language processing filed with its remarkablememory efficiency and computational speed. Inspired by its success, weintroduce SegMamba, a novel 3D medical image \textbf{Seg}mentation\textbf{Mamba} model, designed to effectively capture long-range dependencieswithin whole volume features at every scale. Our SegMamba, in contrast toTransformer-based methods, excels in whole volume feature modeling from a statespace model standpoint, maintaining superior processing speed, even with volumefeatures at a resolution of {$64\times 64\times 64$}. Comprehensive experimentson the BraTS2023 dataset demonstrate the effectiveness and efficiency of ourSegMamba. The code for SegMamba is available at:https://github.com/ge-xing/SegMamba</description><author>Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, Lei Zhu</author><pubDate>Sun, 25 Feb 2024 14:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13560v3</guid></item><item><title>SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</title><link>http://arxiv.org/abs/2403.07547v1</link><description>Neural radiance fields (NeRF) has attracted considerable attention for theirexceptional ability in synthesizing novel views with high fidelity. However,the presence of motion blur, resulting from slight camera movements duringextended shutter exposures, poses a significant challenge, potentiallycompromising the quality of the reconstructed 3D scenes. While recent studieshave addressed this issue, they do not consider the continuous dynamics ofcamera movements during image acquisition, leading to inaccurate scenereconstruction. Additionally, these methods are plagued by slow training andrendering speed. To effectively handle these issues, we propose sequentialmotion understanding radiance fields (SMURF), a novel approach that employsneural ordinary differential equation (Neural-ODE) to model continuous cameramotion and leverages the explicit volumetric representation method for fastertraining and robustness to motion-blurred input images. The core idea of theSMURF is continuous motion blurring kernel (CMBK), a unique module designed tomodel a continuous camera movements for processing blurry inputs. Our model,rigorously evaluated against benchmark datasets, demonstrates state-of-the-artperformance both quantitatively and qualitatively.</description><author>Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee</author><pubDate>Tue, 12 Mar 2024 12:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07547v1</guid></item><item><title>LM2D: Lyrics- and Music-Driven Dance Synthesis</title><link>http://arxiv.org/abs/2403.09407v1</link><description>Dance typically involves professional choreography with complex movementsthat follow a musical rhythm and can also be influenced by lyrical content. Theintegration of lyrics in addition to the auditory dimension, enriches thefoundational tone and makes motion generation more amenable to its semanticmeanings. However, existing dance synthesis methods tend to model motions onlyconditioned on audio signals. In this work, we make two contributions to bridgethis gap. First, we propose LM2D, a novel probabilistic architecture thatincorporates a multimodal diffusion model with consistency distillation,designed to create dance conditioned on both music and lyrics in one diffusiongeneration step. Second, we introduce the first 3D dance-motion dataset thatencompasses both music and lyrics, obtained with pose estimation technologies.We evaluate our model against music-only baseline models with objective metricsand human evaluations, including dancers and choreographers. The resultsdemonstrate LM2D is able to produce realistic and diverse dance matching bothlyrics and music. A video summary can be accessed at:https://youtu.be/4XCgvYookvA.</description><author>Wenjie Yin, Xuejiao Zhao, Yi Yu, Hang Yin, Danica Kragic, Mårten Björkman</author><pubDate>Thu, 14 Mar 2024 14:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09407v1</guid></item><item><title>AccessLens: Auto-detecting Inaccessibility of Everyday Objects</title><link>http://arxiv.org/abs/2401.15996v2</link><description>In our increasingly diverse society, everyday physical interfaces oftenpresent barriers, impacting individuals across various contexts. Thisoversight, from small cabinet knobs to identical wall switches that can posedifferent contextual challenges, highlights an imperative need for solutions.Leveraging low-cost 3D-printed augmentations such as knob magnifiers andtactile labels seems promising, yet the process of discovering unrecognizedbarriers remains challenging because disability is context-dependent. Weintroduce AccessLens, an end-to-end system designed to identify inaccessibleinterfaces in daily objects, and recommend 3D-printable augmentations foraccessibility enhancement. Our approach involves training a detector using thenovel AccessDB dataset designed to automatically recognize 21 distinctInaccessibility Classes (e.g., bar-small and round-rotate) within 6 commonobject categories (e.g., handle and knob). AccessMeta serves as a robust way tobuild a comprehensive dictionary linking these accessibility classes toopen-source 3D augmentation designs. Experiments demonstrate our detector'sperformance in detecting inaccessible objects.</description><author>Nahyun Kwon, Qian Lu, Muhammad Hasham Qazi, Joanne Liu, Changhoon Oh, Shu Kong, Jeeeun Kim</author><pubDate>Fri, 23 Feb 2024 17:06:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15996v2</guid></item><item><title>EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection</title><link>http://arxiv.org/abs/2402.15272v1</link><description>In autonomous driving, cooperative perception makes use of multi-view camerasfrom both vehicles and infrastructure, providing a global vantage point withrich semantic context of road conditions beyond a single vehicle viewpoint.Currently, two major challenges persist in vehicle-infrastructure cooperative3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-viewimages, caused by time asynchrony across cameras; $2)$ information loss intransmission process resulted from limited communication bandwidth. To addressthese issues, we propose a novel camera-based 3D detection framework for VIC3Dtask, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploitholistic perspectives from both vehicles and infrastructure, we proposeMulti-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM)modules to enhance infrastructure and vehicle features at scale, spatial, andchannel levels to correct the pose error introduced by camera asynchrony. Wealso introduce a Feature Compression (FC) module with channel and spatialcompression blocks for transmission efficiency. Experiments show that EMIFFachieves SOTA on DAIR-V2X-C datasets, significantly outperforming previousearly-fusion and late-fusion methods with comparable transmission costs.</description><author>Zhe Wang, Siqi Fan, Xiaoliang Huo, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang</author><pubDate>Fri, 23 Feb 2024 11:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15272v1</guid></item><item><title>False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy</title><link>http://arxiv.org/abs/2403.02639v1</link><description>Recent studies have focused on enhancing the performance of 3D objectdetection models. Among various approaches, ground-truth sampling has beenproposed as an augmentation technique to address the challenges posed bylimited ground-truth data. However, an inherent issue with ground-truthsampling is its tendency to increase false positives. Therefore, this studyaims to overcome the limitations of ground-truth sampling and improve theperformance of 3D object detection models by developing a new augmentationtechnique called false-positive sampling. False-positive sampling involvesretraining the model using point clouds that are identified as false positivesin the model's predictions. We propose an algorithm that utilizes bothground-truth and false-positive sampling and an algorithm for building thefalse-positive sample database. Additionally, we analyze the principles behindthe performance enhancement due to false-positive sampling and propose atechnique that applies the concept of curriculum learning to the samplingstrategy that encompasses both false-positive and ground-truth samplingtechniques. Our experiments demonstrate that models utilizing false-positivesampling show a reduction in false positives and exhibit improved objectdetection performance. On the KITTI and Waymo Open datasets, models withfalse-positive sampling surpass the baseline models by a large margin.</description><author>Jiyong Oh, Junhaeng Lee, Woongchan Byun, Minsang Kong, Sang Hun Lee</author><pubDate>Tue, 05 Mar 2024 04:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02639v1</guid></item><item><title>False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy</title><link>http://arxiv.org/abs/2403.02639v2</link><description>Recent studies have focused on enhancing the performance of 3D objectdetection models. Among various approaches, ground-truth sampling has beenproposed as an augmentation technique to address the challenges posed bylimited ground-truth data. However, an inherent issue with ground-truthsampling is its tendency to increase false positives. Therefore, this studyaims to overcome the limitations of ground-truth sampling and improve theperformance of 3D object detection models by developing a new augmentationtechnique called false-positive sampling. False-positive sampling involvesretraining the model using point clouds that are identified as false positivesin the model's predictions. We propose an algorithm that utilizes bothground-truth and false-positive sampling and an algorithm for building thefalse-positive sample database. Additionally, we analyze the principles behindthe performance enhancement due to false-positive sampling and propose atechnique that applies the concept of curriculum learning to the samplingstrategy that encompasses both false-positive and ground-truth samplingtechniques. Our experiments demonstrate that models utilizing false-positivesampling show a reduction in false positives and exhibit improved objectdetection performance. On the KITTI and Waymo Open datasets, models withfalse-positive sampling surpass the baseline models by a large margin.</description><author>Jiyong Oh, Junhaeng Lee, Woongchan Byun, Minsang Kong, Sang Hun Lee</author><pubDate>Thu, 07 Mar 2024 12:24:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02639v2</guid></item><item><title>Multi-Model 3D Registration: Finding Multiple Moving Objects in Cluttered Point Clouds</title><link>http://arxiv.org/abs/2402.10865v1</link><description>We investigate a variation of the 3D registration problem, named multi-model3D registration. In the multi-model registration problem, we are given twopoint clouds picturing a set of objects at different poses (and possiblyincluding points belonging to the background) and we want to simultaneouslyreconstruct how all objects moved between the two point clouds. This setupgeneralizes standard 3D registration where one wants to reconstruct a singlepose, e.g., the motion of the sensor picturing a static scene. Moreover, itprovides a mathematically grounded formulation for relevant roboticsapplications, e.g., where a depth sensor onboard a robot perceives a dynamicscene and has the goal of estimating its own motion (from the static portion ofthe scene) while simultaneously recovering the motion of all dynamic objects.We assume a correspondence-based setup where we have putative matches betweenthe two point clouds and consider the practical case where thesecorrespondences are plagued with outliers. We then propose a simple approachbased on Expectation-Maximization (EM) and establish theoretical conditionsunder which the EM approach converges to the ground truth. We evaluate theapproach in simulated and real datasets ranging from table-top scenes toself-driving scenarios and demonstrate its effectiveness when combined withstate-of-the-art scene flow methods to establish dense correspondences.</description><author>David Jin, Sushrut Karmalkar, Harry Zhang, Luca Carlone</author><pubDate>Fri, 16 Feb 2024 18:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10865v1</guid></item><item><title>URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural Radiance Fields</title><link>http://arxiv.org/abs/2403.10119v1</link><description>We propose a novel rolling shutter bundle adjustment method for neuralradiance fields (NeRF), which utilizes the unordered rolling shutter (RS)images to obtain the implicit 3D representation. Existing NeRF methods sufferfrom low-quality images and inaccurate initial camera poses due to the RSeffect in the image, whereas, the previous method that incorporates the RS intoNeRF requires strict sequential data input, limiting its widespreadapplicability. In constant, our method recovers the physical formation of RSimages by estimating camera poses and velocities, thereby removing the inputconstraints on sequential data. Moreover, we adopt a coarse-to-fine trainingstrategy, in which the RS epipolar constraints of the pairwise frames in thescene graph are used to detect the camera poses that fall into local minima.The poses detected as outliers are corrected by the interpolation method withneighboring poses. The experimental results validate the effectiveness of ourmethod over state-of-the-art works and demonstrate that the reconstruction of3D representations is not constrained by the requirement of video sequenceinput.</description><author>Bo Xu, Ziao Liu, Mengqi Guo, Jiancheng Li, Gim Hee Li</author><pubDate>Fri, 15 Mar 2024 10:08:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10119v1</guid></item><item><title>GigaPose: Fast and Robust Novel Object Pose Estimation via One Correspondence</title><link>http://arxiv.org/abs/2311.14155v2</link><description>We present GigaPose, a fast, robust, and accurate method for CAD-based novelobject pose estimation in RGB images. GigaPose first leverages discriminative"templates", rendered images of the CAD models, to recover the out-of-planerotation and then uses patch correspondences to estimate the four remainingparameters. Our approach samples templates in only a two-degrees-of-freedomspace instead of the usual three and matches the input image to the templatesusing fast nearest-neighbor search in feature space, results in a speedupfactor of 35x compared to the state of the art. Moreover, GigaPose issignificantly more robust to segmentation errors. Our extensive evaluation onthe seven core datasets of the BOP challenge demonstrates that it achievesstate-of-the-art accuracy and can be seamlessly integrated with existingrefinement methods. Additionally, we show the potential of GigaPose with 3Dmodels predicted by recent work on 3D reconstruction from a single image,relaxing the need for CAD models and making 6D pose object estimation much moreconvenient. Our source code and trained models are publicly available athttps://github.com/nv-nguyen/gigaPose</description><author>Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, Vincent Lepetit</author><pubDate>Fri, 15 Mar 2024 16:05:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14155v2</guid></item><item><title>RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban Scenes</title><link>http://arxiv.org/abs/2403.09419v1</link><description>The task of separating dynamic objects from static environments using NeRFshas been widely studied in recent years. However, capturing large-scale scenesstill poses a challenge due to their complex geometric structures andunconstrained dynamics. Without the help of 3D motion cues, previous methodsoften require simplified setups with slow camera motion and only a few/singledynamic actors, leading to suboptimal solutions in most urban setups. Toovercome such limitations, we present RoDUS, a pipeline for decomposing staticand dynamic elements in urban scenes, with thoughtfully separated NeRF modelsfor moving and non-moving components. Our approach utilizes a robustkernel-based initialization coupled with 4D semantic information to selectivelyguide the learning process. This strategy enables accurate capturing of thedynamics in the scene, resulting in reduced artifacts caused by NeRF onbackground reconstruction, all by using self-supervision. Notably, experimentalevaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness ofour method in decomposing challenging urban scenes into precise static anddynamic components.</description><author>Thang-Anh-Quan Nguyen, Luis Roldão, Nathan Piasco, Moussab Bennehar, Dzmitry Tsishkou</author><pubDate>Thu, 14 Mar 2024 15:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09419v1</guid></item><item><title>ERASOR++: Height Coding Plus Egocentric Ratio Based Dynamic Object Removal for Static Point Cloud Mapping</title><link>http://arxiv.org/abs/2403.05019v1</link><description>Mapping plays a crucial role in location and navigation within automaticsystems. However, the presence of dynamic objects in 3D point cloud mapsgenerated from scan sensors can introduce map distortion and long traces,thereby posing challenges for accurate mapping and navigation. To address thisissue, we propose ERASOR++, an enhanced approach based on the Egocentric Ratioof Pseudo Occupancy for effective dynamic object removal. To begin, weintroduce the Height Coding Descriptor, which combines height difference andheight layer information to encode the point cloud. Subsequently, we proposethe Height Stack Test, Ground Layer Test, and Surrounding Point Test methods toprecisely and efficiently identify the dynamic bins within point cloud bins,thus overcoming the limitations of prior approaches. Through extensiveevaluation on open-source datasets, our approach demonstrates superiorperformance in terms of precision and efficiency compared to existing methods.Furthermore, the techniques described in our work hold promise for addressingvarious challenging tasks or aspects through subsequent migration.</description><author>Jiabao Zhang, Yu Zhang</author><pubDate>Fri, 08 Mar 2024 03:45:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05019v1</guid></item><item><title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</title><link>http://arxiv.org/abs/2309.09737v4</link><description>Mobile autonomy relies on the precise perception of dynamic environments.Robustly tracking moving objects in 3D world thus plays a pivotal role forapplications like trajectory prediction, obstacle avoidance, and path planning.While most current methods utilize LiDARs or cameras for Multiple ObjectTracking (MOT), the capabilities of 4D imaging radars remain largelyunexplored. Recognizing the challenges posed by radar noise and point sparsityin 4D radar data, we introduce RaTrack, an innovative solution tailored forradar-based tracking. Bypassing the typical reliance on specific object typesand 3D bounding boxes, our method focuses on motion segmentation andclustering, enriched by a motion estimation module. Evaluated on theView-of-Delft dataset, RaTrack showcases superior tracking precision of movingobjects, largely surpassing the performance of the state of the art. We releaseour code and model at https://github.com/LJacksonPan/RaTrack.</description><author>Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu</author><pubDate>Mon, 19 Feb 2024 17:53:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09737v4</guid></item><item><title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</title><link>http://arxiv.org/abs/2309.09737v6</link><description>Mobile autonomy relies on the precise perception of dynamic environments.Robustly tracking moving objects in 3D world thus plays a pivotal role forapplications like trajectory prediction, obstacle avoidance, and path planning.While most current methods utilize LiDARs or cameras for Multiple ObjectTracking (MOT), the capabilities of 4D imaging radars remain largelyunexplored. Recognizing the challenges posed by radar noise and point sparsityin 4D radar data, we introduce RaTrack, an innovative solution tailored forradar-based tracking. Bypassing the typical reliance on specific object typesand 3D bounding boxes, our method focuses on motion segmentation andclustering, enriched by a motion estimation module. Evaluated on theView-of-Delft dataset, RaTrack showcases superior tracking precision of movingobjects, largely surpassing the performance of the state of the art. We releaseour code and model at https://github.com/LJacksonPan/RaTrack.</description><author>Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu</author><pubDate>Wed, 06 Mar 2024 13:32:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09737v6</guid></item><item><title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</title><link>http://arxiv.org/abs/2309.09737v5</link><description>Mobile autonomy relies on the precise perception of dynamic environments.Robustly tracking moving objects in 3D world thus plays a pivotal role forapplications like trajectory prediction, obstacle avoidance, and path planning.While most current methods utilize LiDARs or cameras for Multiple ObjectTracking (MOT), the capabilities of 4D imaging radars remain largelyunexplored. Recognizing the challenges posed by radar noise and point sparsityin 4D radar data, we introduce RaTrack, an innovative solution tailored forradar-based tracking. Bypassing the typical reliance on specific object typesand 3D bounding boxes, our method focuses on motion segmentation andclustering, enriched by a motion estimation module. Evaluated on theView-of-Delft dataset, RaTrack showcases superior tracking precision of movingobjects, largely surpassing the performance of the state of the art. We releaseour code and model at https://github.com/LJacksonPan/RaTrack.</description><author>Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu</author><pubDate>Sat, 24 Feb 2024 17:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09737v5</guid></item><item><title>OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy Prediction</title><link>http://arxiv.org/abs/2403.05329v1</link><description>3D occupancy prediction based on multi-sensor fusion, crucial for a reliableautonomous driving system, enables fine-grained understanding of 3D scenes.Previous fusion-based 3D occupancy predictions relied on depth estimation forprocessing 2D image features. However, depth estimation is an ill-posedproblem, hindering the accuracy and robustness of these methods. Furthermore,fine-grained occupancy prediction demands extensive computational resources. Weintroduce OccFusion, a multi-modal fusion method free from depth estimation,and a corresponding point cloud sampling algorithm for dense integration ofimage features. Building on this, we propose an active training method and anactive coarse to fine pipeline, enabling the model to adaptively learn morefrom complex samples and optimize predictions specifically for challengingareas such as small or overlapping objects. The active methods we propose canbe naturally extended to any occupancy prediction model. Experiments on theOpenOccupancy benchmark show our method surpasses existing state-of-the-art(SOTA) multi-modal methods in IoU across all categories. Additionally, ourmodel is more efficient during both the training and inference phases,requiring far fewer computational resources. Comprehensive ablation studiesdemonstrate the effectiveness of our proposed techniques.</description><author>Ji Zhang, Yiran Ding</author><pubDate>Fri, 08 Mar 2024 14:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05329v1</guid></item><item><title>Seeing the World through Your Eyes</title><link>http://arxiv.org/abs/2306.09348v2</link><description>The reflective nature of the human eye is an underappreciated source ofinformation about what the world around us looks like. By imaging the eyes of amoving person, we can collect multiple views of a scene outside the camera'sdirect line of sight through the reflections in the eyes. In this paper, wereconstruct a 3D scene beyond the camera's line of sight using portrait imagescontaining eye reflections. This task is challenging due to 1) the difficultyof accurately estimating eye poses and 2) the entangled appearance of the eyeiris and the scene reflections. Our method jointly refines the cornea poses,the radiance field depicting the scene, and the observer's eye iris texture. Wefurther propose a simple regularization prior on the iris texture pattern toimprove reconstruction quality. Through various experiments on synthetic andreal-world captures featuring people with varied eye colors, we demonstrate thefeasibility of our approach to recover 3D scenes using eye reflections.</description><author>Hadi Alzayer, Kevin Zhang, Brandon Feng, Christopher Metzler, Jia-Bin Huang</author><pubDate>Sat, 02 Mar 2024 16:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09348v2</guid></item><item><title>MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction</title><link>http://arxiv.org/abs/2402.03762v5</link><description>Monocular SLAM has received a lot of attention due to its simple RGB inputsand the lifting of complex sensor constraints. However, existing monocular SLAMsystems are designed for bounded scenes, restricting the applicability of SLAMsystems. To address this limitation, we propose MoD-SLAM, the first monocularNeRF-based dense mapping method that allows 3D reconstruction in real-time inunbounded scenes. Specifically, we introduce a Gaussian-based unbounded scenerepresentation approach to solve the challenge of mapping scenes withoutboundaries. This strategy is essential to extend the SLAM application.Moreover, a depth estimation module in the front-end is designed to extractaccurate priori depth values to supervise mapping and tracking processes. Byintroducing a robust depth loss term into the tracking process, our SLAM systemachieves more precise pose estimation in large-scale scenes. Our experiments ontwo standard datasets show that MoD-SLAM achieves competitive performance,improving the accuracy of the 3D reconstruction and localization by up to 30%and 15% respectively compared with existing state-of-the-art monocular SLAMsystems.</description><author>Heng Zhou, Zhetao Guo, Shuhong Liu, Lechen Zhang, Qihao Wang, Yuxiang Ren, Mingrui Li</author><pubDate>Fri, 08 Mar 2024 18:42:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03762v5</guid></item><item><title>Extend Your Own Correspondences: Unsupervised Distant Point Cloud Registration by Progressive Distance Extension</title><link>http://arxiv.org/abs/2403.03532v1</link><description>Registration of point clouds collected from a pair of distant vehiclesprovides a comprehensive and accurate 3D view of the driving scenario, which isvital for driving safety related applications, yet existing literature suffersfrom the expensive pose label acquisition and the deficiency to generalize tonew data distributions. In this paper, we propose EYOC, an unsupervised distantpoint cloud registration method that adapts to new point cloud distributions onthe fly, requiring no global pose labels. The core idea of EYOC is to train afeature extractor in a progressive fashion, where in each round, the featureextractor, trained with near point cloud pairs, can label slightly fartherpoint cloud pairs, enabling self-supervision on such far point cloud pairs.This process continues until the derived extractor can be used to registerdistant point clouds. Particularly, to enable high-fidelity correspondencelabel generation, we devise an effective spatial filtering scheme to select themost representative correspondences to register a point cloud pair, and thenutilize the aligned point clouds to discover more correct correspondences.Experiments show that EYOC can achieve comparable performance withstate-of-the-art supervised methods at a lower training cost. Moreover, itoutwits supervised methods regarding generalization performance on new datadistributions.</description><author>Quan Liu, Hongzi Zhu, Zhenxi Wang, Yunsong Zhou, Shan Chang, Minyi Guo</author><pubDate>Wed, 06 Mar 2024 08:18:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.03532v1</guid></item><item><title>Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis</title><link>http://arxiv.org/abs/2403.04116v1</link><description>X-ray is widely applied for transmission imaging due to its strongerpenetration than natural light. When rendering novel view X-ray projections,existing methods mainly based on NeRF suffer from long training time and slowinference speed. In this paper, we propose a 3D Gaussian splatting-basedframework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, weredesign a radiative Gaussian point cloud model inspired by the isotropicnature of X-ray imaging. Our model excludes the influence of view directionwhen learning to predict the radiation intensity of 3D points. Based on thismodel, we develop a Differentiable Radiative Rasterization (DRR) with CUDAimplementation. Secondly, we customize an Angle-pose Cuboid UniformInitialization (ACUI) strategy that directly uses the parameters of the X-rayscanner to compute the camera information and then uniformly samples pointpositions within a cuboid enclosing the scanned object. Experiments show thatour X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoyingless than 15% training time and over 73x inference speed. The application onsparse-view CT reconstruction also reveals the practical values of our method.Code and models will be publicly available athttps://github.com/caiyuanhao1998/X-Gaussian . A video demo of the trainingprocess visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .</description><author>Yuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, Alan Yuille</author><pubDate>Thu, 07 Mar 2024 00:12:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04116v1</guid></item><item><title>MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection</title><link>http://arxiv.org/abs/2402.11677v1</link><description>Multi-modal 3D object detection models for automated driving havedemonstrated exceptional performance on computer vision benchmarks likenuScenes. However, their reliance on densely sampled LiDAR point clouds andmeticulously calibrated sensor arrays poses challenges for real-worldapplications. Issues such as sensor misalignment, miscalibration, and disparatesampling frequencies lead to spatial and temporal misalignment in data fromLiDAR and cameras. Additionally, the integrity of LiDAR and camera data isoften compromised by adverse environmental conditions such as inclementweather, leading to occlusions and noise interference. To address thischallenge, we introduce MultiCorrupt, a comprehensive benchmark designed toevaluate the robustness of multi-modal 3D object detectors against ten distincttypes of corruptions. We evaluate five state-of-the-art multi-modal detectorson MultiCorrupt and analyze their performance in terms of their resistanceability. Our results show that existing methods exhibit varying degrees ofrobustness depending on the type of corruption and their fusion strategy. Weprovide insights into which multi-modal design choices make such models robustagainst certain perturbations. The dataset generation code and benchmark areopen-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt.</description><author>Till Beemelmanns, Quan Zhang, Lutz Eckstein</author><pubDate>Sun, 18 Feb 2024 18:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11677v1</guid></item><item><title>Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image</title><link>http://arxiv.org/abs/2403.09632v1</link><description>At the core of portrait photography is the search for ideal lighting andviewpoint. The process often requires advanced knowledge in photography and anelaborate studio setup. In this work, we propose Holo-Relighting, a volumetricrelighting method that is capable of synthesizing novel viewpoints, and novellighting from a single image. Holo-Relighting leverages the pretrained 3D GAN(EG3D) to reconstruct geometry and appearance from an input portrait as a setof 3D-aware features. We design a relighting module conditioned on a givenlighting to process these features, and predict a relit 3D representation inthe form of a tri-plane, which can render to an arbitrary viewpoint throughvolume rendering. Besides viewpoint and lighting control, Holo-Relighting alsotakes the head pose as a condition to enable head-pose-dependent lightingeffects. With these novel designs, Holo-Relighting can generate complexnon-Lambertian lighting effects (e.g., specular highlights and cast shadows)without using any explicit physical lighting priors. We train Holo-Relightingwith data captured with a light stage, and propose two data-renderingtechniques to improve the data quality for training the volumetric relightingsystem. Through quantitative and qualitative experiments, we demonstrateHolo-Relighting can achieve state-of-the-arts relighting quality with betterphotorealism, 3D consistency and controllability.</description><author>Yiqun Mei, Yu Zeng, He Zhang, Zhixin Shu, Xuaner Zhang, Sai Bi, Jianming Zhang, HyunJoon Jung, Vishal M. Patel</author><pubDate>Thu, 14 Mar 2024 18:58:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09632v1</guid></item><item><title>Dynamic 3D Point Cloud Sequences as 2D Videos</title><link>http://arxiv.org/abs/2403.01129v1</link><description>Dynamic 3D point cloud sequences serve as one of the most common andpractical representation modalities of dynamic real-world environments.However, their unstructured nature in both spatial and temporal domains posessignificant challenges to effective and efficient processing. Existing deeppoint cloud sequence modeling approaches imitate the mature 2D video learningmechanisms by developing complex spatio-temporal point neighbor grouping andfeature aggregation schemes, often resulting in methods lacking effectiveness,efficiency, and expressive power. In this paper, we propose a novel genericrepresentation called \textit{Structured Point Cloud Videos} (SPCVs).Intuitively, by leveraging the fact that 3D geometric shapes are essentially 2Dmanifolds, SPCV re-organizes a point cloud sequence as a 2D video with spatialsmoothness and temporal consistency, where the pixel values correspond to the3D coordinates of points. The structured nature of our SPCV representationallows for the seamless adaptation of well-established 2D image/videotechniques, enabling efficient and effective processing and analysis of 3Dpoint cloud sequences. To achieve such re-organization, we design aself-supervised learning pipeline that is geometrically regularized and drivenby self-reconstructive and deformation field learning objectives. Additionally,we construct SPCV-based frameworks for both low-level and high-level 3D pointcloud sequence processing and analysis tasks, including action recognition,temporal interpolation, and compression. Extensive experiments demonstrate theversatility and superiority of the proposed SPCV, which has the potential tooffer new possibilities for deep learning on unstructured 3D point cloudsequences. Code will be released at https://github.com/ZENGYIMING-EAMON/SPCV.</description><author>Yiming Zeng, Junhui Hou, Qijian Zhang, Siyu Ren, Wenping Wang</author><pubDate>Sat, 02 Mar 2024 08:18:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01129v1</guid></item><item><title>UniMODE: Unified Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2402.18573v1</link><description>Realizing unified monocular 3D object detection, including both indoor andoutdoor scenes, holds great importance in applications like robot navigation.However, involving various scenarios of data to train models poses challengesdue to their significantly different characteristics, e.g., diverse geometryproperties and heterogeneous domain distributions. To address these challenges,we build a detector based on the bird's-eye-view (BEV) detection paradigm,where the explicit feature projection is beneficial to addressing the geometrylearning ambiguity when employing multiple scenarios of data to traindetectors. Then, we split the classical BEV detection architecture into twostages and propose an uneven BEV grid design to handle the convergenceinstability caused by the aforementioned challenges. Moreover, we develop asparse BEV feature projection strategy to reduce computational cost and aunified domain alignment method to handle heterogeneous domains. Combiningthese techniques, a unified detector UniMODE is derived, which surpasses theprevious state-of-the-art on the challenging Omni3D dataset (a large-scaledataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing thefirst successful generalization of a BEV detector to unified 3D objectdetection.</description><author>Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao</author><pubDate>Wed, 28 Feb 2024 18:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18573v1</guid></item><item><title>An Endoscopic Chisel: Intraoperative Imaging Carves 3D Anatomical Models</title><link>http://arxiv.org/abs/2402.11840v1</link><description>Purpose: Preoperative imaging plays a pivotal role in sinus surgery where CTsoffer patient-specific insights of complex anatomy, enabling real-timeintraoperative navigation to complement endoscopy imaging. However, surgeryelicits anatomical changes not represented in the preoperative model,generating an inaccurate basis for navigation during surgery progression. Methods: We propose a first vision-based approach to update the preoperative3D anatomical model leveraging intraoperative endoscopic video for navigatedsinus surgery where relative camera poses are known. We rely on comparisons ofintraoperative monocular depth estimates and preoperative depth renders toidentify modified regions. The new depths are integrated in these regionsthrough volumetric fusion in a truncated signed distance functionrepresentation to generate an intraoperative 3D model that reflects tissuemanipulation. Results: We quantitatively evaluate our approach by sequentially updatingmodels for a five-step surgical progression in an ex vivo specimen. We computethe error between correspondences from the updated model and ground-truthintraoperative CT in the region of anatomical modification. The resultingmodels show a decrease in error during surgical progression as opposed toincreasing when no update is employed. Conclusion: Our findings suggest that preoperative 3D anatomical models canbe updated using intraoperative endoscopy video in navigated sinus surgery.Future work will investigate improvements to monocular depth estimation as wellas removing the need for external navigation systems. The resulting ability tocontinuously update the patient model may provide surgeons with a more preciseunderstanding of the current anatomical state and paves the way toward adigital twin paradigm for sinus surgery.</description><author>Jan Emily Mangulabnan, Roger D. Soberanis-Mukul, Timo Teufel, Manish Sahu, Jose L. Porras, S. Swaroop Vedula, Masaru Ishii, Gregory Hager, Russell H. Taylor, Mathias Unberath</author><pubDate>Mon, 19 Feb 2024 05:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11840v1</guid></item><item><title>EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization</title><link>http://arxiv.org/abs/2402.13537v1</link><description>Camera relocalization is pivotal in computer vision, with applications in AR,drones, robotics, and autonomous driving. It estimates 3D camera position andorientation (6-DoF) from images. Unlike traditional methods like SLAM, recentstrides use deep learning for direct end-to-end pose estimation. We proposeEffLoc, a novel efficient Vision Transformer for single-image camerarelocalization. EffLoc's hierarchical layout, memory-bound self-attention, andfeed-forward layers boost memory efficiency and inter-channel communication.Our introduced sequential group attention (SGA) module enhances computationalefficiency by diversifying input features, reducing redundancy, and expandingmodel capacity. EffLoc excels in efficiency and accuracy, outperforming priormethods, such as AtLoc and MapNet. It thrives on large-scale outdoorcar-driving scenario, ensuring simplicity, end-to-end trainability, andeliminating handcrafted loss functions.</description><author>Zhendong Xiao, Changhao Chen, Shan Yang, Wu Wei</author><pubDate>Wed, 21 Feb 2024 05:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13537v1</guid></item><item><title>Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration Framework</title><link>http://arxiv.org/abs/2403.06658v1</link><description>Large vision models based in deep learning architectures have beenconsistently advancing the state-of-the-art in biometric recognition. However,three weaknesses are commonly reported for such kind of approaches: 1) theirextreme demands in terms of learning data; 2) the difficulties in generalisingbetween different domains; and 3) the lack of interpretability/explainability,with biometrics being of particular interest, as it is important to provideevidence able to be used for forensics/legal purposes (e.g., in courts). To thebest of our knowledge, this paper describes the first recognitionframework/strategy that aims at addressing the three weaknesses simultaneously.At first, it relies exclusively in synthetic samples for learning purposes.Instead of requiring a large amount and variety of samples for each subject,the idea is to exclusively enroll a 3D point cloud per identity. Then, usinggenerative strategies, we synthesize a very large (potentially infinite) numberof samples, containing all the desired covariates (poses, clothing, distances,perspectives, lighting, occlusions,...). Upon the synthesizing method used, itis possible to adapt precisely to different kind of domains, which accounts forgeneralization purposes. Such data are then used to learn a model that performslocal registration between image pairs, establishing positive correspondencesbetween body parts that are the key, not only to recognition (according tocardinality and distribution), but also to provide an interpretable descriptionof the response (e.g.: "both samples are from the same person, as they havesimilar facial shape, hair color and legs thickness").</description><author>Henrique Jesus, Hugo Proença</author><pubDate>Mon, 11 Mar 2024 13:27:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06658v1</guid></item><item><title>Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery</title><link>http://arxiv.org/abs/2403.09063v1</link><description>Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidablechallenge and is often hindered by depth ambiguities and reduced precision.Existing works resort to either pose priors or multi-modal data such asmulti-view or point cloud information, though their methods often overlook thevaluable scene-depth information inherently present in a single image.Moreover, achieving robust HMR for out-of-distribution (OOD) data isexceedingly challenging due to inherent variations in pose, shape and depth.Consequently, understanding the underlying distribution becomes a vitalsubproblem in modeling human forms. Motivated by the need for unambiguous androbust human modeling, we introduce Distribution and depth-aware human meshrecovery (D2A-HMR), an end-to-end transformer architecture meticulouslydesigned to minimize the disparity between distributions and incorporatescene-depth leveraging prior depth information. Our approach demonstratessuperior performance in handling OOD data in certain scenarios whileconsistently achieving competitive results against state-of-the-art HMR methodson controlled datasets.</description><author>Jerrin Bright, Bavesh Balaji, Harish Prakash, Yuhao Chen, David A Clausi, John Zelek</author><pubDate>Thu, 14 Mar 2024 04:07:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09063v1</guid></item><item><title>OBMO: One Bounding Box Multiple Objects for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2212.10049v2</link><description>Compared to typical multi-sensor systems, monocular 3D object detection hasattracted much attention due to its simple configuration. However, there isstill a significant gap between LiDAR-based and monocular-based methods. Inthis paper, we find that the ill-posed nature of monocular imagery can lead todepth ambiguity. Specifically, objects with different depths can appear withthe same bounding boxes and similar visual features in the 2D image.Unfortunately, the network cannot accurately distinguish different depths fromsuch non-discriminative visual features, resulting in unstable depth training.To facilitate depth learning, we propose a simple yet effective plug-and-playmodule, \underline{O}ne \underline{B}ounding Box \underline{M}ultiple\underline{O}bjects (OBMO). Concretely, we add a set of suitable pseudo labelsby shifting the 3D bounding box along the viewing frustum. To constrain thepseudo-3D labels to be reasonable, we carefully design two label scoringstrategies to represent their quality. In contrast to the original hard depthlabels, such soft pseudo labels with quality scores allow the network to learna reasonable depth range, boosting training stability and thus improving finalperformance. Extensive experiments on KITTI and Waymo benchmarks show that ourmethod significantly improves state-of-the-art monocular 3D detectors by asignificant margin (The improvements under the moderate setting on KITTIvalidation set are $\mathbf{1.82\sim 10.91\%}$ \textbf{mAP in BEV} and$\mathbf{1.18\sim 9.36\%}$ \textbf{mAP in 3D}). Codes have been released at\url{https://github.com/mrsempress/OBMO}.</description><author>Chenxi Huang, Tong He, Haidong Ren, Wenxiao Wang, Binbin Lin, Deng Cai</author><pubDate>Tue, 20 Feb 2024 08:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10049v2</guid></item><item><title>PIFu for the Real World: A Self-supervised Framework to Reconstruct Dressed Human from Single-view Images</title><link>http://arxiv.org/abs/2208.10769v2</link><description>It is very challenging to accurately reconstruct sophisticated human geometrycaused by various poses and garments from a single image. Recently, works basedon pixel-aligned implicit function (PIFu) have made a big step and achievedstate-of-the-art fidelity on image-based 3D human digitization. However, thetraining of PIFu relies heavily on expensive and limited 3D ground truth data(i.e. synthetic data), thus hindering its generalization to more diverse realworld images. In this work, we propose an end-to-end self-supervised networknamed SelfPIFu to utilize abundant and diverse in-the-wild images, resulting inlargely improved reconstructions when tested on unconstrained in-the-wildimages. At the core of SelfPIFu is the depth-guided volume-/surface-awaresigned distance fields (SDF) learning, which enables self-supervised learningof a PIFu without access to GT mesh. The whole framework consists of a normalestimator, a depth estimator, and a SDF-based PIFu and better utilizes extradepth GT during training. Extensive experiments demonstrate the effectivenessof our self-supervised framework and the superiority of using depth as input.On synthetic data, our Intersection-Over-Union (IoU) achieves to 93.5%, 18%higher compared with PIFuHD. For in-the-wild images, we conduct user studies onthe reconstructed results, the selection rate of our results is over 68%compared with other state-of-the-art methods.</description><author>Zhangyang Xiong, Dong Du, Yushuang Wu, Jingqi Dong, Di Kang, Linchao Bao, Xiaoguang Han</author><pubDate>Fri, 08 Mar 2024 08:16:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.10769v2</guid></item><item><title>RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes</title><link>http://arxiv.org/abs/2309.01513v2</link><description>Room geometry is important prior information for implementing realistic 3Daudio rendering. For this reason, various room geometry inference (RGI) methodshave been developed by utilizing the time of arrival (TOA) or time differenceof arrival (TDOA) information in room impulse responses. However, theconventional RGI technique poses several assumptions, such as convex roomshapes, the number of walls known in priori, and the visibility of first-orderreflections. In this work, we introduce the deep neural network (DNN), RGI-Net,which can estimate room geometries without the aforementioned assumptions.RGI-Net learns and exploits complex relationships between high-orderreflections in room impulse responses (RIRs) and, thus, can estimate roomshapes even when the shape is non-convex or first-order reflections are missingin the RIRs. The network takes RIRs measured from a compact audio deviceequipped with a circular microphone array and a single loudspeaker, whichgreatly improves its practical applicability. RGI-Net includes the evaluationnetwork that separately evaluates the presence probability of walls, so thegeometry inference is possible without prior knowledge of the number of walls.</description><author>Inmo Yeon, Jung-Woo Choi</author><pubDate>Wed, 21 Feb 2024 06:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01513v2</guid></item><item><title>VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks</title><link>http://arxiv.org/abs/2402.13609v2</link><description>In recent years, object-oriented simultaneous localization and mapping (SLAM)has attracted increasing attention due to its ability to provide high-levelsemantic information while maintaining computational efficiency. Someresearchers have attempted to enhance localization accuracy by integrating themodeled object residuals into bundle adjustment. However, few have demonstratedbetter results than feature-based visual SLAM systems, as the generic coarseobject models, such as cuboids or ellipsoids, are less accurate than featurepoints. In this paper, we propose a Visual Object Odometry and Mappingframework VOOM using high-level objects and low-level points as thehierarchical landmarks in a coarse-to-fine manner instead of directly usingobject residuals in bundle adjustment. Firstly, we introduce an improvedobservation model and a novel data association method for dual quadrics,employed to represent physical objects. It facilitates the creation of a 3D mapthat closely reflects reality. Next, we use object information to enhance thedata association of feature points and consequently update the map. In thevisual object odometry backend, the updated map is employed to further optimizethe camera pose and the objects. Meanwhile, local bundle adjustment isperformed utilizing the objects and points-based covisibility graphs in ourvisual object mapping process. Experiments show that VOOM outperforms bothobject-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in termsof localization. The implementation of our method is available athttps://github.com/yutongwangBIT/VOOM.git.</description><author>Yutong Wang, Chaoyang Jiang, Xieyuanli Chen</author><pubDate>Mon, 26 Feb 2024 10:02:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13609v2</guid></item><item><title>VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks</title><link>http://arxiv.org/abs/2402.13609v1</link><description>In recent years, object-oriented simultaneous localization and mapping (SLAM)has attracted increasing attention due to its ability to provide high-levelsemantic information while maintaining computational efficiency. Someresearchers have attempted to enhance localization accuracy by integrating themodeled object residuals into bundle adjustment. However, few have demonstratedbetter results than feature-based visual SLAM systems, as the generic coarseobject models, such as cuboids or ellipsoids, are less accurate than featurepoints. In this paper, we propose a Visual Object Odometry and Mappingframework VOOM using high-level objects and low-level points as thehierarchical landmarks in a coarse-to-fine manner instead of directly usingobject residuals in bundle adjustment. Firstly, we introduce an improvedobservation model and a novel data association method for dual quadrics,employed to represent physical objects. It facilitates the creation of a 3D mapthat closely reflects reality. Next, we use object information to enhance thedata association of feature points and consequently update the map. In thevisual object odometry backend, the updated map is employed to further optimizethe camera pose and the objects. Meanwhile, local bundle adjustment isperformed utilizing the objects and points-based covisibility graphs in ourvisual object mapping process. Experiments show that VOOM outperforms bothobject-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in termsof localization. The implementation of our method is available athttps://github.com/yutongwangBIT/VOOM.git.</description><author>Yutong Wang, Chaoyang Jiang, Xieyuanli Chen</author><pubDate>Wed, 21 Feb 2024 08:22:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13609v1</guid></item><item><title>GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects</title><link>http://arxiv.org/abs/2309.16264v3</link><description>Articulated objects like cabinets and doors are widespread in daily life.However, directly manipulating 3D articulated objects is challenging becausethey have diverse geometrical shapes, semantic categories, and kineticconstraints. Prior works mostly focused on recognizing and manipulatingarticulated objects with specific joint types. They can either estimate thejoint parameters or distinguish suitable grasp poses to facilitate trajectoryplanning. Although these approaches have succeeded in certain types ofarticulated objects, they lack generalizability to unseen objects, whichsignificantly impedes their application in broader scenarios. In this paper, wepropose a novel framework of Generalizable Articulation Modeling andManipulating for Articulated Objects (GAMMA), which learns both articulationmodeling and grasp pose affordance from diverse articulated objects withdifferent categories. In addition, GAMMA adopts adaptive manipulation toiteratively reduce the modeling errors and enhance manipulation performance. Wetrain GAMMA with the PartNet-Mobility dataset and evaluate with comprehensiveexperiments in SAPIEN simulation and real-world Franka robot. Results show thatGAMMA significantly outperforms SOTA articulation modeling and manipulationalgorithms in unseen and cross-category articulated objects. We willopen-source all codes and datasets in both simulation and real robots forreproduction in the final version. Images and videos are published on theproject website at: http://sites.google.com/view/gamma-articulation</description><author>Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang, Cewu Lu</author><pubDate>Fri, 01 Mar 2024 13:29:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16264v3</guid></item><item><title>DyST: Towards Dynamic Neural Scene Representations on Real-World Videos</title><link>http://arxiv.org/abs/2310.06020v2</link><description>Visual understanding of the world goes beyond the semantics and flatstructure of individual images. In this work, we aim to capture both the 3Dstructure and dynamics of real-world scenes from monocular real-world videos.Our Dynamic Scene Transformer (DyST) model leverages recent work in neuralscene representation to learn a latent decomposition of monocular real-worldvideos into scene content, per-view scene dynamics, and camera pose. Thisseparation is achieved through a novel co-training scheme on monocular videosand our new synthetic dataset DySO. DyST learns tangible latent representationsfor dynamic scenes that enable view generation with separate control over thecamera and the content of the scene.</description><author>Maximilian Seitzer, Sjoerd van Steenkiste, Thomas Kipf, Klaus Greff, Mehdi S. M. Sajjadi</author><pubDate>Fri, 15 Mar 2024 14:53:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06020v2</guid></item><item><title>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</title><link>http://arxiv.org/abs/2402.03246v3</link><description>Semantic understanding plays a crucial role in Dense SimultaneousLocalization and Mapping (SLAM). Recent advancements that integrate GaussianSplatting into SLAM systems have demonstrated its effectiveness in generatinghigh-quality renderings. Building on this progress, we propose SGS-SLAM whichprovides precise 3D semantic segmentation alongside high-fidelityreconstructions. Specifically, we propose to employ multi-channel optimizationduring the mapping process, integrating appearance, geometric, and semanticconstraints with key-frame optimization to enhance reconstruction quality.Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-artperformance in camera pose estimation, map reconstruction, and semanticsegmentation. It outperforms existing methods by a large margin meanwhilepreserving real-time rendering ability.</description><author>Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang</author><pubDate>Sat, 02 Mar 2024 13:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03246v3</guid></item><item><title>NECA: Neural Customizable Human Avatar</title><link>http://arxiv.org/abs/2403.10335v1</link><description>Human avatar has become a novel type of 3D asset with various applications.Ideally, a human avatar should be fully customizable to accommodate differentsettings and environments. In this work, we introduce NECA, an approach capableof learning versatile human representation from monocular or sparse-viewvideos, enabling granular customization across aspects such as pose, shadow,shape, lighting and texture. The core of our approach is to represent humans incomplementary dual spaces and predict disentangled neural fields of geometry,albedo, shadow, as well as an external lighting, from which we are able toderive realistic rendering with high-frequency details via volumetricrendering. Extensive experiments demonstrate the advantage of our method overthe state-of-the-art methods in photorealistic rendering, as well as variousediting tasks such as novel pose synthesis and relighting. The code isavailable at https://github.com/iSEE-Laboratory/NECA.</description><author>Junjin Xiao, Qing Zhang, Zhan Xu, Wei-Shi Zheng</author><pubDate>Fri, 15 Mar 2024 15:23:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10335v1</guid></item><item><title>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</title><link>http://arxiv.org/abs/2402.03246v2</link><description>Semantic understanding plays a crucial role in Dense SimultaneousLocalization and Mapping (SLAM). Recent advancements that integrate GaussianSplatting into SLAM systems have demonstrated its effectiveness in generatinghigh-quality renderings. Building on this progress, we propose SGS-SLAM whichprovides precise 3D semantic segmentation alongside high-fidelityreconstructions. Specifically, we propose to employ multi-channel optimizationduring the mapping process, integrating appearance, geometric, and semanticconstraints with key-frame optimization to enhance reconstruction quality.Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-artperformance in camera pose estimation, map reconstruction, and semanticsegmentation. It outperforms existing methods by a large margin meanwhilepreserves real-time rendering ability.</description><author>Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang</author><pubDate>Sun, 25 Feb 2024 17:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03246v2</guid></item><item><title>S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR</title><link>http://arxiv.org/abs/2402.14461v1</link><description>Scene graph generation (SGG) of surgical procedures is crucial in enhancingholistically cognitive intelligence in the operating room (OR). However,previous works have primarily relied on the multi-stage learning that generatessemantic scene graphs dependent on intermediate processes with pose estimationand object detection, which may compromise model efficiency and efficacy, alsoimpose extra annotation burden. In this study, we introduce a novelsingle-stage bimodal transformer framework for SGG in the OR, termedS^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3Dpoint clouds for SGG in an end-to-end manner. Concretely, our model embraces aView-Sync Transfusion scheme to encourage multi-view visual informationinteraction. Concurrently, a Geometry-Visual Cohesion operation is designed tointegrate the synergic 2D semantic features into 3D point cloud features.Moreover, based on the augmented feature, we propose a novel relation-sensitivetransformer decoder that embeds dynamic entity-pair queries and relationaltrait priors, which enables the direct prediction of entity-pair relations forgraph generation without intermediate steps. Extensive experiments havevalidated the superior SGG performance and lower computational cost ofS^2Former-OR on 4D-OR benchmark, compared with current OR-SGG methods, e.g., 3%Precision increase and 24.2M reduction in model parameters. We further comparedour method with generic single-stage SGG methods with broader metrics for acomprehensive evaluation, with consistently better performance achieved. Thecode will be made available.</description><author>Jialun Pei, Diandian Guo, Jingyang Zhang, Manxi Lin, Yueming Jin, Pheng-Ann Heng</author><pubDate>Thu, 22 Feb 2024 11:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14461v1</guid></item><item><title>ActFormer: Scalable Collaborative Perception via Active Queries</title><link>http://arxiv.org/abs/2403.04968v1</link><description>Collaborative perception leverages rich visual observations from multiplerobots to extend a single robot's perception ability beyond its field of view.Many prior works receive messages broadcast from all collaborators, leading toa scalability challenge when dealing with a large number of robots and sensors.In this work, we aim to address \textit{scalable camera-based collaborativeperception} with a Transformer-based architecture. Our key idea is to enable asingle robot to intelligently discern the relevance of the collaborators andtheir associated cameras according to a learned spatial prior. This proactiveunderstanding of the visual features' relevance does not require thetransmission of the features themselves, enhancing both communication andcomputation efficiency. Specifically, we present ActFormer, a Transformer thatlearns bird's eye view (BEV) representations by using predefined BEV queries tointeract with multi-robot multi-camera inputs. Each BEV query can activelyselect relevant cameras for information aggregation based on pose information,instead of interacting with all cameras indiscriminately. Experiments on theV2X-Sim dataset demonstrate that ActFormer improves the detection performancefrom 29.89% to 45.15% in terms of AP@0.7 with about 50% fewer queries,showcasing the effectiveness of ActFormer in multi-agent collaborative 3Dobject detection.</description><author>Suozhi Huang, Juexiao Zhang, Yiming Li, Chen Feng</author><pubDate>Fri, 08 Mar 2024 00:45:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04968v1</guid></item><item><title>RMS: Redundancy-Minimizing Point Cloud Sampling for Real-Time Pose Estimation</title><link>http://arxiv.org/abs/2312.07337v2</link><description>The typical point cloud sampling methods used in state estimation for mobilerobots preserve a high level of point redundancy. This redundancy unnecessarilyslows down the estimation pipeline and may cause drift under real-timeconstraints. Such undue latency becomes a bottleneck for resource-constrainedrobots (especially UAVs), requiring minimal delay for agile and accurateoperation. We propose a novel, deterministic, uninformed, and single-parameterpoint cloud sampling method named RMS that minimizes redundancy within a 3Dpoint cloud. In contrast to the state of the art, RMS balances thetranslation-space observability by leveraging the fact that linear and planarsurfaces inherently exhibit high redundancy propagated into iterativeestimation pipelines. We define the concept of gradient flow, quantifying thelocal surface underlying a point. We also show that maximizing the entropy ofthe gradient flow minimizes point redundancy for robot ego-motion estimation.We integrate RMS into the point-based KISS-ICP and feature-based LOAM odometrypipelines and evaluate experimentally on KITTI, Hilti-Oxford, and customdatasets from multirotor UAVs. The experiments demonstrate that RMS outperformsstate-of-the-art methods in speed, compression, and accuracy inwell-conditioned as well as in geometrically-degenerated settings.</description><author>Pavel Petracek, Kostas Alexis, Martin Saska</author><pubDate>Thu, 29 Feb 2024 13:05:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07337v2</guid></item><item><title>The NeRFect Match: Exploring NeRF Features for Visual Localization</title><link>http://arxiv.org/abs/2403.09577v1</link><description>In this work, we propose the use of Neural Radiance Fields (NeRF) as a scenerepresentation for visual localization. Recently, NeRF has been employed toenhance pose regression and scene coordinate regression models by augmentingthe training database, providing auxiliary supervision through rendered images,or serving as an iterative refinement module. We extend its recognizedadvantages -- its ability to provide a compact scene representation withrealistic appearances and accurate geometry -- by exploring the potential ofNeRF's internal features in establishing precise 2D-3D matches forlocalization. To this end, we conduct a comprehensive examination of NeRF'simplicit knowledge, acquired through view synthesis, for matching under variousconditions. This includes exploring different matching network architectures,extracting encoder features at multiple layers, and varying trainingconfigurations. Significantly, we introduce NeRFMatch, an advanced 2D-3Dmatching function that capitalizes on the internal knowledge of NeRF learnedvia view synthesis. Our evaluation of NeRFMatch on standard localizationbenchmarks, within a structure-based pipeline, sets a new state-of-the-art forlocalization performance on Cambridge Landmarks.</description><author>Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé</author><pubDate>Thu, 14 Mar 2024 18:11:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09577v1</guid></item><item><title>SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios</title><link>http://arxiv.org/abs/2403.09317v1</link><description>Despite the success in 6D pose estimation in bin-picking scenarios, existingmethods still struggle to produce accurate prediction results for symmetryobjects and real world scenarios. The primary bottlenecks include 1) theambiguity keypoints caused by object symmetries; 2) the domain gap between realand synthetic data. To circumvent these problem, we propose a new 6D poseestimation network with symmetric-aware keypoint prediction and self-trainingdomain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression anddeep hough voting to perform reliable detection keypoint under clutter andocclusion. Specifically, at the keypoint prediction stage, we designe a robust3D keypoints selection strategy considering the symmetry class of objects andequivalent keypoints, which facilitate locating 3D keypoints even in highlyoccluded scenes. Additionally, we build an effective filtering algorithm onpredicted keypoint to dynamically eliminate multiple ambiguity and outlierkeypoint candidates. At the domain adaptation stage, we propose theself-training framework using a student-teacher training scheme. To carefullydistinguish reliable predictions, we harnesses a tailored heuristics for 3Dgeometry pseudo labelling based on semi-chamfer distance. On public Sil'eanedataset, SD-Net achieves state-of-the-art results, obtaining an averageprecision of 96%. Testing learning and generalization abilities on publicParametric datasets, SD-Net is 8% higher than the state-of-the-art method. Thecode is available at https://github.com/dingthuang/SD-Net.</description><author>Ding-Tao Huang, En-Te Lin, Lipeng Chen, Li-Fu Liu, Long Zeng</author><pubDate>Thu, 14 Mar 2024 13:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09317v1</guid></item></channel></rss>