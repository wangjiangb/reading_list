<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 04 May 2023 14:21:43 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</title><link>http://arxiv.org/abs/2206.02257v3</link><description>In this survey, we present a systematic review of 3D hand pose estimationfrom the perspective of efficient annotation and learning. 3D hand poseestimation has been an important research area owing to its potential to enablevarious applications, such as video understanding, AR/VR, and robotics.However, the performance of models is tied to the quality and quantity ofannotated 3D hand poses. Under the status quo, acquiring such annotated 3D handposes is challenging, e.g., due to the difficulty of 3D annotation and thepresence of occlusion. To reveal this problem, we review the pros and cons ofexisting annotation methods classified as manual, synthetic-model-based,hand-sensor-based, and computational approaches. Additionally, we examinemethods for learning 3D hand poses when annotated data are scarce, includingself-supervised pretraining, semi-supervised learning, and domain adaptation.Based on the study of efficient annotation and learning, we further discusslimitations and possible future directions in this field.</description><author>Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato</author><pubDate>Wed, 26 Apr 2023 07:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02257v3</guid></item><item><title>Interweaved Graph and Attention Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2304.14045v1</link><description>Despite substantial progress in 3D human pose estimation from a single-viewimage, prior works rarely explore global and local correlations, leading toinsufficient learning of human skeleton representations. To address this issue,we propose a novel Interweaved Graph and Attention Network (IGANet) that allowsbidirectional communications between graph convolutional networks (GCNs) andattentions. Specifically, we introduce an IGA module, where attentions areprovided with local information from GCNs and GCNs are injected with globalinformation from attentions. Additionally, we design a simple yet effectiveU-shaped multi-layer perceptron (uMLP), which can capture multi-granularityinformation for body joints. Extensive experiments on two popular benchmarkdatasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate ourproposed method.The results show that IGANet achieves state-of-the-artperformance on both datasets. Code is available athttps://github.com/xiu-cs/IGANet.</description><author>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li</author><pubDate>Thu, 27 Apr 2023 10:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14045v1</guid></item><item><title>Occlusion Robust 3D Human Pose Estimation with StridedPoseGraphFormer and Data Augmentation</title><link>http://arxiv.org/abs/2304.12069v1</link><description>Occlusion is an omnipresent challenge in 3D human pose estimation (HPE). Inspite of the large amount of research dedicated to 3D HPE, only a limitednumber of studies address the problem of occlusion explicitly. To fill thisgap, we propose to combine exploitation of spatio-temporal features withsynthetic occlusion augmentation during training to deal with occlusion. Tothis end, we build a spatio-temporal 3D HPE model, StridedPoseGraphFormer basedon graph convolution and transformers, and train it using occlusionaugmentation. Unlike the existing occlusion-aware methods, that are only testedfor limited occlusion, we extensively evaluate our method for varying degreesof occlusion. We show that our proposed method compares favorably with thestate-of-the-art (SoA). Our experimental results also reveal that in theabsence of any occlusion handling mechanism, the performance of SoA 3D HPEmethods degrades significantly when they encounter occlusion.</description><author>Soubarna Banik, Patricia Gscho√ümann, Alejandro Mendoza Garcia, Alois Knoll</author><pubDate>Mon, 24 Apr 2023 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12069v1</guid></item><item><title>TAPE: Temporal Attention-based Probabilistic human pose and shape Estimation</title><link>http://arxiv.org/abs/2305.00181v1</link><description>Reconstructing 3D human pose and shape from monocular videos is awell-studied but challenging problem. Common challenges include occlusions, theinherent ambiguities in the 2D to 3D mapping and the computational complexityof video processing. Existing methods ignore the ambiguities of thereconstruction and provide a single deterministic estimate for the 3D pose. Inorder to address these issues, we present a Temporal Attention basedProbabilistic human pose and shape Estimation method (TAPE) that operates on anRGB video. More specifically, we propose to use a neural network to encodevideo frames to temporal features using an attention-based neural network.Given these features, we output a per-frame but temporally-informed probabilitydistribution for the human pose using Normalizing Flows. We show that TAPEoutperforms state-of-the-art methods in standard benchmarks and serves as aneffective video-based prior for optimization-based human pose and shapeestimation. Code is available at: https: //github.com/nikosvasilik/TAPE</description><author>Nikolaos Vasilikopoulos, Nikos Kolotouros, Aggeliki Tsoli, Antonis Argyros</author><pubDate>Sat, 29 Apr 2023 07:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00181v1</guid></item><item><title>Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training</title><link>http://arxiv.org/abs/2206.11215v4</link><description>We consider a certifiable object pose estimation problem, where -- given apartial point cloud of an object -- the goal is to not only estimate the objectpose, but also to provide a certificate of correctness for the resultingestimate. Our first contribution is a general theory of certification forend-to-end perception models. In particular, we introduce the notion of$\zeta$-correctness, which bounds the distance between an estimate and theground truth. We show that $\zeta$-correctness can be assessed by implementingtwo certificates: (i) a certificate of observable correctness, that asserts ifthe model output is consistent with the input data and prior information, (ii)a certificate of non-degeneracy, that asserts whether the input data issufficient to compute a unique estimate. Our second contribution is to applythis theory and design a new learning-based certifiable pose estimator. Wepropose C-3PO, a semantic-keypoint-based pose estimation model, augmented withthe two certificates, to solve the certifiable pose estimation problem. C-3POalso includes a keypoint corrector, implemented as a differentiableoptimization layer, that can correct large detection errors (e.g. due to thesim-to-real gap). Our third contribution is a novel self-supervised trainingapproach that uses our certificate of observable correctness to provide thesupervisory signal to C-3PO during training. In it, the model trains only onthe observably correct input-output pairs, in each training iteration. Astraining progresses, we see that the observably correct input-output pairsgrow, eventually reaching near 100% in many cases. Our experiments show that(i) standard semantic-keypoint-based methods outperform more recentalternatives, (ii) C-3PO further improves performance and significantlyoutperforms all the baselines, and (iii) C-3PO's certificates are able todiscern correct pose estimates.</description><author>Rajat Talak, Lisa Peng, Luca Carlone</author><pubDate>Fri, 28 Apr 2023 20:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11215v4</guid></item><item><title>ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with Unsupervised Implicit Pose Embedding</title><link>http://arxiv.org/abs/2304.14005v1</link><description>Although 3D-aware GANs based on neural radiance fields have achievedcompetitive performance, their applicability is still limited to objects orscenes with the ground-truths or prediction models for clearly definedcanonical camera poses. To extend the scope of applicable datasets, we proposea novel 3D-aware GAN optimization technique through contrastive learning withimplicit pose embeddings. To this end, we first revise the discriminator designand remove dependency on ground-truth camera poses. Then, to capture complexand challenging 3D scene structures more effectively, we make the discriminatorestimate a high-dimensional implicit pose embedding from a given image andperform contrastive learning on the pose embedding. The proposed approach canbe employed for the dataset, where the canonical camera pose is ill-definedbecause it does not look up or estimate camera poses. Experimental results showthat our algorithm outperforms existing methods by large margins on thedatasets with multiple object categories and inconsistent canonical cameraposes.</description><author>Mijeoong Kim, Hyunjoon Lee, Bohyung Han</author><pubDate>Thu, 27 Apr 2023 08:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14005v1</guid></item><item><title>OriCon3D: Effective 3D Object Detection using Orientation and Confidence</title><link>http://arxiv.org/abs/2304.14484v1</link><description>We introduce a technique for detecting 3D objects and estimating theirposition from a single image. Our method is built on top of a similarstate-of-the-art technique [1], but with improved accuracy. The approachfollowed in this research first estimates common 3D properties of an objectusing a Deep Convolutional Neural Network (DCNN), contrary to other frameworksthat only leverage centre-point predictions. We then combine these estimateswith geometric constraints provided by a 2D bounding box to produce a complete3D bounding box. The first output of our network estimates the 3D objectorientation using a discrete-continuous loss [1]. The second output predictsthe 3D object dimensions with minimal variance. Here we also present ourextensions by augmenting light-weight feature extractors and a customizedmultibin architecture. By combining these estimates with the geometricconstraints of the 2D bounding box, we can accurately (or comparatively)determine the 3D object pose better than our baseline [1] on the KITTI 3Ddetection benchmark [2].</description><author>Dhyey Manish Rajani, Rahul Kashyap Swayampakula, Surya Pratap Singh</author><pubDate>Thu, 27 Apr 2023 20:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14484v1</guid></item><item><title>gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction</title><link>http://arxiv.org/abs/2304.11970v1</link><description>Signed distance functions (SDFs) is an attractive framework that has recentlyshown promising results for 3D shape reconstruction from images. SDFsseamlessly generalize to different shape resolutions and topologies but lackexplicit modelling of the underlying 3D geometry. In this work, we exploit thehand structure and use it as guidance for SDF-based shape reconstruction. Inparticular, we address reconstruction of hands and manipulated objects frommonocular RGB images. To this end, we estimate poses of hands and objects anduse them to guide 3D reconstruction. More specifically, we predict kinematicchains of pose transformations and align SDFs with highly-articulated handposes. We improve the visual features of 3D points with geometry alignment andfurther leverage temporal information to enhance the robustness to occlusionand motion blurs. We conduct extensive experiments on the challenging ObMan andDexYCB benchmarks and demonstrate significant improvements of the proposedmethod over the state of the art.</description><author>Zerui Chen, Shizhe Chen, Cordelia Schmid, Ivan Laptev</author><pubDate>Mon, 24 Apr 2023 11:05:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11970v1</guid></item><item><title>CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction</title><link>http://arxiv.org/abs/2304.14633v1</link><description>Recent advances in neural reconstruction using posed image sequences havemade remarkable progress. However, due to the lack of depth information,existing volumetric-based techniques simply duplicate 2D image features of theobject surface along the entire camera ray. We contend this duplicationintroduces noise in empty and occluded spaces, posing challenges for producinghigh-quality 3D geometry. Drawing inspiration from traditional multi-viewstereo methods, we propose an end-to-end 3D neural reconstruction frameworkCVRecon, designed to exploit the rich geometric embedding in the cost volumesto facilitate 3D geometric feature learning. Furthermore, we presentRay-contextual Compensated Cost Volume (RCCV), a novel 3D geometric featurerepresentation that encodes view-dependent information with improved integrityand robustness. Through comprehensive experiments, we demonstrate that ourapproach significantly improves the reconstruction quality in various metricsand recovers clear fine details of the 3D geometries. Our extensive ablationstudies provide insights into the development of effective 3D geometric featurelearning schemes. Project page: https://cvrecon.ziyue.cool/</description><author>Ziyue Feng, Leon Yang, Pengsheng Guo, Bing Li</author><pubDate>Fri, 28 Apr 2023 06:30:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14633v1</guid></item><item><title>ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation</title><link>http://arxiv.org/abs/2305.01618v1</link><description>We propose a new dataset and a novel approach to learning hand-objectinteraction priors for hand and articulated object pose estimation. We firstcollect a dataset using visual teleoperation, where the human operator candirectly play within a physical simulator to manipulate the articulatedobjects. We record the data and obtain free and accurate annotations on objectposes and contact information from the simulator. Our system only requires aniPhone to record human hand motion, which can be easily scaled up and largelylower the costs of data and annotation collection. With this data, we learn 3Dinteraction priors including a discriminator (in a GAN) capturing thedistribution of how object parts are arranged, and a diffusion model whichgenerates the contact regions on articulated objects, guiding the hand poseestimation. Such structural and contact priors can easily transfer toreal-world data with barely any domain gap. By using our data and learnedpriors, our method significantly improves the performance on joint hand andarticulated object poses estimation over the existing state-of-the-art methods.The project is available at https://zehaozhu.github.io/ContactArt/ .</description><author>Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang</author><pubDate>Tue, 02 May 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01618v1</guid></item><item><title>Depth-based 6DoF Object Pose Estimation using Swin Transformer</title><link>http://arxiv.org/abs/2303.02133v2</link><description>Accurately estimating the 6D pose of objects is crucial for manyapplications, such as robotic grasping, autonomous driving, and augmentedreality. However, this task becomes more challenging in poor lightingconditions or when dealing with textureless objects. To address this issue,depth images are becoming an increasingly popular choice due to theirinvariance to a scene's appearance and the implicit incorporation of essentialgeometric characteristics. However, fully leveraging depth information toimprove the performance of pose estimation remains a difficult andunder-investigated problem. To tackle this challenge, we propose a novelframework called SwinDePose, that uses only geometric information from depthimages to achieve accurate 6D pose estimation. SwinDePose first calculates theangles between each normal vector defined in a depth image and the threecoordinate axes in the camera coordinate system. The resulting angles are thenformed into an image, which is encoded using Swin Transformer. Additionally, weapply RandLA-Net to learn the representations from point clouds. The resultingimage and point clouds embeddings are concatenated and fed into a semanticsegmentation module and a 3D keypoints localization module. Finally, weestimate 6D poses using a least-square fitting approach based on the targetobject's predicted semantic mask and 3D keypoints. In experiments on theLineMod and Occlusion LineMod datasets, SwinDePose outperforms existingstate-of-the-art methods for 6D object pose estimation using depth images. Thisdemonstrates the effectiveness of our approach and highlights its potential forimproving performance in real-world scenarios. Our code is athttps://github.com/zhujunli1993/SwinDePose.</description><author>Zhujun Li, Ioannis Stamos</author><pubDate>Thu, 27 Apr 2023 19:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02133v2</guid></item><item><title>Towards Realistic Generative 3D Face Models</title><link>http://arxiv.org/abs/2304.12483v1</link><description>In recent years, there has been significant progress in 2D generative facemodels fueled by applications such as animation, synthetic data generation, anddigital avatars. However, due to the absence of 3D information, these 2D modelsoften struggle to accurately disentangle facial attributes like pose,expression, and illumination, limiting their editing capabilities. To addressthis limitation, this paper proposes a 3D controllable generative face model toproduce high-quality albedo and precise 3D shape leveraging existing 2Dgenerative models. By combining 2D face generative models with semantic facemanipulation, this method enables editing of detailed 3D rendered faces. Theproposed framework utilizes an alternating descent optimization approach overshape and albedo. Differentiable rendering is used to train high-quality shapesand albedo without 3D supervision. Moreover, this approach outperforms thestate-of-the-art (SOTA) methods in the well-known NoW benchmark for shapereconstruction. It also outperforms the SOTA reconstruction models inrecovering rendered faces' identities across novel poses by an average of 10%.Additionally, the paper demonstrates direct control of expressions in 3D facesby exploiting latent space leading to text-based editing of 3D faces.</description><author>Aashish Rai, Hiresh Gupta, Ayush Pandey, Francisco Vicente Carrasco, Shingo Jason Takagi, Amaury Aubel, Daeil Kim, Aayush Prakash, Fernando de la Torre</author><pubDate>Mon, 24 Apr 2023 23:47:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12483v1</guid></item><item><title>HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</title><link>http://arxiv.org/abs/2212.10428v4</link><description>Estimating the 6D pose of objects is a major 3D computer vision problem.Since the promising outcomes from instance-level approaches, research headsalso move towards category-level pose estimation for more practical applicationscenarios. However, unlike well-established instance-level pose datasets,available category-level datasets lack annotation quality and provided posequantity. We propose the new category-level 6D pose dataset HouseCat6Dfeaturing 1) Multi-modality of Polarimetric RGB and Depth (RGBD+P), 2) Highlydiverse 194 objects of 10 household object categories including 2photometrically challenging categories, 3) High-quality pose annotation with anerror range of only 1.35 mm to 1.74 mm, 4) 41 large-scale scenes with extensiveviewpoint coverage and occlusions, 5) Checkerboard-free environment throughoutthe entire scene, and 6) Additionally annotated dense 6D parallel-jaw grasps.Furthermore, we also provide benchmark results of state-of-the-artcategory-level pose estimation networks.</description><author>HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp, Guangyao Zhai, Hannah Schieber, Giulia Rizzoli, Pengyuan Wang, Hongcheng Zhao, Lorenzo Garattoni, Sven Meier, Daniel Roth, Nassir Navab, Benjamin Busam</author><pubDate>Wed, 26 Apr 2023 11:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10428v4</guid></item><item><title>Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN</title><link>http://arxiv.org/abs/2212.09098v3</link><description>Fine-grained semantic segmentation of a person's face and head, includingfacial parts and head components, has progressed a great deal in recent years.However, it remains a challenging task, whereby considering ambiguousocclusions and large pose variations are particularly difficult. To overcomethese difficulties, we propose a novel framework termed Mask-FPAN. It uses ade-occlusion module that learns to parse occluded faces in a semi-supervisedway. In particular, face landmark localization, face occlusionstimations, anddetected head poses are taken into account. A 3D morphable face model combinedwith the UV GAN improves the robustness of 2D face parsing. In addition, weintroduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for faceparing work. The proposed Mask-FPAN framework addresses the face parsingproblem in the wild and shows significant performance improvements with MIOUfrom 0.7353 to 0.9013 compared to the state-of-the-art on challenging facedatasets.</description><author>Lei Li, Tianfang Zhang, Zhongfeng Kang, Xikun Jiang</author><pubDate>Sun, 30 Apr 2023 03:18:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.09098v3</guid></item><item><title>Unsupervised Style-based Explicit 3D Face Reconstruction from Single Image</title><link>http://arxiv.org/abs/2304.12455v1</link><description>Inferring 3D object structures from a single image is an ill-posed task dueto depth ambiguity and occlusion. Typical resolutions in the literature includeleveraging 2D or 3D ground truth for supervised learning, as well as imposinghand-crafted symmetry priors or using an implicit representation to hallucinatenovel viewpoints for unsupervised methods. In this work, we propose a generaladversarial learning framework for solving Unsupervised 2D to Explicit 3D StyleTransfer (UE3DST). Specifically, we merge two architectures: the unsupervisedexplicit 3D reconstruction network of Wu et al.\ and the Generative AdversarialNetwork (GAN) named StarGAN-v2. We experiment across three facial datasets(Basel Face Model, 3DFAW and CelebA-HQ) and show that our solution is able tooutperform well established solutions such as DepthNet in 3D reconstruction andPix2NeRF in conditional style transfer, while we also justify the individualcontributions of our model components via ablation. In contrast to theaforementioned baselines, our scheme produces features for explicit 3Drendering, which can be manipulated and utilized in downstream tasks.</description><author>Heng Yu, Zoltan A. Milacski, Laszlo A. Jeni</author><pubDate>Mon, 24 Apr 2023 22:25:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12455v1</guid></item><item><title>Towards Realistic 3D Embedding via View Alignment</title><link>http://arxiv.org/abs/2007.07066v3</link><description>Recent advances in generative adversarial networks (GANs) have achieved greatsuccess in automated image composition that generates new images by embeddinginterested foreground objects into background images automatically. On theother hand, most existing works deal with foreground objects in two-dimensional(2D) images though foreground objects in three-dimensional (3D) models are moreflexible with 360-degree view freedom. This paper presents an innovative ViewAlignment GAN (VA-GAN) that composes new images by embedding 3D models into 2Dbackground images realistically and automatically. VA-GAN consists of a texturegenerator and a differential discriminator that are inter-connected andend-to-end trainable. The differential discriminator guides to learn geometrictransformation from background images so that the composed 3D models can bealigned with the background images with realistic poses and views. The texturegenerator adopts a novel view encoding mechanism for generating accurate objecttextures for the 3D models under the estimated views. Extensive experimentsover two synthesis tasks (car synthesis with KITTI and pedestrian synthesiswith Cityscapes) show that VA-GAN achieves high-fidelity compositionqualitatively and quantitatively as compared with state-of-the-art generationmethods.</description><author>Changgong Zhang, Fangneng Zhan, Shijian Lu, Feiying Ma, Xuansong Xie</author><pubDate>Mon, 24 Apr 2023 13:36:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2007.07066v3</guid></item><item><title>PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling</title><link>http://arxiv.org/abs/2304.13006v1</link><description>Creating pose-driven human avatars is about modeling the mapping from thelow-frequency driving pose to high-frequency dynamic human appearances, so aneffective pose encoding method that can encode high-fidelity human details isessential to human avatar modeling.To this end, we present PoseVocab, a novelpose encoding method that encourages the network to discover the optimal poseembeddings for learning the dynamic human appearance. Given multi-view RGBvideos of a character, PoseVocab constructs key poses and latent embeddingsbased on the training poses. To achieve pose generalization and temporalconsistency, we sample key rotations in $so(3)$ of each joint rather than theglobal pose vectors, and assign a pose embedding to each sampled key rotation.These joint-structured pose embeddings not only encode the dynamic appearancesunder different key poses, but also factorize the global pose embedding intojoint-structured ones to better learn the appearance variation related to themotion of each joint. To improve the representation ability of the poseembedding while maintaining memory efficiency, we introduce feature lines, acompact yet effective 3D representation, to model more fine-grained details ofhuman appearances. Furthermore, given a query pose and a spatial position, ahierarchical query strategy is introduced to interpolate pose embeddings andacquire the conditional pose feature for dynamic human synthesis. Overall,PoseVocab effectively encodes the dynamic details of human appearance andenables realistic and generalized animation under novel poses. Experiments showthat our method outperforms other state-of-the-art baselines both qualitativelyand quantitatively in terms of synthesis quality. Code is available athttps://github.com/lizhe00/PoseVocab.</description><author>Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, Yebin Liu</author><pubDate>Tue, 25 Apr 2023 18:25:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13006v1</guid></item><item><title>StyleLipSync: Style-based Personalized Lip-sync Video Generation</title><link>http://arxiv.org/abs/2305.00521v1</link><description>In this paper, we present StyleLipSync, a style-based personalized lip-syncvideo generative model that can generate identity-agnostic lip-synchronizingvideo from arbitrary audio. To generate a video of arbitrary identities, weleverage expressive lip prior from the semantically rich latent space of apre-trained StyleGAN, where we can also design a video consistency with alinear transformation. In contrast to the previous lip-sync methods, weintroduce pose-aware masking that dynamically locates the mask to improve thenaturalness over frames by utilizing a 3D parametric mesh predictor frame byframe. Moreover, we propose a few-shot lip-sync adaptation method for anarbitrary person by introducing a sync regularizer that preserves lips-syncgeneralization while enhancing the person-specific visual information.Extensive experiments demonstrate that our model can generate accurate lip-syncvideos even with the zero-shot setting and enhance characteristics of an unseenface using a few seconds of target video through the proposed adaptationmethod. Please refer to our project page.</description><author>Taekyung Ki, Dongchan Min</author><pubDate>Sun, 30 Apr 2023 17:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00521v1</guid></item><item><title>Combining HoloLens with Instant-NeRFs: Advanced Real-Time 3D Mobile Mapping</title><link>http://arxiv.org/abs/2304.14301v2</link><description>This work represents a large step into modern ways of fast 3D reconstructionbased on RGB camera images. Utilizing a Microsoft HoloLens 2 as a multisensorplatform that includes an RGB camera and an inertial measurement unit forSLAM-based camera-pose determination, we train a Neural Radiance Field (NeRF)as a neural scene representation in real-time with the acquired data from theHoloLens. The HoloLens is connected via Wifi to a high-performance PC that isresponsible for the training and 3D reconstruction. After the data stream ends,the training is stopped and the 3D reconstruction is initiated, which extractsa point cloud of the scene. With our specialized inference algorithm, fivemillion scene points can be extracted within 1 second. In addition, the pointcloud also includes radiometry per point. Our method of 3D reconstructionoutperforms grid point sampling with NeRFs by multiple orders of magnitude andcan be regarded as a complete real-time 3D reconstruction method in a mobilemapping setup.</description><author>Dennis Haitz, Boris Jutzi, Markus Ulrich, Miriam Jaeger, Patrick Huebner</author><pubDate>Wed, 03 May 2023 12:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14301v2</guid></item><item><title>Object Semantics Give Us the Depth We Need: Multi-task Approach to Aerial Depth Completion</title><link>http://arxiv.org/abs/2304.12542v1</link><description>Depth completion and object detection are two crucial tasks often used foraerial 3D mapping, path planning, and collision avoidance of Uncrewed AerialVehicles (UAVs). Common solutions include using measurements from a LiDARsensor; however, the generated point cloud is often sparse and irregular andlimits the system's capabilities in 3D rendering and safety-criticaldecision-making. To mitigate this challenge, information from other sensors onthe UAV (viz., a camera used for object detection) is utilized to help thedepth completion process generate denser 3D models. Performing both aerialdepth completion and object detection tasks while fusing the data from the twosensors poses a challenge to resource efficiency. We address this challenge byproposing a novel approach to jointly execute the two tasks in a single pass.The proposed method is based on an encoder-focused multi-task learning modelthat exposes the two tasks to jointly learned features. We demonstrate howsemantic expectations of the objects in the scene learned by the objectdetection pathway can boost the performance of the depth completion pathwaywhile placing the missing depth values. Experimental results show that theproposed multi-task network outperforms its single-task counterpart,particularly when exposed to defective inputs.</description><author>Sara Hatami Gazani, Fardad Dadboud, Miodrag Bolic, Iraj Mantegh, Homayoun Najjaran</author><pubDate>Tue, 25 Apr 2023 04:21:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12542v1</guid></item><item><title>A Cooperative Perception System Robust to Localization Errors</title><link>http://arxiv.org/abs/2210.06289v2</link><description>Cooperative perception is challenging for safety-critical autonomous drivingapplications.The errors in the shared position and pose cause an inaccuraterelative transform estimation and disrupt the robust mapping of the Egovehicle. We propose a distributed object-level cooperative perception systemcalled OptiMatch, in which the detected 3D bounding boxes and local stateinformation are shared between the connected vehicles. To correct the noisyrelative transform, the local measurements of both connected vehicles (boundingboxes) are utilized, and an optimal transport theory-based algorithm isdeveloped to filter out those objects jointly detected by the vehicles alongwith their correspondence, constructing an associated co-visible set. Acorrection transform is estimated from the matched object pairs and furtherapplied to the noisy relative transform, followed by global fusion and dynamicmapping. Experiment results show that robust performance is achieved fordifferent levels of location and heading errors, and the proposed frameworkoutperforms the state-of-the-art benchmark fusion schemes, including early,late, and intermediate fusion, on average precision by a large margin whenlocation and/or heading errors occur.</description><author>Zhiying Song, Fuxi Wen, Hailiang Zhang, Jun Li</author><pubDate>Wed, 26 Apr 2023 01:13:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06289v2</guid></item><item><title>Compositional 3D Human-Object Neural Animation</title><link>http://arxiv.org/abs/2304.14070v1</link><description>Human-object interactions (HOIs) are crucial for human-centric sceneunderstanding applications such as human-centric visual generation, AR/VR, androbotics. Since existing methods mainly explore capturing HOIs, rendering HOIremains less investigated. In this paper, we address this challenge in HOIanimation from a compositional perspective, i.e., animating novel HOIsincluding novel interaction, novel human and/or novel object driven by a novelpose sequence. Specifically, we adopt neural human-object deformation to modeland render HOI dynamics based on implicit neural representations. To enable theinteraction pose transferring among different persons and objects, we thendevise a new compositional conditional neural radiance field (or CC-NeRF),which decomposes the interdependence between human and object using latentcodes to enable compositionally animation control of novel HOIs. Experimentsshow that the proposed method can generalize well to various novel HOIanimation settings. Our project page is https://zhihou7.github.io/CHONA/</description><author>Zhi Hou, Baosheng Yu, Dacheng Tao</author><pubDate>Thu, 27 Apr 2023 11:04:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14070v1</guid></item></channel></rss>