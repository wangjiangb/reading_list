<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 20 Feb 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Occlusion Resilient 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2402.11036v1</link><description>Occlusions remain one of the key challenges in 3D body pose estimation fromsingle-camera video sequences. Temporal consistency has been extensively usedto mitigate their impact but the existing algorithms in the literature do notexplicitly model them. Here, we apply this by representing the deforming body as a spatio-temporalgraph. We then introduce a refinement network that performs graph convolutionsover this graph to output 3D poses. To ensure robustness to occlusions, wetrain this network with a set of binary masks that we use to disable some ofthe edges as in drop-out techniques. In effect, we simulate the fact that some joints can be hidden for periods oftime and train the network to be immune to that. We demonstrate theeffectiveness of this approach compared to state-of-the-art techniques thatinfer poses from single-camera sequences.</description><author>Soumava Kumar Roy, Ilia Badanin, Sina Honari, Pascal Fua</author><pubDate>Fri, 16 Feb 2024 19:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11036v1</guid></item><item><title>Dense Matchers for Dense Tracking</title><link>http://arxiv.org/abs/2402.11287v1</link><description>Optical flow is a useful input for various applications, including 3Dreconstruction, pose estimation, tracking, and structure-from-motion. Despiteits utility, the field of dense long-term tracking, especially over widebaselines, has not been extensively explored. This paper extends the concept ofcombining multiple optical flows over logarithmically spaced intervals asproposed by MFT. We demonstrate the compatibility of MFT with different opticalflow networks, yielding results that surpass their individual performance.Moreover, we present a simple yet effective combination of these networkswithin the MFT framework. This approach proves to be competitive with moresophisticated, non-causal methods in terms of position prediction accuracy,highlighting the potential of MFT in enhancing long-term tracking applications.</description><author>Tomáš Jelínek, Jonáš Šerých, Jiří Matas</author><pubDate>Sat, 17 Feb 2024 14:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11287v1</guid></item><item><title>Lester: rotoscope animation through video object segmentation and tracking</title><link>http://arxiv.org/abs/2402.09883v1</link><description>This article introduces Lester, a novel method to automatically synthetiseretro-style 2D animations from videos. The method approaches the challengemainly as an object segmentation and tracking problem. Video frames areprocessed with the Segment Anything Model (SAM) and the resulting masks aretracked through subsequent frames with DeAOT, a method of hierarchicalpropagation for semi-supervised video object segmentation. The geometry of themasks' contours is simplified with the Douglas-Peucker algorithm. Finally,facial traits, pixelation and a basic shadow effect can be optionally added.The results show that the method exhibits an excellent temporal consistency andcan correctly process videos with different poses and appearances, dynamicshots, partial shots and diverse backgrounds. The proposed method provides amore simple and deterministic approach than diffusion models basedvideo-to-video translation pipelines, which suffer from temporal consistencyproblems and do not cope well with pixelated and schematic outputs. The methodis also much most practical than techniques based on 3D human pose estimation,which require custom handcrafted 3D models and are very limited with respect tothe type of scenes they can process.</description><author>Ruben Tous</author><pubDate>Thu, 15 Feb 2024 11:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09883v1</guid></item><item><title>MogaNet: Multi-order Gated Aggregation Network</title><link>http://arxiv.org/abs/2211.03295v3</link><description>By contextualizing the kernel as global as possible, Modern ConvNets haveshown great potential in computer vision tasks. However, recent progress on\textit{multi-order game-theoretic interaction} within deep neural networks(DNNs) reveals the representation bottleneck of modern ConvNets, where theexpressive interactions have not been effectively encoded with the increasedkernel size. To tackle this challenge, we propose a new family of modernConvNets, dubbed MogaNet, for discriminative visual representation learning inpure ConvNet-based models with favorable complexity-performance trade-offs.MogaNet encapsulates conceptually simple yet effective convolutions and gatedaggregation into a compact module, where discriminative features areefficiently gathered and contextualized adaptively. MogaNet exhibits greatscalability, impressive efficiency of parameters, and competitive performancecompared to state-of-the-art ViTs and ConvNets on ImageNet and variousdownstream vision benchmarks, including COCO object detection, ADE20K semanticsegmentation, 2D\&amp;3D human pose estimation, and video prediction. Notably,MogaNet hits 80.0\% and 87.8\% accuracy with 5.2M and 181M parameters onImageNet-1K, outperforming ParC-Net and ConvNeXt-L, while saving 59\% FLOPs and17M parameters, respectively. The source code is available at\url{https://github.com/Westlake-AI/MogaNet}.</description><author>Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Li</author><pubDate>Fri, 16 Feb 2024 14:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03295v3</guid></item><item><title>ROAM: Robust and Object-Aware Motion Generation Using Neural Pose Descriptors</title><link>http://arxiv.org/abs/2308.12969v2</link><description>Existing automatic approaches for 3D virtual character motion synthesissupporting scene interactions do not generalise well to new objects outsidetraining distributions, even when trained on extensive motion capture datasetswith diverse objects and annotated interactions. This paper addresses thislimitation and shows that robustness and generalisation to novel scene objectsin 3D object-aware character synthesis can be achieved by training a motionmodel with as few as one reference object. We leverage an implicit featurerepresentation trained on object-only datasets, which encodes anSE(3)-equivariant descriptor field around the object. Given an unseen objectand a reference pose-object pair, we optimise for the object-aware pose that isclosest in the feature space to the reference pose. Finally, we use l-NSM,i.e., our motion generation model that is trained to seamlessly transition fromlocomotion to object interaction with the proposed bidirectional pose blendingscheme. Through comprehensive numerical comparisons to state-of-the-art methodsand in a user study, we demonstrate substantial improvements in 3D virtualcharacter motion and interaction quality and robustness to scenarios withunseen objects. Our project page is available athttps://vcai.mpi-inf.mpg.de/projects/ROAM/.</description><author>Wanyue Zhang, Rishabh Dabral, Thomas Leimkühler, Vladislav Golyanik, Marc Habermann, Christian Theobalt</author><pubDate>Thu, 15 Feb 2024 08:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12969v2</guid></item><item><title>Loopy-SLAM: Dense Neural SLAM with Loop Closures</title><link>http://arxiv.org/abs/2402.09944v1</link><description>Neural RGBD SLAM techniques have shown promise in dense SimultaneousLocalization And Mapping (SLAM), yet face challenges such as error accumulationduring camera tracking resulting in distorted maps. In response, we introduceLoopy-SLAM that globally optimizes poses and the dense 3D model. We useframe-to-model tracking using a data-driven point-based submap generationmethod and trigger loop closures online by performing global place recognition.Robust pose graph optimization is used to rigidly align the local submaps. Asour representation is point based, map corrections can be performed efficientlywithout the need to store the entire history of input frames used for mappingas typically required by methods employing a grid based mapping structure.Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNetdatasets demonstrate competitive or superior performance in tracking, mapping,and rendering accuracy when compared to existing dense neural RGBD SLAMmethods. Project page: notchla.github.io/Loopy-SLAM.</description><author>Lorenzo Liso, Erik Sandström, Vladimir Yugay, Luc Van Gool, Martin R. Oswald</author><pubDate>Wed, 14 Feb 2024 18:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09944v1</guid></item><item><title>OCT2Confocal: 3D CycleGAN based Translation of Retinal OCT Images to Confocal Microscopy</title><link>http://arxiv.org/abs/2311.10902v3</link><description>Optical coherence tomography (OCT) and confocal microscopy are pivotal inretinal imaging, each presenting unique benefits and limitations. In-vivo OCToffers rapid, non-invasive imaging but can be hampered by clarity issues andmotion artifacts. Ex-vivo confocal microscopy provides high-resolution,cellular detailed color images but is invasive and poses ethical concerns andpotential tissue damage. To bridge these modalities, we developed a 3D CycleGANframework for unsupervised translation of in-vivo OCT to ex-vivo confocalmicroscopy images. Applied to our OCT2Confocal dataset, this frameworkeffectively translates between 3D medical data domains, capturing vascular,textural, and cellular details with precision. This marks the first attempt toexploit the inherent 3D information of OCT and translate it into the rich,detailed color domain of confocal microscopy. Assessed through quantitative andqualitative evaluations, the 3D CycleGAN framework demonstrates commendableimage fidelity and quality, outperforming existing methods despite theconstraints of limited data. This non-invasive generation of retinal confocalimages has the potential to further enhance diagnostic and monitoringcapabilities in ophthalmology. Our source code and OCT2Confocal dataset areavailable at https://github.com/xintian-99/OCT2Confocal.</description><author>Xin Tian, Nantheera Anantrasirichai, Lindsay Nicholson, Alin Achim</author><pubDate>Sat, 17 Feb 2024 01:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10902v3</guid></item><item><title>Multi-Model 3D Registration: Finding Multiple Moving Objects in Cluttered Point Clouds</title><link>http://arxiv.org/abs/2402.10865v1</link><description>We investigate a variation of the 3D registration problem, named multi-model3D registration. In the multi-model registration problem, we are given twopoint clouds picturing a set of objects at different poses (and possiblyincluding points belonging to the background) and we want to simultaneouslyreconstruct how all objects moved between the two point clouds. This setupgeneralizes standard 3D registration where one wants to reconstruct a singlepose, e.g., the motion of the sensor picturing a static scene. Moreover, itprovides a mathematically grounded formulation for relevant roboticsapplications, e.g., where a depth sensor onboard a robot perceives a dynamicscene and has the goal of estimating its own motion (from the static portion ofthe scene) while simultaneously recovering the motion of all dynamic objects.We assume a correspondence-based setup where we have putative matches betweenthe two point clouds and consider the practical case where thesecorrespondences are plagued with outliers. We then propose a simple approachbased on Expectation-Maximization (EM) and establish theoretical conditionsunder which the EM approach converges to the ground truth. We evaluate theapproach in simulated and real datasets ranging from table-top scenes toself-driving scenarios and demonstrate its effectiveness when combined withstate-of-the-art scene flow methods to establish dense correspondences.</description><author>David Jin, Sushrut Karmalkar, Harry Zhang, Luca Carlone</author><pubDate>Fri, 16 Feb 2024 18:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10865v1</guid></item><item><title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</title><link>http://arxiv.org/abs/2309.09737v4</link><description>Mobile autonomy relies on the precise perception of dynamic environments.Robustly tracking moving objects in 3D world thus plays a pivotal role forapplications like trajectory prediction, obstacle avoidance, and path planning.While most current methods utilize LiDARs or cameras for Multiple ObjectTracking (MOT), the capabilities of 4D imaging radars remain largelyunexplored. Recognizing the challenges posed by radar noise and point sparsityin 4D radar data, we introduce RaTrack, an innovative solution tailored forradar-based tracking. Bypassing the typical reliance on specific object typesand 3D bounding boxes, our method focuses on motion segmentation andclustering, enriched by a motion estimation module. Evaluated on theView-of-Delft dataset, RaTrack showcases superior tracking precision of movingobjects, largely surpassing the performance of the state of the art. We releaseour code and model at https://github.com/LJacksonPan/RaTrack.</description><author>Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu</author><pubDate>Mon, 19 Feb 2024 17:53:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09737v4</guid></item><item><title>MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection</title><link>http://arxiv.org/abs/2402.11677v1</link><description>Multi-modal 3D object detection models for automated driving havedemonstrated exceptional performance on computer vision benchmarks likenuScenes. However, their reliance on densely sampled LiDAR point clouds andmeticulously calibrated sensor arrays poses challenges for real-worldapplications. Issues such as sensor misalignment, miscalibration, and disparatesampling frequencies lead to spatial and temporal misalignment in data fromLiDAR and cameras. Additionally, the integrity of LiDAR and camera data isoften compromised by adverse environmental conditions such as inclementweather, leading to occlusions and noise interference. To address thischallenge, we introduce MultiCorrupt, a comprehensive benchmark designed toevaluate the robustness of multi-modal 3D object detectors against ten distincttypes of corruptions. We evaluate five state-of-the-art multi-modal detectorson MultiCorrupt and analyze their performance in terms of their resistanceability. Our results show that existing methods exhibit varying degrees ofrobustness depending on the type of corruption and their fusion strategy. Weprovide insights into which multi-modal design choices make such models robustagainst certain perturbations. The dataset generation code and benchmark areopen-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt.</description><author>Till Beemelmanns, Quan Zhang, Lutz Eckstein</author><pubDate>Sun, 18 Feb 2024 18:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11677v1</guid></item><item><title>An Endoscopic Chisel: Intraoperative Imaging Carves 3D Anatomical Models</title><link>http://arxiv.org/abs/2402.11840v1</link><description>Purpose: Preoperative imaging plays a pivotal role in sinus surgery where CTsoffer patient-specific insights of complex anatomy, enabling real-timeintraoperative navigation to complement endoscopy imaging. However, surgeryelicits anatomical changes not represented in the preoperative model,generating an inaccurate basis for navigation during surgery progression. Methods: We propose a first vision-based approach to update the preoperative3D anatomical model leveraging intraoperative endoscopic video for navigatedsinus surgery where relative camera poses are known. We rely on comparisons ofintraoperative monocular depth estimates and preoperative depth renders toidentify modified regions. The new depths are integrated in these regionsthrough volumetric fusion in a truncated signed distance functionrepresentation to generate an intraoperative 3D model that reflects tissuemanipulation. Results: We quantitatively evaluate our approach by sequentially updatingmodels for a five-step surgical progression in an ex vivo specimen. We computethe error between correspondences from the updated model and ground-truthintraoperative CT in the region of anatomical modification. The resultingmodels show a decrease in error during surgical progression as opposed toincreasing when no update is employed. Conclusion: Our findings suggest that preoperative 3D anatomical models canbe updated using intraoperative endoscopy video in navigated sinus surgery.Future work will investigate improvements to monocular depth estimation as wellas removing the need for external navigation systems. The resulting ability tocontinuously update the patient model may provide surgeons with a more preciseunderstanding of the current anatomical state and paves the way toward adigital twin paradigm for sinus surgery.</description><author>Jan Emily Mangulabnan, Roger D. Soberanis-Mukul, Timo Teufel, Manish Sahu, Jose L. Porras, S. Swaroop Vedula, Masaru Ishii, Gregory Hager, Russell H. Taylor, Mathias Unberath</author><pubDate>Mon, 19 Feb 2024 05:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11840v1</guid></item><item><title>SDGE: Stereo Guided Depth Estimation for 360° Camera Sets</title><link>http://arxiv.org/abs/2402.11791v1</link><description>Depth estimation is a critical technology in autonomous driving, andmulti-camera systems are often used to achieve a 360{\deg} perception. These360{\deg} camera sets often have limited or low-quality overlap regions, makingmulti-view stereo methods infeasible for the entire image. Alternatively,monocular methods may not produce consistent cross-view predictions. To addressthese issues, we propose the Stereo Guided Depth Estimation (SGDE) method,which enhances depth estimation of the full image by explicitly utilizingmulti-view stereo results on the overlap. We suggest building virtual pinholecameras to resolve the distortion problem of fisheye cameras and unify theprocessing for the two types of 360{\deg} cameras. For handling the varyingnoise on camera poses caused by unstable movement, the approach employs aself-calibration method to obtain highly accurate relative poses of theadjacent cameras with minor overlap. These enable the use of robust stereomethods to obtain high-quality depth prior in the overlap region. This priorserves not only as an additional input but also as pseudo-labels that enhancethe accuracy of depth estimation methods and improve cross-view predictionconsistency. The effectiveness of SGDE is evaluated on one fisheye cameradataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes.Our experiments demonstrate that SGDE is effective for both supervised andself-supervised depth estimation, and highlight the potential of our method foradvancing downstream autonomous driving technologies, such as 3D objectdetection and occupancy prediction.</description><author>Jialei Xu, Xianming Liu, Junjun Jiang, Xiangyang Ji</author><pubDate>Mon, 19 Feb 2024 02:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11791v1</guid></item><item><title>CapHuman: Capture Your Moments in Parallel Universes</title><link>http://arxiv.org/abs/2402.00627v2</link><description>We concentrate on a novel human-centric image synthesis task, that is, givenonly one reference facial photograph, it is expected to generate specificindividual images with diverse head positions, poses, facial expressions, andilluminations in different contexts. To accomplish this goal, we argue that ourgenerative model should be capable of the following favorable characteristics:(1) a strong visual and semantic understanding of our world and human societyfor basic object and human image generation. (2) generalizable identitypreservation ability. (3) flexible and fine-grained head control. Recently,large pre-trained text-to-image diffusion models have shown remarkable results,serving as a powerful generative foundation. As a basis, we aim to unleash theabove two capabilities of the pre-trained model. In this work, we present a newframework named CapHuman. We embrace the "encode then learn to align" paradigm,which enables generalizable identity preservation for new individuals withoutcumbersome tuning at inference. CapHuman encodes identity features and thenlearns to align them into the latent space. Moreover, we introduce the 3Dfacial prior to equip our model with control over the human head in a flexibleand 3D-consistent manner. Extensive qualitative and quantitative analysesdemonstrate our CapHuman can produce well-identity-preserved, photo-realistic,and high-fidelity portraits with content-rich representations and various headrenditions, superior to established baselines. Code and checkpoint will bereleased at https://github.com/VamosC/CapHuman.</description><author>Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang</author><pubDate>Mon, 19 Feb 2024 11:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00627v2</guid></item></channel></rss>