<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 31 Mar 2024 14:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>STRIDE: Single-video based Temporally Continuous Occlusion Robust 3D Pose Estimation</title><link>http://arxiv.org/abs/2312.16221v2</link><description>The capability to accurately estimate 3D human poses is crucial for diversefields such as action recognition, gait recognition, and virtual/augmentedreality. However, a persistent and significant challenge within this field isthe accurate prediction of human poses under conditions of severe occlusion.Traditional image-based estimators struggle with heavy occlusions due to a lackof temporal context, resulting in inconsistent predictions. While video-basedmodels benefit from processing temporal data, they encounter limitations whenfaced with prolonged occlusions that extend over multiple frames. Thischallenge arises because these models struggle to generalize beyond theirtraining datasets, and the variety of occlusions is hard to capture in thetraining data. Addressing these challenges, we propose STRIDE (Single-videobased TempoRally contInuous occlusion Robust 3D Pose Estimation), a novelTest-Time Training (TTT) approach to fit a human motion prior for each video.This approach specifically handles occlusions that were not encountered duringthe model's training. By employing STRIDE, we can refine a sequence of noisyinitial pose estimates into accurate, temporally coherent poses during testtime, effectively overcoming the limitations of prior methods. Our frameworkdemonstrates flexibility by being model-agnostic, allowing us to use anyoff-the-shelf 3D pose estimation method for improving robustness and temporalconsistency. We validate STRIDE's efficacy through comprehensive experiments onchallenging datasets like Occluded Human3.6M, Human3.6M, and OCMotion, where itnot only outperforms existing single-image and video-based pose estimationmodels but also showcases superior handling of substantial occlusions,achieving fast, robust, accurate, and temporally consistent 3D pose estimates.</description><author>Rohit Lal, Saketh Bachu, Yash Garg, Arindam Dutta, Calvin-Khang Ta, Dripta S. Raychaudhuri, Hannah Dela Cruz, M. Salman Asif, Amit K. Roy-Chowdhury</author><pubDate>Thu, 14 Mar 2024 04:36:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.16221v2</guid></item><item><title>Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting</title><link>http://arxiv.org/abs/2402.18330v1</link><description>We present EgoTAP, a heatmap-to-3D pose lifting method for highly accuratestereo egocentric 3D pose estimation. Severe self-occlusion and out-of-viewlimbs in egocentric camera views make accurate pose estimation a challengingproblem. To address the challenge, prior methods employ jointheatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3Dpose conversion still remains an inaccurate process. We propose a novelheatmap-to-3D lifting method composed of the Grid ViT Encoder and thePropagation Network. The Grid ViT Encoder summarizes joint heatmaps intoeffective feature embedding using self-attention. Then, the Propagation Networkestimates the 3D pose by utilizing skeletal information to better estimate theposition of obscure joints. Our method significantly outperforms the previousstate-of-the-art qualitatively and quantitatively demonstrated by a 23.9\%reduction of error in an MPJPE metric. Our source code is available in GitHub.</description><author>Taeho Kang, Youngki Lee</author><pubDate>Wed, 28 Feb 2024 13:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18330v1</guid></item><item><title>Unsupervised Multi-Person 3D Human Pose Estimation From 2D Poses Alone</title><link>http://arxiv.org/abs/2309.14865v3</link><description>Current unsupervised 2D-3D human pose estimation (HPE) methods do not work inmulti-person scenarios due to perspective ambiguity in monocular images.Therefore, we present one of the first studies investigating the feasibility ofunsupervised multi-person 2D-3D HPE from just 2D poses alone, focusing onreconstructing human interactions. To address the issue of perspectiveambiguity, we expand upon prior work by predicting the cameras' elevation anglerelative to the subjects' pelvis. This allows us to rotate the predicted posesto be level with the ground plane, while obtaining an estimate for the verticaloffset in 3D between individuals. Our method involves independently liftingeach subject's 2D pose to 3D, before combining them in a shared 3D coordinatesystem. The poses are then rotated and offset by the predicted elevation anglebefore being scaled. This by itself enables us to retrieve an accurate 3Dreconstruction of their poses. We present our results on the CHI3D dataset,introducing its use for unsupervised 2D-3D pose estimation with three newquantitative metrics, and establishing a benchmark for future research.</description><author>Peter Hardy, Hansung Kim</author><pubDate>Tue, 12 Mar 2024 18:45:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14865v3</guid></item><item><title>DOR3D-Net: Dense Ordinal Regression Network for 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2403.13405v1</link><description>Depth-based 3D hand pose estimation is an important but challenging researchtask in human-machine interaction community. Recently, dense regression methodshave attracted increasing attention in 3D hand pose estimation task, whichprovide a low computational burden and high accuracy regression way by denselyregressing hand joint offset maps. However, large-scale regression offsetvalues are often affected by noise and outliers, leading to a significant dropin accuracy. To tackle this, we re-formulate 3D hand pose estimation as a denseordinal regression problem and propose a novel Dense Ordinal Regression 3D PoseNetwork (DOR3D-Net). Specifically, we first decompose offset value regressioninto sub-tasks of binary classifications with ordinal constraints. Then, eachbinary classifier can predict the probability of a binary spatial relationshiprelative to joint, which is easier to train and yield much lower level ofnoise. The estimated hand joint positions are inferred by aggregating theordinal regression results at local positions with a weighted sum. Furthermore,both joint regression loss and ordinal regression loss are used to train ourDOR3D-Net in an end-to-end manner. Extensive experiments on public datasets(ICVL, MSRA, NYU and HANDS2017) show that our design provides significantimprovements over SOTA methods.</description><author>Yamin Mao, Zhihua Liu, Weiming Li, SoonYong Cho, Qiang Wang, Xiaoshuai Hao</author><pubDate>Wed, 20 Mar 2024 09:47:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13405v1</guid></item><item><title>3D Human Pose Estimation Based on 2D-3D Consistency with Synchronized Adversarial Training</title><link>http://arxiv.org/abs/2106.04274v4</link><description>3D human pose estimation from a single image is still a challenging problemdespite the large amount of work that has been performed in this field.Generally, most methods directly use neural networks and ignore certainconstraints (e.g., reprojection constraints, joint angle, and bone lengthconstraints). While a few methods consider these constraints but train thenetwork separately, they cannot effectively solve the depth ambiguity problem.In this paper, we propose a GAN-based model for 3D human pose estimation, inwhich a reprojection network is employed to learn the mapping of thedistribution from 3D poses to 2D poses, and a discriminator is employed for2D-3D consistency discrimination. We adopt a novel strategy to synchronouslytrain the generator, the reprojection network and the discriminator.Furthermore, inspired by the typical kinematic chain space (KCS) matrix, weintroduce a weighted KCS matrix and take it as one of the discriminator'sinputs to impose joint angle and bone length constraints. The experimentalresults on Human3.6M show that our method significantly outperformsstate-of-the-art methods in most cases.</description><author>Yicheng Deng, Cheng Sun, Yongqi Sun, Jiahui Zhu</author><pubDate>Tue, 05 Mar 2024 10:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.04274v4</guid></item><item><title>Exploring 3D Human Pose Estimation and Forecasting from the Robot's Perspective: The HARPER Dataset</title><link>http://arxiv.org/abs/2403.14447v1</link><description>We introduce HARPER, a novel dataset for 3D body pose estimation and forecastin dyadic interactions between users and \spot, the quadruped robotmanufactured by Boston Dynamics. The key-novelty is the focus on the robot'sperspective, i.e., on the data captured by the robot's sensors. These make 3Dbody pose analysis challenging because being close to the ground captureshumans only partially. The scenario underlying HARPER includes 15 actions, ofwhich 10 involve physical contact between the robot and users. The Corpuscontains not only the recordings of the built-in stereo cameras of Spot, butalso those of a 6-camera OptiTrack system (all recordings are synchronized).This leads to ground-truth skeletal representations with a precision lower thana millimeter. In addition, the Corpus includes reproducible benchmarks on 3DHuman Pose Estimation, Human Pose Forecasting, and Collision Prediction, allbased on publicly available baseline approaches. This enables future HARPERusers to rigorously compare their results with those we provide in this work.</description><author>Andrea Avogaro. Andrea Toaiari, Federico Cunico, Xiangmin Xu, Haralambos Dafas, Alessandro Vinciarelli, Emma Li, Marco Cristani</author><pubDate>Thu, 21 Mar 2024 15:53:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14447v1</guid></item><item><title>Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser</title><link>http://arxiv.org/abs/2403.04444v1</link><description>Recently, diffusion-based methods for monocular 3D human pose estimation haveachieved state-of-the-art (SOTA) performance by directly regressing the 3Djoint coordinates from the 2D pose sequence. Although some methods decomposethe task into bone length and bone direction prediction based on the humananatomical skeleton to explicitly incorporate more human body priorconstraints, the performance of these methods is significantly lower than thatof the SOTA diffusion-based methods. This can be attributed to the treestructure of the human skeleton. Direct application of the disentangled methodcould amplify the accumulation of hierarchical errors, propagating through eachhierarchy. Meanwhile, the hierarchical information has not been fully exploredby the previous methods. To address these problems, a DisentangledDiffusion-based 3D Human Pose Estimation method with Hierarchical Spatial andTemporal Denoiser is proposed, termed DDHPose. In our approach: (1) Wedisentangle the 3D pose and diffuse the bone length and bone direction duringthe forward process of the diffusion model to effectively model the human poseprior. A disentanglement loss is proposed to supervise diffusion modellearning. (2) For the reverse process, we propose Hierarchical Spatial andTemporal Denoiser (HSTDenoiser) to improve the hierarchical modeling of eachjoint. Our HSTDenoiser comprises two components: the Hierarchical-RelatedSpatial Transformer (HRST) and the Hierarchical-Related Temporal Transformer(HRTT). HRST exploits joint spatial information and the influence of the parentjoint on each joint for spatial modeling, while HRTT utilizes information fromboth the joint and its hierarchical adjacent joints to explore the hierarchicaltemporal correlations among joints.</description><author>Qingyuan Cai, Xuecai Hu, Saihui Hou, Li Yao, Yongzhen Huang</author><pubDate>Thu, 07 Mar 2024 12:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04444v1</guid></item><item><title>ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image</title><link>http://arxiv.org/abs/2403.09871v1</link><description>In this work, we present ThermoHands, a new benchmark for thermal image-basedegocentric 3D hand pose estimation, aimed at overcoming challenges like varyinglighting and obstructions (e.g., handwear). The benchmark includes a diversedataset from 28 subjects performing hand-object and hand-virtual interactions,accurately annotated with 3D hand poses through an automated process. Weintroduce a bespoken baseline method, TheFormer, utilizing dual transformermodules for effective egocentric 3D hand pose estimation in thermal imagery.Our experimental results highlight TheFormer's leading performance and affirmthermal imaging's effectiveness in enabling robust 3D hand pose estimation inadverse conditions.</description><author>Fangqiang Ding, Yunzhou Zhu, Xiangyu Wen, Chris Xiaoxuan Lu</author><pubDate>Thu, 14 Mar 2024 22:01:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09871v1</guid></item><item><title>Towards Precise 3D Human Pose Estimation with Multi-Perspective Spatial-Temporal Relational Transformers</title><link>http://arxiv.org/abs/2401.16700v2</link><description>3D human pose estimation captures the human joint points in three-dimensionalspace while keeping the depth information and physical structure. That isessential for applications that require precise pose information, such ashuman-computer interaction, scene understanding, and rehabilitation training.Due to the challenges in data collection, mainstream datasets of 3D human poseestimation are primarily composed of multi-view video data collected inlaboratory environments, which contains rich spatial-temporal correlationinformation besides the image frame content. Given the remarkableself-attention mechanism of transformers, capable of capturing thespatial-temporal correlation from multi-view video datasets, we propose amulti-stage framework for 3D sequence-to-sequence (seq2seq) human posedetection. Firstly, the spatial module represents the human pose feature byintra-image content, while the frame-image relation module extracts temporalrelationships and 3D spatial positional relationship features between themulti-perspective images. Secondly, the self-attention mechanism is adopted toeliminate the interference from non-human body parts and reduce computingresources. Our method is evaluated on Human3.6M, a popular 3D human posedetection dataset. Experimental results demonstrate that our approach achievesstate-of-the-art performance on this dataset. The source code will be availableat https://github.com/WUJINHUAN/3D-human-pose.</description><author>Jianbin Jiao, Xina Cheng, Weijie Chen, Xiaoting Yin, Hao Shi, Kailun Yang</author><pubDate>Mon, 25 Mar 2024 14:33:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.16700v2</guid></item><item><title>Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey</title><link>http://arxiv.org/abs/2402.18844v1</link><description>3D human pose estimation and mesh recovery have attracted widespread researchinterest in many areas, such as computer vision, autonomous driving, androbotics. Deep learning on 3D human pose estimation and mesh recovery hasrecently thrived, with numerous methods proposed to address different problemsin this area. In this paper, to stimulate future research, we present acomprehensive review of recent progress over the past five years in deeplearning methods for this area by delving into over 200 references. To the bestof our knowledge, this survey is arguably the first to comprehensively coverdeep learning methods for 3D human pose estimation, including bothsingle-person and multi-person approaches, as well as human mesh recovery,encompassing methods based on explicit models and implicit representations. Wealso present comparative results on several publicly available datasets,together with insightful observations and inspiring future research directions.A regularly updated project page can be found athttps://github.com/liuyangme/SOTA-3DHPE-HMR.</description><author>Yang Liu, Changzhen Qiu, Zhiyong Zhang</author><pubDate>Thu, 29 Feb 2024 04:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18844v1</guid></item><item><title>A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2403.11310v1</link><description>3D human pose data collected in controlled laboratory settings presentchallenges for pose estimators that generalize across diverse scenarios. Toaddress this, domain generalization is employed. Current methodologies indomain generalization for 3D human pose estimation typically utilizeadversarial training to generate synthetic poses for training. Nonetheless,these approaches exhibit several limitations. First, the lack of priorinformation about the target domain complicates the application of suitableaugmentation through a single pose augmentor, affecting generalization ontarget domains. Moreover, adversarial training's discriminator tends to enforcesimilarity between source and synthesized poses, impeding the exploration ofout-of-source distributions. Furthermore, the pose estimator's optimization isnot exposed to domain shifts, limiting its overall generalization ability. To address these limitations, we propose a novel framework featuring two poseaugmentors: the weak and the strong augmentors. Our framework employsdifferential strategies for generation and discrimination processes,facilitating the preservation of knowledge related to source poses and theexploration of out-of-source distributions without prior information abouttarget poses. Besides, we leverage meta-optimization to simulate domain shiftsin the optimization process of the pose estimator, thereby improving itsgeneralization ability. Our proposed approach significantly outperformsexisting methods, as demonstrated through comprehensive experiments on variousbenchmark datasets.</description><author>Qucheng Peng, Ce Zheng, Chen Chen</author><pubDate>Sun, 17 Mar 2024 20:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11310v1</guid></item><item><title>A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2403.11310v2</link><description>3D human pose data collected in controlled laboratory settings presentchallenges for pose estimators that generalize across diverse scenarios. Toaddress this, domain generalization is employed. Current methodologies indomain generalization for 3D human pose estimation typically utilizeadversarial training to generate synthetic poses for training. Nonetheless,these approaches exhibit several limitations. First, the lack of priorinformation about the target domain complicates the application of suitableaugmentation through a single pose augmentor, affecting generalization ontarget domains. Moreover, adversarial training's discriminator tends to enforcesimilarity between source and synthesized poses, impeding the exploration ofout-of-source distributions. Furthermore, the pose estimator's optimization isnot exposed to domain shifts, limiting its overall generalization ability. To address these limitations, we propose a novel framework featuring two poseaugmentors: the weak and the strong augmentors. Our framework employsdifferential strategies for generation and discrimination processes,facilitating the preservation of knowledge related to source poses and theexploration of out-of-source distributions without prior information abouttarget poses. Besides, we leverage meta-optimization to simulate domain shiftsin the optimization process of the pose estimator, thereby improving itsgeneralization ability. Our proposed approach significantly outperformsexisting methods, as demonstrated through comprehensive experiments on variousbenchmark datasets.Our code will be released at\url{https://github.com/davidpengucf/DAF-DG}.</description><author>Qucheng Peng, Ce Zheng, Chen Chen</author><pubDate>Wed, 20 Mar 2024 02:34:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11310v2</guid></item><item><title>Occlusion Resilient 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2402.11036v1</link><description>Occlusions remain one of the key challenges in 3D body pose estimation fromsingle-camera video sequences. Temporal consistency has been extensively usedto mitigate their impact but the existing algorithms in the literature do notexplicitly model them. Here, we apply this by representing the deforming body as a spatio-temporalgraph. We then introduce a refinement network that performs graph convolutionsover this graph to output 3D poses. To ensure robustness to occlusions, wetrain this network with a set of binary masks that we use to disable some ofthe edges as in drop-out techniques. In effect, we simulate the fact that some joints can be hidden for periods oftime and train the network to be immune to that. We demonstrate theeffectiveness of this approach compared to state-of-the-art techniques thatinfer poses from single-camera sequences.</description><author>Soumava Kumar Roy, Ilia Badanin, Sina Honari, Pascal Fua</author><pubDate>Fri, 16 Feb 2024 19:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11036v1</guid></item><item><title>Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2311.12028v2</link><description>Transformers have been successfully applied in the field of video-based 3Dhuman pose estimation. However, the high computational costs of these videopose transformers (VPTs) make them impractical on resource-constrained devices.In this paper, we present a plug-and-play pruning-and-recovering framework,called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human poseestimation from videos. Our HoT begins with pruning pose tokens of redundantframes and ends with recovering full-length tokens, resulting in a few posetokens in the intermediate transformer blocks and thus improving the modelefficiency. To effectively achieve this, we propose a token pruning cluster(TPC) that dynamically selects a few representative tokens with high semanticdiversity while eliminating the redundancy of video frames. In addition, wedevelop a token recovering attention (TRA) to restore the detailedspatio-temporal information based on the selected tokens, thereby expanding thenetwork output to the original full-length temporal resolution for fastinference. Extensive experiments on two benchmark datasets (i.e., Human3.6M andMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency andestimation accuracy compared to the original VPT models. For instance, applyingto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPswithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,respectively. Code and models are available athttps://github.com/NationalGAILab/HoT.</description><author>Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe</author><pubDate>Wed, 27 Mar 2024 12:43:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12028v2</guid></item><item><title>PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery</title><link>http://arxiv.org/abs/2403.12473v1</link><description>With the recent advancements in single-image-based human mesh recovery, thereis a growing interest in enhancing its performance in certain extremescenarios, such as occlusion, while maintaining overall model accuracy.Although obtaining accurately annotated 3D human poses under occlusion ischallenging, there is still a wealth of rich and precise 2D pose annotationsthat can be leveraged. However, existing works mostly focus on directlyleveraging 2D pose coordinates to estimate 3D pose and mesh. In this paper, wepresent PostoMETRO($\textbf{Pos}$e $\textbf{to}$ken enhanced $\textbf{ME}$sh$\textbf{TR}$ansf$\textbf{O}$rmer), which integrates occlusion-resilient 2Dpose representation into transformers in a token-wise manner. Utilizing aspecialized pose tokenizer, we efficiently condense 2D pose data to a compactsequence of pose tokens and feed them to the transformer together with theimage tokens. This process not only ensures a rich depiction of texture fromthe image but also fosters a robust integration of pose and image information.Subsequently, these combined tokens are queried by vertex and joint tokens todecode 3D coordinates of mesh vertices and human joints. Facilitated by therobust pose token representation and the effective combination, we are able toproduce more precise 3D coordinates, even under extreme scenarios likeocclusion. Experiments on both standard and occlusion-specific benchmarksdemonstrate the effectiveness of PostoMETRO. Qualitative results furtherillustrate the clarity of how 2D pose can help 3D reconstruction. Code will bemade available.</description><author>Wendi Yang, Zihang Jiang, Shang Zhao, S. Kevin Zhou</author><pubDate>Tue, 19 Mar 2024 07:18:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12473v1</guid></item><item><title>Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting</title><link>http://arxiv.org/abs/2403.09437v1</link><description>Current human pose estimation systems focus on retrieving an accurate 3Dglobal estimate of a single person. Therefore, this paper presents one of thefirst 3D multi-person human pose estimation systems that is able to work inreal-time and is also able to handle basic forms of occlusion. First, we adjustan off-the-shelf 2D detector and an unsupervised 2D-3D lifting model for usewith a 360$^\circ$ panoramic camera and mmWave radar sensors. We then introduceseveral contributions, including camera and radar calibrations, and theimproved matching of people within the image and radar space. The systemaddresses both the depth and scale ambiguity problems by employing alightweight 2D-3D pose lifting algorithm that is able to work in real-timewhile exhibiting accurate performance in both indoor and outdoor environmentswhich offers both an affordable and scalable solution. Notably, our system'stime complexity remains nearly constant irrespective of the number of detectedindividuals, achieving a frame rate of approximately 7-8 fps on a laptop with acommercial-grade GPU.</description><author>Pawel Knap, Peter Hardy, Alberto Tamajo, Hwasup Lim, Hansung Kim</author><pubDate>Thu, 14 Mar 2024 15:30:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09437v1</guid></item><item><title>On the Utility of 3D Hand Poses for Action Recognition</title><link>http://arxiv.org/abs/2403.09805v1</link><description>3D hand poses are an under-explored modality for action recognition. Posesare compact yet informative and can greatly benefit applications with limitedcompute budgets. However, poses alone offer an incomplete understanding ofactions, as they cannot fully capture objects and environments with whichhumans interact. To efficiently model hand-object interactions, we proposeHandFormer, a novel multimodal transformer. HandFormer combines 3D hand posesat a high temporal resolution for fine-grained motion modeling with sparselysampled RGB frames for encoding scene semantics. Observing the uniquecharacteristics of hand poses, we temporally factorize hand modeling andrepresent each joint by its short-term trajectories. This factorized poserepresentation combined with sparse RGB samples is remarkably efficient andachieves high accuracy. Unimodal HandFormer with only hand poses outperformsexisting skeleton-based methods at 5x fewer FLOPs. With RGB, we achieve newstate-of-the-art performance on Assembly101 and H2O with significantimprovements in egocentric action recognition.</description><author>Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</author><pubDate>Thu, 14 Mar 2024 19:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09805v1</guid></item><item><title>Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2403.04381v1</link><description>The pursuit of accurate 3D hand pose estimation stands as a keystone forunderstanding human activity in the realm of egocentric vision. The majority ofexisting estimation methods still rely on single-view images as input, leadingto potential limitations, e.g., limited field-of-view and ambiguity in depth.To address these problems, adding another camera to better capture the shape ofhands is a practical direction. However, existing multi-view hand poseestimation methods suffer from two main drawbacks: 1) Requiring multi-viewannotations for training, which are expensive. 2) During testing, the modelbecomes inapplicable if camera parameters/layout are not the same as those usedin training. In this paper, we propose a novel Single-to-Dual-view adaptation(S2DHand) solution that adapts a pre-trained single-view estimator to dualviews. Compared with existing multi-view training methods, 1) our adaptationprocess is unsupervised, eliminating the need for multi-view annotation. 2)Moreover, our method can handle arbitrary dual-view pairs with unknown cameraparameters, making the model applicable to diverse camera settings.Specifically, S2DHand is built on certain stereo constraints, includingpair-wise cross-view consensus and invariance of transformation between bothviews. These two stereo constraints are used in a complementary manner togenerate pseudo-labels, allowing reliable adaptation. Evaluation results revealthat S2DHand achieves significant improvements on arbitrary camera pairs underboth in-dataset and cross-dataset settings, and outperforms existing adaptationmethods with leading performance. Project page:https://github.com/MickeyLLG/S2DHand.</description><author>Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato</author><pubDate>Thu, 07 Mar 2024 10:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04381v1</guid></item><item><title>Personalized 3D Human Pose and Shape Refinement</title><link>http://arxiv.org/abs/2403.11634v1</link><description>Recently, regression-based methods have dominated the field of 3D human poseand shape estimation. Despite their promising results, a common issue is themisalignment between predictions and image observations, often caused by minorjoint rotation errors that accumulate along the kinematic chain. To addressthis issue, we propose to construct dense correspondences between initial humanmodel estimates and the corresponding images that can be used to refine theinitial predictions. To this end, we utilize renderings of the 3D models topredict per-pixel 2D displacements between the synthetic renderings and the RGBimages. This allows us to effectively integrate and exploit appearanceinformation of the persons. Our per-pixel displacements can be efficientlytransformed to per-visible-vertex displacements and then used for 3D modelrefinement by minimizing a reprojection loss. To demonstrate the effectivenessof our approach, we refine the initial 3D human mesh predictions of multiplemodels using different refinement procedures on 3DPW and RICH. We show that ourapproach not only consistently leads to better image-model alignment, but alsoto improved 3D accuracy.</description><author>Tom Wehrbein, Bodo Rosenhahn, Iain Matthews, Carsten Stoll</author><pubDate>Mon, 18 Mar 2024 11:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11634v1</guid></item><item><title>NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images</title><link>http://arxiv.org/abs/2402.18196v1</link><description>Human pose estimation (HPE) in the top-view using fisheye cameras presents apromising and innovative application domain. However, the availability ofdatasets capturing this viewpoint is extremely limited, especially those withhigh-quality 2D and 3D keypoint annotations. Addressing this gap, we leveragethe capabilities of Neural Radiance Fields (NeRF) technique to establish acomprehensive pipeline for generating human pose datasets from existing 2D and3D datasets, specifically tailored for the top-view fisheye perspective.Through this pipeline, we create a novel dataset NToP570K (NeRF-poweredTop-view human Pose dataset for fisheye cameras with over 570 thousand images),and conduct an extensive evaluation of its efficacy in enhancing neuralnetworks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-Bmodel achieves an improvement in AP of 33.3 % on our validation set for 2D HPEafter finetuning on our training set. A similarly finetuned HybrIK-Transformermodel gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.</description><author>Jingrui Yu, Dipankar Nandi, Roman Seidel, Gangolf Hirtz</author><pubDate>Wed, 28 Feb 2024 09:36:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18196v1</guid></item><item><title>GGRt: Towards Generalizable 3D Gaussians without Pose Priors in Real-Time</title><link>http://arxiv.org/abs/2403.10147v1</link><description>This paper presents GGRt, a novel approach to generalizable novel viewsynthesis that alleviates the need for real camera poses, complexity inprocessing high-resolution images, and lengthy optimization processes, thusfacilitating stronger applicability of 3D Gaussian Splatting (3D-GS) inreal-world scenarios. Specifically, we design a novel joint learning frameworkthat consists of an Iterative Pose Optimization Network (IPO-Net) and aGeneralizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism,the proposed framework can inherently estimate robust relative pose informationfrom the image observations and thus primarily alleviate the requirement ofreal camera poses. Moreover, we implement a deferred back-propagation mechanismthat enables high-resolution training and inference, overcoming the resolutionconstraints of previous methods. To enhance the speed and efficiency, wefurther introduce a progressive Gaussian cache module that dynamically adjustsduring training and inference. As the first pose-free generalizable 3D-GSframework, GGRt achieves inference at $\ge$ 5 FPS and real-time rendering at$\ge$ 100 FPS. Through extensive experimentation, we demonstrate that ourmethod outperforms existing NeRF-based pose-free techniques in terms ofinference speed and effectiveness. It can also approach the real pose-based3D-GS methods. Our contributions provide a significant leap forward for theintegration of computer vision and computer graphics into practicalapplications, offering state-of-the-art results on LLFF, KITTI, and Waymo Opendatasets and enabling real-time rendering for immersive experiences.</description><author>Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han</author><pubDate>Fri, 15 Mar 2024 10:47:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10147v1</guid></item><item><title>Generating Images with 3D Annotations Using Diffusion Models</title><link>http://arxiv.org/abs/2306.08103v3</link><description>Diffusion models have emerged as a powerful generative method, capable ofproducing stunning photo-realistic images from natural language descriptions.However, these models lack explicit control over the 3D structure in thegenerated images. Consequently, this hinders our ability to obtain detailed 3Dannotations for the generated images or to craft instances with specific posesand distances. In this paper, we propose 3D Diffusion Style Transfer (3D-DST),which incorporates 3D geometry control into diffusion models. Our methodexploits ControlNet, which extends diffusion models by using visual prompts inaddition to text prompts. We generate images of the 3D objects taken from 3Dshape repositories~(e.g., ShapeNet and Objaverse), render them from a varietyof poses and viewing directions, compute the edge maps of the rendered images,and use these edge maps as visual prompts to generate realistic images. Withexplicit 3D geometry control, we can easily change the 3D structures of theobjects in the generated images and obtain ground-truth 3D annotationsautomatically. This allows us to improve a wide range of vision tasks, e.g.,classification and 3D pose estimation, in both in-distribution (ID) andout-of-distribution (OOD) settings. We demonstrate the effectiveness of ourmethod through extensive experiments on ImageNet-100/200, ImageNet-R,PASCAL3D+, ObjectNet3D, and OOD-CV. The results show that our methodsignificantly outperforms existing methods, e.g., 3.8 percentage points onImageNet-100 using DeiT-B.</description><author>Wufei Ma, Qihao Liu, Jiahao Wang, Angtian Wang, Xiaoding Yuan, Yi Zhang, Zihao Xiao, Guofeng Zhang, Beijia Lu, Ruxiao Duan, Yongrui Qi, Adam Kortylewski, Yaoyao Liu, Alan Yuille</author><pubDate>Fri, 15 Mar 2024 02:16:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08103v3</guid></item><item><title>EgoPoseFormer: A Simple Baseline for Egocentric 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2403.18080v1</link><description>We present EgoPoseFormer, a simple yet effective transformer-based model forstereo egocentric human pose estimation. The main challenge in egocentric poseestimation is overcoming joint invisibility, which is caused by self-occlusionor a limited field of view (FOV) of head-mounted cameras. Our approachovercomes this challenge by incorporating a two-stage pose estimation paradigm:in the first stage, our model leverages the global information to estimate eachjoint's coarse location, then in the second stage, it employs a DETR styletransformer to refine the coarse locations by exploiting fine-grained stereovisual features. In addition, we present a deformable stereo operation toenable our transformer to effectively process multi-view features, whichenables it to accurately localize each joint in the 3D world. We evaluate ourmethod on the stereo UnrealEgo dataset and show it significantly outperformsprevious approaches while being computationally efficient: it improves MPJPE by27.4mm (45% improvement) with only 7.9% model parameters and 13.1% FLOPscompared to the state-of-the-art. Surprisingly, with proper trainingtechniques, we find that even our first-stage pose proposal network can achievesuperior performance compared to previous arts. We also show that our methodcan be seamlessly extended to monocular settings, which achievesstate-of-the-art performance on the SceneEgo dataset, improving MPJPE by 25.5mm(21% improvement) compared to the best existing method with only 60.7% modelparameters and 36.4% FLOPs.</description><author>Chenhongyi Yang, Anastasia Tkach, Shreyas Hampali, Linguang Zhang, Elliot J. Crowley, Cem Keskin</author><pubDate>Tue, 26 Mar 2024 21:02:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18080v1</guid></item><item><title>LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network</title><link>http://arxiv.org/abs/2306.12525v2</link><description>Due to the difficulty of acquiring large-scale 3D human keypoint annotation,previous methods for 3D human pose estimation (HPE) have often relied on 2Dimage features and sequential 2D annotations. Furthermore, the training ofthese networks typically assumes the prediction of a human bounding box and theaccurate alignment of 3D point clouds with 2D images, making direct applicationin real-world scenarios challenging. In this paper, we present the 1stframework for end-to-end 3D human pose estimation, named LPFormer, which usesonly LiDAR as its input along with its corresponding 3D annotations. LPFormerconsists of two stages: firstly, it identifies the human bounding box andextracts multi-level feature representations, and secondly, it utilizes atransformer-based network to predict human keypoints based on these features.Our method demonstrates that 3D HPE can be seamlessly integrated into a strongLiDAR perception network and benefit from the features extracted by thenetwork. Experimental results on the Waymo Open Dataset demonstrate thestate-of-the-art performance, and improvements even compared to previousmulti-modal solutions.</description><author>Dongqiangzi Ye, Yufei Xie, Weijia Chen, Zixiang Zhou, Lingting Ge, Hassan Foroosh</author><pubDate>Sat, 02 Mar 2024 22:36:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12525v2</guid></item><item><title>Diffusion Model is a Good Pose Estimator from 3D RF-Vision</title><link>http://arxiv.org/abs/2403.16198v1</link><description>Human pose estimation (HPE) from Radio Frequency vision (RF-vision) performshuman sensing using RF signals that penetrate obstacles without revealingprivacy (e.g., facial information). Recently, mmWave radar has emerged as apromising RF-vision sensor, providing radar point clouds by processing RFsignals. However, the mmWave radar has a limited resolution with severe noise,leading to inaccurate and inconsistent human pose estimation. This workproposes mmDiff, a novel diffusion-based pose estimator tailored for noisyradar data. Our approach aims to provide reliable guidance as conditions todiffusion models. Two key challenges are addressed by mmDiff: (1)miss-detection of parts of human bodies, which is addressed by a module thatisolates feature extraction from different body parts, and (2) signalinconsistency due to environmental interference, which is tackled byincorporating prior knowledge of body structure and motion. Several modules aredesigned to achieve these goals, whose features work as the conditions for thesubsequent diffusion model, eliminating the miss-detection and instability ofHPE based on RF-vision. Extensive experiments demonstrate that mmDiffoutperforms existing methods significantly, achieving state-of-the-artperformances on public datasets.</description><author>Junqiao Fan, Jianfei Yang, Yuecong Xu, Lihua Xie</author><pubDate>Sun, 24 Mar 2024 16:39:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16198v1</guid></item><item><title>WALT3D: Generating Realistic Training Data from Time-Lapse Imagery for Reconstructing Dynamic Objects under Occlusion</title><link>http://arxiv.org/abs/2403.19022v1</link><description>Current methods for 2D and 3D object understanding struggle with severeocclusions in busy urban environments, partly due to the lack of large-scalelabeled ground-truth annotations for learning occlusion. In this work, weintroduce a novel framework for automatically generating a large, realisticdataset of dynamic objects under occlusions using freely available time-lapseimagery. By leveraging off-the-shelf 2D (bounding box, segmentation, keypoint)and 3D (pose, shape) predictions as pseudo-groundtruth, unoccluded 3D objectsare identified automatically and composited into the background in a clip-artstyle, ensuring realistic appearances and physically accurate occlusionconfigurations. The resulting clip-art image with pseudo-groundtruth enablesefficient training of object reconstruction methods that are robust toocclusions. Our method demonstrates significant improvements in both 2D and 3Dreconstruction, particularly in scenarios with heavily occluded objects likevehicles and people in urban scenes.</description><author>Khiem Vuong, N. Dinesh Reddy, Robert Tamburo, Srinivasa G. Narasimhan</author><pubDate>Wed, 27 Mar 2024 22:24:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19022v1</guid></item><item><title>That's My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation</title><link>http://arxiv.org/abs/2403.04755v1</link><description>This paper is about 3D pose estimation on LiDAR scans with extremely minimalstorage requirements to enable scalable mapping and localisation. We achievethis by clustering all points of segmented scans into semantic objects andrepresenting them only with their respective centroid and semantic class. Inthis way, each LiDAR scan is reduced to a compact collection of four-numbervectors. This abstracts away important structural information from the scenes,which is crucial for traditional registration approaches. To mitigate this, weintroduce an object-matching network based on self- and cross-correlation thatcaptures geometric and semantic relationships between entities. The respectivematches allow us to recover the relative transformation between scans throughweighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus(RANSAC). We demonstrate that such representation is sufficient for metriclocalisation by registering point clouds taken under different viewpoints onthe KITTI dataset, and at different periods of time localising between KITTIand KITTI-360. We achieve accurate metric estimates comparable withstate-of-the-art methods with almost half the representation size, specifically1.33 kB on average.</description><author>Georgi Pramatarov, Matthew Gadd, Paul Newman, Daniele De Martini</author><pubDate>Thu, 07 Mar 2024 18:55:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04755v1</guid></item><item><title>ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living</title><link>http://arxiv.org/abs/2402.17758v1</link><description>Hand-Object Interactions (HOIs) are conditioned on spatial and temporalcontexts like surrounding objects, pre- vious actions, and future intents (forexample, grasping and handover actions vary greatly based on objects proximityand trajectory obstruction). However, existing datasets for 4D HOI (3D HOI overtime) are limited to one subject inter- acting with one object only. Thisrestricts the generalization of learning-based HOI methods trained on thosedatasets. We introduce ADL4D, a dataset of up to two subjects inter- actingwith different sets of objects performing Activities of Daily Living (ADL) likebreakfast or lunch preparation ac- tivities. The transition between multipleobjects to complete a certain task over time introduces a unique contextlacking in existing datasets. Our dataset consists of 75 sequences with a totalof 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained actionannotations. We develop an automatic system for multi-view multi-hand 3D posean- notation capable of tracking hand poses over time. We inte- grate and testit against publicly available datasets. Finally, we evaluate our dataset on thetasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).</description><author>Marsil Zakour, Partha Pratim Nath, Ludwig Lohmer, Emre Faik Gökçe, Martin Piccolrovazzi, Constantin Patsch, Yuankai Wu, Rahul Chaudhari, Eckehard Steinbach</author><pubDate>Tue, 27 Feb 2024 18:51:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17758v1</guid></item><item><title>Dense Matchers for Dense Tracking</title><link>http://arxiv.org/abs/2402.11287v1</link><description>Optical flow is a useful input for various applications, including 3Dreconstruction, pose estimation, tracking, and structure-from-motion. Despiteits utility, the field of dense long-term tracking, especially over widebaselines, has not been extensively explored. This paper extends the concept ofcombining multiple optical flows over logarithmically spaced intervals asproposed by MFT. We demonstrate the compatibility of MFT with different opticalflow networks, yielding results that surpass their individual performance.Moreover, we present a simple yet effective combination of these networkswithin the MFT framework. This approach proves to be competitive with moresophisticated, non-causal methods in terms of position prediction accuracy,highlighting the potential of MFT in enhancing long-term tracking applications.</description><author>Tomáš Jelínek, Jonáš Šerých, Jiří Matas</author><pubDate>Sat, 17 Feb 2024 14:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11287v1</guid></item><item><title>Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation</title><link>http://arxiv.org/abs/2311.17532v3</link><description>Generating vivid and emotional 3D co-speech gestures is crucial for virtualavatar animation in human-machine interaction applications. While the existingmethods enable generating the gestures to follow a single emotion label, theyoverlook that long gesture sequence modeling with emotion transition is morepractical in real scenes. In addition, the lack of large-scale availabledatasets with emotional transition speech and corresponding 3D human gesturesalso limits the addressing of this task. To fulfill this goal, we firstincorporate the ChatGPT-4 and an audio inpainting approach to construct thehigh-fidelity emotion transition human speeches. Considering obtaining therealistic 3D pose annotations corresponding to the dynamically inpaintedemotion transition audio is extremely difficult, we propose a novel weaklysupervised training strategy to encourage authority gesture transitions.Specifically, to enhance the coordination of transition gestures w.r.tdifferent emotional ones, we model the temporal association representationbetween two different emotional gesture sequences as style guidance and infuseit into the transition generation. We further devise an emotion mixturemechanism that provides weak supervision based on a learnable mixed emotionlabel for transition gestures. Last, we present a keyframe sampler to supplyeffective initial posture cues in long sequences, enabling us to generatediverse gestures. Extensive experiments demonstrate that our method outperformsthe state-of-the-art models constructed by adapting single emotion-conditionedcounterparts on our newly defined emotion transition task and datasets. Ourcode and dataset will be released on the project page:https://xingqunqi-lab.github.io/Emo-Transition-Gesture/.</description><author>Xingqun Qi, Jiahao Pan, Peng Li, Ruibin Yuan, Xiaowei Chi, Mengfei Li, Wenhan Luo, Wei Xue, Shanghang Zhang, Qifeng Liu, Yike Guo</author><pubDate>Wed, 27 Mar 2024 16:01:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17532v3</guid></item><item><title>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</title><link>http://arxiv.org/abs/2403.08764v1</link><description>We propose VLOGGER, a method for audio-driven human video generation from asingle input image of a person, which builds on the success of recentgenerative diffusion models. Our method consists of 1) a stochastichuman-to-3d-motion diffusion model, and 2) a novel diffusion-based architecturethat augments text-to-image models with both spatial and temporal controls.This supports the generation of high quality video of variable length, easilycontrollable through high-level representations of human faces and bodies. Incontrast to previous work, our method does not require training for eachperson, does not rely on face detection and cropping, generates the completeimage (not just the face or the lips), and considers a broad spectrum ofscenarios (e.g. visible torso or diverse subject identities) that are criticalto correctly synthesize humans who communicate. We also curate MENTOR, a newand diverse dataset with 3d pose and expression annotations, one order ofmagnitude larger than previous ones (800,000 identities) and with dynamicgestures, on which we train and ablate our main technical contributions. VLOGGER outperforms state-of-the-art methods in three public benchmarks,considering image quality, identity preservation and temporal consistency whilealso generating upper-body gestures. We analyze the performance of VLOGGER withrespect to multiple diversity metrics, showing that our architectural choicesand the use of MENTOR benefit training a fair and unbiased model at scale.Finally we show applications in video editing and personalization.</description><author>Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, Cristian Sminchisescu</author><pubDate>Wed, 13 Mar 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08764v1</guid></item><item><title>iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via Comparing and Matching</title><link>http://arxiv.org/abs/2312.09031v2</link><description>We present a method named iComMa to address the 6D camera pose estimationproblem in computer vision. Conventional pose estimation methods typically relyon the target's CAD model or necessitate specific network training tailored toparticular object classes. Some existing methods have achieved promisingresults in mesh-free object and scene pose estimation by inverting the NeuralRadiance Fields (NeRF). However, they still struggle with adverseinitializations such as large rotations and translations. To address thisissue, we propose an efficient method for accurate camera pose estimation byinverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-baseddifferentiable framework optimizes camera pose by minimizing the residualbetween the query image and the rendered image, requiring no training. Anend-to-end matching module is designed to enhance the model's robustnessagainst adverse initializations, while minimizing pixel-level comparing lossaids in precise pose estimation. Experimental results on synthetic and complexreal-world data demonstrate the effectiveness of the proposed approach inchallenging conditions and the accuracy of camera pose estimation.</description><author>Yuan Sun, Xuan Wang, Yunfan Zhang, Jie Zhang, Caigui Jiang, Yu Guo, Fei Wang</author><pubDate>Wed, 20 Mar 2024 13:00:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09031v2</guid></item><item><title>A Survey on 3D Egocentric Human Pose Estimation</title><link>http://arxiv.org/abs/2403.17893v1</link><description>Egocentric human pose estimation aims to estimate human body poses anddevelop body representations from a first-person camera perspective. It hasgained vast popularity in recent years because of its wide range ofapplications in sectors like XR-technologies, human-computer interaction, andfitness tracking. However, to the best of our knowledge, there is no systematicliterature review based on the proposed solutions regarding egocentric 3D humanpose estimation. To that end, the aim of this survey paper is to provide anextensive overview of the current state of egocentric pose estimation research.In this paper, we categorize and discuss the popular datasets and the differentpose estimation models, highlighting the strengths and weaknesses of differentmethods by comparative analysis. This survey can be a valuable resource forboth researchers and practitioners in the field, offering insights into keyconcepts and cutting-edge solutions in egocentric pose estimation, itswide-ranging applications, as well as the open problems with future scope.</description><author>Md Mushfiqur Azam, Kevin Desai</author><pubDate>Tue, 26 Mar 2024 18:29:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17893v1</guid></item><item><title>Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence</title><link>http://arxiv.org/abs/2403.19160v1</link><description>Neural rendering techniques have significantly advanced 3D human bodymodeling. However, previous approaches often overlook dynamics induced byfactors such as motion inertia, leading to challenges in scenarios like abruptstops after rotation, where the pose remains static while the appearancechanges. This limitation arises from reliance on a single pose as conditionalinput, resulting in ambiguity in mapping one pose to multiple appearances. Inthis study, we elucidate that variations in human appearance depend not only onthe current frame's pose condition but also on past pose states. Therefore, weintroduce Dyco, a novel method utilizing the delta pose sequence representationfor non-rigid deformations and canonical space to effectively model temporalappearance variations. To prevent a decrease in the model's generalizationability to novel poses, we further propose low-dimensional global context toreduce unnecessary inter-body part dependencies and a quantization operation tomitigate overfitting of the delta pose sequence by the model. To validate theeffectiveness of our approach, we collected a novel dataset named I3D-Human,with a focus on capturing temporal changes in clothing appearance underapproximate poses. Through extensive experiments on both I3D-Human and existingdatasets, our approach demonstrates superior qualitative and quantitativeperformance. In addition, our inertia-aware 3D human method can unprecedentedlysimulate appearance changes caused by inertia at different velocities.</description><author>Yutong Chen, Yifan Zhan, Zhihang Zhong, Wei Wang, Xiao Sun, Yu Qiao, Yinqiang Zheng</author><pubDate>Thu, 28 Mar 2024 07:05:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19160v1</guid></item><item><title>CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization</title><link>http://arxiv.org/abs/2402.17214v2</link><description>In the field of digital content creation, generating high-quality 3Dcharacters from single images is challenging, especially given the complexitiesof various body poses and the issues of self-occlusion and pose ambiguity. Inthis paper, we present CharacterGen, a framework developed to efficientlygenerate 3D characters. CharacterGen introduces a streamlined generationpipeline along with an image-conditioned multi-view diffusion model. This modeleffectively calibrates input poses to a canonical form while retaining keyattributes of the input image, thereby addressing the challenges posed bydiverse poses. A transformer-based, generalizable sparse-view reconstructionmodel is the other core component of our approach, facilitating the creation ofdetailed 3D models from multi-view images. We also adopt atexture-back-projection strategy to produce high-quality texture maps.Additionally, we have curated a dataset of anime characters, rendered inmultiple poses and views, to train and evaluate our model. Our approach hasbeen thoroughly evaluated through quantitative and qualitative experiments,showing its proficiency in generating 3D characters with high-quality shapesand textures, ready for downstream applications such as rigging and animation.</description><author>Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu</author><pubDate>Wed, 28 Feb 2024 08:13:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17214v2</guid></item><item><title>Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation</title><link>http://arxiv.org/abs/2403.12728v1</link><description>Fully-supervised category-level pose estimation aims to determine the 6-DoFposes of unseen instances from known categories, requiring expensive mannuallabeling costs. Recently, various self-supervised category-level poseestimation methods have been proposed to reduce the requirement of theannotated datasets. However, most methods rely on synthetic data or 3D CADmodel for self-supervised training, and they are typically limited toaddressing single-object pose problems without considering multi-objectivetasks or shape reconstruction. To overcome these challenges and limitations, weintroduce a diffusion-driven self-supervised network for multi-object shapereconstruction and categorical pose estimation, only leveraging the shapepriors. Specifically, to capture the SE(3)-equivariant pose features and 3Dscale-invariant shape information, we present a Prior-Aware Pyramid 3D PointTransformer in our network. This module adopts a point convolutional layer withradial-kernels for pose-aware learning and a 3D scale-invariant graphconvolution layer for object-level shape representation, respectively.Furthermore, we introduce a pretrain-to-refine self-supervised trainingparadigm to train our network. It enables proposed network to capture theassociations between shape priors and observations, addressing the challenge ofintra-class shape variations by utilising the diffusion mechanism. Extensiveexperiments conducted on four public datasets and a self-built datasetdemonstrate that our method significantly outperforms state-of-the-artself-supervised category-level baselines and even surpasses somefully-supervised instance-level and category-level methods.</description><author>Jingtao Sun, Yaonan Wang, Mingtao Feng, Chao Ding, Mike Zheng Shou, Ajmal Saeed Mian</author><pubDate>Tue, 19 Mar 2024 14:43:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12728v1</guid></item><item><title>MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation</title><link>http://arxiv.org/abs/2403.08019v2</link><description>We propose a single-shot approach to determining 6-DoF pose of an object withavailable 3D computer-aided design (CAD) model from a single RGB image. Ourmethod, dubbed MRC-Net, comprises two stages. The first performs poseclassification and renders the 3D object in the classified pose. The secondstage performs regression to predict fine-grained residual pose within class.Connecting the two stages is a novel multi-scale residual correlation (MRC)layer that captures high-and-low level correspondences between the input imageand rendering from first stage. MRC-Net employs a Siamese network with sharedweights between both stages to learn embeddings for input and rendered images.To mitigate ambiguity when predicting discrete pose class labels on symmetricobjects, we use soft probabilistic labels to define pose class in the firststage. We demonstrate state-of-the-art accuracy, outperforming all competingRGB-based methods on four challenging BOP benchmark datasets: T-LESS, LM-O,YCB-V, and ITODD. Our method is non-iterative and requires no complexpost-processing.</description><author>Yuelong Li, Yafei Mao, Raja Bala, Sunil Hadap</author><pubDate>Fri, 15 Mar 2024 18:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08019v2</guid></item><item><title>Lester: rotoscope animation through video object segmentation and tracking</title><link>http://arxiv.org/abs/2402.09883v1</link><description>This article introduces Lester, a novel method to automatically synthetiseretro-style 2D animations from videos. The method approaches the challengemainly as an object segmentation and tracking problem. Video frames areprocessed with the Segment Anything Model (SAM) and the resulting masks aretracked through subsequent frames with DeAOT, a method of hierarchicalpropagation for semi-supervised video object segmentation. The geometry of themasks' contours is simplified with the Douglas-Peucker algorithm. Finally,facial traits, pixelation and a basic shadow effect can be optionally added.The results show that the method exhibits an excellent temporal consistency andcan correctly process videos with different poses and appearances, dynamicshots, partial shots and diverse backgrounds. The proposed method provides amore simple and deterministic approach than diffusion models basedvideo-to-video translation pipelines, which suffer from temporal consistencyproblems and do not cope well with pixelated and schematic outputs. The methodis also much most practical than techniques based on 3D human pose estimation,which require custom handcrafted 3D models and are very limited with respect tothe type of scenes they can process.</description><author>Ruben Tous</author><pubDate>Thu, 15 Feb 2024 11:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09883v1</guid></item><item><title>3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface</title><link>http://arxiv.org/abs/2403.08768v1</link><description>This paper introduces 3DFIRES, a novel system for scene-level 3Dreconstruction from posed images. Designed to work with as few as one view,3DFIRES reconstructs the complete geometry of unseen scenes, including hiddensurfaces. With multiple view inputs, our method produces full reconstructionwithin all camera frustums. A key feature of our approach is the fusion ofmulti-view information at the feature level, enabling the production ofcoherent and comprehensive 3D reconstruction. We train our system onnon-watertight scans from large-scale real scene dataset. We show it matchesthe efficacy of single-view reconstruction methods with only one input andsurpasses existing techniques in both quantitative and qualitative measures forsparse-view 3D reconstruction.</description><author>Linyi Jin, Nilesh Kulkarni, David Fouhey</author><pubDate>Wed, 13 Mar 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08768v1</guid></item><item><title>Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video</title><link>http://arxiv.org/abs/2312.04784v2</link><description>Recent advancements in 3D avatar generation excel with multi-view supervisionfor photorealistic models. However, monocular counterparts lag in qualitydespite broader applicability. We propose ReCaLaB to close this gap. ReCaLaB isa fully-differentiable pipeline that learns high-fidelity 3D human avatars fromjust a single RGB video. A pose-conditioned deformable NeRF is optimized tovolumetrically represent a human subject in canonical T-pose. The canonicalrepresentation is then leveraged to efficiently associate neural textures using2D-3D correspondences. This enables the separation of diffused color generationand lighting correction branches that jointly compose an RGB prediction. Thedesign allows to control intermediate results for human pose, body shape,texture, and lighting with text prompts. An image-conditioned diffusion modelthereby helps to animate appearance and pose of the 3D avatar to create videosequences with previously unseen human motion. Extensive experiments show thatReCaLaB outperforms previous monocular approaches in terms of image quality forimage synthesis tasks. Moreover, natural language offers an intuitive userinterface for creative manipulation of 3D human avatars.</description><author>Yuchen Rao, Eduardo Perez Pellitero, Benjamin Busam, Yiren Zhou, Jifei Song</author><pubDate>Sun, 24 Mar 2024 14:56:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04784v2</guid></item><item><title>Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach</title><link>http://arxiv.org/abs/2402.19062v2</link><description>To facilitate diagnosis on cardiac ultrasound (US), clinical practice hasestablished several standard views of the heart, which serve as referencepoints for diagnostic measurements and define viewports from which images areacquired. Automatic view recognition involves grouping those images intoclasses of standard views. Although deep learning techniques have beensuccessful in achieving this, they still struggle with fully verifying thesuitability of an image for specific measurements due to factors like thecorrect location, pose, and potential occlusions of cardiac structures. Ourapproach goes beyond view classification and incorporates a 3D meshreconstruction of the heart that enables several more downstream tasks, likesegmentation and pose estimation. In this work, we explore learning 3D heartmeshes via graph convolutions, using similar techniques to learn 3D meshes innatural images, such as human pose estimation. As the availability of fullyannotated 3D images is limited, we generate synthetic US images from 3D meshesby training an adversarial denoising diffusion model. Experiments wereconducted on synthetic and clinical cases for view recognition and structuredetection. The approach yielded good performance on synthetic images and,despite being exclusively trained on synthetic data, it already showedpotential when applied to clinical images. With this proof-of-concept, we aimto demonstrate the benefits of graphs to improve cardiac view recognition thatcan ultimately lead to better efficiency in cardiac diagnosis.</description><author>Sarina Thomas, Cristiana Tiago, Børge Solli Andreassen, Svein Arne Aase, Jurica Šprem, Erik Steen, Anne Solberg, Guy Ben-Yosef</author><pubDate>Fri, 01 Mar 2024 08:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19062v2</guid></item><item><title>Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach</title><link>http://arxiv.org/abs/2402.19062v1</link><description>To facilitate diagnosis on cardiac ultrasound (US), clinical practice hasestablished several standard views of the heart, which serve as referencepoints for diagnostic measurements and define viewports from which images areacquired. Automatic view recognition involves grouping those images intoclasses of standard views. Although deep learning techniques have beensuccessful in achieving this, they still struggle with fully verifying thesuitability of an image for specific measurements due to factors like thecorrect location, pose, and potential occlusions of cardiac structures. Ourapproach goes beyond view classification and incorporates a 3D meshreconstruction of the heart that enables several more downstream tasks, likesegmentation and pose estimation. In this work, we explore learning 3D heartmeshes via graph convolutions, using similar techniques to learn 3D meshes innatural images, such as human pose estimation. As the availability of fullyannotated 3D images is limited, we generate synthetic US images from 3D meshesby training an adversarial denoising diffusion model. Experiments wereconducted on synthetic and clinical cases for view recognition and structuredetection. The approach yielded good performance on synthetic images and,despite being exclusively trained on synthetic data, it already showedpotential when applied to clinical images. With this proof-of-concept, we aimto demonstrate the benefits of graphs to improve cardiac view recognition thatcan ultimately lead to better efficiency in cardiac diagnosis.</description><author>Sarina Thomas, Cristiana Tiago, Børge Solli Andreassen, Svein-Arne Aase, Jurica Sprem, Erik Steen, Anne Solberg, Guy Ben-Yosef</author><pubDate>Thu, 29 Feb 2024 11:45:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19062v1</guid></item><item><title>Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance</title><link>http://arxiv.org/abs/2403.18036v1</link><description>Despite significant advancements in text-to-motion synthesis, generatinglanguage-guided human motion within 3D environments poses substantialchallenges. These challenges stem primarily from (i) the absence of powerfulgenerative models capable of jointly modeling natural language, 3D scenes, andhuman motion, and (ii) the generative models' intensive data requirementscontrasted with the scarcity of comprehensive, high-quality,language-scene-motion datasets. To tackle these issues, we introduce a noveltwo-stage framework that employs scene affordance as an intermediaterepresentation, effectively linking 3D scene grounding and conditional motiongeneration. Our framework comprises an Affordance Diffusion Model (ADM) forpredicting explicit affordance map and an Affordance-to-Motion Diffusion Model(AMDM) for generating plausible human motions. By leveraging scene affordancemaps, our method overcomes the difficulty in generating human motion undermultimodal condition signals, especially when training with limited datalacking extensive language-scene-motion pairs. Our extensive experimentsdemonstrate that our approach consistently outperforms all baselines onestablished benchmarks, including HumanML3D and HUMANISE. Additionally, wevalidate our model's exceptional generalization capabilities on a speciallycurated evaluation set featuring previously unseen descriptions and scenes.</description><author>Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang</author><pubDate>Tue, 26 Mar 2024 19:41:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18036v1</guid></item><item><title>3D Human Reconstruction in the Wild with Synthetic Data Using Generative Models</title><link>http://arxiv.org/abs/2403.11111v1</link><description>In this work, we show that synthetic data created by generative models iscomplementary to computer graphics (CG) rendered data for achieving remarkablegeneralization performance on diverse real-world scenes for 3D human pose andshape estimation (HPS). Specifically, we propose an effective approach based onrecent diffusion models, termed HumanWild, which can effortlessly generatehuman images and corresponding 3D mesh annotations. We first collect alarge-scale human-centric dataset with comprehensive annotations, e.g., textcaptions and surface normal images. Then, we train a customized ControlNetmodel upon this dataset to generate diverse human images and initialground-truth labels. At the core of this step is that we can easily obtainnumerous surface normal images from a 3D human parametric model, e.g., SMPL-X,by rendering the 3D mesh onto the image plane. As there exists inevitable noisein the initial labels, we then apply an off-the-shelf foundation segmentationmodel, i.e., SAM, to filter negative data samples. Our data generation pipelineis flexible and customizable to facilitate different real-world tasks, e.g.,ego-centric scenes and perspective-distortion scenes. The generated datasetcomprises 0.79M images with corresponding 3D annotations, covering versatileviewpoints, scenes, and human identities. We train various HPS regressors ontop of the generated data and evaluate them on a wide range of benchmarks(3DPW, RICH, EgoBody, AGORA, SSP-3D) to verify the effectiveness of thegenerated data. By exclusively employing generative models, we generatelarge-scale in-the-wild human images and high-quality annotations, eliminatingthe need for real-world data collection.</description><author>Yongtao Ge, Wenjia Wang, Yongfan Chen, Hao Chen, Chunhua Shen</author><pubDate>Sun, 17 Mar 2024 07:31:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11111v1</guid></item><item><title>CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model</title><link>http://arxiv.org/abs/2403.05034v1</link><description>Feed-forward 3D generative models like the Large Reconstruction Model (LRM)have demonstrated exceptional generation speed. However, the transformer-basedmethods do not leverage the geometric priors of the triplane component in theirarchitecture, often leading to sub-optimal quality given the limited size of 3Ddata and slow training. In this work, we present the ConvolutionalReconstruction Model (CRM), a high-fidelity feed-forward single image-to-3Dgenerative model. Recognizing the limitations posed by sparse 3D data, wehighlight the necessity of integrating geometric priors into network design.CRM builds on the key observation that the visualization of triplane exhibitsspatial correspondence of six orthographic images. First, it generates sixorthographic view images from a single input image, then feeds these imagesinto a convolutional U-Net, leveraging its strong pixel-level alignmentcapabilities and significant bandwidth to create a high-resolution triplane.CRM further employs Flexicubes as geometric representation, facilitating directend-to-end optimization on textured meshes. Overall, our model delivers ahigh-fidelity textured mesh from an image in just 10 seconds, without anytest-time optimization.</description><author>Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, Jun Zhu</author><pubDate>Fri, 08 Mar 2024 04:25:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05034v1</guid></item><item><title>Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation</title><link>http://arxiv.org/abs/2403.14559v1</link><description>Localizing predefined 3D keypoints in a 2D image is an effective way toestablish 3D-2D correspondences for 6DoF object pose estimation. However,unreliable localization results of invisible keypoints degrade the quality ofcorrespondences. In this paper, we address this issue by localizing theimportant keypoints in terms of visibility. Since keypoint visibilityinformation is currently missing in dataset collection process, we propose anefficient way to generate binary visibility labels from available object-levelannotations, for keypoints of both asymmetric objects and symmetric objects. Wefurther derive real-valued visibility-aware importance from binary labels basedon PageRank algorithm. Taking advantage of the flexibility of ourvisibility-aware importance, we construct VAPO (Visibility-Aware POseestimator) by integrating the visibility-aware importance with astate-of-the-art pose estimation algorithm, along with additional positionalencoding. Extensive experiments are conducted on popular pose estimationbenchmarks including Linemod, Linemod-Occlusion, and YCB-V. The results showthat, VAPO improves both the keypoint correspondences and final estimatedposes, and clearly achieves state-of-the-art performances.</description><author>Ruyi Lian, Haibin Ling</author><pubDate>Thu, 21 Mar 2024 17:59:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14559v1</guid></item><item><title>RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training</title><link>http://arxiv.org/abs/2403.09948v1</link><description>The integration of artificial intelligence (AI) with radiology has marked atransformative era in medical diagnostics. Vision foundation models have beenadopted to enhance radiologic imaging analysis. However, the distinctcomplexities of radiological imaging, including the interpretation of 2D and 3Dradiological data, pose unique challenges that existing models, trained ongeneral non-medical images, fail to address adequately. To bridge this gap andcapitalize on the diagnostic precision required in medical imaging, weintroduce RadCLIP: a pioneering cross-modal foundational model that harnessesContrastive Language-Image Pre-training (CLIP) to refine radiologic imageanalysis. RadCLIP incorporates a novel 3D slice pooling mechanism tailored forvolumetric image analysis and is trained using a comprehensive and diversedataset of radiologic image-text pairs. Our evaluations demonstrate thatRadCLIP effectively aligns radiological images with their corresponding textualannotations, and in the meantime, offers a robust vision backbone forradiologic imagery with significant promise.</description><author>Zhixiu Lu, Hailong Li, Lili He</author><pubDate>Fri, 15 Mar 2024 02:18:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09948v1</guid></item><item><title>SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</title><link>http://arxiv.org/abs/2311.11125v3</link><description>Category-level object pose estimation, aiming to predict the 6D pose and 3Dsize of objects from known categories, typically struggles with largeintra-class shape variation. Existing works utilizing mean shapes often fallshort of capturing this variation. To address this issue, we presentSecondPose, a novel approach integrating object-specific geometric featureswith semantic category priors from DINOv2. Leveraging the advantage of DINOv2in providing SE(3)-consistent semantic features, we hierarchically extract twotypes of SE(3)-invariant geometric features to further encapsulatelocal-to-global object-specific information. These geometric features are thenpoint-aligned with DINOv2 features to establish a consistent objectrepresentation under SE(3) transformations, facilitating the mapping fromcamera space to the pre-defined canonical space, thus further enhancing poseestimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPoseachieves a 12.4% leap forward over the state-of-the-art. Moreover, on a morecomplex dataset HouseCat6D which provides photometrically challenging objects,SecondPose still surpasses other competitors by a large margin.</description><author>Yamei Chen, Yan Di, Guangyao Zhai, Fabian Manhardt, Chenyangguang Zhang, Ruida Zhang, Federico Tombari, Nassir Navab, Benjamin Busam</author><pubDate>Fri, 22 Mar 2024 01:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11125v3</guid></item><item><title>Score-Guided Diffusion for 3D Human Recovery</title><link>http://arxiv.org/abs/2403.09623v1</link><description>We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach forsolving inverse problems for 3D human pose and shape reconstruction. Theseinverse problems involve fitting a human body model to image observations,traditionally solved through optimization techniques. ScoreHMR mimics modelfitting approaches, but alignment with the image observation is achievedthrough score guidance in the latent space of a diffusion model. The diffusionmodel is trained to capture the conditional distribution of the human modelparameters given an input image. By guiding its denoising process with atask-specific score, ScoreHMR effectively solves inverse problems for variousapplications without the need for retraining the task-agnostic diffusion model.We evaluate our approach on three settings/applications. These are: (i)single-frame model fitting; (ii) reconstruction from multiple uncalibratedviews; (iii) reconstructing humans in video sequences. ScoreHMR consistentlyoutperforms all optimization baselines on popular benchmarks across allsettings. We make our code and models available at thehttps://statho.github.io/ScoreHMR.</description><author>Anastasis Stathopoulos, Ligong Han, Dimitris Metaxas</author><pubDate>Thu, 14 Mar 2024 18:56:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09623v1</guid></item><item><title>MogaNet: Multi-order Gated Aggregation Network</title><link>http://arxiv.org/abs/2211.03295v3</link><description>By contextualizing the kernel as global as possible, Modern ConvNets haveshown great potential in computer vision tasks. However, recent progress on\textit{multi-order game-theoretic interaction} within deep neural networks(DNNs) reveals the representation bottleneck of modern ConvNets, where theexpressive interactions have not been effectively encoded with the increasedkernel size. To tackle this challenge, we propose a new family of modernConvNets, dubbed MogaNet, for discriminative visual representation learning inpure ConvNet-based models with favorable complexity-performance trade-offs.MogaNet encapsulates conceptually simple yet effective convolutions and gatedaggregation into a compact module, where discriminative features areefficiently gathered and contextualized adaptively. MogaNet exhibits greatscalability, impressive efficiency of parameters, and competitive performancecompared to state-of-the-art ViTs and ConvNets on ImageNet and variousdownstream vision benchmarks, including COCO object detection, ADE20K semanticsegmentation, 2D\&amp;3D human pose estimation, and video prediction. Notably,MogaNet hits 80.0\% and 87.8\% accuracy with 5.2M and 181M parameters onImageNet-1K, outperforming ParC-Net and ConvNeXt-L, while saving 59\% FLOPs and17M parameters, respectively. The source code is available at\url{https://github.com/Westlake-AI/MogaNet}.</description><author>Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Li</author><pubDate>Fri, 16 Feb 2024 14:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03295v3</guid></item><item><title>Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance</title><link>http://arxiv.org/abs/2403.14781v1</link><description>In this study, we introduce a methodology for human image animation byleveraging a 3D human parametric model within a latent diffusion framework toenhance shape alignment and motion guidance in curernt human generativetechniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear)model as the 3D human parametric model to establish a unified representation ofbody shape and pose. This facilitates the accurate capture of intricate humangeometry and motion characteristics from source videos. Specifically, weincorporate rendered depth images, normal maps, and semantic maps obtained fromSMPL sequences, alongside skeleton-based motion guidance, to enrich theconditions to the latent diffusion model with comprehensive 3D shape anddetailed pose attributes. A multi-layer motion fusion module, integratingself-attention mechanisms, is employed to fuse the shape and motion latentrepresentations in the spatial domain. By representing the 3D human parametricmodel as the motion guidance, we can perform parametric shape alignment of thehuman body between the reference image and the source video motion.Experimental evaluations conducted on benchmark datasets demonstrate themethodology's superior ability to generate high-quality human animations thataccurately capture both pose and shape variations. Furthermore, our approachalso exhibits superior generalization capabilities on the proposed wilddataset. Project page: https://fudan-generative-vision.github.io/champ.</description><author>Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, Siyu Zhu</author><pubDate>Thu, 21 Mar 2024 19:52:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14781v1</guid></item><item><title>Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment</title><link>http://arxiv.org/abs/2402.04599v2</link><description>Video sequences exhibit significant nuisance variations (undesired effects)of speed of actions, temporal locations, and subjects' poses, leading totemporal-viewpoint misalignment when comparing two sets of frames or evaluatingthe similarity of two sequences. Thus, we propose Joint tEmporal and cAmeraviewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3Dskeleton sequences whose camera and subjects' poses can be easily manipulatedin 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), wherematching well temporal blocks (temporal chunks that make up a sequence) ofsupport-query sequence pairs (by factoring out nuisance variations) isessential due to limited samples of novel classes. Given a query sequence, wecreate its several views by simulating several camera locations. For a supportsequence, we match it with view-simulated query sequences, as in the popularDynamic Time Warping (DTW). Specifically, each support temporal block can bematched to the query temporal block with the same or adjacent (next) temporalindex, and adjacent camera views to achieve joint local temporal-viewpointwarping. JEANIE selects the smallest distance among matching paths withdifferent temporal-viewpoint warping patterns, an advantage over DTW which onlyperforms temporal alignment. We also propose an unsupervised FSAR akin toclustering of sequences with JEANIE as a distance measure. JEANIE achievesstate-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3DMultiview Activity II on supervised and unsupervised FSAR, and theirmeta-learning inspired fusion.</description><author>Lei Wang, Jun Liu, Liang Zheng, Tom Gedeon, Piotr Koniusz</author><pubDate>Mon, 25 Mar 2024 14:30:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04599v2</guid></item><item><title>MagicDrive: Street View Generation with Diverse 3D Geometry Control</title><link>http://arxiv.org/abs/2310.02601v6</link><description>Recent advancements in diffusion models have significantly enhanced the datasynthesis with 2D control. Yet, precise 3D control in street view generation,crucial for 3D perception tasks, remains elusive. Specifically, utilizingBird's-Eye View (BEV) as the primary condition often leads to challenges ingeometry control (e.g., height), affecting the representation of object shapes,occlusion patterns, and road surface elevations, all of which are essential toperception data synthesis, especially for 3D object detection tasks. In thispaper, we introduce MagicDrive, a novel street view generation framework,offering diverse 3D geometry controls including camera poses, road maps, and 3Dbounding boxes, together with textual descriptions, achieved through tailoredencoding strategies. Besides, our design incorporates a cross-view attentionmodule, ensuring consistency across multiple camera views. With MagicDrive, weachieve high-fidelity street-view image &amp; video synthesis that captures nuanced3D geometry and various scene descriptions, enhancing tasks like BEVsegmentation and 3D object detection.</description><author>Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, Qiang Xu</author><pubDate>Fri, 01 Mar 2024 06:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02601v6</guid></item><item><title>GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos</title><link>http://arxiv.org/abs/2402.16607v2</link><description>In this paper, we present a novel method that facilitates the creation ofvivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovationlies in addressing the intricate challenges of delivering high-fidelity humanbody reconstructions and aligning 3D Gaussians with human skin surfacesaccurately. The key contributions of this paper are twofold. Firstly, weintroduce a pose refinement technique to improve hand and foot pose accuracy byaligning normal maps and silhouettes. Precise pose is crucial for correct shapeand appearance reconstruction. Secondly, we address the problems of unbalancedaggregation and initialization bias that previously diminished the quality of3D Gaussian avatars, through a novel surface-guided re-initialization methodthat ensures accurate alignment of 3D Gaussian points with avatar surfaces.Experimental results demonstrate that our proposed method achieveshigh-fidelity and vivid 3D Gaussian avatar reconstruction. Extensiveexperimental analyses validate the performance qualitatively andquantitatively, demonstrating that it achieves state-of-the-art performance inphoto-realistic novel view synthesis while offering fine-grained control overthe human body and hand pose. Project page: https://3d-aigc.github.io/GVA/.</description><author>Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang</author><pubDate>Tue, 19 Mar 2024 09:58:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.16607v2</guid></item><item><title>Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing</title><link>http://arxiv.org/abs/2402.17464v3</link><description>Generative 3D part assembly involves understanding part relationships andpredicting their 6-DoF poses for assembling a realistic 3D shape. Prior workoften focus on the geometry of individual parts, neglecting part-wholehierarchies of objects. Leveraging two key observations: 1) super-part posesprovide strong hints about part poses, and 2) predicting super-part poses iseasier due to fewer superparts, we propose a part-whole-hierarchy messagepassing network for efficient 3D part assembly. We first introduce super-partsby grouping geometrically similar parts without any semantic labels. Then weemploy a part-whole hierarchical encoder, wherein a super-part encoder predictslatent super-part poses based on input parts. Subsequently, we transform thepoint cloud using the latent poses, feeding it to the part encoder foraggregating super-part information and reasoning about part relationships topredict all part poses. In training, only ground-truth part poses are required.During inference, the predicted latent poses of super-parts enhanceinterpretability. Experimental results on the PartNet dataset show that ourmethod achieves state-of-the-art performance in part and connectivity accuracyand enables an interpretable hierarchical part assembly. Code is available athttps://github.com/pkudba/3DHPA.</description><author>Bi'an Du, Xiang Gao, Wei Hu, Renjie Liao</author><pubDate>Wed, 27 Mar 2024 04:13:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17464v3</guid></item><item><title>Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing</title><link>http://arxiv.org/abs/2402.17464v2</link><description>Generative 3D part assembly involves understanding part relationships andpredicting their 6-DoF poses for assembling a realistic 3D shape. Prior workoften focus on the geometry of individual parts, neglecting part-wholehierarchies of objects. Leveraging two key observations: 1) super-part posesprovide strong hints about part poses, and 2) predicting super-part poses iseasier due to fewer superparts, we propose a part-whole-hierarchy messagepassing network for efficient 3D part assembly. We first introduce super-partsby grouping geometrically similar parts without any semantic labels. Then weemploy a part-whole hierarchical encoder, wherein a super-part encoder predictslatent super-part poses based on input parts. Subsequently, we transform thepoint cloud using the latent poses, feeding it to the part encoder foraggregating super-part information and reasoning about part relationships topredict all part poses. In training, only ground-truth part poses are required.During inference, the predicted latent poses of super-parts enhanceinterpretability. Experimental results on the PartNet dataset show that ourmethod achieves state-of-the-art performance in part and connectivity accuracyand enables an interpretable hierarchical part assembly.</description><author>Bi'an Du, Xiang Gao, Wei Hu, Renjie Liao</author><pubDate>Tue, 26 Mar 2024 15:15:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17464v2</guid></item><item><title>DD-VNB: A Depth-based Dual-Loop Framework for Real-time Visually Navigated Bronchoscopy</title><link>http://arxiv.org/abs/2403.01683v2</link><description>Real-time 6 DOF localization of bronchoscopes is crucial for enhancingintervention quality. However, current vision-based technologies struggle tobalance between generalization to unseen data and computational speed. In thisstudy, we propose a Depth-based Dual-Loop framework for real-time VisuallyNavigated Bronchoscopy (DD-VNB) that can generalize across patient caseswithout the need of re-training. The DD-VNB framework integrates two keymodules: depth estimation and dual-loop localization. To address the domain gapamong patients, we propose a knowledge-embedded depth estimation network thatmaps endoscope frames to depth, ensuring generalization by eliminatingpatient-specific textures. The network embeds view synthesis knowledge into acycle adversarial architecture for scale-constrained monocular depthestimation. For real-time performance, our localization module embeds a fastego-motion estimation network into the loop of depth registration. Theego-motion inference network estimates the pose change of the bronchoscope inhigh frequency while depth registration against the pre-operative 3D modelprovides absolute pose periodically. Specifically, the relative pose changesare fed into the registration process as the initial guess to boost itsaccuracy and speed. Experiments on phantom and in-vivo data from patientsdemonstrate the effectiveness of our framework: 1) monocular depth estimationoutperforms SOTA, 2) localization achieves an accuracy of Absolute TrackingError (ATE) of 4.7 $\pm$ 3.17 mm in phantom and 6.49 $\pm$ 3.88 mm in patientdata, 3) with a frame-rate approaching video capture speed, 4) without thenecessity of case-wise network retraining. The framework's superior speed andaccuracy demonstrate its promising clinical potential for real-timebronchoscopic navigation.</description><author>Qingyao Tian, Huai Liao, Xinyan Huang, Jian Chen, Zihui Zhang, Bingyu Yang, Sebastien Ourselin, Hongbin Liu</author><pubDate>Fri, 15 Mar 2024 08:25:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01683v2</guid></item><item><title>MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction</title><link>http://arxiv.org/abs/2402.03762v3</link><description>Neural implicit representations have recently been demonstrated in manyfields including Simultaneous Localization And Mapping (SLAM). Current neuralSLAM can achieve ideal results in reconstructing bounded scenes, but thisrelies on the input of RGB-D images. Neural-based SLAM based only on RGB imagesis unable to reconstruct the scale of the scene accurately, and it also suffersfrom scale drift due to errors accumulated during tracking. To overcome theselimitations, we present MoD-SLAM, a monocular dense mapping method that allowsglobal pose optimization and 3D reconstruction in real-time in unboundedscenes. Optimizing scene reconstruction by monocular depth estimation and usingloop closure detection to update camera pose enable detailed and precisereconstruction on large scenes. Compared to previous work, our approach is morerobust, scalable and versatile. Our experiments demonstrate that MoD-SLAM hasmore excellent mapping performance than prior neural SLAM methods, especiallyin large borderless scenes.</description><author>Heng Zhou, Zhetao Guo, Shuhong Liu, Lechen Zhang, Qihao Wang, Yuxiang Ren, Mingrui Li</author><pubDate>Wed, 21 Feb 2024 17:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03762v3</guid></item><item><title>NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent Semantic Navigation</title><link>http://arxiv.org/abs/2304.11342v2</link><description>3D representation disentanglement aims to identify, decompose, and manipulatethe underlying explanatory factors of 3D data, which helps AI fundamentallyunderstand our 3D world. This task is currently under-explored and poses greatchallenges: (i) the 3D representations are complex and in general contains muchmore information than 2D image; (ii) many 3D representations are not wellsuited for gradient-based optimization, let alone disentanglement. To addressthese challenges, we use NeRF as a differentiable 3D representation, andintroduce a self-supervised Navigation to identify interpretable semanticdirections in the latent space. To our best knowledge, this novel method,dubbed NaviNeRF, is the first work to achieve fine-grained 3D disentanglementwithout any priors or supervisions. Specifically, NaviNeRF is built upon thegenerative NeRF pipeline, and equipped with an Outer Navigation Branch and anInner Refinement Branch. They are complementary -- the outer navigation is toidentify global-view semantic directions, and the inner refinement dedicates tofine-grained attributes. A synergistic loss is further devised to coordinatetwo branches. Extensive experiments demonstrate that NaviNeRF has a superiorfine-grained 3D disentanglement ability than the previous 3D-aware models. Itsperformance is also comparable to editing-oriented models relying on semanticor geometry priors.</description><author>Baao Xie, Bohan Li, Zequn Zhang, Junting Dong, Xin Jin, Jingyu Yang, Wenjun Zeng</author><pubDate>Thu, 28 Mar 2024 10:20:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.11342v2</guid></item><item><title>Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields</title><link>http://arxiv.org/abs/2402.13252v1</link><description>In this paper, we propose an algorithm that allows joint refinement of camerapose and scene geometry represented by decomposed low-rank tensor, using only2D images as supervision. First, we conduct a pilot study based on a 1D signaland relate our findings to 3D scenarios, where the naive joint poseoptimization on voxel-based NeRFs can easily lead to sub-optimal solutions.Moreover, based on the analysis of the frequency spectrum, we propose to applyconvolutional Gaussian filters on 2D and 3D radiance fields for acoarse-to-fine training schedule that enables joint camera pose optimization.Leveraging the decomposition property in decomposed low-rank tensor, our methodachieves an equivalent effect to brute-force 3D convolution with only incurringlittle computational overhead. To further improve the robustness and stabilityof joint optimization, we also propose techniques of smoothed 2D supervision,randomly scaled kernel parameters, and edge-guided loss mask. Extensivequantitative and qualitative evaluations demonstrate that our proposedframework achieves superior performance in novel view synthesis as well asrapid convergence for optimization.</description><author>Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu</author><pubDate>Tue, 20 Feb 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13252v1</guid></item><item><title>SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human Generation</title><link>http://arxiv.org/abs/2403.10166v1</link><description>With the development of neural radiance fields and generative models,numerous methods have been proposed for learning 3D human generation from 2Dimages. These methods allow control over the pose of the generated 3D human andenable rendering from different viewpoints. However, none of these methodsexplore semantic disentanglement in human image synthesis, i.e., they can notdisentangle the generation of different semantic parts, such as the body, tops,and bottoms. Furthermore, existing methods are limited to synthesize images at$512^2$ resolution due to the high computational cost of neural radiancefields. To address these limitations, we introduce SemanticHuman-HD, the firstmethod to achieve semantic disentangled human image synthesis. Notably,SemanticHuman-HD is also the first method to achieve 3D-aware image synthesisat $1024^2$ resolution, benefiting from our proposed 3D-aware super-resolutionmodule. By leveraging the depth maps and semantic masks as guidance for the3D-aware super-resolution, we significantly reduce the number of samplingpoints during volume rendering, thereby reducing the computational cost. Ourcomparative experiments demonstrate the superiority of our method. Theeffectiveness of each proposed component is also verified through ablationstudies. Moreover, our method opens up exciting possibilities for variousapplications, including 3D garment generation, semantic-aware image synthesis,controllable image synthesis, and out-of-domain image synthesis.</description><author>Peng Zheng, Tao Liu, Zili Yi, Rui Ma</author><pubDate>Fri, 15 Mar 2024 11:18:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10166v1</guid></item><item><title>FoldToken: Learning Protein Language via Vector Quantization and Beyond</title><link>http://arxiv.org/abs/2403.09673v2</link><description>Is there a foreign language describing protein sequences and structuressimultaneously? Protein structures, represented by continuous 3D points, havelong posed a challenge due to the contrasting modeling paradigms of discretesequences. We introduce \textbf{FoldTokenizer} to represent proteinsequence-structure as discrete symbols. This innovative approach involvesprojecting residue types and structures into a discrete space, guided by areconstruction loss for information preservation. We refer to the learneddiscrete symbols as \textbf{FoldToken}, and the sequence of FoldTokens servesas a new protein language, transforming the protein sequence-structure into aunified modality. We apply the created protein language on general backboneinpainting and antibody design tasks, building the first GPT-style model(\textbf{FoldGPT}) for sequence-structure co-generation with promising results.Key to our success is the substantial enhancement of the vector quantizationmodule, Soft Conditional Vector Quantization (\textbf{SoftCVQ}).</description><author>Zhangyang Gao, Cheng Tan, Jue Wang, Yufei Huang, Lirong Wu, Stan Z. Li</author><pubDate>Tue, 19 Mar 2024 06:29:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09673v2</guid></item><item><title>CLOAF: CoLlisiOn-Aware Human Flow</title><link>http://arxiv.org/abs/2403.09050v1</link><description>Even the best current algorithms for estimating body 3D shape and pose yieldresults that include body self-intersections. In this paper, we present CLOAF,which exploits the diffeomorphic nature of Ordinary Differential Equations toeliminate such self-intersections while still imposing body shape constraints.We show that, unlike earlier approaches to addressing this issue, ourscompletely eliminates the self-intersections without compromising the accuracyof the reconstructions. Being differentiable, CLOAF can be used to fine-tunepose and shape estimation baselines to improve their overall performance andeliminate self-intersections in their predictions. Furthermore, we demonstratehow our CLOAF strategy can be applied to practically any motion field inducedby the user. CLOAF also makes it possible to edit motion to interact with theenvironment without worrying about potential collision or loss of body-shapeprior.</description><author>Andrey Davydov, Martin Engilberge, Mathieu Salzmann, Pascal Fua</author><pubDate>Thu, 14 Mar 2024 03:38:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09050v1</guid></item><item><title>ROAM: Robust and Object-Aware Motion Generation Using Neural Pose Descriptors</title><link>http://arxiv.org/abs/2308.12969v2</link><description>Existing automatic approaches for 3D virtual character motion synthesissupporting scene interactions do not generalise well to new objects outsidetraining distributions, even when trained on extensive motion capture datasetswith diverse objects and annotated interactions. This paper addresses thislimitation and shows that robustness and generalisation to novel scene objectsin 3D object-aware character synthesis can be achieved by training a motionmodel with as few as one reference object. We leverage an implicit featurerepresentation trained on object-only datasets, which encodes anSE(3)-equivariant descriptor field around the object. Given an unseen objectand a reference pose-object pair, we optimise for the object-aware pose that isclosest in the feature space to the reference pose. Finally, we use l-NSM,i.e., our motion generation model that is trained to seamlessly transition fromlocomotion to object interaction with the proposed bidirectional pose blendingscheme. Through comprehensive numerical comparisons to state-of-the-art methodsand in a user study, we demonstrate substantial improvements in 3D virtualcharacter motion and interaction quality and robustness to scenarios withunseen objects. Our project page is available athttps://vcai.mpi-inf.mpg.de/projects/ROAM/.</description><author>Wanyue Zhang, Rishabh Dabral, Thomas Leimkühler, Vladislav Golyanik, Marc Habermann, Christian Theobalt</author><pubDate>Thu, 15 Feb 2024 08:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12969v2</guid></item><item><title>AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization</title><link>http://arxiv.org/abs/2308.10001v2</link><description>Neural Radiance Fields (NeRF) have shown promise in generating realisticnovel views from sparse scene images. However, existing NeRF approaches oftenencounter challenges due to the lack of explicit 3D supervision and imprecisecamera poses, resulting in suboptimal outcomes. To tackle these issues, wepropose AltNeRF -- a novel framework designed to create resilient NeRFrepresentations using self-supervised monocular depth estimation (SMDE) frommonocular videos, without relying on known camera poses. SMDE in AltNeRFmasterfully learns depth and pose priors to regulate NeRF training. The depthprior enriches NeRF's capacity for precise scene geometry depiction, while thepose prior provides a robust starting point for subsequent pose refinement.Moreover, we introduce an alternating algorithm that harmoniously melds NeRFoutputs into SMDE through a consistence-driven mechanism, thus enhancing theintegrity of depth priors. This alternation empowers AltNeRF to progressivelyrefine NeRF representations, yielding the synthesis of realistic novel views.Extensive experiments showcase the compelling capabilities of AltNeRF ingenerating high-fidelity and robust novel views that closely resemble reality.</description><author>Kun Wang, Zhiqiang Yan, Huang Tian, Zhenyu Zhang, Xiang Li, Jun Li, Jian Yang</author><pubDate>Fri, 23 Feb 2024 12:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10001v2</guid></item><item><title>Loopy-SLAM: Dense Neural SLAM with Loop Closures</title><link>http://arxiv.org/abs/2402.09944v1</link><description>Neural RGBD SLAM techniques have shown promise in dense SimultaneousLocalization And Mapping (SLAM), yet face challenges such as error accumulationduring camera tracking resulting in distorted maps. In response, we introduceLoopy-SLAM that globally optimizes poses and the dense 3D model. We useframe-to-model tracking using a data-driven point-based submap generationmethod and trigger loop closures online by performing global place recognition.Robust pose graph optimization is used to rigidly align the local submaps. Asour representation is point based, map corrections can be performed efficientlywithout the need to store the entire history of input frames used for mappingas typically required by methods employing a grid based mapping structure.Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNetdatasets demonstrate competitive or superior performance in tracking, mapping,and rendering accuracy when compared to existing dense neural RGBD SLAMmethods. Project page: notchla.github.io/Loopy-SLAM.</description><author>Lorenzo Liso, Erik Sandström, Vladimir Yugay, Luc Van Gool, Martin R. Oswald</author><pubDate>Wed, 14 Feb 2024 18:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09944v1</guid></item><item><title>SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation</title><link>http://arxiv.org/abs/2311.15707v2</link><description>Zero-shot 6D object pose estimation involves the detection of novel objectswith their 6D poses in cluttered scenes, presenting significant challenges formodel generalizability. Fortunately, the recent Segment Anything Model (SAM)has showcased remarkable zero-shot transfer performance, which provides apromising solution to tackle this task. Motivated by this, we introduce SAM-6D,a novel framework designed to realize the task through two steps, includinginstance segmentation and pose estimation. Given the target objects, SAM-6Demploys two dedicated sub-networks, namely Instance Segmentation Model (ISM)and Pose Estimation Model (PEM), to perform these steps on cluttered RGB-Dimages. ISM takes SAM as an advanced starting point to generate all possibleobject proposals and selectively preserves valid ones through meticulouslycrafted object matching scores in terms of semantics, appearance and geometry.By treating pose estimation as a partial-to-partial point matching problem, PEMperforms a two-stage point matching process featuring a novel design ofbackground tokens to construct dense 3D-3D correspondence, ultimately yieldingthe pose estimates. Without bells and whistles, SAM-6D outperforms the existingmethods on the seven core datasets of the BOP Benchmark for both instancesegmentation and pose estimation of novel objects.</description><author>Jiehong Lin, Lihua Liu, Dekun Lu, Kui Jia</author><pubDate>Wed, 06 Mar 2024 12:23:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.15707v2</guid></item><item><title>Compact 3D Gaussian Splatting For Dense Visual SLAM</title><link>http://arxiv.org/abs/2403.11247v1</link><description>Recent work has shown that 3D Gaussian-based SLAM enables high-qualityreconstruction, accurate pose estimation, and real-time rendering of scenes.However, these approaches are built on a tremendous number of redundant 3DGaussian ellipsoids, leading to high memory and storage costs, and slowtraining speed. To address the limitation, we propose a compact 3D GaussianSplatting SLAM system that reduces the number and the parameter size ofGaussian ellipsoids. A sliding window-based masking strategy is first proposedto reduce the redundant ellipsoids. Then we observe that the covariance matrix(geometry) of most 3D Gaussian ellipsoids are extremely similar, whichmotivates a novel geometry codebook to compress 3D Gaussian geometricattributes, i.e., the parameters. Robust and accurate pose estimation isachieved by a global bundle adjustment method with reprojection loss. Extensiveexperiments demonstrate that our method achieves faster training and renderingspeed while maintaining the state-of-the-art (SOTA) quality of the scenerepresentation.</description><author>Tianchen Deng, Yaohui Chen, Leyan Zhang, Jianfei Yang, Shenghai Yuan, Danwei Wang, Weidong Chen</author><pubDate>Sun, 17 Mar 2024 16:41:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11247v1</guid></item><item><title>Ctrl123: Consistent Novel View Synthesis via Closed-Loop Transcription</title><link>http://arxiv.org/abs/2403.10953v1</link><description>Large image diffusion models have demonstrated zero-shot capability in novelview synthesis (NVS). However, existing diffusion-based NVS methods struggle togenerate novel views that are accurately consistent with the correspondingground truth poses and appearances, even on the training set. This consequentlylimits the performance of downstream tasks, such as image-to-multiviewgeneration and 3D reconstruction. We realize that such inconsistency is largelydue to the fact that it is difficult to enforce accurate pose and appearancealignment directly in the diffusion training, as mostly done by existingmethods such as Zero123. To remedy this problem, we propose Ctrl123, aclosed-loop transcription-based NVS diffusion method that enforces alignmentbetween the generated view and ground truth in a pose-sensitive feature space.Our extensive experiments demonstrate the effectiveness of Ctrl123 on the tasksof NVS and 3D reconstruction, achieving significant improvements in bothmultiview-consistency and pose-consistency over existing methods.</description><author>Hongxiang Zhao, Xili Dai, Jianan Wang, Shengbang Tong, Jingyuan Zhang, Weida Wang, Lei Zhang, Yi Ma</author><pubDate>Sat, 16 Mar 2024 16:39:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10953v1</guid></item><item><title>Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints Scene Coordinate Regression</title><link>http://arxiv.org/abs/2403.10297v2</link><description>Classical structural-based visual localization methods offer high accuracybut face trade-offs in terms of storage, speed, and privacy. A recentinnovation, keypoint scene coordinate regression (KSCR) named D2S addressesthese issues by leveraging graph attention networks to enhance keypointrelationships and predict their 3D coordinates using a simple multilayerperceptron (MLP). Camera pose is then determined via PnP+RANSAC, usingestablished 2D-3D correspondences. While KSCR achieves competitive results,rivaling state-of-the-art image-retrieval methods like HLoc across multiplebenchmarks, its performance is hindered when data samples are limited due tothe deep learning model's reliance on extensive data. This paper proposes asolution to this challenge by introducing a pipeline for keypoint descriptorsynthesis using Neural Radiance Field (NeRF). By generating novel poses andfeeding them into a trained NeRF model to create new views, our approachenhances the KSCR's generalization capabilities in data-scarce environments.The proposed system could significantly improve localization accuracy by up to50% and cost only a fraction of time for data synthesis. Furthermore, itsmodular design allows for the integration of multiple NeRFs, offering aversatile and efficient solution for visual localization. The implementation ispublicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.</description><author>Huy-Hoang Bui, Bach-Thuan Bui, Dinh-Tuan Tran, Joo-Ho Lee</author><pubDate>Wed, 20 Mar 2024 03:34:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10297v2</guid></item><item><title>Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints Scene Coordinate Regression</title><link>http://arxiv.org/abs/2403.10297v1</link><description>Classical structural-based visual localization methods offer high accuracybut face trade-offs in terms of storage, speed, and privacy. A recentinnovation, keypoint scene coordinate regression (KSCR) named D2S addressesthese issues by leveraging graph attention networks to enhance keypointrelationships and predict their 3D coordinates using a simple multilayerperceptron (MLP). Camera pose is then determined via PnP+RANSAC, usingestablished 2D-3D correspondences. While KSCR achieves competitive results,rivaling state-of-the-art image-retrieval methods like HLoc across multiplebenchmarks, its performance is hindered when data samples are limited due tothe deep learning model's reliance on extensive data. This paper proposes asolution to this challenge by introducing a pipeline for keypoint descriptorsynthesis using Neural Radiance Field (NeRF). By generating novel poses andfeeding them into a trained NeRF model to create new views, our approachenhances the KSCR's generalization capabilities in data-scarce environments.The proposed system could significantly improve localization accuracy by up to50\% and cost only a fraction of time for data synthesis. Furthermore, itsmodular design allows for the integration of multiple NeRFs, offering aversatile and efficient solution for visual localization. The implementation ispublicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.</description><author>Huy-Hoang Bui, Bach-Thuan Bui, Dinh-Tuan Tran, Joo-Ho Lee</author><pubDate>Fri, 15 Mar 2024 14:40:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10297v1</guid></item><item><title>HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting</title><link>http://arxiv.org/abs/2403.12722v1</link><description>Holistic understanding of urban scenes based on RGB images is a challengingyet important problem. It encompasses understanding both the geometry andappearance to enable novel view synthesis, parsing semantic labels, andtracking moving objects. Despite considerable progress, existing approachesoften focus on specific aspects of this task and require additional inputs suchas LiDAR scans or manually annotated 3D bounding boxes. In this paper, weintroduce a novel pipeline that utilizes 3D Gaussian Splatting for holisticurban scene understanding. Our main idea involves the joint optimization ofgeometry, appearance, semantics, and motion using a combination of static anddynamic 3D Gaussians, where moving object poses are regularized via physicalconstraints. Our approach offers the ability to render new viewpoints inreal-time, yielding 2D and 3D semantic information with high accuracy, andreconstruct dynamic scenes, even in scenarios where 3D bounding box detectionare highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2demonstrate the effectiveness of our approach.</description><author>Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, Yiyi Liao</author><pubDate>Tue, 19 Mar 2024 14:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12722v1</guid></item><item><title>MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction</title><link>http://arxiv.org/abs/2402.12712v1</link><description>This paper presents a neural architecture MVDiffusion++ for 3D objectreconstruction that synthesizes dense and high-resolution views of an objectgiven one or a few images without camera poses. MVDiffusion++ achieves superiorflexibility and scalability with two surprisingly simple ideas: 1) A``pose-free architecture'' where standard self-attention among 2D latentfeatures learns 3D consistency across an arbitrary number of conditional andgeneration views without explicitly using camera pose information; and 2) A``view dropout strategy'' that discards a substantial number of output viewsduring training, which reduces the training-time memory footprint and enablesdense and high-resolution view synthesis at test time. We use the Objaverse fortraining and the Google Scanned Objects for evaluation with standard novel viewsynthesis and 3D reconstruction metrics, where MVDiffusion++ significantlyoutperforms the current state of the arts. We also demonstrate a text-to-3Dapplication example by combining MVDiffusion++ with a text-to-image generativemodel.</description><author>Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, Rakesh Ranjan</author><pubDate>Tue, 20 Feb 2024 04:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12712v1</guid></item><item><title>HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions</title><link>http://arxiv.org/abs/2403.18575v1</link><description>Reconstructing 3D hand mesh robustly from a single image is very challenging,due to the lack of diversity in existing real-world datasets. While datasynthesis helps relieve the issue, the syn-to-real gap still hinders its usage.In this work, we present HandBooster, a new approach to uplift the datadiversity and boost the 3D hand-mesh reconstruction performance by training aconditional generative space on hand-object interactions and purposely samplingthe space to synthesize effective data samples. First, we construct versatilecontent-aware conditions to guide a diffusion model to produce realistic imageswith diverse hand appearances, poses, views, and backgrounds; favorably,accurate 3D annotations are obtained for free. Then, we design a novelcondition creator based on our similarity-aware distribution samplingstrategies to deliberately find novel and realistic interaction poses that aredistinctive from the training set. Equipped with our method, several baselinescan be significantly improved beyond the SOTA on the HO3D and DexYCBbenchmarks. Our code will be released onhttps://github.com/hxwork/HandBooster_Pytorch.</description><author>Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu</author><pubDate>Wed, 27 Mar 2024 14:56:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18575v1</guid></item><item><title>SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM</title><link>http://arxiv.org/abs/2403.07494v1</link><description>We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3DGaussian representation, that enables accurate 3D semantic mapping, robustcamera tracking, and high-quality rendering in real-time. In this system, weincorporate semantic feature embedding into 3D Gaussian representation, whicheffectively encodes semantic information within the spatial layout of theenvironment for precise semantic scene representation. Furthermore, we proposefeature-level loss for updating 3D Gaussian representation, enablinghigher-level guidance for 3D Gaussian optimization. In addition, to reducecumulative drift and improve reconstruction accuracy, we introducesemantic-informed bundle adjustment leveraging semantic associations for jointoptimization of 3D Gaussian representation and camera poses, leading to morerobust tracking and consistent mapping. Our SemGauss-SLAM method demonstratessuperior performance over existing dense semantic SLAM methods in terms ofmapping and tracking accuracy on Replica and ScanNet datasets, while alsoshowing excellent capabilities in novel-view semantic synthesis and 3D semanticmapping.</description><author>Siting Zhu, Renjie Qin, Guangming Wang, Jiuming Liu, Hesheng Wang</author><pubDate>Tue, 12 Mar 2024 11:33:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07494v1</guid></item><item><title>Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling</title><link>http://arxiv.org/abs/2311.16096v2</link><description>Modeling animatable human avatars from RGB videos is a long-standing andchallenging problem. Recent works usually adopt MLP-based neural radiancefields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs toregress pose-dependent garment details. To this end, we introduce AnimatableGaussians, a new avatar representation that leverages powerful 2D CNNs and 3DGaussian splatting to create high-fidelity avatars. To associate 3D Gaussianswith the animatable avatar, we learn a parametric template from the inputvideos, and then parameterize the template on two front \&amp; back canonicalGaussian maps where each pixel represents a 3D Gaussian. The learned templateis adaptive to the wearing garments for modeling looser clothes like dresses.Such template-guided 2D parameterization enables us to employ a powerfulStyleGAN-based CNN to learn the pose-dependent Gaussian maps for modelingdetailed dynamic appearances. Furthermore, we introduce a pose projectionstrategy for better generalization given novel poses. Overall, our method cancreate lifelike avatars with dynamic, realistic and generalized appearances.Experiments show that our method outperforms other state-of-the-art approaches.Code: https://github.com/lizhe00/AnimatableGaussians</description><author>Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu</author><pubDate>Fri, 15 Mar 2024 09:32:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16096v2</guid></item><item><title>Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving</title><link>http://arxiv.org/abs/2403.04112v1</link><description>This paper presents a novel multi-modal Multi-Object Tracking (MOT) algorithmfor self-driving cars that combines camera and LiDAR data. Camera frames areprocessed with a state-of-the-art 3D object detector, whereas classicalclustering techniques are used to process LiDAR observations. The proposed MOTalgorithm comprises a three-step association process, an Extended Kalman filterfor estimating the motion of each detected dynamic obstacle, and a trackmanagement phase. The EKF motion model requires the current measured relativeposition and orientation of the observed object and the longitudinal andangular velocities of the ego vehicle as inputs. Unlike most state-of-the-artmulti-modal MOT approaches, the proposed algorithm does not rely on maps orknowledge of the ego global pose. Moreover, it uses a 3D detector exclusivelyfor cameras and is agnostic to the type of LiDAR sensor used. The algorithm isvalidated both in simulation and with real-world data, with satisfactoryresults.</description><author>Riccardo Pieroni, Simone Specchia, Matteo Corno, Sergio Matteo Savaresi</author><pubDate>Wed, 06 Mar 2024 23:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04112v1</guid></item><item><title>Towards 3D Vision with Low-Cost Single-Photon Cameras</title><link>http://arxiv.org/abs/2403.17801v1</link><description>We present a method for reconstructing 3D shape of arbitrary Lambertianobjects based on measurements by miniature, energy-efficient, low-costsingle-photon cameras. These cameras, operating as time resolved image sensors,illuminate the scene with a very fast pulse of diffuse light and record theshape of that pulse as it returns back from the scene at a high temporalresolution. We propose to model this image formation process, account for itsnon-idealities, and adapt neural rendering to reconstruct 3D geometry from aset of spatially distributed sensors with known poses. We show that ourapproach can successfully recover complex 3D shapes from simulated data. Wefurther demonstrate 3D object reconstruction from real-world captures,utilizing measurements from a commodity proximity sensor. Our work draws aconnection between image-based modeling and active range scanning and is a steptowards 3D vision with single-photon cameras.</description><author>Fangzhou Mu, Carter Sifferman, Sacha Jungerman, Yiquan Li, Mark Han, Michael Gleicher, Mohit Gupta, Yin Li</author><pubDate>Tue, 26 Mar 2024 16:40:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17801v1</guid></item><item><title>Denoising Diffusion via Image-Based Rendering</title><link>http://arxiv.org/abs/2402.03445v2</link><description>Generating 3D scenes is a challenging open problem, which requiressynthesizing plausible content that is fully consistent in 3D space. Whilerecent methods such as neural radiance fields excel at view synthesis and 3Dreconstruction, they cannot synthesize plausible details in unobserved regionssince they lack a generative capability. Conversely, existing generativemethods are typically not capable of reconstructing detailed, large-scalescenes in the wild, as they use limited-capacity 3D scene representations,require aligned camera poses, or rely on additional regularizers. In this work,we introduce the first diffusion model able to perform fast, detailedreconstruction and generation of real-world 3D scenes. To achieve this, we makethree contributions. First, we introduce a new neural scene representation,IB-planes, that can efficiently and accurately represent large 3D scenes,dynamically allocating more capacity as needed to capture details visible ineach image. Second, we propose a denoising-diffusion framework to learn a priorover this novel 3D scene representation, using only 2D images without the needfor any additional supervision signal such as masks or depths. This supports 3Dreconstruction and generation in a unified architecture. Third, we develop aprincipled approach to avoid trivial 3D solutions when integrating theimage-based rendering with the diffusion model, by dropping out representationsof some images. We evaluate the model on several challenging datasets of realand synthetic images, and demonstrate superior results on generation, novelview synthesis and 3D reconstruction.</description><author>Titas Anciukevičius, Fabian Manhardt, Federico Tombari, Paul Henderson</author><pubDate>Tue, 20 Feb 2024 20:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03445v2</guid></item><item><title>Cameras as Rays: Pose Estimation via Ray Diffusion</title><link>http://arxiv.org/abs/2402.14817v1</link><description>Estimating camera poses is a fundamental task for 3D reconstruction andremains challenging given sparse views (&lt;10). In contrast to existingapproaches that pursue top-down prediction of global parametrizations of cameraextrinsics, we propose a distributed representation of camera pose that treatsa camera as a bundle of rays. This representation allows for a tight couplingwith spatial image features improving pose precision. We observe that thisrepresentation is naturally suited for set-level level transformers and developa regression-based approach that maps image patches to corresponding rays. Tocapture the inherent uncertainties in sparse-view pose inference, we adapt thisapproach to learn a denoising diffusion model which allows us to sampleplausible modes while improving performance. Our proposed methods, bothregression- and diffusion-based, demonstrate state-of-the-art performance oncamera pose estimation on CO3D while generalizing to unseen object categoriesand in-the-wild captures.</description><author>Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Thu, 22 Feb 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14817v1</guid></item><item><title>From Correspondences to Pose: Non-minimal Certifiably Optimal Relative Pose without Disambiguation</title><link>http://arxiv.org/abs/2312.05995v2</link><description>Estimating the relative camera pose from $n \geq 5$ correspondences betweentwo calibrated views is a fundamental task in computer vision. This processtypically involves two stages: 1) estimating the essential matrix between theviews, and 2) disambiguating among the four candidate relative poses thatsatisfy the epipolar geometry. In this paper, we demonstrate a novel approachthat, for the first time, bypasses the second stage. Specifically, we show thatit is possible to directly estimate the correct relative camera pose fromcorrespondences without needing a post-processing step to enforce thecheirality constraint on the correspondences. Building on recent advances incertifiable non-minimal optimization, we frame the relative pose estimation asa Quadratically Constrained Quadratic Program (QCQP). By applying theappropriate constraints, we ensure the estimation of a camera pose thatcorresponds to a valid 3D geometry and that is globally optimal when certified.We validate our method through exhaustive synthetic and real-world experiments,confirming the efficacy, efficiency and accuracy of the proposed approach. Codeis available at https://github.com/javrtg/C2P.</description><author>Javier Tirado-Garín, Javier Civera</author><pubDate>Wed, 27 Mar 2024 19:21:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05995v2</guid></item><item><title>Cameras as Rays: Pose Estimation via Ray Diffusion</title><link>http://arxiv.org/abs/2402.14817v2</link><description>Estimating camera poses is a fundamental task for 3D reconstruction andremains challenging given sparsely sampled views (&lt;10). In contrast to existingapproaches that pursue top-down prediction of global parametrizations of cameraextrinsics, we propose a distributed representation of camera pose that treatsa camera as a bundle of rays. This representation allows for a tight couplingwith spatial image features improving pose precision. We observe that thisrepresentation is naturally suited for set-level transformers and develop aregression-based approach that maps image patches to corresponding rays. Tocapture the inherent uncertainties in sparse-view pose inference, we adapt thisapproach to learn a denoising diffusion model which allows us to sampleplausible modes while improving performance. Our proposed methods, bothregression- and diffusion-based, demonstrate state-of-the-art performance oncamera pose estimation on CO3D while generalizing to unseen object categoriesand in-the-wild captures.</description><author>Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Tue, 19 Mar 2024 08:29:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14817v2</guid></item><item><title>GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</title><link>http://arxiv.org/abs/2312.02069v2</link><description>We introduce GaussianAvatars, a new method to create photorealistic headavatars that are fully controllable in terms of expression, pose, andviewpoint. The core idea is a dynamic 3D representation based on 3D Gaussiansplats that are rigged to a parametric morphable face model. This combinationfacilitates photorealistic rendering while allowing for precise animationcontrol via the underlying parametric model, e.g., through expression transferfrom a driving sequence or by manually changing the morphable model parameters.We parameterize each splat by a local coordinate frame of a triangle andoptimize for explicit displacement offset to obtain a more accurate geometricrepresentation. During avatar reconstruction, we jointly optimize for themorphable model parameters and Gaussian splat parameters in an end-to-endfashion. We demonstrate the animation capabilities of our photorealistic avatarin several challenging scenarios. For instance, we show reenactments from adriving video, where our method outperforms existing works by a significantmargin.</description><author>Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias Nießner</author><pubDate>Thu, 28 Mar 2024 16:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02069v2</guid></item><item><title>Learning to Produce Semi-dense Correspondences for Visual Localization</title><link>http://arxiv.org/abs/2402.08359v2</link><description>This study addresses the challenge of performing visual localization indemanding conditions such as night-time scenarios, adverse weather, andseasonal changes. While many prior studies have focused on improvingimage-matching performance to facilitate reliable dense keypoint matchingbetween images, existing methods often heavily rely on predefined featurepoints on a reconstructed 3D model. Consequently, they tend to overlookunobserved keypoints during the matching process. Therefore, dense keypointmatches are not fully exploited, leading to a notable reduction in accuracy,particularly in noisy scenes. To tackle this issue, we propose a novellocalization method that extracts reliable semi-dense 2D-3D matching pointsbased on dense keypoint matches. This approach involves regressing semi-dense2D keypoints into 3D scene coordinates using a point inference network. Thenetwork utilizes both geometric and visual cues to effectively infer 3Dcoordinates for unobserved keypoints from the observed ones. The abundance ofmatching information significantly enhances the accuracy of camera poseestimation, even in scenarios involving noisy or sparse 3D models.Comprehensive evaluations demonstrate that the proposed method outperformsother methods in challenging scenes and achieves competitive results inlarge-scale visual localization benchmarks. The code will be available.</description><author>Khang Truong Giang, Soohwan Song, Sungho Jo</author><pubDate>Wed, 20 Mar 2024 08:05:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08359v2</guid></item><item><title>LISNeRF Mapping: LiDAR-based Implicit Mapping via Semantic Neural Fields for Large-Scale 3D Scenes</title><link>http://arxiv.org/abs/2311.02313v2</link><description>Large-scale semantic mapping is crucial for outdoor autonomous agents tofulfill high-level tasks such as planning and navigation. This paper proposes anovel method for large-scale 3D semantic reconstruction through implicitrepresentations from posed LiDAR measurements alone. We first leverage anoctree-based and hierarchical structure to store implicit features, then theseimplicit features are decoded to semantic information and signed distance valuethrough shallow Multilayer Perceptrons (MLPs). We adopt off-the-shelfalgorithms to predict the semantic labels and instance IDs of point clouds. Wethen jointly optimize the feature embeddings and MLPs parameters with aself-supervision paradigm for point cloud geometry and a pseudo-supervisionparadigm for semantic and panoptic labels. Subsequently, categories andgeometric structures for novel points are regressed, and marching cubes areexploited to subdivide and visualize the scenes in the inferring stage. Forscenarios with memory constraints, a map stitching strategy is also developedto merge sub-maps into a complete map. Experiments on two real-world datasets,SemanticKITTI and SemanticPOSS, demonstrate the superior segmentationefficiency and mapping effectiveness of our framework compared to currentstate-of-the-art 3D LiDAR mapping methods.</description><author>Jianyuan Zhang, Zhiliu Yang, Meng Zhang</author><pubDate>Wed, 20 Mar 2024 05:45:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02313v2</guid></item><item><title>GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects</title><link>http://arxiv.org/abs/2403.11510v1</link><description>Despite the progress of learning-based methods for 6D object pose estimation,the trade-off between accuracy and scalability for novel objects still exists.Specifically, previous methods for novel objects do not make good use of thetarget object's 3D shape information since they focus on generalization byprocessing the shape indirectly, making them less effective. We presentGenFlow, an approach that enables both accuracy and generalization to novelobjects with the guidance of the target object's shape. Our method predictsoptical flow between the rendered image and the observed image and refines the6D pose iteratively. It boosts the performance by a constraint of the 3D shapeand the generalizable geometric knowledge learned from an end-to-enddifferentiable system. We further improve our model by designing a cascadenetwork architecture to exploit the multi-scale correlations and coarse-to-finerefinement. GenFlow ranked first on the unseen object pose estimationbenchmarks in both the RGB and RGB-D cases. It also achieves performancecompetitive with existing state-of-the-art methods for the seen object poseestimation without any fine-tuning.</description><author>Sungphill Moon, Hyeontae Son, Dongcheol Hur, Sangwook Kim</author><pubDate>Mon, 18 Mar 2024 07:32:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11510v1</guid></item><item><title>DrFER: Learning Disentangled Representations for 3D Facial Expression Recognition</title><link>http://arxiv.org/abs/2403.08318v1</link><description>Facial Expression Recognition (FER) has consistently been a focal point inthe field of facial analysis. In the context of existing methodologies for 3DFER or 2D+3D FER, the extraction of expression features often gets entangledwith identity information, compromising the distinctiveness of these features.To tackle this challenge, we introduce the innovative DrFER method, whichbrings the concept of disentangled representation learning to the field of 3DFER. DrFER employs a dual-branch framework to effectively disentangleexpression information from identity information. Diverging from priordisentanglement endeavors in the 3D facial domain, we have carefullyreconfigured both the loss functions and network structure to make the overallframework adaptable to point cloud data. This adaptation enhances thecapability of the framework in recognizing facial expressions, even in casesinvolving varying head poses. Extensive evaluations conducted on the BU-3DFEand Bosphorus datasets substantiate that DrFER surpasses the performance ofother 3D FER methods.</description><author>Hebeizi Li, Hongyu Yang, Di Huang</author><pubDate>Wed, 13 Mar 2024 09:00:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08318v1</guid></item><item><title>Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks</title><link>http://arxiv.org/abs/2402.14400v1</link><description>Reliable methods for the neurodevelopmental assessment of infants areessential for early detection of medical issues that may need promptinterventions. Spontaneous motor activity, or `kinetics', is shown to provide apowerful surrogate measure of upcoming neurodevelopment. However, itsassessment is by and large qualitative and subjective, focusing on visuallyidentified, age-specific gestures. Here, we follow an alternative approach,predicting infants' neurodevelopmental maturation based on data-drivenevaluation of individual motor patterns. We utilize 3D video recordings ofinfants processed with pose-estimation to extract spatio-temporal series ofanatomical landmarks, and apply adaptive graph convolutional networks topredict the actual age. We show that our data-driven approach achievesimprovement over traditional machine learning baselines based on manuallyengineered features.</description><author>Daniel Holmberg, Manu Airaksinen, Viviana Marchi, Andrea Guzzetta, Anna Kivi, Leena Haataja, Sampsa Vanhatalo, Teemu Roos</author><pubDate>Thu, 22 Feb 2024 09:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14400v1</guid></item><item><title>RadarDistill: Boosting Radar-based Object Detection Performance via Knowledge Distillation from LiDAR Features</title><link>http://arxiv.org/abs/2403.05061v1</link><description>The inherent noisy and sparse characteristics of radar data pose challengesin finding effective representations for 3D object detection. In this paper, wepropose RadarDistill, a novel knowledge distillation (KD) method, which canimprove the representation of radar data by leveraging LiDAR data. RadarDistillsuccessfully transfers desirable characteristics of LiDAR features into radarfeatures using three key components: Cross-Modality Alignment (CMA),Activation-based Feature Distillation (AFD), and Proposal-based FeatureDistillation (PFD). CMA enhances the density of radar features through multiplelayers of dilation operations, effectively addressing the challenges ofinefficient knowledge transfer from LiDAR to radar. AFD is designed to transferknowledge from significant areas of the LiDAR features, specifically thoseregions where activation intensity exceeds a predetermined threshold. PFDguides the radar network to mimic LiDAR network features in the objectproposals for accurately detected results while moderating features formisdetected proposals like false positives. Our comparative analyses conductedon the nuScenes datasets demonstrate that RadarDistill achievesstate-of-the-art (SOTA) performance for radar-only object detection task,recording 20.5% in mAP and 43.7% in NDS. Also, RadarDistill significantlyimproves the performance of the camera-radar fusion model.</description><author>Geonho Bang, Kwangjin Choi, Jisong Kim, Dongsuk Kum, Jun Won Choi</author><pubDate>Fri, 08 Mar 2024 05:15:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05061v1</guid></item><item><title>3D Semantic MapNet: Building Maps for Multi-Object Re-Identification in 3D</title><link>http://arxiv.org/abs/2403.13190v1</link><description>We study the task of 3D multi-object re-identification from embodied tours.Specifically, an agent is given two tours of an environment (e.g. an apartment)under two different layouts (e.g. arrangements of furniture). Its task is todetect and re-identify objects in 3D - e.g. a "sofa" moved from location A toB, a new "chair" in the second layout at location C, or a "lamp" from locationD in the first layout missing in the second. To support this task, we create anautomated infrastructure to generate paired egocentric tours ofinitial/modified layouts in the Habitat simulator using Matterport3D scenes,YCB and Google-scanned objects. We present 3D Semantic MapNet (3D-SMNet) - atwo-stage re-identification model consisting of (1) a 3D object detector thatoperates on RGB-D videos with known pose, and (2) a differentiable objectmatching module that solves correspondence estimation between two sets of 3Dbounding boxes. Overall, 3D-SMNet builds object-based maps of each layout andthen uses a differentiable matcher to re-identify objects across the tours.After training 3D-SMNet on our generated episodes, we demonstrate zero-shottransfer to real-world rearrangement scenarios by instantiating our task inReplica, Active Vision, and RIO environments depicting rearrangements. On alldatasets, we find 3D-SMNet outperforms competitive baselines. Further, we showjointly training on real and generated episodes can lead to significantimprovements over training on real data alone.</description><author>Vincent Cartillier, Neha Jain, Irfan Essa</author><pubDate>Wed, 20 Mar 2024 00:01:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13190v1</guid></item><item><title>High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided Densification and Regularized Optimization</title><link>http://arxiv.org/abs/2403.12535v1</link><description>We propose a dense RGBD SLAM system based on 3D Gaussian Splatting thatprovides metrically accurate pose tracking and visually realisticreconstruction. To this end, we first propose a Gaussian densification strategybased on the rendering loss to map unobserved areas and refine reobservedareas. Second, we introduce extra regularization parameters to alleviate theforgetting problem in the continuous mapping problem, where parameters tend tooverfit the latest frame and result in decreasing rendering quality forprevious frames. Both mapping and tracking are performed with Gaussianparameters by minimizing re-rendering loss in a differentiable way. Compared torecent neural and concurrently developed gaussian splatting RGBD SLAMbaselines, our method achieves state-of-the-art results on the syntheticdataset Replica and competitive results on the real-world dataset TUM.</description><author>Shuo Sun, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson</author><pubDate>Tue, 19 Mar 2024 09:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12535v1</guid></item><item><title>Towards Transferable Targeted 3D Adversarial Attack in the Physical World</title><link>http://arxiv.org/abs/2312.09558v2</link><description>Compared with transferable untargeted attacks, transferable targetedadversarial attacks could specify the misclassification categories ofadversarial samples, posing a greater threat to security-critical tasks. In themeanwhile, 3D adversarial samples, due to their potential of multi-viewrobustness, can more comprehensively identify weaknesses in existing deeplearning systems, possessing great application value. However, the field oftransferable targeted 3D adversarial attacks remains vacant. The goal of thiswork is to develop a more effective technique that could generate transferabletargeted 3D adversarial examples, filling the gap in this field. To achievethis goal, we design a novel framework named TT3D that could rapidlyreconstruct from few multi-view images into Transferable Targeted 3D texturedmeshes. While existing mesh-based texture optimization methods computegradients in the high-dimensional mesh space and easily fall into local optima,leading to unsatisfactory transferability and distinct distortions, TT3Dinnovatively performs dual optimization towards both feature grid andMulti-layer Perceptron (MLP) parameters in the grid-based NeRF space, whichsignificantly enhances black-box transferability while enjoying naturalness.Experimental results show that TT3D not only exhibits superior cross-modeltransferability but also maintains considerable adaptability across differentrenders and vision tasks. More importantly, we produce 3D adversarial exampleswith 3D printing techniques in the real world and verify their robustperformance under various scenarios.</description><author>Yao Huang, Yinpeng Dong, Shouwei Ruan, Xiao Yang, Hang Su, Xingxing Wei</author><pubDate>Wed, 28 Feb 2024 12:28:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09558v2</guid></item><item><title>DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses</title><link>http://arxiv.org/abs/2403.13683v1</link><description>Determining the relative pose of an object between two images is pivotal tothe success of generalizable object pose estimation. Existing approachestypically approximate the continuous pose representation with a large number ofdiscrete pose hypotheses, which incurs a computationally expensive process ofscoring each hypothesis at test time. By contrast, we present a Deep VoxelMatching Network (DVMNet) that eliminates the need for pose hypotheses andcomputes the relative object pose in a single pass. To this end, we map the twoinput RGB images, reference and query, to their respective voxelized 3Drepresentations. We then pass the resulting voxels through a pose estimationmodule, where the voxels are aligned and the pose is computed in an end-to-endfashion by solving a least-squares problem. To enhance robustness, we introducea weighted closest voxel algorithm capable of mitigating the impact of noisyvoxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaversedatasets, demonstrating that our method delivers more accurate relative poseestimates for novel objects at a lower computational cost compared tostate-of-the-art methods. Our code is released at:https://github.com/sailor-z/DVMNet/.</description><author>Chen Zhao, Tong Zhang, Zheng Dang, Mathieu Salzmann</author><pubDate>Wed, 20 Mar 2024 16:41:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13683v1</guid></item><item><title>AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation</title><link>http://arxiv.org/abs/2403.17934v1</link><description>Expressive human pose and shape estimation (a.k.a. 3D whole-body meshrecovery) involves the human body, hand, and expression estimation. Mostexisting methods have tackled this task in a two-stage manner, first detectingthe human body part with an off-the-shelf detection model and inferring thedifferent human body parts individually. Despite the impressive resultsachieved, these methods suffer from 1) loss of valuable contextual informationvia cropping, 2) introducing distractions, and 3) lacking inter-associationamong different persons and body parts, inevitably causing performancedegradation, especially for crowded scenes. To address these issues, weintroduce a novel all-in-one-stage framework, AiOS, for multiple expressivehuman pose and shape recovery without an additional human detection step.Specifically, our method is built upon DETR, which treats multi-personwhole-body mesh recovery task as a progressive set prediction problem withvarious sequential detection. We devise the decoder tokens and extend them toour task. Specifically, we first employ a human token to probe a human locationin the image and encode global features for each instance, which provides acoarse location for the later transformer block. Then, we introduce ajoint-related token to probe the human joint in the image and encoder afine-grained local feature, which collaborates with the global feature toregress the whole-body mesh. This straightforward but effective modeloutperforms previous state-of-the-art methods by a 9% reduction in NMVE onAGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a3% reduction in PVE on EgoBody.</description><author>Qingping Sun, Yanjun Wang, Ailing Zeng, Wanqi Yin, Chen Wei, Wenjia Wang, Haiyi Mei, Chi Sing Leung, Ziwei Liu, Lei Yang, Zhongang Cai</author><pubDate>Tue, 26 Mar 2024 18:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17934v1</guid></item><item><title>Neural Refinement for Absolute Pose Regression with Feature Synthesis</title><link>http://arxiv.org/abs/2303.10087v2</link><description>Absolute Pose Regression (APR) methods use deep neural networks to directlyregress camera poses from RGB images. However, the predominant APRarchitectures only rely on 2D operations during inference, resulting in limitedaccuracy of pose estimation due to the lack of 3D geometry constraints orpriors. In this work, we propose a test-time refinement pipeline that leveragesimplicit geometric constraints using a robust feature field to enhance theability of APR methods to use 3D information during inference. We alsointroduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3Dgeometric features during training and directly renders dense novel viewfeatures at test time to refine APR methods. To enhance the robustness of ourmodel, we introduce a feature fusion module and a progressive trainingstrategy. Our proposed method achieves state-of-the-art single-image APRaccuracy on indoor and outdoor datasets.</description><author>Shuai Chen, Yash Bhalgat, Xinghui Li, Jiawang Bian, Kejie Li, Zirui Wang, Victor Adrian Prisacariu</author><pubDate>Fri, 01 Mar 2024 01:40:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.10087v2</guid></item><item><title>Representing 3D sparse map points and lines for camera relocalization</title><link>http://arxiv.org/abs/2402.18011v1</link><description>Recent advancements in visual localization and mapping have demonstratedconsiderable success in integrating point and line features. However, expandingthe localization framework to include additional mapping components frequentlyresults in increased demand for memory and computational resources dedicated tomatching tasks. In this study, we show how a lightweight neural network canlearn to represent both 3D point and line features, and exhibit leading poseaccuracy by harnessing the power of multiple learned mappings. Specifically, weutilize a single transformer block to encode line features, effectivelytransforming them into distinctive point-like descriptors. Subsequently, wetreat these point and line descriptor sets as distinct yet interconnectedfeature sets. Through the integration of self- and cross-attention withinseveral graph layers, our method effectively refines each feature beforeregressing 3D maps using two simple MLPs. In comprehensive experiments, ourindoor localization findings surpass those of Hloc and Limap across bothpoint-based and line-assisted configurations. Moreover, in outdoor scenarios,our method secures a significant lead, marking the most considerableenhancement over state-of-the-art learning-based methodologies. The source codeand demo videos of this work are publicly available at:https://thpjp.github.io/pl2map/</description><author>Bach-Thuan Bui, Huy-Hoang Bui, Dinh-Tuan Tran, Joo-Ho Lee</author><pubDate>Wed, 28 Feb 2024 03:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18011v1</guid></item><item><title>CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge</title><link>http://arxiv.org/abs/2402.15726v1</link><description>Most of existing category-level object pose estimation methods devote tolearning the object category information from point cloud modality. However,the scale of 3D datasets is limited due to the high cost of 3D data collectionand annotation. Consequently, the category features extracted from theselimited point cloud samples may not be comprehensive. This motivates us toinvestigate whether we can draw on knowledge of other modalities to obtaincategory information. Inspired by this motivation, we propose CLIPose, a novel6D pose framework that employs the pre-trained vision-language model to developbetter learning of object category information, which can fully leverageabundant semantic knowledge in image and text modalities. To make the 3Dencoder learn category-specific features more efficiently, we alignrepresentations of three modalities in feature space via multi-modalcontrastive learning. In addition to exploiting the pre-trained knowledge ofthe CLIP's model, we also expect it to be more sensitive with pose parameters.Therefore, we introduce a prompt tuning approach to fine-tune image encoderwhile we incorporate rotations and translations information in the textdescriptions. CLIPose achieves state-of-the-art performance on two mainstreambenchmark datasets, REAL275 and CAMERA25, and runs in real-time duringinference (40FPS).</description><author>Xiao Lin, Minghao Zhu, Ronghao Dang, Guangliang Zhou, Shaolong Shu, Feng Lin, Chengju Liu, Qijun Chen</author><pubDate>Sat, 24 Feb 2024 05:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15726v1</guid></item><item><title>A PnP Algorithm for Two-Dimensional Pose Estimation</title><link>http://arxiv.org/abs/2312.08488v3</link><description>We propose a PnP algorithm for a camera constrained to two-dimensional motion(applicable, for instance, to many wheeled robotics platforms). Leveraging thisassumption allows accuracy and performance improvements over 3D PnP algorithmsdue to the reduction in search space dimensionality. It also reduces theincidence of ambiguous pose estimates (as, in most cases, the spurioussolutions fall outside the plane of movement). Our algorithm finds anapproximate solution by solving a polynomial system and refines its predictioniteratively to minimize the reprojection error. The algorithm comparesfavorably to existing 3D PnP algorithms in terms of accuracy, performance, androbustness to noise.</description><author>Joshua Wang</author><pubDate>Fri, 08 Mar 2024 05:19:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08488v3</guid></item><item><title>KVN: Keypoints Voting Network with Differentiable RANSAC for Stereo Pose Estimation</title><link>http://arxiv.org/abs/2307.11543v2</link><description>Object pose estimation is a fundamental computer vision task exploited inseveral robotics and augmented reality applications. Many establishedapproaches rely on predicting 2D-3D keypoint correspondences using RANSAC(Random sample consensus) and estimating the object pose using the PnP(Perspective-n-Point) algorithm. Being RANSAC non-differentiable,correspondences cannot be directly learned in an end-to-end fashion. In thispaper, we address the stereo image-based object pose estimation problem by i)introducing a differentiable RANSAC layer into a well-known monocular poseestimation network; ii) exploiting an uncertainty-driven multi-view PnP solverwhich can fuse information from multiple views. We evaluate our approach on achallenging public stereo object pose estimation dataset and a custom-builtdataset we call Transparent Tableware Dataset (TTD), yielding state-of-the-artresults against other recent approaches. Furthermore, in our ablation study, weshow that the differentiable RANSAC layer plays a significant role in theaccuracy of the proposed method. We release with this paper the code of ourmethod and the TTD dataset.</description><author>Ivano Donadi, Alberto Pretto</author><pubDate>Wed, 28 Feb 2024 15:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11543v2</guid></item><item><title>Generative Enhancement for 3D Medical Images</title><link>http://arxiv.org/abs/2403.12852v1</link><description>The limited availability of 3D medical image datasets, due to privacyconcerns and high collection or annotation costs, poses significant challengesin the field of medical imaging. While a promising alternative is the use ofsynthesized medical data, there are few solutions for realistic 3D medicalimage synthesis due to difficulties in backbone design and fewer 3D trainingsamples compared to 2D counterparts. In this paper, we propose GEM-3D, a novelgenerative approach to the synthesis of 3D medical images and the enhancementof existing datasets using conditional diffusion models. Our method begins witha 2D slice, noted as the informed slice to serve the patient prior, andpropagates the generation process using a 3D segmentation mask. By decomposingthe 3D medical images into masks and patient prior information, GEM-3D offers aflexible yet effective solution for generating versatile 3D images fromexisting datasets. GEM-3D can enable dataset enhancement by combining informedslice selection and generation at random positions, along with editable maskvolumes to introduce large variations in diffusion sampling. Moreover, as theinformed slice contains patient-wise information, GEM-3D can also facilitatecounterfactual image synthesis and dataset-level de-enhancement with desiredcontrol. Experiments on brain MRI and abdomen CT images demonstrate that GEM-3Dis capable of synthesizing high-quality 3D medical images with volumetricconsistency, offering a straightforward solution for dataset enhancement duringinference. The code is available at https://github.com/HKU-MedAI/GEM-3D.</description><author>Lingting Zhu, Noel Codella, Dongdong Chen, Zhenchao Jin, Lu Yuan, Lequan Yu</author><pubDate>Tue, 19 Mar 2024 16:57:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12852v1</guid></item></channel></rss>