<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxiv3D Pose</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 27 Feb 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Occlusion Resilient 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2402.11036v1</link><description>Occlusions remain one of the key challenges in 3D body pose estimation fromsingle-camera video sequences. Temporal consistency has been extensively usedto mitigate their impact but the existing algorithms in the literature do notexplicitly model them. Here, we apply this by representing the deforming body as a spatio-temporalgraph. We then introduce a refinement network that performs graph convolutionsover this graph to output 3D poses. To ensure robustness to occlusions, wetrain this network with a set of binary masks that we use to disable some ofthe edges as in drop-out techniques. In effect, we simulate the fact that some joints can be hidden for periods oftime and train the network to be immune to that. We demonstrate theeffectiveness of this approach compared to state-of-the-art techniques thatinfer poses from single-camera sequences.</description><author>Soumava Kumar Roy, Ilia Badanin, Sina Honari, Pascal Fua</author><pubDate>Fri, 16 Feb 2024 19:29:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11036v1</guid></item><item><title>Dense Matchers for Dense Tracking</title><link>http://arxiv.org/abs/2402.11287v1</link><description>Optical flow is a useful input for various applications, including 3Dreconstruction, pose estimation, tracking, and structure-from-motion. Despiteits utility, the field of dense long-term tracking, especially over widebaselines, has not been extensively explored. This paper extends the concept ofcombining multiple optical flows over logarithmically spaced intervals asproposed by MFT. We demonstrate the compatibility of MFT with different opticalflow networks, yielding results that surpass their individual performance.Moreover, we present a simple yet effective combination of these networkswithin the MFT framework. This approach proves to be competitive with moresophisticated, non-causal methods in terms of position prediction accuracy,highlighting the potential of MFT in enhancing long-term tracking applications.</description><author>Tomáš Jelínek, Jonáš Šerých, Jiří Matas</author><pubDate>Sat, 17 Feb 2024 14:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11287v1</guid></item><item><title>Lester: rotoscope animation through video object segmentation and tracking</title><link>http://arxiv.org/abs/2402.09883v1</link><description>This article introduces Lester, a novel method to automatically synthetiseretro-style 2D animations from videos. The method approaches the challengemainly as an object segmentation and tracking problem. Video frames areprocessed with the Segment Anything Model (SAM) and the resulting masks aretracked through subsequent frames with DeAOT, a method of hierarchicalpropagation for semi-supervised video object segmentation. The geometry of themasks' contours is simplified with the Douglas-Peucker algorithm. Finally,facial traits, pixelation and a basic shadow effect can be optionally added.The results show that the method exhibits an excellent temporal consistency andcan correctly process videos with different poses and appearances, dynamicshots, partial shots and diverse backgrounds. The proposed method provides amore simple and deterministic approach than diffusion models basedvideo-to-video translation pipelines, which suffer from temporal consistencyproblems and do not cope well with pixelated and schematic outputs. The methodis also much most practical than techniques based on 3D human pose estimation,which require custom handcrafted 3D models and are very limited with respect tothe type of scenes they can process.</description><author>Ruben Tous</author><pubDate>Thu, 15 Feb 2024 11:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09883v1</guid></item><item><title>MogaNet: Multi-order Gated Aggregation Network</title><link>http://arxiv.org/abs/2211.03295v3</link><description>By contextualizing the kernel as global as possible, Modern ConvNets haveshown great potential in computer vision tasks. However, recent progress on\textit{multi-order game-theoretic interaction} within deep neural networks(DNNs) reveals the representation bottleneck of modern ConvNets, where theexpressive interactions have not been effectively encoded with the increasedkernel size. To tackle this challenge, we propose a new family of modernConvNets, dubbed MogaNet, for discriminative visual representation learning inpure ConvNet-based models with favorable complexity-performance trade-offs.MogaNet encapsulates conceptually simple yet effective convolutions and gatedaggregation into a compact module, where discriminative features areefficiently gathered and contextualized adaptively. MogaNet exhibits greatscalability, impressive efficiency of parameters, and competitive performancecompared to state-of-the-art ViTs and ConvNets on ImageNet and variousdownstream vision benchmarks, including COCO object detection, ADE20K semanticsegmentation, 2D\&amp;3D human pose estimation, and video prediction. Notably,MogaNet hits 80.0\% and 87.8\% accuracy with 5.2M and 181M parameters onImageNet-1K, outperforming ParC-Net and ConvNeXt-L, while saving 59\% FLOPs and17M parameters, respectively. The source code is available at\url{https://github.com/Westlake-AI/MogaNet}.</description><author>Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Li</author><pubDate>Fri, 16 Feb 2024 14:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.03295v3</guid></item><item><title>MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction</title><link>http://arxiv.org/abs/2402.03762v3</link><description>Neural implicit representations have recently been demonstrated in manyfields including Simultaneous Localization And Mapping (SLAM). Current neuralSLAM can achieve ideal results in reconstructing bounded scenes, but thisrelies on the input of RGB-D images. Neural-based SLAM based only on RGB imagesis unable to reconstruct the scale of the scene accurately, and it also suffersfrom scale drift due to errors accumulated during tracking. To overcome theselimitations, we present MoD-SLAM, a monocular dense mapping method that allowsglobal pose optimization and 3D reconstruction in real-time in unboundedscenes. Optimizing scene reconstruction by monocular depth estimation and usingloop closure detection to update camera pose enable detailed and precisereconstruction on large scenes. Compared to previous work, our approach is morerobust, scalable and versatile. Our experiments demonstrate that MoD-SLAM hasmore excellent mapping performance than prior neural SLAM methods, especiallyin large borderless scenes.</description><author>Heng Zhou, Zhetao Guo, Shuhong Liu, Lechen Zhang, Qihao Wang, Yuxiang Ren, Mingrui Li</author><pubDate>Wed, 21 Feb 2024 17:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03762v3</guid></item><item><title>Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields</title><link>http://arxiv.org/abs/2402.13252v1</link><description>In this paper, we propose an algorithm that allows joint refinement of camerapose and scene geometry represented by decomposed low-rank tensor, using only2D images as supervision. First, we conduct a pilot study based on a 1D signaland relate our findings to 3D scenarios, where the naive joint poseoptimization on voxel-based NeRFs can easily lead to sub-optimal solutions.Moreover, based on the analysis of the frequency spectrum, we propose to applyconvolutional Gaussian filters on 2D and 3D radiance fields for acoarse-to-fine training schedule that enables joint camera pose optimization.Leveraging the decomposition property in decomposed low-rank tensor, our methodachieves an equivalent effect to brute-force 3D convolution with only incurringlittle computational overhead. To further improve the robustness and stabilityof joint optimization, we also propose techniques of smoothed 2D supervision,randomly scaled kernel parameters, and edge-guided loss mask. Extensivequantitative and qualitative evaluations demonstrate that our proposedframework achieves superior performance in novel view synthesis as well asrapid convergence for optimization.</description><author>Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu</author><pubDate>Tue, 20 Feb 2024 18:59:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13252v1</guid></item><item><title>ROAM: Robust and Object-Aware Motion Generation Using Neural Pose Descriptors</title><link>http://arxiv.org/abs/2308.12969v2</link><description>Existing automatic approaches for 3D virtual character motion synthesissupporting scene interactions do not generalise well to new objects outsidetraining distributions, even when trained on extensive motion capture datasetswith diverse objects and annotated interactions. This paper addresses thislimitation and shows that robustness and generalisation to novel scene objectsin 3D object-aware character synthesis can be achieved by training a motionmodel with as few as one reference object. We leverage an implicit featurerepresentation trained on object-only datasets, which encodes anSE(3)-equivariant descriptor field around the object. Given an unseen objectand a reference pose-object pair, we optimise for the object-aware pose that isclosest in the feature space to the reference pose. Finally, we use l-NSM,i.e., our motion generation model that is trained to seamlessly transition fromlocomotion to object interaction with the proposed bidirectional pose blendingscheme. Through comprehensive numerical comparisons to state-of-the-art methodsand in a user study, we demonstrate substantial improvements in 3D virtualcharacter motion and interaction quality and robustness to scenarios withunseen objects. Our project page is available athttps://vcai.mpi-inf.mpg.de/projects/ROAM/.</description><author>Wanyue Zhang, Rishabh Dabral, Thomas Leimkühler, Vladislav Golyanik, Marc Habermann, Christian Theobalt</author><pubDate>Thu, 15 Feb 2024 08:50:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.12969v2</guid></item><item><title>AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization</title><link>http://arxiv.org/abs/2308.10001v2</link><description>Neural Radiance Fields (NeRF) have shown promise in generating realisticnovel views from sparse scene images. However, existing NeRF approaches oftenencounter challenges due to the lack of explicit 3D supervision and imprecisecamera poses, resulting in suboptimal outcomes. To tackle these issues, wepropose AltNeRF -- a novel framework designed to create resilient NeRFrepresentations using self-supervised monocular depth estimation (SMDE) frommonocular videos, without relying on known camera poses. SMDE in AltNeRFmasterfully learns depth and pose priors to regulate NeRF training. The depthprior enriches NeRF's capacity for precise scene geometry depiction, while thepose prior provides a robust starting point for subsequent pose refinement.Moreover, we introduce an alternating algorithm that harmoniously melds NeRFoutputs into SMDE through a consistence-driven mechanism, thus enhancing theintegrity of depth priors. This alternation empowers AltNeRF to progressivelyrefine NeRF representations, yielding the synthesis of realistic novel views.Extensive experiments showcase the compelling capabilities of AltNeRF ingenerating high-fidelity and robust novel views that closely resemble reality.</description><author>Kun Wang, Zhiqiang Yan, Huang Tian, Zhenyu Zhang, Xiang Li, Jun Li, Jian Yang</author><pubDate>Fri, 23 Feb 2024 12:45:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10001v2</guid></item><item><title>Loopy-SLAM: Dense Neural SLAM with Loop Closures</title><link>http://arxiv.org/abs/2402.09944v1</link><description>Neural RGBD SLAM techniques have shown promise in dense SimultaneousLocalization And Mapping (SLAM), yet face challenges such as error accumulationduring camera tracking resulting in distorted maps. In response, we introduceLoopy-SLAM that globally optimizes poses and the dense 3D model. We useframe-to-model tracking using a data-driven point-based submap generationmethod and trigger loop closures online by performing global place recognition.Robust pose graph optimization is used to rigidly align the local submaps. Asour representation is point based, map corrections can be performed efficientlywithout the need to store the entire history of input frames used for mappingas typically required by methods employing a grid based mapping structure.Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNetdatasets demonstrate competitive or superior performance in tracking, mapping,and rendering accuracy when compared to existing dense neural RGBD SLAMmethods. Project page: notchla.github.io/Loopy-SLAM.</description><author>Lorenzo Liso, Erik Sandström, Vladimir Yugay, Luc Van Gool, Martin R. Oswald</author><pubDate>Wed, 14 Feb 2024 18:18:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09944v1</guid></item><item><title>MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction</title><link>http://arxiv.org/abs/2402.12712v1</link><description>This paper presents a neural architecture MVDiffusion++ for 3D objectreconstruction that synthesizes dense and high-resolution views of an objectgiven one or a few images without camera poses. MVDiffusion++ achieves superiorflexibility and scalability with two surprisingly simple ideas: 1) A``pose-free architecture'' where standard self-attention among 2D latentfeatures learns 3D consistency across an arbitrary number of conditional andgeneration views without explicitly using camera pose information; and 2) A``view dropout strategy'' that discards a substantial number of output viewsduring training, which reduces the training-time memory footprint and enablesdense and high-resolution view synthesis at test time. We use the Objaverse fortraining and the Google Scanned Objects for evaluation with standard novel viewsynthesis and 3D reconstruction metrics, where MVDiffusion++ significantlyoutperforms the current state of the arts. We also demonstrate a text-to-3Dapplication example by combining MVDiffusion++ with a text-to-image generativemodel.</description><author>Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, Rakesh Ranjan</author><pubDate>Tue, 20 Feb 2024 04:25:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12712v1</guid></item><item><title>Denoising Diffusion via Image-Based Rendering</title><link>http://arxiv.org/abs/2402.03445v2</link><description>Generating 3D scenes is a challenging open problem, which requiressynthesizing plausible content that is fully consistent in 3D space. Whilerecent methods such as neural radiance fields excel at view synthesis and 3Dreconstruction, they cannot synthesize plausible details in unobserved regionssince they lack a generative capability. Conversely, existing generativemethods are typically not capable of reconstructing detailed, large-scalescenes in the wild, as they use limited-capacity 3D scene representations,require aligned camera poses, or rely on additional regularizers. In this work,we introduce the first diffusion model able to perform fast, detailedreconstruction and generation of real-world 3D scenes. To achieve this, we makethree contributions. First, we introduce a new neural scene representation,IB-planes, that can efficiently and accurately represent large 3D scenes,dynamically allocating more capacity as needed to capture details visible ineach image. Second, we propose a denoising-diffusion framework to learn a priorover this novel 3D scene representation, using only 2D images without the needfor any additional supervision signal such as masks or depths. This supports 3Dreconstruction and generation in a unified architecture. Third, we develop aprincipled approach to avoid trivial 3D solutions when integrating theimage-based rendering with the diffusion model, by dropping out representationsof some images. We evaluate the model on several challenging datasets of realand synthetic images, and demonstrate superior results on generation, novelview synthesis and 3D reconstruction.</description><author>Titas Anciukevičius, Fabian Manhardt, Federico Tombari, Paul Henderson</author><pubDate>Tue, 20 Feb 2024 20:59:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03445v2</guid></item><item><title>Cameras as Rays: Pose Estimation via Ray Diffusion</title><link>http://arxiv.org/abs/2402.14817v1</link><description>Estimating camera poses is a fundamental task for 3D reconstruction andremains challenging given sparse views (&lt;10). In contrast to existingapproaches that pursue top-down prediction of global parametrizations of cameraextrinsics, we propose a distributed representation of camera pose that treatsa camera as a bundle of rays. This representation allows for a tight couplingwith spatial image features improving pose precision. We observe that thisrepresentation is naturally suited for set-level level transformers and developa regression-based approach that maps image patches to corresponding rays. Tocapture the inherent uncertainties in sparse-view pose inference, we adapt thisapproach to learn a denoising diffusion model which allows us to sampleplausible modes while improving performance. Our proposed methods, bothregression- and diffusion-based, demonstrate state-of-the-art performance oncamera pose estimation on CO3D while generalizing to unseen object categoriesand in-the-wild captures.</description><author>Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani</author><pubDate>Thu, 22 Feb 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14817v1</guid></item><item><title>Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks</title><link>http://arxiv.org/abs/2402.14400v1</link><description>Reliable methods for the neurodevelopmental assessment of infants areessential for early detection of medical issues that may need promptinterventions. Spontaneous motor activity, or `kinetics', is shown to provide apowerful surrogate measure of upcoming neurodevelopment. However, itsassessment is by and large qualitative and subjective, focusing on visuallyidentified, age-specific gestures. Here, we follow an alternative approach,predicting infants' neurodevelopmental maturation based on data-drivenevaluation of individual motor patterns. We utilize 3D video recordings ofinfants processed with pose-estimation to extract spatio-temporal series ofanatomical landmarks, and apply adaptive graph convolutional networks topredict the actual age. We show that our data-driven approach achievesimprovement over traditional machine learning baselines based on manuallyengineered features.</description><author>Daniel Holmberg, Manu Airaksinen, Viviana Marchi, Andrea Guzzetta, Anna Kivi, Leena Haataja, Sampsa Vanhatalo, Teemu Roos</author><pubDate>Thu, 22 Feb 2024 09:34:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14400v1</guid></item><item><title>CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge</title><link>http://arxiv.org/abs/2402.15726v1</link><description>Most of existing category-level object pose estimation methods devote tolearning the object category information from point cloud modality. However,the scale of 3D datasets is limited due to the high cost of 3D data collectionand annotation. Consequently, the category features extracted from theselimited point cloud samples may not be comprehensive. This motivates us toinvestigate whether we can draw on knowledge of other modalities to obtaincategory information. Inspired by this motivation, we propose CLIPose, a novel6D pose framework that employs the pre-trained vision-language model to developbetter learning of object category information, which can fully leverageabundant semantic knowledge in image and text modalities. To make the 3Dencoder learn category-specific features more efficiently, we alignrepresentations of three modalities in feature space via multi-modalcontrastive learning. In addition to exploiting the pre-trained knowledge ofthe CLIP's model, we also expect it to be more sensitive with pose parameters.Therefore, we introduce a prompt tuning approach to fine-tune image encoderwhile we incorporate rotations and translations information in the textdescriptions. CLIPose achieves state-of-the-art performance on two mainstreambenchmark datasets, REAL275 and CAMERA25, and runs in real-time duringinference (40FPS).</description><author>Xiao Lin, Minghao Zhu, Ronghao Dang, Guangliang Zhou, Shaolong Shu, Feng Lin, Chengju Liu, Qijun Chen</author><pubDate>Sat, 24 Feb 2024 05:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15726v1</guid></item><item><title>OCT2Confocal: 3D CycleGAN based Translation of Retinal OCT Images to Confocal Microscopy</title><link>http://arxiv.org/abs/2311.10902v3</link><description>Optical coherence tomography (OCT) and confocal microscopy are pivotal inretinal imaging, each presenting unique benefits and limitations. In-vivo OCToffers rapid, non-invasive imaging but can be hampered by clarity issues andmotion artifacts. Ex-vivo confocal microscopy provides high-resolution,cellular detailed color images but is invasive and poses ethical concerns andpotential tissue damage. To bridge these modalities, we developed a 3D CycleGANframework for unsupervised translation of in-vivo OCT to ex-vivo confocalmicroscopy images. Applied to our OCT2Confocal dataset, this frameworkeffectively translates between 3D medical data domains, capturing vascular,textural, and cellular details with precision. This marks the first attempt toexploit the inherent 3D information of OCT and translate it into the rich,detailed color domain of confocal microscopy. Assessed through quantitative andqualitative evaluations, the 3D CycleGAN framework demonstrates commendableimage fidelity and quality, outperforming existing methods despite theconstraints of limited data. This non-invasive generation of retinal confocalimages has the potential to further enhance diagnostic and monitoringcapabilities in ophthalmology. Our source code and OCT2Confocal dataset areavailable at https://github.com/xintian-99/OCT2Confocal.</description><author>Xin Tian, Nantheera Anantrasirichai, Lindsay Nicholson, Alin Achim</author><pubDate>Sat, 17 Feb 2024 01:38:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.10902v3</guid></item><item><title>SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation</title><link>http://arxiv.org/abs/2401.13560v3</link><description>The Transformer architecture has shown a remarkable ability in modelingglobal relationships. However, it poses a significant computational challengewhen processing high-dimensional medical images. This hinders its developmentand widespread adoption in this task. Mamba, as a State Space Model (SSM),recently emerged as a notable manner for long-range dependencies in sequentialmodeling, excelling in natural language processing filed with its remarkablememory efficiency and computational speed. Inspired by its success, weintroduce SegMamba, a novel 3D medical image \textbf{Seg}mentation\textbf{Mamba} model, designed to effectively capture long-range dependencieswithin whole volume features at every scale. Our SegMamba, in contrast toTransformer-based methods, excels in whole volume feature modeling from a statespace model standpoint, maintaining superior processing speed, even with volumefeatures at a resolution of {$64\times 64\times 64$}. Comprehensive experimentson the BraTS2023 dataset demonstrate the effectiveness and efficiency of ourSegMamba. The code for SegMamba is available at:https://github.com/ge-xing/SegMamba</description><author>Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, Lei Zhu</author><pubDate>Sun, 25 Feb 2024 14:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13560v3</guid></item><item><title>AccessLens: Auto-detecting Inaccessibility of Everyday Objects</title><link>http://arxiv.org/abs/2401.15996v2</link><description>In our increasingly diverse society, everyday physical interfaces oftenpresent barriers, impacting individuals across various contexts. Thisoversight, from small cabinet knobs to identical wall switches that can posedifferent contextual challenges, highlights an imperative need for solutions.Leveraging low-cost 3D-printed augmentations such as knob magnifiers andtactile labels seems promising, yet the process of discovering unrecognizedbarriers remains challenging because disability is context-dependent. Weintroduce AccessLens, an end-to-end system designed to identify inaccessibleinterfaces in daily objects, and recommend 3D-printable augmentations foraccessibility enhancement. Our approach involves training a detector using thenovel AccessDB dataset designed to automatically recognize 21 distinctInaccessibility Classes (e.g., bar-small and round-rotate) within 6 commonobject categories (e.g., handle and knob). AccessMeta serves as a robust way tobuild a comprehensive dictionary linking these accessibility classes toopen-source 3D augmentation designs. Experiments demonstrate our detector'sperformance in detecting inaccessible objects.</description><author>Nahyun Kwon, Qian Lu, Muhammad Hasham Qazi, Joanne Liu, Changhoon Oh, Shu Kong, Jeeeun Kim</author><pubDate>Fri, 23 Feb 2024 17:06:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15996v2</guid></item><item><title>EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection</title><link>http://arxiv.org/abs/2402.15272v1</link><description>In autonomous driving, cooperative perception makes use of multi-view camerasfrom both vehicles and infrastructure, providing a global vantage point withrich semantic context of road conditions beyond a single vehicle viewpoint.Currently, two major challenges persist in vehicle-infrastructure cooperative3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-viewimages, caused by time asynchrony across cameras; $2)$ information loss intransmission process resulted from limited communication bandwidth. To addressthese issues, we propose a novel camera-based 3D detection framework for VIC3Dtask, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploitholistic perspectives from both vehicles and infrastructure, we proposeMulti-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM)modules to enhance infrastructure and vehicle features at scale, spatial, andchannel levels to correct the pose error introduced by camera asynchrony. Wealso introduce a Feature Compression (FC) module with channel and spatialcompression blocks for transmission efficiency. Experiments show that EMIFFachieves SOTA on DAIR-V2X-C datasets, significantly outperforming previousearly-fusion and late-fusion methods with comparable transmission costs.</description><author>Zhe Wang, Siqi Fan, Xiaoliang Huo, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang</author><pubDate>Fri, 23 Feb 2024 11:35:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15272v1</guid></item><item><title>Multi-Model 3D Registration: Finding Multiple Moving Objects in Cluttered Point Clouds</title><link>http://arxiv.org/abs/2402.10865v1</link><description>We investigate a variation of the 3D registration problem, named multi-model3D registration. In the multi-model registration problem, we are given twopoint clouds picturing a set of objects at different poses (and possiblyincluding points belonging to the background) and we want to simultaneouslyreconstruct how all objects moved between the two point clouds. This setupgeneralizes standard 3D registration where one wants to reconstruct a singlepose, e.g., the motion of the sensor picturing a static scene. Moreover, itprovides a mathematically grounded formulation for relevant roboticsapplications, e.g., where a depth sensor onboard a robot perceives a dynamicscene and has the goal of estimating its own motion (from the static portion ofthe scene) while simultaneously recovering the motion of all dynamic objects.We assume a correspondence-based setup where we have putative matches betweenthe two point clouds and consider the practical case where thesecorrespondences are plagued with outliers. We then propose a simple approachbased on Expectation-Maximization (EM) and establish theoretical conditionsunder which the EM approach converges to the ground truth. We evaluate theapproach in simulated and real datasets ranging from table-top scenes toself-driving scenarios and demonstrate its effectiveness when combined withstate-of-the-art scene flow methods to establish dense correspondences.</description><author>David Jin, Sushrut Karmalkar, Harry Zhang, Luca Carlone</author><pubDate>Fri, 16 Feb 2024 18:01:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10865v1</guid></item><item><title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</title><link>http://arxiv.org/abs/2309.09737v5</link><description>Mobile autonomy relies on the precise perception of dynamic environments.Robustly tracking moving objects in 3D world thus plays a pivotal role forapplications like trajectory prediction, obstacle avoidance, and path planning.While most current methods utilize LiDARs or cameras for Multiple ObjectTracking (MOT), the capabilities of 4D imaging radars remain largelyunexplored. Recognizing the challenges posed by radar noise and point sparsityin 4D radar data, we introduce RaTrack, an innovative solution tailored forradar-based tracking. Bypassing the typical reliance on specific object typesand 3D bounding boxes, our method focuses on motion segmentation andclustering, enriched by a motion estimation module. Evaluated on theView-of-Delft dataset, RaTrack showcases superior tracking precision of movingobjects, largely surpassing the performance of the state of the art. We releaseour code and model at https://github.com/LJacksonPan/RaTrack.</description><author>Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu</author><pubDate>Sat, 24 Feb 2024 17:55:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09737v5</guid></item><item><title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</title><link>http://arxiv.org/abs/2309.09737v4</link><description>Mobile autonomy relies on the precise perception of dynamic environments.Robustly tracking moving objects in 3D world thus plays a pivotal role forapplications like trajectory prediction, obstacle avoidance, and path planning.While most current methods utilize LiDARs or cameras for Multiple ObjectTracking (MOT), the capabilities of 4D imaging radars remain largelyunexplored. Recognizing the challenges posed by radar noise and point sparsityin 4D radar data, we introduce RaTrack, an innovative solution tailored forradar-based tracking. Bypassing the typical reliance on specific object typesand 3D bounding boxes, our method focuses on motion segmentation andclustering, enriched by a motion estimation module. Evaluated on theView-of-Delft dataset, RaTrack showcases superior tracking precision of movingobjects, largely surpassing the performance of the state of the art. We releaseour code and model at https://github.com/LJacksonPan/RaTrack.</description><author>Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu</author><pubDate>Mon, 19 Feb 2024 17:53:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09737v4</guid></item><item><title>MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection</title><link>http://arxiv.org/abs/2402.11677v1</link><description>Multi-modal 3D object detection models for automated driving havedemonstrated exceptional performance on computer vision benchmarks likenuScenes. However, their reliance on densely sampled LiDAR point clouds andmeticulously calibrated sensor arrays poses challenges for real-worldapplications. Issues such as sensor misalignment, miscalibration, and disparatesampling frequencies lead to spatial and temporal misalignment in data fromLiDAR and cameras. Additionally, the integrity of LiDAR and camera data isoften compromised by adverse environmental conditions such as inclementweather, leading to occlusions and noise interference. To address thischallenge, we introduce MultiCorrupt, a comprehensive benchmark designed toevaluate the robustness of multi-modal 3D object detectors against ten distincttypes of corruptions. We evaluate five state-of-the-art multi-modal detectorson MultiCorrupt and analyze their performance in terms of their resistanceability. Our results show that existing methods exhibit varying degrees ofrobustness depending on the type of corruption and their fusion strategy. Weprovide insights into which multi-modal design choices make such models robustagainst certain perturbations. The dataset generation code and benchmark areopen-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt.</description><author>Till Beemelmanns, Quan Zhang, Lutz Eckstein</author><pubDate>Sun, 18 Feb 2024 18:56:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11677v1</guid></item><item><title>An Endoscopic Chisel: Intraoperative Imaging Carves 3D Anatomical Models</title><link>http://arxiv.org/abs/2402.11840v1</link><description>Purpose: Preoperative imaging plays a pivotal role in sinus surgery where CTsoffer patient-specific insights of complex anatomy, enabling real-timeintraoperative navigation to complement endoscopy imaging. However, surgeryelicits anatomical changes not represented in the preoperative model,generating an inaccurate basis for navigation during surgery progression. Methods: We propose a first vision-based approach to update the preoperative3D anatomical model leveraging intraoperative endoscopic video for navigatedsinus surgery where relative camera poses are known. We rely on comparisons ofintraoperative monocular depth estimates and preoperative depth renders toidentify modified regions. The new depths are integrated in these regionsthrough volumetric fusion in a truncated signed distance functionrepresentation to generate an intraoperative 3D model that reflects tissuemanipulation. Results: We quantitatively evaluate our approach by sequentially updatingmodels for a five-step surgical progression in an ex vivo specimen. We computethe error between correspondences from the updated model and ground-truthintraoperative CT in the region of anatomical modification. The resultingmodels show a decrease in error during surgical progression as opposed toincreasing when no update is employed. Conclusion: Our findings suggest that preoperative 3D anatomical models canbe updated using intraoperative endoscopy video in navigated sinus surgery.Future work will investigate improvements to monocular depth estimation as wellas removing the need for external navigation systems. The resulting ability tocontinuously update the patient model may provide surgeons with a more preciseunderstanding of the current anatomical state and paves the way toward adigital twin paradigm for sinus surgery.</description><author>Jan Emily Mangulabnan, Roger D. Soberanis-Mukul, Timo Teufel, Manish Sahu, Jose L. Porras, S. Swaroop Vedula, Masaru Ishii, Gregory Hager, Russell H. Taylor, Mathias Unberath</author><pubDate>Mon, 19 Feb 2024 05:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11840v1</guid></item><item><title>EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization</title><link>http://arxiv.org/abs/2402.13537v1</link><description>Camera relocalization is pivotal in computer vision, with applications in AR,drones, robotics, and autonomous driving. It estimates 3D camera position andorientation (6-DoF) from images. Unlike traditional methods like SLAM, recentstrides use deep learning for direct end-to-end pose estimation. We proposeEffLoc, a novel efficient Vision Transformer for single-image camerarelocalization. EffLoc's hierarchical layout, memory-bound self-attention, andfeed-forward layers boost memory efficiency and inter-channel communication.Our introduced sequential group attention (SGA) module enhances computationalefficiency by diversifying input features, reducing redundancy, and expandingmodel capacity. EffLoc excels in efficiency and accuracy, outperforming priormethods, such as AtLoc and MapNet. It thrives on large-scale outdoorcar-driving scenario, ensuring simplicity, end-to-end trainability, andeliminating handcrafted loss functions.</description><author>Zhendong Xiao, Changhao Chen, Shan Yang, Wu Wei</author><pubDate>Wed, 21 Feb 2024 05:26:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13537v1</guid></item><item><title>OBMO: One Bounding Box Multiple Objects for Monocular 3D Object Detection</title><link>http://arxiv.org/abs/2212.10049v2</link><description>Compared to typical multi-sensor systems, monocular 3D object detection hasattracted much attention due to its simple configuration. However, there isstill a significant gap between LiDAR-based and monocular-based methods. Inthis paper, we find that the ill-posed nature of monocular imagery can lead todepth ambiguity. Specifically, objects with different depths can appear withthe same bounding boxes and similar visual features in the 2D image.Unfortunately, the network cannot accurately distinguish different depths fromsuch non-discriminative visual features, resulting in unstable depth training.To facilitate depth learning, we propose a simple yet effective plug-and-playmodule, \underline{O}ne \underline{B}ounding Box \underline{M}ultiple\underline{O}bjects (OBMO). Concretely, we add a set of suitable pseudo labelsby shifting the 3D bounding box along the viewing frustum. To constrain thepseudo-3D labels to be reasonable, we carefully design two label scoringstrategies to represent their quality. In contrast to the original hard depthlabels, such soft pseudo labels with quality scores allow the network to learna reasonable depth range, boosting training stability and thus improving finalperformance. Extensive experiments on KITTI and Waymo benchmarks show that ourmethod significantly improves state-of-the-art monocular 3D detectors by asignificant margin (The improvements under the moderate setting on KITTIvalidation set are $\mathbf{1.82\sim 10.91\%}$ \textbf{mAP in BEV} and$\mathbf{1.18\sim 9.36\%}$ \textbf{mAP in 3D}). Codes have been released at\url{https://github.com/mrsempress/OBMO}.</description><author>Chenxi Huang, Tong He, Haidong Ren, Wenxiao Wang, Binbin Lin, Deng Cai</author><pubDate>Tue, 20 Feb 2024 08:07:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10049v2</guid></item><item><title>RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes</title><link>http://arxiv.org/abs/2309.01513v2</link><description>Room geometry is important prior information for implementing realistic 3Daudio rendering. For this reason, various room geometry inference (RGI) methodshave been developed by utilizing the time of arrival (TOA) or time differenceof arrival (TDOA) information in room impulse responses. However, theconventional RGI technique poses several assumptions, such as convex roomshapes, the number of walls known in priori, and the visibility of first-orderreflections. In this work, we introduce the deep neural network (DNN), RGI-Net,which can estimate room geometries without the aforementioned assumptions.RGI-Net learns and exploits complex relationships between high-orderreflections in room impulse responses (RIRs) and, thus, can estimate roomshapes even when the shape is non-convex or first-order reflections are missingin the RIRs. The network takes RIRs measured from a compact audio deviceequipped with a circular microphone array and a single loudspeaker, whichgreatly improves its practical applicability. RGI-Net includes the evaluationnetwork that separately evaluates the presence probability of walls, so thegeometry inference is possible without prior knowledge of the number of walls.</description><author>Inmo Yeon, Jung-Woo Choi</author><pubDate>Wed, 21 Feb 2024 06:07:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01513v2</guid></item><item><title>VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks</title><link>http://arxiv.org/abs/2402.13609v2</link><description>In recent years, object-oriented simultaneous localization and mapping (SLAM)has attracted increasing attention due to its ability to provide high-levelsemantic information while maintaining computational efficiency. Someresearchers have attempted to enhance localization accuracy by integrating themodeled object residuals into bundle adjustment. However, few have demonstratedbetter results than feature-based visual SLAM systems, as the generic coarseobject models, such as cuboids or ellipsoids, are less accurate than featurepoints. In this paper, we propose a Visual Object Odometry and Mappingframework VOOM using high-level objects and low-level points as thehierarchical landmarks in a coarse-to-fine manner instead of directly usingobject residuals in bundle adjustment. Firstly, we introduce an improvedobservation model and a novel data association method for dual quadrics,employed to represent physical objects. It facilitates the creation of a 3D mapthat closely reflects reality. Next, we use object information to enhance thedata association of feature points and consequently update the map. In thevisual object odometry backend, the updated map is employed to further optimizethe camera pose and the objects. Meanwhile, local bundle adjustment isperformed utilizing the objects and points-based covisibility graphs in ourvisual object mapping process. Experiments show that VOOM outperforms bothobject-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in termsof localization. The implementation of our method is available athttps://github.com/yutongwangBIT/VOOM.git.</description><author>Yutong Wang, Chaoyang Jiang, Xieyuanli Chen</author><pubDate>Mon, 26 Feb 2024 10:02:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13609v2</guid></item><item><title>VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks</title><link>http://arxiv.org/abs/2402.13609v1</link><description>In recent years, object-oriented simultaneous localization and mapping (SLAM)has attracted increasing attention due to its ability to provide high-levelsemantic information while maintaining computational efficiency. Someresearchers have attempted to enhance localization accuracy by integrating themodeled object residuals into bundle adjustment. However, few have demonstratedbetter results than feature-based visual SLAM systems, as the generic coarseobject models, such as cuboids or ellipsoids, are less accurate than featurepoints. In this paper, we propose a Visual Object Odometry and Mappingframework VOOM using high-level objects and low-level points as thehierarchical landmarks in a coarse-to-fine manner instead of directly usingobject residuals in bundle adjustment. Firstly, we introduce an improvedobservation model and a novel data association method for dual quadrics,employed to represent physical objects. It facilitates the creation of a 3D mapthat closely reflects reality. Next, we use object information to enhance thedata association of feature points and consequently update the map. In thevisual object odometry backend, the updated map is employed to further optimizethe camera pose and the objects. Meanwhile, local bundle adjustment isperformed utilizing the objects and points-based covisibility graphs in ourvisual object mapping process. Experiments show that VOOM outperforms bothobject-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in termsof localization. The implementation of our method is available athttps://github.com/yutongwangBIT/VOOM.git.</description><author>Yutong Wang, Chaoyang Jiang, Xieyuanli Chen</author><pubDate>Wed, 21 Feb 2024 08:22:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13609v1</guid></item><item><title>S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR</title><link>http://arxiv.org/abs/2402.14461v1</link><description>Scene graph generation (SGG) of surgical procedures is crucial in enhancingholistically cognitive intelligence in the operating room (OR). However,previous works have primarily relied on the multi-stage learning that generatessemantic scene graphs dependent on intermediate processes with pose estimationand object detection, which may compromise model efficiency and efficacy, alsoimpose extra annotation burden. In this study, we introduce a novelsingle-stage bimodal transformer framework for SGG in the OR, termedS^2Former-OR, aimed to complementally leverage multi-view 2D scenes and 3Dpoint clouds for SGG in an end-to-end manner. Concretely, our model embraces aView-Sync Transfusion scheme to encourage multi-view visual informationinteraction. Concurrently, a Geometry-Visual Cohesion operation is designed tointegrate the synergic 2D semantic features into 3D point cloud features.Moreover, based on the augmented feature, we propose a novel relation-sensitivetransformer decoder that embeds dynamic entity-pair queries and relationaltrait priors, which enables the direct prediction of entity-pair relations forgraph generation without intermediate steps. Extensive experiments havevalidated the superior SGG performance and lower computational cost ofS^2Former-OR on 4D-OR benchmark, compared with current OR-SGG methods, e.g., 3%Precision increase and 24.2M reduction in model parameters. We further comparedour method with generic single-stage SGG methods with broader metrics for acomprehensive evaluation, with consistently better performance achieved. Thecode will be made available.</description><author>Jialun Pei, Diandian Guo, Jingyang Zhang, Manxi Lin, Yueming Jin, Pheng-Ann Heng</author><pubDate>Thu, 22 Feb 2024 11:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14461v1</guid></item><item><title>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</title><link>http://arxiv.org/abs/2402.03246v2</link><description>Semantic understanding plays a crucial role in Dense SimultaneousLocalization and Mapping (SLAM). Recent advancements that integrate GaussianSplatting into SLAM systems have demonstrated its effectiveness in generatinghigh-quality renderings. Building on this progress, we propose SGS-SLAM whichprovides precise 3D semantic segmentation alongside high-fidelityreconstructions. Specifically, we propose to employ multi-channel optimizationduring the mapping process, integrating appearance, geometric, and semanticconstraints with key-frame optimization to enhance reconstruction quality.Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-artperformance in camera pose estimation, map reconstruction, and semanticsegmentation. It outperforms existing methods by a large margin meanwhilepreserves real-time rendering ability.</description><author>Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang</author><pubDate>Sun, 25 Feb 2024 17:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03246v2</guid></item><item><title>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</title><link>http://arxiv.org/abs/2305.14836v2</link><description>We introduce a novel visual question answering (VQA) task in the context ofautonomous driving, aiming to answer natural language questions based onstreet-view clues. Compared to traditional VQA tasks, VQA in autonomous drivingscenario presents more challenges. Firstly, the raw visual data aremulti-modal, including images and point clouds captured by camera and LiDAR,respectively. Secondly, the data are multi-frame due to the continuous,real-time acquisition. Thirdly, the outdoor scenes exhibit both movingforeground and static background. Existing VQA benchmarks fail to adequatelyaddress these complexities. To bridge this gap, we propose NuScenes-QA, thefirst benchmark for VQA in the autonomous driving scenario, encompassing 34Kvisual scenes and 460K question-answer pairs. Specifically, we leverageexisting 3D detection annotations to generate scene graphs and design questiontemplates manually. Subsequently, the question-answer pairs are generatedprogrammatically based on these templates. Comprehensive statistics prove thatour NuScenes-QA is a balanced large-scale benchmark with diverse questionformats. Built upon it, we develop a series of baselines that employ advanced3D detection and VQA techniques. Our extensive experiments highlight thechallenges posed by this new task. Codes and dataset are available athttps://github.com/qiantianwen/NuScenes-QA.</description><author>Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang</author><pubDate>Tue, 20 Feb 2024 05:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14836v2</guid></item><item><title>SDGE: Stereo Guided Depth Estimation for 360° Camera Sets</title><link>http://arxiv.org/abs/2402.11791v1</link><description>Depth estimation is a critical technology in autonomous driving, andmulti-camera systems are often used to achieve a 360{\deg} perception. These360{\deg} camera sets often have limited or low-quality overlap regions, makingmulti-view stereo methods infeasible for the entire image. Alternatively,monocular methods may not produce consistent cross-view predictions. To addressthese issues, we propose the Stereo Guided Depth Estimation (SGDE) method,which enhances depth estimation of the full image by explicitly utilizingmulti-view stereo results on the overlap. We suggest building virtual pinholecameras to resolve the distortion problem of fisheye cameras and unify theprocessing for the two types of 360{\deg} cameras. For handling the varyingnoise on camera poses caused by unstable movement, the approach employs aself-calibration method to obtain highly accurate relative poses of theadjacent cameras with minor overlap. These enable the use of robust stereomethods to obtain high-quality depth prior in the overlap region. This priorserves not only as an additional input but also as pseudo-labels that enhancethe accuracy of depth estimation methods and improve cross-view predictionconsistency. The effectiveness of SGDE is evaluated on one fisheye cameradataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes.Our experiments demonstrate that SGDE is effective for both supervised andself-supervised depth estimation, and highlight the potential of our method foradvancing downstream autonomous driving technologies, such as 3D objectdetection and occupancy prediction.</description><author>Jialei Xu, Xianming Liu, Junjun Jiang, Xiangyang Ji</author><pubDate>Mon, 19 Feb 2024 02:41:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11791v1</guid></item><item><title>SDGE: Stereo Guided Depth Estimation for 360$^\circ$ Camera Sets</title><link>http://arxiv.org/abs/2402.11791v2</link><description>Depth estimation is a critical technology in autonomous driving, andmulti-camera systems are often used to achieve a 360$^\circ$ perception. These360$^\circ$ camera sets often have limited or low-quality overlap regions,making multi-view stereo methods infeasible for the entire image.Alternatively, monocular methods may not produce consistent cross-viewpredictions. To address these issues, we propose the Stereo Guided DepthEstimation (SGDE) method, which enhances depth estimation of the full image byexplicitly utilizing multi-view stereo results on the overlap. We suggestbuilding virtual pinhole cameras to resolve the distortion problem of fisheyecameras and unify the processing for the two types of 360$^\circ$ cameras. Forhandling the varying noise on camera poses caused by unstable movement, theapproach employs a self-calibration method to obtain highly accurate relativeposes of the adjacent cameras with minor overlap. These enable the use ofrobust stereo methods to obtain high-quality depth prior in the overlap region.This prior serves not only as an additional input but also as pseudo-labelsthat enhance the accuracy of depth estimation methods and improve cross-viewprediction consistency. The effectiveness of SGDE is evaluated on one fisheyecamera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD andnuScenes. Our experiments demonstrate that SGDE is effective for bothsupervised and self-supervised depth estimation, and highlight the potential ofour method for advancing downstream autonomous driving technologies, such as 3Dobject detection and occupancy prediction.</description><author>Jialei Xu, Wei Yin, Dong Gong, Xianming Liu, Junjun Jiang, Xiangyang Ji</author><pubDate>Mon, 26 Feb 2024 12:01:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11791v2</guid></item><item><title>Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot</title><link>http://arxiv.org/abs/2402.14654v1</link><description>We present Multi-HMR, a strong single-shot model for multi-person 3D humanmesh recovery from a single RGB image. Predictions encompass the whole body,i.e, including hands and facial expressions, using the SMPL-X parametric modeland spatial location in the camera coordinate system. Our model detects peopleby predicting coarse 2D heatmaps of person centers, using features produced bya standard Vision Transformer (ViT) backbone. It then predicts their whole-bodypose, shape and spatial location using a new cross-attention module called theHuman Prediction Head (HPH), with one query per detected center token,attending to the entire set of features. As direct prediction of SMPL-Xparameters yields suboptimal results, we introduce CUFFS; the Close-Up Framesof Full-Body Subjects dataset, containing humans close to the camera withdiverse hand poses. We show that incorporating this dataset into trainingfurther enhances predictions, particularly for hands, enabling us to achievestate-of-the-art performance. Multi-HMR also optionally accounts for cameraintrinsics, if available, by encoding camera ray directions for each imagetoken. This simple design achieves strong performance on whole-body andbody-only benchmarks simultaneously. We train models with various backbonesizes and input resolutions. In particular, using a ViT-S backbone and$448\times448$ input images already yields a fast and competitive model withrespect to state-of-the-art methods, while considering larger models and higherresolutions further improve performance.</description><author>Fabien Baradel, Matthieu Armando, Salma Galaaoui, Romain Brégier, Philippe Weinzaepfel, Grégory Rogez, Thomas Lucas</author><pubDate>Thu, 22 Feb 2024 16:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14654v1</guid></item><item><title>CapHuman: Capture Your Moments in Parallel Universes</title><link>http://arxiv.org/abs/2402.00627v2</link><description>We concentrate on a novel human-centric image synthesis task, that is, givenonly one reference facial photograph, it is expected to generate specificindividual images with diverse head positions, poses, facial expressions, andilluminations in different contexts. To accomplish this goal, we argue that ourgenerative model should be capable of the following favorable characteristics:(1) a strong visual and semantic understanding of our world and human societyfor basic object and human image generation. (2) generalizable identitypreservation ability. (3) flexible and fine-grained head control. Recently,large pre-trained text-to-image diffusion models have shown remarkable results,serving as a powerful generative foundation. As a basis, we aim to unleash theabove two capabilities of the pre-trained model. In this work, we present a newframework named CapHuman. We embrace the "encode then learn to align" paradigm,which enables generalizable identity preservation for new individuals withoutcumbersome tuning at inference. CapHuman encodes identity features and thenlearns to align them into the latent space. Moreover, we introduce the 3Dfacial prior to equip our model with control over the human head in a flexibleand 3D-consistent manner. Extensive qualitative and quantitative analysesdemonstrate our CapHuman can produce well-identity-preserved, photo-realistic,and high-fidelity portraits with content-rich representations and various headrenditions, superior to established baselines. Code and checkpoint will bereleased at https://github.com/VamosC/CapHuman.</description><author>Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang</author><pubDate>Mon, 19 Feb 2024 11:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.00627v2</guid></item><item><title>AdvGPS: Adversarial GPS for Multi-Agent Perception Attack</title><link>http://arxiv.org/abs/2401.17499v2</link><description>The multi-agent perception system collects visual data from sensors locatedon various agents and leverages their relative poses determined by GPS signalsto effectively fuse information, mitigating the limitations of single-agentsensing, such as occlusion. However, the precision of GPS signals can beinfluenced by a range of factors, including wireless transmission andobstructions like buildings. Given the pivotal role of GPS signals inperception fusion and the potential for various interference, it becomesimperative to investigate whether specific GPS signals can easily mislead themulti-agent perception system. To address this concern, we frame the task as anadversarial attack challenge and introduce \textsc{AdvGPS}, a method capable ofgenerating adversarial GPS signals which are also stealthy for individualagents within the system, significantly reducing object detection accuracy. Toenhance the success rates of these attacks in a black-box scenario, weintroduce three types of statistically sensitive natural discrepancies:appearance-based discrepancy, distribution-based discrepancy, and task-awarediscrepancy. Our extensive experiments on the OPV2V dataset demonstrate thatthese attacks substantially undermine the performance of state-of-the-artmethods, showcasing remarkable transferability across different point cloudbased 3D detection systems. This alarming revelation underscores the pressingneed to address security implications within multi-agent perception systems,thereby underscoring a critical area of research.</description><author>Jinlong Li, Baolu Li, Xinyu Liu, Jianwu Fang, Felix Juefei-Xu, Qing Guo, Hongkai Yu</author><pubDate>Tue, 20 Feb 2024 20:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17499v2</guid></item></channel></rss>